qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	what is the role of fire loading and cleaning in preventing fire spread in buildings	Fire loading refers to how much material in the building can burn - more flammable items mean higher fire loading. The more things that can burn (wood, cables, paper), the more important it is to find ways to reduce fire spread. Cleaning helps reduce spread, as tidy stacks with walkways between them can limit fire spread to a degree.	['Fire is a common element to the work place; not one of us is immune or can ignore fire in either our homes or workplaces.\nBritish Law now requires specific Fire Risk Assessment in the workplace that will be the first step in assessing the current fire our precautions and where we can make improvements\nIn short, the Fire Safety has a number of factors :.\n- Preventing fire begins\n- If a fire happens sure as to ensure that people can escape and hopefully minimize damage to buildings and equipment\n- ensure that the fire can be detected – to give everyone more opportunities to escape\n- To ensure that you have adequate procedures fire emergency\n- Performance firefighting – generally unwise to encourage its staff to fight fires -. But automatic sprinklers not only to fight the fire while reducing the surface spread and give people more escape time\nFire Prevention This is a key stage – ensures that people can escape equally – but if we can stop the fire gone risks are vastly reduced\nFire is more complex than. we can get brief article – the basic and obvious controls to ensure that you manage sources of heat, controlling combustible materials and flammable substances in the workplace\n- Smoking is obviously guilty. work with virtually worldwide today so many old reasons are no longer relevant.\n- heat – especially those with open flame such as a space heater – need to be managed -. They obviously need to be kept away from anything flammable or combustible\n- Flammable items (those with warning symbols represent them as well as most of the aerosols) have the care they can ignite without flame – so they need to be kept cool and safe places. They should be kept in a suitable container and only the minimum amount held in the work -. Stores should be kept in flammable cabinets\n- Ensure electrics are in good condition and properly ar – PAT testing helps ensure this. It is important that you do not overload sockets and keep equipment clear paper and even dust for a long time\nStopping Fire Spread\nThe features that help stop the fires start also help reduce the risk of fire is started\n· Experts refer Fire Loading -. which means how much material will burn the building. The more things than can burn more fire loading. Wood, cables, paper fire or extreme heat to turn – but if the fire starts will feed the fire and encourage the fire to grow; Thus, the more flammable items on the site, the more important is to find other ways to reduce the spread of fire\n· Cleaning helps -. tidy stack with walkways between not fire spread to a degree\n· Fire Walls are usually in buildings – be they good or brick plasterboard – they will help contain the fire in one area -. but not if you have drilled holes or install new, unsuitable doors into them\n· fire – are there to help contain the fires – they work by sitting in the fire walls and containing the fire to a smaller area 30 minutes or more. It is important that these are kept closed or automatic shutdown device possible. If they are left open fire simply spread through them and further reduce safety and massively reduce the time to evacuate safely\n· Fire Shutters :. In a large retail space, high risk areas and a warehouse fire shutters replace fire doors – these operate either when the alarm is triggered or when the temperature reaches a very high level. It is important that these are kept clear at all times\n· Sprinkler Systems: .. This time turn help control or even extinguish a fire\nAutomated systems reduce fore risks they work 24 hours a day and should cover the entire workplace – they need to be maintained on a regular basis -. at least annually\nCall Points – break glass fire call points shall be distributed construction and also every fire risk to have a break glass point next to it – if fire this allows people to warn you fire has occurred. Again this should be checked and tested regularly\nWhen the alarm is activated it is important staff and guests can escape – and indeed make an escape. . There are maximum distances set by various factors – it may need expert advice if you are unsure\nBut working through the basics:\n· In general, you should always have an option. two exit routes – if one is blocked by fire, you have another option. Not all older buildings have this\n· Make sure any exit route is clear and available -. They should be entered and checked weekly to ensure that no locked or blocked. Equally they are given to allow you to escape you should ask whether it is wise to store things in this\n· Fire Stops height should result in a protected way -. These are areas that when you are relatively safe – so that they will always have at least one set of fire doors leading into them and usually sit with fire walls around them – this means the staff and guests have 30 or more minutes to escape the house to the outside\n· fire drills – allow people to understand what to do in the event of a fire, along with briefings they fire training techniques. Fire drills must be conducted at least every 6 months and must be registered.\n· You need a system to ensure that all is clear. Outdated roll calls have its value especially in small businesses but not limited to large companies or where visitors are frequent. Fire Wardens tend to be more efficient solution – however, they must be trained and clearly briefed on their responsibilities. They easily sweep their house to verify that everyone has left and then report to the person in charge or the fire department.\nThere are more sophisticated system available as Smoke vents that can enhance safe evacuation period but do need experts to help design and set\nFire Fighting :.\nThis can be important to fire safety, but as in most cases you should aim to evacuate staff not encourage them to fight fires. Now sprinklers are automatic and do not require human input which makes them very safe option\n· Fire extinguishers should be placed at regular intervals -. But on every fire risk to have two fire extinguishers clearly positioned them. This needs to be maintained and records kept\n· hoses can have value – but again need training to use effectively and safely, and again needed to maintain\nFire risk assessment allows you to fully in mind. All factors determining the appropriate set of precautions; for small businesses, this can be done quite quickly using the resources of the local regiment within you, larger companies can benefit from general advice and assistance often consultants safety or consultant – but more complex companies will require specialized fire counseling and there are many companies that provide this following recent legislative changes that performed obligation in the next brigade to the company itself.']	['<urn:uuid:39d7ed44-0854-49a1-bb03-8189bd4ceb18>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T06:04:36.674696	15	59	1203
2	How many Japanese planes did the Flying Tigers destroy?	The Flying Tigers destroyed 300 Japanese planes since December 20, 1941, while losing only 14 pilots on combat missions.	"['Germany: At the National Socialist first party congress held at Weimar, Germany, Hitler\'s personal authority over the party is accepted by the majority and his position of party Fuhrer is formally approved.\n1937 — , July 4\nSpain: Day 353 of 985 of the Spanish Civil War.\n1938 — , July 4\nSpain: Day 718 of 985 of the Spanish Civil War.\nCBI - China: Day 363 of 2,987 of the 2nd Sino-Japanese War.\nDay 24 of 139 of the Battle of Wuhan.\n1939 — , July 4\nUSA: Lou Gehrig gives his famous ""Today, I consider myself the luckiest man on the face of the earth."" speech.\nCBI - China: Day 728 of 2,987 of the 2nd Sino-Japanese War.\nDay 21 of 68 of the Battle of Tianjin.\nCBI - Mongolia: Day 55 of 129 of the Battle of Khalkhin Gol, a border dispute between the Soviet Union and Japan.\n1940 — , July 4\nUK: Winston Churchill receives his first standing ovation in the House of Commons as the Prime Minister of the United Kingdom after delivering a speech justifying the attack on French warships.\nAtlantic: German Stukas and motored torpedo boats attack a British convoy south of Bournemouth, England, near Portland, sinking 5 merchant ships.\nETO - UK: The Dame of Sark, ruler of the island of Sark in the English Channel, surrenders to German troops.\nETO - France: In direct response to the devastating British attack on the French fleet at Mers-el-Kebir, the Vichy French government of Marshal Pétain breaks off diplomatic relations with Britain.\nMTO - Egypt: Italian bombers raid on Malta and at Alexandria.\nMTO - Algeria: The British sub HMS PANDORA sinks the French gunboat RIGAULT DE GENOUILLY off Oran. French bombers attack the British fleet at Gibraltar, causing no damage.\nEast Africa: Day 25 of 537 of Italy\'s East African campaign in the lands south of Egypt. Italian troops capture a number of British forts, including Kassala and Gallabat on the Sudanese border.\nCBI - China: Day 1,094 of 2,987 of the 2nd Sino-Japanese War.\nDay 233 of 381 of the Battle of South Guangxi.\n1941 — , July 4\nUSA: In an Independence Day broadcast, Roosevelt warns the American public that the USA, ""will never survive as a happy and prosperous oasis in the middle of a desert of dictatorship.""\nUK: British Communist Party decides to stop campaigning for peace and instead will support the national war effort.\nRussian Front - Finland: Day 6 of 142 of Operation SILVER FOX, a joint German-Finnish campaign to capture the Russian port of Murmansk in the Arctic.\nRussian Front - Finland: Day 4 of 140 of Operation ARCTIC FOX, a joint German-Finnish campaign against Soviet Northern Front defenses at Salla, Finland.\nRussian Front - Finland: Day 13 of 164 of the Battle of Hanko.\nRussian Front: Day 13 of 167 of Germany\'s Operation BARBAROSSA, the invasion of the USSR.\nRussian Front - Center: German Army Group Centre captures Ostrov in northern Russia.\nRussian Front - South: Day 3 of 21 of the Battle of Bessarabia, Russia. German and Romanian troops continue their attack at Bessarabia to take the land and city that Romania was forced to cede to the USSR a year ago.\nRussian Front - South: German and Romanian troops keep advancing toward Vinnitsa and the Black Sea port of Odessa, Ukraine.\nMTO - Libya: Day 86 of 256 of the Siege of Tobruk.\nMiddle East: Day 27 of 37 of the Battle for Syria and Lebanon.\nEast Africa: Day 390 of 537 of Italy\'s East African campaign in the lands south of Egypt.\nCBI - China: Day 1,459 of 2,987 of the 2nd Sino-Japanese War.\n1942 — , July 4\nAtlantic: Day 8 of 14 of Germany\'s Hunt for Allied Convoy PQ-17. During this hunt, U-boats and the Luftwaffe will sink 24 merchant ships. Today\'s raids claim 9 ships as the convoy commander orders the convoy to scatter.\nETO - Germany: The RAF makes their third 1,000-plane raid in Germany, hitting at Bremen.\nETO - Netherlands: The US 8th Air Force flying A-20s join the RAF in a joint attack on airfields in the Netherlands.\nGermany: The Auschwitz Concentration Camp begins mass gassings.\nRussian Front - North: Day 300 of 872 of the Siege of Leningrad.\nRussian Front - North: Day 61 of 658 of the Siege of the Kholm Pocket.\nRussian Front - Center: Day 3 of 22 of Germany\'s Operation SEYDLITZ, a plan to trap and capture numerous Soviet troops.\nRussian Front - South: Day 248 of 248 of the Siege of Sevastopol, Crimean Peninsula. German troops secure the city.\nRussian Front - South: Day 7 of 27 of the Battle of Voronezh, Russia.\nRussian Front - South: Day 7 of 150 of Germany\'s CASE BLUE, the failed offensive to take the Caucasus oil fields.\nMTO - Egypt: Day 4 of 27 of the 1st Battle of El Alamein. While Rommel is waiting for more tanks and supplies to be brought up, the British 8th Army makes a series of attacks.\nMTO - Libya: US B-24s bomb the harbor at Benghazi.\nEast Africa: Day 61 of 186 of the Battle of Madagascar.\nCBI - China: Day 1,824 of 2,987 of the 2nd Sino-Japanese War.\nDay 51 of 124 of Japan\'s Zhejiang-Jiangxi Campaign, launched to punish anyone suspected of aiding the Doolittle raiders in China. Roughly 250,000 Chinese will be killed.\nThe 1st American Volunteer Group, aka The Flying Tigers, is disbanded and ushered into the newly activated China Air Task Force under command of Brigadier General Clare L Chennault. The old unit destroyed 300 Japanese planes since 20 Dec 41, while losing only 14 pilots on combat missions. Wasting no time, they immediately bomb the airfield at Tien Ho and shoot down 13 enemy planes over Kweilin.\n[+] show related dates\nDates related to the Flying Tigers...\n15 Apr 41: FDR secretly approves formation of the AVG\n04 Jul 42: Flying Tigers disbanded, ushered into US task force\n[–] hide related dates\nPTO - Alaska: Day 28 of 435 of the Battle of Kiska, Aleutian Islands.\nPTO - Malaya: Day 136 of 357 of the Battle of Timor Island.\nPTO - New Guinea: US 5th Air Force bombs airfields at Lae and Salamaua.\nPTO - Solomon Islands: Allied reconnaissance reports that the Japanese have begun building an airfield on Guadalcanal.\n1943 — , July 4\nETO - UK: Journalist John Steinbeck of the New York Herald Tribune newspaper spends a day with British airmen at an airfield in England. His experiences on this day will later be found in his work ""Once There Was a War"" in a chapter titled ""Waiting.""\nETO - France: US 8th Air Force B-17s bomb aircraft factories at Le Mans and Nanes, while other B-17s hit the submarine yards at La Pallice.\nRussian Front - North: Day 665 of 872 of the Siege of Leningrad.\nRussian Front - North: Day 426 of 658 of the Siege of the Kholm Pocket.\nRussian Front - South: Germany relaunches Operation CITADEL, aimed at encircling and destroying Soviet troops in the Orel-Belgorod salient near Kursk. However, the Soviets are launching their own offensive.\nMTO: General Sikorski and other members of the Polish government-in-exile are killed in a plane crash in Gibraltar. Sabotage is suspected, but is never proven.\nMTO - Italy: US 9th and 12th Air Forces bombs several targets in Sicily and Italy.\nCBI - Burma: US 10th Air Force attacks the Shweli road bridge, but fail to cause appreciable damage.\nCBI - China: Day 2,189 of 2,987 of the 2nd Sino-Japanese War.\nPTO: The USS SNOOK attacks a Japanese convoy, sinking the cargo ships KOKI MARU and LIVERPOOL MARU and damaging the ATLANTIC MARU.\nPTO: The USS POMPANO sinks the Japanese seaplane carrier SAGARA MARU.\nPTO: Day 39 of 47 adrift in a raft for the survivors of B-24 GREEN HORNET that crashed 850 miles from Hawaii.\nPTO - Alaska: Day 393 of 435 of the Battle of Kiska, Aleutian Islands.\nPTO - New Guinea: Day 74 of 148 of the 2nd Battle of Lae-Salamaua. US 5th and 13th Air Forces provide air support.\nPTO - Solomon Islands: Day 15 of 67 of the Battle of New Georgia. US 5th and 13th Air Forces provide air support. US Marines and US Army troops secure Kaeruka and Cheke Point on Vangunu Island, just southeast of New Georgia. The Japanese make their last large daylight air raid on Rendova.\n1944 — , July 4\nETO - France: Day 29 of 49 of Operation OVERLORD, the Allied invasion of Normandy, France, known forever simply as D-Day. D-Day+28: Troops and supplies continue to pour in, including US nurses assigned to field hospitals to care for wounded soldiers. To give an idea of how hard the fighting is, today one US Division gains just 200 yards and captures 6 Germans in exchange for taking almost 1,400 casualties.\nUS nurses walk along a beach in Normandy, France, on July 4, 1944, after they had waded through the surf from their landing craft. They are on their way to field hospitals to care for wounded allied soldiers. (AP photo)\nMore than 59,000 American nurses served in the Army Nurse Corps during World War II. Nurses worked closer to the front lines than they ever had before. Within the ""chain of evacuation"" established by the Army Medical Department during the war, nurses served under fire in field and evacuation hospitals, on hospital trains and ships and as flight nurses on medical transport planes. The skill and dedication of these nurses contributed to the extremely low post-injury mortality rate among American military forces in every theater of the war. Overall, fewer than 4 percent of the American soldiers who received medical care in the field or underwent evacuation died from wounds or disease.\n01 Jun 41: Red Cross unifies services as ""Services to Armed Forces""\n04 Jul 44: US nurses storm the beaches of Normandy\n24 Sep 44: Flight Nurse earns the Distinguished Flying Cross\n28 Apr 45: Six nurses killed in kamikaze attack on hospital ship\nETO - UK: Day 22 of 86 of the V-1 ""Buzz Bomb"" offensive on Britain.\nETO - France: Day 29 of 62 of the Battle of Caen.\nETO - Denmark: The general strike in Copenhagen ends with the Germans rescinding their curfew to avoid future such uprisings in Denmark.\nGermany: With intelligence obtained by infiltrating the meeting in Berlin on 22 June 1944 between Claus von Stauffenberg and German communists, the Gestapo makes several arrests.\nRussian Front - Finland: Day 10 of 15 of the Battle of Tali-Ihantala. This becomes the largest battle in Scandinavian history.\nRussian Front - Finland: Day 14 of 50 of the Battle of Karelia. Soviet troops continue their offensive against the Finns in eastern Karelia between Lake Ladoga and Lake Onega in northern Russia.\nRussian Front - North: Day 154 of 191 of the Battle of the Narva Isthmus, Estonia. Both German and Soviet troops remain locked in their defensive positions.\nMTO - Romania: US 15th Air Force bombs targets in Romania.\nMTO - Italy: Day 19 of 34 of the Battle of Ancona (north of Rome). Allied Air Forces provide air support. The US 5th Army clears parts of Rosignano, overrun Mount Vitalba and push into Casole d\'Elsa.\nCBI - Burma: Day 122 of 166 of the UK\'s Operation THURSDAY. US 10th Air Force provides air support.\nCBI - Burma: Day 117 of 147 of the Battle of Myitkyina. US 10th Air Force provides air support.\nCBI - Burma: Day 95 of 302 of the Chinese Salween Offensive. US 14th Air Force provides air support.\nCBI - China: Day 2,555 of 2,987 of the 2nd Sino-Japanese War.\nDay 79 of 259 of Japan\'s Operation ICHI-GO.\nDay 13 of 48 of the Battle of Hengyang. US 14th Air Force provides air support.\nCBI - India: Japanese General Wazakazu Kawabe officially terminates Operation U-GO.\nPTO: Radio communications between the USS S-28 and US Coast Guard cutter RELIANCE is broken and the USS S-28 is never heard from again.\nPTO: In the morning, the USS TANG sinks the Japanese freighter ASUKAZAN MARU and in the afternoon sinks the transport YAMAOKA MARU.\nPTO - Caroline Islands: US 7th Air Force B-24s bomb Truk Atoll.\nPTO - Dutch New Guinea: Day 3 of 61 of the Battle of Noemfoor. The airfield at Yebrurro is secured by US troops.\nPTO - Mariana Islands: Day 20 of 25 of the Battle of Saipan. US 7th Air Force provides air support.\nPTO - New Guinea: Day 39 of 83 of the Battle of Biak. There are still 3,000 Japanese soldiers on the island who won\'t give up.\nPTO - New Guinea: Day 203 of 597 of the Battle of New Britain. US 13th Air Force provides air support.\nPTO - New Guinea: Day 74 of 481 of the Battle of Western New Guinea. 5th Air Force provides air support.\nPTO - New Guinea: Day 21 of 80 of the Battle of Lone Tree Hill.\nPTO - Solomon Islands: Day 247 of 295 of the Battle of the Bougainville Islands. US 13th Air Force provides air support.\n1945 — , July 4\nUK: Canadian troops begin a riot in Aldershot, England, in protest against the delay in shipping them home after their service in Europe. Around 500 troops are involved, although little damage is caused and no one is seriously injured.\nGermany: The British 7th Armoured Division, aka The Desert Rats, enters Berlin to establish the British sector.\nCBI: US 14th Air Force attack targets in French Indochina and southern and eastern China.\nCBI - China: Day 2,920 of 2,987 of the 2nd Sino-Japanese War.\nPTO - Alaska: To celebrate the 4th of July, US 11th Air Force B-24s bomb the Kataoka naval base on Shimushu Island with Napalm bombs.\nPTO - Borneo: Day 25 of 67 of the Battle of North Borneo. US 5th and 13th Air Forces provide air support.\nPTO - Dutch East Indies: Day 4 of 21 of the 2nd Battle of Balikpapan.\nPTO - Japan: US 20th Air Force attacks the Yokosuka naval base and airfields in the Tokyo area.\nPTO - New Guinea: Day 555 of 597 of the Battle of New Britain. US 10th Air Force provides air support.\nPTO - New Guinea: Day 439 of 481 of the Battle of Western New Guinea. US 10th Air Force provides air support.\nPTO - Philippines: Day 257 of 299 of the 2nd Battle of the Philippines, aka the Liberation of the Philippines or the Philippines Campaign.\nPTO - Philippines: Day 202 of 244 of the Battle of Luzon. The battle is said to over but hold-outs will continue fighting for a couple of more months.\nPTO - Philippines: Day 117 of 159 of the Battle of Mindanao Island. The battle is said to over but hold-outs will continue fighting for a couple of more months.\nPTO - Philippines: Day 109 of 135 of the Battle of the Visayas region. The battle is said to over but hold-outs will continue fighting for a couple of more months.\n1946 — , July 4\nUSA: The Philippine Islands is granted independence by the United States, forming the Republic of the Philippines.\nDay-By-Day listings for July 4 were last modified on Sunday, January 31, 2016']"	['<urn:uuid:bc96a0d2-1751-4e93-ab40-b1d49c7889da>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T06:04:36.674696	9	19	2555
3	duo mobile app cost money	No, there is no cost to download the DUO mobile app on your smartphone. The DUO mobile app will send push notifications to your smartphone.	"['Learn, Do, Secure\nWhat is MFA?\nMulti-factor Authentication is a tool that provides additional protection for user accounts by ensuring that account owners verify account access requests before any access is granted. This is achieved by providing additional verification along with your Net ID and password when logging into some NEIU applications and accounts (Nmail, VPN, NEIUworks, and Employee Self-Service Portal).\nWhy is NEIU implementing MFA (DUO MFA)?\nCompromised user accounts have become the easiest way that cybercriminals use to gain access to your data, attack organizations to infiltrate IT systems, steal information or disrupt an organization\'s operations. Northeastern Illinois University is implementing DUO MFA to provide strong access control to your data and the information and IT resources you use for work. By using MFA, your user account has more security making it less susceptible to being compromised since your Net ID and password along with an additional form of verification is required, which DUO provides using the DUO mobile app on your device or a DUO hardware token.\nAm I required to use DUO Multi-Factor Authentication?\nYes. Staff and faculty will only be able to access the applications protected by DUO MFA once their smartphone devices are enrolled to use DUO mobile app or when they have been issued DUO hardware tokens.\nWhere can I download the duo mobile app?\nThe DUO mobile app can be downloaded from Apple App Store and Android Play Store.\nHOW DO I ENROLL MY SMARTPHONE TO USE DUO MFA?\nDoes it cost me anything to use DUO MFA on my smartphone?\nNo, there is no cost to download the DUO mobile app on your smartphone. The DUO mobile app will send push notifications to your smartphone.\nDo I have to use DUO every time I want to log into an application?\nNo. DUO\'s ""trust this browser"" and ""remember me"" features make it convenient to use DUO by reducing the number of times that you\'ll be prompted for DUO MFA if you are using the same device and the same browser. These features are set to remember you for 12 hours.\nTo use these features, follow the DUO MFA Remembering a Connection Guide.\nNote: User experience may vary depending on the application a user is logging into.\nIf I use my device for DUO, what information will DUO have access to?\nNote: DUO mobile app will never access your photos and will only use your camera to scan the QR code to help set up DUO. If you have enabled access to your camera when adding your DUO account, you can remove the permissions by going to the Apps section under the device settings, looking for the DUO mobile app\'s permissions, and disabling the camera access.\nIf I have no data plan or internet connection, can I still use DUO?\nIf required, the DUO mobile app provides options that work without a data plan or internet connection. Although if you have an internet connection, the app makes two-factor authentication as easy as sending you a DUO Push notification but if you don’t, you can use the app to generate a six-digit passcode and enter that instead.\nRead more about DUO data usage.\nI do not wish to use my personal device for DUO?\nUsing the DUO mobile app is preferred but DUO hardware tokens can be assigned by UTS User Services.\nHow can I request a DUO hardware token?\nSend an email to UTS User Services at email@example.com, or call (773) 442-HELP (4357) to set up an appointment to collect a hardware token.\nCan I register a new phone number after I have the DUO MFA service?\nYes. Please contact UTS User Services to re-enroll your new device.\nCAN YOU USE A HARDWARE TOKEN AND A PHONE?\nNo. Smartphones are the preferred option. If you don\'t have a smartphone, you can request a hardware token.\nI have lost my smartphone or hardware token. What should I do?\nYou should inform the UTS User Services as soon as possible to disable DUO on the lost device and to provide an alternate option.\nCan I use the Duo Mobile App or hardware token when traveling abroad?\nYes, you can use DUO mobile app and hardware tokens when traveling to receive push notifications or passcodes.\nDO I NEED DUO MFA TO USE university ZOOM?\nNEIU Zoom requires authentication using your NEIU credentials. You will need to use DUO for additional verification to authenticate into your Zoom account.\nNote: If you are already logged into Nmail, you will not need to authenticate to use Zoom when using the same browser.\nI forgot my smartphone or hardware token at home, what should I do?\nYou should contact UTS User Services for a temporary option.\nIf I think that my smartphone has been compromised, what should I do?\nYou should inform the UTS User Services as soon as possible to disable DUO Service. Learn more about securing your mobile devices.\nI have lost or damaged my hardware token, what should I do?\nPlease report it to UTS User Services as soon as possible. Depending on the situation, you may be required to pay for a new hardware token.\nI receive an unsolicited duo PUSH NOTIFICATION, what should I DO?\nIf you receive a DUO Push that you did not request, deny the request and change your account password immediately. Denied requests are automatically reported to UTS.\nIf I have more questions about DUO MFA, who should I contact?\nSend an email to UTS User Services at firstname.lastname@example.org, or call (773) 442-HELP (4357).']"	['<urn:uuid:fc4e5db2-08de-4c8a-9529-5e0a688df49a>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	5	25	923
4	As a marine biologist studying invasive species management, I'm curious about how the Tiburon Laboratory's research combines citizen science initiatives with academic training and professional development in San Francisco Bay. Could you explain their integrated approach?	The Tiburon Laboratory combines several approaches in their research work. They run citizen science networks called PlateWatch and Green Crab Watch that monitor non-native species along the Pacific Coast from Alaska to California, involving scientists from local agencies, citizens, school groups and native groups. Within San Francisco Bay specifically, they have citizen science programs tracking Olympia oyster recruitment and introduced snail movement. They also mentor students through various programs, hosting undergraduate interns each summer through STAR and REU programs at San Francisco State University and the Smithsonian Internship program. The lab is integrated with SF State's Estuary & Ocean Science Center, where students in the MS in Interdisciplinary Marine and Estuarine Sciences program can develop applied knowledge through research while also gaining professional experience through required internships and training in professional skills.	"['Tiburon Laboratory: Invasions and Community Ecology on the US West Coast\nOur core mission is to track the pulse of biotic change in the estuaries and coastlines along the Pacific Coast of North America. We identify non-native species in San Francisco Bay and surrounding coastlines, from Alaska to Panama, learn how their life cycles work in these new environs, and determine what effects they might be having on resident communities. We seek, in short, to make the consequences of moving species to strange new oceans both known and predictable, including how these species will interact with changing climate regimes.\nSince 2000, our researchers have maintained a laboratory on San Francisco Bay, at San Francisco State University’s marine research and teaching facility, the Estuary & Ocean Science Center (EOS). A 2013 Memorandum of Understanding between the Smithsonian and San Francisco State University expanded our relationship and signified our joint commitment to understand and protect the marine environment. With our colleagues at RTC and the San Francisco Bay National Estuarine Research Reserve, we are excited to be part of a community of researchers engaged in working out the effects of global change, invasions, and many other pressing issues in truly complementary fashion.\nWe have undertaken intensive, long-term studies of marine and estuarine communities across several habitats in California and especially San Francisco Bay, including hard substrates (fouling), soft sediment benthos, and plankton. We have used settlement panels to study fouling community development and invasion impacts since 2000 as part of an international effort to understand and document patterns of the species distributions and invasion in marine coastal waters. Recent studies have paired traditional morphological approaches to species identification with genetic approaches (with our collaborator, Dr. Jonathan Geller) to get a more complete picture of the species in California bays and near-shore environments.\nVessels of all kinds move species within and between bays. Our scientific divers conduct in-water hull surveys of boats and ships in West Coast ports and beyond to assess the role of vessel biofouling in moving organisms from port to port. Read more about our work on cargo and cruise ships in the Smithsonian Magazine.\nWe have studied ways to make eradication of targeted non-native pest species more efficient and effective, and have undertaken numerous eradication efforts, including the Japanese Mudsnail (Batillaria attramentaria) and the colonial tunicate Didemnum vexillum. Most recently, we worked with UC Davis and NOAA to monitor and eradicate a local population of non-native European Green Crabs (Carcinus maenas) near Stinson Beach, CA, and have investigated the possibility of functional eradication of the Atlantic oyster drill (Urosalpinx cinerea) as part of native Olympia oyster restoration efforts.\nNative Olympia Oysters\nWe collaborate with a number of agencies and research facilities to investigate native Olympia oyster (Ostrea lurida) demographics along California’s shorelines. Using a rigorous combination of field surveys, laboratory experiments, and population modeling, we examine the influence of environmental factors, climate change, and non-native species on the distribution and abundance of native oysters. We are also investigating oyster enhancement and restoration methods in San Francisco Bay and Elkhorn Slough.\nCitizen scientists form an essential and dedicated component of our research efforts. We run a citizen science network (PlateWatch and Green Crab Watch) to monitor for target non-native species along the Pacific Coast from Alaska to California. Participants include scientists from local state and federal agencies, concerned citizens, school groups and native groups. Within San Francisco Bay, we run citizen science programs to track Olympia oyster recruitment and the movement and predatory effects of the introduced snail Urosalpinx cinerea. If you wish to participate in any SI Citizen Science program, please contact Allison Cawood for more information at firstname.lastname@example.org.\nWe mentor interns and students in a wide range of capacities, and we usually host several undergraduate interns each summer. Our interns come from STAR and REU programs at San Francisco State University as well as the Smithsonian Internship program, which includes an REU program and numerous other opportunities. Each student participates in major lab projects as well as designs and executes an individual research project. Please see each program for their requirements and how to apply.\nJimenez, H., Keppel, E., Chang, A.L. and Ruiz, G.M. (2018). Erratum to: Invasions in Marine Communities: Contrasting Species Richness and Community Composition Across Habitats and Salinity. Estuaries and Coasts, 41 (2) , 611. http://dx.doi.org/10.1007/s12237-017-0315-1\nJimenez, H., Keppel, E., Chang, A.L. and Ruiz, G.M. (2018). Invasions in Marine Communities: Contrasting Species Richness and Community Composition Across Habitats and Salinity. Estuaries and Coasts, 41 (2) , 484-494. http://dx.doi.org/10.1007/s12237-017-0292-4\nNewcomer, Katherine A., Marraffini, Michelle L., and Chang, Andrew L. (2018) Distribution patterns of an introduced encrusting bryozoan, Conopeum chesapeakensis (Banta, Perez and Santagata, 1995) in an estuarine environment in upper San Francisco Bay. Journal of Experimental Marine Biology and Ecology 504: 20–31. DOI: 10.1016/j.jembe.2018.04.001\nZabin, C. J., Davidson, I.C., Holzer, K.K., Smith, G., Ashton, G.V., Tamburri, M.N., and Ruiz, G.M. 2018. How will vessels be inspected to meet emerging biofouling regulations for the prevention of marine invasions? Management of Biological Invasions (9). (Online: June, 18 2018)\nChang, Andrew.L., Brown, Christopher W., Crooks, Jeffrey A., Ruiz, Gregory M. (2017). Dry and wet periods drive rapid shifts in community assembly in an estuarine ecosystem. Global Change Biology. DOI: 10.1111/gcb.13972\nMarraffini, Michelle L., Brown, Christopher W., Ashton, Gail, Chang, Andrew L., and Ruiz, Gregory M. (2017) Do settlement plates effectively sample established fouling communities for non-indigenous species? Management of Biological Invasions.\nJimenez, Haizea, Keppel, Erica, Chang Andrew L., and Ruiz, Gregory M. (2017) Invasions in marine communities: contrasting patterns of species richness and community composition across habitats and salinity. Estuaries and Coasts. http://dx.doi.org/10.1007/s12237-017-0292-4\nTracy, Brianna, Larson, Kristen, Ashton, Gail, Lambert, Gretchen, Chang, Andrew L., and Ruiz, Gregory M. (2017) Northward range expansion of three non-native ascidians on the west coast of North America. BioInvasions Records. http://dx.doi.org/10.3391/bir.2017.6.3.04\nCheng, Brian S., Chang, Andrew L., Deck, Anna and Ferner, Matthew C. (2016). Atmospheric rivers and the mass mortality of wild oysters: insight into an extreme future? Proceedings of the Royal Society B: Biological Sciences, 283 (1844) http://dx.doi.org/10.1098/rspb.2016.1462\nChang, Andrew L., Deck, Anna K., Sullivan, Lindsay J., Morgan, Steven G., and Ferner, Matthew C. (2016) Upstream – downstream shifts in a recruitment hotspot of the native Olympia oyster in San Francisco Bay during wet and dry years. Estuaries and Coasts. http://dx.doi.org/10.1007/s12237-016-0182-1\nCrooks, Jeffrey A., Chang, Andrew L. and Ruiz, Gregory M. (2016). Decoupling the response of an estuarine shrimp to architectural components of habitat structure. PeerJ, 4 http://dx.doi.org/10.7717/peerj.2244\nJimenez, Haizea, and Ruiz, Gregory M. (2016). Contribution of non-native species to soft-sediment marine community structure of San Francisco Bay, California. Biological Invasions. http://dx.doi.org/10.1007/s10530-016-1147-9\nWasson, Kerstin, Hughes, Brent B., Berriman, John S., Chang, Andrew L., Deck, Anna K., Dinnel, Paul A., Endris, Charlie, Espinoza, Michael, Dudas, Sarah, Ferner, Matthew C., Grosholz, Edwin D., Kimbro, David, Ruesink, Jennifer L., Trimble, Alan C., Vander Schaaf, Dick, Zabin, Chela J. and Zacherl, Danielle C. (2016). Coast-wide recruitment dynamics of Olympia oysters reveal limited synchrony and multiple predictors of failure. Ecology, 97 (12) , 3503-3516. http://dx.doi.org/10.1002/ecy.1602\nZabin, Chela J., Wasson, Kerstin and Fork, Susanne. (2016). Restoration of native oysters in a highly invaded estuary. Biological Conservation, 202, 78-87. http://dx.doi.org/10.1016/j.biocon.2016.08.026\nAshton, Gail V., Davidson, Ian C., Geller, Jonathan, and Ruiz, Gregory M. (2016). Disentangling the biogeography of ship biofouling: barnacles in the Northeast Pacific. Global Ecology and Biogeography, 25, 739–750. http://dx.doi.org/10.1111/geb.12450\n2015 and older\nMcCann, Linda, Keith, Inti., Carlton, James T., Ruiz, Gregory M., Dawson, Terence P. and Collins, Ken. (2015). First record of the non-native bryozoan Amathia (Zoobotryon) verticillata (delle Chiaje, 1822) (Ctenostomata) in the Galapagos Islands. BioInvasions Records, 4 (4) , 255-260.http://dx.doi.org/10.3391/bir.2015.4.4.04\nAshton, Gail, Davidson, Ian C. and Ruiz, Gregory M. (2014). Transient small boats as a long-distance coastal vector for dispersal of biofouling organisms. Estuaries and Coasts, 37 (6) , 1572-1581.http://dx.doi.org/10.1007/s12237-014-9782-9\nCanning-Clode, João, Fofonoff, Paul, McCann, Linda, Carlton, James T. and Ruiz, Gregory M. (2013). Marine invasions on a subtropical island: Fouling studies and new records in a recent marina on Madeira Island (Eastern Atlantic Ocean). Aquatic Invasions, 8 , 1-10.\nMcCann, Linda D., Holzer, Kimberly K., Davidson, Ian C., Ashton, Gail V., Chapman, Marnie D. and Ruiz, Gregory M. (2013). Promoting invasive species control and eradication in the sea: Options for managing the tunicate invader Didemnum vexillum in Sitka, Alaska. Marine pollution bulletin, 77 (1-2) , 165-171. http://dx.doi.org/10.1016/j.marpolbul.2013.10.011', ""Interdisciplinary Marine and Estuarine Sciences\nCollege of Science and Engineering\nDean: Dr. Carmen Domingo\nAdministered by the Estuary & Ocean Science Center\nThe Masters of Science (MS) in Interdisciplinary Marine and Estuarine Sciences (IMES) program is an interdisciplinary program offered at San Francisco State University (SF State) by the College of Science and Engineering, administered by the Estuary & Ocean Science Center (EOS Center; http://eoscenter.sfsu.edu). Faculty mentors in the program are from a range of Departments including Biology, Chemistry and Biochemistry, Physics, Geography & Environment, and Earth & Climate Sciences.\nThe MS in IMES program provides the opportunity for students to develop a transdisciplinary knowledge base at the intersection of global change, coastal marine and estuarine ecosystems, and societal challenges faced in urbanized areas. Students develop an applied knowledge in one or more fields of marine and estuarine sciences through conducting independent research under the guidance of faculty from a broad spectrum of physical, biological, and social sciences. That applied knowledge is combined with professional internships and training in professional skills to prepare graduates for a range of careers including scientific research, natural resource management, and science communication. Through the EOS Center, the program provides extensive laboratory resources and access to field sites for advanced study in marine and estuarine sciences.\nMaster of Science in Interdisciplinary Marine and Estuarine Sciences\nAdmission to the Program\nProspective students from a variety of undergraduate backgrounds are encouraged to apply. Applications are administered by the EOS Center and must comply with the application procedures and deadlines outlined here: http://imes.sfsu.edu/admissions. Note, the MS in IMES is distinct from the MS in Biology with a Concentration in Marine Biology, though some faculty may participate in both programs.\nProgram Learning Outcomes\nDesign a research project incorporating complex information involving coastal and marine ecosystems, communities, and organisms, using logic and evidence to answer research questions.\nFacilitate and engage in productive and diplomatic work relationships with peers, supervisors, and other professionals. Articulate how information and tools from different disciplines can be used together to generate novel insights and solutions.\nDemonstrate mastery of a suite of technical and statistical skills appropriate to address the research topic.\nEffectively communicate the research project to different audiences using clear, appropriate, and effective language and delivery style.\nInterdisciplinary Marine and Estuarine Sciences (M.S.) — 32 units minimum\nCore Requirements (25 units)\n|BIOL 708||Scientific Methods for Professional Aquatic Scientists||3|\n|MSCI 709||Foundations in Interdisciplinary Marine & Estuarine Science||4|\n|MSCI 715||Writing for Interdisciplinary Marine and Estuarine Scientists||3|\n|MSCI 717||Professional Skills Workshop I: Data Analysis and Visualization||2|\n|MSCI 718||Writing and Professional Skills Workshop II||2|\n|MSCI 788||Professional Internship in Marine and Estuarine Sciences||3|\n|MSCI 885||Seminar in Interdisciplinary Marine and Estuarine Science (2 unit course taken twice)||4|\nElectives (4-6 units)\nStudents can choose from a wide range of upper-division or graduate-level courses in consultation with their advisor. At least one course must be a graduate seminar such as from the list below:\n|BIOL 863||Advances in Marine Biology||2|\n|ERTH 795||Selected Topics in the Geosciences||3|\n|GEOG 857||Issues in Marine and Estuarine Conservation||3|\nCulminating Experience (3-4 units)\n|MSCI 895||Field Study or Applied Research Project||3|\n|MSCI 898||Master's Thesis||4|\nMoss Landing Marine Laboratories Courses\nMost courses for the MS in Interdisciplinary Marine and Estuarine Sciences are offered at SF State’s Estuary and Ocean Science Center and on the main campus. Students may also take elective courses at the Moss Landing Marine Laboratories on advisement. Consult the current Moss Landing Marine Laboratories course schedule for more information: https://gradprog.mlml.calstate.edu/class-schedule.""]"	['<urn:uuid:7d4cc509-41e9-4f77-85aa-f98d711bcf74>', '<urn:uuid:cc9daab0-0527-45b7-8ce3-66c09c7fc22e>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-13T06:04:36.674696	36	132	1954
5	What happened during the Ice Hellion invasion of Bone-Norman in 3071, and how did the defending forces respond?	The Ice Hellions invaded using the 200th Attack Cluster (Lithe Kill Keshik) under saKhan Connor Rood. The defending Eleventh Provisional Garrison Cluster, though outnumbered, employed clever tactics by designating the entire Hapsburg continent as the Circle of Equals, which negated the Ice Hellions' fast-strike advantage. For two weeks, the Jade Falcons led the 200th through frustrating engagements until a decisive battle on the Red Stone Plains. Though Rood was nearly killed when Jade Falcon ProtoMechs breached his cockpit, the Ice Hellions ultimately won the Trial. However, their victory was short-lived - after the 200th departed, the weak garrison they left behind was quickly overwhelmed by the Falcon Guards.	"['Bone-Norman nearby systems\n|X:Y Coordinates||-185.36 : 424.923[e]|\n|Recharge time||187 hours|\n- 1 System Description\n- 2 System History\n- 3 Political Affiliation\n- 4 Bone-Norman II\n- 5 Military Deployment\n- 6 Geography\n- 7 Image gallery\n- 8 Map Gallery\n- 9 Nearby Systems\n- 10 References\n- 11 Bibliography\n- 2571 - No record\n- 2596 - No record\n- 2750 - Rim Worlds Republic\n- 2764 - Rim Worlds Republic\n- 2765 - Rim Worlds Republic\n- 2775 - Lyran Commonwealth\n- 2785 - Lyran Commonwealth\n- 2786 - Lyran Commonwealth\n- 2822 - Lyran Commonwealth\n- 2864 - Lyran Commonwealth\n- 2995 - Lyran Commonwealth\n- 3025 - Lyran Commonwealth\n- 3030 - Lyran Commonwealth\n- 3040 - Federated Commonwealth\n- 3050 - Federated Commonwealth\n- 3052 - Clan Jade Falcon\n- 3057 - Clan Jade Falcon\n- 3059 - Clan Jade Falcon\n- 3063 - Clan Jade Falcon\n- 3067 - Clan Jade Falcon\n- 3071 - Disputed world (Clan Ice Hellion/Clan Jade Falcon)\n- 3075 - Clan Jade Falcon\n- 3079 - Clan Jade Falcon\n- 3081 - Clan Jade Falcon\n- 3085 - Clan Jade Falcon\n- 3135 - Clan Jade Falcon\n- 3145 - Clan Jade Falcon\n- 3151 - Clan Jade Falcon / Clan Hell\'s Horses (from December)\n- 3152 - Clan Hell\'s Horses \nBone-Norman Orbital view\n|Moons||1 (Devil\'s Pinch)|\n|Atmospheric pressure||Standard (Breathable)|\n|Equatorial temperature||24°C (Temperate)|\n|Highest native life||None|\n|Capital||Red Stone (2785)|\nBone-Norman was a part of the Apollo Province of the Rim Worlds Republic and was conquered by the Lyran Commonwealth during the 2773-2775 Republic-Commonwealth War. A fierce resistance was swiftly and harshly suppressed by the Commonwealth which earned the eternal enmity of the Bone-Norman population.\nSuccession Wars Era\nThe Draconis Combine sent a group of unmarked \'Mechs piloted by Azami Warriors across the border in August of 2785 to attack Bone-Norman. This had the desired effect of causing the Lyran forces to react by sending troops towards the Periphery to thwart a threat that did not exist. This allowed the DCMS to take the worlds of Trolloc Prime and Gram. The raiders that struck Bone-Norman had swiftly wiped out the paramilitary militia in place on the world before rampaging through a number of cities including Red Stone, the planetary capital; some cities were plundered for supplies - both military and industrial - while others were set on fire.\nAt the same time, the Grave Walkers mercenary regiment was using Bone-Norman as a training world. One corporation, BioHarvest, Inc., pursuaded the mercenaries into a formal protection contract. At the same time, the Grave Walkers were secretly testing a new advanced early-warning satellite system on behalf of the Federated Commonwealth.\n- See also Battle of Bone Norman (3050)\nIn March of 3050 Bone-Norman was targeted by Clan Jade Falcon forces as a part of their invasion of the Inner Sphere, Operation REVIVAL by the 8th Falcon Regulars. The world was hit during the Jade Falcon\'s first wave of attacks. The 1st Regiment of the Grave Walkers was currently on world, and the FedCom\'s new early-warning satellite network notified the commander, Colonel Dennis Merwin, of the Jade Falcons\' approach. The Clan\'s strategy was to decapitate resistance with a drop directly on the Red Stone Palace. Merwin ordered a fierce counterattack that devastated the Clan forces, but the Falcons were still able to break out and regroup. However, reinforcements were dispatched by the Jade Falcons, sealing the planet\'s fate. Bending to the reality of this mysterious invader\'s superior technology and skilled MechWarriors, Colonel Merwin warned the Bone-Norman planetary militia of the escalating situation, which sent them into hiding; this led to years of guerrilla warfare on the planet.\nClan Ice Hellion launched an invasion of the Clan Jade Falcon holdings in the Inner Sphere in mid-3071, with Bone-Norman targeted in the early stages of the attack; however, by July the Ice Hellion invasion had bogged down, with Bone-Norman being one of the worlds still seeing heavy fighting between Ice Hellion and Jade Falcon forces. The Ice Hellions invaded Bone-Norman using the 200th Attack Cluster, also known as the Lithe Kill Keshik, commanded by saKhan Connor Rood. The Ice Hellions underestimated the resistance that the defending Eleventh Provisional Garrison Cluster would offer to the Ice Hellion invasion. Operating under the command of Star Colonel Idris, the Eleventh knew that they wouldn\'t be able to defeat the 200th and would be heavily outnumbered; nevertheless, the Falcons were determined to tie down Connor Rood\'s forces for as long as possible, in the hopes that this would cause a delay to the overall Ice Hellion timetable for their invasion of the Jade Falcon Occupation Zone.\nIdris called for a Trial for control of Bone-Norman, and deliberately designated the entirity of the Hapsburg continent as his Circle of Equals. This negated the Ice Hellion fast-strike tactics, by allowing the second-line Jade Falcon units to choose their own engagements, and to mount ambushes that didn\'t break zellbrigen, allowing them to draw the Trial out without forfeiting. For two weeks, the Jade Falcon forces led the 200th around Bone-Norman in a series of frustrating engagements until Rood decided that he had to expose himself to draw the Jade Falcon forces out. The result was a battle on the Red Stone Plains where each force attempted to spring a trap on the other; Rood instructed a subordinate to begin a general melee, a tactic that allowed him to bring in an additional Trinary of troops that hadn\'t been a part of his original bid, only for Idris to bring a formation of Erinyes ProtoMechs to augment his own forces. The ProtoMechs nearly killed Rood, breaking through his cockpit and burning him badly shortly before the Hellions were able to declare the Trial a victory for themselves.\nWhile the 200th did capture Bone-Norman, the world would remain an Ice Hellion holding for just a few weeks - the weak garrison force left behind after the 200th left were overwhelmed by the Falcon Guards.\nAfter the Jade Falcon Occupation Zone became the anarchic Hinterlands, Bone-Norman was taken over by an unknown faction who destroyed the Falcon paramilitary garrison and killed the civilian caste governing members. Landing in December 3151, elements of Clan Hell\'s Horses found evidence of the fighting and were attacked themselves, although the mountain base that served as the planet\'s command center was deserted by the time the Horses breached it.\n- Grave Walkers\n- First Grave Walkers\n- Grave Walkers\n- First Grave Walkers\n- Grave Walkers\n- First Grave Walkers\n- Grave Walkers\n- First Grave Walkers\nBone-Norman is known as a ""desolate"" and ""haunting"" world. It is dominated by a single large landmass, Hapsburg, along with a smaller one, Renick, and handful of islands. The geography is dictated by a challenging phenomenon: the exceptionally dense moon causes powerful tidal forces.\nHapsburg is characterized by a sharp mountain range, the ore-rich Rückens, along the west. Between the mountains and the ocean is a intertidal zone extending for hundreds of meters. The ocean hurls strong winds into the mountains and beyond. The interior is a vast hilly badlands, scoured by wind and eroded into howling channels, rumored to drive men insane. The badlands eventually give way to rolling plains of blue grass and forests of ""werewillows"" - native trees adapted to the high winds and containing high amounts of heavy metals that interfere with sensors.\nRenick is a much smaller continent with a lower elevation but fertile land. Every five years the entire landmass is submerged by tidal flooding.\nThe planet\'s harsh conditions are made bearable with large engineering projects, from massive seawalls, flood management, to stilt-mounted buildings. There is, unfortunately, no engineering solution for the ceaseless winds. Perhaps due to the many hardships Bone-Normanites must endure, they have a centuries-spanning reputation as particularly surly and ill-tempered.\n- Red Stone City: the planetary capital city.\n- Red Stone Palace: site of a battle during Operation REVIVAL.\n|Closest 28 systems (26 within 60 light-years)|\nDistance in light years, closest systems first:\n|Vulture\'s Nest||56.8||Koskenkorva||57.3||Last Chance||58.1||Botany Bay||58.6|\n- Touring the Stars: Bone-Norman, p. 4: ""Atlas - Bone-Norman""\n- Tamar Rising, poster map\n- Tamar Rising, p. 72, ""The Hinterlands June 3152"" (Map)\n- Handbook: Major Periphery States, p. 25: ""Rim Worlds Republic At the Fall of the Star League -  Map""\n- Touring the Stars: Bone-Norman, p. 4\n- Handbook: Major Periphery States, p. 18: ""Rim Worlds Republic after Age of War -  Map""\n- Handbook: House Steiner, p. 25: ""Lyran Commonwealth after Age of Wars -  Map""\n- Historical: Reunification War, p. 159: ""Inner Sphere -  Map""\n- Era Report: 2750, p. 37: ""Inner Sphere -  Map""\n- Field Manual: SLDF: ""Inner Sphere -  Map""\n- Historical: Liberation of Terra Volume 1, p. 11: ""Inner Sphere -  Map""\n- Handbook: Major Periphery States, p. 37: ""The Republic-Commonwealth War""\n- House Kurita, p. 55: ""Bujitsu""\n- First Succession War, pp. 24–25: ""Inner Sphere -  Map""\n- Handbook: House Steiner, p. 36: ""Lyran Commonwealth after First Succession War -  Map""\n- Historical: Liberation of Terra Volume 2, pp. 122–123: ""Inner Sphere -  Map""\n- First Succession War, pp. 112–113: ""Inner Sphere -  Map""\n- Handbook: House Steiner, p. 40: ""Lyran Commonwealth after Second Succession War -  Map""\n- Vision\'s Hunger\n- Handbook: House Steiner, p. 47: ""Lyran Commonwealth after Third Succession War -  Map""\n- Handbook: House Steiner, p. 56: ""Lyran Commonwealth after Fourth Succession War -  Map""\n- Handbook: House Steiner, p. 59: ""Lyran Commonwealth after War of 3039 -  Map""\n- Historical: War of 3039, p. 132: ""Inner Sphere -  Map""\n- Era Report: 3052, p. 10: ""Inner Sphere -  Map""\n- Handbook: House Steiner, p. 61: ""Lyran Commonwealth after Clan Invasion -  Map""\n- Era Report: 3052, p. 22: ""Inner Sphere -  Map""\n- Era Report: 3062, p. 10: ""Inner Sphere -  Map""\n- Objective Raids, p. 58: ""Unit Note""\n- Era Report: 3062, p. 28: ""Inner Sphere -  Map""\n- Handbook: House Steiner, p. 70: ""Lyran Commonwealth after FedCom Civil War -  Map""\n- Jihad: Final Reckoning, p. 43: ""Inner Sphere -  Map""\n- Jihad Hot Spots: 3076, p. 16: ""Timeline of the Jihad""\n- Jihad Secrets: The Blake Documents, p. 64: ""Inner Sphere -  Map""\n- Field Report: Clans, p. 26: ""Clan Jade Falcon/Clan Wolf in Exile Deployment Map - August 3079""\n- Jihad: Final Reckoning, p. 62: ""Inner Sphere -  Map""\n- Field Manual: 3085, p. vii: ""Inner Sphere Map - [October 3085]""\n- Era Report: 3145, p. 10: ""Inner Sphere -  Map""\n- Era Report: 3145, p. 38: ""Inner Sphere -  Map""\n- Field Manual: 3145, p. VI: ""Inner Sphere -  Map""\n- Shattered Fortress, pp. 102–103: ""Inner Sphere -  Map""\n- Tamar Rising, p. 60, ""The Ground Trembles at the Horde\'s Approach""\n- First Succession War, p. 21: ""The Commonwealth Incursions""\n- Touring the Stars: Bone-Norman, p. 5\n- House Kurita (The Draconis Combine), p. 53: ""Bujitsu""\n- Jade Falcon Sourcebook, p. 36: Bone-Norman - World under siege by Clan Jade Falcon and the outcome\n- Touring the Stars: Bone-Norman, p. 6\n- Touring the Stars: Bone-Norman, p. 7: ""Bone-Norman""\n- House Steiner (The Lyran Commonwealth), p. 118: ""Unit Deployment Table""\n- NAIS The Fourth Succession War Military Atlas Volume 1, p. 45: ""Operation GÖTTERDÄMMERUNG""\n- Historical: War of 3039, p. 140: ""Deployment Table""\n- 20 Year Update, p. 25: ""Federated Commonwealth Deployment Table""\n- Jade Falcon Sourcebook, p. 36\n- Field Manual: Crusader Clans, p. 154: ""Crusader Clans Deployment Table""\n- Field Report: Clans, p. 15: ""Clan Jade Falcon Deployment Status""\n- Field Manual: 3145, p. 169: ""Clan Force Deployments - Clan Jade Falcon""\n- 20 Year Update\n- Era Report: 2750\n- Era Report: 3052\n- Era Report: 3062\n- Era Report: 3145\n- Field Manual: 3085\n- Field Manual: 3145\n- Field Manual: Crusader Clans\n- Field Manual: SLDF\n- Field Report: Clans\n- First Succession War\n- Handbook: House Steiner\n- Handbook: Major Periphery States\n- Historical: Liberation of Terra Volume 1\n- Historical: Liberation of Terra Volume 2\n- Historical: Reunification War\n- Historical: War of 3039\n- House Steiner (The Lyran Commonwealth)\n- Jade Falcon Sourcebook\n- Jihad: Final Reckoning\n- Jihad Hot Spots: 3076\n- Jihad Secrets: The Blake Documents\n- NAIS The Fourth Succession War Military Atlas Volume 1\n- Objective Raids\n- Shattered Fortress\n- Touring the Stars: Bone-Norman']"	['<urn:uuid:7fb3f21d-8986-48c4-a7c5-72d5a87d8012>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T06:04:36.674696	18	108	2062
6	I'm interested in ancient drinking habits. How did the Greeks use wine cups in their social gatherings, and what materials were these cups made from?	In ancient Greek symposia (drinking gatherings), wine cups played a central role in cementing cultural norms and social bonds. These gatherings had specific rules - participants reclined on couches set in circles or squares, and everyone drank wine mixed with water in equal rounds to ensure uniform intoxication. Regarding materials, while bronze vessels were more expensive, both bronze and clay cups were common. The clay cups often imitated more expensive metalwork designs, particularly during economic hardships. For instance, in the Late Classical Period, clay cups featured delicate stamped and incised designs to imitate silver prototypes. The bronze vessels were made using hammering techniques for the bodies and cast feet and handles were added using lead solder, while clay vessels employed black-figure and red-figure decorative treatments using iron-rich clay that could turn both red and black in different kiln conditions.	"['A Toast to History: 500 Years of Wine Drinking Cups Mark Social Shifts in Ancient Greece\nUniversity of Cincinnati research examines a timeline of wine drinking\ncups over a 500-year period in ancient Athens. Changes in cup form and\ndesign point to political, social and economic shifts.\nDate: 1/3/2011 12:00:00 AM\nBy: M.B. Reilly\nPhone: (513) 556-1824\nPhotos By: Provided by Kathleen Lynch\nHow commonly used items – like wine drinking cups – change through time can tell us a lot about those times, according to University of Cincinnati research to be presented Jan. 7 by Kathleen Lynch, UC associate professor of classics, at the annual meeting of the Archaeological Institute of America.\nLynch will present the research at the event’s Gold Medal Session, when archaeology’s most distinguished honor will be bestowed on her mentor, Susan Rotroff of Washington University.\nUC’s Lynch will present a timeline of wine drinking cups used in ancient Athens from 800 B.C. to 323 B.C.\nand will discuss how changes to the drinking cups marked political, social and economic shifts.\nVIEW part of Lynch\'s timeline of wine drinking cups in this video\nLynch’s specific area of study, which will result in a forthcoming book, is what’s known as the “symposium” in ancient Athens. These were gatherings held for nearly a millennia where communal drinking of wine was a means for cementing cultural norms and social bonds that carried over into the world of politics and business. Think of these symposia as the ancient world’s ultimate cocktail parties, with established rituals and rules.\nAn important aspect of any symposium was the wine cup, and the form of and the imagery on the cups reflected the shared culture of participants, as well as the larger social realities and changes in their world during the following periods:\n|Rendering of a symposium. The cups used at these gatherings reflected the social, political and economic trends of the time, just as items we commonly use reflect modern trends.|\n- Iron Age (1,100-700 B.C.)\n- The Archaic Period (700-480 B.C.)\n- The Late Archaic Period (525-480 B.C.)\n- The High Classical Period (480-400 B.C.)\n- The Late Classical Period (400-323 B.C.)\n- The Hellenistic Period (323-31 B.C)\nBasic rules of Athenian symposia:\n- Couches or mattresses used by reclining participants were set in a circle or square. So, there was no formal position of status or group “head.”\n- Drinkers imbibed in rounds, so consumption of wine (mixed with water) was equitable. In other words, everyone got drunk at about the same rate. No teetotalers permitted.\n- Said Lynch, “The focus was on drinking communally and in equal amounts. Inhibitions were lost. In-group bonds were formed. “\nWhy study these items? “Because,” stated Lynch, “People’s things tell you about those people and their times. In the same way that the coffee mug with ‘World’s Greatest Golfer’ in your kitchen cabinet speaks to your values and your culture, so too do the commonly used objects of the past tell us about that past. And, often, by studying the past, we learn about ourselves.” IRON AGE SYMPOSIA AND DRINKING CUPS (1,100-700 B.C.)\n- The drinking gatherings (symposia) were reserved for the elite, probably allowing political factions to consolidate power and set themselves apart from the population at large. In other words, the drinking gatherings were for the “in” crowd.\n- At this time, even grave markers for the very wealthy came in the form of the mixing bowls (kraters) used to blend wine with water during symposia. In other words, the ability to sponsor these drinking events was what people wanted to be remembered for.\n|This three-foot-high Iron Age gravemarker is in the form of a mixing vessel (water and wine) used at symposia. It signals the importance of the symposia in Athenian society. People wanted to be remembered for their ability to sponsor these gatherings.|\nTHE ARCHAIC PERIOD (700-480 B.C.)\n- The drinking cups during this period were simply decorated and rested directly on a base (no stem).\n- After the turn of the 6th century B.C., changes in the fashion of drinking cups began, corresponding with Athens’ rising political power and rising dominance in the ceramic market. Variety and quality were high during this period. It was the beginning of black-figured pottery production as well as plain, black-glazed versions. Stemmed cups became more popular, probably because they were easier to hold while reclining.\n- The middle of the 6th century B.C. saw a rapid proliferation of cup types: Komast cups, Siana cups, Gordion cups, Lip cups, Band cups, Droop cups, Merry-thought cups and Cassel cups – last only a few decades in terms of popularity. Some of these remain popular for only a few decades.\nLATE ARCHAIC PERIOD (525-480 B.C.)\n- Explained Lynch, “Possessing what was newest in terms of mode and style of drinking cups was likely equated with knowledge and status. The elites may have been seeking cohesion and self definition in the face of factional rivalries and populist movements. This hypothesis underscores how the drinking symposia – and specific cup forms identified with specific factions – might have been used by aristocratic blocs to cement group bonds in the politically charged environment of the time.”\n- The overall number of wine-drinking vessels increased dramatically during this period, pointing to the democratization of the symposium, as well as the democratization of the political and social arenas. The masses had become the political, if not the social, equals of the elites, and these masses were now enjoying symposia of their own.\n- It’s estimated that drinking vessels for symposia comprised up to 60 percent of the terra cotta fineware (collection of dishes) in the typical Athenian home of this period. “The typical home had few useful dishes for eating in contrast to many vessels designed for drinking wine in communal settings,” explained Lynch.\nHIGH CLASSICAL PERIOD (480-400 B.C.)\n- This period ends with the devastating Persian Wars, which Greece won. The proliferation of cup types fell, with red-figured drinking cups, introduced around 525 B.C., becoming the most popular.\n- Red-figured cups (cups decorated with red figures vs. black) remain popular through the first part of this period of cultural development in Athens, but the cups grow taller and shallower.\n- By the end of the 5th century B.C., Athens was weathering the Peloponnesian Wars and plague, and people were searching for an escape. This came in the form of an aesthetic restlessness. Fads in drinking cups came and went, but few developed into long-lived styles.\n- These new cup innovations tended to emulate the fineness commonly found in silver work at the time. For instance, there were many more plain, black clay cups with shiny surfaces. And delicate stamped and incised designs in clay cup interiors imitated metal prototypes on the cheap. In other words, the common terra cotta cups were “designer knock-offs” of the “high-end” designs found on silver cups.\n|Think of this cup as a ""designer knock off."" It\'s made of clay but was incised and stamped to emulate silver work. It was created in a period of harsh economic and social realities when people sought escape by means of pseudo luxury goods.|\n- Stemmed cups had finally run their course, being 200 years old at this point, and a stemless form became more popular.\n- Said Lynch, “People may have been seeking a visual antidote to the struggles of the period and a yearning for luxury at odds with daily conditions.”\nLATE CLASSICAL PERIOD (400-323 B.C.)\n- Trends toward pseudo luxury (designer knock-offs) in drinking cups continued; however, the variety of these “silver-inspired” clay cup designs diminished after the turn of the 4th century B.C., probably because the forms were impractical. For instance, one clay cup – modeled on a silver drinking vessel – featured delicate high-swung handles that served no useful purpose in clay.\n- Also “running out of steam” in this period was the tradition of decorating cups with human figures. A decorative innovation, called West Slope, became popular at this time. It consisted of colored clay applied atop black-glazed surfaces to create the effects of garlands and wreaths. Human figures were no longer depicted.\n- Finally, as Athens fell under the sway of Philip of Macedon and his son, Alexander the Great, the symposium came full circle. It began in the Iron Age as a practice of the elite. Then, with the movement toward democratization in Athens, participation in symposia broadened. Now, in Athens’ Hellenistic period, the practice was again the prerogative of the elites as a luxury and display of ostentatious consumption. Equality was no longer important in a state that was no longer democratic but monarchical.\nLynch’s research on symposia of ancient Greece received funding from the Louise Taft Semple Fund of the Department of Classics at UC; the Samuel H. Kress Foundation; and the Sheldon H. Solow Foundation, Inc.\nSymposium rendering : Connolly & Dodge, “The Ancient City,” Oxford University Press, 1998, p. 52.\nSymposium scene on cup (Brygos Ptr cup with decoration on exterior): Courtesy of the trustees of the British Museum, website.\nKrater used as grave marker: The Metropolitan Museum of Art website: metmuseum.org/toah/works-of-art/14.130.14\nBand Cup: Located in the Louvre Museum, image courtesy of Wikimedia commons.\nDroop Cup: Located in the Louvre Museum, image courtesy of Wikimedia commons.\nLip Cup: Courtesy of the trustees of the British Museum, website.\nSiana Cup: Courtesy of the trustees of the British Museum, website.\nKylix Cup: Courtesy of the excavations of the Athenian Agora.\nSet of Iron Age vessels: Courtesy of the excavations of the Athenian Agora.\nSet of Kantharoi cups 1: Courtesy of the excavations of the Athenian Agora.\nSet of Kantharoi cups 2: Courtesy of the excavations of the Athenian Agora.\nStemless Stamped Cup: Courtesy of the excavations of the Athenian Agora.\nSet of three stemless cups: Courtesy of the excavations of the Athenian Agora.\nThird Stemless Cup: Courtesy of the excavations of the Athenian Agora.', 'History Contained: Ancient Greek Bronze and Ceramic Vessels\nSeptember 17, 2005–January 2, 2006\nIn designing vessels to serve everyday needs, the Greeks created some of the most enduring forms that continue to be familiar down to the present time. To the Greeks, shape and function rather than medium were primary: a bronze vessel would be more expensive than one in clay, but all media, whether clay, bronze, silver, gold, marble, or wood attracted workmen of extraordinary talent. The Greek bronze vessels from the collection of Shelby White and Leon Levy are therefore naturally complemented by ceramic counterparts from the collection of Judy and Michael Steinhardt. Each offers a glimpse into the magnificent traditions that persisted over many centuries. The exhibition contains fourteen bronze vessels and vessel elements and fourteen painted ceramic containers; a bronze Corinthian helmet and a bronze greave (shin guard) have been added to illustrate the armor depicted in the painted scenes.\nGreek vessels were often used in more than one way. Some were awarded as prizes at athletic competitions; many were dedicated at religious sanctuaries, as votive gifts to the gods or as demonstrations of political grandeur. Huge numbers were made for export and found their way by trade or gift to the corners of the Mediterranean world, and even beyond, from Sudan to Russia, from Egypt to Spain. A great many have been well preserved in rock-cut tombs, for the deceased to enjoy in the afterlife. From these tomb groups it is often clear that fine pottery or fine bronze-work was treasured in a family for a generation or two before consignment to the grave.\nTerracotta black-figure eye-cup, Attic, ca. 530–520 BC, Diam. 14 3/4 in.\nTerracotta jar with heads of Griffins, a mythological animal with the head of an eagle and body of a lion, Western Greek ca. 535 BC, Ht. 9 5/8 in.\nTerracotta Black-Figure Hydria (water jar), Attic, ca. 520–510 BC, Image: Women collecting water at a fountain, Ht. 17 3/4 in.\nBronze Dionysiac Mask; Vessel applique, Greco-Roman; first century BC, Ht. 7 3/4 in.\nBronze reclining lion Vessel applique, Greek, Laconian?, Ca. 525 BC, Length 4 1/8 in.\nThe technique of working bronze goes back to before 3000 BC, when craftsmen in Mesopotamia and Egypt discovered that adding tin to copper produced an alloy, bronze, whose strength was greatly increased while at the same time retaining qualities that allowed for exceptional workmanship. Greek bronze vessels were made using two techniques. The bodies were generally hammered, starting with a disc of thickened metal and raised slowly to the desired contour. This left the metal extremely thin. To these bodies were added, by means of lead solder, cast feet and handles. Decorative appliqués were sometimes added to enliven the austere appearance. In the sixth and fifth centuries BC they were cast, such as the lions; but later, in the course of the fourth century BC, it was discovered that metal could be saved by using repoussé. This technique involved hammering a design from the back of a thin sheet of metal, and then turning it over to sharpen the details from the front. The marvelous crispness possible is illustrated on the calyx-krater and hydria.\nUnfortunately, we are only able to glimpse the Greeks’ remarkable achievements in bronze working. Ancient bronzes are rare because bronze commanded intrinsic value and could easily be melted down for re-use. Furthermore, the thin hammered bodies often corroded and crumbled away.\nWhile treasured, Greek vessels were definitely intended for use, whether in the women’s quarters or the men’s. Characteristic of the former are smaller ones for perfumes and unguents, such as the geometric bronze pyxides. Another shape associated especially with the women’s world is the hydria, or water jug (our word ‘hydraulic’ comes from the same root). In antiquity, it was a woman’s task to fetch water from the well, a sight common in the Mediterranean even in modern times. The marvelous picture of this activity on a hydria by the Antimenes Painter was painted in Athens around 520 BC shortly after the ruling tyrant, Peisistratus, had overhauled the water supply and commissioned a new fountain house. Several hydriai, both in clay and in bronze, underscore the importance of water in ancient Greece, a country of long, very dry summers, and illustrate the similarity in form of the two mediums.\nMost characteristic of vessels from the men’s world are those intended for the symposium, or drinking party. This social institution, from which women were excluded, took place during an evening at which discussion revolved around politics or love, poetry or philosophy. Wine was mixed with water in large containers known as kraters (from the Greek verb ‘to mix’), such as the ceramic column-krater or the bronze calyx-krater. The water was brought in hydriai; the wine in amphorae, like the huge black-figure example with Herakles departing for Olympus. The wine was then drawn off in jugs and poured into cups.\nThe ceramic vases here employed two decorative treatments, black- and red-figure. Both rely on an iron-rich clay that can turn red in an oxidizing kiln as well as black in reducing conditions (without oxygen). In black-figure, the scene is drawn in silhouette over the clay background; texture and detail are added by means of cherry-red or white glaze, or by incision. In red-figure, the scene is left in the color of the red clay, and the background painted in black. Details here are possible in the so-called relief-line, a wiry, crisp line in thick glaze, or in dilute, honey-colored wash.\nBy examining the color, texture, and mineral traces in the clay, it is possible to identify pottery from several different Greek cities, such as Corinth or Athens. By further taking account of the shape of the vessel, which changes over time, and, more especially, the style of drawing, it is possible to identify individual artists who created these vases and assign accurate dates to them (although their actual names are not usually known).\nThe pictures on Greek vases give us the fullest documentation we have of scenes of everyday life, cult, ritual and mythological and religious beliefs. Their fascinating images transport us directly to the societies that produced them, to the worlds of Homer, Pericles, and Alexander the Great.\nFrom the collections of Shelby White and Leon Levy']"	['<urn:uuid:78236322-e39a-4704-a6d8-08b7260b222d>', '<urn:uuid:8d92e039-8d1b-4ec4-9bf6-e1c29ea6d528>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T06:04:36.674696	25	139	2702
7	What are the legal circumstances under which researchers are required to breach participant confidentiality in scientific studies?	Researchers are legally compelled to report participants in four circumstances: when there is a risk of bodily harm to someone or self-harm by the participant, in cases of child abuse or sexual misconduct with a child, when terrorism acts or plans are disclosed, and when there is an imminent threat to public health.	"[""Science depends on research participants to volunteer information regarding individual beliefs and actions on a host of topics. A participant or subject is more likely to provide honest responses when their identity is not going to be exposed.\nConsiderations for confidentiality should be given to the following:\nNature of the response.\nPossible punishment for the response.\nPossible punishment for participation.\nPerceived punishment for the participation.\nFear of embarrassment\nParticipants answer differently if they think that privacy may be compromised\nThe research may focus on topics that are taboo in society and it would be difficult to illicit honest responses to some of the questions posed when a participant did not feel secure in knowing that their identity is protected.\nThere also maybe some personal liability involved depending on the topic of the study. If the study involved cheating on testing at the university they attended and a guarantee of anonymity could not be provided, the likelihood of honest answers being given would be very low, for fear of retaliation.\nBias in employment although illegal is very real, if confidentiality is not upheld and if the wrong information is passed around regarding a participant it could very well effect the ability of the participant to maintain employment and be employed in the future.\nIn addition there are other negative responses that a participant may face if the research is on an unpopular topic, or a controversial issue. If the group involved in the research all works for the same organization and the study is to determine the employer's unethical practices, participants if not kept confidential could suffer great consequences for their responses.\nIn some cultures a subject's participation alone could result in public punishment, being excommunicated from the community or even death.\nThere may only be a perception of punishment for participation if confidentiality is not guaranteed. The perception alone will keep potential subjects from participating in the research, and will also prevent honest answers.\nResearch is never meant to punish anyone, it's only goal should be greater knowledge and understanding working toward a positive goal.\nPrivacy matters should be addressed from the inception of the research to the publication of the results. There should be safety nets put in place to guarantee confidentiality. The only amount of personal data that should be collected for the research is the minimal amount needed to insure a proper sampling of the population. Personal identifiable information should not be collected nor maintained unless absolutely necessary. Research staff should be properly trained in procedure to maintain confidentiality.\nInformed consent is required in all studies and research using human participants. The consent to participate should clearly outline the purpose of the study and what the information gathered will be used for.\nWhen is Confidentiality not an Issue\nConfidentiality is not an issue when observing large groups, where individual responses or actions are not considered or when participants' identifiable information is not involved. Even in cases where there are large groups being assessed it is up to the researcher to use good judgment in making decisions regarding what information should be shared.\nA Legal Breach\nThere is a sticky area when it comes to confidentiality. If a participant in a research event or a study falls into any of the below categories than the researcher is legally compelled to report the participant.\nRisk of bodily harm - if there is a potential that a violent act will occur against someone else or the participant at his/her own hand.\nCases of child abuse - sexual misconduct with a child, or there is imminent danger to a child that has been disclosed.\nTerrorism - there is a terrorist act or plan in place that the participant has disclosed.\nThere is a eminent threat to public health.\nThere is some ethical concern by being required to report someone in any of the categories above. Let us look at these few examples to review the ethical quandary.\nA study is being conducted looking at the effectiveness of a drug on depression. The participants are asked to record feelings of violence, suicide, anger and despair, both before they began taking the medication and after three weeks of being on the medication.\nEach category is assigned a numerical value to display a range of feelings from very likely to least likely. The study is dependent on honest answers, the participants have been promised confidentiality.\nIf 30 of the 100 participants report that they feel more suicidal since taking the medication, is the researcher required to report all thirty of the participants?\nAIDS patients have been asked to participate in a study to gauge sexual habits since being diagnosed. 60% of the participants report that they have adjusted their sexual habits to include safe sex practices or abstinence, but 40% report that they still sometimes do not practice safe sex.\nShould they be reported?\nThey are a definite threat to public health, but the study depends on honest answers, once that confidentiality has been breached it certainly will sour future participants from being honest. How can we ever gauge the effectiveness of programs and education within certain communities if participants have to fear potential criminal charges?\nShould Science Be Required to Act As a Step in the Penal Process?\nA Real Life Story\nA woman in Florida is expecting baby number 3. She appears at a public health department for prenatal care and agrees to participate in a study for drug use and pregnancy.\nShe confesses that she has an addiction to cocaine (a known teratogen). She also answers yes to one of the questions asking if she would accept rehabilitative services if they were available. The information is passed on to the local authorities, her existing children are turned over to the state, and her in uteri child is taken upon birth and placed in foster care while the mother is forced into treatment so she can get her children back.\nThe case goes to court; the mother sues the state of Florida and its public health system for breach of confidentiality. She loses, based on the required reporting of suspected child abuse. The case has gone to the Supreme Court.\nThere are a few ethical issues that are evident in this brief breakdown of this story.\nMom disclosed willingly the information, she evidently had not acted in a manner to bring attention to herself, she disclosed the information believing it would be kept confidential.\nThe case was brought to court and the argument was that the mother was abusing her unborn child by snorting cocaine while pregnant hence the defense in reporting based on the abuse that was purportedly to her unborn child. Can we mandate good health practices by a pregnant women or risk being charged with child abuse and neglect?\nShould willing disclosure be punished especially when it is a part of research?\nThere are many instances of ethical concerns when it comes to confidentiality and privacy matters in research. As a result of allowing a participants information to be revealed there can be insurmountable damages, it should only be done when absolutely necessary.""]"	['<urn:uuid:bf7258af-61a9-4416-8c17-f5e6de333fd8>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T06:04:36.674696	17	53	1184
8	how much climate finance given as grants to poor countries in 2018 total amount	In 2018, $12 billion out of $78.9 billion in climate finance was given as grants, according to the OECD.	"['World leaders promised more support to protect vulnerable countries from the ravages of climate change at a global online summit Monday — but a growing COVID-19 debt crisis threatens to sweep those efforts away.\nAt the meeting convened by the Netherlands, leaders of wealthy nations said while cutting emissions remained vital, more must be done to tackle the consequences of global warming, especially in communities with few financial resources.\n“We have to show ... that there is a form of solidarity that is real and credible,” said EU Green Deal chief Frans Timmermans.\nThe global summit came on the same day that EU foreign ministers committed to increase finance to help poorer countries live with the changed climate.\nYou may like\nU.S. climate envoy John Kerry told the Dutch summit that his government intended to “significantly increase"" finance for adaptation. French President Emmanuel Macron said a third of French climate finance would be earmarked for such efforts.\nBut many of the countries most exposed to rising seas and extreme weather have seen their balance sheets derailed by the economic upheaval of the pandemic. Debt, and the risk of default, is making it harder for them to access finance for sea walls, drought-resilient crops, cooling systems for buildings or cyclone warning systems.\nThe world’s leading development brass are now searching for a sweeping solution that encompasses climate change, debt relief and pandemic recovery.\n“When we take these two things — the impact of the pandemic and looming climate crisis — together, it is so clear we need a new global compact in which we step up support,” said International Monetary Fund chief Kristalina Georgieva.\n“This is absolutely a history-making opportunity,” said Rachel Kyte, dean of the Fletcher School of Law and Diplomacy and a former U.N. special representative for sustainable energy.\nOut of a grouping of 48 countries considered especially vulnerable to the impacts of global warming, more than half are low-income nations rated at moderate to high risk of debt distress by the IMF in December.\nThey “need grants, concessional financing, and those that are under huge burden of debt, they need debt restructuring,” said Georgieva, adding that debt relief should also be part of the solution.\nAlok Sharma, the U.K. president of this year\'s COP26 global climate talks, met with Georgieva to discuss a way to combine debt relief, post-COVID economic stimulus and climate measures, according to two people familiar with the conversation.\nU.N. Secretary-General António Guterres told the conference that debt relief instruments would be necessary. “Recovery cannot only be for the developed world,” he said.\nAt the conference on Monday, the leaders of major economies promised more money, but said little on debt. That\'s not what poorer countries wanted to hear.\nIn Angola, the economic impact of COVID has been “so profound” the country faces a decade of simply rebuilding the most basic services, such as education and health care, said Giza Gaspar-Martins, the country’s top climate change civil servant.\nHe said a “host of projects” that could have cut carbon emissions and built resilience to climate change have been a “direct casualty of the emergency and the slowdown brought about by COVID.”\nMeanwhile the parliament of Angola, which is one of Africa’s largest oil producers, will soon debate a bill that proposes opening conservation areas to mineral and oil exploration.\nAngola isn’t the only country facing that diabolical choice, said Gaspar-Martins. “There is activity all over Africa, new [oil and gas] projects that are being looked at, at the time when we thought those were going to become stranded assets.”\nDespite some relief under a G20 deal last year, Angola has been hit with credit downgrades, making it more expensive to borrow, and remains at risk of defaulting on its loans, according to Fitch Ratings. Countries with a history of government corruption, such as Angola and Gabon, face even greater difficulties convincing creditors to extend relief to them.\nAli Bongo Ondimba, the president of Gabon, said that “COVID-19 is eroding our developmental gains,” and worsening the climate crisis.\n“It would be interesting,” Gaspar-Martins suggested, that if “projects could be demonstrated to be specifically aligned with climate action, that those debts were simply written off.”\nThat\'s a difficult argument to make as the world’s wealthiest governments face their own economic crises, which they are battling with vast increases in public spending. The U.K. recently disturbed developing countries by slashing its overseas aid in the face of budget pressure at home.\nGuterres argued it makes sense for rich donor countries and development banks, in particular, “to significantly increase the volume and predictability of their finance,” saying that recovery from the pandemic would require “trillions of taxpayers’ dollars” to “jump start the low-carbon, high-resilience future we need.”\nThe massive recovery programs in developed countries show the “money is there,” said Andrea Meza, the environment minister of Costa Rica. She suggested that poorer countries could swap debt for decarbonization to ""give us some oxygen at the national level to continue the transformation.”\nShe also said that developing countries ""need a lot of grants"" instead of loans. In 2018, just $12 billion out of $78.9 billion in climate finance came in the form of grants, according to the OECD.\nClimate change and debt need to “be solved as one issue,” said Kyte. But “there\'s a lot of climate finance people who are either not wanting to focus on debt, trying to avoid it or pretend like it\'s not as serious an issue as it is.”\nThere\'s also a need to better direct spending toward adapting to climate change instead of pouring money into renewable energy projects, which are easier to finance as they generate revenues.\n“We need a five to 10-fold increase for funding in adaptation, just in the developing world, and yet only a tiny fraction of the trillions of dollars in economic stimulus funds are so far being earmarked for climate adaptation. That we have to increase,” said Ban Ki-moon, the former United Nations secretary-general who now co-chairs the Global Center on Adaptation.\nMuch will depend on the willingness of big creditors: including the EU, U.S. and China. Europe would “have to … be prepared to explore” debt relief, said Jacob Werksman, international climate adviser to the European Commission.\nWant more analysis from POLITICO? POLITICO Pro is our premium intelligence service for professionals. From financial services to trade, technology, cybersecurity and more, Pro delivers real time intelligence, deep insight and breaking scoops you need to keep one step ahead. Email [email protected] to request a complimentary trial.']"	['<urn:uuid:de7308cb-773c-41f8-bf34-7be3d6306f3d>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	14	19	1088
9	What are the main benefits of using this translation tool?	The LocalizationAdmin interface provides several benefits for editing translations: you can see and edit resources for all languages in one place, quickly search and jump to resources, use Google and Bing Translate for quick interactive translations, and view a multi-locale resource view of all resources for all languages. This interactive editing environment makes it much easier to see missing resources and provides significant data entry time savings compared to traditional methods.	"['I received a message yesterday that asked a simple question regarding the Westwind.Globalization library. Westwind.Globalization is a library that allows you to use a database to store localization resources for editing or runtime loading, along with rich editing and conversion tools that can easily import and export resources to and from Resx.\nThe question was a simple one:\nHow can I edit single, loose Resx files in library projects without having to have a full Web project to manage all of my resources?\nIt\'s a fair question. The Westwind.Globalization.Web package is primarily meant for Web applications and managing the Web application level resource management and it works great for that. Since you\'re already running a Web app the LocalizationAdmin interface can just be accessed directly from within the Web application.\nWhat\'s not so obvious is that the Localization Admin application or the resource manager doesn\'t have to work with the resources contained in the current project - you can easily edit arbitrary resources located on the machine that the Web application is running on (given the Web app has the permissions to read/write the files).\nWhat this means is that you can actually use Westwind.Globalization.Web as a standalone Resx file resource editor of arbitrary RESX files. IOW, you can easily edit resources that live in an arbitrary library project.\nUsing Westwind.Globalization as a RESX Editor\nIdeally for this to work you\'d create a \'generic\' instance of Localization Admin Web application that\'s not part of a localizable Web application. Instead you treat this like a local application that imports and exports RESX data as needed, one resource set at at a time.\nUsing that approach the process is:\n- Import RESX resources from a RESX file into the database\n- Add, edit, translate and delete resources interactively\n- Export RESX resources back into RESX files\nThe idea is that the database is only temporary storage with the final place of truth being the RESX file. The RESX file can go into source control and be synced between developers just like you normally would. The only difference is how you work managing your resources.\nAlthough this sounds complicated it\'s actually quite easy and quick. Importing and Exporting resources is easy and quick from the RESX UI and can also be automated if necessary (the API is available in addition to the UI - I\'ll show the UI usage here).\nSo let\'s take a look on how this can work and the steps required to set this up.\n1. Create a new Web Project\nThe following first two steps are one time steps that set up a reusable Web site. Once you\'ve done that you can use steps 3 and forward to work with Resx files.\nSo Westwind.Globalization.Web and the LocalizationAdmin interface it installs requires a Web application, so you need to create a local Web site for this to work. Local because you\'ll want the Web site to be able to access RESX files from your local (or network) disk. The created Web project should be an empty Web site that can be used generically for any \'loose\' RESX editing you want to do.\nTo do this:\n- Create a new Web Application Project in Visual Studio\n- Choose Empty Project Template\n- Use NuGet and add\nCompile to make sure everything installed properly. Next, you need to configure the library by providing a database connection string. You can use any of the supported providers (SQL Server, MySql, SqLite), but I\'m going to use SqLite here just because it\'s light weight has no dependencies other than the NuGet package. If you want use SQL Server create a database and make sure the Web user your Web site is running under has full access to the database you create by adding the User with the right permissions.\n2. SQLite Setup\nStart by installing the SQLite NuGet package:\nThen add these keys to your\nweb.config that configure a connection string and add the provider key required to use the SqLite ADO.NET driver::\n<add key=""ConnectionString"" value=""SqLiteLocalizations"" />\n<add key=""ResourceTableName"" value=""Localizations"" />\n... additional settings\n<add name=""SqLiteLocalizations"" connectionString=""Data Source=|DataDirectory|\\SqLiteLocalizations.db;Version=3"" providerName=""System.Data.SQLite"" />\n<remove invariant=""System.Data.SQLite"" />\n<add name=""SQLite Data Provider"" invariant=""System.Data.SQLite"" description="".Net Framework Data Provider for SQLite"" type=""System.Data.SQLite.SQLiteFactory, System.Data.SQLite"" />\nNext create an\nApp_Data folder in your Web app where the SqLite database will be stored and matches the connection string above. If you\'re using full IIS make sure the Application Pool Web account for the site you created for testing has rights to read/write/create files there (by default it doesn\'t).\nFinally you need to tell Westwind.Globalization to use SqLite when accessing DbResources by adding the following into\nprotected void Application_Start(object sender, EventArgs e)\n3. Starting up\nAt this point you should be able to startup the localization admin interface. Navigate to:\nor if you use IIS (which I recommend if you use this as a \'generic\' application):\nNext click on the Create Table button in the toolbar. Assuming you have write access in the\nApp_Data folder the SqLite database will be created for your automatically.\nAt this point you should have an operating LocalizationAdmin interface with a couple of sample resource sets:\nOk, all set and ready to run now. You can delete those two initial resource sets if you like.\nImporting Arbitrary Resources\nTo import RESX resources use the Import or Export Resx button to bring up the import/export dialog and then choose Import Resources:\nNote you can type any path on your arbitrary local system at this point to load resources from there. Here I\'m loading resources from library project that is not a Web project.\nNote that you have to make sure you run IIS Express/IIS under an account that has rights to read and write in this folder. For IIS this means using an Application Pool with the appropriate Identity set.\nThis process imports the resources into the resource LocalizationAdmin resource editor where you can now take advantage of the user interface to edit your resources a bit more efficiently.\nDo your Localization Editing\nAt this point you can browse and edit resources as you see fit. Using the LocalizationAdmin interface makes it easy to find, browse edit and translate resources.\nHere are some of the highlights what you can do with the resources once imported:\nSee and edit resources for a resource key for all languages in one place\nUse Google and Bing Translate to help you do quick translations interactively:\nSee a multi-locale resource view of all resources for all languages\nSearch and jump to resources quickly\nSo at this point you can party on the localization data. Any changes you make are stored in the localization database which lives in SqLite table(s) one for each resource file/sets.\nSave your Work: Export back to RESX\nWhen you\'re done making changes or when you think you\'re ready to \'save\' your work or if you need to resources back into your code so you can use strongly typed resources properly, you need to export the resources. You can simply export the resources back to disk and into RESX files. In essence, this work flow treats the RESX file as the final source of truth where you write out the file to disk when you\'re done.\nTo do this you:\n- Go the Import or Export Resx* menu option\n- Choose Export Resources\n- Pick your ResourceSet or All Resources if you have multiples\n- Enter the output folder which should be the same Properties folder you loaded from\nIf you go back to Visual Studio and open the RESX files you should now be able to see the localized data:\nAnd voila, you\'re now back to a fully functioning RESX file that you can share as part of your project and source control.\nStrongly Typed Resources and Visual Studio\nOne extra step is unfortunately required in order to get strongly typed resources to generate: You have to physically access the RESX editor in Visual Studio to trigger the strongly typed class generation as VS does not automatically regenerate strongly typed resources when the file changes (not even on a full build). Accessing the RESX editor for the default RESX and saving does the trick.\nAs an alternative you can also use the Create Classes Option in the the Localization Adminstration editor to generate the resources explicitly. Either way an extra step is unfortunately required to get the classes generated.\nIn this scenario I described I\'m treating the RESX file as the final store of truth which means that whenever I deem the localized data from the Localization Admin interface ready, I write it out to RESX.\nIf you\'re working as a single developer there\'s not much to worry about - you can simply keep exporting and not worry about importing.\nIn a multi-developer environment using source control, however, you\'ll need to make sure that you export to RESX before pushing to the repo, and import after a pull operation to make sure you pick up the latest RESX changes from other developers or localizers. It\'s a manual step for now.\nMixing Web and Library Projects\nAs I mentioned earlier the use case for localizing Web applications and library projects tend to be a bit different. Web applications tend to have a lot of localizable content while most libraries generally have only a few items that need to be localized.\nFor Web apps I use the actual database resource provider in the database while developing and localizing. If all developers on the team are all in one place where we can all access the same database the database can work as the final store of truth. We all use the single store of localization data from the database and that works very well.\nIf the team is distributed then RESX are used since those resources can simply sync with source control. The RESX files are written out in alphabetical order so this should minimize source control churn. We store the final store or truth in the Resx Files and use the Import/Export functionality to handle resource updates.\nIf I have both Web and Class library projects I use two separate Localization Admin interfaces. For the Web app, the Localization Admin interface that is built into the actual Web app is used. For the library project(s) I use the \'standalone\' version I described previous running in full IIS to provide me for the one of RESX import, edit, export cycle.\nIs it worth it?\nSo now you\'ve seen how you can pull in resources and edit them interactively. Clearly there\'s some effort and a little bit of discipline involved in this approach. Is it worth it?\nFor me personally I think the effort is worth it. Having an interactive editing environment where I can see the resources for all languages in one view, along with the grid view for quick editing and seeing what resources are missing easily is a huge data entry time saver. Having the Translation dialog hooked up inline also is very helpful, although if you use Google and/or Bing translation you better have a native speaker do the translations for you as the wording is often quite off from what you\'d call colloquial.\nThe productivity gain from being able to navigate and edit resources quickly far outweighs some of the interactive shuffling required to make this work for external library resources, so for me the answer is that it\'s totally worth it.\nYour mileage might vary, but next time you have a bunch of resources you need to edit give this approach a try and see if it saves you some time (maybe not on the first shot but on subsequent runs).\nOther Posts you might also like']"	['<urn:uuid:bfd86195-297c-4069-8e1f-367a5fdce059>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:04:36.674696	10	71	1947
10	quality control process steps pharmaceutical company rules need follow	The control strategy for pharmaceutical manufacturing quality must incorporate several key elements: Quality control needs to be built into each process step rather than relying on inspection, quality systems must be integrated into the data systems, and there should be a living quantitative risk assessment process that drives controls and verification methods. Additionally, companies should implement a quality operating system rather than just a quality management system. This approach shifts focus from merely interpreting regulations to implementing their intent into process control and quality.	['Quality by Design for FDA Approval: Building Control vs. Inspection\nIn 2011, the FDA implemented a Quality by Design (QbD) pilot program to support their review and approval process. After gathering several years worth of research, the FDA decided to push forward with a QbD approach to evaluate the manufacturing processes used by pharmaceutical companies.\nThe central idea is to focus on improving control in the preliminary and beginning stages of manufacturing, rather than waiting until the inspection stage for a reactionary evaluation.\nManufacturers that embed QbD into their manufacturing process and demonstrate the ability to consistently follow the processes are more likely to receive FDA approval for new applications. They will also benefit from expedited drug applications because they will be aligned with the approach the FDA considers the best option for manufacturing safe, reliable drugs for the end user.\nWhy Build In Control over Inspection?\nQbD in pharmaceuticals boils down to a simple comparison of “why” (control) versus “what” (inspection). Manufacturers that build in the “what” focus on the creation and inspection of rules and procedures. However, if your employees do not understand the “why,” there is possibility for quality, design, and production flaws.\nBuilding in the “why” ensures that employees understand their role in the manufacturing process, the triggers related to certain actions, and how their actions affect quality. If your team is trained to only focus on following the rules and procedures (what), they will not be as focused on the critical element of quality.\nConsider these characteristics of an integrated manufacturing process that aligns with the expectations of the FDA:\n- Control specs and limits that truly reflect output quality and safety.\n- A process that guarantees the consistency of product and process outputs.\n- Definitions of how the process will operate through the monitoring of critical characteristics of that process.\nWhat Does Building in Control Look Like?\nThe key to executing a manufacturing process that focuses on quality is the control strategy (control plan). The control strategy should align with these points of emphasis:\n- Quality control must be built into each process step rather than relying on inspection.\n- Quality systems are integrated into the data systems.\n- Living quantitative risk assessment process rather than a one-time event that drives the controls and verification methods.\n- Implement a quality operating system, rather than a quality management system\nThis paradigm shifts the focus from interpretation of the regulations strictly to implementing the intent of the regulations into process control and quality.\nThe AMLS Way to Support a Quality Operating System\nAM Life Science (AMLS) has developed a unique approach to QbD called Early Quality Integration that helps clients align with the FDA approval requirements.\nMainly, we focus on delivering the required verification for CQV while each element of the construction and installation is being conducted, rather than waiting for the turn-over to CQV phase. This assures that work meets all requirements before being designated as complete, resulting in significant reduction in rework, cost and schedule. It also provides a focus on the “why” of understanding the living process needs, how processes are controlled, and ensuring that each step contributes to a product that consistently meets specifications.\nOur approach also supports the “what” and the “how” of following the applicable standards, mapping out processes to match the regulatory requirements, conducting gap analysis, and collaborating with others throughout the process.\nTo learn more about how we follow and apply QbD in each project to support FDA approval for pharmaceutical companies, we invite you to download our whitepaper on Early Quality Integration.']	['<urn:uuid:7f35e231-51bc-436c-9af3-5609e44284c2>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T06:04:36.674696	9	84	595
11	What specific genetic findings were discovered in the genome-wide association study of long-lived individuals regarding TOMM40 and APOE genes, and how many SNPs were identified in total?	The study identified 281 single nucleotide polymorphisms (SNPs) that correlated highly with long age, with 137 of these SNPs occurring within 130 genes. The SNP with the most substantial correlation corresponded to translocase of outer mitochondrial membrane 40 homolog (TOMM40) and apolipoprotein E (APOE), two genes that have been implicated in the pathogenesis of Alzheimer's disease.	['|Abstract:||The ability to span 122 years of age [Young et al. 2010] marks humans as a long-lived species. While the modal lifespan in wealthy first-world countries is approximately 75-85 years of age, there exist unique statistical outliers that surpass this life expectancy by 15-25 (centenarians) and 25-35 years (supercentenarians). These individuals are hence attractive models for delayed aging and their longevity is presumed to arise from a wide array of environmental and genetic factors [Finch 2009].|\nEnvironmental influences on human aging are made apparent by statistical studies analyzing lifespan as a function of lifestyle or society: Seventh-Day Adventists, who tend to practice a variety of distinct health-related behaviours, have a mean life expectancy of 88 years [Fraser et al. 2001]. Among similarly populated and affluent countries there are sharp differences in the number of recorded supercentenarians. As of 2009, Japan had 20 times the number of living supercentenarians than Germany. Likewise, the United States has had more recorded supercentenarians than Canada, France, Germany, Italy, Japan, New Zealand, Switzerland, and the United Kingdom combined [Young et al. 2009]. Data from a large twin cohort study found that the heritability of longevity was 23% for females and 26% for males, suggesting a powerful role for non-genetic influences on lifespan [Herskind et al. 2005].\nFor individuals subject to similar environmental factors, however, genetic influences are the chief regulators of aging. The notion of heritable genes modulating lifespan is advocated by the findings that offspring of centenarians are notably protected from age-related diseases [Terry et al 2004] and that exceptional longevity can be family-clustered [Perls et al. 2000]. In addition, centenarians are significantly less likely to be hospitalized than their non-centenarian contemporaries [Engberg et al. 2009].\nA recent study by Sebastiani et al. highlights a plethora of genetic signatures associated with exceptional longevity. By performing a genome-wide association study of 801 long-lived individuals (median age of death 104 years) and 914 deceased controls, the authors identified 281 single nucleotide polymorphisms (SNPs) that correlated highly with long age. 137 of these SNPs occurred within 130 genes, many of which are thought to have relevance to aging and age-related disease. The SNP with the most substantial correlation corresponded to translocase of outer mitochondrial membrane 40 homolog (TOM440) and apolipoprotein E (APOE), two genes that have been implicated in the pathogenesis of Alzheimerâs disease. Notable contenders for significance included markers within the disease-associated Werner Syndrome (WRN) and superoxide dismutase 2, mitochondrial (SOD2) genes [Sebastiani et al. 2012].\nNumerous other studies have reported similar findings. Genotyping of 77 Sicilian centenarians, for example, revealed a significant increase in the human leukocyte antigen (HLA) DRB1*18 allele in male centenarians compared to 299 Sicilian controls [Listi et al. 2010]. Analyses of thrombocytes in 81 monozygotic twin pairs found that centenarian twins had platelet membranes with decreased basal lipid peroxide levels and less fluid membranes, suggesting that structural and functional modifications to platelets in centenarians may protect against oxidative stress [Nanetti et al. 2005]. A similar study reported a cholesteryl ester transfer protein (CETP) genotype (homozygosity for I405V) to be associated with exceptional longevity and preserved cognitive function in 158 Ashkenazi Jews [Barzilai et al. 2006].\nAtzmon et al. measured serum thyroid-stimulating hormone (TSH) and free thyroxine (T4) in 232 Ashkenazi Jewish centenarians and their offspring (n = 366). Compared to their shorter-lived counterparts (n = 163), the offspring of centenarians had a higher median serum TSH with an estimated heritability of 0.33. Moreover, two SNPs in the thyroid stimulating hormone receptor (TSHR) gene were associated with this increased TSH in both centenarians and their offspring [Atzmon et al. 2009]. Ashkenazi Jewish centenarians (n = 86) and their offspring (n = 175) have also been found to maintain longer telomeres with age relative to controls (n = 93). Genomic analyses of these centenarians revealed a haplotype of the human telomerase reverse transcriptase (hTERT) gene that was correlated with both longer telomere length and exceptional longevity. Mutations in both hTERT and human telomerase RNA component (hTERC) were also overrepresented in centenarians, suggesting that variations in the human telomerase gene may serve to decelerate aging [Atzmon et al. 2010].\nIt is interesting that these genes and systems have been highlighted as potential regulators of exceptional longevity. Each has a unique physiological function and may provide insight into the mechanisms of aging: As the channel-forming subunit of the mitochondrial outer membrane translocase, TOMM40 plays an imperative role in mitochondrial protein transport [Humphries et al. 2005]. APOE is a plasma protein and ligand for low density lipoprotein receptors that participates in the transport and catabolism of cholesterol and lipids [Mathley et al. 1988]. WRN is a DNA exonuclease and helicase involved in DNA repair and SOD2 is a free radical scavenging antioxidant [Sebastiani et al. 2012]. The HLA system is the major histocompatibility complex in humans [Listi et al. 2010] and CETP is responsible for moving triglycerides and cholesteryl esters between lipoproteins [Barzilai et al. 2006]. TSHR bind the pituitary hormone TSH to stimulate T4 production and secretion, a hormone that profoundly impacts metabolic rate and body temperature [Atzmon et al. 2009]. hTERT is the catalytic component of human telomerase reverse transcriptase and hTERC supplies the template for the addition of nucleotides by hTERT [Atzmin et al. 2010].\nOf the mentioned genes and systems implicated, they are relevant to current theories that link DNA repair, oxidative stress, immunity, metabolism, telomere length, mitochondrial function, and endocrine control to senescence [Kirkwood, 2005; Candore et al. 2006]. These centenarian-studies lend credence to these theories and support a complex model for aging in which a spectrum of factors play regulatory roles. This is in line with the evidence presented for long-lived species, which suggests that evolutionary tweaking of these biochemical systems serves to limit maximal lifespan. The data also supports the current evolutionary theories of aging, which suppose that longer-lived individuals will have invested in unique mechanisms that promote durability and overall maintenance.\nThere are both common and rare longevity-associated variants that may counter the effects of disease-predisposing variants and extend lifespan. The genetic contribution to average human aging can be modest. Genes explain 20-25% of the variability of human survival to the mid-80s [Herskind et al. 1996; Fraser and Shavlik, 2001]. Genetic factors may have a greater impact on survival to the ninth through eleventh decades [Tan et al. 2008]. Exceptional longevity is typically characterized by strong familiarity [Perls et al. 2000, 2002; Atzmon et al. 2005; Schoenmaker et al. 2006] as well as marked delay in disability [Terray et al. 2008]. It is unclear whether the nature of and contribution of genetic variation to exceptional longevity is due to undiscovered rare genetic variants with large effects and/or the presence of many common genetic variants with small effects [Bloss et al. 2010]. Two supercentenarian (age > 110). The woman was homozygous for HSP70. The man carried a cluster of longevity variants in FOXO3A and both suspects had a variant in IGF1R. Both carried the same genotypes of rs9536314 and rs9527025 in KLOTHO (KL). The two subjects carried the same heterozygous genotype for SNP rs4641 in LMNA and the same homozygous genotype GG for SNP rs1801195 in WRN. In both subjects the most significantly enriched category of genes with novel coding mutations was alternative splicing [Sebastiani et al. 2011].\nSNPs in the INS, RAD52 and NTHL1 genes are associated to longevity. The majority of the rare alleles of the identified SNPs were longevity variants, not mortality variants, indicating that longevity is primarily affected by positively acting minor alleles. There are sex-specific differences in the association of the genetic variation with survival during old age. FOXO3A, TERT and TERC were also associated with longevity [Soerensen, 2012].\nVariants between different laboratory mice strain were determined [Yang et al. 2011]: http://cgd.jax.org/cgdsnpdb/build37/data/imputedsnps/ As different strains have different lifespans [http://research.jax.org/faculty/harrison/ger1vi_lifespan.html], it can be asked:\nA standard DR regimen extends the lifespan of C57BL/6, whereas it remains unaffected in DBA/2 [Forster et al. 2003]. DR shortened lifespan in more recombinant inbreed strains than those in which it lengthened life [Liao et al. 2010]. Longevity under DR and AL is under genetic control, exhibiting 34 and 36% heritability, respectively. Candidate quantitative trait loci responsible for this trait were mapped to chromosome 7, 9 and 15 [Rikke et al. 2010].\nYoung RD, Epstein L, Coles LS. Living and all-time world longevity record-holders over the age of 110. Rejuvenation Res 2010; 13(6):759-761.\nFraser GE, Shavlik DJ. Ten years of life: Is it a matter of choice? Arch Intern Med 2001; 161(13):1645-1652.\nYoung RD, Epstein L, Coles LS. Supercentenarian counts by nation of birth. Rejuvenation Res 2009; 12(5):375-377.\nHerskind AM, McGue M, Holm NV, SÃ¸rensen TI, Harvald B, Vaupel JW. The heritability of human longevity: a population-based study of 2872 Danish twin pairs born in 1870-1900. Hum Genet 1996; 97(3): 319-323.\nTerry DF, Wilcox MA, McCormick MA, Pennington JY, Schoenhofen EA, Anderson SL, Perls TT. Lower all-cause, cardiovascular, and cancer mortality in centenariansâ offspring. J Am Geriatr Soc 2004; 52(12):2074-2076.\nPerls T, Shea-Drinkwater M, Bowen-Flynn J, Ridge SB, Kang S, Joyce E, Daly M, Brewster SJ, Kunkel L, Puca AA. Exceptional familial clustering for extreme longevity in humans. J Am Geriatr Soc 2000; 48(11):1483-1485.\nEngberg H, Oksuzyan A, Jeune B, Vaupel JW, Christensen K. Centenarians -- a useful model for healthy aging? A 29-year follow-up of hospitalizations among 40,000 Danes born in 1905. Aging Cell 2009; 8(3):270-276.\nSebastiani P, Solovieff N, DeWan AT, Walsh KM, Puca A, Hartley SW, Melista E, Anderson S, Dworkis DA, Wilk JB, Myers RH, Steinberg MH, Montano M, Baldwin CT, Hoh J, Perls TT. Genetic Signatures of Exceptional\nLongevity in Humans. PLoS ONE 2012; 7(1):e29848.doi:10.1371/journal.pone.0029848.\nListÃ¬ F, Caruso C, Colonna-Romano G, Lio D, Nuzzo D, Candore G. HLA and KIR Frequencies in Sicilian Centenarians. Rejuvenation Res 2010; 13(2-3): 314-318.\nNanetti L, Moroni C, Vignini A, Vannini P, Franceschi C, Mazzanti L. Age-related changes on platelet membrane: a study on elderly and centenarian monozygotic twins. Exp Gerontol 2005; 40(6):519-525.\nBarzilai N, Atzmon G, Derby CA, Bauman JM, Lipton RB. A genotype of exceptional longevity is associated with preservation of cognitive function. Neurology 2006; 67(12):2170-2175.\nAtzmon G, Barzilai N, Surks MI, Gabriely I. Genetic Predisposition to Elevated Serum Thyrotropin is Associated with Exceptional Longevity. J Clin Endocrinol Metab 2009; 94(12): 4768-4775.\nAtzmon G, Cho M, Cawthon RM, Budagov T, Katz M, Yang X, Siegel G, Bergman A, Huffman DM, Schechter CB, Wright WE, Shay JW, Barzilai N, Govindaraju DR, Suh Y.Genetic variation in human telomerase is associated with telomere length in Ashkenazi centenarians. Proc Natl Acad Sci U S A 2010; 107: Suppl 1:1710-1717.\nHumphries AD, Streimann IC, Stojanovski D, Johnston AJ, Yano M, Hoogenaraad NJ, Ryan MT. Dissection of the mitochondrial import and assembly pathway for human Tom40. J Biol Chem 2005; 280(12):11535-11543.\nMahley RW. Apolipoprotein E: cholesterol transport protein with expanding role in cell biology. Science 1988; 240(4852):622-630.\nKirkwood TB. Understanding the odd science of aging. Cell 2005; 120:437â447. Candore G, Colonna-Romano G, Balistreri CR, Di Carlo D, Grimaldi MP, ListÃ¬ F, Nuzzo D, Vasto S, Lio D, Caruso C. Biology of longevity: role of the innate immune system. Rejuvenation Res 2006; 9(1):143-148.\nChen, T., Dong, B., Lu, Z., Tian, B., Zhang, J., Zhou, J., Wu, H., Zhang, Y., Wu, J., Lin, P., et al. (2010). A functional single nucleotide polymorphism in promoter of ATM is associated with longevity. Mechanisms of ageing and development.\nDonlon, T.A., Curb, J.D., He, Q., Grove, J.S., Masaki, K.H., Rodriguez, B., Elliott, A., Willcox, D.C., and Willcox, B.J. (2012). FOXO3 Gene Variants and Human Aging: Coding Variants May Not Be Key Players. The journals of gerontology Series A, Biological sciences and medical sciences.\nJohansson, A., Marroni, F., Hayward, C., Franklin, C.S., Kirichenko, A.V., Jonasson, I., Hicks, A.A., Vitart, V., Isaacs, A., Axenovich, T., et al. (2009). Common variants in the JAZF1 gene associated with height identified by linkage and genome-wide association analysis. Human molecular genetics 18, 373-380.\nLi, L., Yang, Y., Yang, G., Lu, C., Yang, M., Liu, H., and Zong, H. (2010). The role of JAZF1 on lipid metabolism and related genes in vitro. Metabolism.\nSebastiani, P., Riva, A., Montano, M., Pham, P., Torkamani, A., Scherba, E., Benson, G., Milton, J.N., Baldwin, C.T., Andersen, S., et al. (2011). Whole genome sequences of a male and female supercentenarian, ages greater than 114 years. Frontiers in genetics 2, 90.\nSoerensen, M. (2012). Genetic variation and human longevity. Danish medical journal 59, B4454.\nSoerensen, M., Dato, S., Tan, Q., Thinggaard, M., Kleindorp, R., Beekman, M., Jacobsen, R., Suchiman, H.E., de Craen, A.J., Westendorp, R.G., et al. (2012). Human longevity and variation in GH/IGF-1/insulin signaling, DNA damage signaling and repair and pro/antioxidant pathway genes: Cross sectional and longitudinal studies. Exp Gerontol.\nThameem, F., Puppala, S., Arar, N.H., Stern, M.P., Blangero, J., Duggirala, R., and Abboud, H.E. (2008). Endothelial nitric oxide synthase (eNOS) gene polymorphisms and their association with type 2 diabetes-related traits in Mexican Americans. Diab Vasc Dis Res 5, 109-113.\nWalter, S., Atzmon, G., Demerath, E.W., Garcia, M.E., Kaplan, R.C., Kumari, M., Lunetta, K.L., Milaneschi, Y., Tanaka, T., Tranah, G.J., et al. (2011). A genome-wide association study of aging. Neurobiology of aging 32, 2109 e2115-2128.']	['<urn:uuid:556e17f2-ec96-44f8-b779-3dc64cef2e4d>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T06:04:36.674696	27	56	2183
12	What's tobacco's trade policy impact and cancer link?	Tobacco companies use trade agreements to force products into new markets and challenge health regulations, while maintaining lax rules of origin for their global sourcing strategies. Regarding cancer, tobacco is the major avoidable cause of cancer, causing over 155,000 deaths annually in the US, being responsible for most cancers of the lung, trachea, bronchus, larynx, pharynx, oral cavity, nasal cavity, and esophagus.	"['Tobacco and Trade: an unhealthy and harmful marriage\nOn 31st May, we celebrate the World Health Organization (WHO) initiated World No Tobacco Day which this year focuses on tobacco and lung health. While the health-harmful nature of traditional tobacco products is well-evidenced, a recent analysis from EPHA Member, the European Respiratory Society (ERS) highlights why heated tobacco products promoted by the tobacco industry are so harmful to public health, and formulates some evidence-based recommendations.\nIn this issue we also focus on the links between international trade and public health, The tobacco industry, for example has a long history of using international (bilateral and multilateral) trade deals to force their products into new markets. In a more recent development, the industry has rolled out a strategy to systematically challenge, delay and block the implementation of smoke-free legislation and programmes by exploiting investor-to-state dispute settlement (ISDS) clauses to legally challenge any policy decisions by seeking compensation for loss of Intellectual Property and/or future revenue. A more strategic aim of such legal challenges is to ensure ‘regulatory chill’ by intimidating governments to not consider tobacco regulation. Beyond ISDS however, there is another trade policy issue lurking in the background which has not yet caught the attention of the health policy community: rules of origin in trade agreements. EPHA’s Scientific Advisor on Trade, Gabriel Siles-Brügge outlines how big tobacco firms with global value chains prefer more lax rules of origin to accommodate their sourcing strategies.\nAlthough trade policy is considered outside the scope of the Framework Convention on Tobacco Control Article 5.3 provisions, it does impact on health. Was the fact that tobacco was included as an Offensive Interest (OI) in the Japan-EU FTA and the EU-Mercosur deal relating to tariff reduction, non-tariff barriers (such as smoke-free legislation, packaging, labelling requirements, etc) and investment promotion and protection the result of meetings with the tobacco industry and their lobbyists? We also examine the case for the need to strengthen tobacco tax and trade policy which illustrates why tobacco is not an ordinary product and that measures already in place should be health-focused and protected from industry interference and manipulation.\nFor this reason, the tobacco lobby aims to be heavily involved in the design of the investment chapters of international trade agreements, to ensure that ISDS clauses remain. The ISDS challenge has not only been used against low and middle income countries (notably Uruguay, which ultimately won an arbitration challenge brought by Philip Morris International (PMI) using the International Centre for Settlement of Investment Disputes (ICSID) mechanism, but has also been a successful tactic, from an industry perspective in dissuading developed countries, including New Zealand which has postponed introducing tobacco plain packaging for 6 years, until the outcomes of the PMI ISDS challenge against Australia. Therefore, the European Commission’s Trade department has decided to drop ISDS by name, and propose an alternative for future trade agreements, rebranded as an Investment Court System (ICS). And as EPHA has covered, although the European Court of Justice found ICS rules in line with EU law, measures which are legal can still have unhealthy consequences. ICS may limit public health policy space by arbitration, and it will not stop tobacco, alcohol, unhealthy food companies from challenging public health laws.\nTobacco, as a particularly dangerous commercial determinant of health is a good reason for the health community to monitor and challenge international trade negotiations to ensure true coherence between trade and public health. To this end, EPHA together with its member, the European Heart Network has developed a model public health chapter for the EU to include in its trade agreements.\nIn the aftermath of the European Parliament elections, taking forward this proposal would be a good, first step from the new Commission to show that it is serious about bringing the EU closer to its people, by prioritising issues people care about, such as public health – as EPHA has repeatedly pointed out – 70% of Europeans want to see more EU action on health. In light of the indicated links between trade and tobacco, isn’t it time to reconsider treating tobacco like any other ordinary product in trade negotiations, and pursue a trade policy which actually works for health?\nZoltán Massay-Kosubek is Policy Manager for Health Policy Coherence at the European Public Health Alliance. He is leading EPHA’s work on Healthy trade and he is the only public health member of the European Commission Expert Group on EU Trade Agreements\nA cikk magyar nyelvű változata itt olvasható:', ""In the United States, smoking-related illnesses accounted for an estimated 443,000 deaths each year between 2000 and 2004.[1,2] (Also available online.) On average, these deaths occur 12 years earlier than would be expected, so the aggregate annual loss exceeds 5 million life-years. These deaths are primarily due to smoking’s role as a major cause of cancer, cardiovascular diseases, and chronic lung diseases. The known adverse health effects also include other respiratory diseases and symptoms, nuclear cataract, hip fractures, reduced female fertility, and diminished health status. Maternal smoking during pregnancy is associated with fetal growth restriction, low birth weight, and complications of pregnancy. It has been estimated that at least 30% of cancer deaths and 20% of all premature deaths in the United States are attributable to smoking.\nTobacco products are the single, major avoidable cause of cancer, causing more than 155,000 deaths among smokers in the United States annually due to various cancers. The majority of cancers of the lung, trachea, bronchus, larynx, pharynx, oral cavity, nasal cavity, and esophagus are attributable to tobacco products, particularly cigarettes. Smoking is also causally associated with cancers of the pancreas, kidney, bladder, stomach, and cervix and with myeloid leukemia.[4,6]\nSmoking also has substantial effects on the health of nonsmokers. Environmental or secondhand tobacco smoke is implicated in causing lung cancer and coronary heart disease. Among children, secondhand smoke exposure is causally associated with sudden infant death syndrome, lower respiratory tract illnesses, otitis media, middle ear effusion, exacerbated asthma, and respiratory effects such as cough, wheeze, and dyspnea.\nEnvironmental tobacco smoke has the same components as inhaled mainstream smoke, although in lower absolute concentrations, between 1% and 10%, depending on the constituent. Carcinogenic compounds in tobacco smoke include the polycyclic aromatic hydrocarbons (PAHs), including the carcinogen benzo[a]pyrene (BaP) and the nicotine-derived tobacco-specific nitrosamine, 4-(methylnitrosamino)-1-(3-pyridyl)-1-butanone (NNK). Elevated biomarkers of tobacco exposure, including urinary cotinine, tobacco-related carcinogen metabolites, and carcinogen-protein adducts, are seen in passive or secondhand smokers.[7,9-11]\nIn 2011, 21.6% of adult men and 16.5% of adult women in the United States were current smokers. (Also available online.) Cigarette smoking is particularly common among American Indians and Alaska Natives. The prevalence of smoking also varies inversely with education, and was highest among adults who had earned a General Educational Development diploma (49.1%) and generally decreased with increasing years of education. (Also available online.) From 2000 to 2011, significant declines occurred in the use of cigarettes among middle school (10.7% to 4.3 %) and high school (27.9% to 15.8%) students. (Also available online.) Cigarette smoking prevalence among male and female high school students increased substantially during the early 1990s in all ethnic groups but appears to have been declining since approximately 1996.[14,15] (Also available online.)\nThe effect of tobacco use on population-level health outcomes is illustrated by the example of lung cancer mortality trends. Smoking by women increased between 1940 and the early 1960s, resulting in a greater than 600% increase in female lung cancer mortality since 1950. Lung cancer is now the leading cause of cancer death in women.[14,16] In the last 30 years, prevalence of current cigarette use has generally decreased, though far more rapidly in males. Lung cancer mortality in men peaked in the 1980s, and has been declining since then; this decrease has occurred predominantly in squamous cell and small cell carcinomas, the histologic types most strongly associated with smoking. Variations in lung cancer mortality rates by state also more or less parallel long-standing state-specific differences in tobacco use. Among men, the average annual age-adjusted lung cancer death rates from 2001 to 2005 were highest in Kentucky (111.5 per 100,000), where 29.1% of men were current smokers in 1997, and lowest in Utah (33.7 per 100,000), where only 10.4% of men smoked. Among women, lung cancer death rates were highest in Kentucky (55.9 per 100,000), where 28.0% of women were current smokers, and lowest in Utah (16.9 per 100,000), where only 9.3% of women smoked.References\n- American Cancer Society.: Cancer Facts and Figures 2013. Atlanta, Ga: American Cancer Society, 2013. Available online. Last accessed May 2, 2013.\n- Centers for Disease Control and Prevention (CDC).: Current cigarette smoking among adults - United States, 2011. MMWR Morb Mortal Wkly Rep 61 (44): 889-94, 2012. [PUBMED Abstract]\n- Nelson DE, Kirkendall RS, Lawton RL, et al.: Surveillance for smoking-attributable mortality and years of potential life lost, by state--United States, 1990. Mor Mortal Wkly Rep CDC Surveill Summ 43 (1): 1-8, 1994. [PUBMED Abstract]\n- U.S. Department of Health and Human Services.: The Health Consequences of Smoking: A Report of the Surgeon General. Atlanta, Ga: U.S. Department of Health and Human Services, CDC, National Center for Chronic Disease Prevention and Health Promotion, Office on Smoking and Health, 2004. Available online. Last accessed May 9, 2013.\n- Centers for Disease Control and Prevention.: Targeting Tobacco Use: The Nation's Leading Cause of Death 2005. Atlanta, Ga: CDC, 2005.\n- Ontario Task Force on the Primary Prevention of Cancer.: Recommendations for the Primary Prevention of Cancer. Toronto, Canada: Queen's Printer for Ontario, 1995.\n- U.S. Department of Health and Human Services.: The Health Consequences of Involuntary Exposure to Tobacco Smoke: A Report of the Surgeon General. Atlanta, Ga: U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, Coordinating Center for Health Promotion, National Center for Chronic Disease Prevention and Health Promotion, Office on Smoking and Health, 2006. Also available online. Last accessed May 9, 2013.\n- Cinciripini PM, Hecht SS, Henningfield JE, et al.: Tobacco addiction: implications for treatment and cancer prevention. J Natl Cancer Inst 89 (24): 1852-67, 1997. [PUBMED Abstract]\n- Finette BA, O'Neill JP, Vacek PM, et al.: Gene mutations with characteristic deletions in cord blood T lymphocytes associated with passive maternal exposure to tobacco smoke. Nat Med 4 (10): 1144-51, 1998. [PUBMED Abstract]\n- Benowitz NL: Cotinine as a biomarker of environmental tobacco smoke exposure. Epidemiol Rev 18 (2): 188-204, 1996. [PUBMED Abstract]\n- Hecht SS: Human urinary carcinogen metabolites: biomarkers for investigating tobacco and cancer. Carcinogenesis 23 (6): 907-22, 2002. [PUBMED Abstract]\n- Centers for Disease Control and Prevention (CDC).: Vital signs: current cigarette smoking among adults aged ≥18 years with mental illness - United States, 2009-2011. MMWR Morb Mortal Wkly Rep 62 (5): 81-7, 2013. [PUBMED Abstract]\n- Centers for Disease Control and Prevention (CDC).: Current tobacco use among middle and high school students--United States, 2011. MMWR Morb Mortal Wkly Rep 61 (31): 581-5, 2012. [PUBMED Abstract]\n- Jemal A, Thun MJ, Ries LA, et al.: Annual report to the nation on the status of cancer, 1975-2005, featuring trends in lung cancer, tobacco use, and tobacco control. J Natl Cancer Inst 100 (23): 1672-94, 2008. [PUBMED Abstract]\n- Johnston LD, O'Malley PM, Bachman JG: Monitoring the Future: National Survey Results on Drug Use, 1975-2001. Volume I: Secondary School Students. Bethesda, Md: National Institute on Drug Abuse, 2002. NIH Pub. No. 02-5106. Also available online. Last accessed February 15, 2013.\n- U.S. Preventive Services Task Force.: Guide to Clinical Preventive Services: Report of the U.S. Preventive Services Task Force. 2nd ed. Baltimore, Md: Williams & Wilkins, 1996.""]"	['<urn:uuid:95e4bc76-4e61-44d3-9ebf-c0cf452e7020>', '<urn:uuid:b70ae121-7787-4492-8c7c-992922ebaf4e>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T06:04:36.674696	8	62	1926
13	I'm trying to learn about database functions - what are the main types of aggregate functions available in SQL for analyzing data?	The SQL aggregate functions are COUNT, MIN, MAX, AVG, and SUM. These functions allow you to retrieve information about data inside a database, such as finding the average of values in a database column or finding the record with the highest value in a column.	['Databases are absolutely everywhere. They’re used by software engineers to store data for apps and games. They’re used by web developers to store user data. They’re used by companies to manage all the data they create.\nSQL is used to store data in and retrieve data from databases. If you’re new to SQL, this article is for you! We’ve compiled a list of Career Karma tutorials on SQL to help you beef up your knowledge of the database language.\nSQL, which stands for Structured Query Language, is a language that allows you to access the contents of and modify the values in a database. SQL is pronounced as “S Q L” or “sequel”.\nSQL allows you to view data in a database, add data to a database, amend data inside a database, and manage access to a database.\nSQL was invented in the early 1970s. It may be an old technology but it’s still widely used today. In fact, SQL was declared a standard by the International Organization for Standardization (ISO) in 1997. It’s still the number-one database query technology to this data.\nFirst, you’ll need to learn how to create a table in a database. A table organizes data inside rows and columns. To create a table, you can use the CREATE TABLE statement.\nDatabase tables consist of records. Each record is an individual entry in a table. You can create a record by using the SQL INSERT INTO statement.\nYou can create a record that fills in all the column values in a database or you can create a record with some values omitted that you can set later on.\nSQL makes it easy to select data from a table. You can use the SELECT statement to select which data you want to retrieve from a table. You can select all the columns from a table or a specific set of columns.\nThe WHERE statement is used to select records on which a command should be run. It can be used with a SELECT statement to view records that meet a condition. It can also be used with statements like UPDATE and DELETE to select which records should be modified.\nAggregate functions allow you to retrieve information about data inside a database. The SQL aggregate functions are COUNT, MIN, MAX, AVG, and SUM.\nFor instance, you can retrieve the average of values in a database column, or the highest record with the highest in a column.\nThe UPDATE statement modifies the contents of a record or multiple records. By default, the UPDATE statement amends all records in a database. That’s why it is often used with a WHERE statement. The WHERE statement lets you select specific records to change.\nWhat happens if you want to delete a record? That’s where the DELETE statement comes in handy. The DELETE statement can delete all records in a database or a record or set of records that meet a particular condition or list of conditions.\nYou can write comments inside an SQL database. Developers use comments to keep track of their queries and commands and how they work. Comments can span over one single line or over multiple lines.\nA SELECT query could return hundreds or thousands of values in a large database (or even more). The LIMIT statement limits the number of records returned by a query. For example, you could return only the first 10 records when a query is run.\nThe not equal operator (!=) checks if a value is not equal to another value. This allows you to select or modify records which have a column whose value is not equal to a particular value.\nSQL may be an old technology and developers may debate over how to pronounce it. Nevertheless, SQL is still the most widely used database query technology in the world.\nLearning SQL will put you on a good footing for a number of careers in technology. Whether you want to be a database administrator, a data scientist, or a web developer, knowing how to write queries in SQL is helpful.']	['<urn:uuid:f1733a87-03e9-4f71-a621-d572d3986e66>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	22	45	677
14	In industrial settings, what methods and procedures are used to maintain and clean large heating systems to ensure they work efficiently over time?	Several maintenance procedures are used: Chemical cleaning is employed during boiler commissioning, where cleaning and passivating fluids are temperature-controlled and circulated through boiler tubes to clean and condition them. For glass furnaces, sulfate burnouts (SBO) are performed to remove sulfate accumulations that restrict air passage in the checker-pack. Additionally, steam traps are used to drain off condensed water from steam pipelines while preventing high-pressure steam escape. Regular inspection and cleaning are conducted through manholes, which are openings in the boiler shell.	['Alkali Hydrolysis – A failure mechanism that can occur in lightweight castables due to the reaction of carbon dioxide in the atmosphere with lime in the cement. Typically, surface layers of the refractory become friable and delaminate. The failure occurs when the refractory is exposed to certain environmental conditions after placement and when some time passes before dryout. One of the most effective preventative measures is to conduct a timely refractory dryout.\nBake Out – This term is sometimes used to refer to a refractory dryout.\nBulk Density – A quality measure used for refractory. It is the weight of an object divided by the volume that it occupies. In refractory, bulk density can be reduced by excess porosity.\nBurnout Fibers – Refractory manufacturers often create intentional porosity and permeability in their materials by including fibers in the mix that will burn out (and leave voids) at low temperatures.\nBurner Turndown Capability – This is the ratio of the maximum firing rate of a burner divided by its minimum firing rate. Many pre-mix process burners have a turndown capability of 3 to 1.\nCastable Refractories – A mixture of heat-resistant aggregate materials and heat-resistant hydraulic cement. It is typically mixed with water in order to be placed by casting, ramming or gunning. Castable refractories can be installed in the field to conform to the shape of the furnace, vessel, trough, cyclone or duct.\nCeramic Bond – The mechanical strength in refractory that is developed during heat treatment and is the result of cohesion between adjacent aggregate particles.\nCirculating Fluidized Bed Boiler (CFB) – This technology suspends particles of fuel and limestone in a stream of air in order to create efficient low temperature burning and to limit the generation of pollutants such as NOX and SO2. This technology has environmental advantages and flexibility on the quality of the fuel.\nChecker Burnout – A term used by some to refer to the sulfate burnout process for opening clogged regenerators on a glass furnace.\nChemical Cleaning – A commissioning process used in the start up of new boilers. Various cleaning and passivating fluids are temperature controlled and circulated thru the boiler tubes in order to clean and condition them before being placed into service.\nCoating Cure – Various coatings are used in industrial processes to reduce corrosion attack and/or to change the emissivity of a surface. Frequently these coatings are applied wet and need to be dried and cured before being placed into service.\nCold Crushing Strength (CCS) – This is a quality measure of a refractory that measures its resistance to compressive forces.\nCold Water Glass Furnace Drain (CWD) – At the end of a glass furnace campaign, the liquid glass in the furnace must be removed in order to enable repairs to the refractory. One method for removing the liquid glass is to drill a hole in the furnace and drain the hot glass into a water stream that will quench it and transport it to a storage area. In a cold water drain, the water is used in a “once thru” manner.\nCrown Rise Monitor (CRM) – When heating up a glass furnace, the refractory expands and the steel restraining structure must be adjusted to accommodate the expansion. In order to monitor and control this process, it is beneficial to measure the position of the refractory crown on the furnace. Traditionally this has been done by manually measuring “tell-tales” that compare the crown position to its original elevation. Crown Rise Monitor (CRM) uses linear position transducers and a data recorder to continuously monitor the position of the crown.\nCullet – Broken fragments of glass collected either as a byproduct of glass manufacture or as a result of post consumer recycling. This material is frequently used as a raw material in glass manufacturing.\nCullet Filling – When a glass furnace is first put into production, it is heated and must have an initial charge of glass to begin the production process. Cullet filling is often used to provide this initial charge. Cullet is frequently charged into the hot furnace either by blowing or vibrating it into the melter.\nExcess Air Burner – This style of burner does not try to maintain a fixed ratio of fuel to combustion air. It has the advantage of providing a high volume and low temperature stream of gas at the burner discharge nozzle when turned down to low firing rates. This functionality is particularly well suited to refractory dryout.\nExpansion Control Supervision (ECS) – During the heatup of a glass furnace, the refractory is contained by a steel structure and large adjustment bolts. As the furnace heats up and expands, the steel structure must be adjusted to accommodate the growth of the refractory. Expansion Control Supervision is a service provided to manage these adjustments.\nFreeze Protection – On occasion, critical assets must be protected from freezing due to environmental conditions. Usually these issues arise either during construction or during an extended outage. Freeze protection is a service that can be used to protect critical assets during cold weather.\nFurnace Heatup – Many industrial furnaces recycle waste heat and they rely on that recycled heat to sustain the process. Stoves, regenerators, checkers, and recuperators are all examples of technologies used to recycle process heat. When one of these process is completely cold (either due to an outage or new construction), it is often necessary to “kick-start” the process with an external heat source. Furnace heatup is a service to bring these industrial furnaces from ambient temperature up to a sustainable operating condition.\nFurnace Hold Hot – When an industrial furnace interrupts the generation of the process heat but it isn’t desirable to allow the process to cool down, a furnace hold hot service is often used. Emergency hold hot service is sometimes used due to utility interruptions or natural disasters. Planned hold hot services may be used when a portion of the process needs to undergo repairs and it is undesirable to cool down the entire process.\nFurnace Outage – Many large industrial furnaces operate 24 hours per day and 7 days per week. The processes are continuous and are rarely interrupted. On occasion, a furnace outage is scheduled in order to conduct maintenance and repairs that can only occur during a process stop. In some industries, the outages are referred to as a turnaround.\nGunned Refractories – Refractory material may be installed via a method known as gunning. Refractory material is transported in a compressed air stream and is projected onto the refractory surface. The material may have water added at the nozzle or may be pre-moistened. The objective is to have the material stick to the target surface and to avoid rebound.\nHeat Recovery Steam Generator (HRSG) – This is a type of boiler that captures excess process heat. They may be installed before the exhaust stack on an industrial furnace or they can be used in conjunction with a combined cycle power generator. A combined cycle turbine has one stage driven by the products of combustion and a second stage driven by steam from the HRSG that captures some of the heat left over after the first stage.\nHydrocarbon Processing Industries (HPI) – This industry category refers to oil refining, gas processing, LNG, and petrochemical industries.\nMeltouts – On occasion, liquid materials solidify in the wrong place and need to be removed. One method of removing these materials is to melt them back into a liquid and allow the liquid to flow to the desired location.\nModulus of Rupture (MOR) – A quality measure of a refractory that measures its strength in resisting flexural (bending) forces.\nMonolithic Refractory – A refractory that is applied on-site in the unfired state and forms a continuous mass. Examples of monolithic refractory include castable, shotcrete, gunning mixes, plastic and ramming materials. Once placed, a refractory dryout is required to create the refractory properties.\nPermeability – The ability of a gas to pass thru a refractory structure. Permeability is enhanced when porosity interconnects and provides a gas path. Porosity does not necessarily create permeability. During refractory dryout, steam uses permeability to escape from the refractory structure.\nPorosity – Voids in the refractory structure create porosity. Porosity can occur as large individual voids or as micro voids dispersed throughout the structure.\nPlastic Refractory – A refractory mixture utilizing bonding agents such as clays, phosphates or resins. The material is typically delivered for installation as wet blocks. During installation, plastics do not require as much compaction/deformation as ramming material. The blocks are typically tamped together to form a continuous lining. Once placed, a dryout is required to create the refractory properties.\nPost Weld Heat Treating (PWHT) – When metal structures are welded, the heat affected zone has modified metallurgical properties compared to the parent metal. In order to restore metallurgical properties, a thermal cycle is often required. Most post weld heat treating is conducted locally on the weld with electrical resistance heating. In some cases, it is beneficial to heat an entire tank or weldament and, in these cases, combustion post weld heat treating is often deployed.\nRamming Refractory – A refractory mixture utilizing bonding agents such as clay, organics, phosphates, silica or tar. The material may be delivered for installation either in wet or dry form. The material is typically compacted with a pneumatic hammer or other compacting tool. Shear created during ramming contributes to the materials density and final properties. Once placed, a dryout is required to create the refractory properties.\nRatio Burner – Ratio burners attempt to maintain an optimal mixture of air and fuel. This ratio is usually slightly greater than the stoichiometric ratio in order to ensure complete combustion of all fuel. This type of burner is typically selected for process burner applications since it has good energy efficiency, environmental advantages, and is well suited to maintaining high temperature processes.\nRecycled Hot Water Glass Furnace Drain (RHWD) – This service is similar to the cold water glass furnace drain except that the water is collected after separation from the cullet and it is recycled back to be used again in a continuous closed loop.\nRefractory Cure – This is the initial period after placement of a castable refractory before the start of the dryout. During this period, reactions are occurring that result in the hard set of the refractory. Typically the refractory manufacturer specifies the duration of the cure period and it may be a function of the ambient temperature.\nRefractory Dryout – For monolithic refractory, following installation and the cure period, heat is applied per the manufacturer’s schedule in order to drive off both free and chemically combined water. This thermal treatment is the last step in achieving the desired properties in the installed refractory. This process is sometimes referred to as bake-out, heat cure, dry out, dry-out, or preheat.\nRefractory Spall – A mechanical failure of a refractory lining resulting in cracks, fractures, delamination, crumbling and/or explosive failure. Refractory spalling can occur due to many different root causes but, if it occurs, it typically happens during the refractory dryout. During dryout the refractory lining is exposed to high forces as water is converted to steam and it tries to escape the lining. If the refractory properties have been compromised due to any prior activities, then failure is likely to occur during the dryout. Refractory spalling typically occurs because the material does not have the strength to withstand the forces caused by the evolving steam during dryout. The two main issues are the material properties and the rate of steam generation. This condition is also referred to as explosive spalling or refractory blowout. Quality control procedures and adherence to the manufacturer’s mixing, placing, curing and drying instructions are the best preventative measures to avoid this condition.\nRegenerator Decongestion – This is another term for the sulfate burnout process.\nShotcrete – A refractory material that is mixed (like a castable) and then applied by pumping the mixture to a nozzle where it is mixed with air and an activator to begin the curing. The mixture is pneumatically “shot” on to the installation surface. Once placed, a dryout is required to create the refractory properties.\nSulfate Burnouts (SBO) – In glass furnaces with regenerators, sulfates tend to condense in the checker-pack over time. The accumulation of sulfates restricts air passage and eventually furnace operation is impacted. One method of removing sulfates from the checkers involves installing a burner in the bottom of the regenerator and heating the checker pack to above the melting point of the sulfates. This allows the sulfates to run out of the checkers to the bottom of the regenerator and air flow is restored to the furnace. This process is also referred to as regenerator decongestion or checker burnouts.\nTurnaround – In oil refineries and other process industries, periodic maintenance outages are planned where the process is completely stopped. Extensive planning and preparation is done to optimize the work accomplished and to minimize the downtime. These outages are frequently called turnarounds.', '2 Steam Boilers Definition of a Boiler Boiler is defined as a closed metallic vessel in which the water is heated beyond the boiling state by the application of heat liberated by the combustion of fuels to convert it into steam.Function of a BoilerThe function of a boiler is to supply the steam at the required constant pressure with its quality either dry, or as nearly as dry, or superheated.Dept. of Mech & Mfg. Engg.\n3 Classification of Boiler According to the axis of the shell:Horizontal boilerVertical boilerInclined boilerAccording to the applicationStationary boilerMobile boilerAccording to the location of the furnaceInternally fired boilerExternally fired boilerDept. of Mech & Mfg. Engg.\n4 According to the type of fuel used Solid fueled boilerLiquid fueled boilerGaseous fueled boilerAccording to the method of circulation of waterNatural circulation boilerForced circulation boilerAccording to the flow of water and flue gasesFire tube boilersWater tube boilersDept. of Mech & Mfg. Engg.\n5 Fire Tube Boiler:In the fire tube boilers, the hot flue gases produced by the combustion of fuels are led through a tube or a nest of tubes around which the water circulates.The examples are Cochran boiler, Cornish boiler, & Lancashire boiler.Dept. of Mech & Mfg. Engg.\n6 Water Tube Boiler:In the water tube boilers, the water circulates inside the tubes while the hot gases produced by the combustion of the fuels pass around them externallyThe examples are Babcock and Wilcox boiler & Stirling boilerDept. of Mech & Mfg. Engg.\n7 Lancashire boiler. Safety valve Man hole Steam pipe Steam stop valve Pressure gaugeFeed check valveWater gaugeBlowoff valveFusible plugB: Bottom central channel D: Dampers S: side channelsC: Chimney E: Fire grate F :Flue tube K: Main channelDept. of Mech & Mfg. Engg.\n9 Advantages of Water Tube Boilers over Fire Tube Boilers Steam can be raised more quicklySteam at higher pressures can be producedHigher rate of evaporationSediment deposition is lessSuitable for any type of fuel and method of firingMore effective heat transferFailure of water tubes will not affect the working of boilerOccupies less Space, Easy maintenance & Easy transportationDept. of Mech & Mfg. Engg.\n10 Disadvantages of Water Tube Boilers over Fire Tube Boilers Not suitable for ordinary waterNot suitable for mobile applicationHigh initial cost and hence not economicalDept. of Mech & Mfg. Engg.\n11 Differences / Comparison ParticularsFire tubeWater tubeFlow of water & hot gasesHot gases inside the tubes and water outside the tubesWater inside the tubes and hot gases outside the tubesMode of firingInternally firedExternally firedOperating pressureLow (Max. up to 16 bar)High ( Max. up to 75 bar)Rate of steam productionLowHighDept. of Mech & Mfg. Engg.\n12 Differences / Comparison ParticularsFire tubeWater tubeApplicationsSuitable for chemical industries & not for power generationIdeally suitable for power generation plants.Risk of burstingSafe, due to low pressureNot safe, due to high pressureTreatment of waterNot essentialEssentialShell diameterLarge for given capacitySmall for given capacityDept. of Mech & Mfg. Engg.\n13 Boiler Mountings and Accessories 1Two water level indicators2Pressure gauge3Two safety valves4Steam stop valve5Blow-off valve6Feed check valve7Fusible plug8ManholeBOILER MOUNTINGS, are required for the complete controlling of the steam generation, measurement of some of the important steam properties, and to provide safety to the boiler.Dept. of Mech & Mfg. Engg.\n14 BOILER ACCESSORIES are required to improve the efficiency of steam power plant and to enable for the proper working of the boiler.1Economiser2Air preheater3Superhater4Feed pump5Steam separator6Steam trapDept. of Mech & Mfg. Engg.\n15 Function of the Boiler Mountings Water Level Indicator: The function of the water level indicator is to indicate the level of the water in the boiler drum.Pressure Gauge: The function of the pressure gauge is to indicate the pressure of steam in the boiler.Safety Valves: A safety valve suddenly blows off the excess of steam from the boiler and shuts off automaticallySteam Stop Valve: A steam stop valve or Junction valve is used to regulate the flow of steam from the boiler.Dept. of Mech & Mfg. Engg.\n16 Feed Check Valve: When the level of water in the boiler falls, it is brought back to the specified level by supplying the additional water through the feed check valve.Blow off Valve: The function of the blow off valve is to remove periodically the sediments & also used to empty the water in the boiler when required.Fusible Plug: Fusible plug is a safety device used to extinguish the fire in the furnace of the boiler when the water level falls below the normal level.Manhole: Its an opening in the boiler shell through which a man can go in for periodic inspection and cleaning.Dept. of Mech & Mfg. Engg.\n17 Function of the Boiler Accessories Economizer :It is a device, in which the waste heat of the flue gases is utilized for preheating the feed water.Air preheater: It is a device, in which the waste heat of the flue gases is utilized for preheating the air supplied for the combustion of the fuel in the furnace.Superheater: Superheaters are used in boilers to increase the temperature of the steam above the saturation temperatureDept. of Mech & Mfg. Engg.\n18 Steam trap: Steam trap is a device used to drain off the condensed water accumulating in the steam pipe lines while at the same time the high pressure steam does not escape out of it.Steam Separator: A steam separator separates the water particles from the steam flowing in the pipe lines.Feed Pump: A feed pump is a boiler accessory to force the feed water at higher pressure into the boilers.Dept. of Mech & Mfg. Engg.']	['<urn:uuid:7ff9bbbf-b2b9-4989-adeb-86b526508f61>', '<urn:uuid:084cfc56-c0d8-43a8-81c7-2bf67755c1f5>']	factoid	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-13T06:04:36.674696	23	81	3078
15	Working in risk management, I need to understand the difference between how FDIC handles excess deposits in traditional banks versus how Fidelity's FDIC Insured Deposit Sweep Program manages amounts exceeding the standard insurance limits. Can you explain the key distinctions?	Traditional banks and Fidelity's Sweep Program handle excess deposits differently. In a traditional bank, if you have multiple accounts totaling more than $250,000, only $250,000 of the total will be insured, and you would need to manually distribute funds across multiple banks to receive full coverage. In contrast, Fidelity's FDIC Insured Deposit Sweep Program automatically cascades amounts exceeding $245,000 across multiple program banks. The program uses a minimum of five banks, making customers eligible for nearly $1,250,000 of FDIC insurance. For example, a $500,000 deposit would be automatically distributed with $245,000 going to each of the first two available program banks and the remaining $10,000 to a third bank.	"[""The Federal Deposit Insurance Corporation (FDIC) was formed in 1933 as part of the Banking Act of the same year. The formation of the FDIC was in response to the many banks that failed during the Great Depression.\nThe FDIC became an independent government corporation through the Banking Act of 1935.\nThe role of the FDIC is to cover FDIC-insured bank deposits, up to a limit, in the event of a bank failure.\nThis is what you know it for - it's supposed to safeguard your money if a bank goes under. FDIC Deposit Limits have become a popular topic with the rise of online banks and hybrid checking bank accounts. Is your money secure? Here's what you need to know!\nFDIC insurance covers deposits at banks that have FDIC insurance. From the FDIC website, deposits include the following:\n- Checking accounts\n- Negotiable Order of Withdrawal (NOW) accounts\n- Savings accounts\n- Money Market Deposit Accounts (MMDAs)\n- Time deposits such as certificates of deposit (CDs)\n- Cashier’s checks, money orders, and other official items issued by a bank\nStandard FDIC deposit insurance includes coverage up to $250,000 per depositor, per FDIC-insured bank, per ownership category. This limit applies to the total for all deposits owned by an account holder.\nIf you have multiple accounts, they are added together and insured to the limit. For example, if you have a $100,000 account, a $150,000 account, and a $50,000 account, equaling $300,000 in total, only $250,000 of this total will be insured.\nIf you have a joint account (with, say a spouse), your limit is combined - so $500,000.\nFDIC insurance does cover earnings on deposits, assuming the overall account value does not exceed the $250,000 insurance limit. If you have $200,000 in an account that has earned $5,000, the full $205,000 is insured since it does not exceed the $250,000 limit.\nTo better understand the various scenarios that deposits are covered under, check this interactive graph provided by the FDIC. Additionally, you can use the FDIC Electronic Deposit Insurance Estimator (EDIE). The EDIE lets you input your specific deposits and shows amounts that are FDIC-insured.\nTo receive FDIC insurance, you simply need to deposit funds with an FDIC-insured bank and one of the account types mentioned above. No application needs to be filled out to receive coverage.\nIt is critical that the bank has FDIC insurance to receive any coverage. To find out if a bank has FDIC insurance, check that the FDIC seal is present, which is usually on the bank’s door, or ask a bank representative. You can also use the FDIC’s BankFind tool.\nWhat Isn’t Covered\nOnly deposit products are covered by FDIC insurance. The following account types are not covered:\n- Stock investments\n- Bond investments\n- Mutual funds\n- Life insurance policies\n- Municipal securities\n- Safe deposit boxes or their contents\n- U.S. Treasury bills, bonds or notes\nYou can see a comparison chart here. What does this mean for your 401(k)? This will depend on what’s in the 401(k). Is there a cash or money market component to the 401(k)? If so, those assets may be covered but check with your 401(k) administrator to be sure (and they may be covered by another organization called the SIPC).\nFor those with over $250,000 of eligible funds, leaving them at one bank will exceed the insurance limit. But there are ways around the limit. Most involve distributing funds to multiple banks, each with a maximum of $250,000. This means someone with $1 million can distribute funds across four banks to receive full coverage.\n“If you wanted to do that directly at a bank, you’d have to set up differently titled accounts or have your funds literally placed in different banks,” says Erik Lind, vice president of cash management products at Fidelity Investments.\n“A more convenient way to gain the expanded coverage may be to open one account at a brokerage provider that can automatically cascade your assets throughout its bank network coverage, rather than having separate deposits in a bank or multiple banks,” says Lind.\nThese accounts are called cash management accounts, and Fidelity offers a great one that we list on our list of the best free checking accounts.\nWhat Happens to My Money When a Bank Fails?\nWhile rare, banks do fail. If an FDIC-insured bank fails and your money is in an insured account, rest assured that you are covered up to $250,000. Accountholders are insured dollar for dollar. Depositors are usually paid their insurance within only a few business days after the bank’s closing and often by the next business day.\nThere are two ways in which depositors can receive insurance when a bank fails:\n- The FDIC moves all insured accounts within the failed bank to another FDIC-insured bank.\n- The FDIC sends accountholders a check equivalent to their insured account value.\nThere can be delays for some of the more complex account types such as trusts and accounts opened by a third-party broker. These accounts need further review to determine how much is insurable.\nThe FDIC will become the receiver of a failed bank and sell off the bank’s assets. For depositors that have account values in excess of $250,000, proceeds from the bank’s assets are used to pay back uninsured funds. Although, it can take years to sell all of a failed bank’s assets. It is also unlikely those depositors will receive 100% of their uninsured funds.\nWhen checking out a new banking product, make sure that you understand whether your deposit is insured or not. We rely on banks to keep our money safe. This is how they are different than an investment product.\nYour bank account should not lose value as the result of the organization's management. That's why the FDIC exists. Make sure you know if your money is protected."", 'Safeguarding Your Accounts\nHelping protect our customers\' assets is an important part of our commitment to providing the best service possible.\nWhat is FDIC insurance?\nThe Federal Deposit Insurance Corporation (FDIC) is a U.S. government agency that insures cash deposits at FDIC member banks, generally up to $250,000 per account.1\nWhat is eligible for FDIC insurance at Fidelity?\nFidelity\'s FDIC Insured Deposit Sweep Program (the ""Program"")\nThrough the Program, the uninvested cash balance in certain Fidelity accounts is swept to one or more program banks where it is eligible for FDIC insurance.\nThe following Fidelity accounts utilize the Program:\n- The Fidelity® Cash Management Account\n- Certain eligible Fidelity retirement accounts such as Traditional, Rollover, and SEP IRAs; Fidelity Roth IRAs, Fidelity SIMPLE IRAs\n- Fidelity Health Savings Account\nBrokered certificates of deposit (brokered CDs)\nFidelity offers investors brokered CDs, which are issued by banks for the customers of brokerage firms. These CDs are usually issued in large denominations and the brokerage firm divides them into smaller denominations for resale to its customers. Because the deposits are obligations of the issuing bank, and not the brokerage firm, FDIC insurance applies.\nFidelity\'s FDIC Insured Deposit Sweep Program details\nIn utilizing the Program, your uninvested cash balance is swept to a program bank where the deposit is eligible for FDIC insurance. If you have more than $245,000 in uninvested cash in your account, the Program maximizes your eligibility for FDIC insurance by systematically allocating this uninvested cash across multiple program banks. At a minimum, there are generally five banks available to accept customer deposits, making customers eligible for nearly $1,250,000 of FDIC insurance.2\nThe following links provide a current list of the program banks participating in the Program, based on the type of account:\n- Fidelity® Cash Management Account Program Bank List\n- Fidelity Individual Retirement Account (IRA) Program Bank List\n- Fidelity Health Savings Account (HSA) Program Bank List\nPlease note that these lists may change over time as program banks are added or removed.\nHow the Program works\nFidelity automatically performs all transfers between your account and the program banks and provides anytime access to view the amount of cash at each program bank via Fidelity.com.\nEach program bank will receive a maximum of $245,000 to help ensure that any accrued interest is also eligible for FDIC insurance (which has a $250,000 coverage limit). Any deposits over $245,000 will be systematically distributed across multiple available program banks.\nFor example, if $500,000 is deposited, $245,000 will be swept into each of the first two available program banks and the remaining $10,000 will be swept into a third. If a subsequent deposit of $50,000 is made, that will be deposited into that same third program bank.\nDeposit amounts in excess of FDIC limits\nIn the event your balance on deposit at a program bank exceeds $250,000 you will be sent an email alert notifying you of that fact. However, it is important that you independently monitor your deposits at each bank, including deposits at the bank outside the Program to ensure you do not exceed the applicable FDIC insurance limit, because the FDIC calculates the limit based upon all the accounts you hold at a bank in the same right and capacity—not just the funds in the Program.1\nWhat is SIPC?\nThe Securities Investor Protection Corporation (SIPC) is a nonprofit organization that protects stocks, bonds, and other securities in case a brokerage firm goes bankrupt and assets are missing.\nThe SIPC will cover up to $500,000 in securities, including a $250,000 limit for cash held in a brokerage account.\nWhat Fidelity accounts are covered?\nExcess of SIPC\nIn addition to SIPC protection, Fidelity provides its brokerage customers with additional ""excess of SIPC"" coverage. The excess coverage would only be used when SIPC coverage is exhausted. Like SIPC, excess protection does not cover investment losses in customer accounts due to market fluctuation. It also does not cover other claims for losses incurred while broker-dealers remain in business. For example, fraud claims would not be covered if the brokerage firm was still in operation. Total aggregate excess of SIPC coverage available through Fidelity\'s excess of SIPC policy is $1 billion. Within Fidelity\'s excess of SIPC coverage, there is no per customer dollar limit on coverage of securities, but there is a per customer limit of $1.9 million on coverage of cash awaiting investment. This is the maximum excess of SIPC protection currently available in the brokerage industry.\nBoth SIPC and excess of SIPC coverage is limited to securities held in brokerage positions, including mutual funds if held in your brokerage account and securities held in book entry form.\nInvestment assets not covered by SIPC\nCertain assets are not eligible for SIPC protection. Among the assets typically not eligible for SIPC protection are commodity futures contracts, precious metals, as well as investment contracts (such as limited partnerships), and fixed annuity contracts that are not registered with the U.S. Securities and Exchange Commission under the Securities Act of 1933.\nWhile the following investment products are not insured or eligible for FDIC, SIPC, or any specific coverage, Fidelity is proactive in keeping assets safe.\nIf you own Fidelity mutual fund shares directly, not through a brokerage account, your investment is in assets that are the property of the funds, not Fidelity. The funds and Fidelity are separate and distinct legal entities. The assets of each Fidelity fund are held by its custodian separate from any other assets belonging to Fidelity or any other fund. Neither Fidelity nor its creditors may access the funds\' assets to satisfy financial obligations of Fidelity.\nWhile SIPC and Lloyd’s of London protection applies to brokerage accounts, it does not apply to directly held mutual fund accounts.\nWorkplace retirement accounts\nAs a provider of recordkeeping services for workplace retirement plans, including 401(k)s and 403(b)s, Fidelity\'s services are governed by federal laws. These laws generally require retirement plan assets to be held in trust, segregated from the employer\'s or recordkeeper\'s assets. In most situations, when assets are held in trust, they are protected from creditors in the event that an employer or recordkeeper has financial problems.']"	['<urn:uuid:d8521166-ba8d-4cc4-b823-5ee7ead243e3>', '<urn:uuid:6754098f-9937-4ae4-881e-25a39e5ac33f>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T06:04:36.674696	40	109	1998
16	What causes standing waves and how to minimize them?	Standing waves occur when sound bounces between parallel surfaces like opposite walls or floor and ceiling, causing certain frequencies to become unnaturally loud or quiet. They can be minimized by avoiding square rooms and using bass traps in room corners. Additionally, following the 'half and half rule' helps - if one wall is treated with acoustic material, the opposite wall should be left untreated.	"['Guitar | Bass | Keyboard | Microphones | Mixers | Audio Interfaces | Monitors | Sequencers | Soft Synths | Live Sound | Drums | Club | Accessories | Blowouts\nHome Studio Setup Tips\nand accessories for home, project, pro studios\nby Rich the TweakMeister\nOrganization is everything when it comes to building an efficient home recording studio. Its about space relative to your body, gear relative to your hands, ideas and possibilities close to your mind.\nThe most critical variable in the computer based studio is having a keyboard near your computer keyboard. Ideally, you want to be able to press record and play at the same time your other hand triggers notes. You also want to hear your music in perfect stereo when your head is pointed to the computer monitor, and also when you are at your mixing desk. Having 2 sets of speakers helps. I suggest near field monitors pointing at you when you look at your computer screen and larger mid field monitors when you are facing your mixing desk. You switch back and forth as you build the song. While you are editing the song on the computer you can use the near fields at a moderate-to-low volume to save ear fatigue, and as you do your final mix, you can use the big guns to critically evaluate depth, boominess, crispness, balance and other issues. Your tweeters should be at ear level. That\'s pretty important. Speakers sound different off axis. Build everything else around these considerations. Whatever you do, avoid putting speakers in corners. Move them a few feet out if it has to sit in a corner so you don\'t get an unwanted bass boost.\nRectangular rooms are better than square rooms. If possible have the big speakers pointing down the long side of the room to maximize the distance between the back wall and the back of your head. If your back is against the wall, the reflections off the wall will play some tricks on your ears.\nRe-rigging my rig in a 2006 downsize\nWhen setting up or rearranging your studio, think of every possible angle by which you may need to access your equipment. This gets tricky with items that you may need to access from behind, as well as from the front--especially mixing boards. If the jackfield is against a wall in the middle of a table, I can guarantee you you won\'t be repatching very often. I recommend leaving at least 2 feet behind your racks free so you can get back there and troubleshoot.\nGet a good, comfortable\nchair on casters that is adjustable and supports your back properly.\nMany office supply stores have good ones. Make sure it does not squeak.\n(Though I have to admit, I\'ve gotten some great really scary samples out of swivel\nchair squeaks, but normally you don\'t want these in the mix, lol.)\nNow stretch your arm\nout and spin around 360 degrees in your new swivel chair. The circle your\narm makes is your prime studio real estate. If there is an object in\nthis space you never touch when working on music, move it back. Move\nobjects you do touch into this valued inner circle. You will not believe the difference\nthis can make. Move all your set-once-and forget pieces to the back. That\nmight include room EQs, effects boxes, synth modules, amps, midi interface racks,\netc. Stuff that goes up front, obviously, you main keyboard, grooveboxs, synths\nwith real time controllers, patchbays, and samplers (unless you edit them remotely).\nAll that #$%@& DJ Gear. Hey, I like grooveboxs as much as anyone. No dudes. I really do. Yet while they might be great in a DJ Coffin, they pose quite a problem in a home studio, namely, they take too much vertical space because they can\'t be racked. I mean if all my synths were ""desktop modules"" I\'d have to install a little jet pack to my chair to get to it all!\nOf course you could\nget rack drawers, but once you price those you might think different. An attractive\nyet inexpensive solution is a budget computer desk, the kind parents buy for school\nkids. They have a sliding drawer for a keyboard and are about 2-3 feet in\nlength. Yep, you just put your electribe, your Kaoss pad, or whatever else\nthat is thin in the drawers and pu your heftier grooveboxs on the top. And\nyou get space underneath to store your mics, cds, zips, on one side and have some\nstackable space on the other side for stuff like amps, tape decks, whatever else\nyou have that can\'t be racked. You can even store books there, like your manuals,\nwhich you have read cover to cover, right?\nLighting. Dimmers and florescent lamps may induce hum into your audio system. Invest in lo wattage bulbs if you like to work with low light. Make an atmosphere of lighting to make your studio a creative place that\'s easy on the eyes. Besides, VST looks better in low light. Ambient lighting is good, it saves eyestrain, especially if you are spending long sessions staring into a computer monitor. Avoid all situations of glare reflected from the monitor, it will save you headaches, and we don\'t need pain when we create something beautiful, do we?\nKeep a well-stocked\nbox of adapters. Yes. Go to Radio Shack. You have my permission.\nDon\'t let those smirking pros in the corner stop you.\nDo you have a Patchbay? Don\'t think you need one? Take this little test. Have you ever run out of mixer channels? (Who hasn\'t!) Is there ever an unused mixer channel in any of your mixes because it was tied up with a synth you didn\'t need for the piece? Do you ignore the sub outs on your synths and run everything from the main outs? If you answered all 3 of these as YES then you NEED a patchbay and if you answered yes to 2 of the three you will really appreciate having one. Sure it\'s a bit of a pain to set up and you have to buy lots more cables, but it is very much worth it because you can make your studio as flexible as you want. Here\'s the class on setting up your patchbay. Get all your old gear out of the closet and patch it into the bay. If you ever want to use it again, it will be about 2 seconds away from being live and patched into your board. Using the sub outs of synths and samplers dramatically increases your mix quality. If you dedicate a mixer channel to just the bass drum alone you improve your mix greatly.\nIf you do go with\na Patchbay, make a list of what gear is connected to each in and out. A standard\n24 point patchbay has 96 jacks on it. Don\'t trust yourself to remember all\nof them. When you change your rig, update the list.\nKill the Hum. When laying out your studio, route all the AC cords first. Put them in a pathway that will cross audio cables at a 90 degree angle. Should you use cable ties? I say no, though clearly your studio will look neater if you do. However, you might find that if you ever want to move a piece of gear you have to undo the harness and it\'s a pain. Avoid all situations where an audio cable travels parallel with an a/c cable as the audio cable will pick up dreaded 60 cycle HUM. Keep audio cables away from wall-warts (adapters).\nAt a normal listening\nlevel, with nothing playing, gates switched off, your audio should be silent.\nNow, carefully put all your faders to 100% and slowly turn up your amp till you\nhear noises, hums, rfi. Isolate the most offensive of these noises to single\npieces of gear and see if moving the ac/audio cable paths helps. Usually,\nyou can improve your signal-to-noise ratio substantially by running this test, and\nyou learn which pieces in your studio are the noisiest. This helps during\nthe mix as you can make better decisions about boosting a single level or cutting\neverything else to make an instrument cut the mix.\nAcoustics: the really simple way. Treat the walls in your studio. Clap your hands loud in your studio. No, not to find the remote, but to listen for any ringing or resonance. You don\'t want resonance here, you analog filter heads! (smile). Do the foam thing till the only ringing you hear is your tinitus. Ouch! I mean till you clap and the sound does not bounce back. Shoot for a room where instruments sound full and clear, neither sproingy or muffled. Half dead, half alive. You know, the way you feel after 2 weeks of all night sessions.\nHalf and Half rule. If you have carpet, don\'t treat the ceiling. If you treat the north wall, don\'t treat the south wall, ditto for east and west. Square rooms are not good as they create standing waves where a certain frequency will resonate. Of course this is all a simplification to get you going. A more scientific way is place some of them where they will diffuse the early reflections coming off the walls and ceiling over your head and on the wall area your ears point to. But even just adding some foam will help. Sound travels fast (at the speed of sound!) and the more diffusion you add, the more dead the room will sound. You don\'t want totally dead, but a room where things sound good.\nA professional approach will carefully analyze the room and place a variety of bass traps and sound absorbers/diffusers at strategic locations. Bass is a problem, particularly in small studios. Bass frequencies bounce around the room in such a fashion to give uneven bass response, depending on where you sit or stand. A Bass trap is normally put close to the corners, where they can be effective at ""trapping"" the bass frequencies. But this is just to foreshadow the next article, which will deal with acoustics in more detail.\nKeep you cables\nwhere you will need them. Behind your rack is a great place.\nKeeping it clean.\nYou\'ll be happier in your studio if you dust it every now and then. I use\ncotton cloths with either a little Windex or Endust for electronics sprayed on them.\nJust a little. Best not to spray on the gear directly. A featherduster\nis very useful for getting dust off of mixers, where the knobs are so close together\nits hard to get the surface.\nInvest in the absolute best all purpose Phillips screwdriver you can find. Get a good pair of needle nose pliers, jewelers screwdrivers, and wire cutters.\nHeavy drapes over windows will keep outdoor noises from ruining your tracks. Also, tell the neighbors to keep it down after 3am, you\'re trying to make music in here.\nEvery time you burn a CD don\'t forget to write down what\'s on it on the cd itself. You will thank me 10 years from now when you have thousands of songs. Access is everything. Think like this: Anything you fail to mark is lost.\nStudio-Central Member\'s Link\nYou can learn a lot about Room acoustics and solutions to common problems at www.realtraps.com\nAlso see Acoustic Treatment and Design for Recording Studios and Listening Rooms by Ethan Winer\nHave fun in your studio!\nGo to the Next Class\nGo to the Previous Class\nWant to read more about soundproofing and building a great studio room?\nRead Acoustics 101, courtesy of Auralex\nTweak\'s Articles on Essential Studio Concepts', 'Chances are, unless your home studio was specifically designed for recording and mixing music, it’s a less than ideal listening environment. Professional studios are carefully planned out to eliminate naturally occurring acoustic issues in a way that residential rooms are not. A well designed one will have no parallel surfaces and ample acoustic treatment.\nTo get the most out of your home studio, adding acoustic treatment can minimize many of the major problems that misrepresent what recordings and mixes actually sound like.\nProblems to Consider in Your Studio\nStanding waves are produced when a wall’s dimension is equal to, or a multiple of, the wavelength of a particular frequency. They occur when sound waves bounce between parallel surfaces, such as between opposite walls and between the floor and ceiling.\nStanding waves are a form of interference. As reflected waves collide with the original sound source, they can either combine or cancel each other out. This is referred to as constructive and destructive interference, respectively. The result of constructive interference is an unnaturally loud frequency, while the opposite is true of destructive interference. This typically occurs in the low end with frequencies around 300 Hz and below–those with longer wavelengths.\nSuch interference can create an inaccurate representation of the low end material in a mix. If your room has a constructive standing wave at, say, 100 Hz, you may compensate by carving out that particular frequency in the mix. When played in a different environment, your mix may be too thin.\nEarly reflections are those sounds first reaching our ears after bouncing off of a single surface in our listening environment. Psychoacoustically, our brain quickly determines the level and directionality of a sound source.\nCombining with the direct sound leaving our speakers, early reflections can alter our perception of where a sound is placed in the stereo field and how loud it is.\nJust as with standing waves, early reflections at higher frequencies can move through cycles of constructive and destructive interference with the direct sound. The result is a phenomenon known as comb filtering–a frequency response with sharp peaks and dips resembling a fine-tooth comb.\nAcoustic Treatment Options\nEven though home studios are often problematic listening environments, a little bit of acoustic treatment from DIY acoustic panels can make a drastic difference.\nWhile it’s nearly impossible to treat every acoustic issue without building a pro studio from the ground up, working on standing waves and early reflections is a fantastic start. Most importantly, it can be done on a budget, as well. The following are the three most common types of acoustic treatment used to combat the issues mentioned above.\nAcoustic absorption is used to tame the mids and highs in a given environment. You’ll want to use absorption to treat the early reflection points in your room: behind your monitors; on each side wall nearest the monitors; and on the ceiling above your monitors.\nFinding the early reflection points in a room can be as simple as running a mirror along the wall and placing an absorptive panel where a monitor is reflected in the mirror. Again, these points will be somewhere on the back wall behind your monitors, on each side wall nearest the listening position, and on the ceiling above your listening position.\nMany companies produce absorption commercially, but building them yourself is a much more cost-effective alternative if you have the tools and time.\n1″ x 4″ lumber and R13 denim insulation is easy to work with and leaves you with effective 4″-thick absorption! If your handy, check out our guide to building your own acoustic panels.\nPlaced in the corners of rooms, bass traps are used to control the low end response of a space. In small rooms especially, unwanted buildup of low end material can be a huge problem.\nIdeally, floor-to-ceiling bass trapping straddling each corner of the studio at a 45-degree angle is ideal. If you are looking for more guidance on building these out on a budget check out our guide to DIY bass traps for your home studio.\nIf you don’t have the space or resources to fill each corner entirely, don’t hesitate to straddle each corner with a panel like the ones mentioned previously in a size that suits your environment.\nWhile bass traps and absorption are used to absorb frequencies, acoustic diffusion is designed to evenly scatter frequencies throughout a space.\nIn most cases, we don’t want an entirely dead mixing environment. Diffusion, placed on the back wall of a home studio, takes sound waves and scatters them throughout the space.\nWhile we haven’t lost any acoustic energy as we have with absorption, the scattered sound makes it much harder for our ours to determine where reflections are coming from, and how loud they are.\nDiffusion essentially de-concentrates sound waves in the listening environment. Diffusers are commonly made of a hard, reflective material like wood, and may be of the “skyline,” “triangular,” or “spherical” variety.\nFor further information on the basics of acoustic treatment, please enjoy this interview with renowned educator and engineer, Bobby Owsinski!']"	['<urn:uuid:4645c2ab-123e-4f8b-b440-2b818a4f2798>', '<urn:uuid:849b2582-0ac3-4934-adca-8269290b6b9d>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T06:04:36.674696	9	64	2793
17	What happens if I breed from bulls with good survival scores?	By breeding from bulls with positive Calf Survival PTAs, you will increase the probability that more of your youngstock will survive to 10 months of age, increasing the number of heifers to breed from and bring into the herd as replacements.	['As featured in The UKJT March Edition, AHDB Genetics Manager Fern Pearston Explains the heritability of health and fitness traits and how selection for these traits could improve herd management.\nHealth and fitness traits are often overlooked due to their lower heritability but these traits can be improved on farm by selecting the right sires to use on your herd, in addition to improved management techniques. After all, improved genetics for these traits will make management of the herd easier and who wouldn’t want that?\nThe story so far…\nThe Somatic Cell Count (SCC), Lifespan and Fertility indexes have been available for use in bull selection for a number of years, however the trends found when looking at pedigree Jersey cows show not all of these traits appear to have been considered when selecting bulls.\nFigures 1A, 1B and 1C show the trend for pedigree Jersey females in the milking herd (represented by the solid line) and young females yet to join the milking herd (represented by the dashed line). The vertical dotted lines indicate the year these indexes were made available for selecting bulls.\nThe milking cow Predicted Transmitting Abilities (PTAs) are based on on-farm and milk records, whereas the young animals have their PTAs based on parent average calculations. Please note, 2019 is not a complete data set, so is an indication of this birth year.\nGenetics for Lifespan in the national herd shows an improving trend (Figure 1A).\nLifespan improved by 30 days between 2006 and 2016, that’s one month more in lactation on average! This continues to improve with the young animals entering the herd in coming years.\nSCC has fluctuated over the last two decades, peaking in 2015 at +1% (Figure 1B). PTAs greater than zero are unfavourable for SCC and indicates that this PTA has not be consistently selected for in the national herd. The positive story here is that the most recent years of breeding look as though the focus has changed to improving SCC genetics with 2017, 2018 and 2019 animals on average below zero for SCC, reducing cell counts in the national herd.\nFigure 1C shows the national trend for Fertility Index. Again, over the last 20 years, this trait has worsened in the national herd but looks to have made a turn for the better in the last few years of breeding with 2019 animals expected to have improved genetics for fertility than the breed base of zero. Increasing the Fertility Index of the national herd will result in reduced calving interval and improved non return rates.\nKeeping the progress going\nAre there bulls on the market to keep the positive trends for SCC and Fertility Index moving in the right direction? AHDB recommends selecting bulls who rank in the top half of the available list for your calving system then look within this shortened list for bulls who meet your herd’s specific breeding criteria.\nTable 1 lists the average for the top 50% of daughter proven and young genomic Jersey bulls for the above three traits. (This example is based on Profitable Lifetime Index (£PLI) which is recommended for all year round calving herds.)\nTable 1. Average Predicted Transmitted Ability (PTA) for the top 50% of daughter proven and young genomic Jersey ranked on £PLI for SCC, Lifespan and Fertility Index.\n|Trait||Daughter Proven Bull Average||Genomic Young Bull Average|\nOn average, both the daughter proven and young genomic bulls offer the genetics to continue improving these trends for the national herd.\n- For SCC, on average, daughter proven bulls in the top 50% would transmit a 7% reduction in cell counts and the average young genomic bull would offer an 8% reduction compared to the breed base of zero.\n- Bulls in the top 50% daughter proven and young genomic lists are, on average, offering over 75 days and 70 days more Lifespan, respectively, than the breed base.\n- In the case of Fertility Index, the daughter proven bulls in the top 50% of the £PLI ranking will on average improve FI by 1.4 points compared to the breed base. The average for the top 50% of young genomic bulls is around the same level as 2019 born females, however there will be bulls in the top half of the young genomic £PLI list which will be able to improve this trend for the national Jersey herd.\nMore recently, breeding indexes to help address mastitis, lameness and calf survival on farm have been released. The Mastitis index was released in April 2017 and is useful to use alongside the SCC index to identify bulls who will improve cell counts as well as reduce cases of mastitis in your herd, as there are some which address one but not both traits. As with SCC, negative Mastitis PTAs are favourable with a 1% drop in Mastitis PTA indicating 1% drop in cases of mastitis per lactation.\nLameness Advantage and Calf Survival were both released December 2018. Breeding for positive figures in Lameness Advantage will help to reduce cases of lameness in your herd. Based on on-farm recordings of lameness in addition to type classification data and Digital Dermatitis records, this index will help to breed for improved foot health in your herd, where with each 1% increase in Lameness Advantage, 1% fewer cases of lameness per lactation will occur, on average.\nCalf Survival is another index where positive figures are favourable. This index is based on BCMS records between tagging and 10 months of age. By breeding from bulls with positive Calf Survival PTAs you will increase the probability that more of your youngstock will survive to 10 months of age, increasing the number of heifers to breed from and bring into the herd as replacements.\nTools to help\nAll of the traits discussed above are included in the UK national economic indexes; Profitable Lifetime Index (£PLI – for all year round calvers), Spring Calving Index (£SCI – for spring block calvers) and Autumn Calving Index (£ACI – for autumn block calvers). More information on each trait and the UK national economic indexes can be found on the AHDB website: http://dairy.ahdb.org.uk/technical-information/breeding-genetics/useful-resources-related-information\nThe economic index which best suits your calving pattern should be used as an initial filter for bull selection, selecting the top 50% of the list, before drilling down further into the proofs to ensure bulls meet the specific traits to meet your breeding goals.\nHealth and welfare traits should be part of your herd breeding strategy. AHDB’s Breeding Trait Selector helps prioritise which traits to focus on for the coming breeding season: https://breedingdairy.ahdbdigital.org.uk/breedingtraitselector\nNational trends are a great way to get an overview of how the breed is going but to get a better idea of how health and welfare traits look for your herd use the Herd Genetic Report, available through AHDB to all herds fully milk recording in the UK. The report shows how your herd is progressing for these traits genetically and areas you might like to focus more on when selecting the next bulls to use on your herd. To sign up for the Herd Genetic Report complete the form at http://dairy.ahdb.org.uk/technical-information/breeding-genetics/herd-genetic-reports/register-for-the-herd-genetic-report\nHealth and welfare traits are heritable and so can be improved in your herd through breeding. Buying straws from a bull with genetics who will improve your herd health is often the same cost as buying a bad genetic bull, so the real money saving is in the long run when you have more fertile, longer living and healthier cows. Historically, health traits, with the exception of Lifespan, in the national Jersey herd have not been improving but the latest years of youngstock show that there has been a change in breeding priorities.\nGenetic progress is cumulative, building over each generation as long as good genetics for health traits continue to be used. It’s therefore important to keep health and welfare traits in your breeding goals, using tools freely available to you, to ensure that these positive trends continue to improve for generations to come.']	['<urn:uuid:c376af43-1d55-426c-b189-c0d3e283b937>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:04:36.674696	11	41	1331
18	hobby archaeologist time money work	Community archaeology projects combine volunteer work with professional standards. From Document 1, we see volunteers can participate through community projects funded by organizations like the National Lottery (£29,000 over five years in Norton's case). Professional archaeologists oversee these projects to ensure proper standards. As for time commitment, Document 2 explains that archaeological work involves multiple phases: research, fieldwork, laboratory analysis, and reporting, with fieldwork often taking the least time. Most time is actually spent in laboratory analysis, which can take months to years to complete. Money-wise, archaeologists don't keep or sell artifacts - by law, finds on public lands belong to the public, while artifacts from private land belong to the landowner.	"['The sun is shining, scrape-scrape-chink go the trowels, and it seems like most of the people of Norton village are on their hands and knees in the paddock beyond the church, digging up their past. There is one cloud on the horizon, however: a herd of black cows.\nOn most recent mornings during the four-week dig, the school children, IT managers, teachers and pensioners who are volunteering at this community project in Hertfordshire have arrived at their field to find that the cows have knocked down their protective fence and wandered into the area of excavation.\nThe professional archeologist overseeing the dig, Keith Fitzpatrick-Matthews, worries they will have to halt their work if the cows continue their nightly rampages. But morale among the 20 volunteers, carefully sifting through the soil, could not be higher. ""You slip into a bit of a trance. You get to know a small amount of dirt very intimately, which is nice and it takes your mind off everything else,"" says local resident Philip Thomas, who has taken time off work to join his wife, Sam, and eight-year-old son, Harry, at the dig. ""He comes down every year for about 10 minutes,"" says Thomas. Harry likes the old snail shells they find.\nA few years ago, a handful of residents got together in the local pub and decided that they wanted to learn more about Norton\'s secret history. They won funding from the National Lottery and Letchworth Garden City Heritage Foundation, which owns much of the land in the area, including the field they are excavating, with £29,000 over five years paying for conservation work and professional equipment. ""It could be a dormitory village but this has brought a lot of people together,"" says Chris Hobbes, a local resident and management consultant who has helped expand the group from 10 to 130 members.\nAs the villagers are keen to stress, their dig is ""research-based"". North Hertfordshire is the best prehistoric landscape outside Wessex, according to Fitzpatrick-Matthews, who is the archaeology officer for North Herts district council and ensures everything is done to ""professional standards"". He will shout at his workers if they repeatedly do things wrong but trusts his young volunteers. ""I\'m happy to see them digging because in some cases they\'ve got as much experience as a professional archaeologist,"" he says.\nTheir site is carefully chosen: in pasture that has never been ploughed, on an old crossroads of abandoned lanes and over what they hope will be the foundations of a medieval house. The key riddle is: why was this part of Norton abandoned in medieval times? Residents are also studying the village\'s manorial records, which are almost unbroken between 1244 and 1916 and cover births, marriages, deaths and all kinds of rents, fines and disputes.\nAt 25cm down, the group has reached the 16th century and is working down to the 1400s. So far, they have turned up beautiful Roman and medieval pottery but as Thomas says: ""It\'s not about the finds, it\'s about the different layers and how they were built up and link in with each other.""\nLisa Waldock, 18, has finished her A-levels and will study archaeology at university. ""It is fine once you get over the blisters on your hands,"" she says as she delicately sifts through the soil. ""It\'s good to know that with every bit you trowel away you could find something interesting."" When Cameron Gormill\'s history teacher mentioned a local dig, the 13-year-old went along. ""I really enjoyed it and I just kept coming back."" He enjoys how painstaking it is. ""If you see something sticking out you can\'t just dig it up, you\'ve got to work around it."" The most exciting thing he has found is a bronze-age flint arrowhead, which he dug up in a back garden in the village.\nFitzpatrick-Matthews keeps a keen eye on the finds and shows how much can be deduced from small shards of pottery. Many he can identify as coming from local kilns. He would much rather work on a community dig than on a commercial operation. ""This is lovely because you are not constrained by a developer\'s timetable. We have the luxury of being able to trowel everything,"" he says.\nAt the end of the summer, the group will erect a marquee, display their finds and talk about what they have learned. They will also pay for professional archeologists to verify their discoveries, many of which will end up in local museums. As well as retirees, ""there\'s a hardcore of youngsters who have been bitten by the bug"", says Mick James, one of the founder members. Village archaeology has ""taken my life over"", he says. ""If it appeals, archeology becomes like a religion.""', 'After working in the field and the lab the teachers in the Eisenhower Professional Development Project/Elementary and Secondary Education Act Title II entitled Using Archaeology as an Integrated Gateway to Teacher Professional Development Grant put together this list of Frequently Asked Questions. It should help answer some of the most common questions asked by students, educators and the public.\n- Do archaeologists dig up dinosaurs?\n- What are the artifacts worth?\n- Do archaeologists get to keep what they find?\n- Can I conduct an archaeological dig in my own backyard?\n- What do I do if I find an arrowhead?\n- What do you do if you find bone?\n- What is the difference between a prehistoric artifact and an historic artifact?\n- How do you know how old an artifact is?\n- How does an archaeologist decide where to dig?\n- Once an archaeologist finds an artifact what do they do with it?\n- How do archaeologists spend most of their time?\n- Why teach archaeology?\n- How many college and universities in the United States offer a degree in archaeology?\n- What jobs and career paths are open to individuals with an undergraduate degree in archaeology?\nNo, paleontologists dig up dinosaurs. Paleontologists study fossils of plants, animals, and other organisms that lived in prehistoric or geologic times. Archaeologists study past human life and culture by looking at material items people left behind. Dinosaurs were extinct by 60 million years ago, and people have only been around for a few million years, so they never overlapped (although movies love to put them together!). Even if they don’t find dinosaurs, archaeologists do sometimes see remains of extinct animals, such as this Ice Age mastodon bone. (John and Otto Swennes holding the front leg bone (ulna) of an Ice Age mastodon they found after a flash flood in Long Coulee, near Holmen, Wisconsin.)\nTo archaeologists, artifacts have no dollar value. Instead, they are priceless because of what they can teach us. Archaeologists learn about the past by studying artifacts and their contexts-where the artifacts were found and what was found with them. Unfortunately, buying and selling artifacts (and the large amounts of money that are sometimes involved) can tempt some people to loot sites, trespass, sell fakes, and engage in other illegal or unethical behavior.\nProfessional archaeologists do not keep, buy, sell, or trade artifacts—the artifacts do not belong to them. By law, artifacts found on federal or state lands belong to the public and must be curated, or taken care of, on the public’s behalf. Even archaeologists need permits to excavate on public land. Artifacts from private land are the property of the landowner. Many landowners allow archaeologists to remove artifacts so they can be studied, and appreciated by others. Other landowners keep the artifacts but work with archaeologists to make sure the information is recorded properly. Collecting or excavating on any site is illegal without express permission of the owner. As an institution, MVAC houses many artifacts that people have donated or that came from projects where long-term curation was arranged. MVAC staff do not have personal artifact collections.\nIf you find an arrowhead on the surface of private land, make sure you remember exactly where you found it. It’s helpful to plot the location on a sketch map, local map, or air photo (readily available online). If you have the landowner’s permission, you can take the artifact—just make sure you keep the location information with the artifact. An archaeologist can help identify the arrowhead and record where it was found. Each state keeps a listing of known sites, and adding the site to that list will help with future studies and planning. If you find an arrowhead on federal, state, or other public land, you should leave it where it is. Collecting artifacts on public land is illegal. But taking a photo, keeping track of the location, and reporting your find to the park ranger or appropriate office can be a big help. If you have a GPS unit, you can take a reading and report that also.\nIf you find bone while you are digging in your garden, working on a construction site, or walking through the woods, and there is ANY chance that it is human, you should call local law enforcement right away. If the bone seems to be archaeological (for example, someone has disturbed a burial mound or cemetery), it must also be reported. By law, specific processes must be followed in dealing with human remains. State laws vary, but in Wisconsin, any disturbance to a burial site must be reported to the Burial Sites Preservation office at the Wisconsin Historical Society (1.800.342.7834). Speak to a staff member and explain the situation, or leave a detailed voicemail. In Minnesota, contact the Office of the State Archaeologist (612.725.2411). If you’re sure it’s not human, but want to know what it is, you can send MVAC a photo of it (include either a ruler, a coin, or something as a scale).\nArchaeologists use the term “prehistoric” to refer to time periods or cultures for which there are no written records. “Historic,” then, refers to time periods or cultures for which there are written records. Written forms of language were brought to the Americas by Europeans, so artifacts and cultures dating before the arrival of Europeans are often known as prehistoric. This does not mean that Native peoples had no history! Native cultures throughout the Americas had long, rich histories passed from one generation to another by stories, art, songs, and other ways of preserving and communicating traditions. Historic sites in the Americas range from Euro-American farmsteads to factories, military forts, cemeteries, and Native villages that postdate European contact. To avoid confusion, many people use the terms “precontact” and “postcontact,” rather than prehistoric and historic, to separate the times before and after Europeans arrived.\nArchaeologists use many methods for determining ages of artifacts and sites. Relative dating methods tell whether something is older or younger than something else. Usually, older things are found below younger ones, unless there has been some disturbance. Absolute dating methods provide a calendar age. Radiocarbon dating is the best-known form of absolute dating. It can only be used on organic materials. A radiocarbon date obtained from a burned cherry pit found in a hearth can tell us the age of artifacts found with it. A radiocarbon date from these cherry pits can tell us the age of stone tools or pottery found with it.\nWhen archaeologists are conducting research that is not part of a CRM project, they might be looking for new sites, or might be trying to get more information from known sites. If they’re looking for new sites, they could check the written historical records of the area or first hand accounts from local land owners. Land owners typically know a fair amount about their property including the history of the area. They might have found artifacts on their land that can tell archaeologists the kinds of sites that might be present. They would do some survey work to see if they can find a new site in their area.\nIn addition to this, the state archaeology offices have a record of all known archaeology sites around the area, some of which might not have been explored yet. This would be another good way to determine a potential site for archaeologists to dig. The choice of a site would depend on what they were seeking to learn. If the archaeologists wanted to know about how people lived along the Mississippi River, then sites exposed in the bank of the river would be good places to test.\nWhen on the property that an archaeologist plans to dig, several factors come into play in deciding specifically where to start:\n- The lay of the land – In some cases it is possible to choose the place to put the excavation units through looking at a quality topographic map and actually walking the area. After personally surveying the site there may be a natural feature that would appear to be a great place to start.\n- Shovel testing – In other cases, once on the site archaeologist do shovel testing, either at random or in a grid pattern, to try to find evidence of artifact rich areas.\n- Recorded sites – All sites that have been dug by professional archaeologists are carefully recorded. In some cases it may be possible to go to a site that has not been totally excavated and continue work that a previous crew had started. Many sites are tested by different people over many years.\nBefore the artifact is removed from its context, it may be photographed in place and its precise location plotted on graph paper and then transferred to a master map of the site, preserving its context. Once the records have been made the artifact moves to the archaeologist’s laboratory. Each artifact must be minutely examined, and classified as to the type of artifact, its raw material, and so forth. Measurements of the artifact are taken and descriptions written. Then all the information from the analysis is compared with the information on other artifacts from the site, and from other sites.\nWhen people think about the work of an archaeologist, most probably think of a crew of people digging in the earth for remains of the past. The field work is only one of four phases of an archaeological project and it can often be the one that can take the least amount of time.\nResearching a site can be a very lengthy process. This may include finding out what has been done in an area before, securing funding, getting permission to excavate, as well as conducting small-scale testing sampling to determine where to dig.\nThe artifact and data collected at the site during the field work phase, will only be useful after hours of analysis in the laboratory. The laboratory phase of the project is probably the most time-consuming component of an archaeologist work as this process can take months to years to complete.\nFinally, the results of the analysis must be reported in a site report. Often, other publications and presentations, both professional and popular, follow. It is very important to share the results of the archaeology with others, and this can be done through displays, presentations, brochures, web sites, and so forth, as well as professional books and articles.\nThe field work phase of the project can often be conducted in less time than each of the other phases of the project. An archaeologist begins with paper work (research and planning), continues with paper work (data collection, laboratory work) and concludes with more paper work (site report).\nArchaeology is a innovative way to capture students attention and a great vehicle for teaching a wide variety of subjects through a multi-disciplinary approach. Archaeology can be used to teach multiple subjects including art, science, social studies, language arts and math. Students love the connection to the real world of work and seeing a practical need for knowing how to measure, record information, read a map, etc. Archaeology is a great tool for tapping into kids natural curiosity of the world around them and learning about the people of the past.\nThere are actually only a handful of institutions that offer an undergraduate degree in Archaeology. Some institutions offer degrees in classical archaeology with the emphasis on the Greek, Roman and Egyptian finds and history. Other schools offer a comprehensive archaeology program integrating archaeology courses taught from the anthropological, historical and geological perspectives.\nDepending on their interests, students might choose to pursue classical archaeology, or to study the cultures of the New World, generally prehistoric archaeology. With a broad background, students graduating from a comprehensive program will have good understanding of the sub-fields that make up the study of archaeology and be prepared for field work and any number of other applications for their degree. In choosing a graduate program, they will make a more informed choice concerning what aspect of archaeology they want to pursue.\nNote: the University of Wisconsin-La Crosse offers an undergraduate degree in archaeology. Check out the UWL Department of Archaeology & Anthropology web site.\nWith an undergraduate degree in Archaeology, an individual would qualify for entry and possibly intermediate level laboratory work in a museum or university setting. Such candidates would also be able to find work on dig sites, and, with adequate experience may supervise other crews. There are also jobs available in museums setting up displays under the supervision of a curator, or working in museums or other educational places as guides. As entry-level positions, these jobs are often low paying, but additional experience provides more avenues for advancement. However, they would not be able to direct an entire project themselves without an advanced degree.\nAny job in the fields of Archaeology and Anthropology that involve the responsibility of reporting results, supervising the excavation of a site, planning and executing displays in museums and the like, require a graduate degree. Typically, the most employable individual will have a graduate degree in Archaeology with a very strong background in Anthropology, Geography, or other related fields. To do any teaching at the university or college level requires a Ph.D.\nJobs are not found just in archaeology, though. World wide commerce has made understanding other cultures very important to success. People with the educational profile described above are being hired to steer these companies through the sometimes murky waters of cultural traditions and norms when business is being conducted in foreign countries.\nFor more information about job opportunities, check out the SAA web site at http://www.saa.org.']"	['<urn:uuid:ec3de923-e5cd-43d9-aa01-cf2b12de9d6c>', '<urn:uuid:c403538a-ae46-4a14-90fe-3a6856d3e67a>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T06:04:36.674696	5	112	3062
19	nervous problems anxiety vs psychotic disorders	Anxiety disorders and psychotic disorders are distinct psychiatric conditions. Anxiety disorders involve excessive worrying, uneasiness, and apprehension about future uncertainties, with symptoms that can last for days, weeks, or months. These can be continuous or episodic. In contrast, psychotic disorders are serious illnesses affecting the mind, including conditions like schizophrenia, which can involve hallucinations and delusions. Psychotic disorders typically require treatment with specific antipsychotic medications like clozapine, while anxiety-related conditions may be treated with medications like Alprazolam.	"['A blue, football shaped pill with the imprint 2089 V is Alprazolam 1 mg. It is used in the treatment of anxiety, depression, panic disorder, tinnitus, and dysautonomia.\nPanic disorder is an anxiety disorder characterized by recurring severe panic attacks. It may also include significant behavioral changes lasting at least a month and of ongoing worry about the implications or concern about having other attacks. The latter are called anticipatory attacks (DSM-IVR). Panic disorder is not the same as agoraphobia (fear of public places), although many afflicted with panic disorder also suffer from agoraphobia. Panic attacks cannot be predicted, therefore an individual may become stressed, anxious or worried wondering when the next panic attack will occur. Panic disorder may be differentiated as a medical condition, or chemical imbalance. The DSM-IV-TR describes panic disorder and anxiety differently. Whereas anxiety is preceded by chronic stressors which build to reactions of moderate intensity that can last for days, weeks or months, panic attacks are acute events triggered by a sudden, out-of-the-blue cause: duration is short and symptoms are more intense. Panic attacks can occur in children, as well as adults. Panic in young people may be particularly distressing because children tend to have less insight about what is happening, and parents are also likely to experience distress when attacks occur.\nScreening tools like Panic Disorder Severity Scale can be used to detect possible cases of disorder, and suggest the need for a formal diagnostic assessment. tinnitus\nAnxiety disorder is an umbrella term that covers several different forms of a type of common psychiatric disorder, characterized by excessive rumination, worrying, uneasiness, apprehension and fear about future uncertainties either based on real or imagined events, which may affect both physical and psychological health. The disorders once classified as neuroses are now considered anxiety disorders. There are numerous psychiatric and medical syndromes which may mimic the symptoms of an anxiety disorder such as hyperthyroidism which may be misdiagnosed as generalized anxiety disorder.\nIndividuals diagnosed with an anxiety disorder may be classified in one of two categories; based on whether they experience continuous or episodic symptoms. Health\nAbnormal psychology is the branch of psychology that studies unusual patterns of behavior, emotion and thought, which may or may not be understood as precipitating a mental disorder. Although many behaviours could be considered as abnormal, this branch of psychology generally deals with behavior in a clinical context. There is a long history of attempts to understand and control behavior deemed to be aberrant or deviant (statistically, morally or in some other sense), and there is often cultural variation in the approach taken. The field of abnormal psychology identifies multiple causes for different conditions, employing diverse theories from the general field of psychology and elsewhere, and much still hinges on what exactly is meant by ""abnormal"". There has traditionally been a divide between psychological and biological explanations, reflecting a philosophical dualism in regards to the mind body problem. There have also been different approaches in trying to classify mental disorders. Abnormal includes three different categories, they are subnormal, supernormal and paranormal.\nThe science of abnormal psychology studies two types of behaviors: adaptive and maladaptive behaviors. Behaviors that are maladaptive suggest that some problem(s) exist, and can also imply that the individual is vulnerable and cannot cope with environmental stress, which is leading them to have problems functioning in daily life. Clinical psychology is the applied field of psychology that seeks to assess, understand and treat psychological conditions in clinical practice. The theoretical field known as \'abnormal psychology\' may form a backdrop to such work, but clinical psychologists in the current field are unlikely to use the term \'abnormal\' in reference to their practice. Psychopathology is a similar term to abnormal psychology but has more of an implication of an underlying pathology (disease process), and as such is a term more commonly used in the medical specialty known as psychiatry. Health Medical Pharma\ntreatment of anxiety, depression, panic disorder', ""- Schizophrenia Slideshow Pictures\n- Take the Schizophrenia Quiz\n- Physical Symptoms of Depression Slideshow\n- What is clozapine, and how does it work (mechanism of action)?\n- What are the uses for clozapine?\n- What are the side effects of clozapine?\n- What is the dosage for clozapine?\n- Which drugs or supplements interact with clozapine?\n- Is clozapine safe to take if I'm pregnant or breastfeeding?\n- What else should I know about clozapine?\nWhat is clozapine, and how does it work (mechanism of action)?\nClozapine is an anti-psychotic medication that works by blocking receptors in the brain for several neurotransmitters (chemicals that nerves use to communicate with each other) including dopamine type 4 receptors, serotonin type 2 receptors, norepinephrine receptors, acetylcholine receptors, and histamine receptors. Unlike traditional anti-psychotic agents, such as chlorpromazine (Thorazine) and haloperidol (Haldol) as well as the newer anti-psychotics, risperidone (Risperdal) and olanzapine (Zyprexa), clozapine only weakly blocks dopamine type 2 receptors.\nWhat brand names are available for clozapine?\nClozaril, FazaClo, Versacloz\nIs clozapine available as a generic drug?\nDo I need a prescription for clozapine?\nWhat are the side effects of clozapine?\nThe most common side effect of clozapine is drowsiness.\nDizziness is another side effect of clozapine. Dizziness may occur in 1 of 5 persons taking clozapine. In some cases this may be due to orthostatic hypotension, a marked decrease in blood pressure that occurs when going from a lying or sitting position to a standing position. The drop in blood pressure may lead to loss of consciousness or even cardiac and respiratory arrest. This reaction is more common during the first few weeks of therapy while the dose is increasing, when drug is stopped briefly, or when patients are taking benzodiazepines such as diazepam (Valium) or other anti-psychotic drugs.\nSeizures have occurred in approximately 1 of every 20 to 30 persons receiving clozapine. Patients receiving higher doses seem to be at higher risk.\nWhat is the dosage for clozapine?\nClozapine is given once, twice, or three times daily. The dose often is increased slowly until the optimal dose is found. The full effects of clozapine may not be seen until several weeks after treatment is begun.\nWhich drugs or supplements interact with clozapine?\nRisperidone (Risperdal) may cause an increase in the amount of clozapine in the blood. This could lead to an increased risk of side effects from clozapine.\nIs clozapine safe to take if I'm pregnant or breastfeeding?\nThere are no adequate studies of clozapine in pregnant women. Studies in animals suggest no important effects on the fetus. Clozapine can be used in pregnancy if the physician feels that it is necessary.\nAnimal studies suggest that clozapine is secreted in breast milk. Therefore, women taking clozapine should not nurse their infants.\nWhat else should I know about clozapine?\nWhat preparations of clozapine are available?\nTablets (orally disintegrating): 12.5, 25, 100, 150, and 200 mg\nHow should I keep clozapine stored?\nTablets should be kept below 30 C (86 F).\nLatest Mental Health News\nDaily Health News\nClozapine (Clozaril, Fazacio ODT, Versacloz) is a medication prescribed for the management of psychotic disorders such as schizophrenia. Side effects, drug interactions, warnings and precautions, and pregnancy information should be reviewed prior to taking any medication.\nMultimedia: Slideshows, Images & Quizzes\nWhat's Schizophrenia? Symptoms, Types, Causes, Treatment\nWhat is the definition of schizophrenia? What is paranoid schizophrenia? Read about schizophrenia types and learn about...\nSurprising Causes of Weight Gain\nThere are many reasons for sudden weight gain when there are no changes in diet or exercise. Learn to identify the cause of your...\nSchizophrenia Quiz: What is Schizophrenia?\nSchizophrenia is a complex psychiatric disorder. Learn more about the challenges of mental illness with the Schizophrenia Quiz.\nPostpartum Depression: Symptoms, Diagnosis and Treatment\nPostpartum depression is a treatable medical illness which affects women after giving birth. Learn about the symptoms, diagnosis...\nRelated Disease Conditions\nSchizophrenia is a disabling brain disorder that may cause hallucinations and delusions and affect a person's ability to communicate and pay attention. Symptoms of psychosis appear in men in their late teens and early 20s and in women in their mid-20s to early 30s. With treatment involving the use of antipsychotic medications and psychosocial treatment, schizophrenia patients can lead rewarding and meaningful lives.\nMental Illness in Children\nAbout 5 million children and adolescents in the U.S. suffer from a serious mental illness such as eating disorders, anxiety disorders, disruptive behavior disorders, pervasive development disorders, elimination disorders, learning disorders, schizophrenia, tic disorders, and mood disorders. Symptoms of mental illness include frequent outbursts of anger, hyperactivity, fear of gaining weight, excessive worrying, frequent temper tantrums, and hearing voices that aren't there. Treatment may involve medication, psychotherapy, and creative therapies.\nSuicide is the process of intentionally ending one's own life. Approximately 1 million people worldwide commit suicide each year, and 10 million to 20 million attempt suicide annually.\nPsychotic disorders are a group of serious illnesses that affect the mind. Different types of psychotic disorders include schizophrenia, schizoaffective disorder, schizophreniform disorder, brief psychotic disorder, shared psychotic disorder, delusional disorder, substance-induced psychotic disorder, paraphrenia, and psychotic disorders due to medical conditions.\nBrief Psychotic Disorder\nBrief psychotic disorder is a short-term mental illness that features psychotic symptoms. There are three forms of brief psychotic disorder. The first occurs shortly after a major stress, the second has no apparent trauma that triggers the illness, and the third is associated with postpartum onset. Symptoms include hallucinations, delusions, unusual behavior, disorientation, changes in eating and sleeping, and speech that doesn't make sense. Treatment typically involves medication and psychotherapy.\nWhat Is Schizotypal Personality Disorder?\nSchizotypal personality disorder is characterized by odd behaviors, feelings, perceptions, and ways of relating to others that interfere with one's ability to function. Medication and psychotherapy can help the sufferer to manage their symptoms.\nMental health is an optimal way of thinking, relating to others, and feeling. All of the diagnosable mental disorders fall under the umbrella of mental illness. Depression, anxiety, and substance-abuse disorders are common types of mental illness. Symptoms and signs of mental illness include irritability, moodiness, insomnia, headaches, and sadness. Treatment may involve psychotherapy and medication.\nMental illness is any disease or condition affecting the brain that influence the way a person thinks, feels, behaves, and/or relates to others. Mental illness is caused by heredity, biology, psychological trauma and environmental stressors.\nPostpartum depression is a form of depression that occurs within a year after delivery. It is thought that rapid hormone changes after childbirth may lead to depression. Symptoms of postpartum depression include crying a lot, headaches, chest pains, eating too little or too much, sleeping too little or too much, withdrawal from friends and family, and feeling irritable, sad, hopeless, worthless, guilty, and overwhelmed. Treatment typically involves talk therapy and medication.\nTreatment & Diagnosis\n- Altered Mental Status\n- Poor Hygiene\n- Loss of Speech\n- Abnormal Facial Expressions\n- Unusual Behavior\n- Catatonia (Catatonic Behavior)\n- Disorganized Speech\n- Lack of Facial Expressions\n- Inability to Regulate Emotions\n- Borderline Personality Disorder\n- Doctor: Checklist to Take To Your Doctor's Appointment\n- Lewy Body Dementia\n- Brief Psychotic Disorder\n- Schizophrenia FAQs\n- How To Reduce Your Medication Costs\n- Pharmacy Visit, How To Get The Most Out of Your Visit\n- Indications for Drugs: Approved vs. Non-approved\n- Drugs: The Most Common Medication Errors\n- Medication Disposal\n- Dangers of Mixing Medications\n- Drugs: Buying Prescription Drugs Online Safely\n- Generic Drugs, Are They as Good as Brand-Names?\nMedications & Supplements\nPrevention & Wellness\n- Strategies That Work to Help Prevent Suicides\n- Can the Anesthetic Ketamine Ease Suicidal Thoughts?\n- Antipsychotics Don't Ease Delirium in Hospitalized Patients\n- Antipsychotic Drugs Tied to Risk of Early Death in Parkinson's Patients\n- Psychosis Plus Pot a Bad Mix: Study\n- Too Few Psychiatric Patients Screened for Diabetes: Study\n- Are Too Many Young Americans Getting Antipsychotics for ADHD?\n- New Drug Shows Early Promise in Treating Parkinson's Psychosis\n- Bipolar Disorder Drugs May 'Tweak' Genes Affecting Brain\n- Prescription Meds Can Put on Unwanted Pounds\n- Antipsychotics in Pregnancy Risky for Newborns\nHealth Solutions From Our Sponsors\nReport Problems to the Food and Drug Administration\nYou are encouraged to report negative side effects of prescription drugs to the FDA. Visit the FDA MedWatch website or call 1-800-FDA-1088.\nFDA Prescribing Information""]"	['<urn:uuid:3d864fa2-5728-4e11-9f13-99d8f12d1a9b>', '<urn:uuid:f8a4be33-d236-425b-8994-fdff9b1822ab>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-13T06:04:36.674696	6	77	2048
20	What conditions cause irregular heart beats: bundle branch block or endocarditis?	Both conditions can cause irregular heart beats. Bundle branch block causes irregular heartbeats when electrical impulses take different paths and reach ventricles at different times. In endocarditis, as seen in the patient case, irregular heartbeats can occur through various arrhythmias including atrial fibrillation and heart blocks.	['In order to truly understand the concept of right bundle branch block, you will have to understand a few things about bundle branches. Our heart has an electrical pathway which is required to send electrical impulses to the heart’s ventricles. A component of this electrical pathway is the bundle branch. The branch actually divides into two namely the right bundle branch and the left bundle branch. The task of the bundles is to deliver the electrical impulses to the ventricles in order to contract them.\nWhat Is Right Bundle Branch Block?\nThe condition which causes either a hindrance or a delay along the electrical pathway is named the bundle branch block. This condition eventually results in irregular heartbeats. With bundle branch block, the electrical impulses are delivered by the left and right bundle branches at the same time with the same speed. However, when a block develops in either one of them, the electrical impulses have to take a slightly different path and this affects their speed. As a result, one impulse reaches one ventricle early while the other impulse reaches the other ventricle fractionally later. This results in irregular beating of the heart.\nRight bundle branch block, also known as RBBB, is the condition in which the conduction system of the heart doesn’t work properly. In RBBB, the right ventricle of the heart can’t get activated through impulses that travel from the right bundle branch. The left bundle branch delivers electrical impulses as required to activate the left ventricle. In order to activate the right ventricle in such a condition, the impulses travel from the left ventricle’s myocardium to the heart’s right ventricle. This is how right ventricle gets activated during RBBB.\nWhat Causes Right Bundle Branch Block?\nBundle branch block of either kind whether it be the right bundle branch block or the left bundle branch block can prove dangerous. There can be a wide variety of causes behind RBBB. They include:\nA congenital heart defect like the atrial septal defect (the name given to the hole in that wall which separates the heart’s upper chambers)\nMinor or major heart attack\nA heart muscle infection either viral or bacterial\nVery high blood pressure\nBlockage in an artery of the lung, medically called a pulmonary embolism\nHow to Diagnose Right Bundle Branch Block\nThe test that can diagnose an issue like bundle branch block is electrocardiogram. The electrocardiogram forms a record of your heart’s electrical impulses with the help of wires that are attached to your skin in and around the chest as well as other body parts during the testing. Any irregularity can point out the presence of not just bundle branch block but also whether it is right or left bundle branch block.\nIn order to categorize the bundle block on the electrocardiogram as RBBB, the test should show the following:\nThe rhythm of the heart must initiate at sinoatrial or atrioventricular node (on top of the ventricles) for the activation of the conduction system at the right time.\nThe duration or width of QRS (three graphical deflections on an electrocardiogram) must be over 100ms for an incomplete block and must be over 120 ms for a complete block.\nA Q wave is the first downward deflection of the complex.\nAn R wave is the first upward deflection of the complex. A noticeable terminal R wave should be present in lead V1 (like qR, rSR, R, rsR)\nS waves refer to any downward deflection after an R wave. A noticeable indistinct S wave should be present in lead V6 and in lead I.\nThe deflection of QRS complex should be opposite to that of the T wave. This discordance confirms bundle branch block. You can click HERE to get better understanding of diagnosing of RBBB with pictures.\nA Word About Treating Right Bundle Branch Block\nMajority of the people suffering from the condition have no symptoms and require no treatment. Treatment is only required in case the right or left branch block is being caused by an underlying heart disease. These treatments might include medicines to lower high blood pressure or a small surgery like coronary angioplasty that would open the artery that leads to your heart.', 'A 73-year-old female with a history of recent bio-prosthetic aortic valve replacement, stroke, hypertension, breast cancer status post-lumpectomy and radiotherapy, and hyperlipidemia was admitted with shortness of breath and tachycardia. Her blood pressure (BP) was 147/65 and her pulse was 105/minute. Respiratory rate was 18 breaths per minute, and temperature was 97.7°F; oxygen saturation was 100% on room air. She had regular heart sounds with a 2/6 systolic ejection murmur at the left sternal border. Her pertinent abnormal labs included sodium 122 mEq/L, troponin 0.16 ng/ml and brain natriuretic peptide 611 pg/ml.\nInitial electrocardiogram (ECG) revealed sinus tachycardia at 105 beats per minute with left ventricle hypertrophy (LVH) and repolarisation abnormality (see Figure 1). Her transthoracic echocardiography (TTE) confirmed bio-prosthetic aortic valve dehiscence with aortic regurgitation for which she underwent revision of aortic valve replacement. She had a prolonged hospital course complicated by atrial fibrillation, subdural haematoma and respiratory failure requiring intubation. During this hospitalisation she developed Aspergillus flavus endocarditis of the second bio-prosthetic aortic valve. This was adequately treated with amphotericin b and variconazole for 6 weeks, followed by lifelong suppressive therapy. She was successfully discharged to a skilled nursing facility after a long hospital stay.\nThe patient was re-admitted few weeks later with dyspnoea, oedema and hypoxia. Her repeat TTE showed 50% dehiscence of the bio-prosthetic aortic valve with vegetation. She developed complete heart block during her third bio-prosthetic aortic valve replacement and epicardial pacing leads were placed. She failed to wean off the ventilator and required tracheostomy and percutaneous endoscopic gastrostomy (PEG) tube placement. Her condition continued to deteriorate. Unfortunately, she did not recover from this severe illness and passed away while in the hospital.\nDuring her last stay in the hospital, her ECG revealed a very complex and interesting phenomenon. Her baseline ECG (see Figure 1) showed sinus rhythm (SR), heart rate 87/min, left ventricular hypertrophy with repolarisation abnormalities and normal PR, QRS and QTc intervals. Her baseline ECG transformed into sinus tachycardia with left bundle branch block (LBBB) pattern (see Figure 2). Atrioventricular (AV) pacemaker was inserted for complete heart block. The following day her ECG pattern showed SR, heart rate of 100/min with alternating LBBB and normal conduction (Figure 3; 2:1 LBBB pattern). She had three different patterns of ECG on the same day. Her fourth ECG shows development of atrial fibrillation with normal conduction. The fifth ECG was back in SR with prolonged QTc and first-degree heart block along with LBBB.\nAmong all the cardiac valve lesions, aortic valve pathology is the most common to be accompanied by various ECG abnormalities due to its close proximity to the AV node and bundle of His. ECG findings related to aortic valve endocarditis include various degrees of heart block, ST elevation, pericarditis like ECG changes and bradyarrythmia or tachyarrhythmia.1 Development of cardiac conduction abnormalities in patients with infective endocarditis is one of the most ominous signs and predicts high mortality. Regardless of causative organism, ECG abnormalities are more commonly seen in prosthetic valve endocarditis as compared with native valve and disseminated infections.2\nEven though Aspergillus can cause diseases in both healthy and immunocompromised populations, invasive Aspergillosis is usually seen in the latter. Our case was unique, as the patient did not have any apparent immunosuppressed state. Disseminated Aspergillosis has been reported to lead to various cardiac conditions, notably AV block, myocarditis, pericarditis, endocarditis, myocardial infarction, aortitis, intracardiac mass and pericardial tamponade. Aspergillus endocarditis is a fatal condition with poor prognosis despite treatment. Blood cultures rarely show Aspergillus. Our patient was among the rare ones who grew Aspergillus on blood specimen.\nA close view of our case demonstrated an unusual pattern of normal conduction alternating with LBBB or 2:1 LBBB which is referred to a normal QRS alternating with bundle branch block pattern QRS complex due to blockage of the isolated left bundle branch with constant PR, PP and RR intervals. Friedberg first described in 1949 the interesting phenomenon that was initially named as intermittent bundle branch block. Subsequently, more cases were reported associated with atherosclerotic heart disease. Fenichel et al. in 1977 were the first to document this ECG finding in Staphylococcus aureus endocarditis.3\nLeft bundle branch (LBB) usually gets depolarised either anterograde from sinus impulse (normal conduction pattern) and, if injured, then retrograde from the healthy right bundle branch via trans septal conduction (pattern seen in typical 1:1 LBBB). Anterograde conduction is blocked when sinus cycle length is less than the effective refractory period (ERP) of LBB and next impulse arrives before LBB is recovered from prior conduction. Now impulse is conducted via retrograde conduction to LBB and results in a wide LBBB pattern QRS complex. This is a typical pattern seen in 1:1 LBBB.\nThe plausible explanation for 2:1 LBBB is that the activation coming from the right bundle branch through the septum can also be blocked retrogradely over the left bundle branch when the sinus rate is fast enough for the activation to fall in the retrograde refractory period of the left bundle branch. This results in anterograde, as well as retrograde block in the left bundle branch and thus does not allow the resetting of the refractory periods (no linking phenomenon). Thus, the next sinus activation will find a non-refractory left bundle branch and LBBB will disappear (see Figure 3). We also observe 1:1 LBBB for slightly higher heart rates, which cannot be explained by the persistence of lack of retrograde conduction over the left bundle branch (even rather favoured by the higher heart rate) since this obligatorily implies a 2:1 LBBB pattern. Conversely, this is the decrease in His-Purkinje tissues refractory periods during elevated heart rates which can explain an enhancement of retrograde conduction and thus a new linking phenomenon.4\nFirst degree heart block can be easily explained with endocarditis-related vegetation and abscess formation around the AV node and it has been well documented in the literature. Appearance and disappearance of atrial fibrillation was most likely a cardiac ischaemia induced transient phenomenon.']	['<urn:uuid:918fddd8-604a-45f6-b431-b8bf6b89fa1e>', '<urn:uuid:b9c0a180-3b91-4092-b7de-7739f32372ec>']	factoid	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T06:04:36.674696	11	46	1692
21	natural light portrait photography window setup	For portrait photography using natural window light, position the subject at a 1-1.5 meters distance from the window at an angle. Place the camera close to the window with its axis perpendicular to it. Use a screen 1-1.5 meters from the subject to illuminate dark areas - the closer the screen, the more even the lighting and softer the shadows. Cover the lower part of the window to use only upper light, and if sunlight is too bright, cover the window with light white fabric.	['In a portrait shooting, it is important first of all to depict the individuality of the person being photographed. In making a portrait, the photographer should try to transmit his original and unique features, depict his character, and emotions and thoughts. To achieve this aim, various techniques and methods are used – changing the distance of the shooting, degree of definition, background, and lighting. Correct lighting is key in making a quality portrait.\n1. Standard frontal lighting from the camera. Used only in photographing documents. No artistic qualities.\n2. Use of one lighter with screen. The lamp is located in front of the model and a little to the side, and a screen is set on the other side. This is one of the simplest lighting methods. Despite its simplicity, or maybe because of it, this method makes it possible to achieve excellent results.\n3. The light source is located on both sides of the model. At an angle. Using such a setting of variously-located light sources, the portrait acquires more volume and expressiveness.\n4. Use of four lighters. One lighter is set at the level of the camera, a second is set higher at around three meters to a sharp angle to the model. One lighter emitting a narrow ray of light, is set for lighting the head of the person being photographed from behind and from the side. A fourth light source from behind illuminates the background. The use of four lighters located on different sides and from different angles makes it possible to achieve a very expressive combination of shadow and light.\n5. The lighters are aimed at the ceiling and at the wall, to which a white screen is attached. Lighting of the model takes place only against natural light, which makes it possible to obtain a soft depiction.\n6. During photographing in a room with minimum equipment, the use of natural light from a window is optimum. The object being photographed is situated at a certain angle to the side of the window at a distance of 1 to 1.5 meters. The camera is located as close as possible to the window so its axle was perpendicular to the window. It is necessary to illuminate the dark side of the individual using the screen set at a distance of 1 to 1.5 meters from the model. Distance of the screen is determined by lighting – the closer it is, the more even the lighting of the individual is, and lights and shades are softer. The lower part of the window should be closed in order to use only the upper light. If the sunlight brightly illuminates the individual, which is common on a sunny day, the window should be covered with a light white fabric.\n7. If there are two windows in the room located at an angle, one of them can be used as a main source of light, and the other window as a secondary source. Illumination of the model’s face can be made based on where he is in the room.\n8. Outdoors, it is best to avoid photographing under a bright sun, which can create deep sharp shadows that become almost black in the photograph. Therefore, the sun should be somewhat covered by clouds, so that the shadows are not as sharp. If it is impossible to avoiding a shoot in direct sunlight, it should be remembered that due to the difference in lighting, the part of the face in the shadow will appear almost black. To avoid this, a flash can be used, which will make it possible to even out the lighting.\n9. To even out lighting in a portrait shoot outdoors, sometimes are used a screen, which may be a piece of paper, or a white sheet. The screen should be set up facing the shadow direction or lower than the face of the individual. Another way to use a screen is to have it face open space, in such case, photographing takes places in its shadow. Improvised screens may be trees and structures in whose shadows a shooting can be made.\n10. In shooting a group portrait, all of the above-mentioned methods can be used. It should only be taken into account that the entire group should be equally illuminated so as not to have any light “holes” and to show all faces and details of clothing.']	['<urn:uuid:23977c54-ae5c-456d-ba97-591aa5811cfd>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T06:04:36.674696	6	85	730
22	What camera settings matter for good photos, and where should subjects be placed?	The key camera settings are ISO (light sensitivity), aperture (light through lens), and shutter speed (opening/closing speed). For subject placement, the Rule of Thirds states that subjects should be positioned on the one-third to two-thirds line rather than dead center, which creates more visually pleasing photos with better dimensionality.	"[""There are few things more annoying then arriving home from a great vacation to discover your vacation snaps (pictures) are not as great as the vacation. As photos are a great way to refresh and share memories, you naturally want the best photos you can possibly get on vacation. With today's digital cameras becoming the equivalent of our grandparents' Kodak Brownies, there is no reason that all of us can't capture moments we will delight in for years to come.\nPre-Vacation Snaps or Yes, You Have to Read Your Camera Manual\nThere it is. The one thing you have been avoiding since you got your camera is the most basic tool you have at your fingertips to help you get great shots. In our gosh-what-does-this-button-do approach to technology, most of us have disdain for giving our camera manual all but the most cursory of glances.\nRead the manual. Familiarize yourself with all the parts of your camera. If the print in the manual is small, go and see if the manufacturer has published it on-line. That way you can zoom in the text as large as you want. If your manual is relatively slim, pack it away in your camera bag or purse. If that is not feasible, then take some notes on a 3x5 card and stow it away for future reference.\nYour camera manual is your friend. You will find that your camera manufacturer actually wants you to succeed a taking the best pictures you can with your camera. Most manuals also offer a quick tips section to getting better shots.\nAlso, take a moment and clean your camera lens and viewfinder. You might also want to invest in a few in-expensive circular polarizer filters for your camera lens. With these snap on, different colored round plastic discs, you can immediately change the color contrast in your photos, including black and white photos. They also help to improve definition of whatever you it is you are snapping a photo.\nBreak the Repetitive Picture Cycle\nYes, you will want to take plenty of pictures of your traveling companions, but Cousin Harry and Aunt Maggie do not have to be in every shot. Moreover, when they are, make the shots interesting by thinking about composition of the picture.\n- Shots taken in natural light such as early morning and late afternoon are the best times to get shots of people, their faces, and their reactions to their environment.\n- Move to get a great vacation snap. Do not stand in the first place your feet touch the ground to capture that breathtaking waterfall or that imposing church against a cloud swept sky.\n- Try different angles and be careful of reflective light and using your flash on your camera. To be certain you are getting a good shot when reflective light might be an issue, take a shot without your flash and then one with your flash.\n- If you see something that interests you such as the colors in a beautiful rug or a field of flowers, then take the picture. This is your vacation and you want to remember local colors, objects, and people to capture the ambiance of the place you are visiting.\n- Look for moments that can add depth, color, and texture to your photos. Remember to try to change the angle of either the shot or the camera to get shots that are more interesting as well.\nThe Rule of Thirds\nThis rule has been around a long time and the reason is because it is a great rule and can help you improve vacation snaps in just a short time.The Rule of Thirds is basic - the actual subject of a photograph will appear better if the focal point of the camera is not set dead center on the subject. That is the subject should be on the one-third to two-thirds line of the photo through the viewfinder. Many digital cameras have a grid available to turn on or off and which will superimpose itself (invisible in the actual shot) over the photo in the viewfinder. You can use this grid if you like to help you line up your shots.\nUsing the Rule of Thirds, you will get more background in the actual shot, which is generally more visually pleasing and gives the photo a dimensional quality about it.\nTaking great vacation snaps should be part of every traveler's experience. With some practice, some experimentation and some study, you should find yourself taking photos as good as or better than any professional photographer can produce!"", 'Travel Photography – Telling Your Visual Story\nWhether you are dreaming of getting a travel photography job to take you around the world or just want to capture the most exciting moments of your upcoming trip, you should start with learning the theory of travel photography. And even if you think you are quite good at taking shots, travel photography still might catch you off guard since it’s literally all-encompassing and takes much more knowledge and experience to be mastered.\nIndeed, you never know which of your skills are to be put up to the test the next time – street photography, food photography, landscape photography, or any other. It’s only fair to say that travel photography is the pinnacle of the art of photo shooting. A single glance at a focused collection of wonderful travel photos would be enough to understand that it takes a bit more than just a good camera and sunny weather to create such masterpieces.\nNevertheless, mastering travel photography is no different than mastering any other art: just keep putting one foot in front of the other and soon you’ll make your road by walking, as the proverb says. Without further ado, we suggest you to make your first step right now by grasping the tips and tricks on travel photography.\n#1 Pick the Right Camera\nThe right camera in the right hands can do wonders, which is why first of all you should buy a solid device within your budget. How to pick one? The main criterion is the size of the image sensor: the bigger it is, the more light it can use to create an image, and the higher the quality of the shots. And one more thing: please don’t treat megapixels as a crucial factor. Contrary to popular belief, it has little to do with the quality of photographs, or at least not as much as most beginners tend to think. Here are a few reasonable choices depending on your budget and the sensor size you are willing to get:\n- Canon Powershot Elph 190 (~$105). The cheapest option in the list, this point-and-shoot camera from Panasonic has a 10x zoom and is as ergonomic as a smartphone. However, it’s not designed to withstand temperatures below 0C and higher than 40C, which means it can only be used in warm seasons.\n- GoPro HERO 7 Black ($330). Rugged and waterproof, GoPro HERO 7 Black is where the quality meets the price, though you are likely to use it only as an addition to the main camera. It takes 12MP photos, shoots 4K60 video, and has all the stabilization bells and whistles to ensure the highest possible quality of photos and footage.\n- iPhone X (starts at $999). iPhone X is not the cheapest option, but it weighs nothing compared to most cameras, fits the pocket, and delivers excellent quality with its superb f/1.8 aperture lens. In fact, an iPhone of one of the latest models may satisfy you down to the ground, especially if you already own it.\n- Nikon COOLPIX B700 ($499). If you’re going to shoot from a distance, the 60x zoom of this 20.2 MP camera will come in handy. Nikon COOLPIX B700 gives you sufficient control over your shots and can be considered as a one-size-fits-all option.\n- Canon EOS 6D (~$1000 for a used camera) and Canon EOS 6D Mark II ($1699). Both cameras take pictures of incredible quality, both are great in the dark, and both provide good autofocus and continuous shooting speed. Apart from the price, the difference is that Canon EOS 6D Mark II has all characteristics boosted and it enables timelapse movies, allowing you to record changes taking place over time (say, capture the whole day from dawn to sunset).\nNote that most mirrorless cameras go with low or medium-quality kit lenses and therefore it’s always a good idea to buy a lens according to your shooting needs and budget. A good lense, though, would cost you a pretty penny (as much as the camera itself at best). It should have a small aperture and high focal length so that you can zoom out and catch enough light for your pictures.\n#2 Don’t Forget About Photography Accessories\nOverlooking photography accessories might turn your journey into a nightmare. Depending on your plans, you may need:\n- Memory cards. A 64 GB memory card would be enough to take as many photos as you can physically shoot during the day plus you will have a few (most likely a few dozen) spare gigabytes for footage. The price for a 64 GB memory card starts at $10.\n- Storage for photographs. Whether it’s an external hard drive, USB flash drive, or cloud storage, it doesn’t matter, but please make sure you have enough place to upload the packs of photos you shoot daily. A few terabytes must be enough even if you take thousands of pictures a day.\n- Filters for lenses. If you go with an expensive lens, it’s worthwhile to spend a few dozen dollars on filters so that the lens won’t get damaged or scratched during the photo shooting in the field. Also, consider purchasing a polarizing filter to manage reflections, darken skies, suppress glare from the water surface, and basically, change the balance of light to benefit your photos.\n- Batteries and power banks. Running out of power in the middle of a photoshoot is the worst experience ever, so make sure to buy an additional battery and maybe a power bank, especially if you’re going to take more than a few hundred shots a day.\n- Tripod. Whether small or large, a tripod significantly expands your creative horizons. It might be Vanguard VEO 2 204AB for lightweight setups, an ultra-compact Sirui NT-1005X/E-10, or a giant Manfrotto 290 Dual MK290DUA3-BH – just imagine the locations you are going to conquer and try to figure out which tripod could be a win-win option to take versatile shots without sacrificing the comfort.\n- Insurance. Last but not least, consider getting an insurance cover for your camera if the latter costs a few thousand dollars. There are lots of companies offering insurance for photographers, including but not limited to Hill & Usher, Imaging Insurance, Professional Photographers of American Photo Care Insurance, Front Row Insurance, E & I Insurance, etc.\n#3 Preparation at Your Destination\nTo capture the moments in a way that you would love to relive them across the years, you should master how to use your camera in the manual mode by finding the best relationship between ISO (sensitivity of your camera to light), aperture (how much light goes through the lens), and shutter speed (how fast the shutter opens and closes). While that knowledge will definitely come to you with practice, arranging things at the place of destination within a limited time frame might be more challenging.\n- Research the locations. In addition to the research you’ve made at home using Instagram, Google, Pinterest, and social networks (you’ve made it, right?), it would be very helpful to scout the location the day before photo shooting. The better you do your ‘homework,’ the more net time you will have to take pictures instead of choosing positions, angles, and making other adjustments that should have already been done.\n- Get ready to wake up early. The hour after the dawn and the hour before the sunset are known as the best time to take pictures, and this is especially true for natural settings since city lights do not intervene. Warm morning light will breathe liveliness, romantics, and unusual charm into your shots, making them especially warm and dear.\n- Soak up the local culture. The information that you find on the web can acquaint you with all the features of the setting and maybe even convey the atmosphere, but still, you will have to earn the trust of the locals if you want to photo-shoot them. Don’t rush to take pictures – take your time to feel the spirit of the place you are in. Be polite with locals, smile, give gifts, buy souvenirs, and only after that start taking pictures if you are granted permission.\n- Do not book organized tours. It’s always a temptation to relax and book an organized tour where you can just follow the recommendation of your guide and serenely contemplate the beauty around you. Unfortunately, you are unlikely to take great pictures during organized tours for these are usually too crowded, as well as the time is limited. Organized tours might work only for small groups with good photography ethics.\nPhotography Composition Tips\n#4 Use leading lines to start your story\nTo get a great opening line in your photo story, you may take advantage of landscape forms, architecture, or anything else resembling leading lines – the natural lines that lead the viewer to the main object in the picture. These may be a path in the forest, railway rails, whatever – the gist is to slowly move the viewer up the picture to the climax, the main idea of the image.>\n#5 Follow the Rule of Thirds\nThe basic rule of composition is the rule of thirds, which refers to our instincts to focus attention on specific places in the picture. If to follow the science-based theory, you should break the image into nine equal imaginary parts and strive to place the main objects (the objects that you want to direct attention to) close or on the intersection of two lines – that is, not in the very center of the picture but rather to the right or to the left.\n#6 Use a Focal Point to Manage Viewers’ Attention\nAmong many approaches to photography, making stress on the focal point of the shot and blurring the rest is probably the most impressive, though it can be used only for a tiny portion of your photos. After mastering the focal point shooting, you can move ahead and play with the depth of the field by changing the aperture to control which parts on the image should stay in focus.\n#7 Take Voluminous Shots\nOne of the reasons why your photos might not fully convey the beauty of the moment is the lack of volume in them. But that is easy to deal with. Divide your picture into three sections – a foreground, a middle part, and a background – and fill each one with meaningful objects. See the example: both the wooden table on the foreground, the river in the middle part, and the mountains on the background draw attention, creating a feeling of being present at the pictured location.\n#8 Keep the main object in a frame\nIn addition to forming leading lines and creating volume, the peculiarities of the landscape can be used to ‘frame’ the main object in the shot and thereby add symmetry to the image and draw attention to where you want it. Branches, trees, bushes, buildings – whatever you have at your disposal – may be skillfully used for framing.\n#9 Bring Color Theory to Practice\nA piece of theory: there are three color orders – primary, secondary, tertiary – as well as three primary colors – red, yellow, and blue. By mixing two primary colors, we create a secondary color. By mixing a primary and a secondary color, we create a tertiary color. That’s it – the whole color palette works according to these rules while different combinations of colors affect our perception in a different way. Some colors excite us, some make us perceive the picture as a ‘warm’ or ‘cold’ one, and some colors are just bad for perception.\nThere are several types of color harmonies that work well for landscape shots, even though it’s almost impossible to achieve a perfect harmony of colors and you shouldn’t even strive for it. Just take into consideration the theory of primary, secondary, and tertiary colors to add a certain vibe to your pictures when you can do it.\n- Analogous – any two colors adjacent to each other on the color wheel (can be extended to any number of colors, though too many colors will hamper the harmony).\n- Complementary – any two colors lying opposite each other on the wheel.\n- Double complementary (quadratic) – a combination of two complementary pairs of colors.\n- Monochromatic – a single color with different saturation (for example, a black-and-white image).\n- Triadic – any equilateral triangle of three colors.\n- Diad – two colors on the distance of two hues from each other.\nGood Luck in Telling Your Visual Story\nJust like you invest time and effort in picking the place to visit, getting the tickets, and booking the hotel, invest time and effort in learning the basics of travel photography since you might not get a second chance to revisit the location of your dreams. Millions of stories remain untold by those thinking travel photography is easy and no different than taking any other shots, but you are not the one to fall into that trap. With what you’ve just learned, you have all the chance to tell your unique visual story, a story crying to be told.\n3 thoughts on “Tips for Taking Outstanding Travel Photography Shots”\nThanx For Sharing…..\nNice article! Very Helpful. Thanks for sharing. Keep inspiring us.\nInspiring article, thanks for the info. Peace!']"	['<urn:uuid:806eef0d-7032-47a9-815c-5c4930218c53>', '<urn:uuid:ee8c092a-d04f-41a0-8923-99acf835ec45>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T06:04:36.674696	13	49	2991
23	average days off work psychological injury claims 2015 16	In 2015-16, the average duration of psychological injury claims was 156.8 days, which represents the amount of time workers were off work due to their work-related psychological injury.	"[""Dr Quentin Mungomery, Psychiatrist and Medical Assessment Tribunal Member is providing general practitioners with tips on completing the Work capacity certificate – workers' compensation when a patient presents with a work related psychological injury. This webcast provides suggestions as to what meaningful information to include in the key medical assessment sections within the Work capacity certificate. Understand how this information is utilised by insurers and the patient's employer to assist them to recover at work as part of their rehabilitation.\nDownload a copy of this film (ZIP/MP4, 88MB)\nWelcome to today's webcast for Queensland GPs, proudly brought to you by the Office of Industrial Relations. The focus of today's webcast is to support and provide you with practical assistance on preparing a treatment plan to include capacity for work and functional ability when managing a patient with a work-related psychological injury.\nThe Office of Industrial Relations is committed to delivering short and sharp education to better support Queensland doctors working within the workers' compensation scheme.\nThe Office of Industrial Relations prepares and correlates all the workers' compensation scheme data for Queensland from each of the 29 insurers. The data shows that an average psychological and psychiatric claim can cost approximately $50,000 and this is continuing to slightly increase with more time-lost claims finalised. This is much higher than any other injury type claim. For example, a fracture claim or a strain and sprain for a back are between $10,000 and $30,000.\nThe average duration of these claims in 2015-16, were 156.8 days. This is the amount of days that they were off due to their work-related psychological injury. The OIR are committed to reducing the time off for psychological injury claims. To support this, the OIR welcomes Dr. Quentin Mungomery, psychiatrist and member of the Medical Assessment Tribunal. To share with you some tips on both the treatment plan, relating to capacity for work, and the functional ability for your patients who present with a work-related psychological injury. Thank you, Dr. Mungomery.\nHello, this is Quentin Mungomery and thank you for taking time to listen to this webinar.\nIn this webinar, we will be providing additional advice and suggestions to assist with completion of the new work capacity certificate, which was introduced in July of 2016. This is the second of two webinars on this topic with our previous presentation focusing on the diagnosis and mechanism of work-related psychological injuries.\nToday we will be focusing on how to complete the remaining sections of the work capacity certificate, including treatment plan, capacity for work, functional ability, and return to work rehabilitation planning. Reduced capacity to work due to psychiatric illness is not always understandable or clearly observable to others.\nWorkers' rehabilitation can benefit significantly from an increased knowledge and understanding of the impact of psychiatric illnesses on their capacity to rehabilitate and return to work. Having a clear understanding of the potential impact of such injuries on their capacity to work also assists with the timely provision of an individualised treatment and return to work plan and increases the likelihood of a positive outcome.\nThe nature of work-related psychological injuries often requires consideration of an integrated treatment approach, such as the bio, psycho, social model. This model take a broad view that allows attribution of psychiatric illness outcomes to the intricate and variable interaction of a range of factors, including biological factors, such as genetics, comorbid medical conditions, and impact of drugs and alcohol, psychological factors, including premier personality factors and any associated behavioural issues, and various cultural, familial, and social factors.\nConsideration of these issues assists with the formulation of an individualised treatment plan to meet the specific treatment and rehabilitation needs of the injured worker, and can include appropriate medication strategies, psychological counselling, and referral for special psychiatric care, and specific pathological and drug and alcohol testing as required. Considering whether an injured worker's medication may impair their capacity to work or travel, can be important as some psychotropic and opioid medications can cause sedative and cognitive issues, especially those with antihistamine or anticholinergic effects. Comorbid drug and alcohol issues can also effect work capacity, with consideration of therapeutic strategies to encourage reduction or abstinence from these substances increasing the likelihood of successful engagement and return to work activities. Considering how long an injured worker may require treatment based upon the severity of their work-related psychiatric injury, also assists with appropriate treatment planning.\nWhile some work-related psychiatric injuries go on to become chronic and may require more extended treatment, it is more common for injured workers to achieve significant recovery once they have had a period of appropriate treatment and the precipitating work-related stressor or injury has resolved or stabilised.\nThis section of the work capacity certificate asks us to consider the impact of the psychiatric or psychological condition on the injured worker's capacity to return to work and functional ability. Common mental disorders can affect a worker's ability to undertake their work carefully and sustain focus on mentally demanding tasks, and interact appropriately with customers and work colleagues. Workers often experience problems with lack of motivation and getting started in the morning as well.\nThey can also experience problems with feeling anxious prior to work, with associated avoidance, and have difficulties handling workload due to mental fatigue. Impairment and work function often crosses over a number of areas of work functioning, causing increased problems with general areas and difficulties with more complex motor skills and work speed and productivity. Capacity for work can also be impaired by medication and any ongoing illicit drug or alcohol issues.\nCognitive complaints are common in more severe psychiatric conditions, especially in major depressive illnesses. The most commonly reported problems are diminished ability to think or concentrate, which can lead to indecisiveness. Other core symptoms can include problems with psycho-motor retardation or agitation and motivation, mental and physical fatigue, insomnia and mood disturbances, which can also cause cognitive difficulties.\nMajor psychiatric illnesses have been shown to cause more specific deficits identifiable on testing in areas of attention, verbal and nonverbal learning, short-term and working memory, visual and auditory processing, problem solving and processing speed, and motor functioning. All of which can potentially affect the worker's functional capacity and may need to be considered when undertaking return to work planning.\nIf cognitive issues become a significant ongoing issue, consideration of other medical causes is suggested. The injured worker may also benefit from baseline and serial cognitive testing using instruments such as the mini-mental state examination or MoCA. And if more marked deficits are identified, consideration for more formal psychometric testing by a clinical psychologist.\nAnxiety disorders such as post-traumatic stress disorder can also cause similar cognitive deficits but often also present with additional challenges when considering returning to work planning. If you'd had problems with fear avoidance of certain work environments, especially if the index work-related injury was of a more traumatic nature. This may require the provision of suitable duties in an area away from the index traumatic event or the opportunity to undertake host employment in another work environment, in the initial stages of returning to work.\nGraded exposure activities to the feared workplace environment or situation, with the support of a clinical psychologist, can greatly assist with overcoming any phobic avoidance of this nature.\nProvision of a return to work rehabilitation plan at times can seem a somewhat daunting prospect, especially when attempting to assist a worker who has a combination of both psychiatric and physical injuries.\nTaking the opportunity to liaise with the worker's treating psychologist and psychiatrist however, can greatly assist with the formulation of such plans. Identifying tasks that can contribute to emotional refuelling at work, that the worker finds pleasurable or gives them a sense of competence and capacity, can also enhance their success of rehabilitation.\nRehabilitation programmes can generally be tailored to accommodate the impairment associated with most work-related psychiatric injuries, once a degree of stability and improvement has been achieved and can also facilitate their further recovery. In closing then, the provision of relevant clinical information in the work capacity certificate, is of great assistance in facilitating the injured worker's engagement in treatment and rehabilitation activities.\nTimely and accurate completion of the work capacity certificate greatly assists with the coordination of treatment and rehabilitation of the injured worker, towards the goal of returning to work. And understanding of the potential impact of work-related psychiatric injuries on the injured worker's capacity to work will also assist with the provision of an individualised treatment and return to work plan, and increases the likelihood of a positive outcome.\nWith help, practitioner's completion of the work capacity certificate, playing a pivotal role in achieving these goals. Thank you again for taking time to listen to this webinar and hope that the information provided will be of assistance in completing the work capacity certificate for injured workers.\nThank you Dr. Mungomery. And thank you for listening to this webcast for Queensland GPs, presented to you by the Office of Industrial Relations. For more information about workers' compensation services, please visit our dedicated medical practitioner area on our website.\nPlease look out for future webcasts from the Office of Industrial Relations.""]"	['<urn:uuid:e52d104f-a64d-4e9d-a5a7-9fe8207870d4>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T06:04:36.674696	9	28	1530
24	What is the most common injury area in jiu-jitsu competitions?	The elbow is the most frequently injured area in competition, accounting for roughly 20% of the injuries seen.	['I have been working with International Brazilian Jiu Jitsu Federation for the last 14 years covering tournaments in the entire Western region of the United States. I have been privileged to have the opportunity, and over the years, I have seen a lot of injuries. My day is only busy when someone gets hurt or has been hurt and they are looking for information on how to manage their problem are. One of the most common areas that I get questions about is the elbow. It’s also the area of the body that is most frequently injured in competition (roughly 20% of the injuries we see). But the injuries are not always directly related to competition but also to training. This article will discuss elbow injuries, how to manage them, and how to prevent them.\nAn armbar, or armlock, is one of the most basic jiu-jitsu maneuvers, and usually one of the first submissions taught to you in class. It’s a submission attempt whereby the opponent wraps their legs around your armpit and neck from the front side while holding your arm between their legs in a hyperextended position. The lever arm is increased by grabbing the wrist and driving the hips up toward the ceiling. It can cause pain at any part of the elbow, but most commonly the pain is the medial (inside) part of the elbow.\nWith the nature of the sport, gripping is a necessity to performance. Many successful jiu-jitsu practitioners discuss the importance of grip strength. Having good strength here may prevent your opponent or training partner from getting away from you. Gripping requires strength in your forearm and wrist flexors (muscles of the inside of the elbow), but also requires your wrist extensors (muscles on the outside of the elbow). It is also a normal human activity in which we grasp, push, and pull with for everyday undertakings.\nThe bone of the elbow consist of the humerus (upper arm bone) and the radius and ulna (forearm bones). These three bones also make up three joints in the elbow. All of these bones are considered long bones which means they can be used to provide leverage in the case when you get caught in an armlock. The motion that happens among these bones can be significant enough to affect all the way down into your wrist.\nThe ligaments hold the bones together along with the joint capsule. These two structures often blend together seamlessly. These structures are known as static restraints, meaning that they provide stability when no motion is happening. The ligaments of the elbow are primarily on the inside and outside of the elbow. On the outside, you have the radial collateral ligament (RCL) and the annular ligament. This annular ligament wraps around the radial head to allow you to turn your palm up and down like you are waving. The RCL provides stability to any forces that would push laterally (outwardly). The biggest ligament, and arguably the most important, is the ulnar collateral ligament (UCL). This ligament connects the humerus and ulna together and is very broad and long when compared to the RCL. It prevents force moving medially. This is the area of the elbow that is particularly vulnerable to armlocks and even Americana armlocks.\nWhen armlocks occurs, it can also damage the muscular complex around the elbow. Flexion and extension (bending and extending) of the elbow after an armlock is often quite painful if damage happens but can also affect pronation and supination (turning palm down and palm up). When there is damage to the muscle or tendon complex, it can generate a muscle strain/pull or tendinopathy. These muscles create and can control motion at the elbow. These are known as dynamic restraints which means they provide stability when motion is occurring. When an armlock is applied, the muscle that are affected will likely be the biceps brachii, pronator teres, or other wrist flexors. This can also be dependent on the hand position and what is going on at the shoulder during the attack. For instance, if you are placed in a position where you shoulder is also extended behind you, a large amount of tension is placed through the long head of the biceps at the shoulder as it is being tensioned down at the elbow by the attack (shown right). Gripping activities or wrist extension will affect the extensor carpi radialis longus and brevis and extensor digitorum (wrist extensors). These are affected by overtraining or profound changes in your training schedule.\nMechanism of Injury\nThere are two primary mechanisms for elbow injuries in jiu-jitsu: the armlock and tendinopathy injuries due to unaccustomed activities/overtraining. Besides direct trauma, the tendon injuries happen from multiple causes. In men, it is frequent to see tendon injuries at the lateral elbow which is commonly referred to as “tennis elbow.” This can be due to muscle tightness in the elbow, wrist or hand, a recent change in training schedule, work ergonomics, or typing on a computer.\nWith the armlock, if your opponent is able to take the arm into the position of elbow extension and your thumb facing straight upward, then there is a risk of damage to the elbow joint capsule and elbow flexors (biceps brachii, brachialis, and brachioradialis). If your opponent moves your elbow into extension and then turns your palm upward, it makes it harder to finish the submission but does stress the medial (inside) part of the elbow. Then there is risk of damage at the UCL and pronator teres muscle. Lastly, if your opponent moves your elbow into extension and then turns your palm down, it also is harder to finish the submission and stresses the lateral (outside) part of the elbow. Then there is risk of damage at the RCL and lateral elbow muscles.\nWith such a submission, and the long lever force that is generated, there is an inherent potential for both fractures and dislocations. A dislocation can be more obvious because of the large deformity that usually exists, while a fracture can be less obvious. The only way you will know a fracture is to seek a physician consult for imaging study. The rule of thumb for fractures is when in doubt, go get it looked at. For both of these situations, medical intervention should be received for proper injury management to be certain there is no nerve damage or laxity (increased motion) into the elbow joint.\nIf the serious injuries, like dislocation and fracture, have been ruled out, then the injury can likely be managed conservatively. The worst thing you can do is ignore the injury. Most of these elbow injuries can be managed conservatively with four simple steps. It is best to implement these steps immediately and progressively, and if your problem exists without improvement beyond 7-10 days, then seek medical attention or your nearest physical therapy specialist for a thorough assessment. This may include assessing the pin levels and even using wearable body sensors.\n1. Control the pain and inflammation – Training should be halted here, or significantly modified, until these are under control. This can be achieved simply by applying ice to the area. Inflammation control is important because it creates muscle inhibition (makes muscles harder to function appropriately). Although inflammation is a necessary role for the body to heal correctly, excessive amounts are thought to prolong the recovery time. So early intervention is the best intervention here!! With ice, you can apply it to the injured area for 15-20 minutes only. Any longer than that and you risk making the inflammation worst. (This is a shout out to everyone who keeps ice on for an hour. Don’t do it!) If the elbow injury is severe enough, using a sling or elbow strap may be necessary for a short time to remove additional stresses to the injured elbow to reduce the pain and inflammation.\n2. Restore range of motion and increase muscle activation – Training should still be halted or modified here, but you should be attending class and work on the psychological part of your training. Mindfulness and meditation can be extremely beneficial here. It will also build team camaraderie. Range of motion can be achieved by simple range of motion movements by flexing and extending the elbow and also turning your palm up and down at the wrist repeatedly throughout the day. You do want to work within your comfort zone but know that bending and extending will be the most uncomfortable to start. So, go slow! If the movements produce big spikes in pain again, then you are likely going too far and need to back off. As your range of motion improves, then using a light hand weight or light resistance band to perform the same motions will work well. Proprioception, or joint position sense, is also imperative to retrain at this point. This can be achieved by assuming a push up position and holding for thirty seconds to one minute. You can take some of the load away by going onto your knees or doing this push up position on the wall. If it is painful, then it is likely too soon to do this. This needs to be done repeatedly throughout the day, working up to one to two minutes at a time.\nFor those that have lateral elbow pain with gripping, stretching the wrist flexors and hand muscles are very important. The wrist flexors get tight from the activities we do daily plus grabbing in jiu-jitsu. Wrist flexor stretches can be done by simply grasping your fingers with your opposite hand and pulling back while your elbow is straight. Another stretch that works great for this is called a lumbrical stretch. First make a hook grip with your fingers, use your opposite hand to extend your fingers over your big knuckles. You should feel this stretching into the palm of your hand. Hold for at least 30 seconds.\n3. Increase your strength and endurance – Training can start here but still be careful with who you train with and make them aware of your injury. The elbow muscles need to function appropriately to provide muscular stability and control to the injured elbow. If there is still inflammation, this can inhibit muscles from working, and then they must be retrained. Or they may not reactivate. Running can offer cardiovascular benefits, but muscular endurance of the elbow can be trained with loading activities (push-ups, hand walking, and pull-ups to name a few). Strengthening of the elbow can be achieved by progressing the resistance bands and hand weights to improve elbow flexion and extension strength and endurance. Don’t forget about pronation and supination. This can also be achieved by doing more big movement activities like push up variations which require more force production. But remember, never sacrifice QUALITY of movement for quantity. We often will recommend repeated movements for time rather than a number of reps. This way the focus is on the quality. Start slow and controlled and progress to faster but always use correct technique.\nFor those that have lateral elbow pain with gripping, strengthening the wrist extensors with resistance bands and hand weights work well. Usually doing reps of 15-20 of #1-5 bands works well to address both strength and endurance of the lateral elbow muscles. You can also use a dowel with a weighted rope to address the strength in this area. And if you have one available, doing battle ropes up and down work great for the entire shoulder complex and wrist extensors with an overhand grip. Do this for 30 seconds to one minute and you will feel these muscles working.\n4. Increase your functional performance – At this point, you should be feeling great and have already been back to rolling and training. The light is at the end of the tunnel, but you are not quite there yet! This stage is the longest and the easiest for people to stop because they are feeling much better. You need to continue to address overall strength with complex movement activities like plyo push-ups, burpees, and plyo-ball exercises. You can progress from two arm activities to one handed loading activities, which will also challenge your core muscles. It is very important to continue working the proprioceptive of training with faster movements on your hands and dynamic stabilization activities. The more stable you are, the more efficient you will move.\nA delay in treatment often leads to extended healing times and lost time with training and rolling. So get treatment right away to achieve the best outcome. If your injury does not progressively improve, you will need to seek intervention consult with a sports/orthopedic physician and/or physical therapist to properly rehabilitate this injury. Improper management of your injury will increase your risk of future injury. So finding a specialist to tend to your injury is very important in the long run. The specialist will be able to work with you and guide you toward the best outcome to match your goals.\nRisk Reduction – Prevention\nWe know that the number one predictor for injury in jiu-jitsu is a previous injury. As mentioned previously, the ligaments and capsule are static restraints and the muscles and tendons are dynamic restraints. When both function as\nrequired, good stability, motion, and strength at the joint can occur. However, if one or all of these structures are damaged in some way or do not function properly, then there is an increased risk to the joint and therefore for that athlete. Ligament laxity/damage can lead to an impaired joint motion, whether if it moves too much or not enough. This is where strength training and proprioceptive training are very important to continue. The same elements described in “Management” can be applied, particularly steps three and four. If you implement these into your training regimen, then you should see a significant reduction in your own risk of injury and less down time when you do get injured. Also, seeing a movement specialist to identify potential risk factors can be even more beneficial to look for these and then address them.\nWhen it comes to jiu-jitsu, elbow injuries are the most commonly seen in competition. This information cannot necessarily be extrapolated to everyone who trains, but the rate is higher than everywhere else in the body. After an elbow injury, it’s possible to get back quickly by incorporating early intervention strategies for the injury, which often leads to better and faster outcomes. Strength training to regain muscle function and proprioceptive training to controlthe elbow are necessary for recovery. If not, you increase your risk of injury. When exercising, always stay in control. Start slow and progress to quicker movements. Remember that recovery is also an important part of functional performance so listen to your body. If you are feeling tired, then take a rest recovery day. And if your elbow injury is not progressing as it should, seek consult from your physical therapy specialist and/or physician to help get you on track and back to the mats as fast as possible. He/she will be able to give you quick recommendations and intervention after a thorough assessment to get you back quickly. Stay healthy… and see you on the mats!!!']	['<urn:uuid:03296fa5-fd5c-4818-838a-553219c9849a>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	10	18	2537
25	What role did music and warfare play in Catalan castles?	Catalan castles combined military and musical traditions. They served as military fortifications, as seen in places like Miravet Castle, which was the Templars' last stronghold in Catalonia. These castles also had significant musical connections, as demonstrated by Cardona Castle's Roman Collegiate Church, which hosted important musical recordings. This dual nature of war and culture continues today, with many castles hosting both historical war reenactments, like the siege of Miravet performance, and cultural events including concerts.	"['Joan Cererols: Missa Pro Defunctis, Missa de Batalla. La Capella Reial, Jordi Savall. Audivis-Astrée E8704 (1988) barcode 3298490087046\nReissue Astrée Naïve ES 9924 (1999) barcode 3298490099247 see entry on Discogs.com (link will open a new tab)\nCompiled in “La Capella Reial de Catalunya 25th Anniversary” (+ El Cançoner del Duc de Calábria, Mateu Fletxa Ensalasas, Bartomeu Cárceres Villancicos & Ensaladas), 4 SACDs Aliavox AVSA 9897 (2013) barcode 7619986398976\nRecorded November 1987 in the Roman Collegiate Church of the Cardona Castle, Catalonia.\nThe joys of war come as a welcome relief from the mourning of death\n17 November 2020\nAs befits the subject, Cererol’s Requiem is meditative, solemn and slow-moving. It is choral rather than soloistic, written for two choirs in dialog in the Italian Renaissance style, underpinned by brass, but the chorus here is attributed to solo voices, madrigal-like, which commendably lightens the textures, and the soloists include, of course, the stellar Montserrat Figueras, but also Maria Cristina Kiehr and Josep Cabré. It is interrupted by – and dialoguing with passages in plainchant sung by male chorus. The style is typical Renaissance transiting to early baroque: the Spaniard Joan Cererols, 1618-1680, a monk who spent much of his time at the Escolania (choir school) of Montserrat, was a contemporary of Cavalli (1602-1676), Carissimi (1605-1669), Cesti (1623-1669), and just to show that, no, I’m not chosing composers starting with a C on purpose, Schmelzer (1620 or 1623 – 1680). Much of what Savall and team play is conjecture – if I understand the liner notes, the scores come without instrumental accompaniment or with a simple continuo duplicating the vocal bass line, but it is known that the Monastery of Montserrat could dispose of many instrumentists, and Savall has conjectured that the scores are to be understood as mere sketches, each performing ensemble being free to arrange an accompaniment according to what instrumental resources they had at their disposal.\nGiven the constantly meditative and solemn atmosphere of the Missa Pro Defunctis, a touch of boredom may set in if you are not in the right mood, and the Missa de Batalla, with its radiant polyphony (three 4-part choirs) and upbeat mood, comes as a welcome contrast and a ray of light, with brass joined by an ensemble of viols. Astrée and Savall should have put the program in reverse order, really. In general, first comes the war (and the general enthusiasm when it is declared), then the Requiem.\nSavall’s recording, when it was published in 1988, wasn’t exactly a rediscovery of Cererols. The distant heirs of the composer, Escolania & Capella de Música Montserrat under Ireneu Segarra, had been recording the music of Cererols since the late 1960s, first on the label Schwann, and committed both works to LP in 1978 and 1979 for Deutsche Harmonia Mundi, both compiled on a CD from 1988 on DHM’s mid-price series Editio Classica, CDM 7 69280 2, barcode 077776928029. But Savall sent back Segarra, with his funeral tempi even in Messa de Batalla, thick textures and sometimes precarious children’s voices, to prehistory.\nExcellent and informative liner notes, heart-wrenching when the described how little has survived of the early manuscripts and other documentation that were once held at the Monastery of Montserrat, and destroyed during the Napoleonic wars.\nWe’ll never know if this is “authentic” Cererols as he may have been sung in Montserrat 350 years ago, but this is the Cererols we got, and it is as good a conjectural reconstruction as I can imagine. TT 69 minutes.\nPS I had the curiosity to check on Wikipedia what and where was the “Cardona Castle” where the recording was made in 1987. Wow.', 'Castles in Catalonia: buildings that come to life\nFlorejacs Castle (Lleida)\nThe Templar Castle of Miravet\nTamarit Castle, in Tarragona\nThere are many ways to visit and discover Catalonia. Here we suggest visiting its medieval castles, to be found throughout this Region in northeastern Spain, from the Pyrenees to the Mediterranean Sea. There are many different visits and routes, some of which are guided. Here we suggest two of them: the Lleida Castles Route, and the Route of the Templars, in Tarragona. Take note.\nIn the Middle Ages, feudal lords, beautiful maidens and valiant soldiers populated castles which, centuries later, now open their gates and portcullises to you. In Catalonia there are some 600 medieval fortresses, many of which are open for visits all year round.\nIf you explore the castles of Lleida, a province known as the ""Castile of Catalonia” for its prolific number of fortifications, you will find Montsonís, Montclar, Florejacs, Pallargues, Vicfred and Mur: castles that make up this guided tour. On these visits you will learn about everyday life and customs in the Middle Ages, the history of the castles themselves, and anecdotes related with them. But there is more: if you like, you can also take part in cultural activities that go from concerts to courses and workshops, conferences, and medieval and gastronomic events. You can also stay at some of these stone castles, now converted into luxury hotels or restaurants. At the tourist offices you will find detailed information on opening times, prices and guided tours.\nTarragona, land of the Templars\nFurther south, in Tarragona, you can go on the Route of the Templars. This was a military order created to protect pilgrims on their way to the Holy Land in the Middle Ages, and they left behind a major legacy in this part of Spain. Visit Miravet, a small village on the banks of the Ebro River, whose stunning castle dominates the surrounding landscape. The castle was the last stronghold of the Templar knights in Catalonia in the 14th century, and is one of the best examples of Medieval Catalonian military architecture. Explore its rooms and walls, its towers and watchtowers, and let your imagination travel back to a time of kings and warriors. Head up to its highest point, where you can enjoy stunning views. In August, this castle is the stage for a theatre production, The seige of Miravet, which recreates the legendary eight-month seige that the Templars suffered before finally surrendering to king Jaime II.\nLess than 40 kilometres away, in Tortosa, there is another Templar fortress, now a Parador Hotel: La Suda Castle. From here there are views over the city and the beautiful surroundings of the Ebro River. Make the most of the occasion to explore its old town and see the Cathedral and the Episcopal Palace buildings. There are many more castles to be found the length of the Costa Dorada coast. One is in Ulldecona, an excellent example of military architecture; another in Tamarit, built on an outcrop jutting out into the sea; also in Altafulla, or in Torredembarra, which has a beautiful 16th-century Renaissance fortification.\nIs deze informatie nuttig voor je geweest?']"	['<urn:uuid:c9cb23dc-ec4d-406a-9bb7-c174a409adf8>', '<urn:uuid:62a6baf7-b403-4517-944b-6c90213be3bc>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T06:04:36.674696	10	75	1134
26	What role did calligraphy play in Ottoman-Malay relations, and how did Muslims view artistic expression?	Malay calligraphers like Ibrahim al-Khulusi taught the 'Istanbul style' of writing in Mecca, demonstrating cultural exchange between regions. In Muslim artistic tradition, calligraphy was highly valued as decorative art for buildings, ceramics, and books, particularly since the portrayal of God or human figures in religious art was forbidden.	"[""From Anatolia to Aceh: Ottomans, Turks and Southeast Asia\nThe British Academy-funded research project Islam, Trade and Politics across the Indian Ocean, which ran from 2009 to 2012 adminstered by ASEASUK (Association of Southeast Asian Studies in the UK) and the BIAA (British Institute of Archaeology at Ankara), set out to investigate all aspects of links between the greatest Middle Eastern power – the Ottoman empire – and the Muslim lands of the Malay archipelago in Southeast Asia over the past five centuries. The project culminated in a conference held in Banda Aceh in 2012, as well as a travelling photographic exhibition produced by the British Library which toured the UK, with a Turkish version which travelled to Istanbul and Ankara, while Indonesian versions were displayed in various venues in Aceh and in Jakarta at the Bayt al-Qur'an & Museum Istiqlal. Now one of two books arising from the project has just been published – From Anatolia to Aceh: Ottomans, Turks and Southeast Asia, edited by Andrew Peacock and Annabel Teh Gallop – as the auspiciously-numbered 200th volume in the series 'Proceedings of the British Academy', published by Oxford University Press.\nThe first direct political contact between Anatolia and the Malay world took place in the 16th century, when Ottoman records confirm that gunners and gunsmiths were sent to Aceh in Sumatra to help fight against Portuguese domination of the pepper trade across the Indian Ocean. In later years the main conduit for contact was the annual Hajj pilgrimage, and many Malay pilgrims from Southeast Asia spent long periods of study in the holy cities of Mecca and Medina, which were under Ottoman control from 1517 until the early 20th century. During the era of European colonial expansion in the 19th century, once again Malay states turned to Istanbul for help. It now appears that these demands for intervention from Southeast Asia may even have played an important role in the development of the Ottoman policy of Pan-Islamism, positioning the Ottoman emperor as Caliph and leader of Muslims worldwide and promoting Muslim solidarity.\nThe 14 papers in this volume represent the first attempt to bring together research on all aspects of the relationship between the Ottoman world and Southeast Asia, much of it based on documents newly discovered in archives in Istanbul. The book is presented in three sections, covering the political and economic relationship, interactions in the colonial era, and cultural and intellectual influences, with an introduction by the editors and a historiographical survey by Anthony Reid, whose seminal 1969 article, ‘Sixteenth century Turkish influence in western Indonesia’ (Journal of Southeast Asian History, 10 (3): 395-414), could be seen as the starting point for modern research on this topic. A full list of the contents of the volume can be found here: Download 00_Anatolia to Aceh_i-xvi.\nThe beautiful manuscript which adorns the front cover of the volume exemplifies well the myriad interactions documented in the volume: it was copied in Mecca by a Malay calligrapher from the 'Jawi' community of Southeast Asians resident in the Hijaz, and adorned with late Ottoman-style illumination. It is a copy of the Mawlid sharaf al-anam, ‘The birth of the noblest of mankind’, an anonymous compilation of devotional prayers on the Prophet, and the Arabic text is accompanied by a small interlinear translation in Malay. The scribe is named as Ibrahim al-Khulusi ibn Wudd al-Jawi al-Sambawi, his nisba indicating his origins on the island of Sumbawa in eastern Indonesia.\nAlthough the colophon gives the date (in thinly-inked numerals) of 1042 AH (1632/3 AD), this is most likely erroneous and numerous factors indicate that the manuscript was probably copied in the mid-19th century. By coincidence, among the documents in the Prime Ministry Ottoman Archives ((Başbakanlık Osmanlı Arşivi, BOA) in Istanbul relating to Southeast Asia discovered in the course of the research project was an Arabic letter of 1849/50 to Hasib Pasha, the Ottoman Governor of the Hijaz, thanking him for facilitating the Hajj pilgrimage, signed by ten Malay and Yemeni scholars (‘ulama) resident in Mecca. Among the signatories to this letter is one ‘Ibrahim bin Wudd al-Jawi’, whose seal impression reads Ibrahim al-Khulusi ibn Wuddin. Comparing the name ‘Ibrahim’ in the manuscript and the letter leaves little doubt that they were written by the same person.\nLetter in Arabic from Southeast Asian religious scholars in Mecca to Hasib Pasha, the Ottoman Governor of the Hijaz, 1849/50, with Ibrahim's signature and seal fourth from the left. BOA İ.DH 211/12286.\nDetail of the signature of Ibrahim (left) in the letter of 1849/50, and (right) in the colophon of Mawlid sharaf al-anam, with the same concave-convex shape of ba-ra, and the letter alif bisecting the ha-ya ligature in both examples.\nFurther evidence locating this ‘Ibrahim’ as a master Malay calligrapher in the Hijaz in the mid 19th century is found in a letter in Malay and Arabic written in Mecca in 1866 by Abdul Rahman bin Muhammad Saman of Kelantan to Sultan Abdul Hamid of Pontianak on the west coast of Borneo (Islamic Arts Museum Malaysia, 1998.1.3680). In the letter, Abdul Rahman states that he had come to Mecca to study the ‘Istanbul style’ of writing (menyurat Istanbul), and that he is currently being considered the successor to his ‘late teacher Shaykh Ibrahim al-Khulusi al-Sanbawi’ in teaching ‘Istanbul writing’ (patik ini … sudah dilatih? orang besar2 di Mekah akan bahwa patik inilah jadi ganti … al-marhum guru patik tuan Syaikh Ibrahim al-Khulusi al-Sanbawi yang masyhur itu … pada pihak tolong mengajarkan segala muslimin menyurat Istanbulnya). Together, these sources suggest that Ibrahim al-Khulusi died in Mecca probably sometime in the early 1860s.\nDetail from a letter written by Abdul Rahman of Kelantan in Mecca in 1866 giving the name of 'my late teacher the great Shaykh Ibrahim al-Khulusi al-Sanbawi' (al-marhum guru patik Tuan Syaikh Ibrahim al-Khulusi al-Sanbawi yang masyhur). Reproduced courtesy of the Islamic Arts Museum Malaysia, 1998.1.3680.\nIronically, the Ottoman-Malay mawlid manuscript of Ibrahim al-Khulusi only came to light in 2014, well after the completion of the research project, and so is not discussed in the book itself. (The manuscript is currently held in the Islamic Arts Museum Malaysia in Kuala Lumpur, and we are most grateful to the IAMM and its Director, Mr Syed Mohamad Albukhary, for permission to reproduce the manuscript on the front cover of the book). But it is just such a discovery as this which raises hopes that, from time to time, yet more new evidence will emerge of the connections between the Ottoman empire and Southeast Asia, across the Indian Ocean from Anatolia to Aceh.\nAnnabel Teh Gallop\nLead Curator, Southeast Asia, British Library & Co-Director, 'Islam, Trade and Politics across the Indian Ocean'\nWith thanks to Tim Stanley for comments on the illumination.\nFrom Anatolia to Aceh: Ottomans, Turks, and Southeast Asia\nEdited by Andrew Peacock and Annabel Teh Gallop\nOUP/British Academy | Proceedings of the British Academy Vol. 200\n300 pages | 22 illustrations | 234x156mm\n978-0-19-726581-9 | Hardback | 05 February 2015\nAvailable from Oxford University Press"", 'Presentation on theme: ""Objectives Describe the role of trade in Muslim civilization.""— Presentation transcript:\n1 Objectives Describe the role of trade in Muslim civilization. Identify the traditions that influenced Muslim art, architecture, and literature.Explain the advances Muslims made in centers of learning.\n2 Terms and Peoplesocial mobility – the ability to move up in social classFirdawsi – poet known for his history of Persia, the Shah of Namah (Book of Kings)Omar Khayyám – scholar, astronomer, and philosopher, best known for the Rubáiyátcalligraphy – art of beautiful handwritingIbn Rushd – a scholar who put all knowledge, except the Quran, to the test of reason\n3 Terms and People (continued) Ibn Khaldun – devised a set of standards for the scientific study of history; emphasized economics, social structure, and avoiding biasAl-Khwarizmi – a mathematician who pioneered the study of algebraMuhammad al-Razi – studied measles and smallpox; taught to treat the mind as well as the bodyIbn Sina – wrote the Canon on Medicine, an encyclopedia of medicinal cures3\n4 What did Muslims achieve in economics, art, literature, and science? Under the Abbasids, Muslim civilization absorbed traditions from all of the people who lived under Muslim rule, including Jews and Christians. The great works produced by scholars of the Abbasid period shaped Muslim culture and civilization.European scholars began to study Muslim philosophy, art, and science.\n5 Camel caravans crossed the Sahara to West Africa. Muslim rulers united diverse cultures and incorporated learning from many regions.Camel caravans crossed the Sahara to West Africa.The Silk Road brought trade from East Asia and provided a link to Europe.Monsoon winds brought ships to Asia and East Africa.Merchants were valued in the Muslim world. They spread products, cultures, and ideas widely.5\n6 Indian numbers were introduced and became today’s Arabic numerals. Sugar arrived from India.Papermaking came from China.As more people converted to Islam, Arabic became widely understood, facilitating trade and new learning.New business practices, such as partnerships, checks, and credit, grew from the use of a money economy.\n7 Some valued products included: Steel swords—DamascusLeather goods—CórdobaCotton textiles—EgyptCarpets—PersiaHandicraft industries were valued. Heads of each guild regulated quality, price, and production.Muslim rulers extended agriculture.In addition to food, farmers grew cotton, sugar cane, flowers, and herbs.\n8 Slavery did exist, however. There was social mobility, with options to improve one’s standing through religious, military, or scholastic achievements.Slavery did exist, however.Many slaves were house servants, skilled artisans, or soldiers, who could earn their freedom.Slaves could often buy their freedom. If a slave’s father was a freeman, he could be freed as well.8\n9 Islamic art and literature reflected the diverse cultures within the Muslim world. It was forbidden to portray God or human figures in religious art.The Quran itself was the greatest literature.\n10 The rich tradition of Arab storytelling continued in this period. Firdawsi’s the Shah Namah, or Book of Kings, told of the history of Persia.The Rubáiyát, by Omar Khayyám, is a philosophical work in four- line stanzas.Poets wrote tales of romantic and dangerous desert journeys. Some are remembered today—“Ali Baba and the Forty Thieves” and “Aladdin and His Magic Lamp,” from The Thousand and One Nights.\n11 Domes and arches adapted from the Byzantines became symbolic of Muslim architecture. The Dome of the Rock in Jerusalem is the oldest surviving example of Muslim architecture. It was built in 688.Domed mosques and high minarets still dominate Muslim cities such as Medina.\n12 Works in elaborate flowing script, especially illustrating verses of the Quran, are found as decorations on buildings, ceramics, and books.Muslim artists perfected skills in calligraphy.\n13 Muhammad’s great respect for learning inspired advances in philosophy, history, mathematics, and the sciences.Learning from earlier civilizations was translated into Arabic from Persian, Sanskrit, and Greek.Great centers of learning were founded in cities such as Cairo, Baghdad, Córdoba, and Timbuktu.\n14 Ibn Rushd put knowledge to the test of reason. Ibn Khaldun set standards to avoid bias and error in the study of history.Arab scholars translated works from Greek, Hindu, and Buddhist philosophers.Scholars sought to harmonize Greek ideas, based on reason, with Muslim ideas based on divine revelation.\n15 Arab scholars made many mathematical advances. Arab mathematicians developed what became our modern number system.The study of algebra was pioneered by al-Khwarizmi in the 800s.\n16 Building on the work of the Greeks, Muslims greatly advanced medicine and public health. Physicians and pharmacists had to pass tests.There were hospitals and physicians who traveled to rural areas.Pharmacists mixed bitter-tasting medicines with sweet-tasting syrups and gums for the first time.\n17 Muhammad al-Razi studied measles and smallpox Muhammad al-Razi studied measles and smallpox. He also stressed the need to treat the mind as well as the body.Ibn Sina compiled a huge encyclopedia of all known medical knowledge called the Canon on Medicine.Muslim physicians made great advances in medicine.Arabic physicians could even perform cataract surgery using hollow needles.']"	['<urn:uuid:2dfc3dd8-c1c1-4063-8cec-4dfcb647dddb>', '<urn:uuid:88fd6dc5-db56-49ba-8f27-f1d9500e2589>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T06:04:36.674696	15	48	1970
27	capital market intermediaries roles crypto exchanges differences protection	In traditional capital markets, intermediaries like underwriters, brokers, and depositories operate under government rules and regulations, serving as crucial working organs that help facilitate transactions. In contrast, cryptocurrency exchanges, which are online platforms for trading digital assets, are largely unregulated. This key difference means that investors in traditional markets receive regulatory protection, while crypto exchange users lack similar safeguards and are vulnerable to digital theft, hacking, and permanent loss of assets. Traditional intermediaries must follow strict protocols, while crypto exchanges often operate without oversight, leading to increased risks for investors.	['A market including all institutions, organisations, and instruments providing medium and long-term funds is known as a Capital Market.\nA market that serves as a link between the savers and borrowers by transferring the capital or money from those who have a surplus amount of money to those who are in need of money or investment is known as Financial Market. Simply put, Financial Market is a market that creates and exchanges financial assets. In general, the investors are known as the surplus units and business enterprises are known as the deficit units. Hence, a financial market acts as a link between surplus units and deficit units and brings the borrowers and lenders together. One can allocate funds with the help of the following two main ways:\n- Through Banks\n- Through Financial Markets\nThe households (who are the surplus units) may keep their savings in banks or they may use that amount for buying securities from the capital market. The financial market and banks, then lend the funds to the business firms (who are the deficit units). The banks and financial market compete with each other. Financial Markets are classified into two broad categories; namely, Capital Market(Primary Market and Secondary Market) and Money Market.\nA market including all institutions, organisations, and instruments providing medium and long-term funds is known as a Capital Market. A capital market does not include institutions and instruments providing finance for a short term, i.e., up to one year. Some of the common instruments of a capital market are debentures, shares, bonds, public deposits, mutual funds, etc. An ideal capital market is one that allocates capital productively, provides sufficient information to the investors, facilitates economic growth, where finance is available to the traders at a reasonable cost, and where the market operations are fair, free, competitive, and transparent. A capital market is of two types; namely, Primary Market and Secondary Market.\nAccording to V.K. Bhalla, “Capital Market can be defined as the mechanism which channelises saving into investment or productive use. Capital market allocates the capital resources amongst alternative uses. It intermediates flow of savings of those who save a part of their income from those who want to invest it in productive assets.”\nNature or Features of Capital Market\nThe features of a Capital Market are as follows:\n1. Serves as a link between Savers and Investment Opportunities\nA capital market serves as a crucial link between the saving process and investment process, as it transfers money from the savers to entrepreneurial borrowers.\n2. Deals in Long-term Investment\nA capital market provides funds for the medium and long term, and it does not deal with channelising savings for less than one year.\n3. Utilises Intermediaries\nA capital market works by making use of different intermediaries like underwriters, brokers, depositories, etc. The intermediaries of a capital market act as the working organs of the capital market. Hence, they are very crucial elements of a capital market.\n4. Determinant of Capital Formation\nThe activities that take place in a capital market determine an economy’s rate of capital formation. This market offers various attractive opportunities to those who have surplus funds so that they can invest more and more in the capital market, and get encouragement in saving more for profitable opportunities.\n5. Government Rules and Regulations\nEven though a capital market operates freely, it works under the guidance of Government policies. A capital market functions within the framework of government rules and regulations. For example, the stock exchange works under the regulations of a government body, i.e., SEBI.\nClassification of Capital Market\nA capital market can be classified into two categories; viz., Primary Market and Secondary Market.\n1. Primary Market (New Issue Market)\nA market in which the securities are sold for the first time is known as a Primary Market. It means that under the primary market, new securities are issued from the company. Another name for the primary market is New Issue Market. This market contributes directly to the capital formation of a company, as the company directly goes to investors and uses the funds for investment in machines, land, building, equipment, etc.\nMethod of Floatation of Securities in Primary Market\nOne can issue the securities in the primary market with the help of the following methods:\ni) Public Issue through Prospectus\nThe first method of floatation of securities in a primary market is ‘Public Issue through Prospectur’. Under this method, a company issues a prospectus to inform the general public and attract them to invest in the company. The prospectus of a company contains information regarding the purpose for which it wants to raise funds, its past financial performance, its background, and future prospects. The information provided in the prospectus helps the general public, get to know about the earning potential of the company and the risks involved in investing in the company. Based on this information, the public decides whether or not they want to invest in the company. With the help of an IPO, a company can easily approach a large number of persons and can approach the public at large. Sometimes under this method, the companies take help of the intermediaries like underwriters, brokers, and bankers for raising capital from the general public.\nii) Offer for Sale\nThe second method is ‘Offer for Sale’ and under this method, the new securities are offered to the general public not by the company directly, but by an intermediary who has bought a whole lot of securities from the company. These intermediaries are generally the firms of brokers. As the intermediaries offer the new securities to the general public, the company is saved from the complexities and formalities of issuing the securities directly to the public.\nThe sale of securities through Offer for Sale takes place in two steps:\n- Firstly, when the company issues the new securities to the intermediary at face value.\n- Secondly, when the intermediaries issue securities to the general public at a higher price with the motive of earning profit.\niii) Private Placement\nIt is a method in which a company sells securities to an intermediary at a fixed price, and then the intermediaries sell these securities to selected clients at a higher price instead of the general public. The company issuing securities issues a prospectus providing details about the objective and future prospects of the company so that the reputed clients will prefer to purchase the securities from the intermediary. The selected clients to whom securities are issued by the intermediaries are LIC, UTI, General Insurance, etc. As the company does not have to incur expenses on manager fees, brokerage, underwriter fees, the listing of the company’s name on the stock exchange, agent’s commission, etc., it is considered as a cost-saving method. This method is preferred by small-scale companies and new companies that cannot afford to raise funds from the general public.\niv) Right Issue (For Existing Companies)\nUnder this method, new shares are issued to the existing shareholders of a company. It is known as the right issue because it is the pre-emptive right of the shareholders that the company must offer them the new issue of shares before subscribing them to outsiders. The existing shareholders have the right to subscribe to the new shares in the proportion of the shares they already hold.\nThe Companies Act, 1956 states that it is compulsory for a company to issue a Right Issue to the existing shareholders. It means that the stock exchange does not allow a company to issue new shares in the market before giving the pre-emptive rights to the existing shareholders. It is because if the company directly issues the new issue to the new subscribers, then the existing shareholders of the company may lose their share in the capital of the company and cannot have control over the company.\nv) e-IPOs (Electronic Initial Public Offer)\nA new method of issuing securities in which an online system of stock exchange is used is known as e-IPO. Under this method, a company appoints registered brokers to accept applications and place orders. The company which is issuing the security has to apply for the listing of its securities on any exchange. However, it cannot be the same exchange where it has earlier offered its securities. For this method, the manager coordinates the activities with the help of various intermediaries connected with the Issue.\n2. Secondary Market (Stock Exchange)\nA market in which the sale and purchase of newly issued securities and second-hand securities are made is known as a Secondary Market. In this market, a company does not directly issue its securities to the investors. Instead, the existing investors of the company sell the securities to other investors. The investor who wants to sell the securities and the one who wants to purchase meet each other in the secondary market and exchange the securities for cash with the help of an intermediary called a broker.\nShare your thoughts in the comments\nPlease Login to comment...', 'In October 2019, securities regulators on six continents will promote investor education and protection through World Investor Week, an initiative of the International Organization of Securities Commissions (IOSCO). World Investor Week is a global campaign to raise awareness about the importance of investor education and protection, and to highlight the investor education and protection initiatives of securities regulators.\nVirtual and digital currencies/assets are gaining popularity. However, it should be underscored that consumers need to assess the risks associated with holding and/or investing in virtual or digital assets prior to investing in these assets. In particular, consumers should be aware that providers of virtual or digital assets are mostly unregulated. Regional and global regulators emphasize that consumers should know the potential risks of buying or investing in virtual currencies and assets. High volatility, the lack of (proper) regulation, vulnerabilities concerning cybersecurity, misuse for criminal and fraudulent activities, and lack of transparency makes these types of high risk investments unsuitable for retail investors, or investors other than professional, accredited, or sophisticated investors.\nTogether, we are committed to helping you protect yourself and the integrity of our markets.\nEducation is essential to understanding these types of products, so please read the following and\njoin us with your PLEDGE at the end.\nKEY FINTECH CONCEPTS\nCrypto-assets are products that have captured the attention of media, both experienced and inexperienced investors, financial regulators, and governments. But what are they? How can they impact our lives?\nThis section will help you to better understand the key fintech concepts, and the concerns that regulators may have with people investing their savings in them. Click on the terms below to learn more.\nThe term “fintech” is simply a merger of the words “financial” and “technology”. It refers to the use of technology, including computer programs, to deliver or improve banking and financial services.\nThere have been many traditional financial services that have drawn the attention of fintech developers. Mobile banking and software apps that let you buy insurance online, or that provide investment advice over your mobile phone, are a few examples. When crypto-assets are used to meet financial services needs, they are considered fintech as well.\nAdvances in technology can bring new solutions that forever change the way that people do things. New technologies, like crypto-assets, often bring new risks, and it usually takes some time to understand how best to protect consumers and the public from those risks without stopping progress.\nCrypto-assets is a term that captures a wide range of products. Examples include cryptocurrencies, crypto-coins, exchange tokens, non-fungible tokens, security tokens, utility tokens, and virtual currencies. Common to all of these is their reliance on distributed ledger technology (DLT), like the blockchain, to track and record key details, such as the ownership history of the crypto-asset.\nExamples of crypto-assets that you may have heard of include cryptocurrencies like Bitcoin, Ethereum, Ripple and Litecoin.\nHow distributed ledger technology (DLT) is defined varies. However, the key concept with DLT is that a number of participants on a network of computers use cryptography (secret coding) to control the generation of units of digital assets and to record and verify transactions. In the case of cryptocurrency, the DLT used is called the blockchain.\nWith DLT, transactions are recorded digitally across the users on the network and the collective records must all match to be accepted. This is one of the key innovations of DLT. There is no centralised government authority or middleman, such as a Central Bank, involved in the process. The network is the first and final authority.\nThe lack of a central government authority may appeal to different users for various reasons. However, in some cases, it also attracts people with criminal intentions. In fact, crypto-assets have become a popular payment method for some frauds and other crimes which could impact you. For example, they are often used to facilitate ransomware payments.\nAlthough the usage and acceptance of many crypto-assets and ways to exchange them for “real” money are increasing, by no means should you expect to go to the grocery store with your cryptocurrency, or that your landlord or the local power company must accept it from you.\nBecause “currency” is in the name, many people think that a cryptocurrency is a “currency” like US dollars, or euros. US dollars and euros are known as “fiat currencies”. Fiat currencies are backed by the governments that issued them and are legal tender in the countries where they are issued – meaning that everyone in that country accepts them as a means of payment. Cryptocurrencies are not backed by an issuing government. They are not legal tender.\nCryptocurrencyprices are usually based on supply and demand, and they can have very large price swings as a result. This makes them a poor way to measure or store value.\nThere are developers working on cryptocurrencies that maintain stable prices. These cryptocurrencies, known as ‘stablecoins’, have prices that are connected to the prices of stable assets, such as US dollars or gold. Unfortunately, they are often complex and still subject to supply and demand forces that reduce the stability of their prices.\nCryptocurrencies, crypto-coins and crypto-tokens are very similar terms, whose meanings are still not universally defined. ‘Token’ is the term many experts and regulators use more and more frequently when discussing cryptocurrencies and crypto-coins.\n“Tokens” can be divided into three categories, depending on their features:\n1. Exchange tokens\n- intended for use as a means of exchange\n- not issued or backed by a central authority\n- used as a means of trade outside of the central payments and banking system\n2. Security tokens\n- features of securities\n- may be regulated just like securities\n3. Utility tokens\n- grant access to an existing or prospective product or service\n- do not qualify to be regulated like securities\nWhen public companies need to raise capital, or a private company makes the decision to “go public”, they may sell shares of their company in an initial public offering (IPO). Similarly, developers looking to raise money for a project or new venture may use an initial token offering (ITO), where tokens are offered in exchange for money or other assets. ITOs are also known as an initial coin offerings (ICOs), token sales or a coin sales.\nIn well regulated capital markets, the promoter of an IPO must meet strict regulatory standards regarding the information they are required to share with the public about the investment. However, ITOs/ICOs are often unregulated, in which case the supporting documents of the ICO are not required to be reviewed by a regulatory body, or to meet the high standards for disclosure of material information as would be required for an IPO.\nOnce an ITO/ICO is complete, tokens may be traded on a cryptocurrency exchange. A cryptocurrency exchange is an online platform that allows you to buy, sell or exchange cryptocurrencies for other cryptocurrencies, or for fiat currency. In many instances today, cryptocurrency exchanges are still unregulated. As a result, investors do not have the same protections as they would on regulated securities exchanges.\nCryptocurrency exchanges are also frequently the target of digital theft. Many have been hacked and suffered notable losses.\nTokens are stored in digital wallets, or e-wallets, that require a password for access. These passwords are often very long and complicated, to reduce the chance of theft or hacking. However, if an investor loses his or her password or the wallet is hacked, the investor could permanently lose access to the wallet’s contents.\nE-Wallets can be hot or cold. Hot wallets are online and connected to the Internet, so the user can access them to do transactions. However, as they are online, hackers may be able to access them too. To combat theft by hacking, some investors put their tokens into cold wallets. Cold wallets are stored offline and are away from exchanges. People who invest in cryptocurrencies usually have both cold and hot wallets.\nYou may have heard about people investing in cyrpto-assets and making lots of money quickly. While people have made fortunes on risky investments, like crypto-assets, many have lost fortunes and their life’s savings pursuing them. If you do not have a lot of investment expertise or you cannot afford to take heavy financial losses, there are some crypto-asset risk features that you need to know and understand.\n- Risks of financial crime: digital assets can be used by criminals for illicit activities through their lack of transparency. More and more, crypto-asset investors have been the target of outright scams and have lost their entire investment as a result. Without regulation, these investors have no one to turn to.\n- Risks in cybersecurity: tendency to cyber threats and attacks which means investors lose their investments or their assets are transferred out of the owners’ accounts and into others. Some investors have even had their e-wallets hacked or forgotten their passwords, and lost their investment as a result. Without regulation, investors have no one to turn to, for help to retrieve their lost assets.\n- Risks to investing individuals/consumers: One of the most important tools any investor has is information that is accurate, timely, and fairly obtained, which helps them to make sound investment decisions. Unfortunately, there is a lot of hype and often misinformation surrounding crypto-assets, particularly during initial token offerings, when a digital token is first offered for sale to the public. If the token offering is not regulated, a regulator may not be able to help you, even if you were intentionally misled. Without proper knowledge about these products, the concepts and the risks, you may become a victim of a fraudster who will try to get you to invest in a system of unregulated digital assets.\n- Risks to market integrity: increased vulnerability and abusive practices as a result of transparency and lack of regulation, may lead to financial losses and loss of market confidence with adverse financial stability implications.']	['<urn:uuid:4120ad70-bd91-4c5e-adc4-997296bfa127>', '<urn:uuid:6553d850-a050-4343-a349-2ecf497e5d6c>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T06:04:36.674696	8	90	3151
28	looking for hospitals newborn intensive care unit highest level delaware	Christiana Care features a Level III neonatal intensive care unit, which is the only delivering hospital in Delaware that offers the highest level of non-surgical care to the most critically ill newborns.	"[""|Patient Experience Case Study - Christiana Care Health System|\nSee the opportunities and challenges organizations face in addressing the patient experience.\nCase Studies provide real stories of current efforts, including programs being initiated, practices being implemented, and outcomes being targeted and/or achieved. Case studies are presented as both an opportunity for learning from others as well as a spark for further ideas on how we work to improve the patient experience. If you have a case study to share please contact us.\nInterested in receiving Patient Experience Monthly? Subscribe here.\nIntegrating Healthcare and Hospitality to Create a Patient Experience Academy\nWhat was the challenge, opportunity or issue faced?\nChristiana Care Health System is committed to providing respectful, expert health care that our neighbors value. Among the key ways we measure the effectiveness of care and its value to those we serve is by measuring the patient experience and patient satisfaction through surveys and a variety of other channels. This includes evaluating direct and indirect services, including quality of accommodations, communication and auxiliary services.\nThese services are not entirely medical; they have much in common with hospitality service management. Our goal is to make hospitals truly hospitable and prepare current and future healthcare providers, doctors, nurses, healthcare scientists, students and allied healthcare professionals to deeply connect to the emotional side of patient care and experience. These emotional experiences, which rely heavily on effective communication with people from diverse backgrounds and cultures, are critical to patient engagement, patient confidence and healing.\nThrough our partnership with a university, we brought together expertise in health care and expertise in the hospitality industry to create a unique training program that will give our staff the skills and tools they crave to achieve excellence in delivering an exemplary patient experience.\nWhat did you do to address it?\nThe Patient Experience Academy is a first-of-its-kind joint venture between Christiana Care Health System and the University of Delaware’s Alfred Lerner College of Business and Economics Hotel, Restaurant, and Institutional Management Department. This program helps us to deliver on The Christiana Care Way: We serve our neighbors as respectful, expert, caring partners in their health. We do this by creating innovative, effective, affordable systems of care that our neighbors value.\nThis 10-week course, part of the patient experience curriculum of the Christiana Care Value Institute Academy, takes proven customer service and engagement strategies from the hospitality industry and adapts them to a healthcare setting, aligned with the principles of The Christiana Care Way.\nThe Patient Experience Academy was launched April 29 with a cadre of 40 Christiana Care employees, primarily of front-of-house staff — patient guides, valets, guest services and environmental services staff. They learned skills and techniques that are helping them to create an outstanding first impression for patients and visitors at Christiana Care. The program incorporates:\nThe program incorporated many key components of adult learning, including application of learning goals to real situations, sharing achievement stories using crowdsourcing technologies and daily pre-shift meetings to maintain effective communication and inspire a culture of engagement.\nThe first class of the Patient Experience Academy was a resounding success. In the coming fiscal year, this program is expanding rapidly to train a diverse mix of health care professionals at Christiana Care to be leaders in delivering an outstanding health care experience to everyone we serve.\nWhat were the outcomes?\nKnowledge Outcomes: There has been a 50 percent increase across-the-board in program pre-and-post evaluations, including a 40 percent increase in understanding how to apply the new learnings back on the job. Participants' pre-test results averaged a grade of 5.37 (out of possible 10), testing knowledge and competencies targeted as part of the Patient Experience Academy. A post-test was conducted to measure knowledge acquisition in the same competencies. The average grade reached 9.32 (out of possible 10), demonstrating an impressive knowledge acquisition improvement.\nBusiness Results Outcomes: The FY14 HCAHPS Index for the system ended at the target of 73 percent. The leadership at Wilmington Hospital improved the HCAHPS index by 3 percent over last year, exceeding maximum target. Overall, 12 patient care units improved their overall HCAHPS index over the prior year’s results. Overall rating of the hospital improved to 72 percent. We also saw improvement in the Hospital Environment domain, which improved our COTH ranking by 10 percentile points.\nAbout the University of Delaware’s Hotel, Restaurant and Intuitional Management and Hospitality Associates for Research & Training (HART)\nConsistently ranked as a top ten hospitality program, the Department of Hotel, Restaurant and Institutional Management (HRIM) in the Alfred Lerner College of Business and Economics at the University of Delaware prepares students for a career in the hospitality and service industries. The Bachelor of Science degrees and Masters in Hospitality Business Management consists of a curriculum founded in academic disciplines including business and specialized courses in theoretical and experience driven managerial components for the hospitality and service industries. Students gain a foundation in the traditional academic areas to complement state-of-the-art business and hospitality courses.\nTapping into the University of Delaware’s high profile and one of the world’s very best research institutions, HART within the Department of Hotel, Restaurant and Institutional Management engages in health and hospitality research, training and innovation to address modern business challenges. For more information, visit www.del.edu/hrim.\nAbout Christiana Care Health System\nChristiana Care Health System is one of the country’s largest health care systems, ranking as the 22nd leading hospital in the nation and 12th on the East Coast in terms of admissions. The health system includes The Christiana Care Medical Group, a network of primary care physicians, medical and surgical specialists as well as home health care, preventive medicine, rehabilitation services and patient/family advisers for core health care services. A not-for-profit teaching hospital affiliated with Sidney Kimmel Medical College at Thomas Jefferson University, Christiana Care is recognized as a regional center for excellence in cardiology, cancer and women's health services. Christiana Care has an extensive range of outpatient services, and through Christiana Care Quality Partners, Christiana Care works closely with its medical staff to achieve better health, better access to care and lower cost. Christiana Care is home to Delaware's only Level I trauma center, the highest capability center and the only one of its kind between Philadelphia and Baltimore. Christiana Care also features a Level III neonatal intensive care unit, the only delivering hospital in Delaware that offers the highest level of non-surgical care to the most critically ill newborns. Christiana Care includes two hospitals with 1,100 patient beds. For more information about Christiana Care, visit www.christianacare.org/whoweare.""]"	['<urn:uuid:119f8d8f-9633-4e70-a63b-1000a8706d17>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T06:04:36.674696	10	32	1089
29	hiv blood test types viral detection methods transmission through personal items	HIV can be detected through several test methods including ELISA (enzyme linked immunosorbent assay), Western blot (gel protein electrophoresis), and PCR (polymerase chain reaction) tests that measure viral RNA in blood. While these tests can detect HIV in blood, transmission through sharing personal items like toothbrushes, razors, or lipstick is highly unlikely. HIV is a fragile virus that doesn't survive long outside the body, and casual contact through shared items poses minimal risk. Even in theoretical scenarios involving blood exposure through shared items, there have been no documented cases of HIV transmission through these means.	"['Top of the page\nA viral load test measures how much human immunodeficiency virus (HIV) is in the blood. Viral load is first measured when you are diagnosed with HIV infection. This initial measurement serves as the baseline, and future viral load measurements will be compared with the baseline. Since viral load can vary from day to day, the trend over time is used to determine if the infection is getting worse. If your viral load shows a steady increase over several measurements, it means the infection is getting worse. If the trend in viral load decreases over several measurements, it means that the infection is being suppressed.\nThe viral load is measured using one of three different types of tests:\nThese tests measure the amount of the genetic material (RNA) of HIV in the blood. But each test reports the results differently, so it is important to use the same test over time.\nA viral load measurement test is done to:\nYou and your doctor may set up a different schedule for the test, but the most common schedule is the following:\nYou do not need to do anything before you have this test.\nThe health professional drawing blood will:\nYou may feel nothing at all from the needle puncture, or you may feel a brief sting or pinch as the needle goes through the skin. Some people feel a stinging pain while the needle is in the vein. But many people do not feel any pain (or have only minor discomfort) after the needle is positioned in the vein. The amount of pain you feel depends on the skill of the health professional drawing the blood, the condition of your veins, and your sensitivity to pain.\nThere is very little risk of complications from having blood drawn from a vein.\nA viral load test measures how much human immunodeficiency virus (HIV) is in the blood. The results can take up to 2 weeks.\nThe normal values listed here—called a reference range—are just a guide. These ranges vary from lab to lab and depend upon which testing method is used (RT-PCR, bDNA, NASBA). Your lab may have a different range for what\'s normal. Your lab report should contain the range your lab uses. Also, your doctor will evaluate your results based on your health and other factors. This means that a value that falls outside the normal values listed here may still be normal for you or your lab.\nViral load results are reported as the number of HIV copies in a millilitre (copies/mL) of blood. Each virus is called a ""copy,"" because HIV reproduces by making copies of itself (replicating).\nHIV is not detected in the blood.\nHIV is detected in the blood. Your doctor will compare your current measurement with previous values.\nIf your viral load increases, it means the infection is getting worse. If the viral load drops, it means that the infection is being suppressed.\nIf you are not being treated, an undetectable viral load result does not mean that you no longer have HIV in your blood. It simply means that the amount of HIV in the blood was too low for the test to detect. HIV still can be passed to another person even when the viral load cannot be detected in people who are not on medication. U=U (undetectable = untransmittable) only when you are taking PEP (post-exposure prophylaxis) or PrEP (pre-exposure prophylaxis).\nReasons you may not be able to have the test or why the results may not be helpful include:\nAdaptation Date: 8/19/2021\nAdapted By: Alberta Health Services\nAdaptation Reviewed By: Alberta Health Services\nTo learn more about Healthwise, visit Healthwise.org.\n© 1995-2021 Healthwise, Incorporated. All rights reserved. Healthwise, Healthwise for every health decision, and the Healthwise logo are trademarks of Healthwise, Incorporated.', ""Doctor insights on:\nIs It Possible To Get Hiv From Sharing A Straw Or A Cup\nExtremely rare: It is extremely rare for this to happen. You cannot get HIV from eating food contaminated by small amounts of HIV infected blood. HIV does not live long outside the body. Exposure of the food to cooking heat, air or the acid in the stomach will kill the virus. There have been rare cases reported of children contracting HIV from their caregivers who are HIV infected. ...Read more\nHiv infection is caused by a retrovirus....This retrovirus binds to CD4 cells (for the most part). You may detect the virus by several different methods. An elisa test (enzyme linked immunosorbent assay). You may also detect it by doing a test referred to as a western blot (a gel protein electrophoresis). Thirdly by pcr (polymerase chain reaction) which ...Read more\nNo: Simply no. Just as you can't get pregnant that way. Life's hard enough without these kinds of fears. During the bad old days in which there was no treatment, family members simply didn't catch HIV from each other except by sex or sharing IV drug needles. Please relax and enjoy the company of others. ...Read more\nSharing drinks: All health care professionals are trained in infection control and communicable diseases yearly. The cdc standards for preventing blood borne pathogens is key information. Most disease need a minimum amount to be transmitted successfully to another. Fortunately HIV is a very fragile virus out of a host. Dies in seconds. Transmission requires 10cc. That's 2 teaspoons. But still get a check ...Read more\nIs it possible to get HIV from someone by sharing a cookie? The non infected person has their wisdom teeth growing in.\nHIV is Hard to get: Nope. Not in any way.Get a more detailed answer ›\nNo: This is as unrealistic as the idea that one could become pregnant this way. Please forgive my frankness. Family members not sharing the most intimate contact don't infect one another -- we leaned that through the bad years. Best wishes. ...Read more\nVery unlikely: Common sense says not to share razors, to prevent skin infections as well as blood borne infections like HIV and hepatitis B and C. However, it is a theoretical transmission mechanism that has never been reported as the actual source of somone's HIV infection. Don't share razors (except maybe your own mutually monogamous sex partner), but don't worry if it happens once or twice by accident. ...Read more\nUnlikely: In theory, since HIV can be found in saliva of infected individuals, sharing lipstick could transfer virus. However this would be a highly inefficient way to transfer enough virus to cause an infection in the second user of the lipstick and is most unlikely to actually happen. Still it would be prudent to not share lipstick. ...Read more\nLow Risk: But not impossible. There is a possibility that the virus could be transmitted. (for example if both parties had gum disease and bleeding in the oral cavity so that blood could be transmitted) I would check with your physician and discuss your concerns, but if it were my family member I would advise HIV testing for awhile to just to be safe (i tend to be overly cautious). ...Read more\nCan you get HIV from sharing nose drops? Even if the nose drop came into contact with infected blood or nasal secretions? Has this ever happened befor\nPossible: If infected blood on surface of bottle put in nose and a raw spot in the nose. ...Read more\nCan u get hiv from sharing hookah pipes eating utensils or kissing if hiv person has blood in mouth or on lips and u have cuts in your mouth or lips?\nNo.: Hiv is spread in bodily fluids, including semen, vaginal secretions, blood, and breast milk. People usually get hiv/aids by having unprotected sex with an infected partner, sharing needles with an infected person, or breastfeeding from an infected mother. You cannot get HIV from kissing or sharing a soda. ...Read more\nNo: There is no risk of transmission of HIV to you from blowing your friends whistle. Even if your friend had a minute amount of blood on the whistle from some serious blowing, HIV is a very fragile virus and would not survive in the environment to even have the possibility of being infective. Rest assured. Take care. ...Read more\nNot likely: It can possibly happen if the person has a high viral load in the blood and have gums bleed all over the tooth brush and you use it immediately after they do. If you think that you may have been exposed then get tested. ...Read more\nCan you get hepatitis c/hiv from sharing mascara? Opened a box of mascara from store that may have been opened and used previously, but brought back.\nNot likely: These are transmitted through intimate contact, shared needles, exposure to blood. ...Read more\nJust imagine a person with bad teeth shares utensils with a HIV positive person? Will she get HIV? No sex nothing just sharing utensils.\nWas wondering if you can catch hiv, hepatitis or anything from sharing drinks with friends? Thanks\nHuman immunodeficiency virus (HIV) is a lentivirus (a member of the retrovirus family) that causes acquired immunodeficiency syndrome (aids),  a condition in humans in which progressive failure of the immune system allows life-threatening opportunistic infections ...Read more\n- Talk to a doctor online\n- Can i get hiv from sharing a drink?\n- Straw hiv\n- Hiv share\n- Can you get chlamydia from sharing clothes?\n- Can you get herpes from sharing soap?\n- Can you get hpv from sharing a drink?\n- Can you get herpes from sharing a pipe?\n- Can u get aids from sharing a cigarette?\n- Can you get hiv from sharing lipstick?""]"	['<urn:uuid:50c8fbde-699f-42a9-9b5c-67e5e91a7e06>', '<urn:uuid:aa80982e-c38f-41fd-9590-9b390eb60bbe>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T06:04:36.674696	11	95	1609
30	compare catholic jewish refugee aid organizations world war 2 assistance differences	Catholic and Jewish refugee organizations primarily operated separately during and after World War II, following the prevalent ethos that religiously-based aid groups should help 'their own'. Jewish refugee aid organizations served as role models for Catholic organizations since they had started refugee aid work earlier. The Jewish Joint Distribution Committee (JDC) played a crucial role in helping Jewish survivors in DP camps, providing comprehensive assistance including food, medicine, clothing, educational materials, and religious supplies. They also helped organize communal life and represented DPs before military authorities. While both Catholic and Jewish organizations worked to help their respective communities, they engaged more in parallel efforts rather than cooperative ones, though Jewish organizations' success in transnational aid later caused anxiety among some Catholics who feared the Church was underrepresented.	['Catholic Refugee Organizations and Jewish Refugee Organizations\nComplicated circumstances and the clash of several different ideologies led to the uprooting and displacement of hundreds of thousands of people in Europe in the years surrounding and during World War II. Not one, but two totalitarian systems-Communism and Fascism-targeted certain groups within society whom they perceived as threats to their agenda and ultimate goals. At different times, Catholics and Jews would fall victim to both Communist/Soviet and Fascist/Nazi persecution, which ranged from censorship and an atmosphere of intimidation to “the Final Solution”, the extermination of roughly six million Jews in Nazi concentration camps.\n- How did Communist and Fascist persecution of targeted groups differ? How was it similar?\n- How do the authors of these documents react to the concentration camps and the Soviet gulags?\nWhile Catholics and Jews both suffered totalitarian persecution, there are certain key differences in the Nazi treatment of each group. Catholics belonged to a religious group which, however central to their identity, they could always dissociate themselves from if they wished. The Nazis targeted Jews, on the other hand, because of their very ancestry and perceived racial identity, which was hardly something a Jewish person could detach him or herself from. Nazi anti-Semitism, in this way, differed essentially from the anti-Semitism that had plagued European Jews in the past.\nIn fact, by the 1940s, many German Jews no longer practiced their religion, and had given up many of the distinctive forms of dress, dietary prohibitions, and other traditions that had once marked them out as “other”. If we were there, we would probably be unable to distinguish a German person of Jewish ancestry from any other German. Before the rise of the Nazis, modern Germany had not been a hostile environment for Jews- on the contrary, it prided itself for its tolerance. So successful had the German Jewish population been at assimilating into mainstream society that they had a saying, now jarring to post-World War II listeners, that “Berlin is our Jerusalem; Germany is our Fatherland.”\n- What effect did the Holocaust have on Jewish attitudes towards Germany?\n- Do these documents speak predominantly of persecution on the basis of race, or focus more on religiously-motivated persecution?\nThe fact that the Nazis constructed race in a biological way meant that a person could hold Catholic beliefs and still be seen by them as Jewish. Among the Catholics fleeing Germany for America were, in fact, many converts from Judaism, and many others whom the Nazis called “non-Aryans”. A “non-Aryan” was a category created by the Nazis which meant a person with one or more Jewish grandparents.\n- What rights did the Nazis deny those who fell into this “non-Aryan” category?\n- How are “non-Aryan Catholics” spoken about in these documents, and who are some of the main advocates on their behalf?\n- Did the US bishops and other authors of these documents recognize that “non-Aryan” is a term created by the Nazis which says nothing about the real identity of a person, or do they slip into talking about “non-Aryans” as a reality because of Nazi repetition of the term?\nAs these documents reveal, Catholics and Jews in America were both trying to help their fellow believers in Europe and facilitate their escape during these years. However, as we can also see from these documents, Catholics and Jews were more frequently engaged in parallel and separate efforts at aid than in cooperative efforts. In part, this separation arose simply from the prevalent ethos at the time that religiously or nationally-based aid groups should each help “their own”. Frequently, Jewish refugee aid organizations served as role models or blueprints for Catholic organizations, since they had begun the work of refugee aid several years before most Catholics. After the close of World War II, Jewish participation in transnational aid organizations awakened the anxieties of some Catholics, who feared that the Church, by comparison, was underrepresented, as we can see in a letter from Father Killion to Bruce Mohler, the director of the NCWC’S Bureau of Immigration.\n- How do the authors of these documents describe Jewish refugee aid organizations and endeavors? Are they generally critical or admiring?\n- Do you see evidence in these documents that Catholic and Jewish organizations also elected to operate separately from one another?\n- How have ideas of “one’s own” changed since that time? What other criteria, such as need or a simple duty to humanity, might be used to justify a Catholic charitable organization helping another group?\n- Do the authors of these documents try to depict refugees as a primarily Jewish problem? To which factors do they attribute successful Jewish efforts to care for and resettle Jewish refugees?', 'JDC in the Displaced Persons (DP) Camps (1945-1957)\nAs World War II drew to a close, JDC marshalled its forces to meet a crisis of staggering proportions, racing to ensure that tens of thousands of newly liberated Jews would survive to enjoy the fruits of freedom. By late 1945, some 75,000 Jewish survivors of the Nazi horrors had crowded into the displaced-persons (DP) camps that were hastily set up in Germany, Austria, and Italy. Conditions were abominable, with many subjected to anti-Semitism and hostile treatment. Earl Harrison, Dean of the University of Pennsylvania Law School and President Truman’s Special Envoy, asked Joseph Schwartz, JDC’s European director, to accompany him on his official tour of the camps. His landmark report (the Harrison Report) called for separate Jewish camps and for UNRRA’s (United Nations Relief and Rehabilitation Administration) participation in administering them – with JDC’s help. To provide that help, Schwartz virtually recreated JDC, putting together a field organization that covered Europe and later North Africa and designing an operational strategy that valued action and initiative.\nSupplementing the relief provided by the U.S. Army, UNRRA, and the International Refugee Organization (UNRRA’s successor agency), JDC distributed supplies that nourished both body and soul: food, medicine, clothing, tools, equipment, and educational, cultural, and religious materials including books, Torah scrolls, ritual articles, and holiday provisions. JDC funds supported medical facilities, schools, synagogues, and cultural activities, in addition to providing for vocational training programs, legal representation, and emigration assistance to survivors in the DP camps. JDC personnel helped organize communal life in many camps and represented the DPs before the military and other authorities.\nOver the next two years, the number of Jews in the DP camps more than tripled, with a new influx of refugees from Romania, Hungary, and Poland who had been helped to reach Western occupation zones. They included many Polish Jews who had returned from their wartime refuge in the Soviet Union, only to flee once again (westward, this time, through Czechoslovakia) in the face of renewed anti-Semitism and the July 1946 Kielce pogrom. It is estimated that by 1947, about 250,000 Jewish refugees passed through the DP camps and received assistance from JDC.\nAfter the State of Israel was established in 1948 and the United States passed the Displaced Persons Act, DP camps were closed and the number of Jews served by JDC dropped precipitously. Nevertheless, in 1952, JDC was still assisting some 15,500 DPs who were having difficulty finding permanent places of resettlement. Foehrenwald, the third largest Jewish DP camp in the U.S. Zone, was the last camp to close in 1957.\nView archival footage of JDC’s work in the DP camps:\nA Day of Deliverance (excerpts), 1949.\nProduced by JDC. JDC staff and programs serving Holocaust survivors in DP camps in Germany and Austria, beginning in 1945.\nReport on the Living (excerpts), 1947.\nProduced by JDC. Jews fleeing Poland following Kielce pogrom in 1946 on their arduous journey through Czechoslovakia to the DP camps of Germany and Austria.\nA Day of Deliverance (excerpts), 1949.\nProduced by JDC. Survivors on their way to the newly established State of Israel after years of circumscribed living in the Bergen-Belsen DP camp.\nFollowing World War II, JDC provided critical services to European Jews in the displaced persons (DP) camps established by the Allied Armed Forces. JDC placed special attention on the unique needs of the growing population of children in the camps.\nThe following are source materials in PDF format that can be used to teach about JDC’s role in the Displaced Persons (DP) camps:\nIndividuals Noted Above and Their Roles at the Time:\n- Samuel Dallob: AJDC, British Zone in Germany\n- Charles Jordan: Head of the Emigration Department, AJDC Paris (1948)\n- Herbert Katzki: Secretary of JDC’s European Executive Council (EUREXCO) – the title was subsequently changed to Assistant Director General for Overseas Operations.\n- Leo W. Schwarz: Director of JDC’s Operation in the U.S. Zone in Germany\n- Leonard Seidenman: AJDC, Office for France. Director of Reconstruction Activities (1947-1953); Director of Rabbinical Services (1948-1953).\n- William Haber: Adviser on Jewish Affairs to the American Commanders in Germany and Austria, U.S. Army.\n- Jay B Krane: Chief, Reports and Analysis Branch, United Nations Relief and Rehabilitation Administration (UNRRA) Displaced Persons Program in Europe.\n- Kenneth C. Royall: Secretary, U.S. Department of the Army\nFor More Information on this Topic:\nAgar, Herbert. The Saving Remnant: An Account of Jewish Survival. New York: Viking Press, 1960.\nAmerican Jewish Joint Distribution Committee, Henry Friedlander, and Sybil Milton, gen. eds. Archives of the Holocaust: An International Collection of Selected Documents, vol. 10, parts 1 and 2. New York: Garland Publishing, 1995.\nBauer, Yehuda. Out of the Ashes: The Impact of American Jews on Post-Holocaust European Jewry. Oxford: Pergamon Press, 1989.\nMankowitz, Zeev W. Life Between Memory and Hope: The Survivors of the Holocaust in Occupied Germany. New York: Cambridge University Press, 2002.\nPatt, Avinoam J., and Michael Berkowitz, eds. We Are Here: New Approaches to Jewish Displaced Persons in Postwar Germany. Detroit: Wayne State University Press, 2010.\nWarhaftig, Zorach. Jewish Refugees and Displaced Persons after Liberation. New York: Institute of Jewish Affairs, 1946.\nWyman, Mark. DP: Europe’s Displaced Persons. Ithaca and London: Cornell University Press, 1998']	['<urn:uuid:5aa2c5e5-aaa5-4cb5-b0a2-9e83d4d1172e>', '<urn:uuid:e842599c-f902-4ce1-ae31-cc734055b323>']	open-ended	direct	long-search-query	distant-from-document	comparison	novice	2025-05-13T06:04:36.674696	11	127	1656
31	compare different types black and white film fixing chemicals processing steps	Black and white film processing involves four main chemical stages. First is the developer, with options like Fomadon R09 or Tetenal Film Chemistry Starter Kit. Second is the stop bath (like Ilford Ilfostop) which halts development and extends fixer life. Third is the fixer, which clears unexposed emulsion and stabilizes the image - options include Tetenal Superfix Odorless, Fomafix P powder, or Ilford Rapid Fix. Finally, a wetting agent is used to prevent drying marks. Each chemical serves a specific purpose in the development process.	['If you want to know how to develop film at home this guide will walk you through all the things you need.\nFilm processing is much easier than you think. Plus, it is almost as much fun as shooting the film. If you have the time and inclination, processing your own film can be a very rewarding part of the process. It will help you learn and develop your understanding of the basic principles of photography. Also, it is great to know you’re in control of the whole operation, from exposure to development, then onto printing or scanning. So, read on to find out what you need to develop film at home.\nHow To Develop Black And White Film\nOur full guide to film processing walks you through every step of how to develop black and white film at home. You can read it here.\nSimply, there are four different stages of black and white film developing. Each step uses a different chemical. Below we will explain what it step is and what it does.\nThis will, unsurprisingly, develop your image. There are many developers to choose from, and you can find them all here. If you’re new to film processing we recommend the Tetenal Film Chemistry Starter Kit. Alternatively, Fomadon R09 is good value for money. It’s a classic formula and very economical.\nWorth a read is our post on how to Choosing Your Film Developer. This is will talk you through the different black and white developers. And help you choose the right one for you.\nThis, as the name suggests, this stops the development process. Not always 100% necessary, but does give more control over development time and rinses the developer from your film. Plus it will keep your fixer less from getting contaminated, and make it last longer. We always recommend using it when you develop film at home. Ilford Ilfostop comes in a 500ml bottle and because of it’s high dilution it will last you ages.\nFixer clears all the unexposed film emulsion and fixes the silver in the exposed parts of your image. Fixing time is important as it stabilises the image and makes it light proof. Tetenal Superfix Odorless is a popular choice, with the added benefit of no smell, quite nice when processing at home. Fomafix P powder is easy to transport if you are travelling but you will need to store the solution once it is mixed up. Our most popular fix is Ilford Rapid Fix.\nAfter washing its recommended that you soak your film in the wetting agent for a minute or two, this allows the water to slip off the film more easily which helps avoids drying marks. All wetting agents are similar, but if you had to chose one we’d go for Kodak Photo-Flo, it’s a handy size and good price. If you have hard water, as we do in London, add a bit more wetting agent than recommended.\nHow To Develop Colour Film\nOur full guide to colour film processing walks you through every step of processing colour negative film at home. You can read it here.\nIf you are processing colour film, all the chemistry you need comes in one handy kit. We use Tetenal’s C41 kit for developing colour negative film.\nFilm Developing Equipment\nAs well as chemistry, you will also need some equipment to develop film at home. Here is our list of developing equipment you will require.\nWhile some bits are essential, other items you might not need depending on your personal setup. If you are in doubt, speak to us and we will help work out what you need.\nThis is where everything happens. You load your film onto the reel and seal it in the light-tight tank, all chemicals are poured in and out of the top. Paterson Universal two-reel tanks are a good deal as you get two film reels included. All the processing can be done in daylight. Loading, however, must be done in complete darkness, which brings us to our next item…\nUnless you have access to a darkroom, a changing bag is essential to developing film at home. It is a flexible and compact space to load film in complete darkness. The developing tank, reel, scissors, and film are all zipped into this light-tight bag. If you shoot large format film it is also perfect for loading your film slides. You can fit all these items into the Paterson Changing Bag.\nYou will be measuring and mixing up your developer, stop and fixer in this. Two or three make for a smoother operation, but you could manage with one. 1 litre jugs are sufficient if you’re developing one or two rolls. Paterson tanks need 300ml of liquid to cover a roll of 35mm film and 600ml for 120.\nDepending on which developer you use, you might want a graduate in small increments to measure it out accurate amounts of your chemicals. Syringes can also be used for smaller amounts. The Paterson Measuring Graduate 45ml is useful for high dilution developers like Kodak HC-110 or Fomadon R09.\nThis is to make sure your chemistry is the right temperature. It is an essential piece of kit. Getting the temperature right is very important when processing film, particularly for the developer. We would recommend choosing the Paterson Colour Thermometer, even if you’re only interested in black and white right now, as it is useful to have if you decide to process your own colour film in the future, and it worths for both types of film.\nFixer and Stop bath can be used repeatedly once mixed up to a working dilution. It makes sense to buy some storage bottles to keep these in. They will be ready to use the next time you process film. Some developers, such as ID 11, can be used several times as well.\nSqueegee or Chamois Leather\nBoth of these are optional, a lot of people are happy to use their fingers to remove the excess water when the film comes out of the final wash. Be warned, if a Squeegee picks up dirt it can leave scratches down the length of the film, so be careful. The emulsion on film is very is quite soft when wet and it doesn’t take much to scratch it whatever you use.\nThis Ilford Film Cassette Opener is to open up the 35mm film canister in order to load it onto the reel. A bottle opener will work just as well. You don’t need this for 120 films, as these will simply unravel from their spool.\nThese are to help remove the film from the canister. You can tear the film with your fingers, but scissors will give it a straighter edge and make loading much easier.\nIf you’re new to film then read our Beginners Guide To Film Photography.']	['<urn:uuid:9176db38-3b3e-4241-9da1-3c1766444709>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T06:04:36.674696	11	85	1142
32	I've been reading about measuring devices - what's the main difference between ABCD matrices and Abbe refractometers in terms of what they are used to measure?	ABCD matrices are used to describe beam propagation through optical elements like mirrors and lenses, while Abbe refractometers are specifically used to measure the refractive index of substances like liquids and solids. ABCD matrices analyze how light beams travel through optical systems, whereas Abbe refractometers determine how much light bends when moving through a sample by measuring the angle of total reflection.	"['Presentation on theme: ""Gaussian Beam Propagation Code ABCD Matrices Beam Propagation through a series of parabolic optical elements can be described by the use of ABCD matrices.""— Presentation transcript:\nGaussian Beam Propagation Code ABCD Matrices Beam Propagation through a series of parabolic optical elements can be described by the use of ABCD matrices Examples: Matrices for a mirror,lens, dielectric interface\nApplication on a ray defined by position and slope Curved dielectric interface The ABCD matrix algorithm can be applied on a propagating ray as well as on a propagating gaussian beam\nSiegman, LASERS, Chapt. 15, Ray Optics and Ray Matrices\nGaussian Paraxial Wave Optics The ABCD matrix can also be applied to transform the so called q Parameter of a Gaussian beam R radius of phase front curvature w spot size defined as 1/e^2 radius of intensity distribution\nTransformation of the q parameter by an ABCD matrix The q parameter is given by\nRay Matrix System in Cascade M1M1 M2M2 M3M3 Total ray matrix\nGaussian Duct A. E. Siegman, LASERS A gaussian duct is a transversely inhomogeneous medium in which the refractive index and the absorption coefficient are defined by parabolic expressions n(r) r\nand n 2 parabolic refractive index parameter α 2 parabolic gain parameter Parabolic parameters n 2 and α 2 of a gaussian duct\nABCD Matrix of a Gaussian Duct With the definition the ABCD matrix of a gaussian duct can be written in the form\nIn LASCAD the concept of the Gaussian duct is used to compute the thermal lensing effect of laser crystals. For this purpose the crystal is subdivided into short sections along the axis, and every section is considered to be a Gaussian duct.\nA parabolic fit is used to compute the parabolic parameters for every section. Example: Parabolic fit of the distribution of the refractive index\nWith the ABCD matrices of mirrors, lenses, internal dielectric interfaces, and Gaussian ducts most of the real cavities can be described. To compute the eigenmodes of a cavity the q parameter must be self-consistent, that means it must meet the round-trip condition.\nThe round-trip condition can be used to derive a quadratic equation for the q parameter. All these computations are simple algebraic operations and therefore very fast.\nGaussian Optics of Misaligned Systems With 2 x 2 ABCD Matrices only well aligned optical systems can be analyzed. However, for many purposes the analysis of small misalignment is interesting. This feature has not been implemented yet the LASCAD program, but it is under development, and will be available within the next months.\nAs shown in the textbook LASERS of Siegman the effect of misalignments can be described by the use of 3x3 matrices Here E and F are derived from the parameters Δ 1(2) describing the misalignmet of the element\nThese 3x3 Matrices also can be cascaded to describe the propagation of a gaussian beam through any sequence of cascaded, and individually misaligned elements. is the total ABCD Matrix is the total misalignment vector which depends on the individual misalignments and the individual ABCD matrices', 'The customdesigned microprocessor delivers an instant readout on one of five different scales. It is used in the fast control of liquids and the infield analysis. Sep 20, 2016 abbe refractometer the use and how it works duration. Refractometry is generally speaking the measurement of refractive index and its interpretation under different starting points refractometric measurements can be used for example for purity investigations, sample recognition, dilution control or composition. Its most common use is the determination of the concentrations of solutions.\nAbbe refractometer definition is a refractometer in which the critical angle for total reflection at the interface of a film of a liquid between two similar glass prisms is utilized in determining the index of refraction. Atago manufacturer 1211 nar1t liquid mpn upc code atago brand refractometers product refractometer type product line abbe class liquid grade astm number astm application capacity refractive index nd. In the abbe refractometer the liquid sample is sandwiched into a thin layer between an illuminating prism and a refracting prism figure 2. Select from a wide array of units of measure and scale combinations.\nOur best selling standard abbe refractometer measures the refractive indices n d and mean dispersion n fn c of glass, as well as other transparent and translucent liquids and solids. Refractometer history of refractometer ernst abbe designed his refractometer in 1869, but it didnt became commercially available from carl zeiss before 1881. The nar1t solid abbe refractometer was designed for solid sample measurement this model can also measure liquid samples. A refractometer is a laboratory or field device for the measurement of an index of refraction refractometry. Brix is a measure of sugar concentration and is used extensively in the agriculture and food and beverage industries. The palm abbe digital refractometer is fast, convenient, and easytouse.\nAbbe refractometers are named after ernst abbe, who invented the first refractometer in the 19th century. Refractometers are typically used to determine the refractive index of a liquid sample. Place your sample on the main prism, which is rimmed with stainless steel to prevent corrosion, look through the eyepiece and line up the shadowline in the crosshairs. The abbe instrument is the most convenient and widely used refractometer, fig1 shows a schematic diagram of its optical system. Place your sample on the main prism, which is rimmed with stainless steel to prevent.\nThis model has the refractive index scale and brix scale, and operates with d line 589nm light source. Fisherbrand abbe benchtop refractometer abbe refractometers. Refractometer abbe 2waj lcd digital thermometer builtin. Use a pipet to apply your liquid sample to the prism, being careful not to let the glass pipet tip touch the prism since this may scratch the soft prism glass. To obtain the best possible accuracy, it is absolutely necessary to maintain the instrument. Two similar prisms x, y are placed on a table a, the prism x being hinged at h so that it could be swung away from y, a drop of the liquid is placed on the surface a, which is matt, and the prisms are placed together so that the liquid is squeezed into a thin film between. When light enters a dry prism, the field of view in an analog refractometer remains. Uncompromising precision at an economical cost meets astm d 141689, section 3846 requirements. The abbe refractometer is easily implemented in a compact design. Misco is the source for all your refractometer parts and accessories. Abbe refractometer the use and how it works duration.\nParts are available for the misco traditional analog refractometer, the palm abbe digital handheld refractometer, inlineprocess refractometers and sensors, as well as benchtoplaboratory refractometers. Thanks to consistent customer focus paired with smart ideas and. Digital abbe refractometers uses and applications of the abbe refractometers atago s abbe refractometers are widely used in a variety of fields. Ar 3 abbe refractometer same as ar 4, but without ledillumination for prism and scale. Abbe 60 refractometers abbe 60 direct reading refractometer more stringent requirements of quality control and, in some cases, legislation, mean that greater accuracy is being demanded of refractometers. It measures how the light is bent as it moves through a specific substance. Its original design was so successful that even as of today it is over 150 years old, it is still used and copied in new devices. Anton paars abbemat refractometer series ranges from the straightforward abbemat 3x00 series for routine analysis to the sturdy heavy duty line for measurements in harsh environments. Abbe refractometer definition of abbe refractometer by.\nAbbe 60 refractometers can be used to measure the refractive index of liquid or solid samples for a wide range of applications. Refractometer for laboratory and industry refractometry is generally speaking the measurement of refractive index and its interpretation under different starting points. Abbe s refractometer is used to measure the refractive index of the given organic liquid. These abbe refractometers provide highly accurate refractive index and brix measurements of liquids and solids. Working principle behind the abbe refractometer gulpmatrix. Easy to use operation consists of placing a sample on the prism.\nThe device was slowly changing while the general principle was and is up to today still the. Atagos abbe refractometers are highly reliable instruments which measure refractive index and brix scales. Laboratory refractometer for precisely measuring liquids, solids, semiliquids and powders. The mark iii refractometer is a high precision optical instrument. The refractometer is often stored with a piece of tissue in the prism assembly to keep the prism glass from being scratched. Model part number calibration test piece calibration tables 60 95 1003 glass, 1044 not supplied 60 dr 1099 glass, 1044 60 ed 1004 silica, 1046 10295 60 lr 1006 silica, 1046 10297. Abbe refractometers are not shown in hilgers 1914 general catalogue 35 general catalogue of the manufactures of adam hilger, ltd. The device is relatively inexpensive, accurate, requires little maintenance or calibration, uses. Zebra refractometer manual correcting for temperature if, during concentration measurement, the temperature is above or below 10f to that of the temperature during calibration, it is recommended to recalibrate the unit at the testing temperature. Ideal for measuring the refractive index of liquids, solids and sugar. Prism with large windows, equipped with movable shrouds to keep out light. Optical refraction index and brix scale reading system, with an excellent boundary line brightness and contrast. Atago can supply abbe refractometer with measurement range of nd1.\nA laboratory refractometer is an instrument used to measure the refractive index of a substance. Clean the prism face with acetone and carefully blot it dry with a kimwipe. Abbes refractometer, temperature controller, light source and samples. Fg measured with refractometer at room temperature was 1. Operating manual abbe refractometer kern ort 1rs analogue. Digital abbe refractometer, model dra1, atago 10dra1. To study the variation of refractive index with a temperature of the liquid sample. An abbe refractometer is a benchtop device for the highprecision measurement of an index of refraction. Abbe designed a refractometer for measuring the refractive index of liquids whose principle is illustrated as follows. The abbe mark iii has many advanced upgrades, including menudriven operation. Brix refractometer brix and refractive index scales sugar.\nThe refracting prism is made of a glass with a high refractive index e. In order to calculate abv given the challenge of using a refractometer, is the following approach correct. Despite the emergence of modernday digital models, these original refractometers remain irreplaceable due to their unique ability to measure solid samples, such as film, glass, and other transparent materials. This claim is the driving force for the development of our microscope and refractometer ranges. Just started using a refractometer, and trying to calculate abv using a correction calculator. The abbe 5 is an affordable refractometer ideally suited for use where a wide refractive index measurement range is required such as in small contract laboratories or applications where sample throughput is relatively low. The instrument is also ideal for practical demonstrations and experiments in. The abbe 60 direct reading models, available in two measuring ranges, have been designed to meet these requirements. This unit cannot be used to measure solid samples or calculate dispersion values. Check that the cooling water is flowing and record the water temperature on the precision thermometer to 0. It is based on the principle of total reflection which occurs at the boundary between the prism and the sample. Brix refractometer brix and refractive index scales.\nRefractometric measurements can be used for example for purity investigations, sample recognition, dilution control or composition. Since the refractive index of a material varies with its composition, it is useful in measuring the concentration of liquids i. The index of refraction is calculated from snells law while for mixtures, the index of refraction can be calculated from the composition of the material using several mixing rules such as the gladstonedale relation and lorentzlorenz equation. A refractometer is a scientific machine that measures the amount that light is bent or refracted when it moves from the air into a sample.\nConsult bellingham stanley, a xylem brands principles of refractometry brochure on directindustry. Quick manual atago usa online store for refractometer. A brief essay, the chemical refractometer, describes the. Wya2s characteristics characteristics z measurement of refractive index nd of transparent or translucent liquid and solid substances. Pa201 palm abbe digital refractometer 0 to 56 brix. That characteristic is represented by a value on a scale in units known as. The abbe2waj refractometer is a tabletop instrument to quickly and accurately determine the refractive index nd between 1,300 and 1,700 as well as the sugar content in liquids, dispersions, emulsions and other translucent substances. Applications ideal for measuring the refractive index of liquids, solids and sugar concentrations brix, industry. Description uk version 230v euro version 230v us version 110v. To find refractive index of the given liquid samples. Refractometer there are two types of handheld refractometers.\nThe abbe refractometer measures refractive indexes nd, solid contents in % and mean dispersion values nfnc of tranparent and translucent liquids or solids. Measuring the refractive index of a liquid 1 turn on the light. The abbe refractometer provides a quick and easy means for determining refractive index and dispersion of liquids and solids. Dualscale digital refractometer with scales for measuring both brix to 85 and refractive index to 1. Measuring abv with a refractometer community beeradvocate. Its original design was so successful that even as of today it is over. The refractometers listed above have the same scale as the nar2t. Multiwavelength abbe refractometers refractive index or abbe number vd or ve can be measured these refractometers digitallly display the measurement resu the boundary line at the intersection point of the cross hairs these units can be connected to the digital printer. For the next almost 40 years carl zeiss was the only producer of abbe refractometers. Circulator nozzles allow the instrument to be attached to a circulating bath to allow for accurate temperature control of primary and secondary prisms. Ar abbe refractometers ar2008 digital abbe refractometer.\nFor a recent batch, sg measured with refractometer at room temperature was 1. Reichert abbe mark iii refractometer the tradition of excellence continues. The abbe refractometer was developed in 1869 by ernst abbe and is used to determine the refractive index otherwise known as the index of refraction. The nar1tliquid is an abbe refractometer designed for customers who will only be measuring liquid samples at temperatures below 50c. This abbe refractometer serves for the determination of the refractive index and. The angle are measured relative to the normal of the. Uses and applications for measuring the refractive index nd of liquid samples between 5 to 50. Factors affecting ri measurement temperature of the liquid inverse relation for organic liquids ri decreases by approximately 0. The new palm abbe is a fourthgeneration digital handheld refractometer that puts laboratory precision in the palm of your hand. By very simple operation that needs only to set the boundary line of refraction at the cross hairs, this refractometer directly indicates a measured value in refractive index or brix %, selective in digits together with temperature on the display. Pycnometers made easythe best way to measure density. There are different types of refractometers, such as brix scale, salt refractometer, and abbe refractometer. To describe the procedure for operation and calibration of refractometer. The abbe refractometer ar12 is equipped with hose connections at the prism unit for the.\nOpt10 and opt32 models are temperature stabilized within a 18f range. The refractive index is the ratio of the sine of the angle of incidence of a ray of light in vacuum to the sine of the angle of refraction in the medium, which is equal to the ratio of the wave velocity of light in vacuum to the wave velocity in the medium. System maintenantce from noble company noble company. The reichert abbe mark iii is the new standardbearer of abbestyle manual refractometers. Atago abbe benchtop refractometers novatech international. They work on the principle that light entering a prism has a unique characteristic. These first instruments had builtin thermometers and. Abbe refractometers multiwavelength abbe refractometers.\nCompared with other types of refractometers, the abbe refractometers are capable of measuring liquid and solid samples such as plate glasses, plastic sheets, and other solid films. Sep 24, 2011 how to use a refractometer abbe duration. Refractometer for field analysis and for laboratory use. Ernst abbe 18401905, working for carl zeiss ag in jena, germany in the late 19th century, was the first to develop a laboratory refractometer. The abbe 2waj refractometer is a tabletop instrument to quickly and accurately determine the refractive index nd between 1,300 and 1,700 as well as the sugar content in liquids, dispersions, emulsions and other translucent substances. Alternatively, the performance line is ideal for quality control on a large number of samples. Catalog download adobe pdf for information on features, specifications and common applications of atago instruments, select the model below and click. Refractive index is a fundamental physical property of a solution and is related to the speed light travels through it. The performance plus line is designed for research and. Ar 5152 abbe refractometer equipment the same as the ar 4, but with flowthrough cell which can be temperaturecontrolled by a circulating thermostat bath. Refractometer principle pdf you should be familiar with the general principle of refraction. Adjust the micrometer screw to focus the boundary between the bright and dark regions. Atago s abbe refractometers are highly reliable instruments which measure refractive index and brix scales.\nAr 6 abbe refractometer same as ar 4, but with a digital thermometer. Using a particular monochromatic light source, the apparatus is calibrated with water as the liquid. Abbe refractometer was the first refractometer to be offered commercially see refractometer history for more details. Features apparatus equipped with an optical system made of a. Two similar prisms x, y are placed on a table a, the prism x being hinged at h so that it could be swung away from y, a drop of the liquid is placed on the surface a, which is matt, and the prisms are placed together so that the liquid is squeezed into a thin film between them. The refractometer is a measuring instrument for determining the refractive index of translucent liquids, dispersions and emulsions. Abbe refractometer model rmt laboratory refractometer for precisely measuring liquids, solids, semiliquids and powders. The precision and accuracy of a refractive index measurement is largely dependent upon operator skill, cleaning practices.468 1055 753 16 408 118 153 149 367 469 1179 935 328 1350 827 952 1378 341 495 448 1442 848 584 1158 1534 233 620 22 428 1213 393 298 1064 627 669 208 1468 872 1331']"	['<urn:uuid:55f79743-ac40-4fbe-972d-e4c981f81424>', '<urn:uuid:9692a1f7-5425-4eb7-833b-1baee4566efe>']	factoid	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T06:04:36.674696	26	62	3124
33	How do conservation principles apply to matter and energy interactions?	Both matter and energy follow fundamental conservation principles. The Law of Conservation of Matter states that matter cannot be created or destroyed, only changed and rearranged - in a chemical reaction, the number of each type of atom must remain constant. Similarly, the Law of Conservation of Energy, also known as the First Law of Thermodynamics, maintains that energy cannot be created or destroyed, only transferred or transformed between different forms. This principle applies in both physical changes, like temperature transfer between ice and a drink, and chemical reactions, where energy can be stored in chemical bonds or released during reactions. The total energy in the universe remains constant, though it can be transferred between a system and its surroundings during chemical processes.	"['General Chemistry/Properties of Matter/Changes in Matter\nThere are two types of change in matter: physical change and chemical change. As the names suggest, physical changes affect physical properties, and chemical changes affect chemical properties.\nChemical changes are also known as chemical reactions. The ""ingredients"" of a reaction are the reactants, and the end results are called the ""products"". The change from reactants to products can be signified by an arrow.\n- A Chemical Reaction\nReactants → Products\nNote that the number of reactants and products don\'t necessarily have to be the same. However, the number of each type of atom must remain constant. This is called the Law of Conservation of Matter. It states that matter can never be created or destroyed, only changed and rearranged. If a chemical reaction begins with 17 moles of carbon atoms, it must end with 17 moles of carbon atoms. They may be bonded into different molecules, or in a different state of matter, but they cannot disappear.\nWhen changes occur, energy is often transformed. However, like atoms, energy cannot disappear. This is called the Law of Conservation of Energy. A simple example would be putting ice cubes into a soft drink. The ice cubes get warmer as the drink gets colder, because energy cannot be created or destroyed, only transferred. Note that energy can be ""released"" or ""stored"" by making and breaking bonds. When a plant converts the energy from sunlight into food, that energy is stored in the chemical bonds within the sugar molecules.\nChemical or Physical?\nPhysical changes do not cause a substance to become a fundamentally different substance. Chemical changes, on the other hand, cause a substance to change into something entirely new. Chemical changes are typically irreversible, but that is not always the case. It is easier to understand the difference between physical and chemical changes with examples.\n|State changes are physical.||Phase changes are when you melt, freeze, boil, condense, sublimate, or deposit a substance. They do not change the nature of the substance unless a chemical change occurs along with the physical change.|\n|Cutting, tearing, shattering, and grinding are physical.||These may be irreversible, but the result is still composed of the same molecules. When you cut your hair, that is a physical change, even though you can\'t put the hair back on your head again.|\n|Mixing together substances is physical.||For example, you could mix salt and pepper, dissolve salt in water, or mix molten metals together to produce an alloy.|\n|Gas bubbles forming is chemical.||Not to be confused with bubbles from boiling, which would be physical (a phase change). Gas bubbles indicate that a chemical reaction has occurred.|\n|Precipitates forming is chemical.||When dissolved substances are mixed, and a cloudy precipitate appears, there has been a chemical change.|\n|Rotting, burning, cooking, and rusting (for example) are chemical.||The resulting substances are entirely new chemical compounds. For instance, wood becomes ash and heat; iron becomes rust; sugar ferments into alcohol.|\n|Changes of color or release of odors (i.e. release of a gas) might be chemical.||As an example, the element chromium shows different colors when it is in different compounds, but a single chromium compound will not change color on its own without some sort of reaction.|\n|Release/absorption of energy (heat, light, sound) is generally not easily categorized.||Hot/cold packs involve dissolving a salt in water to change its temperature (more on that in later chapters); popping popcorn is mostly (but not completely).|', 'Watching this resources will notify you when proposed changes or new versions are created so you can keep track of improvements that have been made.\nFavoriting this resource allows you to save it in the “My Resources” tab of your account. There, you can easily access this resource later when you’re ready to customize it or assign it to your students.\nThermodynamics is the study of heat energy and other types of energy, such as work, and the various ways energy is transferred within chemical systems. ""Thermo-"" refers to heat, while ""dynamics"" refers to motion.\nThe First Law of Thermodynamics\nThe first law of thermodynamics deals with the total amount of energy in the universe. The law states that this total amount of energy is constant. In other words, there has always been, and always will be, exactly the same amount of energy in the universe.\nEnergy exists in many different forms. According to the first law of thermodynamics, energy can be transferred from place to place or changed between different forms, but it cannot be created or destroyed. The transfers and transformations of energy take place around us all the time. For instance, light bulbs transform electrical energy into light energy, and gas stoves transform chemical energy from natural gas into heat energy. Plants perform one of the most biologically useful transformations of energy on Earth: they convert the energy of sunlight into the chemical energy stored within organicmolecules.\nThermodynamics often divides the universe into two categories: the system and its surroundings. In chemistry, the system almost always refers to a given chemical reaction and the container in which it takes place. The first law of thermodynamics tells us that energy can neither be created nor destroyed, so we know that the energy that is absorbed in an endothermic chemical reaction must have been lost from the surroundings. Conversely, in an exothermic reaction, the heat that is released in the reaction is given off and absorbed by the surroundings. Stated mathematically, we have:\nWe know that chemical systems can either absorb heat from their surroundings, if the reaction is endothermic, or release heat to their surroundings, if the reaction is exothermic. However, chemical reactions are often used to do work instead of just exchanging heat. For instance, when rocket fuel burns and causes a space shuttle to lift off from the ground, the chemical reaction, by propelling the rocket, is doing work by applying a force over a distance.\nIf you\'ve ever witnessed a video of a space shuttle lifting off, the chemical reaction that occurs also releases tremendous amounts of heat and light. Another useful form of the first law of thermodynamics relates heat and work for the change in energy of the internal system:\nWhile this formulation is more commonly used in physics, it is still important to know for chemistry.\nenergy can be transferred and destroyed, but may not be transformed., energy cannot be created or destroyed, but can be transferred or transformed., energy can be created, destroyed, transferred, and transformed., or energy in the universe is constant; therefore, destroying it would lead to its re-creation.\nSource: Boundless. “The First Law of Thermodynamics.” Boundless Biology. Boundless, 21 Jul. 2015. Retrieved 28 Nov. 2015 from https://www.boundless.com/biology/textbooks/boundless-biology-textbook/metabolism-6/potential-kinetic-free-and-activation-energy-69/the-first-law-of-thermodynamics-347-11484/']"	['<urn:uuid:4a1d1249-305e-4aad-9ccd-95b6294cdcdf>', '<urn:uuid:26da7408-f300-45d1-a192-3ee41d2ba20c>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T06:04:36.674696	10	123	1109
34	why is stirring important mixing	Stirring is important because it leads to increased mixing. Due to stirring, turbulence causes an increase of effective mixing and dissipation by several orders of magnitude.	['From the Navier-Stokes equations\nvia the Reynolds decomposition\nto a working turbulence closure model\nfor the shallow water equations:\nThe compromise between complexity and pragmatism.\nLeibniz Institute for\nBaltic Sea Research Warnemünde\nWhy are we stirring our cup of coffee?\nMilk foam: light, because of foam and fat\nCoffee: relatively light, because hot\nMilk: less light, because colder than coffee\nWhy the spoon?\n…OK, and why the coocky?\nlittle stirring strong stirring\nFrom stirring to mixing …\nTea mixing (analytical solution)\nPut 50% of milk into tea.\nLet m(z) be the milk fraction with m=1 at the bottom\nand m=0 at the surface.\nWith a constant mixing coefficient,\nthe m-equation is this:\nLet us take the spoon\nand stir the milk-tea mix\nn-times such that we\nget a sinosodial milk-tea\nvariation in the vertical\nand then see the\nresulting mixing after 1 min:\nConclusion: stirring leads to increased mixing.\nSet of equations that describes turbulent mixing\nNavier-Stokes equations (for velocity vector u1, u2, u3):\n6 equations for 6 unknowns (u1, u2, u3, p, , )\nEquation of state:\nExample for solution of Navier-Stokes equations (KH-instability)\nDirect Numerical Simulation (DNS) by William D. Smyth, Oregon State University\nTo reproduce system-wide mixing, the smallest dissipative scales\nmust be resolved by numerical models (DNS).\nThis does not work in models for natural waters due to limited\ncapacities of computers.\nTherefore, the effects of turbulence needs to be partially\n(= Large Eddy Simulation, LES) or fully (Reynolds-averaged\nNavier-Stokes, RANS) parametersised.\nHere, we go for the RANS method, which means that small-scale\nfluctuations are „averaged away“, i.e., it is only the expected\nvalue of the state variables considered and not the actual value.\nReynolds decomposition (with synthetic tidal flow data)\nAny turbulent flow can be decomposed\ninto mean and fluctuating components:\nThere are many ways to\ndefine the mean flow, e.g.\ntime averaging (upper panel)\nor ensemble averaging (lower\nFor the ensemble averaging,\na high number N of\nexperiments is carried out\nand then the mean of those\nresults is taken. The limit\nfor N is then the\nensemble average (which is\nthe physically correct one).\nFor the ensemble average 4 basic rules apply:\nThe Reynolds equations\nThese rules can be applied to derive a balance equation\nfor the ensemble averaged momentum.\nThis is demonstrated here for a simplified (one-dimensional)\nThe Reynolds stress constitutes a new unknown\nwhich needs to be parameterised.\nThe eddy viscosity assumption\nReynolds stress and\nmean shear are\nto each others:\nThe eddy viscosity assumption\nThe eddy viscosity is typically orders of magnitude larger\nthan the molecular viscosity. The eddy viscosity is however\nunknown as well and highly variable in time and space.\nParameterisation of the eddy viscosity\nLike in the theory of ideal gases, the eddy viscosity can be assumed\nto be proportional to a characteristic length scale l and a velocity scale v:\nIn simple cases, the length scale l could be taken from geometric arguments\n(such as being proportional to the distance from the wall). The velocity scale\nv can be taken as proportional to the square root of the turbulent\nkinetic energy (TKE) which is defined as:\n(cl = const)\nDynamic equation for the TKE\nA dynamic equation for the turbulent kinetic energy (TKE) can be derived:\nP: shear production\nB: buoyancy production\ne: viscous dissipation\nDynamic equation for the length scale (here: e eq.)\nA dynamic equation for the dissipation rate of the TKE) is constructed:\nwith the adjustable empirical parameters c1, c2, c3, se.\nWith this, it can be calculated\nstability functions cm and cm‘.\nAll parameters can be calibrated to characteristic properties of the flow.\nExample on next slide: how to calibrate c3.\nLayers with homogeneous stratification and shear\nFor stationary & homogeneous stratified shear flow,\nOsborn (1980) proposed the following relation:\nwhich is equivalent to\n(N is the buoyancy frequency),\na relation which is intensively used to derive the\neddy diffusivity from micro-structure observations.\nFor stationary homogeneous shear layers, the k-e model reduces to\nwhich can be combined to\nThus, after having calibrated c1 and c2,\nc3 adjusts the effect of stratification on mixing.\nUmlauf (2009), Burchard and Hetland (2010)\nMixing = micro-structure variance decay\nExample: temperature mixing\nTemperature variance equation:\nSecond-moment closures in a nut shell\nInstead of directly imposing the eddy viscosity assumption\none could also derive a transport equation for\nand the turbulent heat flux (second moments). These second-moment\nequations would contain unknown third moments, for which also equations could\nbe derived, etc. The second-moments are closed by assuming local\nequilibrium (stationarity, homogeneity) for the second moments. Together\nwith further emipirical closure assumptions, a closed linear system of equations\nwill then be found for the second moments. Interestingly, the result may be\n, where now cm and cm‘ are\nformulations as follows:\nwith the shear squared, M2.\nSuch two-equation second moment-closures are now the workhorses in\ncoastal ocean modelling (and should be it in lake models) and have been\nconsistently implemented in the one-dimensional\nGeneral Ocean Turbulence Model (GOTM)\nwhich has been released in 1999 by Hans Burchard and Karsten Bolding\nunder the Gnu Public Licence. Since then, it had been steadily\ndeveloped and is now coupled to many ocean models.\nGOTM application: Kato-Phillips experiment\nGOTM application: Baltic Sea surface dynamics\nReissmann et al., 2009\nDue to stirring, turbulence leads to an increase of effective\nmixing and dissipation by several orders of magnitude.\nFor simulating natural systems, the Reynolds decomposition into\nmean (=expected) and fluctuating parts is necessary.\nHigher statistical moments are parameterised by means of\nturbulence closure models.\nAlgebraic second-moment closures provide a good compromise\nbetween efficiency and accuracy. Therefore such models are\nideal for lakes and coastal waters.\nQuestion: Will we be able to construct a robost and more\naccurate closure model which resolves the second moments\n( inclusion of budget equations for momentum and heat flux)?']	['<urn:uuid:b8f5ed2b-546d-4f89-b2db-3d7cf1c87048>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T06:04:36.674696	5	26	969
35	buddhist teachings adaptation west challenges changes	The adaptation of Buddhism in the West has faced several challenges and transformations. While maintaining core teachings, Western Buddhism has moved away from traditional Asian models where monks are the primary practitioners. Western practitioners are increasingly taking what they need rather than strictly adhering to traditional teachings. The movement has also had to navigate complex cultural elements, particularly in Tibetan Buddhism, which brought along sectarian disputes, shamanism, and the tulku system. A key development has been the emergence of serious non-monastic practitioners who drive various Buddhist movements, demonstrating that Westerners can effectively practice Buddhism while adapting it to their cultural context.	"['The first authoritative volume on the totality of Buddhism in the West, Westward Dharma establishes a comparative and theoretical perspective for considering the amazing variety of Buddhist traditions, schools, centers, and teachers that have developed outside of Asia. Leading scholars from North America, Europe, South Africa, and Australia explore the plurality and heterogeneity of traditions and practices that are characteristic of Buddhism in the West.\nThis recent, dramatic growth in Western Buddhism is accompanied by an expansion of topics and issues of Buddhist concern. The contributors to this volume treat such topics as the broadening spirit of egalitarianism; the increasing emphasis on the psychological, as opposed to the purely religious, nature of practice; scandals within Buddhist movements; the erosion of the distinction between professional and lay Buddhists; Buddhist settlement in Israel; the history of Buddhism in internment camps; repackaging Zen for the West; and women\'s dharma in the West. The interconnections of historical and theoretical approaches in the volume make it a rich, multi-layered resource.\nList of Tables\nIntroduction: Paying Homage to the Buddha in the West\nMartin Baumann and Charles S. Prebish\nPART I: PROFILING GLOBAL BUDDHISM: A DESCRIPTION OF THE LANDSCAPE\n1. Who Is a Buddhist? Night-Stand Buddhists and Other Creatures\nThomas A. Tweed\n2. The Spectrum of Buddhist Practice in the West\nB. Alan Wallace\n3. Protective Amulets and Awareness Techniques, or How to Make Sense of Buddhism in the West\n4. Studying the Spread and Histories of Buddhism in the West: The Emergence of Western Buddhism as a New Subdiscipline within Buddhist Studies\nCharles S. Prebish\nPART II: DIFFUSION: THE HISTORIES OF BUDDHISM IN WESTERN COUNTRIES\n5. Buddhism in Europe: Past, Present, Prospects\n6. American Buddhism in the Making\nRichard Hughes Seager\n7. Buddhism in Canada\n8. The Development of Buddhism in Australia and New Zealand\n9. Buddhism in South Africa\n10. Buddhism in Brazil and Its Impact on the Larger Brazilian Society\n11. Buddha in the Promised Land: Outlines of the Buddhist Settlement in Israel\nPART III: CHANGE: ADAPTATIONS AND INNOVATIONS\n12. Camp Dharma: Japanese-American Buddhist Identity and the Internment Experience of World War II\nDuncan Ryuken Williams\n13. The Translating Temple: Diasporic Buddhism in Florida\nDouglas M. Padgett\n14. Repackaging Zen for the West\nDavid L. McMahan\n15. Scandals in Emerging Western Buddhism\nPART IV: LIFESTYLE: BEING A BUDDHIST IN WESTERN SOCIETIES\n16. The Challenge of Community\n17. Buddhist Nuns: Changes and Challenges\nKarma Lekshe Tsomo\n18. Neither Monk nor Nun: Western Buddhists as Full-Time Practitioners\n19. Virtues without Rules: Ethics in the Insight Meditation Movement\nPART V: BUDDHISM FACING NEW CHALLENGES\n20. The Roar of the Lioness: Women\'s Dharma in the West\n21. Engaged Buddhism: Agnosticism, Interdependence, Globalization\nChristopher S. Queen\n22. The Encounter of Buddhism and Psychology\nFranz Aubrey Metcalf\n23. A ""Commodius Vicus Recirculation:: Buddhism, Art, and Modernity\nList of Contributors\nCharles S. Prebish is Professor of Religious Studies at Pennsylvania State University and author and editor of numerous works, including Luminous Passage: The Practice and Study of Buddhism in America (California, 1999), A Survey of Vinaya Literature (1994), and American Buddhism (1979). He is coeditor of The Faces of Buddhism in America (California, 1998) and the electronic Journal of Buddhist Ethics. Martin Baumann is Professor of the History of Religions at the University of Lucerne, Switzerland, and research fellow in the Department of Religious Studies at Hannover University, Germany. He is the author of Diaspora: Hindus and Trinidad (2002), Migration, Religion, Integration (2000), and Deutsche Buddhisten: Geschichte and Gemeinschaften (1993). He is also coeditor of Religions fo the World: A Comprehensive Encyclopedia of Beliefs and Practices (2002). Prebish and Buamann are coeditors of the electronic Journal of Global Buddhism (http://www.globalbuddhism.org).\n“. . . We should be grateful to Charles Prebish and Martin Baumann for cutting and pasting this edited volume, which by tracking the advance of the dharma into the West advances the cause of Buddhist scholarship.”—Buddhadharma\n""Like seeds on the wind, Buddhist teachings continue to reach new lands. This outstanding book brings to light, in rich detail, the current flowering of Buddhism in the West. Long a world religion, Buddhism is now a global one.""—Kenneth Kraft, author of The Wheel of Engaged Buddhism\ndeserves a place on the growing bookshelf of contemporary Buddhist studies. Prebish and Baumann broaden our horizons from North America to the wider Western world, exploring key aspects of Buddhism\'s most recent geographical and cultural expansion.""—Paul David Numrich, coauthor of Buddhists, Hindus, and Sikhs in America.\nList of Contributors Martin Baumann is Professor of the History of Religions at the University of Lucerne, Switzerland, and research fellow in the Department of Religious Studies at Hannover University, Germany. His fields of interest are the spread and adaptation of Buddhist and Hindu traditions outside of Asia, diaspora and migrant studies, and the theory and method in the history of religions. He has published extensively on these topics, in both German and English, and is editor of the online Journal of Global Buddhism (http://www.globalbuddhism.org).\nSandra Bell is a lecturer in the Department of Anthropology at the University of Durham, UK. She has published papers on the expansion and development of Buddhism in the West in the Journal of Contemporary Religion; Novo Religio: the Journal of Alternative and Emergent Religions and other academic journals dealing with religion. She has also written on gender and sexuality in Therav&da Buddhism. She is co-editor of The Anthropology of Friendship: Community Beyond Kinship, with Simon Coleman (Oxford: Berg, 1999) and Celibacy, Culture and Society: The Anthropology of Sexual Abstinence, with Elisa Sobo (University of Wisconsin Press, in press).\nMichel Clasquin lectures in the Department of Religious Studies at the University of South Africa. A practicing Buddhist since 1984, he is currently not affiliated to any Buddhist group.\nGil Fronsdal teaches at the Sati Center for Buddhist Studies in Palo Alto, California. He has a doctorate in Buddhist Studies from Stanford University. He is a Buddhist meditation teacher teaching at the Insight Meditation Center of the Mid-Peninsula, also in Palo Alto and at Spirit Rock Meditation Center in Marin County, California.\nIan Harris was educated at the Universities of Cambridge and Lancaster. He is a Reader in Religious Studies at St. Martin\'s College, Lancaster and co-founder (with Peter Harvey) of the UK Association for Buddhist Studies. The author of The Continuity of Madhyamaka and Yogacara in Early Mahayana Buddhism (1991) and editor of Buddhism and Politics in Twentieth Century Asia (1999), he has written widely on Buddhism in the modern world with particular reference to environmental ethics. He is currently working on a study of Buddhism in Cambodia, to be published by the University of Hawaii Press in 2001.\nBruce Matthews is the C.B. Lumsden Professor of Comparative Religion at Acadia University, Nova Scotia. A former Commonwealth Fellow in P&li and Buddhist Civilization at the University of Ceylon, Peradeniya, he completed his graduate studies at McMaster University, Ontario. The author of Craving and Salvation: A Study of Buddhist Soteriology (1983) and many articles, his research now focuses on Buddhism and politics in Sri Lanka and Myanmar.\nDavid L. McMahan is an Assistant Professor in the Department of Religious Studies at Franklin & Marshall College. He has also taught at the University of California, Santa Barbara and the University of Vermont. He is the author of Empty Vision: Metaphor and Visionary Imagery in Mah&y&na Buddhism (Curzon Press, 2001); ""New Frontiers in Buddhism: Three Recent Works on Buddhism in America,"" Journal of Global Buddhism 1 (2000); ""Orality, Writing and Authority in South Asian Buddhism: Visionary Literature and the Struggle for Legitimacy in the Mahayana,"" History of Religions 37, no. 3, 1998; and ""A Long Awaited Call: A Buddhist Response,"" in Ethics and World Religions: Cross-Cultural Case Studies, edited by Regina Wentzel Wolfe and Christine E. Gudorf (Maryknoll: Orbis Books, 1999).\nFranz Metcalf received his Ph.D. from the Divinity School at the University of Chicago, writing his dissertation on American Zen practice. He has published scholarly work on both contemporary Buddhism and the psychology of religion. He is book review editor for the Journal of Global Buddhism; co-chairs the Person, Culture, and Religion Group of the American Academy of Religion; and teaches comparative religion at California State University, Los Angeles. Franz is a founding member of the Forge Institute for Spirituality and Social Change, and author of two books applying Buddhist teachings to contemporary life.\nLionel Obadia is lecturer in anthropology in the Charles-de-Gaulle University of Arts and Social Sciences at Lille (France). He is the author of Bouddhisme et Occident. La diffusion du bouddhisme tibétain en France. (L\'harmattan, 1999) and of a general introduction Le Bouddhisme en Occident (Presses Universitaires de France, forthcoming 2001). He has published articles, reviews and edited the special issue Le bouddhisme en Occident: approches sociologique et anthropologique of the Belgian journal Recherches Sociologiques (2000/3).\nDouglas M. Padgett is a Ph.D. candidate in the study of religion at Indiana University, where he has concentrated on East Asian Buddhism and issues relating to identity, migration and religion,. He is currently working on a dissertation on Buddhism in Vietnam and in overseas Vietnamese communities.\nCharles S. Prebish is Professor of Religious Studies at the Pennsylvania State University. He is the author of eleven books, including the recently published Luminous Passage: The Practice and Study of Buddhism in America. He is a founding Co-Editor of the online Journal of Buddhist Ethics and Journal of Global Buddhism. He is a former officer and member of the board of directors of the International Association of Buddhist Studies, and has held the Numata Chair of Buddhist Studies at the University of Calgary, as well as a Rockefeller Foundation National Humanities Fellowship at the Centre for the Study of Religion at the University of Toronto.\nChristopher S. Queen is the dean of students for continuing education and lecturer on the study of religion in the Faculty of Arts and Sciences, Harvard University. He has edited and contributed to Engaged Buddhism in the West (2000), American Buddhism: Methods and Findings in Recent Scholarship (with Duncan Ryjken Williams, 1999), and Engaged Buddhism: Buddhist Liberation Movements in Asia (with Sallie B. King, 1996). He is currently working on a monograph on the life and teachings of B. R. Ambedkar and the rise of engaged Buddhism.\nRichard Seager holds a Ph.D. in the Study of Religion from Harvard University. He has written The World\'s Parliament of Religions: The East/West Encounter, Chicago 1893 (Indiana University Press) and Buddhism in America (Columbia University Press), and is currently Associate Professor of Religion at Hamilton College in Clinton, New York.\nJudith Simmer-Brown, Ph.D., is chair of the Religious Studies department at N&ropa University, where she has been a faculty member since 1978. She is also an &c&rya (senior teacher) in the Shambhala Buddhist lineage of Chögyam Trungpa, Rinpoche of Tibetan Kagyü/ and Nyingma Buddhism. In 1981, she organized the very first symposium which launched conversations on women in Western Buddhism, and has been part of the conversation between communities ever since. She writes on American Buddhism, Buddhist-Christian Dialogue, women and Buddhism, and is author of Dakini\'s Warm Breath: The Feminine Principle in Tibetan Buddhism (Boston: Shambhala Publications, 2001).\nMichelle Spuler is an Assistant Professor at Colorado College. She formerly taught in the Department of Religious Studies at the Victoria University of Wellington, New Zealand. She received her Ph.D. on the development of Zen Buddhism in Australia from the University of Queensland in Brisbane, Australia. She is currently working on a book based on her doctoral thesis.\nAjahn Tiradhammo was born in Canada in 1949. He was introduced to Buddhism at university and, during a trip to South Asia in 1971, learned meditation in Sri Lanka. He then travelled to Thailand in 1973 to further meditation practice. There he undertook ordination as a novice and, in 1974, as a bhikkhu. Seeking further meditation guidance he studied with Ajahn Chah for 6 years before moving to the western branch monastery in Britain under Ajahn Sumedho. He spent 6 years as one of the senior monks in several of the branches in Britain then, in 1988, helped establish a new monastery in Switzerland where he has been resident for the last 12 years.\nKarma Lekshe Tsomo is Assistant Professor of Theology and Religious Studies at University of San Diego. An American nun practicing in the Tibetan tradition, she is secretary of Sakyadhita: International Association of Buddhist Women and has been active in the international Buddhist women\'s movement. She completed her doctorate in Philosophy at the University of Hawai\'i with a dissertation on death and identity in China and Tibet, and has edited a number of books on women in Buddhism, including Sakyadhita: Daughters of the Buddha, Buddhism Through American Women\'s Eyes, Innovative Buddhist Women: Swimming Against the Stream, Sisters in Solitude: Two Traditions of Monastic Ethics for Women, and Buddhist Women Across Cultures: Realizations.\nThomas Tweed is Professor of Religious Studies at the University of North Carolina at Chapel Hill, where he also serves as Associate Dean for Undergraduate Curricula. He has authored or edited four books on religion in North America. He edited Retelling U.S. Religious History (University of California Press, 1997) and co-edited, with Stephen Prothero, Asian Religions in America: A Documentary History (Oxford University Press, 1999). He also wrote The American Encounter with Buddhism, 1844—1912: Victorian Culture and the Limits of Dissent, which was originally published in 1992 and recently reissued in a revised edition with a new preface (University of North Carolina Press, 2000). His ethnographic study, Our Lady of the Exile: Diasporic Religion at a Cuban Catholic Shrine in Miami (Oxford University Press, 1997), won the American Academy of Religion\'s Award for Excellence.\nFrank Usarski, Ph.D., between 1988 and 1997 lectured at German universities in Hannover, Oldenburg, Bremen, Erfurt, Chemnitz and Leipzig Since 1998 he has held an appointment as a long-term visiting professor at the Pontifical Catholic University (PUC) of São Paulo, Brazil. Among the projects he has undertaken at the PUC was to found a research group on Buddhism in Latin America, which is current focusing its investigations on the history and contemporary situation of Buddhism in Brazil.\nB. Alan Wallace began his formal studies of Tibetan Buddhism and language in Germany in 1970, and he has been teaching Tibetan Buddhist philosophy and meditation in Europe and America since 1976. Ordained as a Buddhist monk for fourteen years, he has trained under and served as interpreter for many eminent Tibetan scholars and contemplatives, including His Holiness the Dalai Lama. He completed his undergraduate education at Amherst College, where he studied physics and the philosophy of science; and he earned his doctorate in Religious Studies at Stanford University, where he pursued interdisciplinary research into ways of exploring the nature of consciousness. He has written, translated, edited, or contributed to over thirty books on many facets of Tibetan Buddhism and culture, as well as the interface between religion and science. He presently teaches courses on Tibetan Buddhism and the interface between science and religion in the Department of Religious Studies at the University of California, Santa Barbara.\nSylvia Wetzel (born 1949) is a high school teacher for politics and language, and since 1977, for the study of Buddhism. She worked for twenty years in Buddhist institutions: centers, publishing companies, as a board member of the German Buddhist Union (DBU), editor of Buddhist quarterly Lotusblätter, as a fulltime lay teacher of relaxation, meditation and Buddhism in Europe, founding member of Sakyadhita (Bodhgaya 1987), Network of Western Buddhist Teachers (Dharamsala 1993), and Women Awake (Köln 1999). She is currently initiating a non-profit ""International Buddhist Academy"" (Berlin (2000).\nDuncan Ryjken Williams received his Ph.D. in the study of Buddhism at Harvard University and is currently Assistant Professor of Japanese Religions and Culture at Trinity College. His research interest include medieval and early modern Japanese Zen Buddhism, the relationship between Buddhism and hot springs, and the history of Japanese American Buddhism. His recent publications include two co-edited volumes: American Buddhism: Methods and Findings in Recent Scholarship (Curzon Press, 1998) and Buddhism and Ecology (Harvard University Press, 1997).', 'Home Dharma Dew\nHow western Buddhism has changed in 50 years\nby Vishvapani Blomfield, The Guardian, March 16, 2012\nA western Buddhist shares 10 insights into how the religion and its followers have moved on since its arrival in the west\nLondon, UK -- It\'s 50 years since Buddhist teachers started arriving in the west in the early 60s and Buddhism crash-landed into the counterculture. So what have we learned about western Buddhism?\n<< Gathering of the largest number of American Buddhist teachers at any one time, held in June 2011 at the Garrison Institute. Photo courtesy of Roshi Joan Halifax\n1. It\'s not all about enlightenment. Many who found Buddhism in the 60s saw nirvana as the ultimate peak experience. A decade later these recovering hippies were painfully finding out that Buddhism is more concerned with reshaping character and behaviour than big, mystical experiences. Younger Buddhists are often more fired by social action than mysticism.\n2. It doesn\'t focus on monks. In most Asian countries Buddhist monks are the real practitioners, focusing on meditation and study while lay people support them. Distinctions between monks and lay people does not fit in with modern society and western monastic orders are relatively scarce. Non-monastic practitioners are often very serious and they power the various Buddhist movements.\n3. Tibetan Buddhism has baggage. Tibetan lamas arriving in the 1970s seemed to fulfil our Shangri-La fantasies. But, along with inspiration and wisdom, they also brought sectarian disputes, shamanism, the ""reincarnate lama"" (tulku) system, tantric practices and deep conservatism. Westerners love Tibetans, but we notice the baggage.\n4. The schools are mixing together. Most Asian Buddhist teachers assumed they would establish their existing schools in western countries. Hence we have western Zen, western Theravada etc. But the boundaries are breaking down as western Buddhists, motivated by common needs, explore the whole Buddhist tradition. The emerging western Buddhist world is essentially non-denominational.\n5. People take what they need, not what they\'re given. For all the talk of lineage, transmission and the purity of the teachings, western Buddhism is driven by students\' needs as much as teachers\' wishes.\n6. Mindfulness is where Buddhism and the west meet. Buddhist mindfulness practices are being applied to everything from mental health treatments to eating out, and we\'re now seeing a ""mindfulness boom"". These approaches apply core Buddhist insights to modern living, making this the biggest development in western Buddhism since the 1960s. It will probably shape the next 50 years.\n7. But it\'s not the only meeting point. The mindfulness movement is hyped as the ""new Buddhism for the west"". But, unless you\'re following the noble onefold path, there\'s more to Buddhism than mindfulness. Buddhist influence on western culture is strong in the arts, social action, environmentalism, psychotherapy and practitioners\' lives.\n8. Westerners can meditate and maybe even get enlightened. Numerous Buddhists I know who have been practising for several decades have made the teachings their own. Westerners can definitely do Buddhism, and are its future.\n9. But sex doesn\'t go away. Scandals and anguished life stories show that, even for people who prize celibacy, sex doesn\'t go away. Is this really a surprise?\n10. And we still don\'t know if western Buddhism is secular or religious. A growing movement (as Julian Baggini has discussed) wishes to strip Buddhism of ""superstitious"" elements such as karma and rebirth to distil a secular Buddhism that\'s compatible with science. That raises a big question: does following science mean ditching enlightenment? Is Buddhism an alternative source of authority that challenges the west? Ask me again in 50 years.']"	['<urn:uuid:46e1055a-aa23-4988-bdf2-68cdbdde5fe8>', '<urn:uuid:3161123c-037e-439e-bfe8-e23fb955a35f>']	open-ended	with-premise	short-search-query	distant-from-document	three-doc	expert	2025-05-13T06:04:36.674696	6	101	3261
36	I'm designing a comprehensive emergency response course and need insights on both professional and public preparedness. What are the essential training elements needed for both medical professionals and community members to improve cardiac arrest outcomes?	For medical professionals, realistic training is essential - practicing full two-minute CPR cycles and using actual medication calculations rather than accelerated scenarios. They should train with tools like the Broselow tape and practice drawing up proper medication doses. For community members, basic awareness courses covering CPR, AED usage, and bandaging techniques can be completed in 4 hours to 3 days. The public should learn the locations of AEDs, especially in high-traffic areas like MRT stations, and understand basic emergency responses. Both professionals and community members can access additional training through resources like the SCDF website, CEPP programs, and the Resuscitation Academy.	"['First Aid at home, this is a life skill which can be crucial during an emergency. Anyone with simple knowledge, or even awareness can become a life-saver during an emergency.\nSome of the common scenarios which one might face in our day to day encounters (there are many more depictions in TV dramas)\n-Sprains and fractures\n-Punctures and wounds\nMany of the dangers could be alleviated if First Aid is rendered promptly.\nApplying CPR (Cardiopulmonary Resuscitation) and using an AED (Automated External Defibrillator) effectively can save lives. Any help rendered before an ambulance arrives might prevent a situation from deteriorating.\n(In worse case scenario, a paramedic can guide you over the phone on how to react or perform CPR)\nDuring my First Aid course, we knew that someone with Cardiac Arrest (Heart attack) only has 10 minutes before he cannot be resuscitated.\nIf the heart is not pumping blood, brain will suffer permanent tissue damage after 4 to 6 minutes.\n*CPR is crucial to keep the heart alive for blood circulation.\nWith the SGSecure campaign, many are aware of “Run, Hide and Tell”. My kids can tell me about sms 71999.\nBut not many know about “Press, Tie, Tell”.\nIn many scenarios, if blood is flowing out, we must apply Pressure to stop bleeding first. More details on SGSecure website.\nFirst Aid at home, I invited a few models to refresh my skills.\nWe have heard how AED can help shock a heart to pump again. But you need to know where to find an AED first.\nI did notice there are more AEDs now, especially high traffic areas like MRT stations.\nUseful Apps and Links, FirstAid courses\nFor additional tips, I can refer to SCDF’s mySCDF app or YouTube.\nMy little fire fighter at home. I hope we do not need to fight a fire at home one day, but it is crucial for my family to know how to evacuate during an emergency.\nPickup First Aid skills if you have not already done so. Checkout SCDF website for latest updates, click on CEPP (Community Emergency Preparedness Programme) link to register.\nSCDF or commercial CPR courses duration can last from 4 hours to full 3 days (from basic to comprehensive with certification), but the basic awareness (CPR, AED, Bandaging hands on) course should be sufficient for most.\nSome of the safety posts from our previous experiences are shared below.\n-SCDF’s EPC (first hand experience on safety)\n-Learning First Aid with family (PA)\n-First Aid Kit and Fire evacuation\n-SGSecure campaign (Hands on CPR and first aid tips)\n-Saturday Fire Station open house\nAED-Automated external defibrillator\nAmbulance and Fire-995\n* The tips above are only meant to raise awareness for First Aid\n** We will never know whether we are ready to apply our skills, until we are faced with a situation. Actual incident scene might traumatised many First Aiders too ( imagine bloody wounds or amputations ). But if we do not act, a life might be lost.\n*** Salute to all the paramedics out there!\nIf you see an ambulance coming up behind you, give way promptly. You do not need to wait till the ambulance is tailgating you!\n**** If you have more tips, do share with us', 'Improve cardiac arrest survival: 10 tips every medic needs to follow\nFollow these tips to improve patient outcomes in sudden cardiac arrest\nSince 1974, Seattle and King County has been known as the best place to have a heart attack. In 2013, the survival rate for someone in King County who suffered a witnessed cardiac arrest and presented with the shockable rhythms of ventricular tachycardia or ventricular fibrillation reached 62 percent, and patients found in pulseless electrical activity or asystole had a remarkable resuscitation rate of 21percent. Unfortunately, the cardiac survival rates in other cities are often in the single digits.\nMy most recent ACLS refresher, while working at Yosemite National, was taught by Tod Levesh and James Kellogg, two King County Medic One paramedics.\nHere are 10 tips from Levesh and Kellogg shared on how we can increase cardiac arrest survival.\n1. Approach every cardiac arrest patient with a positive attitude\nPeople survive cardiac arrest! Yes, even a patient in PEA or asystole has a chance. However, some EMS providers fall into the trap of reducing their expectations when encountering those rhythms that have expected lower survivability. Remember: you are the patient’s best hope for survival. The statistics may say that resuscitation likelihood is low, but your positive attitude and astute medical practice will make a difference.\n2. Think several steps ahead\nA cardiac arrest is a dynamic scenario. Cardiac rhythms change during a resuscitation, requiring different treatments. Furthermore, the needs of a resuscitated patient are far different from the needs of a patient in cardiac arrest.\nAs you work the patient, predict what may be needed 4 to 6 minutes in the future. Steps that greatly enhance the efficiency of your call include preplanning your move to the transporting unit, drawing up a few medications to have on standby, staging airway management equipment or personnel for extricating the patient. Thinking just a little bit ahead can help you avoid unnecessary delays.\n3. Swallow your pride and put first things first\nAs ALS providers, we are very proud of our ability to initiate advanced procedures and give a host of medications. And rightfully so! This is a big responsibility. BUT, we need to be honest with ourselves.\nThe biggest contributor to a successful resuscitation is NOT those medications and advanced airways of which we are so fond. In fact, the most important thing we can provide to our patients is quality BLS care — compressions, ventilations and defibrillation. ALS procedures should rarely interrupt or disturb these functions. If you see an ALS activity delaying a BLS skill, gauge the importance of the ALS skill. Nine times out of ten, ALS can wait.\n4. Train with realism\nWe often practice CPR scenarios where time is ""accelerated"". Resist this urge. Work the simulated arrest in real time. Perform full two-minute CPR cycles on the manikin. Just like real life, care can\'t be provided until an IV is established.\nWhen training for a pediatric arrest use the Broselow tape or other pediatric medication job aids. Then draw up a 0.01 mg/kg dose of Epi 1:10,000. These training actions breed competence and confidence on real calls.\n5. Avoid tunnel vision\nEMS providers tend to become distracted by the skills aspect of a cardiac arrest, including AED use, airway management, intravenous access, rhythm interpretation, and so on. We all have areas of our practice that need improvement, and it can be easy to get sidetracked by one or more of these problem areas during a real call. That\'s why it’s important to always remember that it’s your job as the lead provider to guide the resuscitation. You can’t do that when you are attempting your fifth IV or airway. Stay in the present, and keep those skills-related blinders off.\n6. Communicate calmly to the team\nA successful resuscitation is only possible when you communicate effectively. As the lead provider on a high-stress call, you set the tone for the other personnel. If you bark orders and communicate haphazardly, the resuscitation will rapidly fall apart. Instead, try to maintain a calm demeanor, making polite requests of your crew members and family/bystanders. You\'ll be surprised how much more smoothly the call will go. Family members will offer up critical pieces of information, crew members will respond quickly and effectively, and you will come across as the confident and competent medical provider that you strive to be.\n7. Debrief every cardiac arrest\nThis can be a tough pill for some to swallow, but nobody will ever run a perfect resuscitation. There will always be events that could have been avoided and better approaches to specific issues. Unfortunately, these specifics are often forgotten as we move on to other calls. That\'s why it\'s so important to get everyone who contributed to the resuscitation together as soon as possible. Have everyone discuss their actions, and have them provide feedback on what worked and what did not. Work hard to incorporate the lessons learned into your next CPR response.\n8. Follow up\nAs you run a resuscitation, you should always work to establish the cause, or etiology, of your patient’s cardiac arrest. Many mnemonics exist for this, because it’s critical to remember that many causes of arrest are reversible! But this isn\'t where your job ends.\nOnce the patient is in the ER, follow up to determine the ultimate diagnosis. If the resuscitation was unsuccessful, follow up with the medical examiner. This kind of medical feedback allows you to compare your care and clinical judgment to the medical reality. Without this step, we are left to constantly guess, and we will never become excellent clinicians.\n9. Leverage technology for quality improvement\nMany of these steps can be incorporated into your care immediately. However, the use of technology in a cardiac arrest QI process requires a system-wide commitment. Most modern cardiac monitors have the ability to record such eye-opening metrics as compression fraction — the actual amount of time that CPR was performed during a resuscitation, the length of pauses during compressions, and the adequacy of compression rate and depth.\nSystems that provide this information to their crews report seeing dramatic improvements in cardiac arrest survival. While you may not be able to get these technical capabilities immediately, you can work towards their implementation. Talk to your service director and medical director.\n10. Learn from others\nMedicine is a complex field. Treatments, underlying science, and best practices of care are constantly in flux. In order to stay on top of it all, you need to approach medicine with an open mind. Much can be learned from your partners, crew members, nurses, and physicians.\nStudying other systems and their approaches may provide insight into a new way for your units to respond and treat patients more effectively. As you work to incorporate these various insights into your practice, the quality of the care you provide will constantly improve. The Resuscitation Academy provides resources and training.\nShare your tips!\nGot a great tip to improve cardiac arrest survival? Comment below and if I use your tip in a follow-up article I will send you a signed copy of my book, Lights & Sirens: The Education of a Paramedic.\nTod Levesh and James Kellogg work as paramedics for King County Medic One and teach continuing medical education classes across the country. For more information, please visit GoInTheKnow.com.']"	['<urn:uuid:d2fd3773-74fa-4609-aa58-dced98971d4f>', '<urn:uuid:ec25f4c0-5602-4e21-a438-26f5ee3ae248>']	factoid	with-premise	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-13T06:04:36.674696	35	101	1764
37	Can the COVID vaccine help prevent long COVID?	According to CDC research, people who are vaccinated and get COVID-19 are less likely to report long COVID symptoms than those who are unvaccinated.	"['Now that it has been two and a half years since the start of COVID-19, countless people around the world are discovering that some of their COVID-19 symptoms are lingering for weeks, months, and even years. These long term COVID effects are called “post-COVID-19 syndrome,” or more commonly referred to as “long COVID.” And what makes something long COVID? The fact that the symptoms impair people’s lives after having COVID-19 and can’t be explained by other health issues.\nAccording to the World Health Organization (WHO), long term COVID effects are coronavirus symptoms that persist long after people contract the virus or re-appear three months or later after a person first contracts COVID-19. Most often these symptoms are caused by persistent inflammation that impacts different parts of the body.\nLong COVID is more common than we think. In fact, as of June 2022, the Center for Disease Control (CDC) and National Center for Health Statistics (NCHS) reported that nearly one in five American adults developed long COVID symptoms. According to some CDC studies, as of September 2022, 13.3% of people experience long COVID for one month or longer. Two and a half percent of people self-reported experiencing long COVID for 3 months or longer, and more than 30% of people that were hospitalized due to long COVID have suffered from long haul effects of COVID for six months or longer.\nA recent study on more than 30,000 individuals with laboratory-confirmed COVID-19 infections found that one in 20 people reported not recovering at all, and four in 10 people said that they still hadn’t recovered months later.\nLONG COVID IS COMPLICATED\nThere are innumerable variables that affect whether someone has long COVID, which will require years of extensive research to fully understand. That is why organizations like the CDC, National Institute of Health (NIH), and other reputable research institutions are continuously launching new research studies to find out what is causing these long term effects. In short, however, there is no simple answer as of yet.\nMOST COMMON LONG COVID SYMPTOMS\nSome long COVID symptoms occur more often than others. Some of the most common long COVID symptoms include the following:\n1. Fatigue and Post-Exertional Malaise\nLong COVID fatigue is described as an extreme, chronic level of exhaustion that impairs people’s lives and cannot be explained by another health issue. Post-exertional malaise (PEM) is when symptoms worsen with even the most minor form of mental or physical exertion, most commonly after 12 to 48 hours. Once worsened, people with PEM continue to experience more severe COVID-19 symptoms for days or weeks.\n2. Respiratory Issues\nGiven that COVID-19 is a respiratory ailment, it’s no surprise that respiratory symptoms are some of the most common long COVID side effects. Some of the most common respiratory issues reported by people with long COVID include shortness of breath, cough, and chest pain.\na. Shortness of Breath/Difficulty Breathing\nShortness of breath is characterized by difficulty breathing after any form of physical exertion.\nShortness of breath can impact both simple and complex COVID-19 cases. In simple COVID-19 cases, for those who were immobile while having COVID-19, shortness of breath is the side effect of having to rebuild stamina and endurance after being sick. Some people can also develop shortness of breath as a novel long COVID symptom.\nMore severe COVID-19 cases and individuals with compromised immune systems due to previous heart or lung issues, diabetes, cancer, or autoimmune disorders are more susceptible to experiencing shortness of breath. If you suffer from long COVID shortness of breath along with leg swelling or an oxygen level that is below 92%, you should immediately seek medical attention.\nb. Long COVID Cough\nOne of the most common distinguishing features of COVID-19 is often a persistent, dry cough. Note: not everyone with COVID-19 develops a long COVID cough. Coughing is caused by irritation in respiratory airways, leading to respiratory inflammation. Some additional side effects include congestion with or without phlegm.\nA long COVID cough can last for months after a person contracts the coronavirus. This is usually because irritation in the airways persists even after the coronavirus itself clears up.\nCOVID-19 is primarily a respiratory or lung disease, although it can also affect the heart. COVID-19 causes inflammation of the heart muscles, leading to a series of possible symptoms such as palpitations, shortness of breath, and chest pain. Some very uncommon cardiovascular ailments that are associated due to long COVID include postural orthostatic tachycardia syndrome (POTS), heart attacks, and heart failure.\nAccording to the American Heart Association, countless studies over the past couple years report that it’s not uncommon for individuals to suffer from heart damage post-COVID. In one study done on select groups of U.S. veterans that was featured in Nature Medicine, researchers found that U.S. veterans that previously suffered from COVID-19 between the dates of March 1, 2020 and Jan. 15, 2021 were 63% more likely to later suffer from cardiovascular issues. Some of the most common heart issues that people suffer from post-COVID include heart palpitations, postural orthostatic tachycardia syndrome (POTS), heart attacks, and even heart failure.\nWhen should I see a doctor if I’m having heart symptoms after coronavirus?\n1. Heart Palpitations\nHeart palpitations occur when a person’s heartbeat is irregular, beating too fast, or is fluttering. If you find yourself continuing to suffer from long COVID heart palpitations, you should see a doctor. This is especially true if you already had a heart condition prior to contracting COVID-19.\nThe definition for Postural orthostatic tachycardia syndrome (POTS) lies within the name itself. Each of the words within the title explains what this condition means: postural referring to body positioning, orthostatic referring to standing upright, tachycardia referring to a heart rate over 100 beats per minute, and syndrome referring to a group of symptoms that happen together. So, in summary POTS means having a faster heart beat, dizziness, and fatigue whenever transitioning from a standing to supine position.\nThere is no treatment for POTS and luckily it is not life-threatening, but there are several ways to manage POTS symptoms, which include adjusting your diet, medication intake, and level of exercise. If you are experiencing any of these symptoms for the first time, you should seek medical attention.\nFor further reading, here’s a great source by John Hopkins University you can reference.\n2. Chest Pain\nThere are many possible causes of long COVID chest pain. According to Dr. Wendy Post at John Hopkins School of Medicine, “if you have chest pain when you inhale, you might have lung inflammation. Sudden, severe chest pain could be a blood clot in the lung (pulmonary embolism).”\nIf you experience new chest pain or your long COVID chest pain can be accompanied by shortness of breath, nausea, or lightheadedness, you should seek medical attention right away, as those symptoms together can be a sign of a heart attack.\nLong COVID chest pain that is extremely severe is also cause for concern, as it can be an indicator of a blood clot in the lungs. Chest pain that occurs when you inhale is a sign of lung inflammation and is something that you should also monitor post-COVID.\n3. Heart Attacks\n“Very few people have a severe heart attack, such as an acute myocardial infarction, or MI, due to COVID-19,” says John Hopkins cardiologist Dr. Wendy Post. However, heart attacks have been associated with previous COVID-19 infections due to its impact on blood vessels and heart rate regulation. These are two of the most common catalysts for developing a Type 2 heart attack (when the heart needs more oxygen than it can get). If you think you may be suffering a heart attack due to long COVID, you should seek medical attention immediately.\n4. Heart Failure\nA diagnosis of heart failure after COVID-19 is very rare. Heart failure describes when a heart cannot keep up with the necessary workload to get blood flowing to all parts of the body. Therefore, when you suffer from heart failure, your body isn’t receiving the oxygen and blood needed to function effectively.\nIf you suffer from heart failure, your heart will overcompensate by either enlarging, developing more muscle mass, or trying to pump out blood at a quicker pace. The body can only function while keeping up such behaviors for so long though.\nWhile uncommon, because SARS-CoV-2 can cause inflammation to bodily organs such as the heart, it’s possible for long COVID to lead to heart failure. Therefore, if you suffer from severe long COVID symptoms or your long COVID symptoms are worsening, seek medical attention as soon as possible.\nWhile very uncommon, individuals with COVID-19 experiencing respiratory issues can experience kidney damage due to continuous low blood oxygen levels. The body’s immune response to suffering from COVID-19 can also lead to long-term kidney damage. This is because some forms of COVID-19 can cause the body to send a rush of small proteins called cytokines into the body to fight the virus. Unfortunately, such an influx of cytokines in the body can lead to damaged kidney tissues and the inflammation of bodily organs.\nIf you’ve suffered from a severe case of the coronavirus that led to acute kidney issues, then you’re more likely to develop long COVID kidney damage. If you experience short or long-term damage to other bodily organs, such as the heart or lungs, you’re also more likely to develop long COVID kidney damage. This is because the functions of all of the bodily organs affect one another. Individuals with kidney or other health issues prior to contracting COVID are also more susceptible to developing long COVID kidney damage.\nEven the fact that COVID-19 can cause blood flow issues that lead to the development of tiny blood clots in the bloodstream can lead to long-term kidney damage. This is because tiny blood clots in the bloodstream can clog small blood vessels in the kidneys. If you let your long COVID kidney damage persist too long, it can eventually cause you to suffer from kidney failure.\nSigns of Kidney Damage from COVID-19\nTo minimize the chances of developing long COVID kidney damage, try to recognize the signs of kidney damage from COVID-19 early. That way you can receive any necessary treatment as soon as possible. Some of the most common signs of kidney damage from COVID-19 include high levels of protein or blood in the urine and abnormal blood work.\nLOSS OF TASTE AND SMELL\nLoss of taste and smell is a common side effect of having COVID-19. This is due to one of two factors: either 1. Damage to the olfactory sensory neurons or surrounding neurons or 2. Congestion that blocks our olfactory sensory neurons and therefore, our ability to smell and taste. With short COVID, 8 out of 10 people may lose their sense of taste and smell and recover it within 30 to 60 days of a COVID-19 infection. However, for those with long COVID symptoms, recovery will take longer. A recent study showed that 75-80% of cases are resolved within two months, and 95% of cases are resolved within six months.\nNEUROLOGICAL AND MENTAL HEALTH ISSUES\nIn research done by Yale School of Medicine, it was found that most neurological issues due to COVID-19 are a result of immune-system mediated injury, as opposed to the virus directly attacking or impacting brain cells.\nNeurological issues caused by COVID-19 can be broken down into two buckets - mild/moderate symptoms vs. more severe. Most individuals will experience no or mild to moderate neurological symptoms, such as confusion, delirium, sleepiness, poor cognitive function, intense headaches, and uncomfortable skin sensations. Severe ailments may include increased risks of strokes, epileptic seizures, hearing and vision abnormalities, or symptoms similar to Parkinsons.\nIn fact, a research study published by the University of San Diego on this topic in June 2022 found that 89% of the study’s participants were experiencing fatigue and 80% of the study’s participants were experiencing headaches. Many of the study’s participants even reported suffering from memory impairment, insomnia, and decreased concentration.\nHowever, the cause of long COVID related neurological issues is much more complex and still unknown. While still uncertain why this is happening, physicians such as Dr. Serena Spudich from the Yale School of Medicine are recognizing that “there are many, many reports now of people having persistent symptoms for months…Often, they’ve had complete resolution of their fevers and breathing problems, but they continue to have problems with thinking, concentration, memory, or difficulty with strange sensations and headaches.”\nResearch conducted by the Washington University School of Medicine in St. Louis states that approximately 40 million people are suffering from neurological disorders due to COVID-19 worldwide, and of those who contracted SAR-CoV-2, 77% are more likely to develop memory problems, often referred to as “brain fog.”\nAccording to research done in September 2022 by a team at Harvard for the Journal of the American Medical Association (JAMA), individuals who suffered from psychological distress before contracting COVID-19 had a 32% to 46% increased risk of continuing to have long COVID mental health ailments after an acute case of COVID-19. Those who reported high levels of two or more forms of psychological distress, such as long COVID depression and longCOVID anxiety, had a 50% increased risk of developing long COVID mental health symptoms. This study goes to show that a person’s mental health can have some sort of effect on a person’s physical COVID-19 symptoms.\nAccording to the Mayo Clinic, acute digestive issues such as heartburn, trouble swallowing, irritable bowel syndrome, constipation, diarrhea, bloating, and incontinence, are very common side effects of a COVID-19 infection.\nIn a study conducted by the Mayo Clinic, approximately 16% of the subjects reported still having digestive symptoms 100 days after their COVID-19 infection resolved. The most common long COVID-19 symptoms they continued to experience included abdominal pain in 7.5%, constipation in 6.8%, diarrhea in 4.1% and vomiting in 4.1%.\nTREATING LONG COVID\nCurrently there is no tried and true treatment for long COVID, however, there is growing evidence that some medications, including some antiviral drugs or anticoagulants, may help. The medications currently being tested to treat long COVID include Paxlovid, colchicine, sirolimus, antihistamines, antidepressants, and/or steroids, beta blockers, and/or fludrocortisone. There is no sufficient evidence yet to confidently say any of these medications can help improve or cure long COVID to date.\nHowever, research conducted by the CDC does suggest that those who are vaccinated and experience an acute COVID-19 infection are less likely to report long COVID symptoms than those unvaccinated.\nWith that said, you do have agency and the opportunity to mitigate the effects of long COVID by making an appointment with your primary care doctor for a thorough check-up, eating healthy, and incorporating exercise into your daily routine. There is always the possibility that some post-COVID ailments are not related to COVID-19 and require medical consultations and examinations to ascertain that they don’t have a different root cause.\nThere is no better time for you to get these symptoms and possible chronic health issues checked out once and for all. According to Michael Mina, MD, PhD, an infectious disease epidemiologist, immunologist, and physician who recently left his post as professor at Harvard School of Public Health and Harvard Medical School to become eMed’s Chief Science Officer, “COVID and Long-COVID are teaching the medical community to consider lasting symptoms that arise from other pathogens too...symptoms that have previously usually been chalked up to ""it’s in your head"" ... much to millions of patients frustration and sanity.”\nTherefore, with the concerns that you may have about your health post-COVID, take comfort in the fact that now more than ever, with proper communication with your doctor, you can get all your healthcare needs met.\nTESTING AND TREATING COVID-19 WITH EMED\nOne of the best ways to possibly combat long COVID is to get diagnosed and treated for COVID-19 as soon as possible. This requires having COVID-19 telehealth kits on hand, and starting to test once you either think you’ve been exposed or start feeling symptoms that may or may not be COVID-19 related. If eligible, this is the fastest route to getting treatment early on to prevent symptoms from getting worse, and possible hospitalization.\nHere at eMed, we recognize navigating the health system alone can be daunting, and may even discourage people from seeking answers or treatment. So, we have made it our mission to assemble the pieces for you through an at-home test that is cost-effective and easy to navigate.\neMed proctors will guide you through the testing process and connect you directly to an eMed telehealth provider who will do a medical assessment and determine the best course of treatment for you. The medical consult will result in instructions for care, and if eligible, a prescription either for pick-up at your desired pharmacy or delivery directly to your home.\nBy cutting out the process of finding a primary care physician, making an appointment, and navigating a complex health care system, we have tried to carve out the fastest possible route to getting your answers and treatment when it’s most effective - within the first 1-2 days of exposure or infection.']"	['<urn:uuid:95391610-2356-48f3-b1f0-866a1a892b8b>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	8	24	2855
38	My mom's fingers have gotten really puffy lately, and they look like sausages. She's also feeling tired all the time - should we be worried about something serious?	These symptoms, particularly the swollen 'sausage-like' fingers (known medically as Dactylitis) and fatigue, could indicate an inflammatory auto-immune condition. The most common causes are Psoriatic Arthritis (PsA) or Rheumatoid Arthritis (RA). When such symptoms appear, it's important to seek medical care right away. The physician will conduct various tests, diagnostic imaging, and question the patient to determine the root cause. Early diagnosis and treatment are crucial for managing these conditions, as while they are incurable, the symptoms can be controlled with proper medical intervention.	['Jeffrey R. Carlson, MD\nDactylitis is a symptom that is most often seen in patients who have inflammatory Psoriatic or Rheumatoid arthritis, which are auto-immune diseases. It is also known as “Sausage Finger” or “Sausage Toe” because of the localized, painful swelling that causes digits to look like sausages. The fingers or toes may also be warm and difficult to move due to the swelling. In this article, I will discuss how Dactylitis is treated, based on the underlying disease that causes the symptom.\nWhen a patient experiences extreme swelling in their finger(s), toe(s), hand(s) or feet, they usually head for medical care right away. When the usual suspect causes are ruled out, such as an insect/spider bite, allergic reaction, or a sprain or broken bone, a physician may begin to suspect another, more serious culprit is at play. The treating physician will order a battery of tests, diagnostic imaging, extensively question the patient and will likely find the root cause of Dactylitis. There are also clues, such as only one hand or finger being affected, vs. both hands and multiple fingers. Does the patient have psoriasis? Joint pain, fevers or fatigue? The answers to these questions help the physician make a diagnosis.\nOnce the underlying cause is determined, treatment can be administered. For most patients, inflammatory arthritis is the culprit. There are other less common causes, such as Sickle Cell Anemia, Tuberculosis, Sarcoidosis, Reactive Arthritis (caused by a bacterial infection) or Syphilis, which I will not address.\nPsoriatic Arthritis (PsA) is an inflammatory, auto-immune disease which severely affects joints, tendons and connective tissues and is seen in 30% of patients with psoriasis. They may have severe psoriasis or barely any skin symptoms. Patients experience pain, fatigue, depression, dry skin and eyes and other body issues. Dactylitis is a common symptom of this disease, seen in about half of the people who have PsA. With PsA, there is typically no symmetry in the disease, which means only one hand or finger on one side of the body may be affected with the swelling and stiffness, but not the other. PsA is treated with prescription anti-inflammatory medications, corticosteroids, DMARDS (Disease Modifying AntiRheumatic Drugs), or Biologic drugs, injectable medications that alter the patient’s immune system. Although there is no cure for PsA, the Dactylitis will subside or completely resolve as the root condition is brought under better control.\nRheumatoid Arthritis (RA) is an inflammatory, auto-immune disease that causes major damage to joints, organs and body systems, because the body’s immune system attacks them. Patients experience pain, low-grade fevers, fatigue, weight loss and depression. Dactylitis is a less common symptom of this disease. With RA, there is symmetry in the disease, which means both sides of the body are affected equally with swelling and stiffness. RA is treated with prescription DMARDS (Disease Modifying AntiRheumatic Drugs), JAK inhibitors, Biologic drugs (injectable medications that alter the patient’s immune system) or corticosteroids. The Dactylitis will lessen or end totally as the root condition is effectively treated, although there is no cure for RA.\nDactylitis sufferers, from both PsA and RA, can benefit from treatment with a Physical Therapist. The Physical Therapist can help the patient maintain function and movement of the small joints in the hand, fingers, feet and toes. Patients can be taught new, less painful ways of working, and doing daily activities. The Physical Therapist can also use thermal modalities to provide warming pain and swelling relief for the patient’s sore and swollen extremities.\nPatients who suffer from Dactylitis caused by inflammatory Arthritis are also encouraged to eat an anti-inflammation diet, such as the Mediterranean Diet, which constricts sugar, gluten, and alcohol, and encourages eating organic produce, lean meats in moderation, seafood and olive oil. Losing weight is also extremely helpful for those who suffer with Arthritis.\nExercise is also encouraged as a treatment for Dactylitis. Yoga, Tai Chi, water aerobics, swimming, walking or biking are all great, low impact exercises that will help to keep joints mobile and will help to reduce pain. The endorphins released by exercise also help with pain and depression.\nAlthough incurable, as inflammatory arthritis is managed and controlled, the symptom of Dactylitis should lessen or cease entirely. If it develops, it is usually a sign that a person needs medical attention right away. The sooner the underlying root condition is diagnosed and treated, the sooner the symptom of Dactylitis will go away, hopefully for good.']	['<urn:uuid:d192fd01-5056-495c-b3d8-d498d75b365c>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:04:36.674696	28	84	737
39	how many elements in computing surface grid	The computational electromagnetic surface comprises a grid of 1,176 delicately tuned elements that modify incident waves' amplitude and phase.	"[""Flexible metasurface computer solves complex equations at near light-speed\n(Nanowerk Spotlight) As our digital devices handle increasingly complex computations, scientists have looked to physics for inspiration on new computing paradigms. Rather than shuttling electrical signals across silicon like conventional processors, an intriguing approach encodes information in electromagnetic or acoustic waves propagating through space. These wave-based computers can solve problems at incredible speeds — theoretically up to the speed of light itself.\nSuch extreme velocity results because data processing happens intrinsically within the wave medium through deliberate interference patterns. There are no cascaded logic gates dragging down performance as in digital circuits. Waveforms contain both amplitude and phase data, enriching the information capacity compared to simple on-off binary digits. And without needing repeated analog-to-digital conversions, wave computers avoid a major bottleneck curtailing the evolution of traditional computing architectures.\nExperts have sought to build practical wave-based systems for years, but inherent challenges around complexity, customizability and manufacturability have inhibited progress. Realizing practical wave-based computers has proven enormously difficult. Prior concepts demanded complex optimization algorithms yielding unmanufacturable designs. Alternate approaches based on configurable circuit arrays needed impractical numbers of phase shifters and amplifiers. All these setups were application-specific, lacking flexibility for broad problem solving.\nNow, scientists from Southeast University in China have achieved a major breakthrough using metasurfaces - a key photonic technology promising revolutionary control over electromagnetic waves. Their pioneering metasurface computer effectively performs rapid analog matrix calculations that would cripple much faster digital supercomputers.\nSchematic diagram of the metasurface-based CME solver, which is composed of a 2N-port transmission network and N identical 4-port couplers. (Reprinted with permission by Wiley-VCH Verlag) (click on image to enlarge)\nMatrix equations feature prominently across science and engineering, modeling everything from machine learning optimizers to structural mechanics simulations. Solving them digitally demands substantial computing power, motivating the intense interest in analog wave-based architectures.\nThe crux of the new metasurface solver lies in its computational electromagnetic surface, comprising a grid of 1,176 delicately tuned elements that modify incident waves’ amplitude and phase. This reprogrammable nanophotonic medium actively transforms input signals into desired output data, physically embedding the math inside the metasurface.\nTo operate the solver, complex matrix equations get converted into two components – a coefficient matrix and a constant vector. These data impress onto electromagnetic waves entering two input ports. As the signals propagate through the metasurface region, they undergo intricate interference computations via scattered reflections. The final result emerges at the output ports, encoded onto outgoing waves.\nRemarkably, this entire process finishes almost instantaneously as the waves transit the setup near light speed. There are no systematic logic gate delays like in digital processors. The metasurface computer also consumes far less power than silicon equivalents, sharply reducing operating costs.\nCrucially, the operational principle allows solving arbitrary complex matrix equations just by varying the metasurface design and input signals. The same hardware platform thus adapts to diverse problems with no fundamental architecture changes. This programmability grants significant versatility lacking in previous wave computers needing bespoke designs even for basic math operations.\nBecause tuning individual metasurface elements has proven difficult thus far, the current prototype demonstrates a non-reconfigurable solver for fixed equations. However, rapid progress in dynamic metasurface technology points toward fully software-defined metasurface computers that researchers reconfigure on-demand. The paper also notes that operating at higher frequencies would reduce the overall size enabling larger metasurfaces to solve bigger matrix calculations.\nAdditionally, the scalable planar geometry of metasurfaces makes expanding the architecture more viable than prior 3D metamaterial structures attempted for wave computing. If robustly developed, such rapid reconfigurable metasurface matrix solvers could markedly transform sectors needing heavy numerical analysis from weather prediction to optimization research.\nThe pioneering metasurface solver establishes a long-awaited bridge between real-time wave computing and practical programmability. While the initial prototype handles a limited matrix size of 5 x 5, more elements could enumerate higher dimensions. In fact, metasurfaces’ scalable planar geometry makes large problem solving viable more straightforwardly than bulky 3D metamaterial structures attempted previously.\nThe researchers comprehensively validated their design via simulations and measurements, accurately solving several test matrix equations. Across four simulation test cases, the meta-computer yielded solutions with reasonably low error rates averaging 21%. The experiments on a fabricated prototype further verified the architecture's feasibility for a 3x3 matrix, successfully computing solutions to eight distinct matrix problems. Quantitatively, these measured solutions showed below 25% error on average - on par with initial benchmarks of competing electronic analog computing schemes. The dominant errors stemmed from tolerances in nanofabrication and challenges precisely reading the output data.\nBoth factors should improve substantially by leveraging state-of-the-art micro-nanofabrication facilities and high-precision metrology equipment. With further refinement, metasurface computers could surpass digital techniques for specialized tasks needing extreme speeds.\nWith further refinement, metasurface computers could surpass digital processors for specialized tasks needing extreme speeds like radar imaging, scientific modeling, and data analytics. Intriguingly, their high efficiencies may also suit low-power edge computing applications. The breakthrough work lays vital foundations for metasurface computing by tackling previous showstopper bottlenecks in complexity, customizability, and physical realizability. If robustly developed, such rapid analog matrix solvers could markedly transform sectors needing heavy numerical analysis from weather prediction to optimization research.""]"	['<urn:uuid:2cdfddb3-d304-439a-be01-e2f4bc88aa32>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T06:04:36.674696	7	19	860
40	how does ribosome control gene expression and what diseases can occur when ribosome function is abnormal	The ribosome controls gene expression by regulating which types of proteins are produced in different cells. It was discovered through studying a mutant mouse that ribosomes play a fundamental regulatory role in embryonic development, dictating where and when gene products are expressed. When ribosome function is abnormal, it can lead to various diseases including congenital birth defects affecting the spine, face, limbs, heart and other organs. Additionally, ribosome dysfunction is implicated in both congenital and acquired bone marrow failure syndromes, known as ribosomopathies, as well as diseases like X-linked dyskeratosis congenita and various cancers.	"['Call it a mystery with a stubby tail: an odd-looking mouse discovered through a U.S. government breeding program in the 1940s that had a short, kinky tail and an extra set of ribs in its neck — and nobody knew why.\nA team of scientists led by researchers at the University of California, San Francisco has now spilled the genetic secrets of this mutant rodent. In doing so, they may have uncovered a new wrinkle in the genetic code — an entirely unrecognized way our bodies regulate how genes are expressed in different tissues throughout life.\nThis discovery has broad implications for how we think about developmental biology, and it may explain the origins of numerous developmental diseases. It also may help suggest new ways of treating certain types of cancer, many of which may be linked, at least in part, to problems in how the body regulates gene expression.\n“The ultimate outcome of gene expression is the production of proteins,” said UCSF Faculty Fellow Maria Barna, PhD, who led the research. “Our study suggests that there is a new way of controlling which types of proteins will be produced in which types of cells.”\nAs described in this week’s issue of the journal Cell, the research identified a molecular machine called the ribosome as the factor that exerts this new control over gene expression. Though well known to scientists as a key component of living cells, the ribosome was never thought to play a regulatory role.\nHOW THE MUTANT MOUSE CAME TO BE\nThe “tail short” mutant mouse first appeared in 1946 at the National Cancer Institute in Bethesda, MD, where several were discovered among a litter of offspring born to a highly inbred strain of mice raised in a breeding program. They all had very unusual skeletal features: short, stubby tails and an extra set of ribs in their neck vertebrae.\nDoctors recognized the uniqueness and potential importance of the mouse immediately. It wasn’t just that the skeleton was malformed — it seemed to be misplaced. The neck vertebrae had ribs and resembled vertebrae lower in the spine. It was as if the body plan of this mouse had been incorrectly mixed up in early development, though it was beyond the ability of scientists in those days to determine why.\nThrough the decades, the mouse remained a curiosity of sorts. Its progeny were carefully bred year after year, but decades passed before anyone could determine which genes were responsible for its unusual features.\nFinally, a few years ago, Barna and her colleagues, became interested in the mouse, and she worked with scientists at the National Institute of Genetics in Japan to identify the exact mutations that cause the malformations. A developmental biologist herself, Barna suspected that the mouse’s peculiar skeletal structures suggested some sort of anomalous “patterning” in early development, where one part of the body forms incorrectly in the shape of a different part. What they found, said Barna, was a complete surprise.\nA MASSIVE MOLECULAR MACHINE\nThe mutations turned out to be in the ribosome, a massive molecular machine that makes proteins and are common to all forms of life. They can be found in every cell in every tissue of the human body, and scientists believe that similar versions have been inside every cell of every creature that ever lived — whether cat, carp, cholera or Caesar.\nThe ribosome is so common because it plays a central role in biology by making proteins that do everything from building the body’s tissues to carrying out crucial biological functions, like breaking down food in the gut and encoding memories in the brain. Despite its importance, scientists had always assumed that the ribosome was something of an automaton — a machine that simply took instructions from a creature’s genetic code and spit out proteins. Mutations in the tail short mouse, however, showed otherwise.\nThese mutations turned out to delete a protein called Rpl38, one of 79 proteins that make up the mouse ribosome. Without Rpl38, the ribosome in the tail short mouse still worked, but it lost the ability to control which proteins it expressed — an ability scientists never thought the ribosome had in the first place. Moreover, the effect was not generalized throughout the body of the mouse, but specific only to certain tissues.\nIn early fetal development, the loss of Rpl38 caused certain parts of the backbone to grow and develop as if they were elsewhere in the spine — thus the extra ribs in the neck. Mutations in the proteins of human ribosomes can also lead to unexpected tissue-specific congenital birth defects including malformations of the spine, face, limbs, heart and other organs. The reasons why have been a mystery, but the explanation may be that the ribosome plays a fundamental regulatory role in embryonic development.\n“It dictates where and when the final outcomes of gene products are expressed,” said Barna. “Therefore the genetic code has a previously unrecognized set of instructions that are carried out by the ribosome to control cell behavior. For example, whether or not a vertebrate has a pair of ribs associated with it.”\nThe work also suggests that ribosomes could act differently in different tissues in the body, adding an entirely new layer of complexity to the already complex program by which the genetic code is expressed in living creatures.\nThe article, “Ribosome-Mediated Speciﬁcity in Hox mRNA Translation and Vertebrate Tissue Patterning” by Nadya Kondrashov, Aya Pusic, Craig Stumpf, Kunihiko Shimizu, Andrew C. Hsieh, Shifeng Xue, Junko Ishijima, Toshihiko Shiroishi, and Maria Barna appears in the April 29, 2011 issue of the journal Cell. See: http://dx.doi.org/10.1016/j.cell.2011.03.028. This work was funded by the Program for Breakthrough Biomedical Research, UCSF, the March of Dimes, a Basil O’Connor Scholar Research Award, and MEXT, Japan.\nUCSF is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care.\nFollow UCSF on Twitter at http://twitter.com/ucsf', ""Abnormalities in ribosome function that are implicated in both congenital and acquired bone marrow failure syndromes in humans are classified under the broad umbrella of ribosomopathies. A recent collaboration between our two laboratories has initiated a completely new avenue of research by establishing an evolutionarily conserved role for ribosome modifications mediated by the pseudouridine synthase dyskerin in specific aspects of translation control. The functional role of pseudouridine (?) modifications is of great medical importance as mutations in DKC1, the gene encoding for dyskerin, are found in a number of diseases including X-linked dyskeratosis congenita (X-DC), Hoyeraal-Hreidarsson syndrome, and numerous cancers. Importantly, our published work has identified that the decreased affinity of rRNA ? defective ribosomes for tRNAs results in increased rates of programmed -1 ribosomal frameshifting (-1 PRF), a molecular mechanism that is emerging as an important regulator of gene expression. This proposal seeks to test the hypothesis that global changes in -1 PRF affect the expression of specific subsets of mRNAs encoding key hematopoietic factors, that may contributing to some of the pathological features associated with X- DC. Support for this hypothesis is evident from our extensive published and unpublished findings showing that increased rates of -1 PRF promotes rapid degradation of specific mRNAs through the Nonsense-Mediated mRNA Decay (NMD) pathway. Furthermore, we have demonstrated that rRNA ? levels are impaired in hematopoietic cells from X-DC patients harboring distinct DKC1 mutations. Importantly, our preliminary and published data also suggest that one important functional class of mRNAs involved in telomere maintenance is regulated by this mechanism in both yeast and human cells and is disrupted in X-DC patients, thus linking rRNA ? defects and telomere shortening. By utilizing X-DC as a disease paradigm and combining three model systems, yeast, mouse and humans, in this proposal we will extend our novel findings and identify the repertoire of mRNAs that underlie bone marrow failure associated with ribosome dysfunction and determine the therapeutic potential of inhibiting NMD and -1 PRF in X-DC.\nAim 1 of this proposal seeks to identify the repertoire of mRNAs whose expression is affected by rRNA ? defects in yeast, mouse and human cells using two complementary approaches.\nAim 2 will determine the mechanisms through which impaired rRNA ? alters expression of -1 PRF containing mRNAs.\nAim 3 is oriented towards determining the therapeutic potential of targeting NMD and -1 PRF in X-DC. By the end of the proposed studies, we will have 1) established a new paradigm for understanding ribosomopathies, 2) identified specific genes that can be used as biomarkers and targeted for therapeutic intervention, and 3) explored novel approaches to treat this class of diseases.\nAbnormalities in ribosome function are implicated in both congenital and acquired bone marrow failure syndromes, often referred to as 'ribosomopathies'. Here we will test the hypothesis that dysregulation of programmed -1 ribosomal frame shifting affects the expression of specific subsets of genes and that when deregulated may contribute to disease pathogenesis. Identification, validation, and therapeutic rescue of genes affected by ribosome dysfunction will provide novel diagnostic and prognostic markers for bone marrow failure and identify a novel therapy for treatment of this class of diseases.\n|Advani, Vivek M; Dinman, Jonathan D (2016) Reprogramming the genetic code: The emerging role of ribosomal frameshifting in regulating cellular gene expression. Bioessays 38:21-6|\n|Dinman, Jonathan D (2016) Pathways to Specialized Ribosomes: The Brussels Lecture. J Mol Biol 428:2186-94|\n|Moomau, Christine; Musalgaonkar, Sharmishtha; Khan, Yousuf A et al. (2016) Structural and Functional Characterization of Programmed Ribosomal Frameshift Signals in West Nile Virus Strains Reveals High Structural Plasticity Among cis-Acting RNA Elements. J Biol Chem 291:15788-95|\n|McMahon, Mary; Contreras, Adrian; Ruggero, Davide (2015) Small RNAs with big implications: new insights into H/ACA snoRNA function and their role in human disease. Wiley Interdiscip Rev RNA 6:173-89|\n|De Keersmaecker, Kim; Sulima, Sergey O; Dinman, Jonathan D (2015) Ribosomopathies and the paradox of cellular hypo- to hyperproliferation. Blood 125:1377-82|\n|Belew, Ashton T; Dinman, Jonathan D (2015) Cell cycle control (and more) by programmed -1 ribosomal frameshifting: implications for disease and therapeutics. Cell Cycle 14:172-8|\n|Dinman, Jonathan D (2015) Molecular biology: Entry signals control development. Nature 517:24-5|\n|Ban, Nenad; Beckmann, Roland; Cate, Jamie H D et al. (2014) A new system for naming ribosomal proteins. Curr Opin Struct Biol 24:165-9|\n|Musalgaonkar, Sharmishtha; Moomau, Christine A; Dinman, Jonathan D (2014) Ribosomes in the balance: structural equilibrium ensures translational fidelity and proper gene expression. Nucleic Acids Res 42:13384-92|\n|Belew, Ashton Trey; Meskauskas, Arturas; Musalgaonkar, Sharmishtha et al. (2014) Ribosomal frameshifting in the CCR5 mRNA is regulated by miRNAs and the NMD pathway. Nature 512:265-9|""]"	['<urn:uuid:24da8c52-4c9a-478b-96b1-3e10c438d205>', '<urn:uuid:e3f6fdc4-cc7f-4ad5-ac26-be83e50b77ea>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T06:04:36.674696	16	94	1756
41	How do these art spaces use their outdoor areas creatively?	These art spaces utilize their outdoor areas in innovative ways. The MAXXI museum features a complex architectural garden with a wooden platform, lawn seating, and a suspended yellow textile volume that provides shade and creates color reflections throughout the day. It also includes a timed water system for cooling visitors and can transform into an event stage in the evening. The GoggleWorks Center for the Arts uses its building's exterior as a creative canvas, with thousands of LED lights installed on its windows to display animated light patterns that reflect the artistic activities happening inside, effectively turning the building's façade into an outdoor art installation.	"[""The annual program is promoted by MAXXI Architettura in partnership with MoMA/MoMAPS1 in New York, CONSTRUCTO in Santiago, Chile and the Istanbul Modern. The initiative intends to promote young architects worldwide by inviting them to create summer installations that, besides researching new architectural forms and techniques, can offer shade to visitors and a space for shows and events.\nProject Description from the Architects:\nHe is a complex architectural garden made of a lawn, a big platform and a light volume suspended in front of the concrete walls of MAXXI’s galleries.\nThe protagonist of the scene is the large suspended volume, which through its size, color, the recreational use of water and its shadow establishes an interaction with the museum outdoor spaces, receiving its resting places.\nThe transparency and the apparent weightlessness of the installation generates a dialogue and a stimulating contrast with the imposing masses of the museum and its sinuous geometries.\nThe large volume follows the wind movement, emphasizing the lightness of the materials that compose it.\nThe color and transparency of the material, in contrast to the mineral character of the museum, give amazing color reflections that vary with the incidence of light onto the volume façades, allowing He to change his appearance during the day.\nAs evening falls, He becomes a big suspended lantern, a landmark that enlightens MAXXI and its piazza.\nThe realization of He is the result of an experimental process and a multidisciplinary approach aimed at resolving both the engineering challenge of suspending the volume upon the square, both the manufacturing of the installation, involving skills, techniques and details typical of rigging, climbing and nautical science.\nThe need to minimize the structures found an answer in a superior ring realized entirely in stretch steel cables, which allows the suspension of the volume.\nThe textile volume have been realized with a material commonly use in agriculture which have been handicraft manufactured with nautical details.\nThis has made possible the inedited use of the material, whose typical features could not allow the flatness of He's surfaces, and the interesting moiré effect from the superimposition of fabric.\nThe design of the square is also ruled by the yellow color, by the out-of-scale logic and the use of materials out of their context.\nThe garden below the textile volume is organized with a wooden platform and a lawn which defines a bank used as an informal seat. The seating system is designed with an ergonomics approach, enhancing the comfort during the layover. On the platform various large seatings are shaped with large backrests ensuring both the comfort of the layover and a playful use.\nAlong the perimeter of the volume a timed system produces a light rain that drops on the platform underneath, defining the space and refreshing the visitors.\nIn the evening the platform become YAP event stage, recreating a new centrality in the museum square.\nProject name: He\nOffice: bam! bottega di architettura metropolitana\nDesign team: Alberto Bottero, Valeria Bruni, Simona Della Rocca, Fabio Vignolo\nCollaborators: Luca Giacosa\nLocation: Rome, Italy\nTypology: Public space, temporary architecture\nSite area: 500 mq\nDesign: Feb 2013 - May 2013\nConstruction period: May - June 2013\nClient: Fondazione MAXXI\nPole structural project: Vittorio Calomeni\nTensostructure project: Davide Enrione\nRigging consultancy and anchoring supervision: Giorgio Giorgis\nTecnical supervision: Antonello De Luca\nPole and tensostructure production: ABC with Associazione Professionale Guide Alpine Gran, Paradiso e Canavese\nPlatform production: MAIDABROS\nLawn production: Flaminia Garden\nSeating production: Zeppelin\nFabric volume production: SPECCHIOPIUMA\nLighting supply: ABC\nLighting project Paola Mastracci in collaboration with Luca Maneli\nMAXXI technical coordinator: Silvia La Pergola, Valentina Zappatore"", 'Colored lights continually ""draw"" across the façade of the GoggleWorks Center for the Arts.\nOver seven thousand LED lights are installed on every window of the Washington Street façade of the GoggleWorks Center for the Arts. This exciting, programmable light sculpture, entitled Thought Process, uses LEDs as an artistic medium to reflect the creativity that goes on inside the building. Thought Process was designed, created and installed by Lyn Godley, a product designer and lighting artist. The installation uses iColor Flex SL from Philips Color Kinetics to literally “draw” light across the building. From dusk until 11 pm, seven innovative light sequences use over 7,000 RGB LED nodes to create various patterns.\nA new light sculpture, Thought Process, wraps light across the façade of the five story GoggleWorks Center for the Arts in Reading, Pennsylvania, USA. Unveiled to celebrate the Center’s third anniversary, the installation uses iColor Flex SL to “draw” flight across the building. The original design has three lines sweeping across the building: a blue line draws in a circular motion, a red line sketches a vertical zigzag pattern, and a green line traces a vertical heartbeat sequence. Built in 1871, the building which now houses the GoggleWorks Center for the Arts is listed on the registry of historical landmark buildings so nothing could be altered on the exterior of the building\'s façade. Godley used the 85 windows to create a canvas of animated light to reflect the creativity taking place inside the walls of the Art Center. However, this is only the beginning of the designs that can be created on this very unique canvas. ""I envisioned using the façade as a canvas with animated lines of color drawing themselves across the façade in a number of curly, zigzag and wiggly lines—as though the sketching taking place inside the building was visible on the outside,"" Godley says. ""The idea was that eventually the façade would also become a canvas to teach students to design upon, thus continuing to change the imagery while using light as a medium for art students to explore.""\nBuilding or project owner : GoggleWorks Center for the Arts\nProject artist/ concept/ design/ planning : Lyn Godley Design\nLight design : Lyn Godley Design\nLight hardware (LED hardware) : Philips Color Kinetics\nLighting control software : Philips Color Kinetics\nProject co-ordination : Lyn Godley Design\nKind of light creation : 7100 LED lights are installed on every window of the façade of the GoggleWorks Center for the Arts. The installation uses iColor Flex SL from Philips Color Kinetics to literally “draw” light across the building. From dusk until 11 pm, seven innovative light sequences use over 7,000 RGB LED nodes to create various patterns.\nResolution and transmitting behaviour : 7100 LED light modules (Philips Color Kinetics iColor Flex SL) programmed in seven animated sequences.\nLuminace : The lights are used from dusk until 11:00pm every night.\nUrban situation : The building can be seen from different angles and from various taller buildings in a dense urban environment.\nDescription of showreel : The sequences display the illusion of colored light drawing across the façade.']"	['<urn:uuid:55f1bbea-ccf5-4eb0-b3e1-1b1f65be535d>', '<urn:uuid:f9c27bb6-aa25-4743-9781-d117fe6158a3>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T06:04:36.674696	10	105	1117
42	How do storage solutions differ between NYC's adAPT micro-units and Bureau Fraai's Amsterdam loft?	In NYC's adAPT project, the micro-units (250-350 square feet) require separate kitchens and full baths, while Bureau Fraai's Amsterdam loft design incorporates all functional elements like bathroom, toilet, storage, and bed into one solid wooden volume made of birch plywood, allowing the surrounding space to be used freely for living, dining, and cooking.	"['Micro-dwellings: Part of the solution or just more problems?\nMost of us are fascinated by micro-homes and tiny apartments. Along with the urban planning benefits they promise, we love the ingenuity of their organization and debate alternate approaches to using the same space three times over. But how is this small-scale approach playing out in the real world? Let\'s take a look at the response to introducing micro-dwellings in major cities including New York City (NYC) and San Francisco (SF).\nMicro-dwellings often seem a mixed salad, combining sweet and bitter together with the odd flavors of exotic dressings. Micro-dwellings offer the potential for large, overcrowded cities to bring people closer to their jobs, reduce the cost of providing infrastructural support to previously undeveloped areas, and curbing energy usage. However, these potential benefits come with a sociological price tag. There is little question that the ambiance of a neighborhood, or that of a building, is likely to change drastically if the population density increases by a factor of three to five times.\nThere is also a collection of stereotypes claiming that crowded neighborhoods with tiny, tightly packed dwellings are poor, dirty, and collect riff-raff. Open spaces disappear, traffic (both foot and auto) goes from quiet to impossible, minorities become segregated therein, anonymity breeds crime, and the noise! Yes, these are stereotypes – but stereotypes embedded in the public consciousness and regularly reinforced by history, the silver screen and the idiot box. Such viewpoints add greatly to the friction retarding evolution of cities better suited to their task, but unfortunately there is no magic wand to remove ingrained opinion.\nDespite the problems, cities across the world are beginning to crack open the door toward welcoming micro-dwellings, with varying degrees of success. For example, NYC\'s Mayor Bloomberg has launched the adAPT NYC micro-apartment competition, looking for the best designs for living the world has to offer.\nNew York City\nNYC has a very large proportion of singles and two-person families living in the city proper, with many families choosing to commute from suburban areas rather than deal with the very large expense of living in the city. There are about 1.8 million one- or two-member households in NYC, but only about a million studio and one-bedroom apartments. The result is that housing prices are magnified by supply and demand considerations, and the use of apartment space is forced to be non-optimal, with a significant number of people rattling around in apartments much larger (and more costly) than they would like.\nThe NYC Department of Housing Preservation and Development (DHPD) is sponsoring the adAPT design competition for micro-apartments. DHPD is going to build a new housing complex in Kips Bay on the Lower East Side in Manhattan. A historic area initially settled by Jacobus Kip in 1655, it was the landing point for the British assault on NYC in the American Revolutionary War. Now a patchwork combination of old and new construction, it includes the last unpaved street in Manhattan. The adAPT building site is just over a block from the East River.\nThe adAPT competition is for a mixed-use apartment building mostly filled with ""micro-units"", which are studio apartments ranging in size from 250-350 square feet of floor area which satisfy a number of livability criteria, such as access to exterior light and air, attractive common spaces, and substantial access to light and air to create a sense of openness. The main living/sleeping area is to be least 150 square feet (14 sq m) in area and at least 8 feet (2.4 m) wide. The micro-units are to have a full bath and a separate kitchen as well. In general, federal, state, and city building codes must be followed. (A conceptual sketch of a different micro-unit appears in the Image Gallery.) Mayor Bloomberg has granted a variance to allow smaller living units (the usual minimum size is 400 sq. ft., or 37 sq m), and to allow more living units in the building than would normally be allowed.\nTo fit the available lot (45 x 105 ft / 4725 sq. ft. or 14 x 32 m / 450 sq m), no more than about eight to ten housing units can be built on each upper floor. No particular building height is mandated, but comments suggest that in the neighborhood of 60-80 micro-units is the goal. The ground floor will include a lobby, a restaurant, and a common area for the tenants. Conceptual sketches of the ground, second, and upper floors can be found in the image gallery.\nThe roof will be used as some variety of common space, probably a mixture of small gardens, recreational areas, and picnic tables. There is to be a back yard measuring about 35 x 45 ft - 1575 sq ft (10.7 x 13.7 m – just under 150 sq m), which will have sitting areas in a landscaped garden. The design must be innovative inside and out, and must work around existing trees and buildings. The competition deadline was the end of July, but the winner has not yet been selected.\nWe now turn from the welcoming attitude of NYC to the mixed response to micro-dwellings in San Francisco. San Francisco has been a forerunner in the adoption of the idea of micro-dwellings. However, the South of Market (SoMa) Community Action Committee (SOMACAC) recently protested at the SF city hall against a proposed amendment to the housing code that would permit dwellings as small as 150 sq ft (14 sq m). This would be a change from the already small (by current standards) limit of 220 sq ft (20.5 sq m). SoMa already has a number of condominium properties specialized as micro-dwellings. Among these are the Cubix Yerba Buena, which has micro-condos as small as 230 sq ft (21 sq m), and the SmartSpace condominiums which comprise four levels of 300 sq ft (28 sq m) micro-condos. Their level of success has been erratic, however. The Cubix Yerba Buena was opened for sales in 2008, just in time for the recession. The condo units did not sell, and eventually the banks foreclosed on the building. The SoMa SmartSpace complex has just opened, and there appears to be some enthusiasm building for the micro-condo concept. SmartSpace has also built an experimental 160 sq ft (15 sq m) apartment which it is showing to developers and city planners around the country.\nThere is certainly a tension between developing housing units for relatively affluent singles and couples, and developing family-oriented housing units. One of the problems is a consequence of Mark Twain\'s famous utterance: ""Buy land - they\'re not making it any more."" Being relatively isolated on a peninsula, San Francisco has a limited amount of land with which to build a suitable tax base for the needs of the community. SoMaCAC is in part concerned that as more young, working singles and couples find affordable housing in SF, families will either be forced out of the city proper, or will face life in a city not willing to continue the present strong emphasis on child and family-oriented functions such as a superior school system.\nUltimately, financial and resource economics will decide the issue. In SF, micro-condos rent for about $1300/month, compared to some $2300/month for a conventional studio apartment. While the studio apartment has perhaps twice the floor area, such are generally not planned for effective use of the extra space, and many people find micro-dwellings to provide a higher standard of living experience. From the viewpoint of the city, encouraging micro-dwellings that primarily house working singles and couples reduces the effect of population growth on the extended infrastructure of the city. True, these benefits are largely blotted out if applied to a city without an excellent mass transportation system, as the infrastructure requirements for a million more cars carrying one and a half million new commuters to and from work each day, with parking at each end, are enormous. However, many cities will see micro-dwellings as a strategy toward handling future population growth without breaking the city\'s treasury.\nOther cities are experimenting with micro-dwellings, a cautious step at a time – Vancouver, Seattle, Santa Monica, Toronto, Paris, Bangkok, Brisbane ... the list goes on. However, the degree to which micro-dwellings can become integrated with the mainstream lifestyle of major cities will depend on a combination of local issues such as population density, rate of growth, changing demographics, urbanization vs. suburbanization, employment portfolios, and more. Given current trends, however, it seems that many of us will occupy some form of micro-dwelling during our lives.', 'Living tiny has become the new living large. With the rise of the tiny house movement and expanding urbanization, designers and developers alike have turned their attention to tiny living. Balancing the bottom line with creative solutions for storage and versatility, these tiny spaces are made to utilize every inch of space. Depending on the city, tiny apartments are generally considered to be less than 550 square feet, with micro apartments classified between 200–400 square feet. These tiny spaces are formed for efficiency, coupling programs and functions together.\nUnlike a traditional studio flat, tiny apartment residents can benefit from the use of communal kitchens, bathrooms and roof gardens. While micro living has faced criticism for packing residents into tight spaces, the following apartments utilize clever solutions to maximize space while preserving comfortable living conditions for inhabitants. Combining durable materials, kinetic elements and precise detailing, these tiny apartments reveal how to live large in small spaces and create room for everyday life.\nThis is a renovation project of an old flat which measures 22 sqm and 3.3m in height. Due to the high housing prices in Taipei City, the living space that young people can afford has become smaller and smaller in the last decade. The living unit may not be spacious, but still has a chance to fulfill all basic living functions. The client agreed with the idea that space is as important as function when it comes to a place where people will live for a long time rather than a room for a short-term stay. This tiny apartment centers on long term living around clever storage solutions.\nThis small 35 square meter flat was designed for a young couple. The main task was to create a comfortable open space plan with enough area for storage with access to natural light. The apartment was designed with a furniture system that would make the most of the existing space. A range of appliances were provided by Electrolux, while many furniture elements were provided by IKEA. A curtain separates the sleeping unit , kitchen and living zone from entrance, which can be pushed back when it’s needed. Living and dining zones are quite flexible, by moving sofa and extending dining table, it easily become a comfortable space for 10 guests.\nThis tiny loft design aimed to convert a small former social housing apartment in the city center of Amsterdam into a modern urban loft. Bureau Fraai, a design firm from Amsterdam, got rid of all doors, walls and tiny rooms around the core of this small apartment. Their design focused on creating one solid wooden volume of birch plywood with extended wall cabinets alongside the whole length of the apartment. By incorporating program like bathroom, toilet, storage and a box-bed all inside the wooden volume, the surrounding space can be freely used for other program like living, dining and cooking.\nThe project is a flat refurbishment in Camogli, a charming village near Genova. This is a small holiday flat, an attic above the old fishermen’s harbor. The project was made to be two bedrooms, a studio, a living room, a kitchen and a bathroom, despite the roof form and the very small floor surface. That led to a tailored apartment, where each room is a piece of furniture: after you have used it, you can close it. The simple design features elegant details and a range of solutions for storage.\nCarmel Place is the winning proposal in the adAPT NYC an initiative launched as part of former Mayor Bloomberg’s administration’s New Housing Marketplace Plan to accommodate the city’s growing small household population. The building was granted several mayoral overrides to allow this prototype to be built, including a relaxation of the minimum unit size, and the maximum density, or number of units permitted in a building. The now completed Carmel Place provides 55 loft-like rental apartments, ranging in area from 260-360sf net, and complemented by generous shared amenities, setting a new standard for micro-living. The project has been watched closely as a new housing prototype in NYC, and for its groundbreaking use of modular construction.\nDesigned as a tiny apartment in a historical building in Wroclaw, Poland, 29sqm was made to maximize space and create an independent bedroom. The ceiling height didn’t allow the designer to create two equivalent levels, but it was possible to build a semi-mezzanine. Above the bathroom and the hallway there is a large bed area and wardrobe space. Moreover, to increase space, the living room, kitchen and dining area are combined into one room. In turn, 3XA designed the space to deceive the senses with a blind door within the apartment walls.\nThis micro live work studio is located in Budapest’s ninth district. The neat studio was reinvented as a micro live + work space featuring a set of pine and ash wood bespoke furniture elements that frame and anchor the main functionalities of the apartment. Built with an open floor plan, high ceilings and custom furniture, the plan allows the compact space to house a fully-functional kitchen, lofted bedroom, event promotion headquarters, and ample space for after parties. The apartment includes furniture from Innovation Living and Pedrali, with porcelain by Mirage.\nThis 27sqm studio apartment is located in Sydney. The project was an exercise in modest, low cost, good quality design that’s affordable. The micro apartment offers a proposal for future high-density urban living for one person families, the fastest growing demographic. With a budget of less than $40k and a four week window for construction, the client requested a flexible canvas for daily living. To conserve space, light and the city skyline outlook, a joinery pod was inserted to address issues of privacy, storage and a lack of living space inherent in apartments this size. The pod utilizes full height, wall-to-wall sliding doors and accommodates an entry foyer, storage, equipment, washing and sleeping zones.\nAt twenty square meters, this singular urban shelter is an enclosed space where a single person can live and work. The apartment, even with its small size, offer generous spatial arrangements and different functional uses. The pieces that make it up do not have a fixed or clearly defined use: the kitchen is a walk-through room to get to the living space, and there are stands rather than stairs to go down to the living area, which in turn is over a cellar-storage room. At the same time, you can take the ladder to the indoor terrace, a place to be used as a study or room to relax. The focus is on an unadorned way of life that’s both simple and bright.\nThis tiny penthouse duplex apartment renovation in a brownstone building on the Upper West Side held a number of challenges to make it a liveable space. The client wished to use it as a pied-a-terre in the city for himself and his family. With only 425 square feet divided among 3 levels, it was important to open up the space as much as possible to make it livable. Inside, the bathtub and shower were provided by Gaggenau, while cooktops were provided by Caesarstone and kitchen furniture by Vola. The upper level was extended via a built-in bed cantilevered over the floor edge, providing for the necessary head-room at the level transition below. Storage was built entirely into the spaces below the stairs with built-in drawers.']"	['<urn:uuid:63d1aba6-641e-4ef3-9ae5-f4a12e2825f0>', '<urn:uuid:53024d10-8273-4abf-b716-80964d3916b0>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T06:04:36.674696	14	53	2653
43	As someone interested in ancient cultures, I'm curious how the locations of the Tiliviche geoglyphs and the Nazca Lines compare in terms of how they can be viewed?	The Tiliviche geoglyphs and Nazca Lines have different viewing characteristics based on their locations. The Tiliviche geoglyphs, located in Chile's Atacama Desert, are carved into hillsides and are visible both from the air and from ground level. They depict images including thin and fat persons herding llamas facing the ocean. In contrast, the Nazca Lines in Peru can only be truly appreciated from the sky, requiring an altitude of at least 1,500 feet to see their true dimensions. These lines cover approximately 450 square kilometers of desert floor and include various designs of living creatures, stylized plants, imaginary beings, and geometric figures several kilometers long.	"['By now, most of you have either downloaded or are at least aware of Google Earth. The program, which utilizes satellite imagery and aerial photographs to map the earth in amazing detail, can be used for a variety of reasons. Some people use it to locate areas they have lived or traveled, while others use it to visit remote places of the planet that one would not otherwise be able to see. I belong to this group, as I am sure many others are as well. Google Earth is the perfect tool to explore and discover the physical remains of ancient cultures or unexplained anomalies found throughout this wonderful world. Believe it or not, amazing discoveries have been found using this program.\nAmazing and Mysterious Places on Google Earth\nIn this article, I list ten mysterious places on Google Earth (in no particular order) that are quite fascinating. In addition to the photographs imaged through Google Earth, there is a brief summary of the area and its actual coordinates so you can visit these strange and fascinating places yourself through Google Earth.\n10. Badlands Guardian\nLynn Hickox discovered the Badlands Guardian utilizing the Google Earth program. Although this area is a completely natural geological feature in southwest Alberta, Canada, one cannot help but notice the striking resemblance of a human head wearing his full native American headdress. Some have also claimed that the figure appears to be wearing earphones, which is nothing more than a road leading to an oil well. Even though this area is a natural formation, it is amazingly ironic to see these arid badlands formations resemble the fully detailed shape of the very same indigenous people that lived in these lands.\nGeocoordinates: 50° 0’36.30″N, 110° 6’46.82″W\n9. Egyptian Ruins Coordinates\nIt doesn’t take long to discover an interesting find when using Google Earth to explore the shifting sands of Egypt. As you can see from this attached image, the angles and straight lines of what appears to be a fortification wall protecting smaller similar structures in the interior. If you use Google Earth to explore the surrounding area, you can see additional suspected man-made ruins buried in the desert sand. I am not sure if the ruins captioned in this photograph have been thoroughly explored, but experts do state only a fraction of ancient Egypt has been discovered. Every year new tombs, structures, and even pyramids are rediscovered in this ancient land.\nGeocoordinates: 29°32’7.65″N, 30°40’7.88″E\n8. Machu Picchu Coordinates\nLocated on a mountain ridge above the Urubamba Valley in Peru, Machu Picchu is one of the most mystical sites on our planet. Machu Picchu is a pre-Columbian Inca site built around 1400 AD. The area only thrived for 100 years and the place was abandoned following the Spanish conquest of the Inca empire in Peru. Theories of its purpose range from it being the birthplace of the Incan“Virgins of the Suns”, an estate of the Incan emperor Pachacuti, a prison for the select few who commit extremely heinous crimes, or as a settlement to control the economy of the conquered lands. For more information on Machu Picchu, please see our article here.\nGeocoordinates: 13° 9’49.69″S, 72°32’42.65″W\n7. Nazca Hummingbird\nThe Nazca Hummingbird is only one of many designs and strange “airstrips” sketched across the Nazca desert of Peru. These unusual man-made features were first discovered in the 1930s by passengers on aircraft flying over this vast desert on the way to other destinations. The Nazca lines were created by brushing the reddish pebbles from the desert floor, which revealed the lighter colored sand underneath. Due to the lack of rainfall and wind erosion, these lines and drawings have been preserved for centuries. Their purpose remains uncertain, although one theory states the lines and drawings were created to help bring rain. For more information on the Nazca Lines, please see our article here.\nGeocoordinates: 14°41’32.27″S, 75° 8’56.91″W\n6. Chichén Itzá Coordinates\nThe Mayan civilization from the Yucatan peninsula of Mexico is responsible for building this structure. The translation of its name literally means “at the mouth of the well of the Itza”. Although built around 600 AD, records indicate it fell around 1000 AD. Although Chichén Itzá was no longer a flourishing city by the time the Spanish Conquistadors arrived, the Mayans managed to initially push back the Spanish invaders until it was eventually conquered. The Spanish used the site as a cattle ranch in 1588 following the Mayan defeat. Chichén Itzá had once been major economic power in the Mayan lowlands and archaeological work has recovered many fine examples of Mayan relics.\nGeocoordinates: 20°40’58.64″N, 88°34’5.50″W\n5. Cerne Abbas Giant\nThe Cerne Abbas Giant, also called the “Rude Man” or the “Rude Giant,” is a figure roughly 180 ft long and 167 ft wide. It is carved in the hillside of Cerne Abbas, just north of Dorchester, England. Its origin and date are unknown, although there are no records of this carved giant prior to the late 17th century. Its purpose is also unknown, although most scholars tend to agree it is either a symbol of fertility or the representation of the Roman Heracles. Cerne Abbas folklore claims the figure is the actual outline of a real giant that came from Denmark. Local villagers soundly defeated the giant as he slept on the hill. Although we may never know the real purpose of this carving on the hill, it is an important part of the local culture and re-chalked every 25 years to preserve its features.\nGeocoordinates: 50°48’49.00″N, 2°28’28.99″W\n4. Carved Lines in China\nThere isn’t much information available on these strange, yet beautiful mosaic lines carved in the desert of the Gansu Sheng province in China. Some records indicate they were created in 2004, but nothing that seems official. Of note is that these lines are somewhat near the Mogao Caves, which is a World Heritage Site. The lines span a very huge distance. However, they still retain their linear proportions despite the curvature of the rough terrain. Perhaps this is a military site to test kill probabilities of Chinese missile systems. If utilizing Google Earth to view this feature, you will notice many questions from fellow Googlers wondering what this feature is and its intended purpose.\nGeocoordinates: 40°27’28.56″N, 93°23’34.42″E\n3. Geoglyphs at Tiliviche\nChile’s Atacama Desert holds many examples of ancient Incan geoglyphs. The Geoglyphs at Tiliviche is a fine example of a thing person and fat person herding llamas. The llamas are facing the ocean and perhaps carved into the earth as a landmark for llama caravans coming down the mountain. The llamas were as important to transportation for the ancient Incas as the camel was in the Egyptian desert. These geoglyphs may be over 2,000 years old and usually found near trading routes.\nGeocoordinates: 19°32’56.62″S, 69°58’4.21″W\n2. Geoglyphs of Chiza\nAnother example of ancient Incan geoglyphs in the Atacama desert of Chile is the geoglyphs of Chiza. These geoglyphs are not too far away from the ones located at Tiliviche. They created these designs by arranging large dark stones on the lighter sand. These particular glyphs represent images of people, birds, and other animals. What is remarkable about these Incan glyphs is that they are usually located on hillsides and visible to people in the air and on land. This particular feature has a nearby road, and as you can see by the photo. Someone has taken the opportunity to stop their vehicle and admire these amazing designs in such a dry desert.\nGeocoordinates: 19°12’12.51″S, 70° 0’29.37″W\n1. Atacama Giant Coordinates\nOur last example of mysterious places found on Google Earth is also located in the Atacama desert of Chile. The Atacama Giant is the largest prehistoric anthropomorphic figure in the world and measures 282 ft long. Records indicate it was built around 1000 to 1400 CE. Some skeptics and “ancient aliens” theorists believe this is a representation of alien visitors who visited the Incas. However, the Atacama Giant’s unusual features actually have a specific purpose. The giant represents the deity of the local people. Additionally, the various points around his head is a calendar marking days, seasons, and when to plant certain crops. They accomplished this by marking where the moon sets in reference to these points.\nGeocoordinates: 19°56’56.88″S, 69°38’1.87″W\nOf course, there are many other mysterious places that can be visited in the comfort of your own home using Google Earth. Stonehenge, the Pyramids of Giza, and Nan Madol come to mind immediately. As satellite imagery improves and our ability to peer into every corner of our ancient planet becomes more routine, many other strange and fascinating discoveries will surely emerge.', 'Nazca offers like destination a unique place in the world, full of mystery and mysticism, wonderful lines of immense figures and lines, of spectacular perfection, works of a very old Peruvian civilization, the Lines of Nazca.\nJust two hours from Ica, 50 square kilometers of desert floor were covered centuries ago by vast drawings, figures of mammals, insects and deities. The Nazca Lines, discovered in 1927, are the most extraordinary legacy left by a culture that flourished in 300 BC. The lines are a series of complex designs, some up to 300 meters long which can only be seen in their true dimension from the sky, from an altitude of at least 1,500 feet. The Nazca culture is not believed to have been capable of manned flight. But the question remains as to how they crafted the drawings, what technology they used and what purpose the lines served.\nThe Nazca Lines was declared Culture World Heritage Site by UNESCO.\nThe town of Nazca was founded in 1591 by the Spaniards, on the valley of Nazca, close to towns inhabited by old civilizations that had been dominated by the Inca. The ancestral name was Nanasca.\nNazca (some spelled Nasca), is a gentle town, slowly developing; in which its main economic activity is based on the agriculture, trade and tourism.\nNear to Nazca city (50 Km) , more than 600 geoglyphs were discovered in the Palpa area. This recent discovery includes numerous new ruins and geoglyphs and has shed new light on the Nazca investigations.\nTourism to this area is related directly with the Nazca Lines and other archaeological complex as Cahuachi, Estaquería, Chauchilla cemetery, Paredones and Palpa.\nNazca City Overview\nCentral Coast, 1,929 feet (588 m.) above se level.\nDistances to Nazca:\nNazca City: Is a little town, located in an oasis valley in the Nazca desert, it has basic and comfortable hotels, restaurants, cybercafes. In the city you will be able to find many ceramic artisans whose style remembers to their ancestor, the Antonini Museum and Planetarium Maria Reiche.\nNazca Lines: Declared ""Cultural World Heritage Site"" by UNESCO (1994). Located in the arid Peruvian coastal plain, the geoglyphs of Nazca and the pampas of Jumana cover about 450 sq. Km. These lines, which were scratched on the surface of the ground between 500 B.C. and A.D. 500, are among archaeology\'s greatest enigmas because of their quantity, nature, size and continuity. The geoglyphs depict living creatures, stylized plants and imaginary beings, as well as geometric figures several kilometers long. They are believed to have had ritual astronomical functions. In order to appreciate the lines in all its magnitude, it is only possible from an airplane in flight.\nArchaeological Places in Nazca: Archaeological remains of the Nazca culture, it shows the high development reached in hydraulic engineering, complex of underground aqueducts and reservoirs as Cantayoc, also Paredones, Estaquería and Chauchilla cemetery and geoglyphs of Palpa.\nCahuachi: Cahuachi was a ceremonial center, a sacred destination of Nazca pilgrims between 100 and 500 AD. The ceremonies in the place included the construction of temples using thousands of conical or wedge adobe bricks. Each participating community demonstrated their true belonging to their religious community by singing, dancing and banqueting, thus explaining why in Cahuachi there is little garbage, while offerings abound (pan flutes and musical drums, sacrificed llamas and guinea pigs, fine textiles, human burials and pottery representing deities). Main cultural-urban center, built with great urban sense. The ancient Nazca people built their pyramidal temples by terracing the fossil sand dunes. In the lowers parts, smaller architectural mounds, streets and squares give the site a general city aspect. Attached to this town were also discovered (1980) a group of lines traced on the floor in form of trapezes and serpentines that embrace 1 square Km. It is located at 27 Km (16.77 miles) to south of Nazca.\nAround Nazca: Interesting places to visit around Nazca city as Pampas Galeras and San Fernando natural protected areas, Sacaco is a rich location of fossil remains, Maria Reiche Site Museum, Cerro Blanco the highest-know sand dune in the world for sandboarding.\nAntonini Didactic Museum: Located in Avenida La Cultura 600. Visiting hours: Mon. – Sun. 9:00 A.M. – 7:00 P.M. A collection of archaeological pieces of the different stages of the Nazca culture as well as trophy heads, musical instruments like pan flutes, textiles, mummies, etc. are exhibited in this museum. These discoveries are the result of the excavations done in ""Cahuachi"", the largest mud maid ceremonial center in the world. The museum also provides the chance to see the Bisambra canal, which shows the magnificent hydraulic engineering work of the Nazcas.\nPlanetarium Maria Reiche: Venturing into boundaries of the world archae-astronomy, the Nazca Lines show present Maria Reiche\'s theory, and the connection she found between the Nazca Lines and some astronomical events. She theorized the lines were oriented towards the places on the horizon where different celestial objects appeared and disappeared. After the show, weather permitting in a backyard exploration of the night sky, have a naked-eye view of the constellations and bright stars of the season along with telescope, observation of the moon, planets, and other celestial marvels. Located in Jr. Bolognesi Nº 300 - Hotel Nazca Lines.\nIf you visit Nazca, you can continue your trip to Cuzco by bus (12 hours) or go directly to Arequipa City (8 hours). These trips are at night and thus you will gain a day if you do not return to Lima (Go to bus services).']"	['<urn:uuid:80c6d19c-75e4-42c9-9910-52bf20279f26>', '<urn:uuid:837ac8a5-07b0-435d-924c-1f3d96ca63d8>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T06:04:36.674696	28	105	2358
44	How can strict schema validation prevent XML attacks?	Strict schema validation can fully stop coercive parsing attacks by containing detailed descriptions of allowed elements, attributes, and data types in the WSDL. When elements are strictly defined, injection of additional tags is not possible and any SOAP message violating the schema is rejected.	"['Difference between revisions of ""Coercive Parsing""\n(→Categorisation by number of involved parties)\nm (1 revision imported: Import from WS-Attacks)\nLatest revision as of 11:26, 31 October 2015\n- 1 Attack description\n- 2 Attack subtypes\n- 3 Prerequisites for attack\n- 4 Graphical representation of attack\n- 5 Attack example\n- 6 Attack mitigation / countermeasures\n- 7 Attack categorisation\n- 8 References\nCoercive Parsing is one of the simplest attacks to mount! It aims at exhausting the system resources of the attacked web service. The attacker just sends a SOAP message with an unlimited amount of opening tags in the SOAP Body. In other words: The attacker sends a very deeply nested XML document to the attacked web service.\nTest on AXIS 2 web services showed that the attack results in a CPU usage of 100% while the SOAP message is processed. When using a socket on the attacker side the attack can last for as long as a connection between the attacker and the victim exists. All the attacker has to do is ""pump"" opening tags in the socket for as long as he wishes to disable the web service.\nThis attack is one of the more devastating denial of service attacks, however countermeasures are available.\nNOTE: Only web services using a DOM parser are susceptible to this attack. The DOM Parser creates an in-memory representation of the SOAP message. During this process the SOAP message size can raise by a factor of 2 to 30. When very large documents are processed memory exhaustion is often the result. When using a streaming based parser like SAX it is very unlikely for the attack to succeed, since the entire document is never loaded in memory.\nThere are no attack subtypes for this attack.\nPrerequisites for attack\nIn order for this attack to work the attack has to have knowledge about the following things:\n- Attacker knows the endpoint of web service. WSDL is not required, since the attack is solely focused on the XML Parser. It doesn\'t matter if the Operations within the SOAP Message are valid.\n- Attacker can reach the endpoint from its location. Access to the attacked web service is required. If the web service is only available to users within a certain network of a company, this attack is limited.\nGraphical representation of attack\n- Red = attacked web service component\n- Black = location of attacker\n- Blue = web service component not directly involved in attack.\nThe following SOAP message shows an example with a ""Coercive Parsing Attack"" payload.\n<soapenv:Envelope xmlns:soapenv=""..."" xmlns: soapenc:""...""> <soapenv:Body> <x> <x> <x> <x> <x> <!-- Continued for as long as wanted by the attacker -->\nListing 1: ""Coercive Parsing Attack"" payload.\nAttack mitigation / countermeasures\nThe ""Coercive Parsing"" attack can be fully stopped when using strict schema validation. Each WSDL should contain a detailed description of the used elements, attributes, and data types. For example when only one Element <Surname> is expected within the SOAP body the XML Schema should contain the following elements:\n.. <!-- excerpt fictional XML Schema --> <xs:element name=""Surname"" type=""xs:string""/> ..\nListing 2: Excerpt of a XML Schema for the tag ""Surname""\nBy using the data type ""string"" only strings are allowed within the element tags. The injection of more tags within the <Surname> tag is not possible. Since the default maximum and minimum number of occurrences is 1, the element has to show up exactly one time in the SOAP body. If no other tags are defined within the XML Schema of the SOAP body, any other tag is prohibited by default too, making it impossible to mount the attack. Therefore any SOAP message that violates this schema is rejected.\nFor a more detailed tutorial on how to create strict XML schemas refer to .\nIt is understood that a strict schema validation is resource intensive, however one should be clear how easy it is to compromise the availability of a web service when turning off schema validation.\nCategorisation by violated security objective\nThe attack aims at exhausting the system resources, therefore it violates the security objective Availability.\nCategorisation by number of involved parties\nCategorisation by attacked component in web service architecture\nCategorisation by attack spreading\n- Meiko Jensen, Nils Gruschka, and Ralph Herkenh ̈ner. A survey of attacks on web services. Springer-Verlag, 2009.\n- Meiko Jensen, Lijun Liao, and J ̈rg Schwenk. The curse of namespaces in the domain of xml signature. Technical report, Horst G ̈rtz Institute for IT-Security, 44780 Bochum, Germany, 2009.\n- Nishchal Bhalla and Sahba Kazerooni. Web services vulnerabilities. http://www.blackhat.com/presentations/bh-europe-07/Bhalla-Kazerooni/Whitepaper/bh-eu-07-bhalla-WP.pdf, February 2007. Accessed 01 July 2010.\n- Jan Peters. Use of soa appliances in service-oriented infrascructeres. CAST-Workshop- SOA Security, Juni 2009.\n- N/A. Protecting enterprise, saas & cloud based applications – a comprehensive threat model for rest, soa and web 2.0. Technical report, Intel Corporation, 2009.\n- Leroy Metin Yaylacioglu. Business value einer web service firewall. Master’s thesis,Hochschule für Angewandte Wissenschaften Hamburg, 2008.']"	['<urn:uuid:feceaabd-e756-4db2-b5fd-5ddc0392db2f>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T06:04:36.674696	8	44	825
45	As a sports scientist focused on fluid dynamics, I'm curious - what's the difference between how typical athletes consume water versus what experts recommend for healthy hydration?	The key difference is that many athletes drink water on a fixed schedule without being thirsty, while experts recommend drinking to thirst with electrolytes. Studies show that consuming between 4-8oz of water every 10-20 minutes during activities longer than 60 minutes, combined with electrolytes (300-600mg sodium per hour), is recommended. Simply guzzling plain water on a set schedule can lead to hyponatremia, a dangerous condition of low blood sodium that can be fatal. The safer and more effective approach is drinking electrolyte-enriched water according to thirst, which allows the body to naturally regulate its fluid balance.	"[""By Holley Samuel MEd, RD, LD, CPT\nWith summer on the horizon, one of your biggest concerns as an endurance athlete is probably hydration. You may be wondering what to drink, how much, and when to drink it, along with how to know if you're hydrating well enough to achieve your goals. Whether it's summer or not, endurance athletes sweat during their workouts, and they're losing water and electrolytes. Our environmental conditions, along with more specific details like which phase we're in of the menstrual cycle, if we're female, can also impact how much we sweat and its composition.\nOur bodies store carbohydrates as glycogen and body fat for energy, but we cannot store water and electrolytes in the same way. This is why we need to replace what we lose during our training or race. Once we become too dehydrated and start to experience symptoms like headache, fatigue, chills, cramping, or gastrointestinal distress, we can probably kiss our performance goals goodbye.\nFirst, let's talk about the fluid component of your hydration strategy. On average, we lose about 12-32oz of fluid in sweat per hour or as much as 50-80oz on the hottest days. How much we sweat varies from person to person, independent of their environment. We don't need to replace 100% of what is lost in sweat, but aiming for at least 80% can help prevent significant impacts on performance.\nOptional Mini Experiment: To understand your unique sweat rate, weigh yourself without clothes before going out for a run. Then run for 60 minutes, keeping track of any and how much water you drink or if you use the bathroom. Then weigh yourself immediately after you finish your run without clothes. If you want to calculate your sweat rate, visit this calculator and enter the data you collected from this mini-experiment.\nThe goal is to prevent anything more than 2-3% body weight loss by replacing fluids and electrolytes during your activity. Studies show that performance is significantly impacted when we lose too much weight through sweat during training, and dehydration negatively impacts our overall health. In order to meet your hydration needs, consuming between 4-8oz of water every 10-20 mins during activities is recommended, especially for activities that last longer than 60 minutes. Remember that 1oz is about one gulp to make running math easier! On hotter days, you'll likely have to drink more water more frequently.\nElectrolytes include sodium, calcium, potassium, bicarbonate, magnesium, and chloride as the most notably lost in sweat, with sodium and potassium being the most significantly lost. Electrolytes help us maintain fluid balance, which allows our muscles to contract and relax. Since running requires many muscles to contract and relax, including our heart, maintaining proper fluid balance through adequate electrolyte intake during exercise is vital to maintaining your performance and keeping you safe and healthy.\nWhen we consume water only or not enough electrolytes, this can cause a condition called hyponatremia, which can be extremely dangerous or fatal as a result of improper fluid balance. Other common symptoms of dehydration or improper hydration (meaning you don't take in the correct ratio of electrolytes to fluid for your unique needs) include calf cramps, side stitches, gastrointestinal distress like vomiting and diarrhea, and muscle spasms that can also lead to injury.\nThe American College of Sports Medicine recommends between 300-600mg of sodium per hour during prolonged exercise and notes that athletes with saltier sweat may need up to 1200mg of sodium per hour. Research indicates that sodium loading prior to competition intended to achieve the status of euhydration- or being optimally hydrated- leads to improved performance outcomes in both men and women.\nYou may be thinking about how the health industry encourages Americans to limit their sodium intake- while you should consult with your registered dietitian and physician about what applies to you individually, typically, athletes need far more sodium than the average person to replace what's lost daily in sweat.\nJust like we all have different sweat loss rates, we also have different sweat compositions. Traditionally, it is challenging to assess your unique sweat composition without access to fancy equipment at university and sports performance research centers. Still, new products are being released like the Gatorade Patch that can help you assess and understand your sweat right at home.\nFor other electrolytes, meeting the recommended daily intake for athletes through daily diet should suffice, but as a dietitian, I typically recommend endurance athletes include between 100-200mg potassium and some magnesium and calcium in their hydration supplement during workouts longer than 90 minutes or on shorter workouts on hot days. Keep in mind that some forms of magnesium can cause gastrointestinal distress in those with sensitive guts, so it's best to look for magnesium citrate or chelated magnesium (otherwise known as magnesium bisglycinate) instead of magnesium oxide to avoid this.\nHow To Implement And How does this all come together?\nAim to drink 4-8oz water every 10-20 mins, especially on runs lasting longer than 60 minutes.\nAlternate every other drinking interval with plain water and an electrolyte beverage to avoid potential gastrointestinal distress from taking too much at once.\nWhile your running fuel like gels and chews may contain some electrolytes, read the labels and assess if you need an electrolyte supplement to meet the recommendation of taking 400-800mg sodium and 100-200mg potassium per hour.\nMy favorite brands are Skratch Labs, NUUN Podium series, Liquid IV, Stage Hydration, Elete, Ultima, Vitalyte, Generation UCAN Hydration, and Gatorade Endurance Formula.\nFind a vessel you can use to carry your hydration like a handheld water bottle, hydration vest, or practice stashing bottles on the course, or doing a loop course around your car or home.\nPractice with whatever you plan to use on race day to get your body accustomed to it ahead of time.\nKeep sweating and stay hydrated this summer!\nBelval LN, Hosokawa Y, Casa DJ, Adams WM, Armstrong LE, Baker LB, Burke L, Cheuvront S, Chiampas G, González-Alonso J, Huggins RA, Kavouras SA, Lee EC, McDermott BP, Miller K, Schlader Z, Sims S, Stearns RL, Troyanos C, Wingo J. Practical Hydration Solutions for Sports. Nutrients. 2019 Jul 9;11(7):1550. doi: 10.3390/nu11071550. PMID: 31324008; PMCID: PMC6682880.\nSims ST, Rehrer NJ, Bell ML, Cotter JD. Preexercise sodium loading aids fluid balance and endurance for women exercising in the heat. J Appl Physiol (1985). 2007 Aug;103(2):534-41. doi: 10.1152/japplphysiol.01203.2006. Epub 2007 Apr 26. PMID: 17463297.\nSims ST, van Vliet L, Cotter JD, Rehrer NJ. Sodium loading aids fluid balance and reduces physiological strain of trained men exercising in the heat. Med Sci Sports Exerc. 2007 Jan;39(1):123-30. doi: 10.1249/01.mss.0000241639.97972.4a. PMID: 17218894.\nVitale, K., & Getzin, A. (2019, June 7). Nutrition and Supplement Update for the Endurance Athlete: Review and Recommendations. Retrieved April 22, 2020, from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6628334/\nVon Duvillard SP, Braun WA, Markofski M, Beneke R, Leithäuser R. Fluids and hydration in prolonged endurance performance. Nutrition. 2004 Jul-Aug;20(7-8):651-6. doi: 10.1016/j.nut.2004.04.011. PMID: 15212747."", ""In the world of sports and athletic performance, meticulous attention is given to training routines, technique refinement, and sports nutrition. Yet, there’s one indispensable aspect that often doesn’t receive the attention it rightly deserves—hydration.\nHydration is not merely about quenching thirst. It’s a complex and dynamic process that has a direct and profound impact on an athlete’s physical function, cognitive ability, and overall performance.\nIn this article, we’ll explore the crucial role of fluids and electrolytes in sports, providing strategies to optimize intake for enhanced performance and overall health.\nThe Science of Hydration\nHydration refers to the process of absorbing and retaining sufficient water to enable the body's cells to function correctly. Water is integral to our bodies, making up approximately 60% of an adult's body weight, and is involved in numerous crucial functions, a few of which include:[*]\n- Nutrient transportation: Water acts as a transport medium in the body, aiding in the absorption and distribution of essential nutrients from the food we consume to all cells throughout the body.\n- Waste removal: It assists in flushing out waste products and toxins from the body. It plays a vital role in renal function, helping form urine in the kidneys, which excretes waste from the body.\n- Body temperature regulation: Perhaps most crucially for athletes, water aids in regulating body temperature through perspiration, a vital process during strenuous exercise when the body generates excess heat.\n- Maintaining skin health: Adequate hydration helps maintain skin elasticity and resilience, contributing to overall skin health.\n- Lubrication and protection: Water helps lubricate joints, reducing the friction between bones. It also acts as a shock absorber in the spinal cord and brain, protecting against injury.\n- Digestion: Water is crucial for digestion, aiding in the breakdown of food and the absorption of nutrients in the digestive tract.\nDehydration and Overhydration in Athletes\nDehydration, or the state of not having enough water in the body to carry out these necessary functions, can severely impact athletic performance. Even a small fluid loss—as little as 1-2% of body weight—can lead to a noticeable decline in physical and cognitive performance.[*]\nSigns of dehydration in athletes can include:[*]\n- Decreased urine output\n- Increased heart rate\n- Dry mouth and lips\nSevere dehydration can even lead to confusion, fainting, and in extreme cases, death.\nIt’s equally essential to note that overhydration, also known as hyponatremia, is a significant concern, especially for endurance athletes. Overhydration occurs when the body has too much water relative to its salt (sodium) level. The excess water dilutes the body’s electrolytes, leading to symptoms including:[*]\n- Muscle weakness\nIn severe cases, overhydration can lead to seizures, coma, and even death. This emphasizes the importance of not just the amount of water you take in, but also the balance of fluids and electrolytes in the body.\nElectrolytes: The Invisible Powerhouses\nAs crucial as water is to our bodies, it doesn’t act alone in maintaining our physical functions. This is where electrolytes come into play. Electrolytes are minerals that, when dissolved in water, produce electrically charged ions. During physical activity, the body not only loses water through sweat, but also sodium and other electrolytes.\nThese charged particles play a pivotal role in a range of physiological processes. For example:[*]\n- Sodium: Sodium aids in maintaining fluid balance, nerve signal transmission, and muscle contraction.\n- Potassium: Potassium is essential for maintaining healthy heart function and significantly contributes to the contraction of skeletal and smooth muscles. It also works in tandem with sodium to maintain fluid balance.\n- Calcium: Calcium is crucial for developing and maintaining strong bones and teeth. It also plays a vital role in muscle contraction, nerve signaling, and blood clotting.\n- Magnesium: Magnesium is involved in over 300 essential metabolic reactions, including energy production, protein synthesis, gene maintenance, muscle movements, and nervous system regulation.\nFluids and Electrolytes: The Athletic Connection\nHydration is integral to an athlete’s endurance and capacity for physical exertion. Water assists in maintaining the body’s temperature, lubricating the joints, and transporting nutrients to provide energy.\nOn the flip side, dehydration can lead to fatigue, reduced coordination, and muscle cramping, all of which can significantly hamper an athlete’s performance.\nSimultaneously, the role of electrolytes, particularly sodium and potassium, in athletic performance cannot be overstated. They are crucial for initiating muscle contraction and relaxation and supporting fluid balance.\nHydration and electrolyte balance also have a profound effect on cognitive performance. Both dehydration and electrolyte imbalance can impair concentration, coordination, and decision-making ability—aspects that are crucial in sports.\nIn endurance sports, where athletes may compete for hours under strenuous conditions, the risk of dehydration and electrolyte imbalances is heightened due to sweating. Consuming water alone won’t suffice to replace the lost electrolytes, making the use of sports drinks or electrolyte supplements an important consideration.\nTips for Effective and Safe Exercise Hydration\nExercise hydration isn’t as simple as drinking a glass of water before heading out the door. It involves understanding your body, your workout intensity, the climate, and how these factors influence your hydration needs.\nHere are some practical tips to ensure you stay well-hydrated during your exercise routine:\n#1: Drink Electrolyte Water\nWhen you drink water, make it electrolyte-infused water. Adding a high-quality electrolyte supplement, such as IQMIX, to your water replenishes both water and electrolytes lost through sweat and urine.\nThis strategy is particularly beneficial during intense workouts or endurance sports, where significant amounts of electrolytes can be lost.\nIQMIX provides a balanced blend of essential electrolytes (sodium, potassium, and magnesium) in a convenient, tasty format. Simply pour a packet of IQMIX into a glass of water to create an instant, sugar-free hydration solution.\nIQMIX is ideal for those with diabetes or those following a low-carb diet, as it contains zero sugar and only one gram of carbs per packet. Compare that to your typical 16-ounce sports drink, which contains between 28 and 38 grams of added sugar—equal to about 7 to 10 teaspoons! Learn more about why hydration drinks don’t need to contain glucose (sugar) to be effective here.\n#2: Drink to Thirst\nMany athletes guzzle electrolyte-free water on a set schedule, despite not being thirsty. There are two problems with this. First, without electrolytes, the athlete will not properly retain water. Second, guzzling water can lead to hyponatremia, a potentially fatal state of low blood sodium caused by excessive consumption of H20.\nA safer and more effective approach is to drink water to thirst. Thirst is your body’s natural alarm telling you when you need to replenish lost fluids. By drinking water according to your thirst, you allow your body to naturally regulate its fluid balance, taking in just the right amount of water it needs to function optimally.\n#3: Hydrate Before, During, and After Exercise\nHydration isn’t an activity confined to the period of exercise. An effective hydration strategy requires drinking electrolyte water to thirst before, during, and after exercise. This will support your performance and recovery.\nOur final tip? Don’t get too hung up in the details. Your body is intelligent and if you follow its cues, you should be golden. And don’t forget to have fun!\nWritten by Katie Koschalk, a health and wellness writer, certified holistic nutritionist, and certified personal trainer based in California.""]"	['<urn:uuid:a153404d-be93-4581-a9c1-88b31f214a56>', '<urn:uuid:edc8c3f8-ff6a-479b-b349-166d04bc3ec3>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T06:04:36.674696	27	96	2357
46	I'm learning to cook with different fats. What's the main difference between kefir butter and ghee?	Kefir butter and ghee are quite different cooking fats. Kefir butter is made by culturing cream with probiotic kefir grains before churning it into butter, which creates tart flavors and increases nutritional value. Ghee, on the other hand, is made by heating butter until the milk solids separate and brown, resulting in a clear, golden liquid with a high smoking point and no lactose or casein. While kefir butter retains its dairy proteins, ghee has them removed, making it suitable for those who can't digest lactose and casein.	"['Kefir butter is a term Bar Tartine uses to describe cream cultured with kefir grains and churned into butter. Kefir grains are opaque, probiotic colonies of lactic acid bacteria, yeast, lipids, sugars, and proteins that turn milk lactose into tangy lactic acid. Uncultured cream may also be used to make butter, but culturing increases its nutritional value and adds layers of tart flavors and aromas. Use the best quality cream you can find for this recipe. After making it, you’ll have both butter and buttermilk—and you can reserve kefir cream as well. Kefir grains are sometimes found at health food stores, and readily available online.\nWrite a review\n- Glass or plastic container large enough to hold at least 2½ quarts cream\n- Butcher string\n- Plastic wrap or clean kitchen towel\n- Ice, for ice bath\n- Small bowl, for ice bath\n- Medium bowl, for catching buttermilk\n- Stand mixer\n- Plastic wrap\n- Wax paper or small container with lid, to store butter\n- 2 quarts heavy whipping cream\n- 1 teaspoon kefir grains\n- Culturing Cream\n- 1. Pour cream into large glass or plastic container, allowing space at the top for carbon dioxide production while cream cultures. Wrap kefir grains in cheesecloth, making a loose pouch by tying ends of cloth with butcher string (once cream cultures and thickens, retrieving loose grains without cloth will be difficult). Tuck kefir pouch into cream, resting tied end over side of the bowl. Cover loosely with plastic wrap or towel and let sit at room temperature for 24 to 48 hours.\n- 2. The time required for the cream to culture depends on the temperature of the room in which it’s sitting (72°F is ideal; otherwise, it may take up to 48 hours to culture). Cream is done culturing when it becomes as dense as sour cream or thick yogurt. Don’t worry about its flavor; once the cream is this thick, it has cultured, and will naturally taste as tangy as crème fraîche.\n- 3. Once cream is cultured, remove cheesecloth pouch. (Chef Cortney Burns notes that kefir grains may be re-used forever; to do so, retrieve grains from pouch and place them in a container filled with a pint to a quart of milk and refrigerate. This will put them “to sleep” until you’re ready to use them again. Make sure to change the milk every seven days.)\n- 4. Refrigerate kefir cream for three hours or overnight.\n- Making Butter\n- 5. Prepare an ice bath by filling a small bowl with roughly one-quarter ice and the rest water. Place colander over a medium-size bowl and set aside.\n- 6. Add 4 cups kefir cream to the bowl of a stand mixer. Use the machine’s splashguard, or tent the opening between the bowl and top of the machine with plastic wrap. Turn mixer on medium. Once cream begins to froth, turn mixer to high.\n- 7. Watch cream closely, and stop machine once butter has separated from buttermilk, about 4 to 7 minutes. Once separated, butter curds will be about the size of cottage cheese. Pour into a colander lined with cheesecloth and set over a bowl to catch buttermilk.\n- 8. Next, immerse the butter curds in an ice water bath, squeezing butter mass under ice water to expel more whey. Squeeze until water pressed from the butter runs clear.\n- 9. Remove butter from water and squeeze again to extract remaining liquid. Repeat with remaining cream.\n- 10. Reserve 1 cup of unsalted butter for pound cake, and add salt as desired to remaining butter.\n- TIP: If making Burns’s pound cake, reserve 1 cup cultured cream for the garnish. If making her seeded buttermilk crackers, be sure to save the buttermilk produced during the butter-making process at right; it will keep in the refrigerator for two weeks.\nculture: the word on cheese http://culturecheesemag.com/', ""Ghee is the traditional cooking fat used by people from India. It’s the most delicious, as well as healthy, fat that is used as medicine in Ayurveda to cure many ailments. Ghee has recently been popularized all over the world and is available in every ethnic grocery store, but many people make it at home. That sounds like too much work, you say? The task of ghee-making is not as difficult as you may think, however.\nIf you are making ghee from grass-fed cow milk, then the ghee will be rich in vitamin A, D, E, and K as well as the beneficial fatty acid conjugated linoleic acid.\nApart from this nutritional value, those who cannot digest lactose and casein (the milk protein) can also use ghee because these compounds are removed from milk while you make ghee.\nMoreover, the high smoking point of ghee makes it a stable cooking medium and you can use it to stir fry, sauté, or deep fry your foods.\nPeople usually look at me strangely when I tell them that I make ghee at home. They probably think I am either from a different planet or from a different time period. Because who in their right mind would even imagine of spending time in the kitchen to make ghee, when you can just buy every single thing in the supermarket?\nI actually have lots of childhood memories of my Mom standing at our stove making homemade ghee with me staring at her every move. She started the process directly from the raw milk that was delivered to our door every morning by the milkman. Upon receipt, the milk was boiled at medium heat and then let cool. My favorite part was the thick, creamy layer of skin that formed on the cooling surface of the milk but, instead of giving me a taste, my mom would store it in a separate covered bowl.\nOnce the bowl was all filled up or when she planned to make some poori she would pour all the saved cream from the bowl into a wide mouthed glass bottle along with some filtered water.\nI fondly remember the fun part of making ghee was watching her vigorously shake the closed bottle. After some time, the butter separated from the liquid part and floated to the top. It all seemed magical at that time.\nShe then heated the fresh yellow butter until all the solids melted and produced the golden ghee – the fruit of our shared effort of making it. Of course my part of the effort was just looking over her shoulder. I always appreciated my Mom’s lovely creation, but what excited me more was the thought of that night’s dinner with large, puffy, deep-fried bread – a family favorite.\nI still enjoy making ghee myself and remember all those happy times I spent with my Mom working in the kitchen. I agree that it’s a little more work than necessary but I can tell you that it’s totally worth it.\nNow it’s your turn!\nIf you want to start the process from scratch, you’ll have to collect the cream from the milk. This is not quite possible since supermarket milk is pasteurized and homogenized so that the large fat globules of milk are broken into very tiny molecules that hardly float to the top of the heated milk. To really get some good cream, you will want to find a source of raw milk from local farms. Because the majority of us do not have this privilege, grass-fed cream or butter will also work.\nThe two stages from where you can start making ghee are:\n1. From Cream: Agitate the cream vigorously to separate the fat from the remaining liquid part, known as buttermilk.\n2. From Butter: Simply heat the butter until the point when the milk solids separate to yield a clear, gold-colored product called ghee.\nFirst we'll talk about stage 1 - from cream. To agitate the cream, you can use a simple glass bottle with a tight fitting lid, as my Mom was using, or you can also use a blender or mixer, for convenience.\nI always use the glass bottle method since I usually do not make ghee from a very large amount of either cream or butter and therefore the glass bottle is sufficient to separate the butter. If you want to use a mixer, then you’ll have to experiment to find the best way for making butter from the cream.\n• Put a pint of heavy whipping cream into the device you would like to use and start shaking it vigorously. At first, the cream will form peaks - from soft peaks to stiff peaks - after which it will break and the fat will separate from the liquid part.\n• Continue agitating the contents for some more time in order to pull all the butter globules to collect as a single blob.\nDo not discard the liquid part - this is buttermilk, which you can save to use in your favorite recipes. Strain the buttermilk into a clean glass bottle.\n• Collect the butter in a small bowl and wash it under tap water couple of times until clear water comes out. It is important to remove any buttermilk that may remain in the butter.\nNow you have old-fashioned butter! You can stop here if you want to make only homemade butter. At this stage, you can add salt and herbs to flavor it.\nBut if you want ghee then proceed to the following step.\nThis is where stage 2 starts – If you have store-bought butter, start here!\n• Place the butter in a saucepan over medium-high heat and bring it to a boil (this will take about 2-3 minutes). Once the butter starts to boil and sputter, lower the heat to medium and let the butter simmer. You will see white foam forming on the surface of the liquid butter. Do not try to remove the foam. You will see the foam slowly start to disappear after a while and the butter underneath should be bubbling.\n• Continue simmering the butter. You will start getting a pleasant, mild, roast aroma and you will see the foam reappearing on the surface. This means that your ghee is just about ready.\n• At this time, if you try to push the foam to the side with a metal spoon you will see the liquid butter has changed its color to a light golden brown and the milk solids are brown and settling at the bottom of the saucepan.\nTurn off the heat and remove the saucepan from the stove and let it cool.\n• After a few minutes, gently pour the ghee through a fine muslin cloth to trap all of the brown milk solids.\nYou can also use a fine mesh strainer but some brown bits of milk solids might escape through the mesh and settle at the bottom of the glass bottle where you will store the ghee. This won’t really affect the taste of the ghee. Discard the brown milk solids in the muslin/strainer.\n• Let the ghee cool at room temperature until it is solid and slightly deep gold in color. Ghee can be stored at room temperature for a couple of months.\nHow To Use Ghee\nIf you are thinking that ghee can only be used to make Indian food, you are missing out! Ghee is a versatile fat and it’s flavorful, but less than coconut oil or other animal fats like duck fat or tallow. The following is a list of many ways you can incorporate this healthy and delicious fat into your daily diet:\n· Spread on toast as an alternative to butter\n· Melt it and pour over grilled fish or scallops, or use it as a dip for lobster or crab legs\n· Use it to stir-fry your favorite foods\n· Use it to replace oil while making homemade mayonnaise\n· Use it to cook your morning eggs and sausage\n· Spread it on warm tortilla to make tacos or burritos\n· Pour melted ghee on bean soup\n· Use it to make fried rice or rice pilaf\nDr. Gayatri Borthakur, a nutritionist turned entrepreneur, has a PhD in nutrition and a passion for delicious but healthy foods.""]"	['<urn:uuid:bad4c1da-3f02-4675-92a5-27aa345c0a81>', '<urn:uuid:e3e34336-8ece-44b4-8c4e-8749be1420dc>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T06:04:36.674696	16	88	2037
47	Is white wine better served too cold or too warm?	Neither extreme is good for white wine. When served too cold, important flavors like fruits, florals and spices get masked, and the wine's 'edges' become more prominent. When served too warm, the wine becomes more alcoholic and its aromas become too volatile and intense. The ideal serving temperature for white wines varies: light, young whites should be served at 7-10°C, while fuller-bodied whites are best at 10-13°C.	['THE LIFE OF WINE.\nAlthough it surprises many people, one must take into account that wines behave like living beings: they are born, grow, come into full force and eventually die. The longevity of a wine depends on its initial aptitudes and storage conditions. The saying, “the older the better” is not true. Many do not improve over time, even those that are well preserved. But others, due to their structure, vintage and origin, become greater.\nIt is important to know that age is not synonymous with evolution, that is, there are older wines that will keep even better than younger wines. Therefore, we should distinguish between the four main groups:\nYou should also know that wine, when it has the ability to age well, does so better in larger containers. In large bottles, wine remains younger than in small containers, because it is contact with the exterior, through the glass, that makes it evolve. This is easy to understand if you compare it with cheese (which is also a living food which evolves): a one-half kilo cheese will not last as long as a whole kilo cheese.\nFORMAT, PRESENTATION, AND STOPPERS.\nToday there are more and more types of containers for wine, but the glass bottle remains the most common. Traditionally, its form corresponded to productions in specific areas (Bordeaux, Burgundy, Jerez), but with the development of marketing and innovation, this tendency has been lost. Now you can find all shapes and colors, including unique models related to a specific winery. Dark colors are usually used for wine that will be aged, since they preserve wines from the effect of light better.\nThe formats vary widely, although the most common remains three-fourths of a liter or 750ml. The bottles of 3/8, 375ml, or a half liter (500ml), are good choices for moderate consumption and are the most accepted (be careful about the vintage, because the wine evolves more quickly). Among the largest, the most common is called the magnum, holding 1 1/2 liters and ideal for a festive meal for four people. Beyond that, although it is less common, you can find the double magnum.\nCurrently, there is a new container showing up called BIB (Bag in Box), which is more and more popular abroad for young wines. As its name suggests, it is composed of a bag inside a box, packed with a tap. It has the advantage of a self-draining system, which prevents the wine’s contact with air and keeps it in good condition until its final consumption. In addition, some new formats in aluminum cans or bottles are even beginning to gain ground.\nAmong the stoppers, the ultimate still remains the cork. Corks vary greatly in quality, formats, and prices. The highest quality is found in natural cork of great length (55mm), but there are other, cheaper options, such as plywood and composite plugs. If the corks become contaminated, they pass on an unpleasant and characteristic musty cork smell to the wine (or as they say in France, “bouchonné”), which spoils it.\nHence the development of synthetic corks (made of plastic) and the screw cap, which avoid this problem and are becoming more common in young wines. These have been being tested for quite a long time, and meet all the conditions for proper conservation of wines that do not require aging in the bottle.\nDespite these trends, the makers of the finest wines in the high-end category still prefer to use traditional materials such as glass bottles and natural corks.\nWhat most affects wine, like any other food, is temperature and light, hence the statement, “Store in a cool place.” Food was traditionally always stored in warehouses, often underground, where it was always cool and dark. Today it is hard to find a place like this, especially when living in the city. In an apartment, always look for a place with few temperature swings, the cool est spot. The kitchen is usually the worst place, as there are many sources of heat. A closet is much better; keep away from radiators and put bottles on the floor, where the temperature is lower. Assuming that this situation is not ideal, try not to keep the wine too long (a few months) or go shopping according to your consumption.\nBottles should be stored lying down so that the cork is kept moist and prevents them from losing liquid. The ideal temperature is between 54 ºF and 65 ºF / 12 ºC and 16 ºC. When the temperature is high, the wine ages more quickly, and if it is too low, it may encourage the formation of sediment. A slight shift up or down is not serious as long as the temperature remains more or less constant.\nHumidity is also important, as are extremes: in the desert and the tropics, wine keeps poorly. It should be kept at about 70-80% humidity. A dark place, as we have said, is fundamental, and furthermore, it should be quiet and without vibrations (the room next to the elevator is not advised). It should also be well ventilated, to prevent odors.\nIf you cannot meet these conditions, at least the basic ones, there is also the option of buying a portable “cellar,” a thermo-regulated box, often with humidity control, designed to store wine.\nBy now you have learned how to taste, to better understand wine (namely, to choose what you like), and now we have reached the turning point, consumption, which can spoil everything. A bad wine served too warm in a coarse glass, and in bad company, can be downright unpleasant.\nWe again touch on the temperature of the wine because it is incredibly important when serving, not only because it can take away much of the pleasure, but because it markedly changes aromas and flavors.\n– Aroma: The higher the temperature of a wine, the more volatile the aromas and, therefore, the more intense they will be to us. Above a certain temperature, wines come to seem alcoholic and burning. In contrast, the lower the temperature, the less aromatic it will be.\n– Sweet: The sensations of sweetness and alcohol are enhanced with increasing temperature, so sweet wines must be served cold (but not freezing cold).\n-The fiery nature: The sum of the acidity and alcohol is also enhanced with heat.\n-Saltiness, bitterness and astringency: These, in contrast, are reinforced with decreasing temperature. This is why a structured red should never be served cold.\n– Hardness: The sum of the acidity and tannin, this is enhanced by the cold.\n– Effervescence: Carbon dioxide is released more quickly if the temperature is high. Therefore, sparkling wines should be served cold and kept in a bucket of ice water throughout the meal.\nOPTIMUM TEMPERATURES FOR SERVING VARIOUS WINES\n|– Fine young whites . . . . . . . . . . . . . . . . . .. 45-46 ºF / 7-8 ºC||– Strong, full-bodied reds . . . . . . . . . . . . 57-63 ºF / 14-17 ºC|\n|– Sweet whites . . . . . . . . . . . . . . . . . . . .. . . 41-43 ºF / 5-6 ºC||Aged reds . . . . . . . . . . . . . . . . . .. . . . . . 61-65 ºF / 16-18 ºC|\n|– Sparkling wines and Champagnes . . . .. 43-50 ºF / 6-10 ºC||– Amontillados . . . . . . . . . . . . . . . . . . . . . 54-59 ºF / 12-15 ºC|\n|– Whites made with oak . . . . . . . . . . . . . 50-54 ºF / 10-12 ºC||– Oloroso Sherry . . . . . . . . . . . . . . . . . . . 57-61 ºF / 14-16 ºC|\n|– Rosés . . . . . . . . . . . . . . . . . . . . . . . . . . . 46-50 ºF / 8-10 ºC||– Port and Madeira . . . . . . . . . . . . .. . . . . 55-57 ºF / 13-14 ºC|\n|– Light young reds . . . . . . . . . . . . . . . . . . 54-59 ºF / 12-15 ºC|\nIdeally, wine should reach the optimal consumption temperature gradually (don’t stick it in the freezer!). Perhaps this is most problematic in the service of red wines. You may think you have to serve them at room temperature, which is what “they” say to do, but this refers to what the temperature used to be in houses (61-64 ºF / 16-18 ºC), not the temperature we are used to keeping our houses at now (75-82 ºF / 24-28 ºC). So, if one day you are served a red that is too warm, don’t hesitate to ask for a bucket of cold water and a few pieces of ice. Within minutes, the wine will improve considerably.\nDrinking a good glass of wine is not only a small luxury but also a great pleasure. Of course, a basic wine in a wineskin, a porrón (a traditional glass wine pitcher) or even a beer glass can be fine in the right time and place, but you would never drink a great wine in these conditions; there is a lot of work and effort behind it, and it deserves much better treatment.\nWhen choosing glasses, look for a tall stem (so you can hold it by the foot of the glass, which prevents the bowl from getting dirty from the oil on your hands and the wine from being warmed) and crystal, or clear, uncolored glass (colored or buffed glasses will not let you appreciate the visual nuances of wine). As for the form, it’s best if the bowl is not too open (to concentrate the flavors) and not too large. On certain occasions and for special wines, it might be nice to use a large glass, but in general choose a versatile glass that can be used with different types of wine (white, rosé and red), and between 10 and 13 oz. capacity.\nThere are manufacturers that have optimized the adaptation of glasses for different types of wines, developing models of different forms, not only to improve your sense of smell, but also to direct the liquid to one or another area of the mouth. These can be fun but are not essential.\nBy contrast, for sparkling wines, it is crucial to choose a fluted glass (the elongated shape) which allows the carbon dioxide to gradually be released. Always avoid drinking from very open glasses (unfortunately quite typical), which are much better for shrimp cocktail or ice cream.\nNow we have the wine at the correct temperature and have chosen a glass, but there is a doubt about whether or not to decant the wine – because we’ve all heard that we should do so and that it’s good for the wine. Decanting the wine from the bottle is an operation that consists of separating the wine from the sediment that has formed over time during aging. However, this action always entails a certain aeration which, depending on the type of wine you have in hand, can be beneficial or harmful.\nRegarding the old question of whether to open a bottle for a while before you serve it, the answer is clear and simple: no. If a wine needs to be aerated before consumption, the gas exchange that takes place through the neck of the bottle is almost zero, since the contact surface is very small. In this case, transfer the contents of wine to a spacious decanter, where it will get the sought-after aeration.\nTo give some general guidelines, you could say that aeration is good for powerful, aged wines, full of youth and tannins. With it, you will achieve the effect of softening the tannins and allow the aromatic potential of wine to be expressed and intensified. However, you must be very cautious when decanting an aged bottle, with a delicate and fragile bouquet; it can be completely ruined by an abrupt and inadequate aeration. In such cases, handling the bottle with care is sufficient to avoid serving sediment.\nSometimes, what you want is to oxygenate wine quickly, because it is very close and you know that it doesn’t have any sediment. In this case, look for a large pitcher and transfer the wine energetically, so that the liquid hits against the glass and increases the aeration effect. We call this aerating wine.\nFOOD AND WINE PAIRING.\nThe harmony between wine and food or, as it is often called, pairing (from the French “marriage,” meaning marriage), is an exciting field where many factors come into play: the physiochemical reactions occurring and how we perceive them through our senses, one’s personal taste, cultural backgrounds, the association of ideas, etc.\nWhen thinking about the pairing, we must consider all the sensations which both wine and food produce in us: the flavors (sour, salty, sweet, bitter), tactile and chemical sensations (texture, spiciness), temperature (which greatly changes our perception), and aromas.\nIn general, the majority of pairings are established by association or complementariness: color (whites with white meat, reds with red meat), flavors (sweet desserts with sweet wines), sensations (fatty foods with tannic wines), aromas (smoked foods with oaky whites), or intensity (strong foods with full-bodied wines). But sometimes, contrast pairing – like blue cheese with sweet wine – can yield excellent results.\nObviously, there are also foods that are difficult to pair with wine, such as those based on very bitter foods (like artichokes) or very hot and spicy (spices). Don’t think that common pairings are definitive or that there are any hard and fast rules to it. Curiosity and the pursuit of pleasure should prevail. Allow your personal preferences to lead and remember that the combination possibilities are endless. Here are some ideas, depending on the type of wine:\n– Simple young white wines: a light appetizer.\n– Noble varieties (Chardonnay) of young white wines: white fish, goat cheese.\n– Oaky Chardonnay: fatty fish (sea bream, turbot), smoked fish or meats, white meat.\n– Rosé wines: vegetables, pasta, rice, cured meats, soft cheeses.\n– Young red wines: cured meats, pâtés, medium cheeses, bluefish and even some dishes made with salt cod.\n– Crianza red wines: grilled meats, roasts, stews, cured cheeses.\n– Reserva, Gran Reserva, and Vanguardista (“cutting edge”) wines: more subtle and elaborate game dishes and stews.\n– Sweet wines: fruit-based desserts for lighter wines, tarts with dried fruits for denser wines.', 'In Millesima Tips #6 on possible ways to store bottles of wine, we mentioned Wine Serving Cabinets, which bring bottles of wine to the correct serving temperature before they are opened and tasted. In addition to a careful opening of the bottle, proper wine service also includes making sure that the wine is tasted at the correct temperature. This is because even if the wine is of excellent quality, the bottle aged under the right conditions, then opened and decanted in the correct way, the tasting may still go wrong if the wine is served too hot or too cold.\nWhy Temperature Matters\nAs with anything that can be tasted, serving temperature affects the flavours observed in a wine. While warmer temperatures generally bring out various elements of a wine’s aromatic bouquet and flavour profile, colder temperatures tend to mask subtler notes. Think of how a glass of whiskey served neat will express so many nuances, while a whiskey on the rocks will seem somewhat simpler. At the same time, a chilled whiskey will taste and smell somewhat less alcoholic than one served at room temperature. This is also true in the case of wine. Of course, temperatures over a certain point can “cook” a wine, replacing the flavours of fresh fruit with those of stewed, baked or caramelised fruit. And temperatures under a certain point can freeze it, causing water in the wine to separate and expand, pushing out the cork and causing oxidation. But even between these extremes, there are many different ranges, each corresponding to a type of wine. Let’s take a look at some of the most common styles.\nServing Temperature for Sparkling Wines\nSparkling wines, like Champagne, Franciacorta and Prosecco should be chilled before serving. Refrigerating them for around 2-3 hours or keeping them on ice for around 30 minutes will bring them to the right temperature, which lies somewhere between 5 to 10 °C. When served too warm, bubbles of carbon dioxide inside a bottle of sparkling wine will be more foamy than fine. Upon popping the cork, you may lose up to half of the volume, as the wine may foam wildly out of the bottle. Chilling will also bring out the crisp and fresh qualities sought after in a bottle of sparkling wine.\nServing Temperature for White Wines\nWhite wines occupy a wide spectrum from dry to sweet, and each style has its own serving temperature. In general, white wines that are young, aromatic and crisp are fully expressed at lower temperatures, around 7-10 °C. Think Pinot Gris or Chablis. Meanwhile, fuller-bodied, mature and complex white wines (like oaked Chardonnay, Viognier and Semillon), open up at temperatures between 10 and 13 °C. While it is generally understood that white wines should be served at colder temperature than reds, people often make the mistake of serving them too cold. Low temperatures brings out the “edges” of a wine and serving them below the recommended temperature may mask many of its most important flavours: the fruits, florals and spices. If a wine is accidentally cooled to too low a temperature, just keep it out for a bit longer before serving. While it is best to keep Champagne chilling as you drink it, white wine can be left out to express changing flavours as it gradually warms.\nServing Temperature for Rose Wines\nRose wines are the most seasonal, popularly consumed between spring and the early months of autumn. They should be served at a temperature between 9 and 13 °C. Generally, the drier the rose, the cooler it should be when tasted.\nServing Temperature for Red Wines\nJust as common as over-cooling white wine is not cooling, at all, a red. And while it is generally true that red wines should be served at higher temperatures than whites, this definitely does not mean that chilling is unnecessary across the board. Red wines served too warm can seem too alcoholic, while excessively low serving temperatures can make a wine taste too harsh and tannic. Most red wines should actually be served cool, between 12 and 20 °C (as opposed to room temperature, which is around 24 °C). Light reds like Pinot Noir and fruity Beaujolais actually benefit from being served at a colder temperature (similar to most whites), between 10 and 12 °C. Medium-bodied reds like Merlot can be served between 12 and 15 °C. Finally, full-bodied and tannic wines like Syrah or Cabernet Sauvignon are best served at the warmer end of the spectrum, around 18 °C. Since there is a very high degree of variation in serving temperatures for red wines, we recommend consulting with your seller to make the right choice.\nServing Temperature for Dessert Wines\nIn the case of dessert wines, the serving temperature should correspond to the characteristics you would most like to accentuate in the wine. Fortified dry wines, like Marsala or Jerez, and sweet wines like Tokaji can be served at cooler temperatures (10-14 °C) to accentuate freshness and to suppress the alcohol in the wine. Dessert wines like Port are best served at higher temperatures (14-18 °C) to accentuate the complex flavours and sweetness of the wine. Beware, however, that a warmer temperature will also bring alcohol to the forefront. This does not always harm the tasting, especially when the wine is paired to dessert or cheese.']	['<urn:uuid:239315d6-0ba9-4bf3-90ac-f4c007653cbc>', '<urn:uuid:0052b2a9-43c6-4319-bcc6-165b4ff61b67>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T06:04:36.674696	10	67	3346
48	What challenges do birds face with changing spring timing?	Birds are facing significant timing challenges as migratory birds arrive two days earlier every 10 years, while spring is advancing by seven days over the same period. This creates a mismatch where birds are no longer migrating at the optimal time for nesting and feeding chicks. This misalignment can lead to difficulties in feeding offspring and ultimately result in sharp population declines.	['Rising temperatures, sea levels, retreat and melting of glaciers… global warming has many impacts on the environment. Lotois birds are beginning to suffer the effects, and not necessarily for the better.\nThe three parts of the IPCC report, the last of which was released in April, alarm the population on the urgency to act in the face of global warming. The consequences on the environment are numerous. The four winds, the birds do not escape.\nAt the LPO Occitanie, Lot delegation, some phenomena are beginning to be observed. Even if Nathan Finderie, in charge of environmental studies, warns: these changes are linked to global warming, of course, but not only. Other factors can sometimes be taken into account, such as the disappearance of habitats (in particular with the mechanization of agriculture, the use of phytosanitary products, the disappearance of hedges, etc.).\nDisrupted migratory flows\nThe biggest upheavals in the lives of birds due to climate change relate to the alteration of migratory flows. Most birds traditionally spend the winter in Africa and return to Europe for the spring. But the winters becoming milder here, some species no longer find it useful to migrate, a journey that is often long and perilous (it is necessary, for example, to brave the sandstorms of the Sahara).\nThis is the case of the hoopoe. This little bird, with its recognizable shrill song, is still in an in-between. Some individuals continue to migrate, and only return to the Lot between March and August, the usual route. But others become sedentary in the department. If the Lot winters continue to get milder, the entire hoopoe population is likely to stay in the area, and this for good.\nThe bet to migrate, or not, remains specific to each individual. Risks exist for both solutions: the dangers incurred during a migration can kill some birds. And conversely if the winter gets colder, it is the sedentary people who risk perishing, not being used to this kind of temperature. “This does not, for the moment, endanger the survival of the species”, optimizes Nathan Findie, since it splits in two.\nMoreover, between the birds that have been sedentary for a long time and the newcomers, a competition for food could be created. Especially in winter when resources are scarce.\nFinding food can become a complicated task. First, migratory birds arrive two days earlier every 10 years. With global warming, spring is advancing by seven days over the same period. “This will inevitably create a shift, observes Nathan Findie. The birds are trying to adapt, but they no longer migrate at the optimal time to nest and feed the chicks. This can lead to a sharp drop in population.”\nAnother danger: extreme episodes (such as drought or frost) that occur during periods that are usually temperate. “The birds will, once again, have difficulty feeding the chicks,” explains the researcher. And who says poor reproduction, says population decline and therefore the risk of the extinction of an entire species.\nThe arrival of new species\nWith warming temperatures, the department sees the arrival of new species, which until then lived in much warmer countries. This is the case of the white kite, a small raptor. These birds were mainly found in Africa, but in recent years they have moved up to Europe. They are now found in the white Quercy. They coexist, for the moment, very well with the emblematic kestrel.\nOverall, generalist species, which adapt very quickly to a change in environment, will have few problems. In the Lot, this is the case for the magpie. A slightly different observation in specialist species, which can only live in a single environment. Like the ortolan bunting that is found a lot in the department. This small bird lives in areas dotted with trees, such as meadows or clearings, and nowhere else.\nRead also :\nLot: the IPCC report worries, including the head of the Conservatoire d’Espaces Naturels\nHelp preserve wildlife\nThe LPO Occitanie, delegation of the Lot, is committed to the environment and in particular to different species, whether they take flight or not. For example, special attention is given to the ocellated lizard (largest lizard in Europe), which has a very important stronghold in the Lot. This species, classified “Vulnerable” on the national red list of threatened species established by the IUCN in 2015, needs to live in open environments to facilitate its movements. With global warming, this reptile moves one kilometer per generation (knowing that they live on average 5 to 6 years). It is therefore necessary to clear the rocky surroundings so that this ocellated lizard can continue to exist in the Lot.']	['<urn:uuid:76c4bb3c-d164-4d3a-8331-698c4e809ac3>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T06:04:36.674696	9	62	773
49	I'm curious about old flour mills - how many existed in the US?	In the 1850s, there were estimated to be around 22,000 stone flour mills throughout the United States. In the Hudson Valley region specifically, there were 21 operating mills in the Croton area alone following the American Revolution.	['In Februray of 2020, research published in The Journal of Food Science revealed that “whole grain” foods marketed across the United States contain as little as 9 percent actual whole grain.\nThe discrepancy results from there being no certification guaranteeing whole grain content (as opposed to, for example, the USDA-certified “Organic” seal). Even the “Nutritional Facts” table on foods can be misleading as manufacturers add bran to white flour to distort the ingredients in and nutritional value of their products, rather than using the whole grain flour consumers would justifiably expect.\nLuckily, Hudson Valley residents can avoid being duped. Local grain mills ensure that residents have access to true whole grain products, while also keeping alive an industry that traces its roots in the region back to the 17th century.\nLocated in Clinton Corners, Wild Hive Farm sources grains like corn, wheat, spelt, rye and oat from regional farms and grounds them on site each week using a stone mill. Previously reserved for renowned New York City restaurants like Gramercy Tavern, Wild Hive Farm now offers its products directly to individual buyers both online and through regional retailers.\nFor Don Lewis, founder of Wild Hive Farm, genuine whole grain is about simplicity. “‘Whole grain’ is a term we use to describe a straightforward process of making flour,” he says. “Unlike commercial flours, ours is the result of one process on our stone mill. There is no reappropriation of ingredients.”\n“Commercial ‘whole wheat’ flour is the result of a high heat milling process creating white flour, which then has bran — from who knows where or what grain — added back into it,” he says. “The process removes all the natural vitamins and minerals, so the producers have to re-enrich it with synthetics. For us, whole grain means we take a bunch of grain, mill it, and get flour all from a single source and process.”\nLewis describes Wild Hive Farm as the only full-time stone flour mill in the Hudson Valley, but its work carries on a long tradition in the region.\n“Europeans introduced mechanical, water-powered milling to the Hudson Valley in the 17th century,” explains Rob Yasinsac, operations manager of Historic Hudson Valley, a nonprofit committed to preserving and sharing the region’s past.\nThere are estimated to have been some 22,000 stone flour mills throughout the United States in the 1850s. Gristmills could be small and inconspicuous, making their Hudson Valley numbers hard to pinpoint. But Yasinsac notes that there were 21 operating in the vicinity of Croton alone following the American Revolution — nearly a century before the height of wheat production in New York State, from the 1860s to ’80s.\nMilling was so significant in the Hudson Valley, in fact, that Historic Hudson Valley still operates a gristmill from 1682 at the Philipsburg Manor in Sleepy Hollow. While primarily intended for educational purposes, the Philipsburg Manor mill can yet produce cornmeal, which Historic Hudson Valley offers for sale during its annual “CORNucopia” celebration.\nHistoric sites like Madam Brett Park in Beacon and the Tuthilltown Spirits distillery in Gardiner were also once homes to gristmills, but aside from Wild Hive Farm, few other local milling operations remain. Yasinsac, for his part, can only point to the restaurant Blue Hill at Stone Barns in Pocantico Hills, which mills its own wheat in-house.\nIn addition to dietary concerns, the absence of local mills can be seen as exacerbating environmental concerns as well. For Lewis, the genuine whole grain products produced by Wild Hive Farm are not just about truth in advertising, but about creating more sustainable systems of food production, too.\n“Ethically, we must all contribute to minimizing our impact on the environment,” he argues. “Part of that is rebuilding our regional food systems to build a more sustainable world. Regional or local food systems are built differently than the usual corporate models that have dominated for so long.”\nThat said, Lewis also believes that Wild Hive Farm’s genuine whole grain products also simply taste better than their adulterated counterparts.\n“The quality difference between flour milled this week and flour milled this year — well, they’re just not the same product,” he says. “What does a considerate customer want to eat? Fresh food or old food?”\nVideo courtesy @jonbowermaster, Hudson River Stories']	['<urn:uuid:62200822-d5aa-4323-b18d-9598838f3193>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	13	37	712
50	living downtown apartment with small backyard what plants avoid messy garden	For a small urban garden, you should avoid plants labeled as 'good for naturalising' or 'vigorous', as well as those that spread via underground roots or self-seed. Also avoid plants that are prickly or spiky. Instead, opt for evergreen and scented plants. Be cautious with climbers as they can become tangled and messy, especially in shady areas where they'll grow toward the light leaving woody stems behind.	['How to Design a Garden if…it’s a city garden\nHow to Design a City Garden.\nMore and more people are living in urban areas so what do you need to consider when designing a city garden.\nThis is a common problem, you look out of your window or back door and all you can see are other buildings and fences. How do you create a garden that feels like you are in your own little oasis when all around you are windows and walls?\nIt’s all about tricking the eye and fooling the senses.\nThe more your garden holds your interest through sight, touch, sound and smell – not forgetting taste…. the less interest you have in the world outside.\nTake this example of a small city garden space\nSo far, there is a small area for a lawn, a path, a large dividing hedge and a patio at the end. So what to do?\nYou want to maximise the available space you have, but also include attractive looking plants that provide some privacy. The best shape of plant to choose are tall but skinny plants. Here are some tips:\n- Avoid anything spiky (no matter how ‘architectural’ it may look) in pots or in your borders ; no-one likes being impaled!\n- Keep to a simple design and avoid irregular shaped borders.\n- It is better to have 1 large impressive border in a small garden than lots of narrow or small borders.\nLight or Privacy?\nThis is always the paradox with small urban gardens, to create a more private space, you need to plant tall – this however cuts down the available sunshine and increases the amount of shade. You need to decide what is more important for you.\nIf you choose privacy, then you can create a tall oasis of ‘jungle’ plants. Use lots of different foliage shapes and textures and only use 1 or 2 flower colours (with one being white).\nIf you prefer to maximise the available light, then you have more of a choice with the planting styles, bold and bright colours will capture your attention more than a pastel palette holding your vision inside your garden.\nHowever choose plants that will move in a breeze and also those with plenty of scent. Our eyes are conditioned to notice movement, as such swaying plants are more interesting to look at than boring shrubs. Secondly as you are enclosed, the perfume from scented plants will stay in your garden for longer, so you can enjoy it more!\nWe all love a lawn, but in a small garden you really need to ask whether you can grow a good lawn and if not whether you really should have one at all.\nLawns need sunshine to grow lush and thick. The smaller the lawn, the greater the proportionate wear and tear, so you quickly develop muddy patches and threadbare bits. Plus, you need a mower and sheds take up a lot of space in a small garden.\nPaths and Patios must be low maintenance, so choose the right medium for the right place.\n- In shady spots, you will get green algae growing, which is slippery and ankle-breakingly slippery on decking.\n- Light coloured paving will go green too in the shade.\n- Decorative stone paths (not pea shingle) are impossibly difficult to rake clear of leaves.\n- Pine needles are even worse to try to rake or remove, so have a sweepable path or patio if you get needle drop.\n- Decorative concrete setts / block paving have lots of joins – so lots of places for seeds to germinate, so a putting an ornamental grasses garden upwind of your patio…you know you will have work to do!\nMessy Trees & Untidy Plants:\nIf you read lots of garden magazines you will get lots of tips for plants and trees for small gardens, many will advocate the virtues of fruit bearing trees as these trees have ‘something going on’ all year round and that is great.\nBut no one shows you the picture of the red or purple bird poop droppings everywhere or the rotten fruit that has fallen on the ground attracting wasps nor do they mention that the fruit drop stains the patio.\nWe think these things are actually important, the boss’ garden at home is surrounded by birch trees – ‘ooh how wonderful’, according to garden design books.\nBirch trees cast a light dappled shade, are not too overpowering, have pretty bark but also drop thousands of leaves, bucket loads of the tiniest seeds that invariably blow into your wine glass as well as dropping armfuls of twiggy branches – ALL YEAR!\nHere are our top plant design tips for small urban gardens!\n- ‘Good for Naturalising’ plants are not good in a small urban garden.\n- ‘Vigourous’ – is a word to avoid on any plant label.\n- Spreads via underground roots/suckers – maybe only plant this in a large sturdy pot.\n- ‘Self Seeds’ you have a choice, don’t buy it or cut the seeds heads off before they are ripe.\n- Evergreen – this is good!\n- Scented…this is good too\n- Avoid anything prickly – it will be a pain in the proverbials\n- Climbers, these can easily become a tangled mess. Climbers will grow toward the light, so if your garden is quite shady, you will eventually only see the woody stems as the plant heads up to the sunshine and puts it’s flowers up there.\nHow to Design a Garden if:….\nCreating a Low Maintenance Garden…']	['<urn:uuid:669172d2-75af-48d5-8e9e-f45f99d3f580>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T06:04:36.674696	11	67	929
51	medical specialists involved craniofacial abnormalities care	The management of craniofacial abnormalities involves multiple specialists including a plastic/craniofacial surgeon, neurosurgeon, pediatrician, orthodontist, pediatric dentist, speech and language specialist, otolaryngologist, audiologist, ophthalmologist, genetic counselor, nurse team coordinator, social worker, and psychiatrist. Each specialist provides specific expertise, from surgical interventions to psychological support, working together as a multidisciplinary team to provide comprehensive care.	"[""The craniofacial team\nWhat kind of medical providers treat craniofacial abnormalities?\nThe treatment of these disorders requires the involvement of experts from multiple medical and surgical specialties (disciplines). The American Cleft Palate-Craniofacial Association and other cleft palate associations worldwide agree that management of patients with craniofacial anomalies is best provided by a multidisciplinary team of specialists.\nWhen should my child see a craniofacial team?\nThe optimal time for the first evaluation is within your child's first few weeks of life. The focus of the team visit is to help correct your child's physical problems, as well as facilitate adaptation at each stage of life. Usually, your child will be seen at frequent intervals as an infant and then on an annual, or semi-annual basis thereafter.\nThe craniofacial anomaly treatment team:\nThere may be many people involved in the management of craniofacial anomalies for your child, because the skills of many different areas are needed to help with the problems that can occur. The following are some of the members of the craniofacial team:\n- plastic/craniofacial surgeon - a surgeon with specialized training in the diagnosis and treatment of skeletal abnormalities of the skull, facial bones, and soft tissue; will work closely with the orthodontists and other specialists to coordinate a surgical plan.\n- neurosurgeon - a surgeon who specializes in the brain, spinal cord, and nerves; also coordinates all surgical interventions of head abnormalities with the craniofacial surgeons (i.e., craniosynostosis).\n- pediatrician - a physician who will follow your child as he/she grows and help coordinate the multiple specialists involved.\n- orthodontist - a dentist who evaluates the position and alignment of your child's teeth and coordinates a treatment plan with the surgeon and other specialists.\n- pediatric dentist - a dentist who evaluates and cares for your child's teeth.\n- speech and language specialist - a professional who will perform a comprehensive speech evaluation to assess your child's communicative abilities and who will closely monitor your child throughout all developmental stages.\n- otolaryngologist (ear-nose-throat specialist) - a physician who will assist in the evaluation and management of ear infections and hearing loss that may be side effects of your child's cleft abnormality.\n- audiologist (hearing specialist) - a professional who will assist in the evaluation and management of any hearing difficulties your child may have.\n- ophthalmologist - a physician who specializes in the structures, functionality, and diseases of the eye. An ophthalmologist evaluates and plans treatment of associated eye problems in coordination with other surgical interventions.\n- genetic counselor - a professional who reviews the medical and family history, as well as examines your child to help in diagnosis. A genetic counselor also counsels your family regarding risk for recurrence of craniofacial abnormalities in future pregnancies.\n- nurse team coordinator - a registered nurse who combines experience in pediatric nursing with specialization in the care of your child, and acts as liaison between your family and the craniofacial team.\n- social worker - a professional who provides guidance and counseling for your child and your family in dealing with the social and emotional aspects of a craniofacial abnormality and assists your family with community resources and referrals (i.e., support groups).\n- psychiatrist - a physician who assesses the psychosocial function and behavioral development of your child. The psychiatrist will assist the family in identifying therapy resources and coordinates referrals with the social services department.""]"	['<urn:uuid:0fa172d6-1507-49b6-a3d6-50826a4ef8cd>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T06:04:36.674696	6	54	566
52	How do Orphée and Stalker show entering supernatural zones through color changes?	In Orphée, the supernatural world ('zone') is entered through mirrors filmed using pools of mercury, while in Stalker the transition into 'The Zone' is shown through a shift from sepia tones to color, similar to The Wizard of Oz. Both films use these visual techniques to mark the boundary between the normal world and their respective supernatural realms.	['In Orphée, Death rides in a Rolls Royce, served by leather clad bikers; she broadcasts on radio and publishes literary magazines. Jean Cocteau’s bizarre, dreamlike, extremely funny and daringly experimental work took the Venice Film Festival by storm in 1950, and was so popular with audiences upon release that one German cinema showed it every Saturday night for the next 15 years. It continues to influence filmmakers today, with the BFI prepares releasing a new remastered version into cinemas.\nIn 2004 the critic David Thompson wrote that The Matrix could not have been made without Orphée – and you could now add to that list Inception, Eternal Sunshine of the Spotless Mind, Pan’s Labyrinth and even Netflix’s recent series Stranger Things and Maniac. Filmmakers Chris Marker and Andrei Tarkovsky, in their own eras, both admired the film hugely for its bold retelling of the ancient Greek myth of Orpheus and Eurydice, in which a poet sets off on a journey into the underworld in search of his dead wife.\nCocteau’s masterstroke was to find the contemporary cinematic language for this age-old myth – his underworld exists in the architectural nooks and crannies of the real, entered and exited through mirrors (filmed using pools of mercury). In this ‘zone’ gravity can be upended at a moment’s notice, and people either glide around with ease or struggle as though moving through treacle. “The laws of the other world,” declares one character, “are different to ours.”\nCocteau’s film – which liberally uses negative effects, rewinding, weird cuts and camera angles – is as much about the joyously breakable laws of cinema as it is about the mythical underworld. Tarkovsky’s Stalker, which breaks into colour from black-and-white as soon as its characters enter its mysterious ‘zone’, owes much to Cocteau’s sensibility.\nLike Tarkovsky’s zone, Cocteau’s spirit world also has a very modern psychological dimension: it is not just a land of the dead but “a place of men’s memories and the ruins of their habits”. In Maniac (as in Eternal Sunshine of the Spotless Mind) Emma Stone and Jonah Hill have to go on the run through the collapsing architecture of their own minds and memories. Likewise Cocteau’s Orphée (played by his lover and long-term partner, the ageing but still ruggedly handsome Jean Marais) is engaged in a battle not just with Death but with his own memories of an impossible love; he can only win through uneasy forgetfulness and oblivion.\nOther details blur the lines further between psychology, technology and magic. Throughout the film radio sets suddenly start declaiming fragments of surrealist poetry that Orphée desperately scribbles down – whether they are coming from his own subconscious, or from the spirit world, is unclear.\nThe disembodied voices over the radio are another part of Cocteau’s influential vision. As with the Nokia phones and blinking green computer screens of The Matrix, Orphée’s supernatural fantasies take place via the normal technology of modern life, making it seem all the more eerily plausible. Just as many a childhood in the 2000s was spent waiting for computer screens to spell out the words ‘follow the white rabbit’, after watching Orphée mirrors begin to seem like ‘the doors through which Death passes’, potential gateways to another world.\nThe film has its uneven points – Cocteau is certainly more interested in his notions of symbolic and poetic wonder than he is in human relationships – and there are plenty of highly conventional moments of egregious and disturbing sexism. For its cinematic inventiveness and far-reaching influence, however, it is destined to continue finding new audiences.\nThe post In praise of Orphée – Jean Cocteau’s mould-breaking masterpiece appeared first on Little White Lies.', 'Few directors command as much respect from their peers as the late Andrei Tarkovsky. Ingmar Bergman famously called him “the greatest [director] of them all.” Three of his films appear in the Sight & Sound Top 50 Greatest Films of All Time poll, a feat surpassed only by Jean-Luc Godard.\nOne of those films, his 1979 feature Stalker, is out in a gorgeous new 2K restoration by Mosfilm.\nA surreal and sprawling sci-fi meditation, Stalker is set in a dystopian future society whose fabric is forever altered by the appearance of “The Zone,” a mysterious geographic space of seemingly otherworldly origins.\nDespite government efforts to prevent human entry via a militarized perimeter, a class of people known as “stalkers” have learned to navigate “The Zone,” which is governed not by the laws of physics but by the thoughts and emotions of those who enter it.\nFor a fee, stalkers will escort visitors into “The Zone” and help them navigate its terrain of existential booby traps to find the supernatural rewards it’s purported to contain. The film follows one such stalker (Alexander Kaidanovsky) as he leads a writer (Tarkovsky regular Anatoli Solonitsyn) and a professor (Nikolai Grinko) through the sentient landscape.\nLike Tarkovsky’s Solaris, Stalker is a loose and abstract adaptation of a much more cerebral sci-fi novel.\nWhile the original authors examine the political and metaphysical ramifications of sci-fi scenarios, Tarkovsky is interested in the spiritual ramifications of these phenomena. His source material tells us about “The Zone,” while Tarkovsky asks what “The Zone” can tell us about ourselves.\nFew filmmakers ask such questions better than Tarkovsky. More than presenting a sci-fi setting or scenario, “The Zone” places the viewer in an unsettling headspace that necessitates a radical change in perspective. Needing to interpret a world purely through emotion, discarding reality’s iron-clad laws, forces the viewer to reconsider the prism through which they view their own life in addition to the fiction.\nWhen the writer and the professor give their reasons for wanting to visit “The Zone,” both answers feel incomplete or deceptive. Whether they’re trying to deceive each other, themselves or the viewer is a complex knot that the film only partially untangles. Only by examining their own attitudes can the viewer begin to answer those questions, and like these characters, they might not like what they see when looking inward.\nTarkovsky and cinematographer Alexander Knyazhinsky present these quandaries with a quiet that masks the film’s visual gusto. The film begins outside “The Zone” in sepia tones, transitioning to colour as the characters leave their dystopia behind.\nThe parallel to The Wizard of Oz is obvious, but that’s less a statement than a further question to the audience. What did that colour scheme say about Depression-era America in Oz, and what does it say about Tarkovsky’s contemporary Soviet Union?\nWhatever it said wasn’t liked by Soviet authorities, who made it impossible for Tarkovsky to work in the USSR after this film. In the 1990s, former KGB agents who purported to have had a role in Tarkovsky’s death claimed that the poison which caused his fatal cancer was administered on the set of Stalker.\nStalker plays at Cinematheque Sept. 23-29.']	['<urn:uuid:90e922f7-f0bd-432f-a7d3-bcc9f1f37fc4>', '<urn:uuid:10477b4b-c959-4e10-8b39-0d9f094994f0>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T06:04:36.674696	12	58	1142
53	What tools are required to build a device that mimics high elevation conditions, and what warning signs should someone look for to know if they're having a dangerous reaction to altitude?	The required tools include a drill with bits, 5 liter stainless steel and Pyrex bowls, rubber gasket, brass hose barbs, vacuum pump, vacuum regulator, vacuum gauge, needle valve and filter, and flexible tubing. The warning signs of dangerous altitude reactions include severe symptoms like a cough with blood-tinged discharge, tightness in the chest, blueness of face and lips (signs of HAPE), as well as severe headache not relieved by painkillers, confusion, disorientation, loss of balance, and blurred vision (signs of HACE). These conditions can be fatal if ignored and require immediate descent to lower altitude.	"['A high-altitude or hypobaric chamber mimics the environment found high above sea level. High altitudes such as those at the tops of mountains have low ambient air pressure and low levels of oxygen. Therefore humans and devices will function and work differently there than at sea level. Instructors train pilots, flight crews and athletes in high-altitude chambers. You can experiment with the effects of high altitude by making your own hypobaric chamber -- a much easier task than transporting your equipment to the top of a mountain.\n- Drill with drill bits\n- 5 liter stainless steel bowl\n- 5 liter Pyrex bowl\n- Rubber gasket\n- Two 1/4-inch brass hose barbs\n- Vacuum pump\n- Vacuum regulator\n- Vacuum gauge\n- Needle valve and filter\n- 3 lengths of 1/4-inch flexible tubing\nDrill two holes on the side of the stainless steel bowl using a drill. The holes should be about 1/4 inch in diameter, as they are needed for the 1/4-inch brass hose barbs. Practice finding the exact size of hole you need by drilling holes in a piece of spare plastic. If in doubt, start with a small hole and then gradually make it bigger until the barbs fit.\nMix the epoxy together and spread it around the brass hose barbs. Insert the barbs into the drilled holes and allow for the epoxy to cure completely.\nPlace a rubber gasket on top of a 5-liter stainless steel mixing bowl. Use a gasket that it sits along the entire edge of the mixing bowl. The gasket allows for the two bowls to form an airtight seal. It is a good idea to bring the mixing bowl with you when you purchase the gasket at the hardware store so you can buy one that fits properly.\nTurn a Pyrex mixing bowl, similar in size to the stainless steel bowl, upside down. Place the edge of the upside-down Pyrex bowl on top of the gasket on the edge of the stainless steel bowl.\nConnect a vacuum regulator to the dry vacuum pump by screwing it tightly into place. Tighten it as much as you can by hand, then use a wrench to tighten it further.\nPush one end of 1/4"" flexible tubing over one of the hose barbs in the stainless steel bowl. Insert the hose barb into the tube opening. Make sure the tubing covers at least two of the notches on the hose barb. The other end of this 1/4"" tubing connects to the vacuum regulator on the vacuum pump. The tube goes over the regulator bar just like the hose bar in the stainless steel bowl.\nConnect a second length of 1/4-inch tubing to the second hose barb in the stainless steel bowl. Run this tubing to the inside of a vacuum gauge. The vacuum gauge will have a barb like the hose barb in the bowl. Attach the tubing in the same way to the ""in"" side of the gauge. The gauge will have an arrow running across the body to show in which direction it should be placed. The ""in"" side is the back end of the arrow and the ""out"" side is the pointy end of the arrow.\nConnect a needle valve and filter to the vacuum gauge using a short length of 1/4-inch tubing. Connect the tubing from the ""out"" side of the gauge to the needle valve in the same way as in previous steps.\nTurn the vacuum pump on and adjust the vacuum regulator for a specific flow of air moving out of the bowl. Check the gauge and stop pumping when you achieve the desired altitude or pressure. Check an altitude chart for the pressure at different altitudes. If the altitude chamber is not at sea level, you will need to make a correction for your current altitude.\nThings You\'ll Need\nAbout the Author\nLiz Tomas began writing professionally in 2004. Her work has appeared in the ""American Journal of Enology and Viticulture,"" ""BMC Genomics"" and ""PLoS Biology."" She holds a Master of Science in food science from Cornell University and a Bachelor of Science in biochemistry from the University of New Hampshire. She is pursuing her Ph.D. in oenology at Lincoln University.', 'High Altitude Climbing and What is Altitude Sickness?\nHigh Altitude climbing may bring on altitude sickness, the reaction of the body adjusting to decreasing amounts of oxygen. Normally, the higher the altitude, the less oxygen available for the body to carry on normal functions. Altitude sickness most commonly occurs from above 2,800 metres (9,200 ft) but this is different for everyone.\nThere is simply no way of knowing your own susceptibility prior to being at altitude thus it is vital you monitor your own health. Symptoms may be mild and subside/go away after a day’s rest, or if it is ignored it could lead to death.\nSymptoms of altitude sickness\nSymptoms can appear within 1-2 hours although most often appear 6-10 hours after ascent and generally subside in 1-2 days as the body adjusts to altitude. They may reappear as you continue to go higher. Symptoms usually occur gradually and can be one or a combination of the following:\n- Loss of appetite\n- Disturbed sleep or drowsiness\n- Swelling of hands, feet & face\nIf the body is unable to adjust to altitude these symptoms will persist and, if they are left untreated, altitude sickness may progress to High Altitude Cerebral Edema (HACE) or High Altitude Pulmonary Edema (HAPE). Both can be fatal if ignored.\nSymptoms of HAPE (fluid on the lungs):\n- A dry cough, developing to a wet one with blood-tinged discharge or saliva.\n- Tightness in the chest & blueness/darkness of face, lips & tongue\n- Low fever up to 38°C/100°F\n- Severe fatigue, progressing to coma\nSymptoms of HACE (fluid on the brain):\n- Severe headache symptoms not relieved by painkillers or lying down\n- Confusion, disorientation & drowsiness\n- Loss of balance or coordination\n- Blurred or double vision/retinal hemorrhage\nHow to avoid Altitude Sickness (AMS) Certain medical conditions (such as respiratory disease) or medications (such as sleeping pills) can increase the risk of altitude sickness – it is important that you inform your leader of any medical conditions or medications before ascending to altitude. You can help your body to acclimatize and avoid altitude sickness by:\n- Avoiding alcohol, tobacco and substances that can interfere with good delivery of oxygen to the body or cause dehydration.\n- Eating small, frequent meals high in carbohydrates.\n- Drinking plenty of water – at least 4 litres per day\n- Taking it easy or have a rest. Walk at a slower pace than you would at sea level and avoid overexertion.\n- Check with your health care provider to see if Diamox would be appropriate for you to take while climbing. See below.\n- If possible, don’t fly or drive to high altitude. Start below 10,000 feet (3,048 meters) and walk up. If you do fly or drive, do not over-exert yourself or move higher for the first 24 hours.\n- Hike high and sleep low. You can climb more than 1,640 feet (500 meters) in a day as long as you come back down and sleep at a lower altitude.\nTreatment Most travelers are able to successfully acclimatize by following the previously mentioned guidelines. However, there are instances where medical treatment is required. Ultimately, the best treatment for acute altitude sickness is to descend to a lower altitude. There may be times when your leader makes the decision that you or a member of your group is at risk of serious altitude sickness and for safety insists that you cannot ascend further – please respect that they are within their rights to do so and are making that decision in the best interests of your health and well-being. If you are experiencing any altitude sickness symptoms, we encourage you to discuss them with your leader straight away so you both can follow your acclimatization progress or seek the advice of a trained medical professional if necessary. Everyone will have a different perception of the severity of their symptoms, the key is to personally assess whether your symptoms are improving or worsening. If in doubt, go down!\nDiamox (Acetazolamide) allows you to breathe faster so that you metabolize more oxygen, while high altitude climbing, thereby minimizing the symptoms caused by poor oxygenation. This is especially helpful at night when respiratory drive is decreased. Since it takes a while for Diamox to have an effect, it is advisable to start taking it 24 hours before you go to altitude and continue for at least five days at higher altitude. The recommendation of the Himalayan Rescue Association Medical Clinic is 125 mg. twice a day (morning and night). (The standard dose was 250 mg., but their research showed no difference for most people with the lower dose, although some individuals may need 250 mg.) Possible side effects include tingling of the lips and finger tips, blurring of vision, and alteration of taste. These side effects may be reduced with the 125 mg. dose. Side effects subside when the drug is stopped. Contact your physician for a prescription. Since Diamox is a sulfonamide drug, people who are allergic to sulfa drugs should not take Diamox. Diamox has also been known to cause severe allergic reactions to people with no previous history of Diamox or sulfa allergies. Frank Hubbell of SOLO recommends a trial course of the drug before going to a remote location where a severe allergic reaction could prove difficult to treat. It is very important that you make yourself aware of the cause and effects of traveling at altitude, monitor your health and seek assistance accordingly.\nThe above is for information purposes only and is in no way intended to replace the advice of a trained medical professional. As such, BenefacTours Inc. is unable to accept responsibility for any loss, injury or inconvenience sustained by any person, caused by errors and omissions, or as a result of the advice and information given here.']"	['<urn:uuid:0b2f832a-9bba-47a5-94b5-6bcb5424f591>', '<urn:uuid:ff43dea7-c47c-4133-bcb7-48f28c25697a>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T06:04:36.674696	31	95	1679
54	What were the key advantages that made smaller vessels like sloops so popular among pirates during the golden age of piracy, and how did these advantages help them in their operations?	Smaller boats had several distinct advantages that made them popular among pirates. They offered superior speed and maneuverability, were easier to careen, and had a shallow draft that allowed them to sail in areas where larger ships couldn't go. However, the main reasons for their popularity were their ready availability in pirate-operating areas and the fact that they were easier for small pirate crews to capture. Since merchants typically staffed their ships with minimal crews to save money, smaller vessels offered less resistance when pirates attacked. They also required smaller crews to operate, though pirates would often pack them with as many men as possible to appear more threatening.	"['The Pirate Surgeon\'s Quarters in the Golden Age of Piracy, Page 3\nPirate Vessels: Small Craft\n""... they steered for the coast of Virginia, and in their way, met with a large New-England brigantine, laden with provisions, bound for Barbadoes. This they made prize of, and shifting their own guns on board her, sent the master away in the sloop, after forcing some of his men with them. They had now a vessel of ten guns, and a crew of 80 men, of whom one James was captain, and [Thomas] Howard quarter-master."" (Captain Charles Johnson, The History of the Pirates, p. 144)\nA Listing of Various Small Pirate Ships by Type, from Pirate Ship names list,\nPyracy.com Forum, posted 3/31/10 by The Island, gathered 11/7/12\nWhile the large, three-masted ships were impressive, most pirates during the golden age of piracy operated from smaller, single- and double-masted ships. Even those in the larger ships started out in smaller ships. Angus Konstam tells us that ""for the most part, pirates relied on smaller, less spectacular craft, such as sloops, brigantines and early schooners.""1 A listing of various small pirate vessels sorted by type can be seen at left.\nIt is immediately apparent which type of vessel was the most popular among the pirates. Of the 46 small ships identified in this list, 70% of them were Sloops, 24% are Brigs or Brigantines, 4% of are Schooners and the remaining 2% Snows.2 As we shall see shortly, snows were basically brigs with a different sail configuration so the number of brigs may be lower and snows higher, although this doesn\'t affect the basic fact that the sloop was the most popular small pirate ship at this time.\nCharles Vane in front of his Sloop Ranger\nfrom The General History of the Pirates (1724) The observant may note that the same pirates are sometimes listed for different ships. This is because they took ships when they captured a better one. As pirate captain William Fly explained to the master of a captured sloop, he must ""make bold to try if Capt. Fulker\'s sloop was a better sailer than [his] snow. If she was, she would prove much fitter for their business, and they must have her.""3\nSmaller boats had several advantages - speed, maneuverability, ease of careening and a shallow draft which allowed them to sail in in places where larger ships could not. However the primary reasons they were so popular was because they were readily available in the areas where the pirates operated and were easier for a small crew of pirates to take than a larger ship. Smaller vessels required smaller crews and since merchants usually staffed their ships with the minimum number of men in an effort to save money, the pirates found less resistance in smaller craft.\nA smaller boat would likely have much less space available to a surgeon. This doesn\'t mean the pirates wouldn\'t have surgeons on these vessels. Bartholomew Roberts had three different surgeons at one point including Peter Scudamore, who served on the Ranger sloop. Scudamore didn\'t remain there, however, because he was elected to be the Chief Surgeon ""by the good Will of the Ranger\'s People, who, in general, voted for Scudamore, to get rid of him, (the chief Surgeon being always to remain with the Commodore [ on board Bartholomew Robert\'s ship the Royal Fortune].)""4\nPhotographer: Michael Lamonica\nCockpit of the Duyfken Replica. Although this is a 1606 ship and larger than a sloop it gives\nan idea of the surgeon\'s cramped quarters. Pirates usually tried to put as many men on board a ship as they could to man the guns and make like a more imposing threat. Captain Lewis took a sloop of about 90 tons ""and mounted her with 12 guns. His crew was... about 80 men, whites and blacks.""5 All of these men on a smaller vessel probably produced some pretty cramped quarters aboard such a small pirate craft.\nAs with large ships, the surgeon\'s place on a sloop would typically be on the lowest deck. Like the men, he would probably find his quarters overcrowded, which would be exacerbated by having to share his space with everything required for the vessel\'s care and maintenance, the booty that the boat might be hauling and possibly even cannon if they were mounted there.\n1 Angus Konstam, The Pirate Ship 1660 -1730, p. 3; 2 Calculated from data found at Pirate Ship names list, Pyracy.com Forum, posted 3/31/10 by The Island, gathered 11/7/12; 3 Captain Charles Johnson, The History of the Pirates, p. 137; 4 Captain Charles Johnson, The General History of the Most Notorious Pirates, p. 313-4; 5 Johnson, The History of the Pirates, p. 155\nPirate Vessels: Small Craft - Schooners\n""[Low\'s pirates] mann\'d and arm\'d their Boat [a Brigantine], and took Possession of every one of them [13 ships in the Harbor of Port Rosemary], plundered them of what they thought fit, and converted one to their own Use, viz,, a Scooner of 80 Tuns, aboard of which they put 10 Carriage Guns, and 50 Men, and Low himself went Captain, and nam\'d her the Fancy, making one Charles Harris... Captain of the Brigantine"". (Captain Charles Johnson, General History of the Pyrates, p. 370)\nLow clearly thought more of the schooner than he did his brigantine or the twelve other ships that he took that day, suggesting that at least that schooner was a superior vessel. This is probably because she was faster, a trait prized by pirates, as Johnson revealed later telling us that Low\'s ""Scooner coming up first [to a target ship], attacked them""1. Of course, Johnson goes on to say that ""there happening to be an Officer and some Soldiers on Board, who gave them a warm Reception, Low chose to stay till he should be joyned by the Brigantine""2. This highlights a downside of smaller vessels: they couldn\'t hold as many men and cannon, which was one of the pirates\' primary advantages in a fight.\nSchooner Sails of the Anna B. Smith, photographed by the navy (1915)\nSchooners are the least represented vessel in our list of small pirate vessels other than the snow, which is actually a type of brigantine. Schooners account for only 4% of the 46 ships in the list.3 David Cordingly suggests that this was because they didn\'t appear until near the end of the golden age of piracy, when they are ""mentioned in two issues of the Boston News Letter.""4\nSchooners could have anywhere from one to three masts, although two-masted schooners were the most common.5 They could be rigged with several different sail configurations, although the most common is to gaff rig them - that is use four-corned sails with the top of the square being held taut by a spar (pole) which keeps it square.6 This gives the sail four-sides instead of three providing more area for the sail to catch the wind and allowing the vessel to travel faster. Bemuda-built vessels often have a mainstaysail between the two masts (not shown in the photo above left) and one or two forestaysails.7\nBuilding a Schooner, from The Last Cruise of the Saginaw by\nGeorge H. Read, illustration by Lieutenant Commander Sicard (1912)\nAs mentioned previously, the naming of boat types at this time had more to do with the masts and sails than it did with the size or configuration of the body of the boat. So schooners would have been of a variety of different sizes. This is a something we will encounter repeatedly when trying to imagine how much space a surgeon might have aboard such a vessel.\nA possible proxy for ship size was the number of guns she carried, with the idea being that the pirates would load as many guns as they possibly could onto the deck space they had. Thus the more guns the pirates put aboard, the larger the ship.\nBritish Navy-rated schooners were recommended to have a single deck, 4-14 guns and a complement of 20-90 men.8 Of our two identified schooners, we only know that Low\'s ship had 14 guns, the maximum for which the navy rated them.9 Of course, this is based on data from a single vessel.\n1,2 Calculated from data found at Pirate Ship names list, Pyracy.com Forum, posted 3/31/10 by The Island, gathered 11/7/12; 3,4 David Cordingly, Under the Black Flag, p. 168; 5 Schooner, wikipedia, gathered 11/11/12; 6 Gaff Rig, wikipedia, gathered 11/11/12; 7 Schooner; 8 Rating system of the Royal Navy, wikipedia, gathered 11/3/12, 1706 Establishment, wikipedia, gathered 11/14/12. and 1719 Establishment, wikipedia, gathered 11/14/12; 9 Pirate Ship names list\nPirate Vessels: Small Craft - Brigantines, Brigs and Snows\n""...the Majority [of Charles Vane\'s pirate crew] was for boarding [a French Man-of-War - something Captain Vane opposed]... At length the Captain made use of his Power to determine this Dispute, which, in these Cases, is absolute and uncontroulable... so the [pirate\'s] Brigantine having the Heels... of the French Man [being faster], she came clear off."" (Captain Charles Johnson, General History of the Pyrates, p. 146)\nThe ship in that account was Vane\'s brigantine Ranger. While Captain Vane won the argument over whether or not they should try to take the French man-of-war ship, he lost his fast-sailing Brigantine as a result. As\nA Brigantine from Nouveau Voyage aux Isles de l\'Amerique, Volume 2\nby Jean Baptiste Labat (1722). This boat has the square rigged topsail &\nforesail (left) and the fore-and-aft rigged spanker (right) raised. Johnson explains, ""the next Day, the Captain\'s Behaviour was obliged to stand the Test of a Vote, and a ResoIution passed against his Honour and Dignity, branding him with the Name of Coward, deposing him from the Command, and turning him out of the Company, with Marks of Infamy, and, with him, went all those who did not Vote for boarding the French Man of War.""1\n\'Calico\' Jack Rackham was made the new captain. There was some honor among these thieves, however, for they gave the cast-off crew a sloop before leaving them.\nThe characteristics of a brigantine during the golden age of piracy were that it was a) relatively small, b) two-masted, and c) rigged with square sails on the foremast and fore-and-aft rigging on the (rear) mainmast.2 The Ranger herself was ""a fast, two-mast vessel that carried a square-rigged foresail and topsails, a fore-and-aft rigged spanker abaft [behind] her mainmast, a triangular main staysail between her masts, and a jib, secured to her bowsprit.""3\nThere were two other types of vessels that were basically brigantines with slightly different sail configurations: the brig and the snow. A brig (a term derived by shortening \'brigantine\') carried square sails on both masts with a small fore-and-aft sail on the mainmast.4 Snows looked like the other brigantine vessels with the addition of a small mast just behind the mainmast that carried a sail.5 The differences are slight on a ship as seen as seen below.\nFulling Rigged Brigantine, Brig and Snow Sail Plans\nThe British Navy used brigs because they were fast and maneuverable6, although they were too small\nto be assigned a class rating.7 Snows were mostly popular among merchants.8 Konstam suggests\nArtist: Ray Brown\nA Merchant Snow from American Merchant Ships\nand Sailors by Willis J. Abbot, p. 29 (1902) such ships ""lacked the high proportion of sail area to displacement that characterised the sloop, and to a lesser extent the faster square-rigged ships of the period such as slavers""9. While pirates used them, they were much less popular than sloops as our listing of vessel types at the top of this page reveals.\nFor armament, the Navy recommended that Brogantines carry 4 - 14 guns and 20-90 men to man them.10 Our small pirate ship data from the golden age shows them carrying much more than that: between 14 - 32 guns based on the six ships for which we have data, with the average being 18 guns.11 Charles Vane\'s Ranger had 12 guns and crew of 80.12\nBrigantines could also have oars or sweeps13 which could take up space on the lower deck where the surgeon plied his trade, as we learn when feisty Captain Tucker of a Bermuda brigantine targeted by Captain Lewis\' pirates put ""out his oars, [and] got in among them.""14 If a brigantine had oars, the space available to the surgeon on a such a vessel might have to be shared with oarsmen at times, which would not be conducive to the establishment of a good surgery.\nBrigs were sometimes created from other small vessels. John Bowen\'s pirate crew procured a sloop and converted her into a brig by adding a second mast15, the addition of which would have further limited the space belowdecks where the surgeon worked.\n1 Captain Charles Johnson, General History of the Pyrates, p. 146; 2 Brigantine, wikipedia, gathered 11/4/12; 3 Angus Konstam, The Pirate Ship 1660 -1730, p. 44; 4 Brig, wikipedia, gathered 11/5/12; 5 Manuel Schonhorn, A General History of the Pyrates. Introduction to the Dover Edition, p. xlviii; 6 Brig; 7 Rating system of the Royal Navy, wikipedia, gathered 11/3/12, 1706 Establishment, wikipedia, gathered 11/14/12. and 1719 Establishment, wikipedia, gathered 11/14/12; 8 Snow (ship); 9 Konstam, p. 21; 10 Rating system of the Royal Navy; 11 Calculated from data found at Pirate Ship names list, Pyracy.com Forum, posted 3/31/10 by The Island, gathered 11/7/12; 12 Konstam, p. 34; 13 Captain Charles Johnson, History of the Pirates, p. 494; 14 Johnson, History of the Pirates, p. 154; 15 Johnson, History of the Pirates, p. 49']"	['<urn:uuid:15b6a9d0-fc9f-489c-99c8-0145d1a446f1>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T06:04:36.674696	31	109	2260
55	How do mealworms compare to crickets in terms of their farming requirements and culinary uses?	Both mealworms and crickets are relatively easy to farm in small spaces. Mealworms can be raised in a container with oats and vegetable scraps, while crickets need a covered aquarium with soil for laying eggs, egg cartons for climbing, wet cotton balls for water, and food scraps. Mealworms are actually easier to raise and don't make noise, unlike crickets which chirp. Both insects are considered tasty for human consumption - mealworms have a nutty, pecan-like flavor and crunchy texture, while crickets can be pan-fried into salty treats and even ground into high-protein flour for baking things like 'chocolate chirp cookies'.	['There’s no shortage of insects in the world, which makes for a very reliable food source. While most people in the western world have an aversion towards making a meal of insects, they can provide protein and essential vitamins on a fraction of the resources of larger animals.\nOne of the insects most commonly used by humans are bees. In this case, we don’t consume the bee for meat or give it to our animals, but rather farm the wonderful honey that they make. If you’re interested in more information about bees, you can take a look at our how-tos on building your own bee hive, bait hive and wax melter.\nFor the purpose of this article, it’s the wax worms that we’re more interested in. These bugs often live in symbiosis with bees, eating the comb that bees make. When in balance, they do not harm the hive, and can in fact serve as cleaners. However, when the health of the hive is compromised, the population of the wax worms can increase too much and become detrimental.\nYou can help the bees by harvesting the out of control wax worms. We haven’t yet had a problem with them in our hives, so we don’t speak from experience, but it is said that of all insects, the wax worm is the most delicious. It makes sense; all they eat is wax and honey. What’s not to like?\nMealworms are the larvae of the darkling beetle, and they are a very easy insect to raise in a small space. There are three main stages in development: adult beetles, larvae (worms), and pupae. The three should be kept apart to avoid the adult beetles eating any of the helpless pupae.\nThings like temperature and moisture can affect each stage, but to get an idea of the time scale, here is a breakdown of their life cycle:\nWheat bran, a byproduct of wheat production, is typically used as the feed, but animal manures, especially dry ones from herbivores like rabbits or guinea pigs, can be included at a 1:1 ratio. They also need a bit of vegetable matter, like carrot peels, potatoes, or squash for water. They are highly efficient at converting feed into protein, with a FCR of 2:1 .\nThis is an insect that we have started to farm properly. With the right set-up, using a few square feet of floor space, you can create a tower that puts out a few of pounds of worms each week.\nThey are a great supplement for omnivores like poultry or pigs. We have tamed many baby birds using these little treats. However, they are also very good for humans to eat. We eat them on occasion, and they are really tasty. They have a slightly nutty flavor (like pecan), and the texture is crunchy, subtle, and much better than some other insects.\nFor our own use, we put a bunch in a bag in the freezer for about 15 minutes, and we then fry them quickly with a dash of butter and garlic.\nThere are people all around the world that eat grasshoppers, and some that consider them a delicacy. They are high in protein, plentiful and easy to harvest. If you don’t think about what you’re eating, they’re not bad.\nGrasshoppers are a plague where we live, and thus very plentiful. We therefore don’t have to expend any time and infrastructure in farming them. We will often catch a bunch in season, freeze them, and give them to the poultry in winter.\nPlease not that there are many types of grasshoppers, some of which are poisonous. If your chickens and guineas won’t touch a particular kind, you shouldn’t either.\nWe have eaten grasshoppers on several occasions. The best time to catch them is in the evening, when they climb high in plants to roost. Try to get younger ones, not big females that have already laid their eggs.\nThey’re not as good as mealworms, but they’re not too bad. It’s the more the texture than the taste (which is of garlic and chili) that you can get hung up on.\nSomething we haven’t yet tried, but intend to, is grubs. Whenever we dig through our compost piles, we always find these huge, nasty looking grubs. They are similar to maggots in appearance, but much bigger and fatter.\nWe’re not a hundred percent sure, but we think they are the larva of either June/ Japenese or Rhinoceros beetles.\nThey are said to be good to eat, and the main reason that we haven’t yet partaken of this culinary delight is that our chickens would get very upset with us. Whenever we are turning a compost pile that the chickens don’t have access to, they gather around, outside the fence. They look at you with their beady little eyes and somehow compel you to throw them the treats. It’s perhaps shameful to admit how easily we are manipulated by our birds, but we do get their eggs, so fair’s fair. One day we’ll keep the grubs and give them a try for ourselves.\nThe great thing about our experiments with eating bugs is that our kids don’t balk at trying them. They are too young to have any preconceptions about eating insects, and just judge it on their natural merits. You never know what kind of world they will be inheriting, but having a reliable source of protein that is cheap and easy to farm or harvest (even in a drought) can’t be a bad thing.', 'As you put together a dinner plan for this Thanksgiving, perhaps you’re looking for something to add a little variety to the traditional holiday meal, or ways to eat healthy food while supporting good environmental practices. How about adding insects to the menu?\nAlong with crackers and cheese, grasshopper fritters make excellent appetizers. Or consider adding sautéed crickets and greens as a side to your potatoes, turkey, and gravy. What about cricket flour fruitcake instead of the typical pumpkin pie?\nLike the majority of North Americans, you probably cringe at the thought of insects for dinner. Yet people in many cultures eat insects. And many of the insects they think are delicious are familiar in the Northeast.\nWe usually see crickets and grasshoppers in fields, but many people around the world think they look good served on a skewer. They were eaten by Native Americans, including the Ute and Shoshone people from the southwest. In the Great Basin (covering Nevada and Utah) crickets, locusts, and grasshoppers would congregate at certain times of the year in huge numbers, making them easy to catch and roast. Crickets here in the Northeast aren’t so easy to catch, but they’re just as edible.\nMealworms, which originated in Europe and can now be found in temperate regions everywhere, are also popular as snacks, and why not? Apparently, after being salted and fried, they rival potato chips in crunchy deliciousness.\nNot sure what to do when millions of cicadas come out every 13 or 17 years? You can eat them; they’re considered a delicacy in Shanghai. Or there are the mobs of June bugs that show up every May and June. Next summer, try roasting them until they pop; add salt and you have a crunchy popcorn-like snack.\nIn our region, entomophagy – the practice of eating insects – has been gaining attention due to the efforts of formerly Vermont-based environmentalist Rachael Young. As the founder of Eat Yummy Bugs, Rachael arranged events across the state, often in collaboration with local chefs or even chocolatiers.\nRachael favors crickets and mealworms in her recipes, because they are easy to raise and easy to cook. Both can be pan-fried into salty treats that make good snacks or additions to all kinds of dishes. Crickets can be ground into a high-protein flour and substituted in baking recipes, such as “chocolate chirp cookies.”\nInsects are a tasty, nutritious, and healthy source of protein. As an example, a 100 gram serving of ground cricket is just 121 calories, with 13 grams of protein, only 5.5 grams of fat, and includes many vitamins and minerals, including calcium and iron. An equal portion of ground beef is 288 calories, with 23.5 grams of protein and a whopping 21 grams of fat.\nYet what is really turning heads these days about entomophagy are the environmental benefits. Rearing birds or mammals for meat is a resource-expensive way to make food. Consider this: 2.5 pounds of feed is needed to make just one pound of poultry, but only 1.7 pounds of feed will get you a pound of cricket. Better yet, nearly the whole cricket can be eaten (it’s preferable to remove their legs, which are spikey), whereas your holiday turkey has many inedible bits including feathers and bones.\nInsects may be the easiest of all animals to raise. They need very little space, food, light, or water. They can be raised at high densities with little environmental impact, unlike large-scale commercial poultry, pork or beef farming. Insect farming hasn’t really caught on in North America, although it’s gaining momentum. The two most prominent sources of commercial crickets, World Entomophagy and Aspire Food Group, merged this summer to take cricket farming and entomophagy education to a whole new level.\nOr, if you prefer to eat local, try raising insects at home. Crickets require so little space you can keep them in a closet. A covered aquarium with some soil (where they lay eggs), an egg carton to climb on, wet cotton balls for water, and food scraps are all you need to be a cricket farmer. Mealworms are even easier to raise and will spare you the chirping. They’re perfectly happy with a container of oats and leftover veggie scraps.\nThis Thanksgiving, as you admire your traditional spread of roasted turkey, potatoes mashed with cream and butter, and rich gravy, followed by a big slice of pumpkin pie, consider how a few six legged ingredients might round out a classic meal.\nRachel Sargent is an editor for a pharmacology journal, as well as a freelance nature writer and illustrator. The illustration for this column was drawn by Adelaide Tyrol. The Outside Story is assigned and edited by Northern Woodlands magazine and sponsored by the Wellborn Ecology Fund of New Hampshire Charitable Foundation: [email protected]']	['<urn:uuid:136bbbce-b341-4fa5-9937-bf169df3c92d>', '<urn:uuid:91e1b384-3dff-4667-8008-adc8c0b46385>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T06:04:36.674696	15	100	1720
56	What happens when the equipment or tools needed by researchers in Antarctica stop working or need to be replaced during their stay?	When equipment breaks or ideas change, the station members use their ingenuity to craft solutions, creating sampling devices from old or recycled items found around the station. This is particularly important since there are no shops and the station only receives its main re-supply once a year.	['8 February, 2016 Rothera\nI am sat gazing with child-like wonder through my office window at the huge snow-flakes falling outside and, despite the fact that the snowstorm is obscuring my usual spectacular view of Ryder Bay, I can’t help but smile. When I first came to Antarctica with BAS in 2006 my biggest fear was that I would lose my love for snow, but I needn’t have worried, if anything it has fuelled my obsession and I can’t help but get excited by it.\nI am lucky to have worked in several different roles for BAS and am currently manager of the Rothera Bonner laboratory facility. I find myself chasing the sun – spending the UK summers in Cambridge planning and preparing for the upcoming Antarctic summer season which I spend on station at Rothera supporting the marine and terrestrial scientists.\nHere at Rothera I find that no two days are the same. I am primarily responsible for the smooth running of the laboratory facilities; ensuring the scientists have what they need and that everything from the laboratory pipettes to the all-important coffee machine (and everything in between) is working perfectly. There are no shops down here and the station has its main re-supply only once a year. This means it is really important that the visiting scientists pack everything they will need for their projects, including spare parts for machines and sampling equipment, and all this has to be done up to 5 months before they even arrive on station as the BAS ship the James Clark Ross leaves the UK in late September and reaches Rothera in late December. Inevitably things go wrong, equipment breaks or ideas change and it is then that the ingenuity of station members comes into its own, it’s a real ‘Blue-Peter’ moment when the perfect sampling device is crafted out of old or re-cycled items from around the station!\nMy job also involves directly supporting the scientist’s research, which means I am often allowed out of the office to assist with boating, scuba-diving, soil and plant collections or the weekly skua survey.\nThe South Polar skuas nest on Rothera Point during the summer and I undertake regular surveys to monitor their breeding success. This season I have been the envy of people on station as I was asked to accompany a film crew on a trip to one of our local islands. This meant living on board their small boat for 3 days and having day-trips ashore to enable the crew to film the resident wildlife. It was also an opportunity for me to do some wildlife counts and to collect plant (lichen, moss & algae) samples to enable BAS scientists to document the species in this previously under-studied area.\nThe science programs at Rothera are a mix of NERC-funded, external grants and BAS-core science. In 2013 the Dutch NWO (Netherlands Organisation for Scientific Research) opened the Dirk Gerritsz laboratory building at Rothera. The Dutch do not have an Antarctic station of their own but after several years of successful scientific collaborations with BAS they secured funding and the agreement to place their own containerised laboratories at Rothera Station.\nThis season we have 16 different science projects running out of the Bonner and Dirk Gerritsz laboratories with many studies focusing on how the ocean chemistry, biology and terrestrial life is responding to climate change. The labs are a hive of activity, with good weather days being in high demand for water sampling, scuba-diving and visits to the local islands for sample collections. Afterwards, long hours are spent in the labs and aquarium processing samples and number-crunching datasets. But it’s not all work and no play for the scientists and support staff here, after a hard day at work we like to get out and make the most of our surroundings through skiing, climbing or simply taking a relaxing walk to meet the wildlife around Rothera Point. In common with most people on station we feel incredibly privileged to be working here on this inspiring and beautiful continent.']	['<urn:uuid:357edbe3-d194-4589-a86f-db76eac00d4a>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:04:36.674696	22	47	675
57	water drought effects methods combat southwest	The American Southwest has been experiencing a megadrought for the last 20 years, which is the second driest in 1,200 years. This drought is characterized as a 'hot drought' where low precipitation combines with higher temperatures due to global warming, causing increased evaporation from soil and plants. The Colorado River Basin has seen a 30-50% decline in streamflow attributable to humans. To combat these conditions, irrigation organizations have developed formal drought plans, with about 40% of large organizations (serving over 10,000 acres) having written plans. These plans include protocols for water delivery restrictions, special drought pricing to incentivize reduced water use, and strategies like water banking to increase storage during non-drought years as a buffer against shortfalls.	"['UArizona Paleoclimatologist Weighs in on \'Hot Drought\' as a Lead Author on IPCC Climate Report\nThe University of Arizona has again contributed to the Intergovernmental Panel on Climate Change report, with paleoclimatologist Jessica Tierney helping pen much of the section on drought and aridity.\nThe Intergovernmental Panel on Climate Change, or IPCC, has released the first of three climate reports expected in the coming months, and a University of Arizona faculty member is a lead author, continuing the university\'s tradition of contributing to the global, informational resource.\nDepartment of Geosciences associate professor Jessica Tierney is one of about 20 U.S. authors on the IPCC Working Group 1 contribution to the Sixth Assessment Report, which provides the latest assessment of scientific knowledge about the warming of the planet as well as projections for future warming and its impacts on the climate system. Tierney helped write much of the Working Group 1 report, including the sections on drought and aridity in the water cycle chapter. The other two reports will come out in December and March.\nTierney was nominated and selected from a pool of scientists to work on the report. The scientists work as a team to write chapters that then go through a long and multilevel review process in which climate scientists, the public and government officials provide feedback.\n""The University of Arizona has a tradition of someone involved in the reports consistently over the years,"" Tierney said.\nIn 2018, Diana Liverman, who recently retired as director of the School of Geography, Development and Environment, contributed to the IPCC\'s ""Summary for Policymakers of the Special Report on Global Warming of 1.5 degrees Celsius."" Former UArizona faculty member Jonathan Overpeck also contributed to past reports.\nOne of the major updates in the latest report has to do with the human influence on drought and aridity, which are particularly relevant for the American Southwest, Tierney said.\nIn 2013, the IPCC reported with low confidence that any changes in droughts could be attributed to human influence. According to the new report, there is now medium to high confidence that recent droughts and drying trends can be attributed to humans in some regions.\n""For example, the 2012-2014 California drought has been studied intensely by climate scientists who determined that it was the worst one in 1,200 years,"" Tierney said. ""Anywhere from about 10 to 30% of that drought was caused by humans.""\n\'Hot Drought\' Becoming More Common\nThe California drought and many others are being driven by what is now being called ""hot drought,"" Tierney said.\n""You have to have low precipitation for drought, but what\'s making droughts really bad now is that it is hotter due to global warming,"" Tierney said. ""When it\'s hot, the atmosphere has a higher demand for moisture. To meet that demand, it evaporates moisture from the soil or through plants. And when you lose all moisture in the land surface, it only makes the drought worse.""\nTo determine how much of the drought was human-caused, scientists used what\'s called detection attribution. They compared preindustrial climate models of a climate not yet perturbed by humans to climate models that reflect current conditions. The difference in outcomes can be attributed to human influence.\n""The U.S. Southwest and California are the case study for this,"" Tierney said, noting that the American Southwest has been in the grip of a megadrought for the last 20 years. It is the second driest megadrought in 1,200 years.\n""But we also observe hot drought in the Mediterranean,"" Tierney said. ""These are two hot spots that we\'ve seen really bad droughts that we can attribute to humans.""\nHot droughts will become more common in more places around the world, according to the Working Group 1 report. Places expected to be more prone to hot drought include Central America, the Amazon, Chile, Southwestern Australia and South Africa.\nAnother issue affecting water in the American Southwest is a decline in streamflow. For example, Colorado River water comes from snowpack, but as the world warms and less precipitation falls as snow, the snowpack declines. According to the new IPCC report, between 30 and 50% of the recent decline in streamflow in the Colorado River Basin can be attributed to humans.\nThe rest of the report covers topics such as the changing state of the climate system, human influence on the climate, and scenario-based projections of future climate, as well as ocean, cryosphere, and sea level change and more.\nResources for the media\nMikayla Mace Kelley\nScience Writer, University Communications\nDepartment of Geosciences\nUniversity of Arizona in the News', 'Some Irrigation Organizations Rely on Formal Drought Plans\nMany irrigation organizations develop drought plans that provide guidance to farmers and other water users on how the organizations plan to respond in the event of a drought. USDA’s Economic Research Service and National Agricultural Statistics Service jointly produced the Survey of Irrigation Organizations (SIO) in 2019. The 2019 survey provided the first updated dataset of local water-supply management entities since the 1978 Census of Irrigation Organizations. The SIO provides details on how irrigation organizations plan for and respond to drought.\nAbout one-fifth of irrigation organizations have a formal, written drought plan. Drought planning is most common among large delivery organizations that serve more than 10,000 irrigable acres. Large delivery organizations, which collectively served about 80 percent of the acreage that could have received off-farm water in 2019, are likely to have greater staff and financial resources to develop a formal plan. About two-fifths of large organizations have a formal, written drought plan. Drought planning is also common for irrigation organizations that manage groundwater but do not deliver water to farms. Groundwater organizations, as defined in this survey, perform at least one of seven roles, such as monitoring groundwater use or permitting new wells, that directly influence on-farm groundwater use. Formal planning is least common among small delivery organizations serving 1,000 or fewer irrigable acres. One reason smaller organizations are less likely to have a formal plan might be that as more acreage and users are involved, coordination of irrigation water use becomes more complex. However, a formal planning process could provide greater benefits to a wider set of users.\nThe most common component in formal drought plans is a set of protocols for how water delivery or pumping restrictions, often referred to as curtailments, would be implemented. Between 68 and 80 percent of irrigation organizations, depending on their type and size, include details about delivery or pumping restrictions in their formal plans. Additional questions in the SIO revealed that organizations tended to use similar approaches for managing water curtailments regardless of whether or not they had a drought plan, which suggests that drought plans may be more of a tool for committing to certain drought response strategies in advance rather than a means for changing those strategies.\nIn some cases, organizations use special pricing that increases water delivery charges or pumping charges during a drought. Higher water prices during the drought season incentivize farmers to reduce water use while helping organizations recover a share of revenue losses caused by reduced deliveries and/or the purchase of additional water supplies. About one-third of the plans for large delivery organizations address expected increases in the price charged for water deliveries or the intent to purchase additional water supplies in the event of drought. Only about 10 to 13 percent of other delivery organizations include drought pricing in their plans.\nSome drought plans include specific provisions to increase water supply for irrigators. This can be done either by purchasing additional supplies during a drought or by increasing storage in reservoirs or aquifers during nondrought years as a buffer against supply shortfalls in drought years (known as water banking). About 30 percent of large organizations’ plans indicate intent to increase water supply during drought years, compared with only about 12 to 15 percent of other organizations’ plans. Between 15 and 21 percent of all plans specify an intention to use water banking.\nLand fallowing, a farming technique that leaves a plot of land uncultivated for a season or more, is a less common component of formal drought plans. In a drought plan, organizations may have provisions for compensating farmers who fallow land or for requiring some form of land fallowing. Land fallowing provisions are included in only 12 to 15 percent of plans for medium and large delivery organizations and groundwater organizations but just 4 percent of small delivery organizations’ plans. For areas served by organizations that do not have fallowing provisions in their drought plan, there is still likely to be some fallowing during drought, but these decisions will be at the farm level without direct input or explicit fallowing incentives provided by the irrigation organization.\nIrrigation Organizations: Drought Planning and Response, by Steven Wallander, Aaron Hrozencik, and Marcel Aillery, ERS, January 2022\nIrrigation & Water Use, by Aaron Hrozencik, USDA, Economic Research Service, May 2022\nIncentives to Retire Water Rights Have Reduced Stress on the High Plains Aquifer, by Andrew B. Rosenberg, USDA, Economic Research Service, October 2020\nFarmers Employ Strategies To Reduce Risk of Drought Damages, by Steven Wallander, Elizabeth Marshall, and Marcel Aillery, USDA, Economic Research Service, June 2017\nUnderstanding Irrigated Agriculture, by Glenn Schaible, USDA, Economic Research Service, June 2017\nClimate Change, Water Scarcity, and Adaptation in the U.S. Fieldcrop Sector, by Elizabeth Marshall, Marcel Aillery, Scott Malcolm, and Ryan Williams, ERS, November 2015']"	['<urn:uuid:d5d53e66-19cb-432f-8edb-e5f5623791ba>', '<urn:uuid:27392021-baae-4d26-aa84-a35e5f6bc5bd>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T06:04:36.674696	6	117	1562
58	liver cancer death change vs lung cancer mortality trend 2005 2016	While liver cancer deaths increased by 87.6% between 1980 and 2014, lung cancer mortality showed a decrease, dropping from 159,292 deaths in 2005 to 148,945 in 2016, representing a 6.5% reduction.	['World Cancer Day was observed on Saturday, February 4. Although the observance day has some and gone, cancer hasn’t; especially liver cancer. A recent article published by JAMA* reported that between 1980 and 2014, cancer mortality decreased by 20 percent in the United States. The number of deaths from nearly all the major forms of cancer have decreased significantly. The exceptions are liver, kidney, and non-Hodgkin lymphoma (NHL). Deaths due to NHL and kidney cancer remained the same. Liver cancer increased by 87.6 percent.\nThis is quite disturbing. The number of deaths from lung cancer dropped by nearly 69 percent; colon cancer dropped by 35 percent; breast cancer by 32 percent. Liver cancer is not following the trend.\nIt isn’t just in the US. World Hepatitis Alliance CEO, Raquel Peck wrote that 800,000 people died worldwide from liver cancer last year (Preventing Cancer Before It Strikes). Of these, 642,500 were related to hepatitis B and C virus. Actually, hepatitis C may cause many more cases of cancer and death from it than are captured by the data. First, risk of death is increased for all causes of mortality including other types of cancer among people with hepatitis C. Second, we know that people with hep C are at increased risk for risk of non-Hodgkin lymphoma and kidney cancer. These are the two other cancers mentioned in the JAMA article that did not decrease in incidence between 1980 and 2014.\nIn order to reduce the risk of liver cancer, we have to treat people with hepatitis C sooner, before they reach cirrhosis. However, if you already have cirrhosis, talk to your doctor about liver cancer screening recommendations. In the US, screening includes imaging (ultrasound, CT, or MRI) every 6 months.\nIf you have hepatitis B, your liver does not need to be cirrhotic in order to be at risk for liver cancer. Talk to your doctor about the current cancer screening recommendations for your risk factors.\nPeople with hepatitis C tend to have a higher risk for non-Hodgkin lymphoma in the early stages, before the liver is cirrhotic. This is one reason why we need to ramp up our viral hepatitis prevention message, screen more people and treat them in the earliest stages of hepatitis C.\nThe JAMA article pointed to obesity and poor diet as factors that may be fuelling the incidence of cancer. These lifestyle-related factors certainly have been implicated in liver cancer. Non-alcoholic fatty liver disease (NAFLD) is quickly becoming the most common liver disease in the United States. People with NAFLD are at increased risk of liver cancer. The same is true for alcoholic liver disease.\nSo, don’t wait around wondering if you might get liver cancer. Here are some ways to reduce your risk:\n- If you haven’t had hepatitis B or been immunized against it, get vaccinated.\n- If you have hepatitis B, talk to your doctor about how to manage it.\n- If you have any risk factors for hepatitis C, get tested and then treated if you have it.\n- Don’t drink too much alcohol.\n- Maintain a healthy weight and prevent diabetes.\n- Eat a low-fat, high nutrition\n- Aim for regular physical activity.\n- If you have cirrhosis or other risk factors for liver cancer, talk to your medical provider and ask about regular cancer surveillance.\n#WeCanICan stop liver cancer.\n*Trends and Patterns of Disparities in Cancer Mortality Among US Counties, 1980-2014 by Ali H. Mokdad, PhD, Laura Dwyer-Lindgren, MPH, Christina Fitmaurice, MD, MPH JAMA 2017', 'Lung cancer is currently the most common cancer worldwide. In most countries, lung cancer is also the most malignant form of tumor with the lowest survival rates. As of 2018, in most countries worldwide, lung cancer is the primary cancer-related cause of death. A staggering 1.8 million deaths from lung cancer in both sexes combined were recorded in 2018 globally. In the USA lung cancer mortality peaked at 159,292 in 2005 but has since decreased by 6.5% to 148,945 in 2016.\nThe five-year survival rate for lung cancer in the U.S.A. as of 2015 is 18.6 percent, much lower than other types of cancers such as colorectal (64.5 percent), breast (89.6 percent) and prostate (98.2 percent). When the disease is diagnosed at an early stage, the survival rate of lung cancer can be as high as 56 percent. Unfortunately, most lung cancer patients are rarely, if ever, diagnosed at an early stage. As a result of the spreading of the malignancy, more than half of lung cancer patients die within one year of being diagnosed. The 5-year survival rate regardless of stage is only 15% among lung cancer patients.\nThe latest statistics provided by the World Cancer Research Fund International revealed a total of 2,093,876 new cases of lung cancer being diagnosed in 2018 for both sexes worldwide. Prevalence of lung cancer was the highest among men worldwide, representing 15.5% (1,368,524 new cases) of the total number of new cases diagnosed in 2018. Lung cancer ranked number 3 for women, representing 8.8% of the total number of new cancer cases diagnosed in 2018, or 725,352 new cases in lung cancer alone, worldwide.\nWhen individuals stop working due to cancer this represents a loss to society in the loss of productivity. The National Institutes of Health estimated that annual costs for the care of lung cancer in the U.S.A. reached $13.4 billion in 2015. Loss of earnings/productivity due to early death from lung cancer in the U.S.A adds another $21.3 billion in costs in 2015. In other countries, including Ireland, lung cancer is responsible for the highest productivity loss for cancer-related mortality.\nProjected productivity loss for cancer-related mortality 2011-2030 in Ireland\n[Source: Alison Pearce, Cathy Bradley, Paul Hanly, Ciaran O’Neill, Audrey Alforque Thomas, Michal Molcho and Linda Sharp Projecting productivity losses for cancer-related mortality 2011 – 2030 BMC Cancer. 2016 Oct 18;16(1):804.PMCID: PMC5069877 DOI: 10.1186/s12885-016-2854-4]\nIn the past, lung cancer was regarded as a disease for the elderly as the disease was found mostly in men over 50 years old, between the ages of 60-75 years. About 75% of lung cancer cases were attributed in part to tobacco smoking, with a higher estimate of 85% to 90% for the U.S.A. Compared to people who do not smoke, men who smoke are 23 times more likely to develop lung cancer, whereas women are 13 times more likely. Duration of smoking is considered to be the strongest determinant of lung cancer risk in smokers.\nEver since the early 1950’s when the carcinogenic effects of tobacco smoke on the lung were repeatedly shown in scientific studies, the rate of tobacco smoking began to decline sharply after the recognition by public health regulatory authorities in the mid-1960’s. In the U.S.A., the self-reported adult smoking rates peaked in 1954 at 45%, and remained at around 40% or more through the early 1970’s. The average rate of smoking across the decades began falling from 40% in the 1970s to 32% in the 1980s, 26% in the 1990s, and 24% since 2000. In 2008, the percentage of U.S. adults saying they smoked cigarettes was only 21%, a decrease of more than 50% from the peak at 45% in 1954.\nPER CAPITA CONSUMPTION OF CIGARETTES AMONG ADULTS 18 YEARS AND OLDER FROM 1900 – 2004 IN THE U.S.A.\n[SOURCE: THE NATIONAL ACADEMY PRESS, ENDING THE TOBACCO PROBLEM: A BLUEPRINT FOR THE NATION (2007) https://www.nap.edu/read/11795/chapter/4]\nDespite the dramatic reduction of close to 50% in smoking rates between 1975 to 2004 in the U.S.A., the age-adjusted cancer incidence rates for lung cancer actually INCREASED by 6.3 percent from 52.2 per 100,000 in 1975 to 55.5 per 100,000 in 2011.\nEven though the development of lung cancer is attributed mainly to smoking, approximately 25% of lung cancer incidences worldwide are not related to tobacco use. Lung cancers in never-smokers account for over 300,000 deaths worldwide each year.\nStriking differences in the epidemiological, clinical and molecular characteristics of lung cancers arising in never-smokers versus smokers have been identified, suggesting that they are separate entities. Never smokers are people who have smoked less than the equivalent of 100 cigarettes in their entire lifetime. Lung cancer in never-smokers affect females more than males. It is estimated that 53% of all women with lung cancer worldwide compared to 15% of men, are never-smokers. In the U.K. 67% of never-smokers who underwent lung cancer surgery from 2008 to 2014 were females.\nLung Cancer Age‐Adjusted Incidence Rates by Sex, 1975‐ 2011\n[Source: National Cancer Institute. SEER Cancer Statistics Review, 1975‐20011 Credit: American Lung Association Epidemiology and Statistics Unit Research and Program Services Division, Trends in Lung Cancer Morbidity and Mortality November 2014 https://www.lung.org/assets/documents/research/lc-trend-report.pdf]\nDistinct clinical, pathological and biological features of lung cancer in never-smokers are also observed, in that small-cell lung cancer that is highly correlated with smoking is quite rare in never-smokers; and lung cancer in never-smokers is almost exclusively made up of non-small-cell lung cancer, with a predominance of adenocarcinoma over squamous cell carcinoma in the ratio of around 8:1 in Europe. Studies before 1990 also confirmed the observation that adenocarcinoma was most common among lung cancer in never-smokers.\nThe most troubling aspect in the diagnosis and treatment of lung cancer in never-smokers worldwide is that the demographics now include younger patients in their 20s, 30s, and 40s, instead of the traditional mean age of 70 at diagnosis.\nCompared to smokers, the carcinogenesis of lung adenocarcinoma in never-smokers is more complicated. Many factors may cause the development of lung cancer in never-smokers, the major contributors are considered to be:\nEnvironmental Tobacco Smoke (ETS)\nEnvironmental Tobacco Smoke (ETS) is defined as “sidestream smoke from the smoldering tobacco between puffs and exhaled mainstream smoke from the smoker.” The link between ETS and lung cancer was first reported in 1981. In 1992, the Environmental Protection Agency published a review on the effects of ETS on lung cancer and indicated that ETS is associated with increased risk for lung cancer. However, in 2005 a large population study involving over 500,000 volunteers by the European Prospective Investigation into Cancer (EPIC) and Nutrition, an ongoing multi-centre prospective cohort study designed to investigate the relationship between nutrition and cancer, revealed that there is no statistically significant hazard in the development of lung caner from exposure to ETS. The current evidence seems to suggest that ETS plays a modest role in the development of lung cancer in never-smokers.\nRadon is a naturally occurring radioactive gas produced by uranium decay in the earth’s crust. It emits alpha particles, decaying to polonium and bismuth. indoor levels can be quite variable depending on soil composition, building foundations and ventilation. Radon can accumulate to unsafe levels in basements and lower building levels. Radon exposure in underground workplaces is regulated in the U.S.A. Radon exposure is considered to be the second leading cause of lung cancer after tobacco smoke. Radon is not only an independent risk factor; it also increases the risk of lung cancer in smokers.\nOccupational exposure to carcinogens is estimated to account for 5–10% of lung cancers. Of these, asbestos is the most common. The risk for lung cancer from asbestos exposure is dependent on both fiber type and dose, as larger chrysotile fibers can be cleared from the lungs more rapidly than amphibole fibers. Studies that did not detect increased lung cancer mortality from nonoccupational asbestos exposure involved populations that were predominantly exposed to chrysotile fibers. In general, it is believed that nonoccupational exposure to asbestos may not have a significant role in increasing mortality from lung cancer in never-smokers.\nInfection & Inflammation\nDamage to the lungs from inflammation and infection is often implicated in tumorigenesis. Pre-existing lung disease like tuberculosis increases odds ratio up to 1.76 for the development of lung cancer, regardless of smoking status. Chronic inflammatory lung diseases also confer additional risks for cancer development. However, the majority of lung cancer patients who are never-smokers do not have a history of active interstitial lung disease.\nInherited Genetic Susceptibility\nA positive family history of lung cancer has been found to be a risk factor in several registry-based studies that have reported a high familial risk for early-onset lung cancer. Family history of lung cancer is correlated with increased risk in the development of lung cancer in both smokers and never-smokers. For never-smokers, genetic influences would explain the increased risk in the younger age group.\nGenetic Risk Factors\nLung cancer in never-smokers is almost exclusively made up of NSCLC (non-small-cell lung cancer), with a predominance of adenocarcinoma. NSCLC patients often show metastasis to major organs such as liver (33–40%), brain (15–43%), kidney (16–23%), adrenal glands (18–38%), bone (19–33%), and abdominal lymph nodes (29%). The difficulty in treating primary tumors and related metastatic secondary mutations accounts for the poor prognosis of NSCLC. The 5-year survival rate for advanced NSCLC patients remain stubbornly below 5%.\nScientists now believe that genes may be involved in the increased susceptibility to adenocarcinoma in young never-smokers, especially in the gender disparity as the incidence of lung cancer in never-smokers worldwide indicate a female predominance. 53% of females who develop lung cancer are never-smokers while only 15% of male lung cancer patients never smoked, and the incidence of lung cancer in women can vary by 30-fold even in countries reported to have low prevalence of female smoking.\nThe higher incidence of lung cancer in female never-smokers has led scientists to explore gender-dependent hormones in the development of lung cancer. It has been observed that women who received anti-estrogen therapies showed a reduction in lung cancer incidence while women placed on hormonal replacement therapy showed increased risks for NSCLC.\nAlthough lung cancer in never-smokers share molecular features that are typical in tobacco-related carcinogenesis, the presence of unique genetic and epigenetic markers indicate that a different but possibly overlapping carcinogenic pathway is responsible for the development of lung cancer in never-smokers.\nSpecific genetic mutations have been associated with a higher prevalence of adenocarcinomas in never-smokers. Those genes include EGFR, PTEN, ALK, ROS1, and RET; whereas a different set of genetic mutations in smokers have been identified. These genes are K-RAS, TP53, BRAF, STK11, and JAK2/3. Mutations and hypermethylation of p16 and LGALS4 are also implicated in progression of lung cancer in smokers.\nDifferent Frequencies of Oncogenic Drivers in Never-smoking vs. Smoking Non-Small Cell Lung Cancer\n[Shin Saito, Fernando Espinoza-Mercado, Hui Liu, Naohiro Sata, Xiaojiang Cui, and Harmik J. Soukiasian, Current status of research and treatment for non-small cell lung cancer in never-smoking females, Cancer Biol Ther. 2017; 18(6): 359–368. doi: 10.1080/15384047.2017.1323580]\nAs a result of dedicated efforts committed to the control of tobacco use, smoking prevalence and lung cancer mortality have decreased over the past several decades in the USA. However, tobacco smoking remains the major cause for lung cancer, whereas different carcinogens as well as genetic influences could be involved for different groups of never-smokers. With ever increasing understanding of the mechanisms behind the pathogenesis of lung cancer for smokers and never-smokers, it is anticipated that successful treatment can be tailored for the individual patient based on the presence or absence of critical molecular alterations.']	['<urn:uuid:5a7b661b-0746-4dbf-9cc9-f1272d3155b4>', '<urn:uuid:b7ac78f4-395d-4516-b558-e7a11828430d>']	factoid	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-13T06:04:36.674696	11	31	2508
59	bacteria types found himalayan glaciers	In the Himalayan glaciers, four main types of bacteria were found: Firmicutes (making up 71.42%), Actinobacteria (14.28%), Alpha-proteobacteria (9.52%), and Beta-proteobacteria (4.76%). These bacteria were found to be mostly psychrotolerant, meaning they can grow at low temperatures, with optimal growth occurring between 15 and 20°C, pH 6-8, and NaCl 0-2%.	['Bacterial Diversity in an Alpine Debris-Free and Debris-Cover Accumulation Zone Glacier Ice, North Sikkim, India\nThe Himalayas are water tower for billions of people; however in recent years due to climate change several glaciers of Himalaya are receding or getting extinct which can lead to water scarcity and political tensions. Thus, it requires immediate attention and necessary evaluation of all the environmental parameters which can lead to conservation of Himalayan glaciers. This study is the first attempt to investigate the bacterial diversity from debris-free Changme Khang (CKG) and debris-cover Changme Khangpu (CK) glacier, North Sikkim, India. The abundance of culturable bacteria in CKG glaciers was 1.5 × 104 cells/mL and CK glacier 1.5 × 105 cells/mL. A total of 50 isolates were isolated from both the glacier under aerobic growth condition. The majority of the isolates from both the glaciers were psychrotolerant according to their growth temperature. Optimum growth temperatures of the isolates were between 15 and 20 °C, pH 6–8 and NaCl 0–2%. The phylogenetic studies of 16S RNA gene sequence suggest that, these 21 isolates can be assigned within four phyla/class, i.e., Firmicutes, Beta-proteobacteria, Gamma-proteobacteria and Actinobacteria. The dominant phyla were Firmicutes 71.42% followed by Actinobacteria 14.28%, Alpha-proteobacteria 9.52% and Beta-proteobacteria 4.76%. The isolate Bacillus thuringiensis strain CKG2 showed the highest protease activity (2.24 unit/mL/min). Considering the fast rate at which Himalayan glaciers are melting and availability of limited number of research, there is urgent need to study the microbial communities confined in such environments.\nKeywordsPsychrophiles Psychrotolerant Changme Khang Changme Khangpu Glacier\nThe authors would like to thank the Department of Microbiology, Sikkim University for providing laboratory facilities. Thanks to DST (IUCCC) for providing JRF fellowship to MTS and Forest Department, Govt. of Sikkim for providing research permit and access.\nNT designed the study, reviewed and edited manuscript, MTS did the experimental works, analysis and prepared the manuscript, INN and SD contributed in growth profiling experiments.\nCompliance with Ethical Standards\nConflict of interest\nAuthors have no conflict of Interests.\n- 1.Byers A (2012) Committee on Himalayan Glaciers, hydrology, climate change, and implications for water security. The National Academies Press, Washington, pp 78–103. https://www.nap.edu/initiative/committee\n- 2.Jarraud M (2012) WMO statement on the status of the global climate in 2012. In: World meteorological organization, Geneva 2, Switzerland, pp 55–67. https://library.wmo.int/\n- 3.Petschel G (2007) Mitigation of climate change: contribution of working group III to the fourth assessment report of the Intergovernmental panel on climate change. In: Netherlands environmental assessment agency, Netherland. pp 71–76. https://www.ipcc.ch/pdf/assessment\n- 4.Gupta C, Prakash DG (2014) Role of microbes in combating global warming. Int J Pharm Sci Lett 4:359–363Google Scholar\n- 6.Raman KV, Singh L, Dhaked R (2000) Botechnological application of psychrophiles and their habitat to low temperature. J Sci Indian Res 59:87–101Google Scholar\n- 9.Bowman JP, Rea SM, McCammon SA, McMeekin TA (2000) Diversity and community structure within anoxic sediment from marine salinity meromictic lakes and a coastal meromictic marine basin, Vestfold Hills, Eastern Antarctica. Environ Microbiol 2:227–237. https://doi.org/10.1046/j.1462-2920.2000.00097.x CrossRefPubMedGoogle Scholar\n- 19.Ramakrishna TPM (1988) Self association of a Chymotypsin: effect of amino acids. J Biosci 3:15–22Google Scholar\n- 23.Miteva V, Miteva V (2008) Psychrophiles: from biodiversity to biotechnology. Springer, New York, pp 78–99Google Scholar']	['<urn:uuid:f96f788d-fc37-4948-8eff-a2c821ae38f6>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	5	50	530
60	doing research on decision making how do lab rats demonstrate complex cognitive abilities and how can this connect to modern farming optimization techniques	Rats demonstrate sophisticated decision-making abilities through their capacity to experience regret, as shown in experiments where they recognize and modify behavior after suboptimal choices. In the 'Restaurant Row' study, rats displayed regret when passing up good food deals only to encounter worse options later, looking back at their mistake and showing distinctive brain activity patterns. This understanding of decision-making processes can connect to modern farming optimization, particularly in vertical farming systems where automated sensors and artificial intelligence are used to make complex decisions about crop management. These systems employ automated control over environmental conditions through sensors and imaging techniques combined with crop simulation models, demonstrating how both biological and technological decision-making processes can be optimized for better outcomes.	"['Rat regret informs decision research\nRats that passed up a short wait at one feeding station only to encounter a much longer wait at the next showed evidence of regret.\nJune 19, 2014\nWhen his graduate student Adam Steiner walked in and announced, “My rats are expressing regret,” neuroscience professor David Redish responded along the lines of ""You\'ve got to be kidding.""\nRedish uses rats to probe fundamental mechanisms of decision-making, and sees no reason other animals’ brains shouldn’t resemble humans’. But the idea of regret in rats still came as a surprise, and it required solid evidence—which he and Steiner have now supplied.\nOn June 8 the researchers reported in Nature Neuroscience that rats can recognize when they’ve made a boneheaded decision and change their behavior in response. That, says Redish, is the essence of regret. And it means rats may act as stand-ins for humans as researchers probe how the brain makes decisions.\n“When we understand how rats make decisions, it tells us something about how humans do it,” says Redish. “The more we understand about how decision-making processes work, the more we can understand about how they go wrong and how to fix them.\n“Addiction is one example. Addicts always say they’re going to stop, and then they go smoke ‘one last cigarette,’ which they regret.”\n“In a normal situation, you’d expect that if you do something you really don’t like, you won’t do it again,” says Steiner. “Addicts realize its’ bad, but maybe that signal of regret isn’t strong enough. Before we can understand what’s going wrong with the regret mechanism we have to understand how it goes right and exerts its beneficial effect. It’s easier to see how something’s broken if you know how it works.”\nAlthough findings will be tested in humans, the researchers say rats are valuable for teasing out the mechanisms of regret. Lacking language, rats don’t complicate experiments by, for example, hedging or lying about what they’re experiencing,\nThe researchers placed the rats on “Restaurant Row”: a square runway with corner spokes, each leading to a “restaurant” offering a food morsel in one of four flavors. As the rats reached each spoke, a tone told the rat how long the wait was (higher pitch=longer wait).\nRats had individual flavor preferences and showed a “threshold” delay they were willing to accept for each flavor.\n“These furry little guys know what they like. For instance, they generally didn’t like the chocolate-flavored pellets,” says Steiner. “They make decisions in many cases based on individual preferences, and their needs of the moment.”\nBecause trials only lasted an hour, the rats were under time pressure to take only “good deals”—i.e., to wait for short delays for preferred morsels—and pass up bad deals (long delays for less preferred morsels). Once a rat had eaten its food or passed it up and gone to the next spoke, it couldn’t go back for a do-over.\nRedish and Steiner recorded patterns of neural activity from two areas of the brain associated with regret in humans to see if regret-like processes modified the rats’ decision-making as they faced “stay or go” food choices.\nLet’s make a deal\nThe rats faced regret-inducing situations whenever they passed up a good deal only to encounter a bad deal at the next spoke. That was akin to refusing a wait of 20 minutes at your favorite restaurant, then driving 10 minutes to your second favorite and finding a 30-minute wait.\nIn that case, the rats turned and looked backward toward the spoke where they had made the mistake. And their brain activity signaled that they were, mentally, back at that bad decision point. The rats also rushed through eating the “bad deal” morsels and hurried to the next spoke.\nNone of this happened when the rats were merely “disappointed” by a bad deal. In a disappointment situation, the bad deal followed either another bad deal or a good deal they had taken, giving the rats no cause to rethink a decision.\nIn other words, rats’ behavior revealed that they could distinguish regret from disappointment.\n“In the regret situation, it turned around and looked back. But it didn’t do that in a disappointment situation,” Redish explains. “In disappointment, the world is just bad—it’s not their fault. But in regret, the rat made a mistake. We now know they recognize that difference.”\n“It was fairly surprising,” says Steiner, “but in retrospect, it makes sense. Regret keeps you from making the same mistake over and over.”\nNext stops on the research path\nHaving found that rats make suitable subjects for research on regret, Redish looks forward to investigating new questions that this research opens up:\n• Do rats avoid regret?\n• Why do we avoid regret?\n• How does regret change decision-making, and how does it interact with other mental phenomena such as deliberation and habit?\nRedish, along with colleagues, is in the planning stages for a human version of Restaurant Row. Those experiments will examine how brain function correlates with various problems, or lack of problems, in decision-making ability.\nThe research was supported by the National Institute on Drug Abuse.\nContact the writer at email@example.com', 'Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012']"	['<urn:uuid:ef2df1a2-1e74-469e-aee5-37fd459e69fc>', '<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T06:04:36.674696	23	118	1944
61	How do early symptoms of hypothyroidism differ from jaundice in newborns?	Both conditions can affect newborns but have distinct symptoms. Congenital hypothyroidism symptoms include problems sleeping, feeding, breathing, poor temperature control, and yellowing skin. In contrast, jaundice primarily manifests as yellow coloration of the skin and whites of the eyes due to high bilirubin levels. While both conditions can have serious consequences if untreated - hypothyroidism leading to growth problems and mental retardation, and severe jaundice potentially causing permanent brain damage - they are both routinely screened for and treatable.	"['Congenital hypothyroidism is a condition found in newborns. When a baby has this condition, his/her thyroid gland is not making enough hormones for the brain and body to grow and develop.\nThe thyroid gland is located in the neck and is shaped like a butterfly. The thyroid gland makes many hormones that are important for growth and development. The hormones are called thyroxin (T4) and triiodothyronine (T3). These hormones are also important for energy and for the heart, liver, kidneys and skin. The brain makes a hormone called thyroid-stimulating hormone (TSH). TSH tells the thyroid gland how much T4 and T3 to make. When a child’s TSH is too high, the brain is working hard to tell the thyroid to make more T4 and T3. Sometimes the thyroid gland doesn’t make enough hormone no matter how hard the brain works. This is known as hypothyroidism.\nThere are many causes of congenital hypothyroidism. A baby can be born without a thyroid gland, the thyroid gland may be in the wrong place or the gland may not work correctly.\nCongenital hypothyroidism can have little or no symptoms. If your baby has symptoms, he or she may have:\n- Problems sleeping\n- Problems feeding\n- Problems breathing\n- Yellowing skin\n- Poor temperature control\nAll babies are tested for this condition within the first week of life. When a newborn screening (blood test) shows values that are not normal, your doctor may suspect hypothyroidism.\nCongenital hypothyroidism can be easily treated with a daily medicine. The medication replaces the T4 that your child’s thyroid gland is unable to make. It is very important to make sure your child gets his/her medicine every day at about the same time. Children who do not take medicine to treat his/her hypothyroidism will have trouble growing and can develop mental retardation. Most children take this medication for life.\nThe medicine can be given with small amounts of formula or food. You may break the tablets in half or crush them to make it easier for your child to take.\nThe doctor will want to see you frequently during the first year to check on growth and development. Your doctor may want to see you as often as once a month. Sometimes the medicine dose will need to be changed. Blood draws during these visits will help your doctor make sure the medication dose is correct. Never change a medicine dose on your own. Once your child starts taking the medicine, there are symptoms to watch for that can help your doctor treat your child with the correct dose of medication.\nYour child’s dose may be too high if he/she has:\n- Trouble sleeping\n- Shaking (tremors)\n- Weight loss\n- Excessive hunger\nYour child’s dose may be too low if he/she:\n- Is sleeping too much\n- Has constipation\n- Has cold, dry skin\n- Has gained weight too quickly\n- Has low energy/activity level\nSome infants will sleep less and seem more irritable after starting on thyroid medicine. This may mean that your baby has more energy and is a normal reaction to the medicine. A baby who has hypothyroidism and is not on medicine does not have as much energy as a baby who has hypothyroidism and is on medication.\nIt is important to make sure your child stays on the same brand of thyroid medicine. There are very small differences between brands of thyroid medicine that might affect your child’s labs and the way he/she feels. Please notify your nurse or doctor if your pharmacy changes brands of thyroid medication.', ""Blood type and Rh determination in mother and infant. This is because there's a small risk the bilirubin could pass into the brain and cause brain damage. Ensure that routine metabolic screening (including screening for congenital hypothyroidism) has been performed. But only around 1 in 20 babies has a blood bilirubin level high enough to need treatment. What causes jaundice? Jaundice is 1 of the most common conditions that can affect newborn babies. Approximately 60% of term and 80% of preterm babies develop jaundice. Is it safe to delay your period for your holiday? If a baby with very high levels of bilirubin is not treated, there's a risk they could develop permanent brain damage. I. Upgrade to Patient Pro Medical Professional? Neonatal jaundice What is jaundice? Further investigation is essential for any baby who is also unwell, presents in the first 24 hours or has prolonged (after 10 days) jaundice. Reducing substance in urine: screening test for galactosaemia (provided the infant has received sufficient quantities of milk). Check SBR (note that a direct SBR very rarely indicated within the first 5 days of life). jaundice beyond 14 days of life) you must refer to the Prolonged Neonatal Jaundice - Management in the Community protocol. This information explains the advice about jaundice in newborn babies that is set out in NICE guideline CG98. Do not measure bilirubin levels routinely in babies who are not visibly jaundiced. Follow expert advice about care for babies with a conjugated bilirubin level greater than 25 µmol/L because this may indicate serious liver disease. haemolytic disease of the newborn (rhesus). Symptoms of newborn jaundice. Jaundice is the name given to the yellow appearance of the skin and the whites of the eyes. Any baby presenting with jaundice in the first 24 hours of life should therefore be seen urgently for assessment in hospital. This guidance is changing frequently. What should the junior doctor know? Neurological signs - eg, changes in muscle tone, seizures, or altered crying - require immediate attention to avoid kernicterus. Physiological jaundice is by far the most common cause and is easily recognisable, typically appearing two to three days after birth in an otherwise well infant and normally resolving by 2 weeks of age. By the time a baby is about 2 weeks old, their liver is more effective at processing bilirubin, so jaundice often corrects itself by this age without causing any harm. This often requires inpatient hospital treatment with phototherapy. They are written by UK doctors and based on research evidence, UK and European Guidelines. There were 7 hospital admissions for kernicterus in England in 2015-16. He is now nearly 2... Assess your symptoms online with our free symptom checker. NICE has issued rapid update guidelines in relation to many of these. Learn about Neonatal Jaundice from our professional reference. Jaundice reaches its peak at about four days of life and This is known as kernicterus. What is jaundice? 1 It is usually clinically detected when the serum bilirubin levels are greater than 40-50µmol/L (when the jaundice is observable). Each of the Neonatal Unit leaflets are detailed below, select the heading of the one you would like to view and the content will expand with an option for you to download the PDF version. Neonatal Unit Jaundice in New-born Babies Information Leaflet Jaundice is common in new-born babies and is when their skin and whites of their eyes (sclera) look yellow. Adv Neonatal Care. Add filter for Diabetes UK (13) ... Add filter for Patient (261 ... Base the diagnosis of neonatal jaundice on clinical observation at every contact, particularly within the first 72 hours. … Care and support of people growing older with learning disabilities Cataracts Dementia Coronavirus: how quickly do COVID-19 symptoms develop and how long do they last? For most babies, jaundice is physiological and not harmful. Biliary atresia requires surgery within the first two months of life for a better prognosis. Hepatosplenomegaly, petechiae and microcephaly are associated with haemolytic anaemia, sepsis and congenital infections. Treatment for newborn jaundice is not usually needed because the symptoms normally pass within 10 to 14 days, although they can occasionally last longer. Neonatal jaundice describes a condition in which an infant’s skin appears yellow within the first few days of life. How to treat constipation and hard-to-pass stools. Disclaimer: This article is for information only and should not be used for the diagnosis or treatment of medical conditions. As Walls (2004) says, with appropriate training and enthusiastic community support, treating neonatal jaundice at home appears to be feasible, safe, and well accepted by families and medical staff alike. from the best health experts in the business, Neonatal jaundice, NICE Quality Standards (Mar 2014), Jaundice in the newborn; NICE CKS, November 2015 (UK access only). Patient aims to help the world proactively manage its healthcare, supplying evidence-based information on a wide range of medical and health topics to patients and health professionals. Background: Neonates with jaundice are usually managed according to their serum bilirubin despite an unclear overall correlation between bilirubin levels and patient-important outcomes (PIOs) such as kernicterus spectrum disorder (KSD). Usually, a total serum bilirubin level is the only testing required in a moderately jaundiced infant who presents on the second or third day of life and is otherwise well. The bilirubin level does not usually rise above 200 μmol/L and the baby remains well. Blanching reveals the underlying colour. Neonatal jaundice is one of the most common conditions needing medical attention in newborn babies. Neonatal jaundice refers to yellow colouration of the skin and the sclera (whites of the eyes) of newborn babies that results from accumulation of bilirubin in the skin and mucous membranes. Cochrane Database Syst Rev. Jaundice is one of the most common conditions affecting newborn babies with approximately 60% babies (80% of premature babies) developing jaundice in the first week of life. Patients who present with jaundice in the third trimester may require delivery. Quality standard - Jaundice in newborn babies under 28 days. Jaundice is common in newborn babies because babies have a high number of red blood cells in their blood, which are broken down and replaced frequently. 2013 Apr2(2):61-9. doi: 10.4103/2249-4847.116402. Neonatal jaundice can often be physiological due to increased break down of premature erythrocytes and insufficient Glucuronyl Transferase in the newborn liver but jaundice … Pregnancy and neonatal jaundice. This is known as prolonged jaundice.\nMarge Vs Itchy And Scratchy Tv Tropes,\nFour Poster Wood Bed,\n974 Bus Route Dtc Delhi,\nOne Piece Season 22 Release Date,\nLady With Lemon Painting Meaning,""]"	['<urn:uuid:98c0b09d-1656-488e-91a3-05e4103b6988>', '<urn:uuid:04a5cafe-1419-477f-932c-bc23d3f91b21>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-13T06:04:36.674696	11	79	1687
62	how do technological advancements impact horse racing safety and coral reef restoration methods	Technological advancements have significantly impacted both horse racing safety and coral reef restoration. In horse racing, modern technology includes thermal imaging cameras that can detect overheated horses after races, and 3D-printed prosthetics and casts are now used to help injured horses. In coral reef restoration, technical methods include using special undersea vacuums to clear transplant areas, and sophisticated techniques for coral reattachment using cement, nails, and rebar. Additionally, navigational aid buoys are now installed to prevent ship groundings on reefs, and coral nurseries are utilized to regrow and rehabilitate damaged coral populations.	['The History of Horse Racing\nHorse racing began in the ancient world as a contest of speed. This evolved into a public entertainment business with a large field of runners, a huge crowd and a spectacle. In modern times, horse race betting has become a global phenomenon. Betting on a horse race involves placing a bet on the winner, place, number of finishers or as an accumulator bet.\nThe oldest record of horse racing comes from the Greek Olympic Games in 700 to 40 B.C. The race involved the competition of two horses bareback on a track. The race is considered to be the first documented horse race.\nDuring the reign of Louis XIV (1643-1715), the popularity of gambling and racing was widespread. Racing officials could not keep up with the many new drugs and substances that were being introduced. New medications included powerful painkillers and anti-epilepsy products.\nWith the introduction of synthetics, race officials found it impossible to keep track of all the drugs. It was difficult to distinguish whether a horse had been given too much of the new drugs. As a result, horse races were often fixed. One method was to juice the horse. A horse could be overheated before or after the race.\nSince then, horse racing has been a part of the culture and mythology of many nations. Some countries, such as France, England, Spain and Japan, have a Triple Crown, which includes the King’s Plate, the Prix de l’Arc de Triomphe, and the Coronation Cup. These races feature the biggest purses and are considered prestigious.\nIn the United States, there are several types of horse races. There are classic American races such as the Preakness, the Belmont and the Kentucky Derby. These are the most prestigious flat races, which are seen as tests of stamina and speed.\nA handicap race is a Thoroughbred horse race in which the horses are assigned different weights based on their ability. The goal of handicapping is to give all the horses a chance to win. Often, handicaps are set centrally in the racing area where they are controlled, but may vary from track to track.\nBefore the Civil War, American Thoroughbreds were recognized for their stamina and endurance. But the Civil War changed the focus of the sport, with the emergence of the American racing industry and the demand for larger, public events. Dash and steeple chases were replaced by shorter, more competitive races. Also, heats were reduced to 2 miles.\nHistorically, the best horses were given a silver cup. However, this monetary incentive didn’t help to promote the welfare of the horse. To avoid this, California banned wagering on racing in 1909. By the late 1930s, California’s ban was lifted. Still, the California ban wasn’t meant to help the horses, but to stamp out a criminal element.\nIn the 21st century, technological advances have altered the sport and impacted its popularity. Thermal imaging cameras are now available that can detect overheating horses after the race. Even 3D-printed prosthetics and casts are being used to assist injured horses.', 'Growing less than a quarter inch per year, the elaborate coral reefs off the south coast of Puerto Rico originally took thousands of years to form. And over the course of two days in late April 2006, portions of them were ground into dust.\nThe tanker Margara ran aground on these reefs near the entrance to Guayanilla Bay. Then, in the attempt to remove and refloat the ship, it made contact with the bottom several times and became grounded again. By the end, roughly two acres of coral were lost or injured. The seafloor was flattened and delicate corals crushed. Even today, a carpet of broken coral and rock remains in part of the area. This loose rubble becomes stirred up during storms, smothering young coral and preventing the reef’s full recovery.\nNOAA and the Puerto Rico Department of Natural and Environmental Resources have been working on a restoration plan for this area, a draft of which they released for public comment in September 2014 [PDF]. In order to stabilize these rubble fields and return topographic complexity to the flattened seafloor, they proposed placing limestone and large boulders over the rubble and then transplanting corals to the area.\nThis is in addition to two years of emergency restoration actions, which included stabilizing some of the large rubble, reattaching around 10,500 corals, and monitoring the slow comeback and survival of young coral. In the future, even more restoration will be in the works to make up for the full suite of environmental impacts from this incident.\nCaribbean Cruising for a Bruising\nUnfortunately, the story of the Margara is not an unusual one. In 2014 alone, NOAA received reports of 37 vessel groundings in Puerto Rico and the U.S. Virgin Islands. About half of these cases threatened corals, prompting NOAA’s Restoration Center to send divers to investigate.\nAfter a ship gets stuck on a coral reef, the first step for NOAA is assessing the situation underwater. If the vessel hasn’t been removed yet, NOAA often provides the salvage company with information such as known coral locations and water depths, which helps them determine how to remove the ship with minimal further damage to corals. Sometimes that means temporarily removing corals to protect them during salvage or figuring out areas to avoid hitting as the ship is extracted.\nOnce the ship is gone, NOAA divers estimate how many corals and which species were affected, as well as how deep the damage was to the structure of the reef itself. This gives them an idea of the scale of restoration needed. For example, if less than 100 corals were injured, restoration likely will take a few days. On the other hand, dealing with thousands of corals may take months.\nNOAA already has done some form of restoration at two-thirds of the 18 vessel groundings with coral damage in the region this year. They have reattached 2,132 corals to date.\nWhat does this look like? At first, it’s a lot of preparation. Divers collect the corals and fragments knocked loose by the ship; transport them to a safe, stable underwater location where they won’t be moved around; and dig out any corals buried in debris. When NOAA is ready to reattach corals, divers clear the transplant area (sometimes that means using a special undersea vacuum). On the ocean surface, people in a boat mix cement and send it down in five-gallon buckets to the divers below. Working with nails, rebar, and cement, the divers carefully reattach the corals to the seafloor, with the cement solidifying in a couple hours.\nProtecting Coral, From the Law to the High Seas\nNearly a third of the total reported groundings in Puerto Rico and the U.S. Virgin Islands this year have involved corals listed as threatened under the Endangered Species Act. In previous years, only 10 percent of the groundings involved threatened corals. What changed this year was the Endangered Species Act listing of five additional coral species in the Caribbean.\nAnother form of protection for corals is installing buoys to mark the location of reefs in areas where ships keep grounding on them. Since these navigational aids were put in place at one vulnerable site in Culebra, Puerto Rico this summer, NOAA hasn’t been called in to an incident there yet.\nBut restoring coral reefs after a ship grounding almost wouldn’t be possible without coral nurseries. Here, NOAA is able to regrow and rehabilitate coral, a technique being used at the site of the T/V Margara grounding. Stay tuned because we’ll be going more in depth on coral nurseries, what they look like, and how they help us restore these amazingly diverse ocean habitats. [Update: Read how NOAA uses coral nurseries to restore damaged reefs.]']	['<urn:uuid:50f0bfa0-1fb9-432e-a412-19fc805820a5>', '<urn:uuid:80d403e7-bb6a-49e9-915c-5140b9c8d1a5>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T06:04:36.674696	13	92	1297
63	anti depression meds vs therapy panic attacks effectiveness	Both treatments are effective for panic disorder. CBT has been shown to be as effective as, and in some studies superior to, medications. Studies show 70-90% of patients achieve panic-free status with CBT, while 55-60% achieve this with medication. CBT has better long-term benefits and helps prevent relapse, while medications may have faster initial results but limited long-term effects and risk of relapse after discontinuation. CBT is also safer for pregnant patients and those with bipolar disorder or substance use issues, though it has higher costs and limited access to expert therapists.	"[""Approximately 6 million individuals experience panic disorder each year. Panic disorder is characterized by recurrent, intense periods of anxiety/panic, which are often unprovoked, or, “out-of-the blue,” and are accompanied by anticipatory anxiety regarding the possibility of future attacks. Panic disorder can be quite debilitating, sometimes accompanied by agoraphobia, or avoidance of pubic places due to fear of being in a setting or situation from which escape or finding help may be difficult.\nBefore we get to what DOES NOT work, let’s discuss what DOES work. Evidence for effective treatment of panic disorder is very well documented. The data show with great robust that cognitive behavioral therapy (CBT) is effective and is superior to a number of other treatment strategies.\nCognitive behavioral therapy for panic disorder often includes:\n1) Psychoeducation – Education regarding the nature and course of the disorder; the autonomic nervous system and the fight-or-flight response; conceptualization of panic as the magnification and misinterpretation of somatic experiences which result in further physiological arousal and panic symptoms; treatment rationale; and, course of treatment.\n2) Relaxation training – Learning to engage the relaxation response via training in diaphragmatic breathing; paced breathing; progressive muscle relaxation; or biofeedback.\n3) Cognitive restructuring – Identifying the individual’s cognitive structure/sequence during panic; learning to identify cognitive distortions; challenging cognitive distortions; and restructuring these thoughts to be more realistic, accurate, and helpful.\n4) Exposure – Eliciting panic symptoms and utilizing coping skills to reduce symptoms; creating a hierarchy (least anxiety-provoking to most anxiety-provoking) of avoided or distressing situations and exposing the individual to these experiences in a structured, systematic manner, allowing them to utilize their coping skills to minimize anxiety.\nThe KEY TO EFFECTIVE TREATMENT is creating mastery experiences, “I did it” experiences, in which the individual has a new emotional experience in these previously distressing situations, one of newly perceived control over anxiety.\nYou may have noticed that I have not mentioned medication yet. While this is a blog about effective psychotherapy, psychotropic medication is an effective treatment option for many psychiatric disorders.\nIn the case of panic disorder, CBT has been shown to be as effective, and, in some studies, superior to psychotropic medication. Common medications approved for the treatment of panic disorder include: selective serotonin reuptake inhibitors (SSRIs), tricyclic antidepressants (TCAs), and benzodiadepines, which brings me to what DOES NOT work… actually, more of a caution and highlight of potential for unintentional maintenance of the problem…\n(I step onto my “soap box.” ) Benzodiazepines (e.g., Xanax, Ativan, Klonopin) are frequently prescribed by physicians for patients who present with anxiety (and panic). Often, when primary care physicians or psychiatrists prescribe these medications, these patients are at my door several months later, anxious, overwhelmed, and relying on their benzodiazepine for a small amount of temporary relief. They cling to it, yet they report that it is not very helpful. Although it may “take the edge off,” the anxiety quickly returns to which they can only respond by taking another pill.\nThus, herein lies what DOES NOT work. I truly enjoy treating panic disorder; I do. It is why I became a psychologist. But, time and time again, I see patients who present as prescribed benzodiazepines who are not likely to improve until they are no longer relying on benzodiazepines. Here’s why:\n1) There is a risk of physiological dependence. Yes, that’s right; it’s habit forming.\n2) There is a risk of psychological dependence. So frequently, I see patients who are fearful of leaving benzodiazepines behind, as benzodiazepines have been their only source of potential relief, albeit minimal. Benzodiazepines can undermine CBT treatment, in this way, as it unintentionally validates their belief that they cannot control the anxiety and must rely on external forces to do so.\n3) It can create a dysfunctional treatment response. As noted above, although it may “take the edge off,” the anxiety quickly returns to which they can only respond by taking another pill, as they have learned.\n4) Most importantly, benzodiazepines can be counterproductive to CBT. The aforementioned problems notwithstanding, benzodiazepines, which enhance the action of the neurotransmitter, GABBA (Gamma Amino Butyric Acid), resulting in a “calming” or dulled excitatory response. In essence, it dulls the emotional experience, thereby reducing the potential for anxiety/panic intensity.\nAy, here’s the rub, as a cognitive behavioral therapist, I WANT you to experience anxiety (in our planned, systematic manner) in order to have mastery experiences, experiences in which you are able to learn that you can, indeed, effectively use your newly acquired coping skills to control and minimize your anxiety. If your ability to experience anxiety is blunted by benzodiazepines, you may be less likely to benefit from CBT and may have a higher potential for relapse.\nI’ll step down off my “soap box” now.\nWhat are your experiences?\nLead photo available at 123rf\nThis post currently has\nYou can read the comments or leave your own thoughts.\nFrom Psych Central's World of Psychology:\nBest of Our Blogs: January 15, 2013 | World of Psychology (January 15, 2013)\nFrom Psych Central's website:\nPanic Disorder: Therapy That DOESN’T Work – PsychCentral.com (blog) (January 15, 2013)\nBest of Our Blogs: January 15, 2013 | healthhat.com (January 15, 2013)\nWhy Benzodiazepines May Not Work for Driving Anxiety and Panic (January 16, 2013)\nBipolar Schools | Is Bipolar Hereditary (January 17, 2013)\nWorry Condition: Therapy That DOESN'T Operate | Panic Treatment Info (January 18, 2013)\nPanic Attack Treatment » Panic Disorder: Therapy That DOESN’T Work (January 18, 2013)\nPanic Disorder: Therapy That DOESN’T Work - Counseling Wise (January 27, 2013)\nERP Therapy, Drugs, and Anxiety « ocdtalk (February 10, 2013)\nLatest Symptoms For Panic Attacks News (February 24, 2013)\nLast reviewed: 15 Jan 2013"", 'Treatment Planning for Panic Disorder\nTreatment Planning for Panic Disorder\nPanic disorder with or without agoraphobia is a chronic, debilitating psychiatric illness that affects about 4.7% of the general US population.1 Kessler and colleagues2 report that close to one third of the general population has met criteria for panic disorder within the past year.2 The mean age at onset is in one\'s 20s, and women are twice as likely as men to present with panic disorder.3\nPanic disorder is associated with poor quality of life4,5 and with substantial and moderately severe functional impairment in 45% and 30% of persons, respectively.2 Many patients have at least one other psychiatric diagnosis, most commonly substance use disorder, mood disorder, or another anxiety disorder.3 Panic disorder is associated with a 2-fold increased risk of coronary heart disease6 and frequent use of emergency and medical services.7,8\nDespite its chronic nature and severity of illness, many patients do not receive therapy that meets standard treatment guidelines.3 In the primary care setting, only 22% of patients with panic disorder receive adequate medication and only12% receive adequate psychotherapy.8 The purpose of this brief report is to increase the awareness of evidence-based treatments for panic disorder. Evaluation and differential diagnosis\nPatients should be thoroughly evaluated to eliminate any conditions that may mimic symptoms of panic disorder, including medical illnesses (eg, cardiac, respiratory, or thyroid diseases) and substance use (eg, marijuana, cocaine, methamphetamines). One of the core symptoms of panic disorder is recurrent, unexpected panic attacks. A full-symptom panic attack requires the abrupt onset of at least 4 physical symptoms (eg, palpitations or dizziness) or cognitive symptoms (eg, fear of going crazy or losing control) that reach a peak within 10 minutes. Another core feature is anticipatory anxiety about future panic attacks or the implications of these attacks. This often leads to agoraphobia in which the person avoids situations where escape or help is not readily available if a panic attack occurs. Thus, expected, or cued, panic attacks can be triggered by the anticipation or presence of a phobic situation.\nPanic attacks and phobic avoidance are also present in other anxiety disorders, and examining the focus of the patient\'s fear helps in the differential diagnosis. According to the cognitive model of panic disorder, panic attacks are the result of catastrophic misappraisals of bodily sensations.9 The focus of fear is the perceived harmful consequence of panicking (eg, losing control, going crazy, and so forth) and the associated physical sensations (eg, having a heart attack or stroke). In other anxiety disorders, the fear is not of the panic attacks themselves. In generalized anxiety disorder, for example, panic attacks are the exacerbation of persistent worries and anxiety about daily life events and the anticipation of future bad events. Panic attacks in social anxiety disorder are limited to the anticipation or presence of social or performance situations caused by fear of embarrassment or humiliation. Panic attacks in posttraumatic stress disorder are triggered by reminders of the traumatic event. In specific phobia, the focus of fear is often the perceived danger of the object or situation.10 Acute treatment\nCognitive-behavioral therapy (CBT) and pharmacotherapy are among the empirically supported treatments for panic disorder. Defining treatment response and remission\nTreatment response is usually defined by a ""panic-free"" status that is the absence of full-symptom panic attacks. Other measures include clinician\'s ratings of overall improvement and severity on the Clinical Global Impression (CGI) scale11 and changes in panic symptoms in the Panic Disorder Severity Scale (PDSS).12 Remission has been defined in one of several ways. The international consensus group considers remission as the complete resolution of symptoms for at least 3 months.13 However, most pharmacotherapy studies have defined remission by using a combination of panic-free status and improved CGI and/or PDSS scores. CBT trials have focused on rates of ""clinically significant improvement,"" such as achieving high end-state functioning14 or meeting the criteria developed by Jacobson and colleagues.15 Cognitive-behavioral therapy\nTreatment components of CBT include education, breathing retraining, cognitive restructuring, interoceptive exposure, and in vivoexposure.16,17 Education helps establish treatment alliance and teach key concepts, including the course of panic disorder, the nature of panic attacks, behaviors that maintain the panic cycle, the cognitive model, and the treatment rationale. Breathing retraining was initially thought to reduce physical symptoms of panic, but it was later conceptualized as a way to demonstrate how hyperventilation can exacerbate physical symptoms.18\nThe goal of cognitive restructuring is to identify and challenge anxious thoughts regarding a panic attack. In interoceptive exposure, patients perform various exercises (eg, hyperventilation, spinning in a chair) in a controlled setting in order to repeatedly confront anxiety-provoking physical symptoms. With exposure, patients learn that symptoms are not as dangerous as originally perceived. In vivo exposure traditionally has been used in the treatment of agoraphobia. During in vivo exposure, patients confront the actual phobic situation, such as traveling alone or being in an airplane or elevator. These treatment components are based on 2 CBT approaches to panic disorder: panic control treatment (PCT) developed by Barlow and colleagues,16 which introduced interoceptive exposure as a key element; and the cognitive therapy (CT) program developed by Clark,9 which emphasized cognitive restructuring.\nIn the first controlled trial of PCT (N = 56), 3 active treatments—PCT, muscle relaxation, and PCT plus muscle relaxation—were compared with wait-list controls.16 PCT outperformed the other treatment conditions. Approximately 87% of patients in the PCT groups, 60% of patients who received muscle relaxation, and 36% of controls were panic-free at the end of their respective treatments. There was a greater distinction when patients who had dropped out were included in the analysis: 74% to 79% of PCT patients, 40% of patients who received muscle relaxation, and 33% of controls were panic-free. However, only 46% of patients who were treated with PCT achieved high-end functioning, despite the high panic-free status associated with this group. This suggests that many patients were still symptomatic at the end of treatment.\nA controlled study of 2 versions of CT (N = 42) showed favorable results. A full 12-session and a brief 5-session course of CT were compared with wait-list controls.19 A greater proportion of patients who received CT had panic-free status relative to controls; 79% in the 12-session CT group, 71% in the 5-session CT group, and 8% in the control group had panic-free status. Thus, the investigators concluded that brief intervention was as effective as full 12-session treatment.\nOther randomized controlled studies have confirmed the efficacy of CBT for panic disorder, with findings of panic-free status achieved in approximately 70% to 90% of patients.17,20-22 Rates of clinically significant improvement were found to be 38% to 79%, depending on the criteria used.23-25\nDismantling studies have attempted to determine which treatment component is the ""active"" ingredient or the most effective intervention. With the exception of breathing retraining, which was not shown to be helpful and may possibly increase the risk of relapse,24 the other 3 main interventions—cognitive restructuring, interoceptive exposure, and in vivoexposure—have been shown to be equally effective.23,26-28 Pharmacotherapy\nSeveral classes of psychotropics have been shown to be effective and have comparable efficacy in the treatment of panic disorder, including tricyclic antidepressants (TCAs), benzodiazepines, and SSRIs.29-34 SSRIs are now considered first-line pharmacotherapy because of a more favorable adverse-effect profile than with TCAs30 and less concern for withdrawal and dependence than with benzodiazepines.35 Based on clinical practice, SSRIs are generally started at one half to one third of the typical starting dose for depression to limit the adverse effects of jitteriness and anxiety.36 Benzodiazepines are better tolerated than antidepressants and may help with adverse effects.33Table 1 presents common adverse effects and suggested dosages.\nOn average, 55% to 60% of patients treated with pharmacotherapy achieve panic-free status.30,33 Efficacy data are most extensive for imipramine, fluvoxamine, paroxetine, and alprazolam. There is relatively less controlled data for monoamine oxidase inhibitors, the newer SSRI escitalopram, and the serotonin-norepinephrine reuptake inhibitor venlafaxine. Studies have shown either minimal or modest efficacy for the newer drugs.37,38\nA recent placebo-controlled trial compared the efficacy of venlafaxine extended release (ER) with that of paroxetine.39 Venlafaxine ER and paroxetine achieved a greater panic-free status compared with placebo (70%, 58%, and 48%, respectively). Venlafaxine ER also resulted in greater improvement than paroxetine in the PDSS scale. Based on CGI-improvement responders, active treatments resulted in a response rate of 80% to 85%, compared with a placebo response of 60%.\nIn addition to monotherapy, there is some evidence that combining an antidepressant with a benzodiazepine may facilitate early treatment response. For example, one study randomized patients (N = 50) to clonazepam or placebo during the initial 4 weeks of treatment with sertraline.40 Compared with sertraline alone, combination treatment resulted in a greater reduction of panic symptoms. Treatment gains were maintained even after clonazepam was discontinued.\nRemission rates in pharmacotherapy trials are infrequently reported. A retrospective analysis of paroxetine studies found a rate of 24.6% to 35.5%, depending on the definition of remission.41 Remission was achieved in 50% of patients treated with venlafaxine ER based on CGI severity score.39 These rates are probably an overestimation of true remission, because the more stringent definition proposed by the international consensus group was not used. CBT, medication, or combination?\nA large (N = 312) randomized, placebo-controlled, head-to-head comparison trial examined the efficacy of monotherapy (CBT alone or imipramine alone) with combination treatment (CBT and imipramine).42 All active treatments were superior to placebo during the acute treatment phase, but imipramine produced a higher quality of response than CBT. During maintenance, combination treatment fared better than monotherapy. However, imipramine appeared to decrease the long-term efficacy of CBT.\nOnce treatment was discontinued, patients treated with medication (alone or with CBT) were more vulnerable to relapse than those treated with CBT alone. One possible reason may be that medication was discontinued too rapidly (over 1 to 2 weeks). Based on this study, all 3 treatment modalities are effective for acute stabilization. Medication is helpful if a more potent response is desired, but the long-term effects of medication are limited. Discontinuation of medication over a longer period may reduce the chance of relapse. CBT alone is better tolerated and has a more lasting effect.\nIn addition to treatment efficacy, other factors should be considered when recommending treatment, including patient preference, treatment history, severity of illness, and presence of comorbid disorders. Table 2 summarizes the advantages and disadvantages of the different treatment modalities.\nAdvantages and disadvantages of different treatments for panic disorder\n|Antidepressants (SSRIs)||Low cost (generic form); treatment of comorbid depression and anxiety disorders; single daily dosing available||Delayed onset of action; medication adverse effects|\n|Benzodiazepines||Rapid onset of action; well-tolerated; low cost||Medication adverse effects; withdrawal and dependence; may interfere with exposure; multiple daily dosing|\n|Cognitive-behavioral therapy (CBT)||Long-term benefits, prevention of relapse; safe in pregnancy; safe in those with history of bipolar disorder and substance use||High cost; time commitment; limited access to expert therapists|\n|Combination medication and CBT||Rapid onset of action; treatment of comorbid disorders||Highest cost; time commitment; limited access to expert therapists; medication adverse effects; no additional long-term benefits|']"	['<urn:uuid:3cfc45d2-e255-4682-aebc-fb5718e873ad>', '<urn:uuid:e5e92be6-4eb8-4f41-a7a5-12a33261c9d4>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T06:04:36.674696	8	92	2771
64	Why do they store propane underground?	Large volumes of HGL (including propane) are primarily stored as pressurized liquid in underground caverns, usually in salt formations. This storage is necessary because production volumes may exceed transportation capacity and because production doesn't always match seasonal demand - propane production is consistent year-round but demand is higher in fall and winter.	['Hydrocarbon gas liquids are transported by various means\nHydrocarbon gas liquids (HGL) that are extracted from natural gas or that are produced at petroleum refineries may be transported as liquids in mixtures of HGL or as separate HGL purity products in pipelines, rail cars, trucks, ships, and barges.\nHGL are transported in five main forms:\n- Y-grade (raw, unseparated HGL)\n- E-P mix (most frequently 80% ethane and 20% propane)\n- P/P mix (refinery-grade propane-propylene mixture)\n- LPG (mixture of liquefied propane, normal butane, and isobutane)\n- Purity products (separate, distinct products; mostly ethane, propane, and normal butane)\nPipelines carry hydrocarbon gas liquids from where they are produced to where they are stored\nMost of the HGL produced in the United States are transported in pipelines from where they are produced to places where they are used or to places where they are stored for distribution.\nMost of HGL transported by pipeline is Y-grade quality and is transported to fractionation plants. Purity ethane and E-P mix is transported by pipelines from fractionation plants to ethylene crackers, where they are used to produce ethylene and other olefins. P/P mix is often transported by dedicated pipelines on the Gulf Coast, or by rail in other regions, from refineries to propylene splitters that separate refinery-grade propylene (a propylene mix containing impurities) into higher quality, polymer-grade or chemical-grade propylene, which is then sold to petrochemical plants. Liquefied petroleum gases (LPG)1 (propane, normal butane, and isobutane) are transported by dedicated high vapor pressure (HVP) pipelines or in batches as purity products in pipelines that transport other kinds of petroleum products.\nRailroads and trucks transport HGL to consumers\nMany regions in the United States (such as the West, New England, and Florida) are not served by HGL pipelines. In these areas, railroads often transport large volumes of HGL to wholesale and bulk purchasers in pressurized railroad tankcars. Railroads and trucks are also used to transport HGL to consumers. The primary HGL product delivered to consumers is consumer-grade propane, which is transported by truck in pressurized tanks to homes, farms, and businesses where it can be used as engine fuel, for crop drying, for space heating and water heating, and for cooking, among other applications.\nSpecial ships are used to transport HGL to and from the United States\nSpecial ships are used to transport HGL (usually LPG) to and from shipping ports in the United States. The ships, called gas tankers, vary in size and vary by the method used to keep the HGL in liquid form. The HGL may be pressurized, refrigerated, or both. Over short distances, propane and normal butane are also moved by barge along intercoastal waterways and navigable rivers.\nA propane delivery truck\nSource: Stock photography (copyrighted)\nAn ocean-going tanker ship transporting liquefied petroleum gas\nSource: Dorian LPG Ltd. (copyrighted)\nHGL are stored in a variety of ways\nStorage of HGL is necessary because HGL may be produced in volumes that exceed the capacity of the different modes of transportation used to carry the HGL to consumers. Production may also not match HGL seasonal requirements. For example, production of propane is relatively consistent throughout the year, but demand for propane is usually lower in the summer and higher in the fall and winter. Propane is stored when demand is low, and it is withdrawn from storage when demand is high.\nPropane storage tanks of various sizes\nSource: Stock photography (copyrighted)\nLarge volumes of HGL are primarily stored as a pressurized liquid in underground caverns. Most of the caverns are in salt formations, but some propane storage caverns are mined out of shale, granite, or limestone rock. In regions where geology is not well suited for underground caverns, large aboveground tanks may be used. Aboveground tanks are the primary storage method for propane and butanes in New England.\nOnce HGL are transported close to consumers, they are stored in pressurized (or sometimes refrigerated) tanks located above or below ground. LPG is stored and distributed in many different sizes of tanks, from the small canisters used for torches and camping stoves to 90,000-gallon bullet-shaped tanks used at industrial facilities.\n1Liquefied petroleum gas (LPG) is a generic industry term that is used to refer to propane, butanes, or mixtures of those HGL. However, these products are rarely distributed as mixtures. Virtually all HGL product distributed in the U.S. consumer market with the LPG label is HD-5 propane. Propane constitutes most of U.S. marine HGL/LPG imports and exports.\nLast reviewed: October 31, 2019']	['<urn:uuid:e96b9eef-d762-4b8a-b70c-cb730445c368>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:04:36.674696	6	52	746
65	How did the Cracker Line Operation help besieged Union forces?	The Cracker Line Operation, initiated by Generals Thomas and Grant on October 26, opened a road from Brown's Ferry to Chattanooga and secured the Kelley's Ferry Road. This created a supply line that allowed Union forces to receive supplies, weapons, ammunition, and reinforcements.	['Submitted by C.J. Johnson\nDuring the four years of the War Between the States, there were rare nighttime battles. In October 1863, in Marion and Hamilton Counties, Tennessee and Dade County, Georgia, perhaps the best known overnight battle occurred in a valley illuminated by moonlight. The action was supposed to begin at 10 p.m., but didn’t start until midnight. Better known as the Battle of Wauhatchie Station or Brown’s Ferry, this engagement ensued as the Rebels attempted to prevent Federal supplies from reaching Chattanooga.\nFollowing the Confederate victory at Chickamauga in north Georgia in mid-September, Confederates set up their positions on the eastside of Chattanooga along Missionary Ridge and Lookout Mountain, while Union troops retreated into Chattanooga. With the Rebels to the east and with control of the Tennessee River, Union supplies going into the city had to cross the mountains to the west. The route was subject to Rebel attack and was a difficult trip as well.\nThe National Park Service describes the battle. “In an effort to relieve Union forces besieged in Chattanooga, Maj. Gen. George H. Thomas and Maj. Gen. Ulysses S. Grant initiated the “Cracker Line Operation” on October 26. This operation required the opening of the road to Chattanooga from Brown’s Ferry on the Tennessee River with a simultaneous advance up Lookout Valley, securing the Kelley’s Ferry Road.\nUnion Chief Engineer, Military Division of the Mississippi, Brig. Gen. William F. “Baldy” Smith, with Brig. Gen. John B. Turchin’s and Brig. Gen. William B. Hazen’s…brigades…, was assigned the task of establishing the Brown’s Ferry bridgehead. Meanwhile, Maj. Gen. Joseph Hooker…marched from Bridgeport through Lookout Valley towards Brown’s Ferry from the south. At 3:00 am, on October 27, portions of Hazen’s brigade embarked upon pontoons and floated around Moccasin Bend to Brown’s Ferry. Turchin’s brigade took a position on Moccasin Bend across from Brown’s Ferry.\nUpon landing, Hazen secured the bridgehead and then positioned a pontoon bridge across the river, allowing Turchin to cross and take position on his right. Hooker, while his force passed through Lookout Valley on October 28, detached Brig. Gen. John W. Geary’s division at Wauhatchie Station, a stop on the Nashville & Chattanooga Railroad, to protect the line of communications to the south as well as the road west to Kelley’s Ferry.\nObserving the Union movements on the 27th and 28th, Confederate Lt. Gen. James Longstreet and Gen. Braxton Bragg decided to mount a night attack on Wauhatchie Station…. Surprised by the attack, Geary’s division, at Wauhatchie Station, formed into a V-shaped battle line.\nHearing the din of battle, Hooker…sent Maj. Gen. Oliver Otis Howard with two XI Army Corps divisions to Wauhatchie Station as reinforcements. As more…Union troops arrived, the Confederates fell back to Lookout Mountain. The Federals now had their window to the outside and could receive supplies, weapons, ammunition, and reinforcements via the Cracker Line.”\nThe Georgia Blue and Gray Trail explains the reference to the Mule Brigade. During the battle, Confederate commander “John Bratton thought he has a good position, forming an open V to attack Geary’s obverse line. Some of Bratton’s men crossed Kelley’s Ferry Road and struck the lightly guarded [Union] wagon train that was the original object of the attack. Rebels quickly drove off the Federal soldiers, but the commotion stampeded some mules giving the battle a somewhat derogatory nickname, ‘Charge of the Mule Brigade.’ The mules delayed the Rebels from forming a line and a federal counterattack drove the Confederates off.”']	['<urn:uuid:7ad443db-8148-4709-9212-d5664c8250f1>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T06:04:36.674696	10	43	577
66	data management social media crisis vs bird monitoring comparison	In social media crisis management, data is managed through monitoring tools that track mentions and sentiment in real-time, focusing on immediate response. Bird monitoring data management systems are more comprehensive, maintaining long-term population data with associated metadata and summary data, integrated across spatial scales for conservation and management purposes.	"['Knowing how to maintain an online reputation is an essential component of healthcare marketing. In this blog post, I will show you how to put an effective crisis response strategy in place for your healthcare brand.\nHaving an online presence has so many advantages when it comes to healthcare marketing, but it also comes with some risks. With the click of a mouse, patients can share their experiences online – good and bad – and their comments travel at lightening-speed through their social network. A social media crisis can escalate rapidly and you must be ready to step in and remedy the situation without delay. The only way to do this is to have a crisis plan already in place.\nCrisis management involves dealing with threats before, during, and after they have occurred. Let’s look at these three stages in more detail.\nStage 1 Preparation\nProactively prepare by developing a crisis response plan. The following elements are involved.\n#1 Crisis Definition\nFirst, define what constitutes a crisis. Three elements are common to a crisis (a) a threat to the organization, (b) the element of surprise, and (c) a short decision time.\nA crisis can fall into several categories including:\n(a) Technological (eg; your website has been hacked);\n(b) Confrontation (disgruntled employee, client, or patient attacks you online);\n(c) Rumours (eg; spreading false information about you, your product or service online);\n(d) Malevolence (eg; In 1982, a murderer added cyanide to some Tylenol capsules on store shelves, killing seven people).\n#2 Monitor Online Chatter\nAn effective social media strategy requires active listening to the online chatter about your healthcare organization. Should a crisis occur, listening to the conversation will help you shape a more insightful and effective response. Responding in real time to issues strengthens public perception that your focus is firmly on patient satisfaction. In addition, use monitoring to find the healthcare conversations you can add value to. Investing in community building online now will pay dividends in the form of support should a crisis hit you.\nThere are many free and paid monitoring tools available to you. These tools vary in scope and range across a number of sites, real-time or delayed searching, the sophistication of analytics, the flexibility of data presentation, integration with other applications, and of course, price. When it comes to reputation management, choose a tool that does more than just track mentions of your name. You need to be able to evaluate the sentiment (the ratio of mentions that are positive to those that are negative) attached to the mentions. Mention is a freemium monitoring tool that includes sentiment. Tweets that include words like “not working,” “fail” or “poor experience” should be resolved immediately.\n#3 Create a Written Plan\nYour written plan should include the following:\n- Clear guidelines on how to respond to each of the different situations outlined above in #1.\n- Links to your terms of service.\n- Who should respond – establish a clear chain of command and list contact information.\n- Make sure every member of your team knows this plan is in place, how to access it, and how to put the plan into action.\nStage 2: Action\nNow’s the time to put your carefully crafted crisis plan into place. The following are key considerations:\n- Determine the exact nature of the crisis. How and where did it originate? How is it affecting your patients or clients?\n- Go to the source. Find where the complaint originated and with whom. Determine their sphere of influence. If a blogger has published something that is untrue or misrepresentative of you, ask them to remove, amend, or modify the piece if this is appropriate.\n- Be respectful, polite and engaged. Never get into a public argument or talk down to anyone.\n- Be as transparent as possible as quickly as possible. Acknowledge that you are aware of the situation and that you are dealing with it straight away.\n- Respond swiftly and appropriately. Every moment counts on social media. The longer you wait, the more the conversation will heat up. Twitter, in particular, is a place where people expect a quick response no matter what time of day.\n- Don’t lie or try to hide the truth; admit when the fault is yours.\n- Use the same channel on which you were criticized to respond.\n- Don’t censor or remove the critical comments that appear on your social media platforms. Tempting as this may appear, it will only fan the flames of the social media fire.\n- Channel communication to your own website. Develop an area on your website or blog that houses the information about the crisis and what your organization is doing about it.\n- Communicate your story. A story gets out of control when you haven’t told your side and people begin to speculate. While you can’t control the story, you can provide the facts, information, and access to key people that allow journalists and bloggers to help you frame it in the right way.\nStage 3: Review\nWhen the crisis has passed, go over what happened. Ask yourself the following questions:\n- How well did you handle the situation?\n- Did it escalate to a bigger problem than it was?\n- What could you have done differently?\n- Prepare to deliver on your word. Make changes based on feedback if those changes are warranted and if you have promised to put them in place.\nIf handled well a crisis may even turn out to be an opportunity to show your commitment to your patients and consumers. Remember the Tylenol example above? Johnson & Johnson recalled and destroyed 31 million capsules at a cost of $100 million. The CEO appeared in television ads and at news conferences informing consumers of the company’s actions. Tamper-resistant packaging was quickly introduced, and Tylenol sales bounced back to near pre-crisis levels.\nWhile you can’t control everything that happens on social media, you can control your response. The best way to handle a crisis is to have your response plan in place. If you haven’t already made one, then do it today.\n- Don’t Be Scared, Be Prepared – How to Manage a Social Media Crisis\n- 3 Steps for Communicators to Implement a Crisis Ready Culture\n- The Five Social Media Emergencies Your Next Client Could Have\n- In the Trenches with Crisis Comms: 10 Things to Prepare\n- Crisis Communication and Social Media in Healthcare\n- 5 Steps to Planning for Effective PR Crisis Management\n- 7 Tips for Taking the Stress Out of a Social Media Crisis\n- How to Use Social Media Effectively in a Crisis\n- The Secret Behind Crisis Communications', ""Research and Monitoring Resources\nBird monitoring is a strategic activity that can be used to assess conservation status, ascertain and predict immediate or cumulative effects of habitat change, establish management and conservation priorities, and determine the effects of management so it can be adapted to meet its objectives. On the other hand, if it is ill-conceived, monitoring can waste funds, equipment, and personnel time. The DoD Partners in Flight Monitoring Working Group, like other agencies and organizations, is working to make monitoring more effective. Here is some current information on monitoring activities in the bird conservation community.\nU.S. NABCI Monitoring Subcommittee\nThe Monitoring Subcommittee's Opportunities for Improving Avian Monitoring report provides a number of recommendations for general consideration by the bird conservation community and a set of specific actions for implementation by the U.S. North American Bird Conservation Initiative (U.S. NABCI) Committee and Monitoring Subcommittee; all recommendations and actions address current challenges to our ability to achieve four monitoring goals:\nFully integrate monitoring into bird management and conservation practices and ensure that monitoring is aligned with management and conservation priorities.\nCoordinate monitoring programs among organizations and integrate them across spatial scales to solve conservation or management problems effectively.\nIncrease the value of monitoring information by improving statistical design.\nMaintain bird population monitoring data in modern data management systems. Recognizing legal, institutional, proprietary, and other constraints, provide greater availability of raw data, associated metadata, and summary data for bird monitoring programs.\nPIF Research and Monitoring Working Group resources\nHigh Priority Needs for Range-wide Monitoring of North American Landbirds\nCompanion document to PIF North American Landbird Conservation Plan, indicating the monitoring programs that would best address the needs of each species, with a downloadable table for easy sorting.\nPIF Continental Watch List Species Research & Monitoring Needs\nSpecies accounts for the 100 Continental WatchList species, with priority research and monitoring needs extracted from PIF regional plans and other sources.\nPIF Research & Monitoring Needs Database Search Form\nSearchable database with research and monitoring needs extracted from PIF regional plans.\nMonitoring Avian Productivity and Survivorship (MAPS)\nThe Institute for Bird Populations's Monitoring Avian Productivity and Survivorship (MAPS) Program comprises a continent-wide network of hundreds of constant-effort mist netting stations that assess and monitor the vital rates and population dynamics of North American landbirds and inform bird conservation efforts. Analyses of the resulting banding data provide critical information relating to the ecology, conservation, and management of North American landbird populations, and the factors responsible for changes in their populations. MAPS demographic monitoring techniques are specifically recommended for monitoring the effectiveness of avian conservation efforts in the publication Opportunities for Improving Avian Monitoring released by the U.S. NABCI Monitoring Subcommittee. IBP winter monitoring projects include the MoSI (Monitoreo de Sobrevivencia Invernal) program across the northern Neotropics and Caribbean and the MAWS (Monitoring Avian Winter Survival) program in temperate North America.\nSince 1994, IBP has partnered with the DoD Legacy Resource Management Office and DoD natural resource managers to:\n- Monitor the demographics of landbird populations of concern,\n- Model demographics as functions of landscape pattern and change,\n- Quantify demographic effects of weather/climate variation,\n- Provide management guidelines to maintain source habitat, and\n- Monitor the efficacy of pro-conservation management actions.\nAs part of this work, IBP has a special section on their website devoted to Monitoring, Modeling, and Managing Landbird Populations on Department of Defense Lands. Reports on all IBP Legacy-funded projects, and additional publications derived from DoD data, can be found here.""]"	['<urn:uuid:03c69d2d-f6ba-4770-8c63-4c267d49ae49>', '<urn:uuid:e8a33784-9996-42fb-bba9-2cbf02761468>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-13T06:04:36.674696	9	49	1691
67	How far back in time can we trace the first signs of human beings developing the ability to use complex vocal communication?	According to a groundbreaking study from 2017, the first anatomical evidence for increased vocal capability appeared roughly 4.5 million years ago. This was discovered by comparing skulls of chimpanzees with those of A. ramidus, an extinct bipedal hominin from the early Pliocene epoch. Researchers identified the emergence of anatomical features associated with vocal ability. Interestingly, this anatomical change has been correlated with decreased aggression in the species, which would have enabled more social interaction and the development of communication.	['By Jesse Shanahan\nDespite Earth being replete with biological and cognitive diversity, human language remains an entirely singular communicative system; no other living creature has an analogous method of self-expression. The unparalleled power of language to represent thought is the foundation for interpersonal relationships, culture, and is an inextricable component of all societies. The emergence of language is one of the most remarkable evolutionary events in the history of our planet and yet, although it is a universal part of human life and even intrinsic to human identity, its origin and exact nature remain a mystery.\nThe problem is that language is necessarily complex and weaves together human cognition, neurology, biology, behaviour, and psychology in a way that inhibits comprehensive or collaborative studies. For every aspect of human life that language enables, another theory of its origin and evolution exists as well. In a sense, the way language is studied depends entirely on the perspective of the person analysing it. A biologist, for example, will characterise language differently than a psychologist and will use different terminology to do so. Although seemingly contradictory, each of these views holds a piece of the puzzling origin of language: a question that remains one of the deepest unanswered mysteries concerning what it truly means to be human.\nMuch like astronomers, evolutionary linguists cannot directly interact with their object of study and must extrapolate backwards using data gathered from present day languages. This is possible because languages follow evolutionary patterns in terms of how sounds develop or words change. However, if the goal is to study the origin of language as a whole, then what was the first language? Even more so, how can we identify the first language if we cannot first define what a language even is? Surprisingly, constructing an exhaustive definition has proven to be an immensely difficult task that is further complicated by language’s numerous features, each of which interacts differently with human cognition, has different genetic roots, and likely has a different evolutionary path. Given just how interdisciplinary language is, settling on an accepted definition is further impeded by the contradicting terminology, distinct methodology, and differing norms in those various scientific disciplines.\nBroadly speaking, linguists divide language into the following six categories: phonetics, morphology, phonology, syntax, semantics, and pragmatics. Phonetics and phonology are concerned with the speech sounds or signs of a language and how discrete units of sound (phonemes) are organised into meaningful words. Morphology studies the formation of words from meaningful units (morphemes), and syntax refers to how sentences are constructed from those words. More abstractly, semantics analyses the nature of meaning while pragmatics observes the relationship between context and meaning.\nThese divisions are solely linguistic, however. Other branches of science divide the study of language differently. For example, a more biological approach might distinguish between the genetic foundation of language, the anatomy of the human vocal tract, and the neurological features of the brain that allow for the production of speech and interpretation of meaning. Still, the linguistic model enables us to subdivide language into parts like sentences, words, morphemes, and phonemes, and then compare how those parts appear in different languages. Ideally, the characteristics that are common to all languages would then form a definition of language. A version of this approach was popularised by Noam Chomsky, an American linguist and philosopher, who argued that there are universal elements to all languages and that human beings will always develop languages with those elements.\nIn Chomsky’s theory, ‘Universal Grammar,’ these ubiquitous traits of language are hardwired into human beings via genetics, explaining why language is a uniquely human attribute. Although this is an attractive idea, the theory remains an ongoing source of disagreement among linguists. The sheer degree of linguistic diversity appears to undermine Chomsky’s claim that all human brains have the same set of structural rules for language, and languages have been discovered that seemingly lack those structure rules. More broadly, some scientists have argued that, because Universal Grammar is defined based the observed features of human language, it cannot be proven wrong. Instead, they argue that a truly scientific theory should make predictions, which can then be supported or refuted with evidence. Furthermore, some evolutionary biologists have also objected to Chomsky’s theory because, despite the claims of a genetic origin to human language, it conflicts with modern principles of Darwinian evolution. For example, how could language require a universal biological foundation when it evolves at significantly faster speeds? Simply put, how could a genetically coded grammar ever keep pace with a constantly evolving language?\nEven if Chomsky’s theory has been heavily criticised and largely discredited, it is the perfect example of a common linguistic strategy in defining language. By comparing diverse human languages, it is not only possible to identify the so-called ‘linguistic universals’ that define language but also to deduce probable features of the very first proto-language. Another similar approach to defining language is to compare human languages with the numerous methods of communication in the animal world. Complexity is hardly an attribute reserved solely for human language, and many species have robust, detailed systems for transmitting meaning. However, linguists reserve the term ‘language’ only for systems that give users both an unlimited array of meaningful, arbitrary expressions and the ability to innovate brand new ones. As far as we know, no animal method of communication has these features. An easy example of this distinction can be illustrated with what has been ironically named ‘body language’. Whether a person crosses their arms or gesticulates wildly is certainly meaningful, but it hardly constitutes a language. Similar analogies can be used with facial expressions, training commands taught to dogs, and musical humming. In contrast, gestural or signed languages provide the user with an infinite variety of phrases and the ability to craft new terms as the language evolves. Whereas both body language and gestural languages have meaning of one type or other, only the signed languages have the complexity, scope, and creativity required of a full language.\nAmerican linguistic anthropologist Charles Hockett described this distinction in terms of a set of attributes that he called ‘design features’. Like Chomsky, he attempted to define a list of the universal characteristics of all human languages. Notable design features include semanticity, which is the fact that certain combinations of sounds are connected to specific meanings, and arbitrariness, or the lack of connection between a word and its meaning. Hockett’s design features also list discreteness, or the ability of language to be broken into distinct parts, and displacement: the ability of human speakers to reference things that do not exist, e.g. the past or the future. Similarly, Hockett also required that language is productive; it enables speakers to create an unlimited number of new expressions. Although not every linguist agrees with Hockett’s requirements, his set of design features provides an excellent framework to understand the distinction between communication and language.\nFor example, ornithologists have discovered that bird communication utilises both semanticity and arbitrariness. Bird calls and songs also have discreteness, as they can be divided into individual units of meaning which can then be arranged using a kind of syntax. Dialect variations, mimicry, and even limited acquisition of human language has been observed, though debate still remains over whether the birds truly learned some human language or are simply emulating human behaviour. Still, the communication systems of birds fail Hockett’s productivity requirement, as they have a limited list of musical expressions and cannot innovate new ones. Birds also cannot reference nonexistent things (displacement), and they cannot utilize their calls to reference abstract concepts. Similarly, there are numerous other animal systems of communication, like those of bees or chimpanzees, that seem to parallel human language but invariably lack some critical trait when analysed using Hockett’s design features. Even if other linguists dispute some of Hockett’s framework, it aptly illustrates the complexity, breadth, and uniqueness of human language in addition to the difficulty of defining it.\nWhile comparative analyses or guidelines like Hockett’s design features bring us closer to an understanding of what language is, numerous ontological questions remain. We cannot even say whether language initially developed in songs, gestures, or speech, or when exactly proto-humans began to utilise complex systems of communication. A groundbreaking study published in 2017 discovered that the first anatomical evidence for increased vocal capability occurred roughly 4.5 million years ago. By comparing the skulls of various chimpanzees and those of A. ramidus, an extinct bipedal hominin from the early Pliocene epoch, researchers identified the emergence of anatomical features associated with vocal ability. Not only does this potentially suggest that language could have developed significantly earlier than previously thought, but this change in anatomy has also been correlated with decreased aggression in the species, which would further enable social interaction and the development of communication.\nSimilarly, a study at Yale University in June 2018 revealed a relationship between the number of phonemes in a given language and the expression of a gene associated with dyslexia. Another study conducted in 2012 discovered that humans developed a specific biological adaptation that enables us to keep up with the quick pace of language evolution, and for humans that evolved in one geographical region to quickly and adeptly learn languages that developed in another. Additionally, social groups, familial relationships, the increased use of tools, and the desire for self-expression have also all been suggested as potential origins of language.\nInterestingly, Hockett’s design features apply to this problem as well. By looking at the uses language has, we can identify even more possible motivations for its emergence. For example, the productivity of language suggests human beings developed a creative impulse in addition to a desire for self-expression. Likewise, the abstraction feature of language has a connection with our ‘theory of mind’: the ability to conceptualise our own thoughts, beliefs, intents, desires, etc., and ascribe similar ones to other people. Whether language caused those characteristics to develop in human beings or whether our own needs drove its design is still a matter of contentious debate. Thus, we often cannot even distinguish between a cause of language development or an effect of its evolution in our species.\nStill, these setbacks are understandable. Modern language is incredibly multifaceted; in the same way that language is entwined with our anatomy, cognition, psychology, and society, the first languages likely emerged due to a complex combination of influences as well. The strength of the relationship between language and the human experience makes disentangling its effects, causes, evolution, and origin even more difficult. When, if ever, we are finally able to understand the birth and evolution of language, we will have found an answer to one of the hardest unsolved problems in science.\nAnd yet, this explanation has an unsettling implication. Given that no other species on Earth has developed a system with the complexity, scope, and capability of human communication, achieving all of the requirements for language might be an incredibly rare event. Just as the uniqueness of biological life on Earth drives us to wonder if we are isolated in the universe, the singular existence of language in human beings raises the question: even if we aren’t alone, will we ever find another species who can truly understand us?']	['<urn:uuid:efdeda06-6917-4d77-b92e-e376cde268e7>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	22	79	1871
68	Why do cannabis businesses need to be particularly careful about using personal vehicles for transporting their products, and what kind of auto insurance do they need?	Employees should not use their personal vehicles to transport cannabis products because personal insurance will not cover cannabis transportation. Instead, businesses need specific commercial auto coverage, as many traditional auto insurers refuse coverage to businesses that haul cannabis.	['Regardless of what provider you use, it is important to consider six primary types of insurance coverage for your operations.\nIt’s no surprise. New issues arise practically daily for cannabis business owners and they often feel like an uphill battle. While this industry is still prohibited on a federal level, cannabis business owners have traditionally looked to smaller or state licensed resources for support finding a friendly approach.\nInsurance agencies may be opening doors, rather than closing them. As the number of cannabis businesses in need of coverage has grown, a handful of larger carriers, most notably Germany’s Hannover Re, have begun offering insurance products tailored to the cannabis industry. Regardless of what provider you use to obtain insurance for your cannabis business, it is important to consider purchasing six primary types of insurance coverage for your operations.\nSix Major Types of Coverage:\nComprehensive first-party property insurance is an important policy for any cannabis business. The policy should include coverage for theft, property damage, and business interruption costs and in particular, ensure it contains the following:\n- The highest possible policy limits to protect against theft as cannabis businesses tend to be cash-heavy\n- A higher-limit property damage coverage for those businesses in the cultivation and processing sectors of the industry to address the heightened risk of fire associated with these types of operations\n- Robust business interruption coverage to offset any losses or delays resulting from a break in the supply chain\n- Language ensuring the carrier won’t later seek to deny coverage on the basis of cannabis being prohibited on a federal level\nFor those cannabis businesses that transport products, it is essential to obtain commercial auto coverage. Cannabis-specific commercial auto policies extend many of the same benefits as traditional auto policies, such as accident coverage and rental reimbursement but many traditional auto insurers will refuse coverage to businesses that haul cannabis. For companies that do provide commercial auto policies to cannabis business owners, pricing for these policies may differ depending on the type of cannabis business (processor vs grower vs retailer) you are. Make sure your employees are not use their own vehicles to transport product because their personal insurance will not cover the transportation of cannabis.\nEvery cannabis business should consider purchasing product liability insurance as they can be brought into litigation with the wide range of legal exposure tied to their products. The industry is vulnerable to product liability litigation due to the numerous different cannabis strains and derivative products of varying potency available for purchase. When reviewing such a policy for your business, it is critical to review any exclusions to ensure the products you provide are covered.\nCannabis businesses are not exempt from the threat of cyberattacks and should consider purchasing specialized cyber insurance. Data obtained from seed-to-sale tracking is a vulnerability to cannabis businesses as data hackers are known to target customer’s personal information. We have seen this play out in Washington State many times. In addition to the seed-to-sale information, any kind of ‘trade-secret’ stored on company computers may be vulnerable as well to cyber threats. Cyber policies for cannabis businesses can be customized to cover a wide range of risks to protect this sensitive information.\nDirectors and Officers\nAs cannabis in an emerging industry, robust directors and officers (D&O) insurance is recommended to shield executives and board members against potential claims by disgruntled investors and others. For those individuals looking to join the board of directors of a cannabis business, it is encouraged to review the company’s D&O policy to ensure the insurance is sufficient and has a high policy limit.\nIndoor growers may be able to secure insurance over their crop. Coverage includes crop failure due to bugs, natural disaster and mold. We have not seen the same coverage for outdoor producers but it may be coming.\nHow JDSA Can Help\nThese are just some of the types of insurance cannabis owners should consider. There may be additional insurance needs depending on the type of cannabis establishment you operate. If you have questions or need advice regarding this—or maintaining a compliant cannabis business—contact JDSA Law for assistance.']	['<urn:uuid:d9665fe5-370f-4b98-9a4e-55d0907f02e7>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	26	38	688
69	How did naming customs vary between regions of Ireland?	There were distinct regional naming practices across Ireland. Austin (for Augustine) was common among Catholic peasantry in Connaught but rare elsewhere. Bernard and Sylvester were common in Cavan, while Dominick was prevalent among Catholics in Mayo and Galway. Florence was used as a boys name amongst Catholics in Cork, where Jasper and Horatio were also popular. Lancelot was common in Monaghan, Lettice was widespread among Protestant families in Cavan, and Moses, typically a Protestant name, was popular among Catholics in Wexford.	"[""The peculiarities in Christian names are not as likely as to mislead as\ndifferences in surnames- but may be thrown out of their proper place\nin an index or escape notice altogether, or, if seen might be taken\nto refer to some other person.\nThe peculiarities in Christian names in Ireland may be divided into five classes:\n1. NAMES APPLICABLE TO BOTH SEXES:\nNames applicable to both sexes\nNames usually given to one sex, but applied to the other\nDiminutives materially differing from the original names\nNames which are different but for varied reasons are used interchangeably\nIrish equivalents for English names and English equivalents for Irish names\nSurnames given at baptism as Christian names frequently by Protestants and sometimes applied to both sexes.\n2. NAMES USUALLY GIVEN TO ONE SEX-BUT APPLIED TO THE OTHER:\nFlorence: more usually a male name in Ireland, particularly with the McCarthy family, but nevertheless found albeit rarely, as a female name even before the popularity of Florence Nightingale in the 19th C\nSydney or Sideny: used for both sexes but more commonly as a female name.\nEvelyn: both sexes\nEdie: a male name in Ireland, confused with diminutive of Edith.\nKitty: usually the diminutive of Catherine, but also used in the forms Kit and Kitty as a diminutive for Christopher.\nConstant or Constance: found for a boy in Co. Clare., Constance and Constantia are found as girls' names.\nGiles: usually a male name but found in Ireland as a female name and an anglization of Sheila.\nAll confused due to their slight differences.\n3. DIMINUTIVES MATERIALLY DIFFERING FROM THE ORIGINAL NAMES:\n4. NAMES WHICH ARE DIFFERENT BUT FOR VARIED REASONS ARE USED INTERCHANGEABLY:\nAnty of Anastsia (Anstace)\nBartle, Bat, Batty, Bartly are forms of Bartholomew.\nBessie, Betsy and Lizzie forms of Elizabeth\nBiddy, Bride and Beesy of Bridget\nCastor and Kit of Christopher\nCon, Connor, Corny and Neily of Cornelius\nCenty of Hyacinth\nDarby of Dermot\nHonor, Honny, Onny, Noey and Norah are forms of Honorah\nJuggy form of Judith<BR Lack and Lacky of Laughlin\nNancy and Nany are forms of Anne and Hannah\nNell and Nelly of ELlen, Helen and Eleanor\nPeggy, Maggy, Meg of Margaret\nPolly, Molly and Mally forms of Mary\nPolly, Patsy used to Martha\nRory and Roddy of Roderick\nSandy of Alexander\nToby form of Theobold\n5. THE IRISH OR ENGLISH EQUIVALENTS OF ONE ANOTHER:\nAbigail: Deborah (because of the similarity of their respective diminutives)\nAbbie and Debbie and of Gubbie (the diminutive of the Irish Gobnet)\nAlice: Ellen (probably due to the diminutive Eily for both the Irish names Eilish and Eileen)\nBridget: Bedelia: Delia: Bessy\nDaniel: David (due to poor penmanship and mis-reading or mis-copying)\nEdward: Edmond ( because of phonetic similarity)\nGerald: Garrett, Gerard\nGiles: Cecily, Cecilia, Celia, Julia (as renderings of the Irish Sheelagh)\nGrizell: Grace (In Ulster)\nHannah: Honora, Johanna\nJacob: James (because of latin form Jacobus)\nJane: Joan, Jean (all being rendered Johanna in Latin)\nJudith: Julia (perhaps due to the simillarity of their diminutives Judy and Julie)\nOwen: Eugene (both being used as translations of the Irish Eoghain)\nPatrick: Bartholomew (solely through confusion of respective diminutives Pat and Bat)\nPeter: Patrick (in Ulster)\nRandal: Randolph: Ralph ( all variants of the same name and rendered Randolphus in latin)\nSusan: Johanna (as renderings of the Irish Siobhán)\nTheobold: Tobias (because of common diminutive = Toby)\nBut not necessarily being a correct translation of such names.\n- Irish: English\n- Brian: Bernard, Barnabas (Barney)\n- Diarmaid (Dermot): Jeremiah, Darby, Demetrius\n- Tiernan: Terence\n- Teige: Thaddeus (Thady)\n- Morrogh: Morgan\n- Aodh: Hugh, Edie\n- Tirlogh: Terence\n- Eoghain: Owen, Eugene\n- Cormac: Charles\n- Cathal: Charles\n- Eamonn: Edmond, Edward, Aimon\n- Conchobar: Connor, Cornelius, Constantine\n- Donogh: Denis, Donat\n- Dhonal: Daniel, Donald\n- Eileen: Ellen, Helen, Eleanor\n- Eilish: Alice\n- Siobhán: Johanna, Susan, Jane\n- Sheelagh: Cecilia, Cecily, Giles, Sheila, celia, Julia\n- Oonagh: Una, WInifred\nThe correct or standard translation of such Irish names as Sean-John; Seamus- James etc., can be found in a good Irish-English lexicon.\nUntil the end of the mid 18th C it was unusual for a child to receive more than one Christian name in Ireland although there were some standard favourite combinations such as Ann Jane, Mary Anne. Even in the 19th C the practise of giving a second Christian name was slowly adopted....starting with the richer gentry.\n6. Favourite Catholic Christian names were:\nJohn, Patrick, James, Denis, William, Darby, Dermot, Daniel, Cornelius, Henry, Timothy, Thomas, Michael, Jeremiah, Bartholomew, Brian, Laurence, Thady, Terence, Owen, Martin, Mathias, David and Jospeh.\nDominick enjoyed vogue in the 17thC..Columb, Malachy, Miles, Felix, Ambrose and Stanislaus were less commonly used. Aloysius is rare before the 19thC.\nMary, Catherine, Bridget, Honora, Margaret, Ellen, Anastasia, Johanna, Judith, Julia, Rosanna, Maryanne, Elizabeth and Jane. Less common were Magdalen Monica and Theresa. Marcella is found in Ireland but is rare in England.\n7. Protestants were more varied:\nBoys names were:\nArthur, John, Henry, James, William, Frederick, George, Edward, Richard, Charles, Philip, Oliver, Jonathan, Anthony, Andrew, Simon, Marmaduke and Stephen. They also used old testament names which were rarely used by Catholics such as Abraham, Jacob, Moses, Isaac, Samuel, Joshua, Gamaliel.\nFavourite Protestant girls names seem to have been:\nMary, Sarah, Elizabeth, Eleanor, Lucy, Catherine, Susanna, Hannah, Margaret, Jane, Isabella, Frances and Alice.\nLess frequently: Barbara, Gertrude, Dorothea, Charlote, Diana, Rebecca, Lydia , Grace, Phoebe, Henrietta, Lettice, Ursula, Penelope, Esther and Heather.\n8. Some Regional Naming Practices were:\n- Austin (for Augustine) was common in the Catholic peasantry in Connaught but was uncommon elsewhere.\n- Bernard and Sylvester in Cavan\n- Dominick was common amongst Catholics in Mayo and Galway\n- Hyacinth in Galway\n- Ignatius and Xaverius were common amongst Catholics in Mayo and Galway\n- Florence was used as a boys name amongst the Catholics in Cork\n- Jasper and Horatio had a vogue in Cork\n- Lancelot in Monaghan\n- Lettice was widespread amongst Protestant families in Cavan.\n- Moses, usually a name used by Protestants was a popular Catholic name in Wexford.\n- Catholics in 19thC sometimes gave male children second name of Mary or Maria, and even rarely Anne.\nBaptisms and marriages were recorded in either Latin or English. Never in Irish. Generally where English was more common English was used and Latin was in Irish speaking parishes. There is however, no consistency. The Latin version of the first name was given while the surname and placename were still written in English.\nLatin Name: English Equivalent (s)\n- Anna: Anne\n- Cornelius: Cornelius, Conor, Neil\n- Demetrius: Jeremiah, Jerome, Jerry, Dermot or Derby. (Demetrius is a latinised form of the Irish name Diarmaid which has also been anglicised as Jeremiah)\n- Gulielmus: William\n- Hugones: Hugh\n- Ioannes: John or Owen\n- Jacobus: Jacob, James\n- Johana: Johanna, Hannah, Joan, Jane\n- Johanes, Joannes: John\n- Honoria: Hannah, Nora, Norry\n- Margarita: Margaret, Peg (Peig is actually the Irish name for Margaret)\n- Maria: Mary, Marie\n- Nigellus: Neil, Niall\n- Timotheus, Thaddeus: Timothy, Tadgh, Thady""]"	['<urn:uuid:d6960f3c-1fc0-48cc-84e4-6ad3fa21a3d6>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T06:04:36.674696	9	81	1177
70	How is ninebark useful in rain garden construction and drainage?	Ninebark (Physocarpus opulifolius) is particularly well-suited for rain gardens as it naturally grows in moist soils along streams and is tolerant to a wide range of site conditions. In rain gardens, it can be used as a specimen plant or in border plantings, helping to slow down and filter water. It's drought tolerant and grows well even in harsh conditions with clay or rocky soils, making it ideal for the soil layer of rain gardens where it needs to handle both wet and dry conditions.	['Rain gardens are becoming an increasingly popular option for gardeners looking to create an aesthetically pleasing garden that manages runoff, improves water quality, and provides wildlife habitat. Rain gardens are “depression gardens” – designed and located to receive runoff from a roof, driveway, or lawn. These features work with nature to collect, filter, and infiltrate runoff, while showing off a variety of vibrant, colorful, and low-maintenance plants.\nPhysocarpus opulifolius, from the rose (Rosaceae) family, is a native plant well-suited for rain gardens that provides year-round color from foliage, flowers, and bark. It is often called common ninebark and Atlantic ninebark. The common name – ninebark – refers to the winter appearance of the shrub (Figure 1), when the showy bark, consisting of reddish inner-bark framed by paper-like strips of older bark, peels away in layers; sometimes as many as 9 layers of exfoliating bark are visible\nHistory and Traditions\nPhysocarpus comes from the Greek Physo (bladder or bellows), and carpus (fruit) refers to the bladder-shaped, swollen fruit that form after flowering. The specific epithet, opulifolius is derived from the latin opulo, meaning wealth/profusion, and folius, meaning leaves2. Ninebark is native to the eastern US; Native Americans used a decoction (extract resulting from boiling tissues down to concentrate desired compounds) made from the inner bark as pain relievers, analgesics, emetics, laxatives, and cathartics3 ‡. However, excessive doses of the bark decoctions can be toxic!\nNinebark is native to South Carolina and is a fast growing species that is free of serious insect and disease pests. The white or magenta (red) flowers add life to your garden (Figure 2), and are a nectar source for native bees, honey bees4, and butterflies. Native insect species, such as the spiraea leaf beetle (Calligrapha spiraeae), caterpillars of the dimorphic Eulithis moth (Eulithis molliculata), and aphids (Aphis neilliae)4 rely on ninebark foliage their primary food-source. Despite potential for foliar damage (potentially defoliation with large populations), feeding by these insects will not kill established plantings5. The fruit serve as a food source for a variety of game birds and small mammals6. The beautiful foliage, that ranges in color from green to bright or burgundy red (Figure 3), also provides excellent cover or habitat for native bird species due to its dense habit.\nPlanting and Care\nNinebark is tolerant to a wide range of site conditions. Flowers are at their most colorful in full sun conditions, but color fades in shady conditions for most cultivars. However, in hotter regions (Zone 7b—8), ninebark may perform better with afternoon shade. Ninebark is drought tolerant and grows well in harsh conditions (clay or rocky soils). In natural settings, ninebark is commonly found growing in moist soils as a thicket, along streams, in sand or gravel bars, or on rocky slopes and bluffs6. Named cultivars (see list of selected cultivars below) are usually preferred in cultivated landscape settings, as the straight species is more unkempt with less showy foliage.\nOvergrown plantings of ninebark can be rejuvenated by pruning plants to the ground in late winter1,6. If the peeling bark of the shrub is desired for winter interest, care should be taken to avoid pruning too frequently, as the showy bark only forms on older stems, greater than ½” in diameter. If ninebark is used as a specimen (focal point) planting, selectively prune branches from both the interior and exterior of the plant, leaving both old and new foliage for contrasting bark coloration. Removing branches from the interior of the plant also increases air circulation and reduces the potential for foliar diseases in susceptible cultivars.\nThe straight species character of ninebark is “difficult to use in small home landscapes,” but cultivated varieties have better foliar color and more appealing aesthetics7. Ninebark use in the landscape should be influenced by (1) desired size of mature plantings and (2) cultivar selection for mature size and foliar coloration. Ninebark is especially suited for use as a specimen, but can be grouped in border plantings within landscape beds, or used as a hedge or screen, or in open woodland and naturalized areas (Figure 4).\nNinebark pairs well with many other garden species; companion plants to consider should also tolerate similar soil moisture conditions, for best garden performance. Plants to consider planting with ninebark include:\n- River birch (Betula nigra): textured, exfoliating bark, delicate appearance, sun\n- Fringetree (Chionanthus virginicus): white clouds of delicate flowers hand below newly emerging foliage in late spring, sun to part shade\n- Carolina allspice (Calycanthus floridus): burgundy purple, fragrant flowers in late spring, full sun to part shade\n- Star anise (Illicium floridanum): dark red flowers in mid-spring, evergreen, part shade\n- Eastern bluestar (Amsonia tabernaemontana): pale blue flowers rise above medium textured foliage in spring, foliage turns yellow in fall, full sun to part shade\n- Siberian iris (Iris sibirica): white, yellow, purple, and blue flowers in spring, full sun\n- Spiked speedwell (Veronica spicata): rose-pink to deep purple flowers in early- to mid-summer, full sun\n- Sweetgrass or muhlygrass (Muhlenbergia capillaris): white or pink blooms provide fall and winter interest, full sun to part shade\n- River oats (Chasmanthium latifolium): delicate seed heads from late summer to fall, part shade to shade\nCenter GlowTM – 7-8’ h and 8-9’ w. Leaves emerge yellowish green and darken to reddish purple. Fast, dense, rounded growth habit. White, pink-tinged flowers.\nCoppertinaTM – 8-10’ h x 6-8’ w. Coppery-orange spring flush darkens to burgundy-red in summer. Gently arching habit. White flowers. Showy red seed-capsules in fall.\nDiabolo® (‘Monlo’) – 8-10’ h x w. Deep-purple foliage. Upright, spreading, open habit. Pinkish-white flowers, red drooping fruit.\nLady in RedTM – 3-5’ h and 5’ w. Chestnut-red foliage from spring through fall. Pink flowers. Little pruning needed for tidy habit.\n‘Luteus’ (‘Aureus’) – 8-10’ h x w. Yellow foliage matures to green in late summer. Cream-colored flowers. Extensive suckering.\n‘Nanus’ – 1-2’ h and 2-3’ w. Green foliage. Dwarf, spreading, somewhat coarse growth habit. White flowers.\n‘Nugget’ – 3-6’ h x w. Yellow to lime green foliage. Clusters of white flowers along stems. Dark brown seed capsules. Vase-shaped habit. Susceptible to leaf spot, powdery mildew, and fire-blight.\n‘Snowfall’ – 6-10’ h x w. Dark green foliage. Dense pink and white floral display over nearly all branches. Bright red, densely packed fruits. Form is open and “messy,” cinnamon colored bark is spectacular.1\nSummer WineTM – 4-6’ h x w. Deeply-cut, wine red foliage. Pinkish-white blooms. Graceful, Dense, free-branching, mounding habit. Less likely to spread by suckering. Prune after bloom.\n‡Clemson University Extension does not promote the use of any herb or medicine without your doctor’s knowledge and supervision.\n1 Lattier, J.D. and T. Anísko. 2008.The True Colors of Ninebark (Part 2). American Nurseryman. p 36-43.\n2 Lady Bird Johnson Wildflower Center. 2013. Physocarpus opulifolius L. (Maxim). Accessed 13 Feb 2013. <http://www.wildflower.org/plants/result.php?id_plant=PHOP>\n3 Native American Ethnobotany Database. 2013. Accessed 13 Feb 2013. <http://herb.umd.umich.edu/herb/search.pl?searchstring=Physocarpus+opulifolius>\n4 Hilty, J. 2013. Illinois Wildflowers: Ninebark. Accessed 13 Feb 2013. <http://www.illinoiswildflowers.info/trees/plants/ninebark.htm>\n5 Hahn, J. 2015. Calligrapha beetles. University of Minnesota Extension. Accessed 13 Nov 2015. <http://www.extension.umn.edu/garden/insects/find/calligrapha-beetles/>\n6 Guala, G. 2003. Plant Guide : Atlantic ninebark Physocarpus opulifoius (L.). USDA-Natural Resource Conservation Service, National Plant Data Center, Baton Rouge, LA.\n7 Dirr, M.A. 2009. Manual of Woody Landscape Plants. 6th ed. Stipes Publishing. Champaign, IL.', 'Having drainage or flooding issues with your yard? A muddy spot or a steep slope? A rain garden could be a great addition to your landscape! It’s not only eye pleasing, but it also provides a great deal of benefits beyond stormwater control.\nWhat is a Rain Garden?\nA rain garden is a depression or channel in the ground designed to hold stormwater runoff using planted vegetation. When stormwater runoff passes through the rain garden, the vegetation will slow the water down and gather the water, allowing it to infiltrate into the soil.\nBenefits of a Rain Garden\nReduces and controls stormwater runoff.\nReduces erosion caused by fast-moving stormwater.\nFilters out chemicals and pollutants, keeping them out of local waterways.\nSlows down runoff allowing it to infiltrate into the ground. This recharges groundwater and keeps the water that falls on your landscape in your soil.\nAttracts and supports native wildlife.\nLayers of a Rain Garden\nFrom Bottom to Top\nDrainage Substrate Layer – Sand or gravel for good drainage. This layer is optional, depending on the amount of drainage offered by existing soil.\nSoil Layer – Compost amended soil to plant into.\nPlants – Native plants that slow and filter water.\nMulch – Place a 2-3 inch layer of mulch for the plants and to control erosion.\nRocks – Place larger rocks around the edges, under downspouts, or otherwise where the water will be flowing in to slow it down. This layer is also optional, depending on how fast your stormwater is coming in the rain garden.\nHow to Make a Rain Garden\nStep 1: Placement\nThe perfect place to have a rain garden is where water naturally gathers. This can be under a downspout, at the bottom of slopes, or at depressions. Water from a downspout can also be redirected to a desired location. Another great place to add a rain garden is at the edge of sloped yards, to stop water from flowing onto the street.\nAt this stage, it’s a great time to plan out what materials you need and what type of plants you want to add. Plant choice will depend on the amount of sunlight the area gets and how wet the soil stays.\nStep 2: Gather Your Materials\n- Digging tools\n- Gravel or sand for good drainage in the bottom layer\n- Soil or compost amended soil for planting\n- Native plants for filtration and erosion control\n- Rocks or large boulders to slow water flow\n- Mulch for plants (optional)\nStep 3: Put it all together\n- Start by digging a depression in the ground. Create a raised bank around the depression or on the side opposite the water flow, like at the bottom of a slope.\n- Add in the drainage substrate of your choice. You can add a layer of gravel and a layer of sand or use just one or the other.\n- Add in soil and mix in compost.\n- Plant your desired native plants.\n- Add mulch and rocks if needed.\nImportant Tip: Rain gardens work best when all the water soaks in after 24-48 hours. If your garden is holding water for longer than 72 hours, you may need more drainage substrate at the bottom.\nRain Garden Plant Choice\nRain gardens will be very wet sometimes but will likely stay at typical soil moisture between rain events. Many rain gardens are not irrigated, so for North Texas it is smart to choose plants that are also happy to be dry during our hot summers. For this reason, the best rain garden plants are native plants that are ok with wet feet but are happy to be dry, like those that naturally grow near streams and in floodplains.\nFirst, find native plants based on the amount of sunlight the area gets. Choose the plants that have medium water needs and are okay with being wet or dry. Take a look at the suggestions below or easily search by sun and water needs at http://www.txsmartscape.com.\nPlace more water-loving plants in the middle, where the lowest depression is, and plants that like it drier near the edges. Plant more thick, fibrous plants like large grasses or bushes in the middle, and plant smaller flowers and ferns around the edges. Another great choice for a low maintenance rain garden is to seed it all with wildflowers.\nRain Garden Plant Suggestions\nBushy Bluestem – Andropogon glomeratus\nEastern Gamagrass – Tripsacum dactyloides (pictured)\nInland Seaoats – Chasmanthium latifolium\nButtonbush – Cephalanthus occidentalis (pictured)\nBeautyberry – Callicarpa americana\nFlame Acanthus – Anisacanthus quadrifidus\nGregg’s Mistflower – Conoclinium greggii (pictured)\nGiant Coneflower – Rudbeckia maxima\nIronweed – Vernonia baldwinii\nNative Wood Fern – Thelypteris kunthii (pictured)\nFrog Fruit – Phyla nodiflora\nHorseherb – Calyptocarpus vialis\nNow you are ready to make your own rain garden!\nLearn even more about rain gardens here: https://www.epa.gov/soakuptherain/soak-rain-rain-gardens']	['<urn:uuid:852b46c8-f21d-4928-94dd-5dabfebba1a4>', '<urn:uuid:062ee0be-4c00-4f71-81f9-8fbc94ce2e6f>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T06:04:36.674696	10	85	2018
71	orchestra musician collaboration methods prevent physical strain techniques	Orchestra musicians must work closely together as a unified body, requiring sensitivity, flexibility, awareness, and open communication. They need to alternate between leading and supporting roles while maintaining trust and open-mindedness. To prevent physical strain, musicians should focus on injury prevention basics including proper sitting and standing positions, taking regular breaks, and using appropriate stretching exercises. Medical studies have highlighted the importance of performance health, leading music schools and professional orchestras to implement protective measures such as proper chairs and hearing protection shields.	['Damian Iorio, violinist, conductor and music director of the UK-based National Youth String Orchestra, writes about the importance of ensemble skills. From 2018\nAny ensemble – regardless of size, whether symphonic or chamber, orchestra or opera – should aspire to perform together as if component parts of a single body. One only needs to look at the great string quartets who think, feel and play as one entity to realise the importance of this ideal. Playing in any ensemble is complicated; it is a microcosm of society, a group of individuals from different social backgrounds, with often diverse experiences, who must work closely together in order to achieve one common goal. One moment you may be required to lead, the next moment follow or support, and this demands great sensitivity, flexibility, awareness, hard work, shared trust, open ears and open minds. The art of listening, engaging and communicating is fundamental not only when performing music, but in life in general.\nAlongside my career conducting orchestras and theatres around the world I am also very involved in education and outreach, and have been music director of the National Youth String Orchestra for a number of years. I was a violinist before I changed career, and youth orchestras were very important to my development. When I conduct an orchestra such as the London Philharmonic or Philharmonia, which command such a high technical level, the basics of ensemble playing – playing together, listening intonation, articulation – are often taken for granted. However, these demanding skills require nurture within a youth orchestra setting, and in the National Youth String Orchestra we intensely focus on the development of ensemble playing skills.\nGood ensemble skills are invaluable for musicians at every level. These skills are transferable to a career outside of the instrumental world where participants will also find themselves having to learn and listen to others, as well as work collaboratively with a diverse group of people – some of whom they may not get on with personally but must collaborate with professionally in order to work towards a greater purpose.\nAt NYSO we structure our courses very carefully in order to develop secure ensemble skills. There are two orchestras for students of different ages and levels (In 2019 we will launch a third orchestra for younger children) and we curate one programme each year, preparing the music over two courses that take place at Easter and Summer, before a public concert performance. This gives the young musicians time to learn the repertoire in detail and depth, and removes the pressure of having to perform before they are ready.\nWe have a dedicated coach for each section, and sectional rehearsals take up a lot of the rehearsal schedule, especially for the Easter course. The coaching staff are highly experienced professionals who play at the highest level and are respected teachers. The students benefit from the full attention of each coach, enabling individual focus and assistance in resolving technical difficulties. Preparation is key as each section is small, so everyone is treated with equal importance and given equal responsibility. Working with such experienced professionals is important, and the insight they give is invaluable, including the ’war stories’ of the profession!\nI have great trust in the instrumental staff and we cooperate very closely. They each have their own approach to teaching and coaching. At the beginning of the course they may start by playing through the pieces, picking out hard passages, organising fingers, learning how to sit properly (often a problem for young people!), or they may start with warm up exercises and stretching and a little education about Alexander Technique. The students will be playing for many hours a day, more than they normally do, and it is very important that they understand how to look after their bodies, stretch and release tension.\nAt the end of the first day, I will take a full orchestra rehearsal so that everyone, students and coaches, can hear how everything is coming along. The coaches sit in on the majority of the full rehearsals, so are on hand to help out with details or issues that crop up, such as a fingering or bowing which might work in the sectional but doesn’t produce the desired effect in context. As the course progresses, I mix in more full rehearsals to the schedule, but I am always flexible with the rehearsal planning and adapt as the course progresses.\nWe work the students hard but I make sure that they have enough free time during the courses not only recharge their batteries and rest their bodies, but also to spend time together socially. The social side is very important as these are experiences that they will remember for the rest of their lives. A good group atmosphere translates positively into their playing.\nAll of this hard work allows us to explore something very valuable for ensemble playing, and that is the development of the musical connection between everyone in the orchestra – the building of an understanding so that we can perform chamber music together. This is what I try to develop not only with youth orchestras, but with any orchestra I work with around the world. As a conductor, I express music with visual communication, using my hands, eyes and body language. If I can develop this connection between the players it also allows me to build a relationship with the orchestra which transcends the visual communication, freeing me and all of the musicians in the orchestra to express ourselves, and the music of the composer, as one.\nTrust, listening, communicating, cooperating. Important not only for music, but for life.\nFor more information about the National Youth String Orchestra, visit www.nyso.uk', 'Injury Prevention for Musicians: Two Books Reviewed by Joanna White\nHorvath, Janet. Playing Less Hurt, An Injury Prevention Guide for Musicians. (New York: Hal Leonard Books, 2010. Print.)\nKlickstein, Gerald. The Musician’s Way, A Guide to Practice, Performance, and Wellness. (New York: Oxford University Press, 2009. Print)\nIf you play or teach the flute, chances are you or your students have experienced pain or injury at some point as a result of playing. While most of us are aware of problems such as repetitive motion strain or hearing loss, there are other less common issues that afflict us and turn up in our students. We are not always thoroughly trained in injury prevention or taught to recognize every sign of danger, and we might not know what to do or where to get help in case of injury. I will highlight two books that may not be familiar to flutists; both have extensive information about prevention of specific injuries for musicians.\nJanet Horvath’s award winning book, Playing Less Hurt, An Injury Prevention Guide for Musicians (New York: Hal Leonard Books, 2010. Print), should be essential reading for performers and teachers. Horvath, associate principal cellist of the Minnesota Orchestra is convincing in her plea for our awareness; she is a constant advocate for performance health. Easy to read, this volume includes scientific information along with photos, diagrams, and multitudes of suggestions, solutions, and meticulous resource lists. While it was originally written in 2000, it has been revised three times, most recently in 2010, keeping up with the rapidly advancing field of performance medicine. Horvath also has a website, which includes a resource list and blog posts about her topics, http://playinglesshurt.com.\nHorvath, in her book, gives reasons that musicians are susceptible to injury. She lists risk factors and danger signals critical for both performers and for teachers who have a great responsibility to keep their students of all levels safe. She gives detailed explanations of injuries such as back and disc problems, muscle and tendon disorders, nerve entrapments, hand and forearm pain, and other conditions like Raynaud’s disease, focal dystonia, fibromyalgia, TMJ, and arthritis, among others. There is even a brief section on performance anxiety and the use of beta-blockers.\nThe author discusses preventative and restorative approaches including stretching, strengthening, and comfort. Chapter 15 discusses hearing loss, a critical topic not often discussed in music schools. Finally Horvath gives suggestions for what to do if you do get hurt and how to come back from injury. She gives concrete information about often asked questions like: when it is safe to use heat as opposed to ice? She even includes a chapter on instrument modifications, discussing splints, slings, supports, and orthotics.\nHow to practice, a topic many of us teach, is discussed in detail in Horvath’s book. Body-safe playing including warm-ups, stretches, and lists for injury prevention, can be found within. Hand drawn diagrams show the author’s favorite ways to stay flexible.\nThe resource list, essential for any player and teacher wanting to know where to turn, will be the hardest part of the book to keep up to date but we can turn to the Playing Less Hurt website for updates. Resource categories (in the book) include: Books, Organizations, Websites, Videos, Products, and Clinics. Particularly valuable are the detailed listings of medical personnel in many specialties.\nIt is timely to revisit the topic of performance health as music schools come to the realization that it is critical to train performers and teachers in injury prevention and recognition of symptoms. One Northwestern University School of Music study (2008 findings) done by the late Alice Brandfonbrener, known for being a pioneer in performance medicine, showed that 84% of freshmen instrumentalists struggled with pain. (Horvath, p. 87) An unrelated study by Medical Problems of Performing Artists reported that 68% of teachers spent less then five minutes teaching injury prevention in a lesson. (p. 87) Since, as the Horvath points out, rigorous demands are placed upon performing musicians, it is critical that this disconnect between prevalence of problems and large numbers of organizations and teachers without knowledge of how to address them be narrowed.\nHorvath reports on the Health Promotion in Schools of Music Project, a group that presented findings to the National Association of Schools of Music [United States] in 2005 and 2006. (Horvath, p. 86-7) The group made clear recommendations to schools that train musicians and teachers that they incorporate new information about performance health into their philosophies and curricula. NASM has begun to take a more active stance in promoting healthy playing. For example, along with the Performing Arts Medicine Association, (PAMA,) NASM now lists basic information about hearing loss on its website: (http://nasm.arts-accredit.org/index.jsp?page=NASM-PAMA_Hearing_Health)\nProfessional orchestras have also become more aware of new standards, providing proper chairs and shields for hearing protection.\nThe Musician’s Way: , A Guide to Practice, Performance, and Wellness by Gerald Klickstein (New York: Oxford University Press, 2009. Print), another valuable resource, is divided into three major sections: Artful Practice, Fearless Performance, and Lifelong Creativity. Part III is specific to performance health, but the first two sections also go a long way in the promotion and development of healthy musicians because Klickstein emphasizes a musical, artful, and mindful approach to learning and practice. There are also photos of stretching exercises and a whole section on taking breaks.\nIn the first chapter on prevention, there are sections on: musician’s injuries (with particular attention to causes of injuries,) warning signs, how we should respond to warning signs, a huge section on injury prevention, and information on recovery. The simple injury prevention basics are invaluable for performers, and teachers should know them; the ideas are logical, practical, and easy to incorporate.\nIn the second prevention section, Klickstein discusses balance and sitting and standing, which many of us are familiar with from Alexander Technique and Body Mapping. Photographs are included although they are not specific to flute. There is also an extensive section on hearing loss prevention.\nKlickstein, an international classical guitarist and Professor of Music at the University of North Carolina School of the Arts, has also become known for his Musician’s Way Blog: http://musiciansway.com/blog/tag/gerald-klickstein/ [which is part of the companion to the book at www.musiciansway.com, Ed.] Here he adds to and updates the information in his book and you can sign up to receive his newsletter as well. Reference lists under the wellness section of the website include: Injury Prevention for Instrumentalists, Locate a Medical Specialist, Alexander Technique, Feldenkrais Method, Ergonomics, Reference, General Health for Musicians, Hearing Protections, and blog posts on topics like “Fight or Flight,” and “Safely increasing Practice Time.”\nOrganizations like PAMA now provide websites and organize conferences where musicians, educators, administrators, and medical professionals come together to share information about the prevention and treatment of musical maladies, and many groups have specific websites about individual problems such as performance anxiety, musician’s dystonia, and many more. The journal, Medical Problems of Performing Artists, devoted to music medicine also can be found online. Thanks to the work of Janet Horvath, Gerald Klickstein, and others like them, information about performance health issues is much more widely disseminated than when the earlier edition of Playing Less Hurt was published in 2000. If more musicians and teachers read and use these books and learn about facts and resources, musicians with performance problems will feel less alone and we will all be a step closer to “playing less hurt.”\nDr. Joanna Cowan White, professor of flute at Central Michigan University, performs as principal flutist of the Saginaw Bay and Midland Symphony Orchestras, and records with her chamber groups, Crescent Duo and Powers Woodwind Quintet on White Pine Music and Centaur Records. She has been Secretary of the National Flute Association and has written articles for Flute Talk, Flutist Quarterly, The National Association of Wind and Percussion Instructors, and Der Rohrblatt, among others. She is also a published poet.\nComments are closed.']	['<urn:uuid:5625473c-e056-446d-8337-ca0ab80da657>', '<urn:uuid:edd54d08-b47b-4a2f-a6b6-69198de29213>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T06:04:36.674696	8	83	2269
72	plant based diet health benefits list	Plant-based diets offer multiple health benefits as these foods are rich in fiber, vitamins, and minerals. The benefits include decreased blood pressure, reduced low-density lipoproteins (bad cholesterol), lower risk of diabetes, help in maintaining a healthy weight, and reduced chances of heart disease.	"['By Ashley Jouhar on July 03, 2019\nThe definition of wellbeing is given as ‘The state of being comfortable, healthy, or happy’. The synonyms associated with it are ‘Welfare, Health, Good Health, Happiness, Comfort, Security, Safety, Protection, Prosperity, Profit, Success, Good Fortune and Advantage.\nFrom this we can see that wellbeing encompasses more than simply exercising and eating a balanced diet. Wellbeing also includes good mental health, a sense of security, friendship, some financial security and contentment with your lot in life.\nSo wellbeing is no longer limited to a healthy body; today’s vision is more holistic.\nIndeed, emotional health is now on par with physical health as more and more people find life satisfaction and a greater sense of purpose in the balance of mind, body and spirit.\nNic Marks of the New Economics Foundation says, “Well Being is not only about the individual but also about values grounded in a broader, shared understanding of how the world is and should be.”\nOver the last 50 years, the population of Great Britain has become richer - but despite this, evidence shows that mental wellbeing has not improved.\nMany of the things people often aspire to and believe will improve their mental wellbeing - such as possessions or more money for luxury holidays - on their own, do not lead to a lasting improvement in the way they feel about themselves and their lives.\nStills and video that convey not only physical fitness but also communicate a healthy mind, a positive approach to life, mindfulness, tranquility and a sense of being at one with your surroundings are popular. These images have many uses in marketing too, communicating key concepts around Health, Wellness, Escapism, Freedom, Getting Away from it all, Tranquility, Care, Fitness and Purity.\nBeing aware ofthe present – to your own thoughts and feelings right now, and to the world around you – can improve your mental wellbeing. This is particularly difficult to achieve, however in our tech-driven world, where a moment doesn’t pass, it seems, without us reaching to check our smartphones; our attention directed to a screen, rather than the world around us.\nMindfulness is recommended by the National Institute for Health and Care Excellence (NICE) as a way to prevent depression.\nRichard Davidson, one of the early pioneers, scholars and teachers of mindfulness in the US believes that working with kids and teaching them about mindfulness early on as part of their education is going to be really important. He explains, “Teaching kids these kinds of skills early in life can have multiplicative effects as the kids develop. Being able to practice these skills at a very early age can set a child up for a much more positive developmental trajectory.”\nThis mental wellbeing, self-esteem and self-confidence are very important to young people, along with good relationships, and a positive engagement with the world. As Sarah Stewart-Brown, professor of public health at the University of Warwick says, ”It\'s useful to start with the idea that overall wellbeing involves both the mind and the body. And we know that physical and mental wellbeing are closely related.""\nNutrition is another very topical piece in the jigsaw of our overall wellbeing. This includes not only our health but the health of the planet too. The provenance of our food is very important nowadays. How the food was produced, where it’s from and whether it is sustainable are questions that are not going away any time soon. Take for instance beef and its impact on our health as well as the impact of its production on the environment. The demand for beef worldwide is resulting in the destruction of a delicate ecological balance on earth – and this is now a big problem that needs reversing.\nThe knock on effect is that more and more people are choosing plant-based diets and among young people in particular, there is an increase in veganism.\nThe benefits of a plant-based diet are many and varied as such foods are rich in fiber, vitamins, and minerals. This can result in a decrease in both blood pressure and in low-density lipoproteins (bad cholesterol) that in turn reduce the risk of diabetes and help maintain a healthy weight. All of these can result in lessening the chances of heart disease.\nHowever, obesity is a huge global problem that is steadily increasing. In the USA, for example, obesity affects 78% of Hispanics, 76% of Blacks and around 66% of Whites over the age of 20. For the under 20’s the figure is 31%.\nThis is down to an imbalanced diet coupled with inactivity and a sedentary lifestyle where work (and leisure) involves sitting for hours on end looking at a screen. 60% of Americans don’t get the recommended amount of physical activity and 25% of those adults aren’t active at all.\nIn light of the rise in obesity amongst youngsters, a wellness movement called Children’s Healthcare of Atlanta launched a campaign they called ‘Strong4Life’, which makes improving family nutrition and physical activity habits fun and engaging. It also provides parents and caregivers the support they need to accomplish their goals.\nGretchen Reynolds of The New York Times says of physical activity, “You get prolonged life, reduced disease risk — all of those things come in in the first 20 minutes of being active. This is all that’s needed to reach the level where happiness and productivity in every day life peaks.”\nTop concepts associated with stills and motion showing a physically active lifestyle are Competition, Speed, Fun, Vitality, Adventure, Strength, Success, Skill, Determination and Effort.\nPeople are equating happiness with good health more than ever, as good physical and emotional health allows us to do the things in life that we’d like to do. This is the motivation to keep minds and bodies healthy\nIn a study of more than 10,000 participants from 48 countries, psychologists, Ed Diener of the University of Illinois and Shigehiro Oishi of the University of Virginia found that worldwide, people rate happiness as being more important than any other life outcome - including living a life with meaning, become rich, and getting into heaven!']"	['<urn:uuid:c7f1aefd-dcda-41ca-bfde-189ef04d50cb>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T06:04:36.674696	6	43	1022
73	How does Kongo graphic writing cross cultures and what role can technology play?	Kongo graphic writing crosses cultures through varied communication tools that spread from Central Africa to the Americas, particularly Cuba, enabling cultural transmission and collective memory. Technology through social networking tools can enhance this cultural exchange by allowing museums to share artifact information globally, facilitate knowledge sharing, and create participatory communication platforms for continued learning.	['Kongo Graphic Writing and Other Narratives of the Sign\nPublication Year: 2013\nAuthor Bárbaro Martínez-Ruiz, a practitioner of the Palo Monte devotional arts, illustrates with graphics and rock art how the Bakongo’s ideographic and pictographic signs are used to organize daily life, enable interactions between humans and the natural and spiritual worlds, and preserve and transmit cosmological and cosmogonical belief systems.\nExploring cultural diffusion and exchange, collective memory and identity, Kongo Graphic Writing and Other Narratives of the Sign artfully brings together analyses of the complex interconnections among Kongo traditions of religion, philosophy and visual/gestural communication on both sides of the African Atlantic world.\nPublished by: Temple University Press\nTitle Page, Copyright\nNo project of this scale is possible without the support of an incredible number of people, including friends, mentors, colleagues, and family. Such support has enabled me to research Kongo graphic expression as a form of communication in Africa and across the Atlantic, and then to bring ...\nKongo Graphic Writing is a study of structured visual expression among the Bakongo people in Central Africa and their descendants in Cuba. The book is built around the central argument that multiple, varied communication tools, including written symbols, religious objects, oral traditions, and body language, have consistently been integrated by the Bakongo into structured ...\n2. The Atlantic Passage: The Spread of Kongo Belief in Africa and to the Americas\nThe Bakongo people are found today in northern Angola, southern Democratic Republic of the Congo, southern Gabon, and the Republic of the Congo. A subset of broader Bantu culture that today stretches across much of eastern, central, and southern Africa, the Bakongo first settled in Central Africa as a result of larger migrations across the continent. It is generally be-...\n3. The Process of Meaning Making: The Kongo Universe\nComplex belief systems are used across cultures to help individuals and communities identify themselves and understand their place in the world. While varied in their substances and outward expression, such systems serve a similar purpose in that they create a narrative through which culture is formed and transmitted to later generations. This narrative engenders a ...\n4. Afro-Atlantic Graphic Writing: Bidimbu, Bisinsu, and Firmas\nThe term graphic writing systems can be credited to Gerhard Kubik. Building on existing scholarly work on particular writing traditions that facilitated an awareness of graphic expression in Africa,1 Kubik was the first scholar to study and explain in systematic terms graphic writing traditions.2 He argued that graphic writing must be understood as a “visual ...\n5. Beyond the Scripture: Physical Forms of Graphic Writing\nAlthough scholars have traditionally conceptualized graphic writing only in two-dimensional form, Kongo graphic communication is better under-stood as involving a wider range of forms, some two-dimensional, but others multi- or nondimensional. These varied types of communicative devices are bound together in a structured, consistent way in Bakongo culture and ...\nIntended to encapsulate my work on visual practices in Central Africa and the Kongo diaspora in Cuba, Kongo Graphic Writing and Other Narratives of the Sign has explored the extensive range of visual communication forms documented across Bakongo and Bakongo-descended communities in Central Africa and Cuba and demonstrated the systematic usage of such ...\nPage Count: 248\nPublication Year: 2013\nOCLC Number: 839438537\nMUSE Marc Record: Download for Kongo Graphic Writing and Other Narratives of the Sign', '1 SOCIAL NETWORKING TOOLS: EFFECTIVE KNOWLEDGE MANAGEMENT TOOLS IN AFRICAN MUSEUMS NAME OF AUTHOR: OKPALANOZIE OGECHUKWU ELIZABETH JOB TITLE: CONSERVATOR 1 HOME INSTITUTION: NATIONAL MUSEUM, LAGOS, NIGERIA, WEST AFRICA. P. M. B , LAGOS, NIGERIA ADDRESS OF AUTHOR: Knowledge management includes all the processes that are used in creating, disseminating and utilizing knowledge. Its main objective is the communication and sharing of enterprise knowledge between different people (Kavakali and Bakogianni, ). The objective of knowledge management systems is to support creation, transfer and application of knowledge in organizations (Alavi and Leidner, 2001). The museums in Africa are custodians of different types of artefacts, most of which are the tangible cultural heritage of the indigenous community. These cultural pieces were collected by the museums in different ways: purchase, seizure or gift. The acquired objects were documented before they were put in the storage area. Documentation of the artefacts entails recording the object s details in books and on index cards. Once this is done, the objects remain there and are never made available for public viewing unless they are brought out for exhibition or loan. Sometimes, the exhibitions are not done frequently. This traditional approach to documentation of museum objects in African museums does not allow the museums to maximise their potentials in educating the public which is one of the key responsibilities of a museum. Museums need to be more and more conscious of their functions and purposes to the public, not only of their objects and how they are placed, but also in the presentations of these objects and the physical spaces in which they exist (Ignjatovic, 2004). An effective way of educating the public in museums is the use of knowledge management (KM) tools. According to International Council of Museums (ICOM), a museum is a non profit making permanent institution in the service of society and of its development, and open to the public, which acquires, conserves and researches, communicates and exhibits for purposes of study, education and enjoyment, material evidence of people and their environment (ICOM, 2001). The definition implies that museums have to make the knowledge they have about the artefacts available to the public. The educative role of museum can be carried out by employing social networking tools in KM. KM practitioners use a wide range of IT tools to share, create, codify, and share knowledge. The trend in the development of Information Technology (IT) for organisations is toward more communication tools (Ghani, 2009). Although traditional method of documentation and storage of objects are in line with good museum practice, the public should not be denied complete access to these collections. This approach which is a common practice in African museums can be complimented with the use of social networking tools, a proactive approach to object documentation. A network is generally defined as a specific type of relation linking a defined set of persons, objects, or events (Mitchell (1969) cited by Kristina Groth. There is also a concept called computer supported social networks, which only includes relationships supported through computer environments, e.g., chat, news, and (Wellman et al., 1996). Social networking tools (SNT) can be defined as tools which are used for online interaction. They are interactive medium used to communicate by a group with common interest. SNT tools include facebook, twitter, blogs, myspace, youtube, flickr, podcasts, e.t.c. SNT can be used as good KM tool to share information about these objects with anybody who is interested in the objects. An audience interacts, create and share knowledge using social networking tools (Falk, 2000). Use of internet is one of the ways that SNT can be used in museums (Leonhard). The museums form social communities when it signs up in any of these SNT websites. The information about these rich collections in African museums can be shared with the global public by posting them on the pages of these websites. Presently, the documentation of objects in most African museums is done using the well - known traditional method of documenting. The acquisition of objets is followed by documentation. In this\n2 approach, the name of the object, catalogue number, mode of acquisition, provenance and vendor s name are all recorded in index cards and documentation books. Traditional method has its limitations. 1. Access to information about the objects can only be made by referring to the index card or the documentation book. 2. Objects must be seen physically in order to know how it looks. 3. Knowledge about the objects are limited to the information on the index card. The information about the objects is only in hard copy form and only accessible to museum staff. As a result of this, it cannot be accessed by more than one person at a time and it is not possible for people to access the objects from a distance. In recent times, museums do not only serve the purpose of keeping objects for storage and exhibition, they have moved a step further to imbibe knowledge management concept. Museum learning theories are intertwined with the notion of community of practice where the importance of learning is not only central to the individual but within a process of co-participation within a social context (Kelly et al., 2006). The usefulness of knowledge management as it concerns documentation is that the information about the objects in a museum is shared by all interested parties. This type of participatory communication encourages learning and helps museums to play their roles as educators. Most African museums are lacking in this aspect. The antidote to this lies in introducing digitization in documentation of objects and employing the use of social networking tools in KM. The first step in the use of SNT is putting all the information about the object in an online database (digitization). Online digitization of these objects will make them easily accessible to the public. With this, SNT can be used to share information about the objects. This practice is good because it widens the horizon of each participant about the object. This information includes the name, provenance, dimension and picture of the object. Other information like catalogue number, mode of acquisition and vendor should not be included for security purpose. With the aid of SNT, comments can be made about the objects and information about the objects can also be shared because nobody can boast about a monopoly of knowledge of these objects. The beauty of SNT is that it is interactive. Although each African museum has information about the objects in their custody, it may not be complete. The use of SNT as an interactive medium aids in gathering new information about these objects. However, this approach to KM has the disadvantage of amassing a pool of information which may not be correct or true. African museums have the authority and responsibility to investigate all the information being fed into the websites about the artefacts so as to ensure that false information are not passed on to the public. In doing so, authenticity of information may be guaranteed. Apart from sharing knowledge about the African objects through contributions using facebook, tweeters or blogs, enquiries about them can be made using these SNT. Some African museums are on facebook but they are not active. Museums remain slow to recognise their users as active cultural participants in many to many cultural exchanges (Russo et al., 2006). An interactive SNT must be active and not dormant. The museums themselves must be posting comments about the objects, events and activities of the museums in these SNT. Even though upcoming events and exhibitions do not fall into the class (group) of object documentation, these advertisements draw attention of the public to the museums. They are used to create awareness about these museums and to attract people to the museum. These events also help visitors and non museum staff to develop interest in the objects in the museum, be eager to learn (know) more about the objects and visit the social networking sites of the museums. There are other museological areas where documentation is paramount. A typical example is conservation. The practice in African museums is that prior to any interventive conservation work, photograph of the object is taken, the damage recorded and other information about the object are also documented. This information isl stored in a hard copy form. Some of the activities of the conservation department can also be made known online. SNT can also be used for this purpose and knowledge will be shared. PROACTIVE STEPS TO ENCOURAGE AND IMPROVE ON THE USE OF SOCIAL NETWORKING TOOLS IN KNOWLEDGE MANAGEMENT\n3 African museums should be encouraged to use SNT as a KM tool. The antiquities in these museums are masterpieces and very unique. They are seen and valued only by the people in the locality where they are kept, mostly museum staff and some visitors. The museum professionals in Africa should be enlightened on the importance of KM, the effectiveness of SNT in KM and the advantage of its use. This can be done through training, workshops and seminars. All these will not only help to showcase the objects in African museums and improve KM but will help them to be on the same platform with other museums in different parts of the world. African museums should be more responsive when using SNT. They should respond to comments and answers posted on their walls on facebook. The museums should also be encouraged to tweet and post comments on their blogsites. Apart from transmitting information about the objects on the SNT websites, African museums should advertise their exhibitions, conferences and seminars. Many of the African museums are object based and the activities in these museums revolve around the objects. Putting up such adverts will not only make the public to know what is happening in the museum but will also attract their attention to the museum. EXPECTED IMPACT OF APPLICATION OF SOCIAL NETWORKING TOOLS IN KNOWLEDGE MANAGEMENT OF AFRICAN MUSEUMS 1. The museum professionals will also learn from their audience. 2. Awareness about the objects will be created resulting in renewed interest in the artefacts and the museums themselves. 3. Awakened global interest in the objects will lead to more research, exhibition, training and workshops as it concerns these objects. 4. There will be up-to-date information about these objects. 5. The rich cultural heritage of Africa will be made known to the general public. 6. Information about the objects in the African museums can easily be accessed. SNT are good and effective KM tools in African museums. Their use in African museums will be of immense benefit to the museums: information will be shared, knowledge acquired and staff capacity greatly improved. Generally, the use of SNT in African museums will assist in improving the KM of these museums so that they will be valued and appreciated by all.\n4 REFERENCES Alavi, M. and Leidner, D. (2001). Knowledge Management and Knowledge Management Systems: Conceptual Foundations and Research Issues. MIS Quarterly. 25 (1) Falk, J. and Dierking, L. (2000). Learning from Museums: Visitor experiences and the making of meaning. Walnut Creet: AltaMira Press. Ghani, S. (2009). Knowledge Management: Tools and Techniques. DESIDOC Journal of Library and Information Technology. 29(6): Groth, K. Using social networks for knowledge management., Department of Numerical Analysis and Computing Science, Royal Institute of Technology, Stockholm, Sweden Kavakli, E. and Bakogianni S. (2003) Building Museum Information Systems - A Knowledge Management Approach, the 6th Hellenic European Research on Computer Mathematics & its Applications Conference (HERCMA 2003), E.A. Lipitakis (ed.), LEA Publishers, Athens, Greece,. 2: ICOM (2001)ICOM News Thematic Files: The definition of the museum. ICOM News, 57, (2). pp 4-5 Ignjatoviv D. (2004). Knowledge management Systems in museums: The next generation for assimilating museum information resources in an electronic environment.thesis for Masters in Master of Art. Seton Hall University Kelly, L., Cook, C. and Gordon, P. (2006). Building Relationships through Communities of Practice: Museums and Indigenous People. Curator. 49 (2): Leonhard, H. Application Areas of Knowledge Management in Museums. University of Applied Sciences in Information and Management. Austria. 2. Mitchell, J. C. (1969), The Concept and Use of Social Networks, in J. C. Mitchell, ed., SocialNetworks in Urban Situations, Manchester University Press, pp Russo, A., Watkins, J., Kelly, L. and Chan, S. (2006). How will social media affect museum communication? Nordic Digital Excellence in Museum Knowledge (NODEM 06). Oslo, Norway, http//:www.tii.se/v4m/nodem/nw_06/papers/papers.htm. 4. Wellman, B., Salaff, J., Dimitrova, D., Garton, L., Gulia, M. & Haythornthwaite, C. (1996), Com-puter Networks as Social Networks: Collaborative Work, Telework, and Virtual Community,Annu. Rev. Sociol. 22,\n5 NAME OF AUTHOR: OKPALANOZIE OGECHUKWU ELIZABETH JOB TITLE: CONSERVATOR 1 HOME INSTITUTION: NATIONAL MUSEUM, LAGOS, NIGERIA, WEST AFRICA. P. M. B , LAGOS, NIGERIA ADDRESS OF AUTHOR:\nSocial Media Marketing benefits for businesses Why and how should every business create and develop its Social Media Sites? This 2012 Master Thesis report will highlight the main business benefits of Social\nGUIDELINES FOR E-REFERENCE LIBRARY SERVICES FOR DISTANCE LEARNERS and other remote users by Ian M. Johnson Dr. Peter H. Reid Dr. Robert Newton with assistance from Graeme Baxter Claire Killen 20 May 2011\nAustralian Code for the Responsible Conduct of Research REVISION OF THE JOINT NHMRC/AVCC STATEMENT AND GUIDELINES ON RESEARCH PRACTICE AUSTRALIAN CODE FOR THE RESPONSIBLE CONDUCT OF RESEARCH [This Code\nCYBERSECURITY WORKFORCE DEVELOPMENT MATRIX RESOURCE GUIDE October 2011 CIO.GOV Workforce Development Matrix Resource Guide 1 Table of Contents Introduction & Purpose... 2 The Workforce Development Matrix\nTable of Contents To make it easier to use this file, there are links in the table below. Hover your mouse over the text and then select control and click to follow the link. Introduction 3 Responsibility\nDiploma Programme The Diploma Programme From principles into practice Diploma Programme The Diploma Programme From principles into practice Diploma Programme The Diploma Programme: From principles into\nCataloging and Metadata Education: A Proposal for Preparing Cataloging Professionals of the 21 st Century A response to Action Item 5.1 of the Bibliographic Control of Web Resources: A Library of Congress\nGreen Paper on Citizen Science Citizen Science for Europe Towards a better society of empowered citizens and enhanced research Green Paper on Citizen Science Citizen Science for Europe Towards a better\nQuestion/Answer Transaction Protocol Overview First Committee Working Draft NISO Committee AZ: September 22, 2003 NISO Committee AZ is developing a Question/Answer Transaction (QAT) Protocol to support\nABC of Knowledge Management Freely extracted from the NHS National Library for Health for the FAO as a knowledge organization initiative at http://www.library.nhs.uk/knowledgemanagement/ Creator: NHS National\nThe 1 st International Conference on Virtual Learning, ICVL 2006 147 Knowledge Management Audit of a SME in New Zealand Amitrajit Sarkar Schools of Business and Computing Christchurch Polytechnic Institute\nHIGH-SPEED INTERNET Understanding The Real Advantage MAXIMIZING HIGH-SPEED TECHNOLOGY TO ENHANCE PRODUCTION AND ENCOURAGE GROWTH IN RURAL AREAS. Document presented by MJSCONNEXION on behalf of the Chaboillé\nIntegrating information literacy into the curriculum Open University Library Services COBE Practical pedagogy series This booklet is part of the Practical Pedagogy series, which aims to promote good practice\nInformation and Communications Technology (ICT) in the Primary School Curriculum Guidelines for Teachers INFORMATION AND COMMUNICATIONS TECHNOLOGY (ICT) IN THE PRIMARY SCHOOL CURRICULUM: GUIDELINES FOR\nPROJECT FINAL REPORT Grant Agreement number: 212117 Project acronym: FUTUREFARM Project title: FUTUREFARM-Integration of Farm Management Information Systems to support real-time management decisions and\nUNIT 16: USING RESEARCH AND EVIDENCE 221 16 Using Research and Evidence to Inform your Teaching Ruth Heilbronn When you reach the end of your period of induction you will be looking forward to moving on\nUse of social media by the library current practices and future opportunities A white paper from Taylor & Francis OCTOBER 2014 2014 Taylor & Francis Group CC-BY-NC Why study social media in the library?\nIntegrating virtual mobility in international work placements Edited by Mariet Vriens & Wim Van Petegem Background: the EU-VIP project Table of contents 01 Context and definitions................................................\nTurning Point Network final evaluation report Annabel Jackson Associates Ltd June 2011 Part one: Main text Part two: Evidence Turning Point Network final evaluation report Part one: Main text Annabel Jackson\nSurvey report on Nordic initiative for social responsibility using ISO 26000 2013 Contents SUMMARY... 3 1. INTRODUCTION... 4 1.1 Objective of the survey... 4 1.2 Basic information about the respondents...\nSocial Networking: A Guide to Strengthening Civil Society Through Social Media DISCLAIMER: The author s views expressed in this publication do not necessarily reflect the views of the United States Agency\nInformation for registrants Continuing professional development and your registration Contents Introduction 2 About this document 2 CPD and HCPC registration: A summary of CPD and the audit process 2 CPD\nEffective practice with e-portfolios: How can the UK experience inform practice? Gordon Joyes School of Education, University of Nottingham, UK Lisa Gray Joint Information Services Committee, UK Elizabeth\nCategory: IT ducation 1349 -Libraries and Distance Learning Merilyn Burke University of South Florida-Tampa Library, USA IntroductIon With the explosion of distance learning, academic libraries have had']	['<urn:uuid:e46336fb-d2c5-4cbe-aad2-d9653ac7d8b3>', '<urn:uuid:331d1245-dc0f-4b1b-9ce1-02403a336a28>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T06:04:36.674696	13	54	3375
74	need help understanding metrics cost per customer vs lifetime value relationship mobile vs saas	For both mobile apps and SaaS businesses, the relationship between customer acquisition cost (CAC) and lifetime value (LTV) is crucial for profitability. In mobile apps, LTV should be greater than the cost per acquisition (CPA) - if it isn't, you're paying too much for customers. The mobile app LTV is calculated by multiplying average value of conversion × average number of conversions × average customer lifetime. For SaaS businesses, they typically aim for the customer lifetime value (CLV) to be at least three times more than their CAC. If the ratio is lower, it means they're spending too much on acquisition, while if it's higher, they're not spending enough. Both business types use these metrics to ensure their business model is viable and to properly manage growth.	"['How well is your mobile app really performing? These essential engagement and profitability metrics are the best way to find out.\nHow do you really find out if your mobile app is doing well?\nThe answer is simple: data.\nJust like any other marketing channel, mobile comes with a set of metrics that help marketers differentiate between success and failure.\nIt’s crucial to track and analyze these metrics because they help you understand how users interact with your app and why.\nThey can also help you discover if your app is making money.\nWhy is that necessary? Because you need to fully understand how your mobile app performs before you can improve it.\n“But there’s so much data out there – daily active users (DAU), monthly active users (MAU), return on investment (ROI) – what’s more important?”\nFor someone who is new at mobile app analytics, it’s easy to get confused. Especially with all those acronyms. But don’t worry, that’s why we are here to narrow it down for you.\nThis article will help you discover some of the key metrics you should track for your mobile app, along with steps on how to measure them.\nWe will cover:\n- Usage & Engagement Metrics\n- Profitability Metrics\n- The Golden Metric\nApp Usage and Engagement Metrics\nWith increasing competition in the mobile app market, it’s important for marketers to understand how people engage with their app and what makes them continue to use it.\nIn this section, we will cover some of the key usage and engagement metrics you need to track for your mobile app.\n1. Downloads and Installs\nYou should always know how many people have installed your app.\nHowever, keep in mind that this metric alone will never be able to tell you about the real success of your mobile app. What if users downloaded your app because of a certain marketing campaign and never used it again or uninstalled it right away?\nStill, the number of app installs is important because it lays the foundation for all other metrics you should track and analyze.\nHow to Track App Downloads and Installs\nYou can easily set up app install tracking using Google Analytics for both Android and iOS. Here is a step-by-step guide on how to do that.\n2. App Acquisition\nSo, you know how many people download your app. But, do you know where those downloads came from? That’s what app acquisition tells you.\nApp acquisition shows the effectiveness of your marketing campaigns and channels and will help you make the most of your marketing spend.\nFor example, if you’re running multiple ads or campaigns, this metric will help you find what’s working and what’s not, so you can focus on investing in the problem areas.\nHow to Track App Acquisition\nHere’s how Mixpanel typically categorizes the campaign source of your users:\nThe graph shows how app users found your app – Facebook ads, Twitter ads, AdMob, Brightroll, Tapjoy.\n3. Active Users\nNot all people who download your app are active users. Active users regularly open and engage with your app.\nThis metric is important because it shows how engaging and useful your app is to users.\nSo, what classifies a user as “active”? It depends on the analytics tool you use.\nAccording to Google Analytics, an active user is one who has opened and interacted with an app within the last 30 days.\nActive users can be segmented into Daily Active Users (DAU) and Monthly Active Users (MAU).\n- DAU measures the number of users who have a session with your app at least once a day\n- MAU is the number of users who have a session at least once a month\nHow to Track Active App Users\nYou can measure DAU and MAU using most analytics tool.\nHere is how Google Analytics typically shows active mobile app users:\nThe dark blue line shows 7-day active users, or the number of unique users who initiated sessions on your site or app over the last 7 days. The light blue line shows 14-day active users.\nThe stickiness ratio shows how often users come back to your app.\nTo calculate stickiness, you just divide the DAU by the MAU. Multiply the result by 100 to get a percentage.\nHow to Track App Stickiness\nStickiness = (Daily Active Users / Monthly Active Users ) x 100\nThe higher the percentage, the better. It means you have more MAUs sticking around long enough to become DAUs.\n5. Average Daily Sessions Per Daily Active User (DAU)\nAverage daily sessions per DAU shows how many times on average users open and engage with your app in one day.\nSo, what’s a good number of average daily sessions per daily active user? It depends.\nSocial media apps such as Facebook, Instagram or WhatsApp want their users to have a higher number of daily sessions on average.\nOther apps that don’t need to be used as much to be successful may be happy with a lower number.\nHow to Track Average Daily Sessions Per DAU\nAverage Daily Sessions Per DAU = Number of Daily Sessions / Number of DAU\n6. Average Session Length\nThe average app session length tells you how much time a typical user spends interacting with your app in a single session.\nThis metric is important because it tells you how engaged your users are. For example, if you have an e-commerce app, are users browsing products for longer periods of time? Or, do they close the app after just a few seconds?\nHow to Track Average App Session Length\nYou can track average session length using Google Analytics or any other analytics tool. Here is how to use Google Analytics to track average app session.\n7. Screen Flow\nScreen flow analysis breaks down how users interact with your app, screen-by-screen.\nHere is what a typical report looks like:\nFor example, you can track exits by screen, find out the navigation path users take through different screens, and the total number of visits for each screen.\nScreen flow highlights the problem areas in your app, such as the screen users interact with before they exit your app.\nYou can use this data to improve the problem areas and re-engage lost users.\nHow to Track App Screen Flow\nGoogle Analytics is a great tool to help you track screen flow.\n8. Retention Rate\nApp retention rate measures how many users return to your app after using your app at another time. Simply put, retention measures how many users you retain after a set period of time.\nPeople get bored easily. Retaining app users over time is one of the biggest challenges on your way to success.\nThis metric also may be useful if you implement new features or release a new update. It can help you find out if the new updates or features are keeping users engaged or driving them away.\nHow to Track App Retention\nApp Retention Rate % = (Number Users Retained at End of Time Period / Total Users at Start of Time Period ) x 100\nYou can also check out this step-by-step guide on how to measure retention with Google Analytics.\n9. Churn Rate\nApp churn rate is the opposite of retention rate.\nIt tells you how many users stopped using your app after a given time.\nHow to Track App Churn Rate\n1 – Retention Rate = Churn Rate\nTrack how much money you make with your mobile app.\nHere are some key profitability metrics you should track for your mobile app.\n10. Average Revenue Per User (ARPU)\nThis metric tells you how much revenue is being generated on average by each user in the form of subscriptions, in-app purchases, paid downloads, ad clicks, or any other app monetization channel.\nThis figure is important because it tells you the overall value a single user brings to your app.\nHow to Track Average Revenue Per User (ARPU)\nAverage Revenue Per User (ARPU) = Total App Revenue in Given Time / Number Users in Given Time\n11. Cost Per Acquisition (CPA)\nIn the beginning, it might seem like the total number of downloads or conversions are enough to tell you how well your app is performing.\nBut you need to know how much acquiring new users costs as well.\nHow to Track Cost Per Acquisition (CPA)\nCost Per Acquisition (CPA) = Total Cost of Campaign / Total Acquisitions or Conversions\n12. Return On Investment (ROI)\nROI measures the money you make (return) on the money you spend building and marketing your app.\nHow to Track App ROI\nApp Return on Investment (ROI) % = (Gain From investment – Cost of Investment ) / Cost of Investment) x 100\n13. Lifetime Value (LTV)\nApp lifetime value (LTV) helps you identify how much value each customer brings to your app.\nRemember, your LTV should always be greater than your CPA. If it isn’t, it probably means you are paying too much for your customers.\nThe average number of years a customer stays with you, also known as the average customer lifetime, varies depending on the type of customers you have.\nFor example, if your target customers are aged 18-24, they might grow out of that age group after 4 or 5 years on average.\nHow to Track App Lifetime Value (LTV)\nApp Lifetime Value (LTV) = Average Value of Conversion x Average Number of Conversions x Average Customer Lifetime\n14. The ‘Golden’ Metric\nNo matter how many metrics you track, the app star rating is by far the most important (and obvious) one. You should focus on ways to increase mobile app reviews.\nPeople almost always check an app’s rating before they decide to download it.\nRegardless of how much you invest in all other aspects of your app, if it’s poorly rated on the App Store or Google Play, it will end up driving potential users away.\nFocus on the Metrics That Matter\nGoing through mobile analytics might seem easy, but you need to really understand what you’re looking for before you dive into the data.\nBe clear on the goals you set for your app, and don’t waste too much time on metrics that don’t matter. Try to focus only on the key areas that help you achieve your goals.', ""Top 10 SaaS Growth Metrics in 2023\nPlanning ahead to expand on these marketing strategies can give you an edge over the competition by allowing you to craft a plan that will deliver measurable results.\nOver the past few years, the SaaS industry has exploded onto the scene and won’t be slowing down anytime soon. The highest performing companies continue to grow exponentially, more doubling their teams and initial public offerings (IPOs) per year.\nAs SaaS companies continue to expand so rapidly, the possibility of market saturation does too. For that reason, it’s imperative that they are paying attention to the right growth metrics.\n- The rapid expansion of the SaaS industry necessitates a focus on the right growth metrics.\n- SaaS growth metrics are distinct due to recurring revenue models and emphasize customer retention.\n- The top 12 SaaS growth metrics include churn rates, customer lifetime value, customer acquisition cost, months to recover CAC, and monthly recurring revenue.\n- Also important are MRR growth rate, quick ratio, CAC : LTV ratio, customer engagement score, and customer health score.\n- Continuous tracking and analysis of these metrics are crucial for effective SaaS marketing strategies.\nWhat is a SaaS Growth Metric?\nThe key SaaS growth metrics are vastly different from metrics of other businesses that are one-time sales and transaction based. Primarily due to the fact that revenue is collected over time rather than upfront. This means that there should be a strong focus on retention.\nA strategic SaaS growth model should not only center around customer retention rates, but also be able to answer the following questions:\n- What is working well?\n- Is my business financially operable?\n- What areas need improvement?\n- What areas should be focused on to accelerate growth?\nThe growth metrics that matter most can be compared to the body’s temperature, heartbeat, and blood pressure. Much like a routine check-up at the doctor’s office, it’s important that SaaS companies are regularly monitoring these growth metrics to keep an eye on the health of their business.\nIt can be confusing to know which analytics will actually help you evaluate your growth accurately and which ones will not. In this blog post, we’ll discuss the top 12 SaaS growth metrics that will help you analyze the momentum, or your business’s ability to grow and keep growing, and make the right decisions to help your company expand.\n1. Churn Rates\nThe first SaaS growth metric we’d like to highlight is customer churn rate. This is a measurement of how much business you’ve lost within a certain amount of time and it is an essential aspect of monitoring the day-to-day vitality of your SaaS business.\nChurn metric provides you with specific insight on customer activity across a certain date or time period to help you better understand your customer retention rates.\nIf you only focus on driving new customers to your business, you won’t get too far as an SaaS. Since most SaaS businesses are based on annual subscriptions, it is just as important to maintain your existing customers. A certain amount of customer or revenue churn is inevitable, but tracking it can help you stay ahead of it, and identify any issues early on that might be pushing your customers away.\nIn order to effectively track your churn rates on a monthly or quarterly basis, it’s crucial to not only look at the customer count, but also any definitive information about the customers that might identify the reason why they decided to not renew their subscription. This information might include their demographic, job title, industry or other persona details. Be sure to discuss these characteristics across all departments, including marketing, sales and customer service to align goals.\nThere are multiple ways to measure your customer churn rate. User churn is calculated as the percentage of customers that were lost during a given time frame, while revenue churn is the percentage of revenue lost to churn during a given time frame.\nEither one of the two following churn rate calculations can be used to monitor growth. However, many companies will argue revenue churn is more crucial to focus on due to the fact that revenue is the ultimate goal.\nUser Churn = (Cancelled Customers in the last 30 days divided by Active Customers 30 days ago) x 100\nRevenue Churn = (MRR Lost to Downgrades & Cancellations in the last 30 days divided by MRR 30 days ago) x 100\n2. Customer Lifetime Value\nNext on the list as an essential metric to effectively evaluate your SaaS growth rates is your customer lifetime value, or CLV. This describes the average amount of money that your customers pay throughout their engagement with your business.\nThis SaaS growth metric will provide your company with a true representation of your growth and can be explained in just a few steps.\n- First, find your customer lifetime rate. To calculate your customer lifetime rate, you want to divide the number 1 by your customer churn rate (see above). For example, if your monthly customer churn rate is 1%, you would divide 1 by 0.01, which would give you a customer lifetime rate of 100.\n- Second, find your average revenue per account (ARPA). This is calculated by dividing your total revenue of a certain time frame by the total number of customers. For example, if your monthly revenue is $100,000 and your total number of customers is 100, your ARPA would be $1,000.\n- Finally, find your customer lifetime value. This is calculated by multiplying your customer lifetime rate by your ARPA. Continuing with the examples above, you woud multiply 100 (customer lifetime) by $1,000 (ARPA) and discover that your customer lifetime value is $100,000.\nUnderstanding your CLV gives you insight into what your average customer is worth. It can also work as a way to display value of your company to investors if your business is in the early stages and still considered a startup.\nBecause most SaaS businesses are based on subscription-based models, each renewal has the ability to yield another year of recurring revenue. This increases the lifetime value per customer.\n3. Customer Acquisition Cost\nAs you build your SaaS growth strategy, another key metric to pay attention to is your customer acquisition cost, or CAC. This will tell you exactly how much it costs your business to acquire new customers and the amount of value they bring to your business. Customer acquisition costs works hand in hand with CLV to help SaaS companies ensure that their business model is viable.\nIn order to calculate your CAC rate, you need to divide your total sales and marketing spend by the total amount of new customers added during a certain time period. Be sure to include personnel expenses. For example, if your monthly expenditures come out to a total of $100,000 and your acquired customers come out to a total of 100, your CAC would be $1,000.\nThis particular SaaS growth metric is especially important for newer companies to focus on. It allows you to manage your growth and correctly gauge the value of your acquisition processes.\n4. Months to Recover Your CAC\nTo piggyback on the importance of CAC, it is also helpful to analyze the amount of time it takes to recover your total customer acquisition costs. This metric allows you to understand how quickly a customer begins to generate revenue for your company. The goal is for this number to decrease over time as your business grows.\nTo accurately calculate this metric, you can follow this equation: CAC divided by MRR x GM. In other words, divide your CAC by your total monthly-recurring revenue (see below) and your gross margin.\n5. Monthly Recurring Revenue (MRR)\nThe recurring nature of the typical subscription-based payment model makes it somewhat simple to track and gauge revenue. Monitoring your monthly recurring revenue (MRR) allows you to work out the amount of predictable revenue your customers generate each month.\nSaaS revenue run rates are guesses, at best. For example a SaaS platform that raised Series A of $10M, but is still pre-revenue stage needs to throw out big numbers for investors. While dozens of factors make-u a composite of monthly recurring revenue, it is a very core financial metric to track religiously in order to properly manage the month-over-month revenue growth of your business and boost momentum. When appropriately paired with your SaaS customer acquisition costs (CAC) and your customer churn rates, your SaaS monthly recurring revenue (MRR) can be used to anticipate and predict future revenue.\nTo successfully calculate your MRR for any given month, you simply add the recurring revenue generated by that specific month’s customers. The next few metrics we’ll be covering dive a little bit deeper into how MRR can help you manage your growth.\n6. MRR Growth Rate\nOnce you’ve calculated your MRR, you can then measure your SaaS MRR growth rate and evaluate the improvement of your revenue generation over time. For SaaS platforms, monthly recurring revenue growth rate is a percentage of your revenue run rate. For example, if your business generates $1,500 in MRR in March and then $2,000 in April, your MRR growth rate will be about 33%.\nTo maintain a steady MRR growth rate, your business needs to continuously generate more revenue each month. Therefore, if your MRR growth rate is steady, this is indicative of rapid, augmented growth.\n7. Quick Ratio\nThis next SaaS growth metric, known as the quick ratio, is the measurement of a business’s growth efficiency. Quick ratio allows you to understand how reliably your business can grow its revenue numbers given its current churn rate.\nTo accurately calculate your company’s quick ratio, you must divide your gained MRR by your lost MRR. If you have a quick ratio that is above 1.0, your company is growing. If it is under 1.0 however, this means your company is not. The bottom line is the higher your quick ratio, the healthier your company’s growth is going to be.\nQuick Ratio = (New MRR + Expansion MRR + Reactivation MRR) ÷ (Contraction MRR + Churned MRR)\nWhen calculating your quick ratio, it is important to note that all MRR is not created equal. There are different types of MRR that contribute to your MRR growth and understanding each of these types is key. The following is a list of the different MRRs to consider.\n- New MRR: MRR from new customers\n- Expansion MRR: MRR from existing customers (upgrades)\n- Reactivation MRR: MRR from churned customers who reactivated their account\n- Contraction MRR: Lost MRR from existing customers (downgrades)\n- Churned MRR: Lost MRR from canceled customers\nFor example, if a company has $10,000 in MRR growth, this might be a combination of any of the types of MRR listed above. The quick ratio helps you comprehend the difference in growth efficiency between them and provides the most straightforward picture of your business’s health.\n8. CAC : LTV Ratio\nAnother important calculation to include in your SaaS strategy to monitor your SaaS growth rates is your CAC to LTV ratio. This is a single metric that displays the lifetime value (LTV) of your customers and the total amount of capital that you spend to acquire them. It can point to the health of your SaaS marketing program as a way to show you what is working and what isn’t, so you can know what campaigns to invest in or change.\nTo find your CAC to LTV ratio, simply compare the two calculations. Generally speaking, your SaaS company should have an LTV that is at least three times more than your CAC. If it is lower, that could mean you’re spending too much money. If it is higher, you’re not spending enough.\nCustomer engagement scores allow you to see how likely a customer is to churn. It shows you how often they log in, what they use your software for and any other metrics pertaining to their engagement with your product.\nThe scales vary from business to business, depending on how a typical customer uses the software. In order to build your own SaaS customer engagement score, you should create a list of inputs and value assignments that predict a customer’s satisfaction and longevity. Start by looking at your happiest and longest-lasting customers.\nWith a clear list, you can calculate an overall client engagement score for your customers. This will allow you to efficiently evaluate customer health from one data point.\n10. Customer Health Score\nAnother metric that is similar to your customer engagement score is your customer health score. This helps your front-line customer service managers understand the health of a customer’s relationship with your company and if there are any issues that might indicate that it is at risk.\nCustomer heath scores can be generated by using a SaaS specific customer success tool that includes predictive customer analytics and forward looking client engagement trends. Health scoring assigns different values to different signals of customer longevity or churn. It also provides your customer-facing team members with insight into how their customers are doing so that they can properly engage with any users that might be at risk - before they cancel or fail to renew their subscription.\nThe SaaS growth metrics above are essential to any SaaS marketing strategy. It is important to continuously track, measure and report on these growth metrics at the various SaaS growth stages. Working with a leading SaaS marketing agency can help you stay up to date with these growth metrics and effectively put them into action.\nFrequently Asked Questions\nWhich SaaS growth metrics are essential for assessing customer acquisition?\nKey customer acquisition metrics include Customer Acquisition Cost (CAC), Customer Lifetime Value (CLTV), and the Customer Acquisition Rate.\nHow can I measure customer retention and satisfaction in SaaS?\nCustomer retention metrics like Churn Rate, Net Revenue Retention, and Customer Satisfaction (CSAT) surveys help gauge customer loyalty and satisfaction.\nWhat are expansion revenue metrics, and why are they significant in SaaS growth?\nExpansion revenue metrics, such as Upsell Rate and Expansion MRR, measure the revenue generated from existing customers, indicating the potential for upselling or cross-selling.\nWhat is Monthly Recurring Revenue (MRR), and how is it calculated?\nMRR is a fundamental SaaS metric representing the total recurring revenue a company generates each month. It's calculated by summing up all subscription revenue during a specific month.\nHow can I track SaaS growth in terms of product usage and user engagement?\nMetrics like Active Users, Daily or Monthly Active Users (DAU/MAU), and User Engagement Rate provide insights into how customers are using your SaaS product and its impact on growth.\nWhat are SaaS growth metrics, and why are they important?\nSaaS growth metrics are key performance indicators (KPIs) used to measure and evaluate the growth and success of a SaaS company. They are crucial for tracking progress, making data-driven decisions, and attracting investors.\nYou May Also Like\nThese Related Stories""]"	['<urn:uuid:575bf188-466d-4c6d-a63a-84337bb0bf20>', '<urn:uuid:9ef06023-7363-47d4-b81a-60549e486cb2>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-13T06:04:36.674696	14	127	4206
75	best ways remove fish tank red goo and what causes it to show up	Red slime (cyanobacteria) is actually a bacteria, not algae, that appears as dark red sheets covering aquarium surfaces. It grows due to two main factors: Dissolved Organic Carbon (DOC) and lighting conditions. To remove it, you need to control DOC through frequent water changes, good water movement, and using a protein skimmer. High phosphates and nitrates from food, tap water, or decaying matter contribute to its growth. Lighting also plays a role - bulbs that are below 10,000k or aging lights that shift toward the red spectrum can promote growth. The lights should be replaced every 6-8 months and operation time should be reduced to 8 hours until the red slime is under control.	"['Free shipping for orders over $75\nThe common name for cyanobacteria is ""red slime"" algae. Red slime algae is actually a bacteria and not an algae. Cyanobacteria means blue-green algae and comes from its color yet there are many other colors of cyanobacteria. The one color of cyanobacteria that aquarist are most concerned with is red. Red slime algae can start out in small patches and then combine to cover a large area.\nRed slime algae needs light and nutrients to grow.\nHaving the lights that are below 10,000k can contribute to the growth of red slime algae. As metal halide lights age they tend to lean toward the red spectrum and loose their intensity and should be replaced every 6 to 8 months for optimal efficiency. Another factor that can contribute to the growth of red slime algae is the operating time of the aquarium lights. If the aquarium is being lit 10 to 12 hours a day, you may want to consider turning them back to 8 hours until the red slime algae is under control..\nThe Dissolved Organic Compounds in the marine aquarium is the biggest contributing factor to red slime algae. Phosphates (PO 4) and nitrates (NO 3 ) are the two thing that need to be most looked at in removing red slime algae. Take the phosphates and nitrates out of the marine aquarium and you will starve the cyanobacteria out!\nPhosphates (PO 4) are commonly introduced into aquariums by different means and the following should be looked at as a way to reduce DOC’s. Using unfiltered fresh tap water, inadequate R/O system, or filters in the R/O being worn out should be one of the first places to look at for phosphates. A TDS meter (Total Dissolved Solids) should be used to check your R/O system for any problems and problems found should be corrected before continuing.\nMany aquarium products may contain some phosphate but a big source for phosphates in the aquarium is the foods for feeding the marine fish. Cutting back on your feeding may help reduce these phosphates. If you’re not feeding too much to the fish and your water source is good, you may want to reduce the bio-load in the aquarium or look at other means of removing phosphates. You can use a phosphate reactor to lower phosphates but don’t forget about the nitrates. You will need to lower your nitrates to be completely successful in getting rid of red slime algae.\nNitrates are the final byproduct produced in the nitrogen cycling process. Allowing excess DOCs to accumulate in an aquarium can raise nitrates (NO 3) to a high level. Water changes and removing detritus can help with bringing nitrates down. If that does not work you may want to look at replacing the substrate. If the substrate is old it can hold a lot of nutrients that the red slime algae will feed on. Having a good current flow in the aquarium will help prevent detritus from building up on substrate in low current flow areas in the future. Good current flow can also help the good bacteria in the substrate to break down nitrates better.\nAnything rotting or dying in the aquarium will also contribute to DOC’s and should be removed quickly. Uncured live rock and food will fall in the category and with the food, having a clean-up crew like crabs and snails to remove the un-eaten food will help with water quality. Some snails will even eat red slime algae!\nAdditional equipment and products to be considered for reducing DOC’s:\nSump and additional Live rock\nPhosphate media and reactor\nRemoving red slime algae from the tank will not be a quick process and may take a month or two to turn around. Don’t be tempted to use some of the red slime algae removing products. These products cure the problem quickly but are only temporary because they do not fix the problem. Some products can also be harmful to the reef inhabitants and the much needed good bacteria.', 'Please welcome back Desiree Leonard with another “What’s this and What do I do?” article.\nThis frequently encountered problem is Cyanobacteria or “Slime Algae”.\nThe name “slime algae” is a misnomer. Because Cyanobacteria are photosynthetic and aquatic, they are often called “blue-green algae”. In reality they are NOT algae, but something more in between algae and bacteria. Cyanobacteria are bacteria that manufacture their own food and live in colonies — large enough for you to see them! It’s these colonies that cause trouble for aquarists. They are not necessarily blue-green but can be black, green, blue green, and the familiar dark red sheets covering many surfaces in an aquarium.\nThe first thing aquarists who find an unwanted colony of cyanobacteria in their aquarium want to know is how to get rid of it. Well, this is where it gets tricky. To eradicate the problem – the particular trigger for the cyano bloom must be identified and treated. Not every bloom is in response to the same trigger and while throwing a chemical at the problem will perhaps clear it up temporarily, it will come back, and it will be worse. (More on this later.)\nAs with all types of algae, any uncontrolled growth indicates an imbalanced system. An imbalance in one or both of two main triggers can set off a cyano bloom.\n• DOC – Dissolved Organic Carbon is a food source of the bacterial side of the bacteria-algae. Sources of dissolved carbon include: fish slime, algae, bacteria, digested/uneaten food, metabolic waste, live food, some aquarium additives etc.\n• Lighting – The food source for the algal side of the bacteria-algae is light. Light bulb spectra shift to red as they age, resulting more favorable conditions for photosynthesis to take place more vigorously.\nNote: It is said that slime is caused from phosphates and silicates in the water. It’s true that these 2 elements will certainly grow algae of all sorts, but if removed will not reduce or remove a slime problem.\nOkay – so what DOES remove the problem?\n• Control your DOC. This is best done by frequent water changes, good water movement (power heads and closed system circulation) and (this is important!) a good protein skimmer. An undersized or ineffective protein skimmer, high waste loads, or a combination thereof will increase the dissolved carbon level. As a rule of thumb for skimmers; buy one that is rated for at least twice the size of your tank. It may take some adjusting but a properly functioning skimmer can remove ½ cup of thick organic scum from a tank a day.\n• Use an RO/DI filtering system (Reverse Osmosis/Deionization) for water changes whenever possible. This eliminates adding DOC into your tank via tap water.\n• Add more lighting or change your bulbs. Change bulbs at least once every 9 to 12 months, don’t wait till they burn out. To be more cost effective, you can stagger your replacements rather than replacing them all at once, but if the slime persists you may have to go all out and do full replacement.\n• Watch what you feed. Feed once a day. If you wish to feed twice, simply split the amount in half – don’t feed twice as much food. If you feed grocery store bought seafood or are making your own foods, rinse all foods thoroughly as seafood sold for human consumption is treated with phosphates and preservatives to keep it fresher longer. (It’s true!) Avoid flake foods, these dissolve too fast – pellets and crisps are much better and more palatable.\n• If you aquarium is freshwater, the above treatments still apply, but a protein skimmer is not used. Water circulation, frequent water changes, extra charcoal filtration and changing lights all will be effective controls.\nDisclaimer: I am in no way saying that if you have a slime outbreak, that you are a bad aquarist and your water is swill. Even in the best kept tanks there are still cyanobacteria. You will, in fact, see outbreaks in systems which are free of phosphate and silicate; they also have new halides, actinics and great water flow. There is always another factor – vitamin supplements, liquid foods, and other additives can add the organics that can trigger a cyano explosion. Look for anything different you are doing and stop doing it.\nRight – that covers the long term, not so easy fix. But for those who still want a quick fix, there are products that are available to help remedy the problem. BUT – if the underlying issue is not addressed, don’t say I didn’t warn you…..\n• Cyano is a gram negative (thin cell membrane) bacteria, much like most bacteria in the aquatic environment. A dose of Erythromycin will knock out the colony of slime quite quickly. However, since the nitrifying bacteria you need in your tank are gram negative as well, they will be affected also, either being killed or severely damaged. This treatment is more advisable in freshwater aquaria, but only with careful attention paid to water quality while treating. Like all antibiotics, if dosed frequently the cyano will develop a resistance.\n• Chemi-Clean by Boyd Enterprises and Red Slime Control by Blue Life are highly effective reef safe treatments for slime. These are non-antibiotic formulations and will do less damage to your biological filter. If used frequently however, there is still a chance of the cyano developing a resistance.\nI hope this info is helpful in your endeavors to keep a slime free tank. For more and more thorough information, check out these links! Happy Fish keeping!']"	['<urn:uuid:9ec37541-0cdd-4123-a86d-419b4402e16a>', '<urn:uuid:37f3d9bf-d4a0-41cc-90f3-e0d4c48f9448>']	open-ended	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T06:04:36.674696	14	114	1605
76	spotify bot automation vs traditional label control	There is a notable contrast between automated streaming manipulation and traditional label control over music distribution. While bots like SpotiBot can automate plays and potentially manipulate Spotify's royalty system by playing tracks repeatedly beyond the 30-second threshold, traditional labels maintain control through formal agreements and infrastructure. Labels own the masters when artists are signed to them, handle distribution, marketing, and promotion, and are responsible for accounting and issuing royalty statements. They recoup their investments in advances, recording costs, and promotion before passing down proceeds to artists. This traditional system is being challenged by the possibility of streaming fraud, where bots can approximate human listener behavior, potentially undermining the economic revenue models of streaming services.	['Producing and coding bot ‘listeners’ has today become almost as easy as automated music production has been for years. Machines can thus both ‘create’—and ‘listen’ to ‘music’ (whatever we mean by these categories). In fact, such notions are capricious within the contemporary streaming music landscape. The project, “Streaming Heritage: Following Files in Digital Music Distribution” (financed by the Swedish Research Council) studies emerging streaming media cultures in general, and the music service Spotify in particular, with a bearing on the digital challenges posed by direct access to musical heritage. Rediscovering older music is key for Spotify, and the project is hence on the one hand geared towards investigating the institutional challenges of streaming musical heritage, and on the other hand—and foremost—to develop new digital research methods. Situated at HUMlab (Umeå university) part of the project is essentially about Turing testing Spotify. Building on the tradition of ‘breaching experiments’ in ethnomethodology, my research group seeks to break into the hidden infrastructures of digital music distribution in order to study its underlying norms and structures. The key idea is to follow files’ (rather than the people making or using them) on their distributive journey through the streaming ecosystem. The setting include the distribution and aggregation of self-produced music/sounds through Spotify; the set-up of our own record label (for research purposes); the programming of bots to inform, explore, mimic, and ultimately subvert notions of usage and listening; the tracing of Spotify’s history through constantly changing interfaces (web archiving and documenting these). Research questions range from various way how streaming music is commodified? What sounds are perceived as music (or not) according to Spotify and adjacent aggregating services? How is metadata generated, ordered, and valued—and what kind of metadata is actually available? What normative world views are promoted and materialized by streaming architectures? What kind of infrastructures proliferate behind the surfaces of on-demand services?\nMy presentation departs from the fact that one-fifth of Spotify’s catalogue of 30 million songs haven’t once been listened to at all. Under the computational hood of streaming services all streams are equal, and every stream thus means (potentially) increased revenue from advertisers. Spotify is hence likely to include—rather than reject—various forms of (semi-)automated music, sounds and (audio)bots. At HUMlab we therefore set up an experiment—SpotiBot—with the purpose to determine if it was possible to provoke, or even to some extent undermine, the Spotify business model (based on the 30 second royalty rule). Royalties from Spotify are only disbursed once a song is registered as a play, which happens after 30 seconds. The SpotiBot engine was be used to play a single track repeatedly (both self-produced music and Abba’s ”Dancing Queen”), during less and more than 30 seconds, and with a fixed repetition scheme running from 10 to n times, simultaneously with different Spotify account. Based on a set of tools provided by Selenium the SpotiBot engine automated the Spotify web client by simulating user interaction within the web interface. From a computational perspective the Spotify web client appeared as black box; the logics that the Spotify application was governed by was, for example, not known in advance, and the web page structure (in HTML) and client side scripting complex. It was not doable within the short experiment to gain a fuller understanding of the dialogue between the client and the server. As a consequence, the development of the SpotiBot-experiment was (to some extent) based on ‘trial and error’ how the client behaved, and what kind of data was sent from the server for different user actions. Using a single virtual machine—hidden behind only one proxy IP—the results nevertheless indicate that it is possible to automatically play tracks for thousands of repetitions that exceeds the royalty rule. Even if we encountered a number of problems and deviations that interrupted the client execution, the Spotify business model can be tampered with. In other words, one might ask what happens when—not if—streaming bots approximate human listener behavior in such a way that it becomes impossible to distinguish between a human and a machine? Streaming fraud, as it has been labeled, then runs the risk of undermining the economic revenue models of streaming services as Spotify.', 'Disclaimer: This information is brought to you by Debbie Egel, an attorney whose practice includes writing and reviewing music contracts, running an independent label for over 10 years, and developing indie artists. — She is knowledgeable of the economics of music, the DIY process, and has written an instruction manual for Indie artists, labels and managers called, “For The Record”, and teaches an online course. Debbie has a deep appreciation of the business of music as well as her legal knowledge as a practicing attorney. Always consult with your personal legal and tax professionals regarding your specific situation before making any decisions.\nWhere Does The Money from Master Recordings Come From?\nBefore we dive in, let’s talk basics.\nWho Owns The Master?\nIt comes down to two easy aspects. If an artist is independent, they own their own masters. If they’re signed to a label, the label owns them.\nTraditional labels produce, distribute, market and promote music. They prepare legal agreements and are responsible to account and issue royalty statements that have been earned for each record.\nIf they want to, independent artists can act as their own label. However, there is a benefit to the labels use of their extensive expertise to develop their artistry and brand value for them. It takes time, money and resources to market and promote an artist.\nThere is a symbiotic relationship between the label and artist. Artists are able to successfully fulfill their dreams while record labels invest and profit from them. Although you can do it independently, that doesn’t mean you can do it alone.\n- Check out, “How To Know If Signing a Recording Contract Is Right For You” to learn more…\nRecord labels want a partner. They rely on signing talent to the label to grow and make money. Labels win when the artist’s wins, and recording agreements are what allow an artist and a label to work together.\nArtists sign a legal contract with deal terms they agree to which allows the label to navigate and grow the artists career.\nIn a traditional arrangement, income flows from a variety of sources. It comes from things like:\nFirst, the label will need to recoup their investment. This may include any advances, recording costs, touring and promotion and other fees. After the expenses are deducted, the artist receives the percentage that was legally agreed upon in the contract.\nSharpen your skills…\nWhat About Commercially Released Music? — Where does that money come from?\nSales & Streaming // Once your music is commercially released artists and labels have the ability to earn income thru sales and streaming royalties. — Here’s how it works…\n- Record labels give their music to the distributor who then delivers to stores (digital and retail).\n- The distributor keeps a small portion of the income earned from sales and streaming.\n- Then, they pass down the proceeds to the label who also keeps a small portion.\n- Then, more money is passed down to the artist in the form of “royalties”, only after the label “recoups” their investment.\nIn 2020, here’s what those break down to…\nTidal // $0.019 per stream // 1,000,000 streams = approx. $19,000.00\nApple Music // $0.01284 per stream // 1,000,000 streams = approx. $12,840.00\nDeezer // $0.00676 per stream // 1,000,000 streams = approx. $6,760.00\nSpotify // $0.0064 per stream // 1,000,000 streams = approx. $6,400.00\nAmazon // $0.00437 per stream // 1,000,000 streams = approx. $4,370.00\nPandora // $0.00402 per stream // 1,000,000 streams = approx. $4,020.00\nYouTube has multiple forms of payout for streaming.\n“YouTube is a multi-sided platform in terms of payout. There are at least three separate payouts under the brands’ umbrella: per YouTube Red/YouTube Music stream ($0.008), per video-stream on the official artist’s channels ($0.00164) and videos monetized through Content ID ($0.00087).”\nIn addition, when it comes to music videos on YouTube, payment is split among 3 payees (owner of the video, owner of the SR and owner of the composition).\nWhere Else Does The Money Come From?\nIt’s more than just streaming. Money also comes from:\nKeep in mind, every career is unique. Different arrangements work for different artists. While streaming revenue has grown significantly, there are a limited number of streams spread amongst all the songs uploaded. That means you need to do more than just rely on streams.\nIndependent artists don’t necessarily need major labels anymore. Another emerging model outside of traditional labels is companies who offer integrated solutions for artists and indie labels. Now, there are independent companies who provide management services, label services, and data analytics in addition to distribution, all to help indie labels and artists access to a greater level of service and technology than ever before. Before you jump into any major deals, remember that you’ve got options. Play the field. Do your research. You wouldn’t marry someone after one date, would you?\nPlease feel free to contact me with any questions at email@example.com.']	['<urn:uuid:0a992732-1439-4d8a-9291-79cd98565f8c>', '<urn:uuid:3b89f08a-299d-4aa2-8f07-87bc38df9238>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-13T06:04:36.674696	7	114	1518
77	What preparation and safety rules apply for decorative walls?	The walls require proper preparation before applying the parchment faux finish technique. For safety, the paint application needs well-ventilated areas to prevent vapor buildup, and finished walls should be treated as combustible materials, kept away from ignition sources. The paint mixtures should be stored in properly sealed containers in cool, dry places.	['Parchment Faux Finish\nThe Parchment Faux Finish is a very popular and easy look to achieve.\nLike the leather faux finish, the “old world” look of the parchment faux finish works great in offices, libraries, and any place you would like to achieve an “aged” or “antique” appearance. It warm hues bring to mind ancient maps and illuminated books.\nThe great news is, this is a relatively simple and painless look to achieve.\nThe color palette for this technique is a mix of creams, light browns, and sunny goldens that create a wonderful feeling of warmth and comfort.\nStop! Very Important! Before beginning, make sure your walls are properly prepared. Not sure? Take a look at our paint preparation instructions (opens in a new window).\nHere is what you’ll need:\nNecessary Paint Products\n- Latex Satin Paint (white)\n- Latex Satin Paint (in two different shades in the same cream/light brown tonality) (more on this in Step 2)\n- Faux Technique Glaze (untinted)\nNecessary Paint Supplies and Applicators\n- 2” Nylon/Poly Brushes (2)\n- 3/8” Nap Roller Cover\n- Roller Frame\n- 2” Painter’s Tape\nApplying the Basecoat\nBegin by taping around the ceiling, windows, doors, floor trim, and any other woodwork or trim that is present.\nThe parchment faux finish technique does not require extensive base-coating, like many of the other faux finishes do. You simply need a plain white background in an eggshell or satin finish to begin.\nIf your wall is already white, and finished in eggshell or satin, you can skip the rest of this step. If not…\nUse a high-quality nylon-polyester brush to “cut in” around the ceiling and trim work.\nUsing a moderately-loaded 3/8” nap roller, roll on the semi-gloss base coat. When rolling, finish each section with a smooth ceiling-to-floor stroke to help eliminate roller marks.\nPer paint manufacturer’s instructions, wait at least four, and preferably six, hours before applying the necessary second coat. Wait at least 24 hours for the second coat to dry before beginning the parchment faux finish technique.\nMixing the Glaze\nBegin with two different shades of the same tonality, in either a light brown, cream, or golden color.\nNow we need to create a glaze/paint mixture that will be workable on the walls. Measure one part paint, one part untinted glaze, and one part water into a small dish and blend them well.\nDo the same with both of the tones you have chosen. The finished mixture should have a fairly thick composition.\nApplying the Parchment Faux Finish\nYou will need two separate 2” brushes to begin, one for each color. Choose a convenient starting point (typically a top corner).\nWorking in an approximately 3’x3’ area, begin by loading each brush with the paint/glaze mixture and applying two 1 foot long squiggly lines approximately 6” apart.\n(Sounds unorthodox, huh? Don’t worry, its simple and looks great.)\nNow, wad the cheesecloth up into a ball in your fist, not allowing any ends to dangle (these may smear or mar the glaze).\nUsing the cheesecloth, begin on the outside of your two squiggly lines of paint/glaze, and lightly rub the paint into the surface, blending the two colors seamlessly together.\nYou should rub in a swirling, figure-eight type pattern to give the surface a random, rounded effect. The colors should blend well, giving varying shades of lighter and darker tones.\nAs you work across the wall, the cheesecloth will accumulate paint and become sticky. Simply rinse the cheesecloth with warm water and wring it out well, then continue as before.\nDon’t worry too much about the varied light and dark areas (remember, parchment naturally has a mixture of light and dark shades). If you begin to notice extremely “light spots” in the finish that do not look natural, simply brush some paint/glaze on the cheesecloth and dab more color into that area. Be sure to smooth out any rough lines or textures.\nFinish the entire wall before stopping, so the paint/glaze mixture remains workable throughout the process.\nRemoving the Tape\nRemove the tape from the trimwork, ceiling, and any adjoining sections while the glaze is still wet. Be careful not to mar your beautiful new parchment faux finish.\nEnjoy the warm, old world charm of faux parchment.\nThanks for visiting our parchment faux finish instructions. Click here to return to the How To Faux Finish home page.', 'Ever wondered if acrylic paint is flammable once it dries? Well, the answer might surprise you! In this article, we will explore the burning question of whether or not acrylic paint poses any fire hazards once it has dried on your canvas. So, if you’re curious about the flammability of acrylic paint and want to ensure a safe art-making experience, read on to find out more.\nIs Acrylic Paint Flammable When Dry\nAcrylic paint is a versatile and popular medium used by many artists, both professional and amateur. It dries quickly, provides vibrant colors, and offers great durability. However, one of the concerns that often arises is whether acrylic paint is flammable when dry. In this article, we will explore the flammability of acrylic paint, the factors that affect its flammability, the associated fire risks, and the precautions that should be taken to ensure safety.\nWhat is Acrylic Paint\nBefore delving into the flammability of acrylic paint, it is important to understand what exactly it is. Acrylic paint is made up of synthetic polymers that form a water-based emulsion. This emulsion allows the paint to dry quickly, resulting in a water-resistant and flexible finish. Unlike oil paint, which consists of pigments suspended in oil, acrylic paint does not require any solvents for cleanup, making it easier to work with and more environmentally friendly.\nFlammability of Acrylic Paint\nWhile acrylic paint is not highly flammable when dry, it is important to handle it with care. When wet, acrylic paint contains a small amount of flammable solvent, usually methyl alcohol or ammonia, which evaporates as it dries. Once the paint has dried completely, the solvents are no longer present, reducing the risk of combustion.\nHowever, it is worth noting that some acrylic paints, especially those that contain additives or mediums, may still have some flammability even when dry. These additives can have a lower flashpoint, which means they have the potential to ignite under certain conditions. Therefore, it is crucial to carefully read the product labels and safety data sheets to determine the flammability of specific acrylic paint brands and additives.\nFactors Affecting Flammability\nSeveral factors can influence the flammability of acrylic paint, even when dry. One of the main factors is the presence of additives or mediums used with the paint. Some additives, such as retarders or flow enhancers, can contain flammable substances that can increase the flammability of the paint. It is essential to be aware of the composition of these additives and use them in well-ventilated areas, away from potential ignition sources.\nAdditionally, the thickness of the paint layer can affect its flammability. Thicker layers can take a longer time to dry, which means the flammable solvents may still be evaporating, making the paint more susceptible to ignition. Thin, even layers of paint are less likely to pose a fire hazard when fully dry.\nFire Risks Associated with Acrylic Paint\nWhile the flammability of dry acrylic paint is relatively low, it is important to understand the potential fire risks associated with its use. If ignited, acrylic paint can produce thick, black smoke, which can be toxic if inhaled. Additionally, burning acrylic paint can release toxic fumes, such as carbon monoxide and hydrogen cyanide, depending on the pigments used in the paint’s composition.\nMoreover, if acrylic paint catches fire, it can spread rapidly, especially if there are flammable materials nearby. Therefore, it is crucial to be mindful of the surroundings and take necessary precautions to prevent fires.\nPrecautions to Take\nTo minimize the risk of fire when working with acrylic paint, it is essential to adhere to basic safety precautions. Here are some measures you can take:\n- Work in a well-ventilated area: Ensure that there is proper air circulation in your workspace to prevent the buildup of flammable vapors. If possible, use exhaust fans or open windows to improve ventilation.\n- Keep flammable materials away: Store flammable materials, such as solvents or mediums, away from potential sources of ignition. This includes heat sources, open flames, sparks, and electrical equipment.\n- Avoid smoking and open flames: It goes without saying that smoking or the presence of open flames should be strictly prohibited in areas where acrylic paint is being used. Even a small spark can ignite the flammable vapors.\n- Do not store in extreme temperatures: Avoid exposing acrylic paint to extreme temperatures, as this can increase the risk of combustion. Store the paint in a cool, dry place, away from direct sunlight and sources of heat.\n- Dispose of waste properly: Discard any waste materials, such as rags or paper towels saturated with acrylic paint, in-lined metal containers specifically designed for the disposal of flammable materials.\nSafety Measures for Storage\nProper storage of acrylic paint is crucial for ensuring safety. Here are some safety measures you should consider when storing acrylic paint:\n- Choose appropriate containers: Use properly sealed containers made of metal, glass, or high-density polyethylene plastic to store acrylic paint. Avoid storing it in containers made of materials that can easily combust, such as paper or cardboard.\n- Label containers: Clearly label containers with the contents and date of storage. This will help identify the paint and ensure that it is used within its recommended shelf life.\n- Store away from heat sources: Keep acrylic paint away from direct sunlight, heat sources, and any potential ignition sources. Choose a cool, dry area for storage.\n- Keep containers tightly sealed: Ensure that containers are properly sealed to prevent the evaporation of flammable solvents and the entry of oxygen, which can cause the paint to deteriorate.\nDifference Between Flammable and Combustible\nTo accurately assess the flammability of acrylic paint, it is important to understand the difference between flammable and combustible materials.\nFlammable materials have a flashpoint below 100 degrees Fahrenheit (37.8 degrees Celsius), which is the lowest temperature at which a substance can generate enough vapor to form an ignitable mixture in the air. Combustible materials, on the other hand, have a flashpoint above 100 degrees Fahrenheit (37.8 degrees Celsius).\nWhile acrylic paint is considered flammable when wet due to the presence of flammable solvents, once it has dried completely, it becomes less flammable and falls into the category of combustible materials. Nonetheless, it is always important to handle both flammable and combustible materials with care and take necessary precautions.\nLegal Aspects and Regulations\nThe flammability of art supplies, including acrylic paint, falls under the purview of various regulations and legal aspects to ensure the safety of consumers. For instance, in the United States, the Consumer Product Safety Commission (CPSC) regulates art materials to ensure they meet specific safety standards.\nTo determine the flammability of art supplies, including acrylic paint, manufacturers are required to conduct tests that assess their flashpoint and provide clear labeling indicating potential risks and precautionary measures. It is important to follow these labels and instructions carefully to ensure safe use of acrylic paint.\nAlternatives to Acrylic Paint\nIf concerns regarding the flammability of acrylic paint persist, there are alternative painting mediums available that may offer a safer option. Some common alternatives include watercolors, gouache, and tempera paints, which have different compositions and drying processes.\nWatercolors, for example, are composed of pigments suspended in a water-based binder, eliminating the need for any flammable solvents. Gouache and tempera paints are also water-based and do not pose significant flammability risks. These alternative paints can provide artists with different creative possibilities while minimizing potential fire hazards.\nIn conclusion, while acrylic paint is not highly flammable when dry, it is important to handle it with caution. The presence of certain additives or mediums can still increase its flammability even in its dry state. By adhering to safety precautions, such as working in well-ventilated areas, storing properly, and avoiding potential ignition sources, the risks associated with acrylic paint can be minimized. Additionally, understanding the difference between flammable and combustible materials can help gauge the level of risk involved. Ultimately, by being knowledgeable and practicing safety measures, artists can enjoy the benefits of acrylic paint while ensuring their own safety and that of their surroundings.']	['<urn:uuid:542fedbf-d02d-481f-9e23-8e60b138eb1f>', '<urn:uuid:5224f44a-87be-4843-b681-a5fc0ce40802>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T06:04:36.674696	9	52	2070
78	Do kids need parent permission for therapy?	When working with children 18 years old or younger, parent permission and parent/family involvement is required. Although Washington state allows children 13+ years old to receive healthcare services without parental permission, this option is utilized only in rare or dire circumstances of the child.	"['Therapy & counseling can be very emotional & growth-determined, impacting many areas of your life. Communication is key throughout our process; it is healthy & helpful to express disagreement or your need to be redirected, and it is productive when concerns are expressed respectfully. I view all language expression as pertinent and valuable as long as negative, derogatory or otherwise offensive use of language is not personally directed.\nMy personal & professional \'philosophy of care\' is holistic, looking at the body-mind-spirit and other systems that impact your experience (systemic). According to meta-analysis studies regarding which type of therapy ""works"", consumers should be aware there are over 300 ""types of therapy"" today, and it doesn\'t necessarily matter which one you choose as long as ""what"" you\'ve engaged in is effective.\nSeveral styles of therapy may be utilized and oftentimes combined depending on your receptivity, need, and their effectiveness with you. Your comfort level & nature of concern(s) you bring will likely encourage your full range of emotions. My intention is based on your needs and goals we will identify together and as time passes and things change for you. Some therapy styles I may draw upon with you include:\nVital associations and ""direct address"" within context\nFamily of origin (FOO) / Culture of origin (COO) exploration\nStructural Family Therapy (SFT)\nChange / re-organization within personal system of influence\nRe-telling experiences claiming ownership of dialogue & experiences while shifting perspective, value, meaning\nSolution Family Focused Therapy (SFFT)\nFocus on individual & family strengths & solutions instead of problem(s)\nStrategic intervention; communication toward changing negatives\nRational Emotive Behavior Therapy (REBT)\nRe-structure irrational beliefs & perceptual distortions\nMotivational Interviewing (MI)\nPersonal movement toward expressed goals\nEMDR (attachment focused, L. Parnell, PhD)\n(Eye Movement Desensitization and Reprocessing)\nUsed with individuals with poor attachment and trauma experiences.\nAddresses the natural need to feel seen, safe, soothed, and secure―the foundation for a healthy mind by integrating memory + internal & external personal narrative + states of being + bilateral stimulation.\nCognitive Behavioral Therapy (CBT)\nPairing thoughts & understandings with behavior & change\nUse of topic-specific ""media""\nInteractions using art, toys, games, music, movement\nThe use of Gestalt psychology, holistic & western practices of body-mind integration & processes, and your faith or religious practices heavily influence how I work with you. Your belief system is very important in your process.\nThere exist many styles of therapy & counseling, and based on your \'need\' a good \'fit\' will be determined in your best interest. ""Good fit"" means several things ... you feel a connection with your therapist & your therapist senses a good connection with you; and, the style(s) utilized with you are effective. Does this mean you will always feel emotionally comfortable? No. Sometimes during the therapy/counseling process the very problem that brings you in begins the un-comfortableness you need to address ... this is healthy, it\'s a start. Your therapist should be competent to create a \'safe space\' for you during your process as you will experience change, which is not always \'comfortable\' yet requires trust.\nYour needs are paramount in your therapy experience and I highly recommend you talk & ask questions with any therapist you are curious to work with prior to enlisting them. If you or I experience a significant ""dis-connect"" after engaging therapy, that will be addressed immediately so we can determine how best to proceed.\nWhen working with children 18 years old or younger, I require both parent permission and parent/family involvement. This a therapeutic philosophy, not a punitive process, as children oftentimes present as more \'symptomatic\' of needs a family may have more than as the actual \'problem\' they are oftentimes identified as. Sometimes the two go hand in hand.\nAlthough WA state allows for children 13+ years old to receive healthcare services without parental permission, I utilize this option in rare or under dire circumstances of the child, as the law intended.']"	['<urn:uuid:c8e34554-fabd-42ab-b839-05004d9ef14b>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T06:04:36.674696	7	44	652
79	one throat to choke principle benefits and risk management challenges in competitive bid process	The 'one throat to choke' principle means the prime contractor remains liable for all obligations under the outsourcing contract, regardless of subcontractor involvement. This requires careful identification and allocation of obligations among subcontractors from business, technical, operational, financial, and legal perspectives. However, running a competitive bid process with multiple providers offers important risk management benefits. It encourages competitive pricing, provides diverse viewpoints that can identify potential pitfalls, and creates fallback options if negotiations or delivery fail. This approach helps balance the single-point accountability of the prime contractor while maintaining strategic alternatives for the customer.	['In almost all business process outsourcing transactions it is virtually impossible for one service provider to perform all of the services required. So subcontractors become fundamental to the delivery of the services. Here’s our list of some key issues to manage when dealing with subcontractors.\n1. One Throat to Choke.\nOr perhaps more politely, one hand to shake. Remember that whether your deal involves one subcontractor or a consortium of them, the customer will insist on the prime contractor remaining liable for performing all of the obligations under the outsourcing contract, regardless of whether some of the obligations have been subcontracted. As a result, it is essential that the prime contractor carefully and clearly identifies, and flows to its subcontractors, any and all obligations to be performed by the subcontractors to ensure complete performance of the services. Each link in the service delivery chain must be carefully considered from a business, technical, operational, financial and legal perspective to ensure that the liability for performance of the subcontracted obligations is appropriately and fully allocated, contractually, among the subcontractors. This is where negotiating limitations of liability (an certain other “sacred cow” provisions) can become tricky, as they are often the last agreement made before signing, and must be flowed through to the subcontractors.\n2. Subcontractor Failures.\nA failure under one subcontract may, in the context of that subcontract, be contained and relatively inconsequential, in the scheme of the larger deal. However, for the prime contractor, that failure may trigger onerous consequences under its contract with the customer. For example, a specific failure by a subcontractor may required that the subcontractor be removed from the deal. The removal of a subcontractor is never without consequences. In order to back stop the prime contractor’s obligations to its customer, the consequences of failure by the subcontractor, regardless of the materiality of the subcontract or the subcontractor’s obligations, must be addressed . . . carefully!\n3. Timing is Everything.\nThe time constraints involved in negotiating an outsourcing transaction are often unrealistic . . . and time is money for the prime contractor and the customer! While the negotiations between the customer and the prime contractor inevitably, completely absorb much of the negotiating time in an outsourcing deal, it is critical that the prime contractor engage its subcontractors early and often. It is even more critical that the customer keep tabs on the status of the subcontractor negotiations. The consequences of failing to manage the timing of the subcontractor negotiations range from delay to deal jeopardy.\n4. Negotiating Strategy.\nIdeally, the prime contractor and its subcontractors, as a team, will have a strategy in place as to various puts and takes that the prime contractor may have in its back pocket for negotiations with the customer. But we all know that deal teams do not live in ideal worlds. When negotiating complex outsourcing arrangements, the implications of decisions made at the core negotiation table (and sometimes the lack of decisions), can significantly impact the subcontractors. Additional due diligence may be required, the subcontractor’s solution or services may need to be re-tooled or completely over-hauled, or the subcontractor’s input (or multiple subcontractors’ input) is required to find a solution to a problem. All of this takes time and adds complexity. Good management of this process, behind the curtain from the customer, is essential to a smooth, successful deal, being completed on time. For the prime contractor, subcontractor negotiations can be a sizeable task (any magnified by the complexity of the transaction, the complexity of the solutions, the number of subcontractors to manage).\nWhether the prime contractor uses separate deal teams to negotiate the subcontracts running almost concurrently with the core table negotiations, or the subcontract negotiations take place on specific days of the week using the prime contractor team, or the prime contractor crystallizes the business deal first before negotiating with the subcontractors, or some other negotiation strategy is implemented, the prime contractor must have a plan and a strategy for managing its negotiations with subcontracts before it starts the negotiations with the customer. Mismanaged subcontractor negotiations not only jeopardize the entire outsourcing, but can also add significantly to the pre-contract deal costs of all involved.\n5. Managing Subcontractor Defaults.\nKey to the one throat to choke concept is the management of subcontractor defaults. Customers may often ask for one throat to choke (that is to say, only one party to the contract – the prime contractor), but when defaults occur, customers can sometimes be quick to jump the queue in an attempt to deal directly with the subcontractor. The relationships need to be honoured when addressing subcontractor defaults. This needs to be facilitated in the default provisions agreed to in the outsourcing contract. It is in the interests of both the customer and the prime contractor to ensure that notification periods, cure periods, reporting obligations, and problem escalation procedures will work from a practical level when flowed down to the subcontractor, and through the prime contractor as gate keeper. These processes and timelines should be carefully mapped out to ensure that workable processes can be managed that allow the parties to focus on solving the problem first, and dealing with the liabilities second.\n6. Practical Realities.\nWhile having one throat to choke, or one hand to shake, by directing all dealings with a subcontractor through a prime contractor may be the preferred approach for many customers, the customer and the subcontractor may need the ability to work directly with each other, instead of through the prime contractor. While the contract will customarily say that all dealings between the customer and the Subcontractor will be through the prime contractor, the on-the-ground operations will likely involve the co-operation of all three parties working together. Appropriate clauses should be negotiated between the customer and the prime contractor, and between the prime contractor and the Subcontractor, to facilitate the cooperation of the parties so they can they can work jointly together required (which may include joint decisions where they are all affected), while still honouring the concept of one hand to shake.', 'When planning to outsource all or part of an organization’s IT functions, it\\’s important to perform active risk management throughout all stages of the outsourcing lifecycle. As a quick recap of Part 1 of this series, managing project risk is a process of identifying potential failure points in a plan, determining the probability of occurrence, and then estimating the impact of each. With that information in hand, an organization can move to the next step of actively managing risks by deciding which risks are tolerable and which ones need mitigation.\nAfter determining your overall strategy, it\\’s time to seek out potential providers of the outsourced IT services your company needs. While you can always just call Lou, the second cousin of your sister-in-law who works for a big acronym company based in D.C, (don’t laugh too hard — this happens all too often); the best method for accomplishing this selection is to run a competitive bid process and issue a Request for Proposal (RFP).\nBy reaching out to multiple providers experienced in the outsource services that your firm needs, you will encourage the bids to be competitively priced. In addition, you will likely get a disparity of viewpoints that will point out potential pitfalls that may have otherwise been missed. Finally, in deference to the adage about keeping all of one’s eggs in one basket, having multiple vendors bid (and in some case jointly win) an outsourcing opportunity provides your organization with a fallback strategy if negotiations or delivery later go awry.\nA simple but frequently forgotten risk management tenet to remember is that you want to provide every opportunity for your vendor to be a success. While this seems obvious, often executives can be heard saying things like, “I don’t worry about the details. That’s what we pay them for…” or “Our contract is ironclad. One misstep on their part and the penalties are so bad we’ll practically own their company.” While your trains sit idle as engineers try to figure out why the two tunnels didn’t meet under the channel or your order screens remain dark while you sit in court arguing with your call center provider over liquidated damages, your clients are flocking elsewhere.\nThe more clarity and detail you can provide in your RFP, the better your potential partners can determine if they can provide a solution and what it might entail. This does not mean that you have to tell them exactly what the solution is; one of the great benefits of a competitive bid process is the opportunity to have input from a variety of very knowledgeable organizations on how best to meet your goal. It does mean, however, that you should provide as much specificity as possible in regard to your current state. If you lack a complete inventory of your company’s IT architecture/infrastructure including existing (legacy) application portfolio, IT support structure and current projects, costs, and service levels, then a high level assessment of components and their suitability for outsourcing (and/or insourcing) needs to be accomplished first.\nWithin the RFP process there are other ways of reducing risk. The more time that you can provide your potential partners to perform discovery and develop their responses, the better. If for some reason your schedule is constrained, then provide as much access to internal subject matter experts for question and answers, follow-up meetings, etc. as possible.\nAnother easy tactic for reducing risk: Throughout the RFP, try to use quantitative descriptions instead of qualitative ones wherever possible. A personal favorite is when I see a client request or a vendor promise “best-in-class.Ó Firsthand experience has shown that when it comes to IT, “best in class” means something completely different to an aerospace firm than it does to a highway-paving company.\nCutting to your bottom line, the more unknowns your service provider faces, the greater the risk. Risk costs money; the more risk that can be driven out of an IT outsourcing solution, the less a vendor will charge you and the greater the chance you have for a successful outsourcing endeavor.\nReprinted with permission from Alsbridge.\nRead Part 1, doing risk assessment in your outsourcing endeavors:\nRead Part 3, evaluating service provider proposals:\nRead Part 4, covering disputes in the contract\nRead Part 5, a guide to setting SLAs\nRead Part 6, managing outsourcing after the contract is signed']	['<urn:uuid:721b2ac1-61f5-4cd7-8b79-776b9c2fad44>', '<urn:uuid:c9e3617e-ca92-4d76-b859-c88e06deeb75>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T06:04:36.674696	14	94	1735
80	How many miles of pipeline does Hillsborough's wastewater system have?	The Town of Hillsborough's wastewater collection system includes 19 miles of pipeline.	['Hillsborough, NH, Cuts Flow to the Wastewater Treatment Plant in Half With Effective Pipeline Inspection Management\nThe Town of Hillsborough, New Hampshire, has prioritized the maintenance of their aging wastewater collection system, which includes 19 miles of pipeline, 2 pump stations, and a lagoon wastewater treatment system. Following Capacity, Management, Operations, and Maintenance (CMOM) regulations and using a sophisticated but user-friendly pipeline inspection management software, the town is successfully repairing and maintaining their century-old distribution system and managing the infiltration/inflow (I/I) that had plagued the pipelines.\nIn 2009, the Town of Hillsborough, ready to proactively address their infiltration/inflow issues, began working with contractor Ted Berry Company Inc. (TBCI) to implement their CMOM performance goal targets. The CMOM approach helps municipal wastewater operators reduce regulatory noncompliance and can help utilities optimize their use of resources by shifting\nmaintenance activities from “reactive” to “predictive”. One of Hillsborough’s CMOM goals was to clean and inspect a certain percentage of their pipe each year. And, according to Paul Dutton, Hillsborough’s wastewater treatment plant facility chief operator since 1992, “once you find the flaws, you’d better fix them!”\nAlthough Hillsborough was progressive in the mid- 1990s developing a CCTV inspection program to record the interior of their sewer lines, there hadn’t been a standard way to transfer all video into a system that would give actionable intelligence and usable results. TBCI implemented a software system for Hillsborough, using PACP, to take years of CCTV VHS video and\ndigitize and standardize the inspections.\nAfter about 1.5 years, looking for a software that would go beyond just organizing video footage digitally, the Town upgraded to PACP and MACP NASSCO-certified ITpipes. The software provider easily transferred consolidated footage and other collected information into the new software. Their new management software was critical to the success of Hillsborough’s ongoing rehabilitation program. Once the information is collected and securely managed, the software can create deliverables after work is performed, sharing information so engineers, treatment plant operators, owners, and others can review results. Filtering tools using PACP overall pipe ratings simplify this even further. With information easily available, Hillsborough prioritized their rehabilitation efforts, starting with projects that were easy, obvious, and less costly.\nThe town tackled those larger points of infiltration and leaks for about five years, with localized points of trenchless rehabilitation. In pipelines that were structurally intact, more than 40 cured in place sectional point repairs were conducted in locations where grade 4 or 5 defects were identified by the software. In pipelines with more widespread damage, other, more appropriate, methods were used. With the use of SmartTabs and ESRI GIS software, pipe bursting was deemed the best rehabilitation method on streets with schools due to volume of traffic, capacity of the pipe, and the extent of defects throughout the pipe segment: the proximity of the defective pipe to the school was clearly visible on the screen. Pipe burst ing was also used to replace 8” vitrified clay pipe (VCP) on Mill Street for the same reasons. Dutton says the results were noticeable immediately.\nEventually, the town moved on to more significant rehabilitation and repair projects, including full pipeline rehabilitation, pipe bursting, and Ultraviolet (UV) CIPP projects. The Town of Hillsborough successfully conducted these improvements and repairs as a result of careful prioritization of their asset management. Chief operator Dutton is very pleased with the results: “The results for Hillsborough have been huge. Since we did the repairs identified by TBCI and easily located with ITpipes, our flows to the plant have been cut in half.”\nCredits to: NASSCO Pipelines Newsletter, May 2017']	['<urn:uuid:86374930-e756-4001-aad6-9274eafbcf94>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	10	12	594
81	sustainable agriculture methods urban rural compare	Sustainable agriculture methods differ between urban and rural contexts while sharing core principles. In Michigan's urban setting, sustainability focuses on minimizing food miles, recycling waste, rotating crops, and utilizing vacant land to reduce rural farmland development. In rural Chad, sustainability practices include proper soil care, companion planting techniques, and strategic crop selection for local conditions. Both approaches emphasize community involvement and long-term solutions, with urban areas focusing on reducing fossil fuel usage and rural areas emphasizing traditional farming methods enhanced by modern training.	"['Standing between rows of bright green bean sprouts, Berthe Nekarbaye dunks her watering can beneath the surface of a shallow well, filling it for what might be the hundredth time that day.\nIt’s 109 degrees in the sun in Ngondong, a small village, miles from the nearest paved road in the savannah of southern Chad. In spite of the relentless heat, small green garden plots pepper the soil, crawling over the low rolling hills just east of the brick and grass huts of the community.\nTheir lush appearance is the result of the dedicated work of Nekarbaye and around two dozen women who make up Ngondong’s gardening team.\n“We have maize, beans, watermelon, carrots, lettuce…,” says Nekarbaye. “Our favorite is beans because beans grow faster — everything’s done in three weeks and then we can sell it.”\nEach and every day of the dry winter season, Ngondong’s gardening team, many of them grandmothers, trek 20 minutes to their gardens and spend the daylight hours dutifully watering and caring for their crops.\nNekarbaye and her team were trained by MCC partner Baobab, a short-form name for Bureau d’Appui aux Organisations de Base that references the tree common in Africa. Baobab provides agricultural training and equipment to rural farmers like Nekarbaye and helps them raise healthier livestock, farm more sustainably and find new opportunities to earn an income from their land.\nAnother goal of Baobab’s training is to empower women in Ngondong and similar villages to break the cultural perception of what women like Nekarbaye can offer to their communities.\n“At first, everybody saw us — and we saw ourselves — as ‘old women’ because there was no activity for us to do,” says Nekarbaye. “Now that Baobab came, discovered us and put us to work, we’re working, so we realize we have strength. We used to be ‘old women’ but now we’re ladies, beautiful ladies, and we can work!”\n“Now that Baobab came, discovered us and put us to work, we’re working, so we realize we have strength. We used to be ‘old women’ but now we’re ladies, beautiful ladies, and we can work!”\n- Berthe Nekarbaye\nOn the other side of the village, another group of women has been trained in raising poultry and producing marketable items like soap or lotion from the byproducts of their crops.\nLucienne Panoudji is the leader of another all-female team and says the skills she’s learned have been valuable in more ways than one.\n“The chickens are more valuable to sell than to eat,” says Panoudji. “One of our women has a daughter who was 3 months old and got very sick in her lungs. She could easily sell her chickens right away to make money to buy medicine and now her girl is healthy again.”\nIn addition to having an income source for health care or school fees, Panoudji says team members also learned to be more intentional about how they eat what they produce.\n“We were always eating exactly the same thing every day for a whole year. Baobab came in and taught us to eat with more variation, to eat different things, and it’s improved our health,” she says.\nAs the rainy summer season encroaches on southern Chad, gardening efforts halt, but the women of Ngondong continue to look ahead to how they can use their new skills to improve their lives.\nSoapmaking training was introduced earlier this year and there hasn’t been enough made to bring to market. But once the peanut and shea crops that flourish in the rainy season are harvested, Panoudji says they’ll be able to make enough soaps and lotions to sell in nearby towns.\nNekarbaye’s next goal is to build a hedge fence around the garden area to prevent livestock from wandering in and devouring all their hard work — an unfortunate reoccurrence due to nomadic cattle in the region, she says.\nThat step becomes more important as the women’s gardening efforts expand.\nBefore Baobab’s training, there were only small home gardens. Women and their families would eat whatever they grew. They were not able to set aside anything to sell, store or use as seeds for the next growing season.\nNow, with seeds and farming tools from Baobab and training in topics like how to better care for the soil and what plants grow well together, the scale of the women’s gardens has grown exponentially — with plants stretching across the hillside and bringing new opportunities for Nekarbaye and the other gardeners.\n“We can sell from these gardens, eat from them and get money from them and then help children to go to school or to the hospital,” Nekarbaye says.', 'The Michigan Urban Farming Initiative is a 501(c)(3) nonprofit organization that seeks to engage members of the Michigan community in sustainable agriculture. We believe that challenges unique to the Michigan community (e.g., vacant land, poor diet, nutritional illiteracy, and food insecurity) present a unique opportunity for community-supported agriculture. Using agriculture as a platform to promote education, sustainability, and community—while simultaneously reducing socioeconomic disparity—we hope to empower urban communities.\nOur core values reflect what is truly important to MUFI and its members. They provide the foundation for our motives, strategies, and implementation. The core values of MUFI are as follows\n""Give a man a fish and he will eat for a day; teach him to fish and he will eat for life."" By viewing urban farming and gardening as an educational opportunity we are hoping to provide a long term solution to the problem of food insecurity in urban areas. When people develop new skills they become actively engaged in the learning process. Through a combination of workshops and fieldwork, we hope to educate the citizen farmers and provide hands-on experience necessary for successful food production.\nMost food is currently produced with synthetic fertilizers and pesticides that are derived from fossil fuels. Fields are often planted with monoculture crops that deplete the soil of the same nutrients. This food is then processed and transported far distances by diesel burning semi trucks. The reliance of our nation on the finite resources of fossil fuels is polluting our atmosphere and compromising our security. Growing food locally minimizes environmental impacts on a local and global scale. When waste is recycled and crops are rotated, minimal fertilizer is needed to maintain high yields. Through using abandoned and vacant land, we also lessen the need to develop more farmland in rural areas.\nBy building relationships with the community we are better able to acheive our objective. Far to often organizations identify struggling populations, raise some money, build something, and give themselves a pat on the back as they leave, never to be seen or heard from again. This is not an effective strategy in engendering long term and sustainable change.\nSome challenges we hope to target:\nSome recent additions to the Michigan scenery include abandoned buildings and houses, unkempt land, and other poorly used spaces. Redeveloping these locations into food producing plots would be make them valuable assets to any community.\nWith the current state of Michigan\'s economy, a large community of unemployed people exists. These people are not bound by the constraints of 9-to-5 employment and may have more time available to participate in community service projects. Community farming can support a healthy lifestyle, especially in times of limited income.\nAccess to nutritious food\nUrban areas have particular difficulty providing consistent access to nutritious food and fresh produce. Such circumstances are particularly acute in low-income neighborhoods, where people may not have access to transportation. Local urban gardens and farms provide a source of fresh, affordable produce available to the whole community.\nFood miles and nutrition information\nMany people are disconnected from their food and where it comes from. We intend to provide ongoing educational opportunities for the community concerning the growing and harvesting of produce, in addition to its nutritional value. We want people to develop a certain consciousness about where their food comes from and their role in the process.']"	['<urn:uuid:34af5600-f66b-4507-94d7-49a17aef1895>', '<urn:uuid:d0f2ab56-9423-4883-8ce7-b0409a87d0f2>']	open-ended	direct	short-search-query	distant-from-document	comparison	novice	2025-05-13T06:04:36.674696	6	83	1332
82	In my analysis of solar power storage capabilities, I'd like to know how the heat storage duration of Morocco's Ouarzazate Solar Power Station compares to Japan's floating solar installations?	The Ouarzazate Solar Power Station has significant heat storage capabilities ranging from 3 to 7.5 hours depending on the phase (Phase 1 has 3 hours, Phase 2 has 7 hours, and Phase 3 has 7.5 hours of heat storage), while Japan's floating solar installations do not have any heat storage capabilities at all. The floating solar technology described focuses on direct photovoltaic conversion without thermal storage components, making them fundamentally different from concentrated solar power systems like Ouarzazate.	"['Ever since Japan’s Fukushima Daiichi plant was destroyed by a combined earthquake and tsunami, the country has been faced with a significant shortage of generating power. In the short term, Japan ramped up its reliance on coal and liquefied natural gas, and instituted electricity rationing and asked consumers to cut their consumption by 15%. Advocates of green power have called for the deployment of additional wind and solar power generators, but both have been limited by Japan’s small land area. Limited space for power generation has led the country to look elsewhere, including deploying a technology called floating solar.\nFloating power generation is arguably more important to Japan than virtually any other first-world nation. While it doesn’t have the highest largest coast/area ratio in the world, the nations that surpass it, like Denmark, with a ratio of 125, it combines a high ratio of 79.6 with high population density and a fully modern, industrialized economy. This combination puts enormous pressure on Japan to make full use of every scrap of available land — or inland water source.\nTechon reports that Kyocera Communications Systems has paid for the installation of a new floating solar plant, dubbed Sakasamaike. The article steps through the construction of a floating solar plant and the unique technology required to bring it online. Workers first assemble scaffolding before deploying pre-constructed “floats” that support the solar panel and its wiring and infrastructure. All told, Kyocera deployed 9,072 solar panels on the pond’s surface and laid resin-coated pipes to preserve the internal cable that stretches from the installation back to shore. The pipes are designed to float if they remain water-tight, giving inspectors an easy way to determine if a pipe has been breached or not.\nThe floating cables are arranged in straight-line configurations to pipe power back to ground-based collection stations, with a maximum cable length of 400 meters. Several criteria have to be met in order for pond installation to proceed, according to the article. Ponds need to have a uniform depth, shouldn’t be too deep in the first place, (10-15 feet is apparently ideal) and have a bank that can support the scaffolding construction required to deploy the boats and floating solar arrays.\nCost is apparently nearly identical to ground-based solar, but there are still challenges related to retrieving parts if dropped into the lake, and the need for on-site diving teams to anchor the floats and link them to the rest of the grid. Total deployment time is only slightly higher than ground-based solar, and there are advantages as well — floating solar plants are easier to cool since the water in the pond soaks up heat, while simultaneously shading the water from direct sunlight. While the solar arrays still produce heat of their own, the shade effect is apparently greater than the indirect heating — water temperatures tend to cool after arrays are installed, which inhibits algae growth.\nThe price of PV installations has dropped dramatically in recent years as Japan has ramped up its installation of both solar and wind facilities. The country has begun testing offshore wind development near Fukushima itself, and we could see solar power deployed in bays or coves as well in coming years — provided companies can come up with a method of dealing with the corrosive effects of seawater and the need to clean the panels on a regular basis. Making use of Japan’s shoreline is likely key to the island nation’s power generation in the future.', 'Largest solar thermal power stations (CSP) list\nLargest operational solar thermal power stations list. Commercial concentrating solar power (CSP) plants, also called ""solar thermal power stations"".\nList.solar has compiled the global rating of top CSP plants sorted by capacity. Only megawatt-scale systems are included in the list (50MW+). Most of the winning stations use parabolic trough technology. Solar power tower systems rank second. Dish Stirling technology is not currently used in utility-scale CSP stations.\n|Electrical capacity (MW)||Name||Country||Location||Developer||Technology type||Storage hours||Notes|\n|510||Ouarzazate Solar Power Station||Morocco||map||TSK-Acciona-Sener||Parabolic trough and solar power tower (Phase 3)||3 / 7 / 7.5||160 MW Phase 1 with 3 hours heat storage. 200 MW phase 2 with 7 hours heat storage is online from January 2018. 150 MW (Phase 3) with 7.5 hours storage is online from November 2018|\n|392||Ivanpah Solar Power Facility||USA||map||BrightSource Energy, Bechtel||Solar power towe||Operational since February 2014. Located southwest of Las Vegas.|\n|310||Solar Energy Generating Systems (SEGS)||USA||map||Luz Industries||Parabolic trough||Collection of 9 units 1984-1990. Originally 354 MW. First two units (44 MW out of total 354 MW) were decommissioned after 30 years and replaced by solar PV.|\n|280||Mojave Solar Project||USA||map||Abengoa Solar||Parabolic trough||Completed December 2014. Gross capacity of 280 MW corresponds to net capacity of 250 MW|\n|280||Solana Generating Station||USA||map||Abengoa Solar||Parabolic trough||Completed in October 2013, with 6 hours thermal energy storage|\n|280||Genesis Solar Energy Project||USA||map||NextEra Energy Resources||Parabolic trough||Online April 24, 2014|\n|200||Solaben Solar Power Station||Spain||map||Abener/Teyma||Parabolic trough||Solaben 3 completed June 2012. Solaben 2 completed October 2012. Solaben 1 and 6 completed September 2013.|\n|150||Solnova Solar Power Station||Spain||map||Abengoa Solar||Parabolic trough||Solnova 1 completed May 2010. Solnova 3 completed May 2010. Solnova 4 completed August 2010.|\n|150||Andasol solar power station||Spain||map||Solar Millennium (25%), ACS Cobra (75%)||Parabolic trough||7.5||Completed: Andasol 1 (2008), Andasol 2 (2009), Andasol 3 (2011). Each equipped with a 7.5 hour thermal energy storage.|\n|150||Extresol Solar Power Station||Spain||map||ACS/Cobra Group||Parabolic trough||7.5||Completed: Extresol 1 and 2 (2010), Extresol 3 (2012). Each equipped with a 7.5-hour thermal energy storage.|\n|125||Crescent Dunes Solar Energy Project||USA||map||SolarReserve||Solar power tower||10||with 10h heat storage; commercial operation began September 2015|\n|125||Dhursar||India||map||Rajasthan Sun Technique Energy||fresnel reflector||Completed November 2014, referred as 125 MW is some sources|\n|121||Ashalim Power Station (Negev Energy)||Israel||map||BrightSource Energy, General Electric||Parabolic trough||4.5||4.5h heat storage. Completed August 2019 and located in Negev desert|\n|121||Megalim Power Station (Negev Energy)||Israel||map||Megalim Solar Power Ltd||Solar power tower||Completed April 2019 and located in Negev desert|\n|100||Kathu Solar Park||South Africa||map||ENGIE||Parabolic trough||4.5||Completed February 2018, With 4.5h heat storage|\n|100||KaXu Solar One||South Africa||map||Abengoa Solar||Parabolic trough||2.5||With 2.5h heat storage|\n|100||Xina Solar One||South Africa||map||Abengoa Solar||Parabolic trough||5.5||Commissioned in September 2017 with 5.5h heat storage.|\n|100||Manchasol Power Station||Spain||map||ACS/Cobra Group||Parabolic trough||7.5||Manchasol 1 and 2 completed in 2011, each with 7.5h heat storage|\n|100||Valle Solar Power Station||Spain||map||SENER||Parabolic trough||7.5||Completed December 2011, with 7.5h heat storage|\n|100||Helioenergy Solar Power Station||Spain||map||Abengoa Solar, EON||Parabolic trough||Helioenergy 1 completed September 2011. Helioenergy 2 completed January 2012.|\n|100||Aste Solar Power Station||Spain||map||Elecnor/Aries/ABM AMRO||Parabolic trough||Aste 1A Completed January 2012, with 8h heat storage. Aste 1B Completed January 2012, with 8h heat storage.|\n|100||Solacor Solar Power Station||Spain||map||Abengoa Solar, JGC||Parabolic trough||Solacor 1 completed February 2012. Solacor 2 completed March 2012.|\n|100||Helios Solar Power Station||Spain||map||Helios I Hyperion Energy Investments, Helios II Hyperion Energy Investments||Parabolic trough||Helios 1 completed May 2012. Helios 2 completed August 2012.|\n|100||Shams solar power station||United Arab Emirates||map||Shams Power Company (Masdar, Total, Abengoa Solar)||Parabolic trough||Shams 1 completed March 2013|\n|100||Termosol Solar Power Station||Spain||map||NextEra, FPL||Parabolic trough||Both Termosol 1 and 2 completed in 2013.|\n|100||Palma del Río I & II||Spain||map||Acciona Energía||Parabolic trough||Palma del Rio 2 completed December 2010. Palma del Rio 1 completed July 2011.|\n|100||Ilanga 1||South Africa||map||SENER, Emvelo and Cobra||Parabolic trough||5||With 5h heat storage. Operational since 2018|\n|100||Shouhang Dunhuang||China||map||Beijing Shouhang IHW||Solar power tower||7.5||With 7.5h heat storage. Operational since end of December 2018|\n|75||Martin Next Generation Solar Energy Center||USA||map||Florida Power & Light Company||SCC with parabolic trough||Completed December 2010|\n|75||Nevada Solar One||USA||map||Acciona Solar Power||Parabolic trough||Operational since 2007|\n|50||Guzmán||Spain||map||FCC Energy||Parabolic trough||Completed July 2012|\n|50||Khi Solar One||South Africa||map||Abengoa Solar - IDC||Solar power tower||2||Completed Feb 2016. With 2h heat storage.|\n|50||Bokpoort||South Africa||map||ACWA Power, Acciona, SENER and TSK and South Africa’s Crowie||Parabolic trough||9||With 9h heat storage|\n|50||Puertollano Solar Thermal Power Plant||Spain||map||Iberdrola Renovables Castilla-La Mancha||Parabolic trough||Completed May 2009|\n|50||Alvarado I||Spain||map||Acciona Energy||Parabolic trough||Completed July 2009|\n|50||La Florida||Spain||map||Renovables SAMCA||Parabolic trough||7.5||Completed July 2010|\n|50||Arenales PS||Spain||map||RREF/OHL||Parabolic trough||7||Start production date november 2013|\n|50||Casablanca||Spain||map||ACS - COBRA group||Parabolic trough||7.5||Land area 200 hectares.|\n|50||Majadas de Tiétar||Spain||map||Acciona Energía||Parabolic trough||Completed August 2010|\n|50||La Dehesa||Spain||map||Renovables SAMCA||Parabolic trough||7.5||Completed November 2010|\n|50||Lebrija-1||Spain||map||Solucia Renovables 1, S.L.||Parabolic trough||Completed July 2011|\n|50||Astexol 2||Spain||map||Elecnor/Aries/ABM AMRO||Parabolic trough||7.5||Completed November 2011, with 7.5h thermal energy storage|\n|50||Morón||Spain||map||Ibereólica Solar||Parabolic trough||Completed May 2012|\n|50||La Africana||Spain||map||Ortiz/TSK/Magtel||Parabolic trough||7.5||Completed July 2012, with 7.5h thermal energy storage|\n|50||Olivenza 1||Spain||map||Ibereólica Solar||Parabolic trough||Completed July 2012|\n|50||Orellana||Spain||map||Acciona||Parabolic trough||Completed August 2012|\n|50||Godawari Green Energy Limited||India||map||Godawari Green Energy Limited||Parabolic trough||Land Area 150 hectares|\n|50||Enerstar Villena Power Plant||Spain||map||FCC Energy||Parabolic trough||Completed 2013|\n|50||Megha Solar Plant||India||map||Megha Engineering and Infrastructure||Parabolic trough||Completed 2014|\n|50||Delingha Solar Plant||China||map||CGN Delingha Solar Energy||Parabolic trough||9||Completed July 2018 with 9 hours of thermal energy storage|\n|50||Supcon Solar Delingha||China||map||SUPCON||Solar power tower||7||Completed December 2018|\n|50||Shagaya CSP||Kuwait||map||TSK||Parabolic trough||10||Commercial operation started in February 2019, 10 hours thermal storage|\n|50||Waad Al Shamal ISCC Plant||Saudi Arabia||map||General Electric||ISCC with parabolic trough||Commercial operation started in 2018, 1,390 MW plant with 50 MW solar|\n|50||Qinghai Gonghe CSP||China||map||Supcon Solar||Power tower||6||With 6 h heat storage.|\nConcentrating solar, or solar thermal power plants, utilize systems of mirror or lenses and trackers to focus a huge volume of sunlight onto a receiver and generate heat energy. The thermal energy is either harnessed for industrial process heating or for creating steam, which turns a turbogenerator, producing electricity.\nA CSP station can be supplemented with a storage system, which allows generating electrical power even at night or in dull weather.\nThere are four key groups of solar thermal systems, each of them having different variations and configurations. There is no single opinion as to which technology is optimal, all of them have their own pros and cons.\n- Parabolic trough is the oldest and best-developed CSP tech. The solar field consists of parallel rows of parabolically curved solar collectors comprised of reflectors, which concentrate sunlight onto a receiver tube filled with working fluid. The latter is heated to considerable temperatures and produces steam, the heat of which is utilized for electricity making. The reflectors are installed on single-axis trackers, going after the sun to keep its radiation always concentrated on a receiver pipe, which runs right above the trough-shaped mirror.\n- Power tower tech is not so developed as a parabolic trough, but it is a fast-developing and promising system. The heat-transfer fluids in such systems reach temps much higher than in parabolic troughs, and consequently they provide higher conversion efficiency. The technology is comprised of a tall tower with a receiver above, surrounded with heliostats – reflectors that track the sun’s position along 2 axes and focus solar irradiance onto a receiver. The working fluid gets heated, which results in formation of steam running a power generator. Energy storage systems are often integrated into power towers to provide uninterrupted electricity generation.\n- Linear Fresnel technology is much similar to a parabolic trough. The difference is in the form of mirrors used. Instead of curved reflectors, Fresnel system contains parallel rows of strip-shaped flat mirrors. They also follow the sun’s rays to concentrate them onto a system of receiver pipes filled with heat transfer fluid. The concentrating surface of flat reflectors is larger compared to parabolically curved mirrors and they are less expensive. Like parabolic troughs and solar power towers, Fresnel projects can also be integrated with energy storage.\n- Parabolic dish systems are made of a dish-shaped concentrator that focuses solar energy onto a receiver installed at a focal point. The reflector is installed on a double-axis tracking system. Unlike other CSP technologies where heat energy is converted into electrical power by a turbogenerator, in most dish systems steam is used to drive an engine (commonly Stirling engine). Today, dish-engine solutions are rarely used for commercial power production. A parabolic dish may be heated to extremely high temperature, which is a promising option for making a solar fuel.']"	['<urn:uuid:7c2a04f7-021e-4758-a003-57a14f550d22>', '<urn:uuid:01473e93-25ba-483c-a1e2-ebe967fef781>']	factoid	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T06:04:36.674696	29	78	1888
83	what pioneering theories explore comparison between infinite number sets present 19th century mathematics	Set theory, developed by Cantor, explores the comparison of infinite sets. His theory introduced the concept of different sizes of infinity through cardinal numbers. He proved that some infinite sets, like the set of natural numbers (N), have a different size than others, like the set of real numbers. Cantor's Theorem established that there are different levels of infinity, and his work led to fundamental questions in mathematics like the Continuum Hypothesis.	"['Ships same day or next business day via UPS (Priority Mail for AK/HI/APO/PO Boxes)! Used sticker and some writing and/or highlighting. Used books may not include working access ...code or dust jacket.Read moreShow Less\nNow in its fifth edition, A Mathematics Sampler presents mathematics as both science and art, focusing on the historical role of mathematics in our culture. It uses selected topics from modern mathematics—including computers, perfect numbers, and four-dimensional geometry—to exemplify the distinctive features of mathematics as an intellectual endeavor, a problem-solving tool, and a way of thinking about the rapidly changing world in which we live. A Mathematics Sampler also includes unique LINK sections throughout the book, each of which connects mathematical concepts with areas of interest throughout the humanities. The original course on which this text is based was cited as an innovative approach to liberal arts mathematics in Lynne Cheney\'s report, ""50 HOURS: A Core Curriculum for College Students"", published by the National Endowment for the Humanities.\nWilliam P. Berlinghoff is visiting professor of mathematics at Colby College. Kerry E. Grant is professor of mathematics at Southern Connecticut State University. Dale Skrien is professor of computer science at Colby College\nChapter 1 Preface Chapter 2 To the Student Part 3 Chapter 1: Problems and Solutions Chapter 4 What Is Mathematics? Chapter 5 Problem Solving Chapter 6 It All Adds Up Chapter 7 The Mathematical Way of Thinking Chapter 8 Topics for Papers Chapter 9 For Further Reading Part 10 Chapter 2: Mathematics of Patterns: Number Theory Chapter 11 What Is Number Theory? Chapter 12 Divisibility Chapter 13 Counting Divisors Chapter 14 Summing Divisors Chapter 15 Proper Divisors Chapter 16 Even Perfect Numbers Chapter 17 Mersenne Primes Chapter 18 LINK: Number Theory and Cryptography Chapter 19 Topics for Papers Chapter 20 For Further Reading Part 21 Chapter 3: Mathematics of Axiom Systems: Geometries Chapter 22 What is Geometry? Chapter 23 Euclidean Geometry Chapter 24 Euclid and Parallel Lines Chapter 25 Axiom Systems and Models Chapter 26 Consistency and Independence Chapter 27 Non-Euclidean Geometries Chapter 28 Axiomatic Geometry and the Real World Chapter 29 LINK: Axiom Systems and Society Chapter 30 Topics for Papers Chapter 31 For Further Reading Part 32 Chapter 4: Mathematics of Chance: Probability and Statistics Chapter 33 The Gamblers Chapter 34 The Language of Sets Chapter 35 What Is Probability? Chapter 36 Counting Processes Chapter 37 LINK: Counting and the Genetic Code Chapter 38 Some Basic Rules of Probability Chapter 39 Conditional Probability Chapter 40 LINK: Probability and Marketing Chapter 41 What Is Statistics? Chapter 42 Central Tendency and Spread Chapter 43 Distributions Chapter 44 Generalization and Prediction Chapter 45 LINK: Statistics in the Psychology of Learning Chapter 46 Topics for Papers Chapter 47 For Further Reading Part 48 Chapter 5: Mathematics of Infinity: Cantor\'s Theory of Sets Chapter 49 What Is Set Theory? Chapter 50 Infinite Sets Chapter 51 The SIze of N Chapter 52 Rational and Irrational Numbers Chapter 53 A Different Size Chapter 54 Cardinal Numbers Chapter 55 Cantor\'s Theorem Chapter 56 The Continuum Hypothesis Chapter 57 The Foundations of Mathematics Chapter 58 LINK: Set Theory and Metaphysics Chapter 59 Topics for Papers Chapter 60 For Further Reading Part 61 Chapter 6: Mathematics of Symmetry: Finite Groups Chapter 62 What Is Group Theory? Chapter 63 Operations Chapter 64 Some Properties of Operations Chapter 65 The Definition of a Group Chapter 66 Some Basic Properties of Groups Chapter 67 Subgroups Chapter 68 Lagrange\'s Theorem Chapter 69 Lagrange\'s Theorem Proved [Optional] Chapter 70 Groups of Symmetries Chapter 71 LINK: Groups in Music and in Chemistry Chapter 72 Topics for Papers Chapter 73 For Further Reading Part 74 Chapter 7: Mathematics of Space and Time: Four-Dimensional Geometry Chapter 75 What Is Four-Dimensional Geometry? Chapter 76 One-Dimensional Space Chapter 77 Two-Dimensional Space Chapter 78 Three-Dimensional Space Chapter 79 Four-Dimensional Space Chapter 80 Cross Sections Chapter 81 Cylinders and Cones [Optional] Chapter 82 LINK: 4-Space in Fiction and in Art Chapter 83 Topics for Papers Chapter 84 For Further Reading Part 85 Chapter 8: Mathematics of Connection : Graph Theory Chapter 86 What Is Graph Theory? Chapter 87 Some Basic Terms Chapter 88 Edge Paths Chapter 89 Vertex Paths Chapter 90 Crossing Curves Chapter 91 Euler\'s Formula Chapter 92 Looking Back Chapter 93 LINK: Diagraphs and Project Management Chapter 94 Topics for Papers Chapter 95 For Further Reading Part 96 Chapter 9: Mathematics of Machines: Computer Algorithms Chapter 97 What Is a Computer? Chapter 98 The Traveling Salesman Problem Chapter 99 The Speed of a Computer Chapter 100 Algorithms and Sorting Chapter 101 Comparing Algorithms Chapter 102 Complexity Analysis Chapter 103 NP-Completeness Chapter 104 Implications of NP-Completeness Chapter 105 LINK: Algorithms, Abstraction, and Strategic Planning Chapter 106 Topics for Papers Chapter 107 For Further Reading Part 108 APPENDICES Part 109 Appendix A: Basic Logic Chapter 110 Statements and Their Negations Chapter 111 Conjunctions and Disjunctions Chapter 112 Conditionals and Deduction Chapter 113 Topics for Papers Chapter 114 For Further Reading Part 115 Appendix B: A Brief History of Mathematics Chapter 116 Preliminary Thoughts Chapter 117 From the Beginning to 600 B.C. Chapter 118 600 B.C. to A.D. 400 Chapter 119 400 to 1400 Chapter 123 The Fifteenth and Sixteenth Centuries Chapter 124 The Seventeenth Century Chapter 125 The Eighteenth Century Chapter 126 The Nineteenth Century Chapter 127 The Twentieth Century Chapter 128 Topics for Papers Chapter 129 For Further Reading Part 130 Appendix C: Literacy in the Language of Mathematics']"	['<urn:uuid:363be3d1-b244-4778-bfb1-820a6f4a7443>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T06:04:36.674696	13	72	905
84	What is PAIR in problem-solving for schools?	PAIR stands for: problem identification, analysis of the problem, intervention implementation, and response to intervention.	"['Volusia County Schools Problem Solving process is a data driven process that assists students, families and teachers in seeking positive solutions for all students. The primary goal of the PST is to support teachers and parents by generating effective research-based academic and behavioral strategies for individual targeted students. In addition, Problem Solving Teams can use school-wide and class-wide data to monitor the success and difficulties of groups of students and can offer academic and behavioral interventions to be applied to class or school- wide issues.\nWithin this problem-solving model, teams follow the PAIR format. PAIR stands for: problem identification, analysis of the problem, intervention implementation, and response to intervention.\nProblem Identification- occurs when the team identifies in objective and measurable terms what is expected and what the student can do (e.g., Expected – Observation = Problem). For example, a third-grade student whose oral reading rate near the end of the school year is 58 words correct per minute (wcpm) with the end of the year expectation being 100 wcpm. Thus, the expectation (100wcpm) – observation (58wcpm) equals the problem (42wcpm). Therefore, the problem is that the student is reading 42 words fewer per minute than what is expected.\nAnalysis of the Problem- is intended to determine why the problem is occurring. Thus, using the above example, why is the student reading 42 words fewer per minute than what is expected. During this step, the relevant information known about the problem is considered, potential hypotheses about the possible causes of the problem are generated, and information is gathered to confirm or disconfirm the hypotheses.\nThe domains assessed for information to analyze the problem are instruction, curriculum, environment, and learner (ICEL). Some of the questions asked are – ""Has the child received instruction in the target skill?"" (Instruction), ""Does the curriculum contain the target skill?"" (Curriculum), and ""Does the environment support the acquisition of the skill?"" (Environment). Specifically, the analysis of the problem phase includes:\n- Gaining a clear understanding of the causes (functions) of the problem.\n- Determining if the problem is a skill deficit or performance deficit.\n- Development of hypotheses as to why the problem is occurring.\n- Identifying if the problem is Instructional, Curriculum, Environmental, or Learner (ICEL) related?\n- Identifying relevant obstacles.\n- Developing a goal to address the problem (observable and measurable).\nOften, this is not a linear process. Consideration of known information/data, possible causes, and unknown information happens nearly simultaneously. Hypothesis generation involves the balancing of known information, possible causes, and gathering of unknown information in an ongoing process until a hypothesis with a high likelihood of correctness is derived.\nIntervention Implementation- occurs once the problem has been defined and analyzed. The goal is to take the information gathered through problem analysis and utilize it to develop an instructional plan that matches the identified student need. This is accomplished through intervention design and establishing a goal so the team can identify when the child has learned the desired skill/concept. A goal should be attainable and specific. An example is as follows:\nIn 10 weeks, Student will read aloud a 2nd grade level passage from DIBELS Next ORF OPM at 75 words read correctly in 1 minute with at least 97% accuracy.\nThis goal is very specific in terms of time, skill, and level of accuracy. The identified missing skills are targeted for explicit instruction within a supported learning environment.\nAn intervention should be purposeful, planned and grounded in data. It is about making decisions about alterable variables within instruction, curriculum and the environment. Problem Solving teams focus on those modifications in these areas that will directly impact or alter the targeted behavior. Instructional strategies that are based on the nature of the defined problem and yield the most likelihood for success are selected.\nResponse to Intervention- requires ongoing progress monitoring, which is a methodology for measuring the effectiveness of an intervention. In order to design an intervention, the problem must have been analyzed adequately. For problem analysis to have occurred, the problem must have been accurately defined. So, intervention progress monitoring should not occur unless the first three steps of problem solving have been conducted.\nSince we need to make decisions quickly if our interventions are not delivering the desired results, it is necessary we gather this information frequently. Thus, key features of the instrumentation used to collect these data are that they can be administered frequently and are sensitive to small changes in behavior.\nAlso important, this information must be plotted on a graph so that trends in student performance can be visualized. That is, we must be able to see where we\'re headed in order to evaluate the effectiveness of instructional efforts. Data are used to make important decisions about whether to continue the intervention, modify it, or change it completely.\nIf an intervention is not producing the desired results, a first step is to evaluate whether the intervention plan is being implemented as designed. If not, adjustments are made to ensure that it is. If the treatment integrity has been verified, all the previous problem-solving steps should be reviewed. A mistake may have been made in problem identification, problem analysis, or intervention design.\nIf an intervention is not producing the desired results it does not necessarily mean that it is the wrong intervention. It may be the right intervention, but the intensity needs to be increased. Three basic ways to increase the intensity of an intervention are: 1) Reduce the size of the group; 2) Increase the amount of time that the intervention is delivered; and 3) narrow the focus of the lesson. These strategies for intensification may be used individually or in combination.\nProblem solving is a self-correcting methodology dependent upon instructional decisions made using reliable data collected frequently.']"	['<urn:uuid:59fe23e8-6d9e-4ff7-913f-68052eaadf8c>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	7	15	958
85	child development psychological theory basic needs highest goals	A child's development follows a hierarchical progression from basic physiological needs to self-actualization. At the base are fundamental needs like breathing, drinking, eating, sleeping, warmth, and avoiding pain. Once these are met, children develop needs for safety and security, followed by belonging needs and demands for love and attention in the first two years. Around age 2, they develop esteem needs. The highest levels include self-actualization and self-transcendence, where individuals can achieve their full potential, think clearly, be creative, and form healthy relationships. This developmental journey is oriented toward growth and reaching one's fullest potential, with each level building upon the previous ones.	"['Revisiting a Person-Centered Science\nThrough the work of existential-humanistic and transpersonal psychologists Carl Rogers, Abraham Maslow, and Rollo May in the 1950s, 60s, and 70s, the psychotherapeutic hour became a living laboratory in which the individual’s discovery of the growth-oriented, self-actualizing dimension of his or her personality was educed. Rogers, Maslow, and May gallantly challenged the reductionistic, university-based, experimental research methods and the cognitive-behavioral models of trait in redefining the person, and ushering in a new era of a person-centered science.\nIn “Toward a Theory of Creativity,” in On Becoming a Person (1961), Rogers wrote about the person, who is open to experience. “Instead of perceiving in predetermined categories… the individual is aware of this existential moment as it is, thus being alive to many experiences that fall outside the usual categories” (p. 353). Rogers’ fully functioning person had five qualities: (1) Openness to experience; perception of one’s feelings and experience of the world, (2) Existential living; living in the present; (3) Organismic trusting; trusting one’s thoughts and feelings, (4) Experiential freedom; acknowledging freedom while taking responsibility for one’s actions, and (5) Creativity; participating in the world and one’s generative contribution to the lives of others.\nSubsequently, “Toward a Science of the Person,” in Behaviorism and Phenomenology: Contrasting Bases for Modern Psychology (1964), Rogers identified three ways of knowing in psychology: experimental (based on an objective, external frame of reference), psychodynamic (based on a subjective, internal frame of reference), and existential-humanistic (based on intrapersonal, phenomenological knowing or the empathic understanding of one’s private world of meaning and internal frame of reference). With these distinctions, he widened the epistemological base from which psychological science could operate.\nIn Motivation and Personality (1970), Maslow used the term self-actualization, first employed by Kurt Lewin, to refer to the individual’s desire for self-fulfillment, namely the tendency to become actualized in what he or she was potentially. However, Maslow’s primary contribution was to developmental psychology. His hierarchy of human needs was presented as a pyramid with physiological needs, which were necessary for survival at its base, and self-actualization at the apex, as the highest of those needs. Once survival needs were met, the individual could concentrate on the second layer—the needs for safety and security. The third layer is the need for love and belonging and the fourth the need for esteem. The fifth layer is self-actualization and the sixth layer the need for self-transcendence.\nIn The Farther Reaches of Human Nature (1971), Maslow described various meanings of the word “transcendence” with regard to the ego, among them, the “Taoistic feeling of letting things happen, rather than of making them happen; of being happy and accepting of the state of nonstriving, nonwishing, noninterference, noncontrolling, and nonwillling; the state of having rather than not having” (p. 277). Earlier, in The Psychology of Science (1966), Maslow applied these same concepts in identifying a subjective, Taoistic, receptive, non-interfering mode of conducting science that emphasized the phenomenological, holistic, empirical (non-a priori) nature of investigation.\nIn Existence: A New Dimension in Psychiatry and Psychology (1958), Rollo May, Ernest Angel, and Henri Ellenberger defined three modes of the world distinguished by existential analysts: (1) Umwelt, “world around” (the biological world or environment), (2) Mitwelt, “with world” (the world of beings of one’s own kind, or one’s fellow humans), and (3) Eigenwelt, “own world” (the mode of relationship to oneself. To existential psychology, personality can be seen as a trajectory toward its future. Placing time in the center of the psychological picture of Eigenwelt, the most profound human experiences occur more in the dimension of time than in space. As existence emerges, it is always in the process of becoming, always developing in time. In Rollo May’s The Courage to Create (1975) and “Creativity and the Unconscious,” in Review of Existential Psychology and Psychiatry (1999), he notes that Westerners are afraid of the unconscious and of irrational experience. Out of fear, they put tools and techniques between themselves and the world of the unconscious. However; he notes, “It was this very creativity of spirit that gave birth to the values that directed their technical power” (p. 33).\nWhat can the science of psychology, which still strives to categorize the person, reducing him or her to an object in nature to be studied, learn from these theorists? While Rogers’ clinical theory of personality centered on the actualizing tendency or the motivation potential present in every life form to develop to its fullest, Maslow professed that human beings had a growth-oriented, biologically-based, instinctive nature that was fulfilled in spiritual self-actualization and transcendence as the highest and most inclusive or holistic level of human consciousness, behaving and relating. He identified a subjective, Taoistic, receptive, non-interfering mode of conducting science that emphasized the phenomenological, holistic, empirical (non-a priori) nature of investigation. Likewise, for May, the creativity of science was bound up with the capacity of the human being to create freely in the realm of spirit. May’s writings explore how anxiety arises from not being able to know the world that one is in, while insight, often born with anxiety, possesses the joy and gratification that comes from experiencing a new aspect of the world through the breakthrough of this insight. It is here that a person-centered science begins.\nMaslow, A. H. (1966). The psychology of science. New York, NY: Harper & Row.\nMaslow, A. H. (1970). Motivation and personality (2nd ed.). New York, NY: Harper & Row.\nMaslow, A. H. (1971). The farther reaches of human nature. New York, NY: Viking.\nMay, R. R. (1975). The courage to create. New York, NY: W. W. Norton.\nMay. R. R. (1999). Creativity and the unconscious. Review of Existential Psychology and Psychiatry, 24, 33-39.\nMay, R. R., Angel, E., & Ellenberger, H. F. (1958). Existence: A new dimension in psychiatry and psychology. New York, NY: Basic Books.\nRogers, C. (1961). On becoming a person: A therapist’s view of psychotherapy. Boston, MA: Houghton Mifflin.\nRogers, C. (1964). Toward a science of the person. In T. W. Wann (Ed.), Behaviorism and phenomenology: Contrasting bases for modern psychology (pp. 109-140). Chicago, IL: University of Chicago Press.\n-- Susan Gordon', 'As you watch your toddler or preschooler play with his toys or test the limits you have set, you might not realize it, but your child is developing along principles set by Dr. Abraham Maslow. Back in 1943, the Brooklyn-born psychologist described children\'s development as a journey toward their full potential, or ""self-actualization."" He pictured this progress as a pyramid with five levels. A child\'s most basic needs are at the bottom and most advanced ones at the top.\nThe toddler or preschooler who cuddles in your lap for comfort has developed beyond the basic needs he had when you brought him home from the hospital. These included the needs to breathe, drink, eat, sleep, stay warm, get rid of bodily waste and avoid pain. Once these basic ""physiological"" needs are satisfied, says Maslow, your child starts to crave safety and security. The hugs you give are just as important as food in helping your child grow.\nWhen your toddler tugs at your hand or demands, ""Look at me, look at me!"" for the hundredth time, you\'ll be glad to know it\'s a positive sign. She\'s developing right on schedule. In the first two years of life, says Maslow, children feel powerful, ""belonging needs"" and make growing demands for love and attention. Around the age of 2 years, they also develop ""esteem needs,"" craving recognition of their own importance in the world.\nMaslow\'s ideas confirm what moms and dads always knew: there\'s more to the job than just keeping the kids fed, clean and warm. Give your little ones security, love and attention, too and they will blossom. For Maslow, only children whose physiological, safety, belonging and esteem needs have all been met will be in a position to achieve their full potential in later life. Only then they will be free to study, think clearly, be creative and form healthy relationships.\nAll those hugs add up. When you praise her new drawing or spend time watching him dance, you help build a firm foundation for your child. Young children face huge setbacks if their early needs are not met, Maslow argues. They ""fixate"" on what they lack, maybe for the rest of their lives. Children who often endure hunger will obsess about food, even as adults. The same holds true, Maslow suggests, of their needs for security, affection and self-esteem.\nMaslow has been criticized for lacking rigor, since there is no way to measure whether a child\'s needs have been met. Maslow claimed that the lives of people he admired proved his theories to be valid, leading critics to question his objectivity. Some also say Maslow fails to account for the people who had tough childhoods, but still shone in later life. But Maslow\'s big idea continues to guide parents: a child who is cherished will always stand the best chance in life.\n- Shippensburg University: Abraham Maslow\n- Education.com: The Work of Abraham Maslow\n- Investing in Children: Maslow\'s Hierarchy of Needs\n- Scotland\'s Colleges; Theories and Issues in Child Development; Alan Slater et al\n- Mater Academy: Child Growth and Development -- Child Development Theories\n- The Literary Link: Understanding Children\n- Universiti Teknology Malaysia; Abraham Maslow -- The Needs Hierarchy; Azizi Hj. Yahaya, Ph.D.\n- Learning Theories: Maslow\'s Hierarchy of Needs\n- Simply Psychology: Maslow\'s Hierarchy of Needs\n- Comstock Images/Comstock/Getty Images']"	['<urn:uuid:9003b734-015b-46a1-806b-b207b285a2b0>', '<urn:uuid:307ab358-fd2c-4c63-ab99-85ad4048ef5f>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T06:04:36.674696	8	103	1573
86	how does deceiving enemy compare with ambiguity in military warfare	Deception is like a bomb that creates surprise and shock when sprung on the enemy, while ambiguity is a constant state of mind that creates anxiety, unease and terror. While deception's effects can wear off before objectives are achieved, ambiguity allows commanders like Genghis Khan to routinely defeat larger forces by making the enemy construct their own demons.	"[""The best strategist is not the one who knows he must deceive the enemy,\nbut the one who knows how to do it.\nPolish SciFi master Stanislaw Lem (1921 – 2006)\nWe often think of Soviet doctrine as tanks lined up tread to tread, rolling forward until either they conquer or fall. Mass makes might. While there is a lot of truth to the Soviet, and so presumably Russian, respect for mass, it may surprise you to learn that the Soviets had, and so presumably the Russians have, a well thought-out doctrine of deception called maskirovka. The BBC ran a nice piece on the subject a few days back, “How Russia outfoxes its enemies,” by Lucy Ash.\nBoyd had great respect for deception, “an impression of events as they are not,” as he wrote on Patterns chart 115, “Essence of Maneuver Conflict.” A person who is being deceived is not confused. He knows what the situation is. His orientation is coherent; his mental model of the world fits all the facts. It’s just wrong. Boyd’s primary vehicle for using deception was the cheng / chi maneuver, which he borrowed from Sun Tzu and reformulated in more modern terms as the Nebenpunkte / Schwerpunkt concept (see charts 78, 114, and many others). Basically, the deceiver shapes the orientation of the victim to expect (cheng) certain actions to take place. Think all of the stuff the allies did to shape Hitler into expecting the D-Day attack across the Pas de Calais. The deceiver then springs something entirely unexpected, the chi, and tries to exploit the resulting shock and confusion.\nA neat trick, but sometimes overlooked is Boyd’s even greater emphasis on ambiguity, “alternative or competing impressions of events as they may or may not be.” The “fog of war,” in other words, a concept that he originally took from Clausewitz. To appreciate the distinction between the two, think of deception as like a bomb. When it goes off, that is, when the opponent springs the trap, you’ll be surprised and shocked. If the opponent can exploit before you recover, it can be decisive, although its effects often wear off before such objectives can be achieved.\nOn the other hand, ambiguity is, as Clausewitz noted, a climate, a state of the mind. It manifests as anxiety, unease, terror, and trauma. You construct your own demons to round out those provided by the opponent. Some commanders — Genghis Khan and Nathan Bedford Forrest were two examples that Boyd often cited — raised ambiguity and its consort, terror, to such high art forms that they routinely defeated much larger forces. On Patterns 132, Boyd makes his case that the tactic of “operating inside the OODA loop” plays a vital role in both deception and ambiguity, so it isn’t an either-or sort of thing. Clearly each can reinforce the other. Perhaps a reader can comment on whether the Soviet / Russian concept of maskirovka includes what Boyd called “ambiguity.”\nCan the Russians actually pull off effective deceptions? Apparently they had little difficulty against Ukraine, although that militarily weak country is a special case, with large Russian-speaking minorities and political turmoil from having just overthrown an elected government considered friendly to Moscow. Against NATO? Who cares? We’re not going to fight them over anything of significance, nothing approaching Sun Tzu’s definition of war as “of vital importance to the state … the road to survival or ruin.”\nIt might be useful to consider our own policy regarding deception. From the BBC article:\nSo what sets Russia apart? Maj Gen Skip Davis [Chief of intelligence and operations at NATO HQ in Brussels] argues Western forces are sometimes economical with the truth but says they don’t tell outright lies: “We are talking about denial of information – in other words, not confirming facts – versus blatantly denying. Saying, ‘No that’s not us invading, that’s not our forces there, that’s someone else’s.'”\nI don’t need to point out that General Davis’s denial that we use deception only adds to its effectiveness.\nAs strategists from Sun Tzu to Boyd have insisted, deception isn’t dishonorable or evil. It’s the only humane way to conduct military operations. All of you are familiar with Sun Tzu’s maximum that “All warfare is based upon deception.” A logical consequence of this is that if it isn’t based upon deception, it isn’t war. It’s probably not even magnifique (speaking of the Crimea). More likely it’s just going to kill a lot of people on one or both sides and thus flunk one of Boyd’s tests of effective grand strategy because it will likely set the stage for future (unfavorable) conflict (Patterns 139).\nOne final thought. Planning and executing a successful deception is difficult. It requires considerable creativity, because we’re trying to deceive a clever opponent, after all, and at the operational and strategic levels, large numbers of people must be trained to execute and exploit before the opponent recognizes what we’re up to. Obviously we have to deceive the opponent about what all this training means. Unless you have a truly clueless opponent — always a dangerous assumption — it takes a force built around people, ideas, and hardware in that order.""]"	['<urn:uuid:a80eca6b-08fd-4cda-8879-4ad2d27ed4d9>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	10	58	865
87	Which newspapers won awards for water coverage and what's the current water crisis impact?	The Fresno Bee won first place in Public Service Journalism for their coverage of contaminated drinking water in San Joaquin Valley. The investigation revealed that many families must choose between buying shoes for their children and bottled water, while small water systems struggle to afford treatment technology for contaminated water. The largest concentration of unsafe water systems is in the San Joaquin Valley, particularly affecting low-income communities of color.	"['To protect and serve the common interests of its newsmedia members, to help members inform and thereby strengthen their communities, and to foster the highest ideals, ethics and traditions of journalism, a free press and the news profession.\nFirst-place and second-place plaques will be shipped directly to the winning publications in the coming weeks.\nPersonalized certificates will also be mailed to winners in each category.\nA complete list of all California Journalism Award winners will be provided on May 18th.\n|Circ Group(s)||Category Name||Award||Organization||City||Entry Title||Comments||Credits|\n|Dailies: 50,001 & over,Dailies: 15,001 - 50,000,Dailies: 15,000 & under||Public Service Journalism||First Place||The Fresno Bee||Fresno||Contaminated drinking water in the San Joaquin Valley||This series of articles could almost define public service journalism: Bad drinking water provision and regulation was put under the microscope, starting with granular detail in one small area and sweeping out to cover a large part of California\'s central valley. It went after how and why things went wrong and offered practical information and online tools to help readers protect themselves and their community\'s health.||Monica Vaughan, Tim Sheehan, Nathalie Vera, John Walker|\n|Dailies: 50,001 & over,Dailies: 15,001 - 50,000,Dailies: 15,000 & under||Public Service Journalism||Second Place||The San Diego Union-Tribune||San Diego||RETURNED -- Part I: Protecting the most vulnerable Part II: Who gets asylum? Part III: Fear of death Part IV: The system is broken||The flaws and gaps in the United States system for seeking asylum. An excellent, massive report involving on-ground international research and deep database number-crunching, focused heavily on the human experience of the situation. The danger - from many directions - involved in the reporting makes the result the more remarkable, as amid the proliferation of reporting on the asylum system.||Kate Morrissey, Nelvin Cepeda, Lauryn Schroeder|\n|Dailies: 50,001 & over,Dailies: 15,001 - 50,000,Dailies: 15,000 & under||Public Service Journalism||Third Place||The Desert Sun||Palm Springs||Desert Sun: Private prisons and immigration detention centers||Here\'s a terrific example of serious, important and results-rich investigative reporting, a massive, intensive project from a paper that isn\'t among the state\'s top metros. It carefully and with excellent investigative work tracks a series of backroom deals with one of the nation\'s largest private prison businesses out of the small city of Adelanto, and largely out of public view (until it blew up into a local controversy). Deeply impressive for its writing, organization, depth and perspective. The series notes that Adelanto likes to call itself ""The city with unlimited possibilities""; the series nails how those possibilities aren\'t all good ones, for that city or for others the paper located.||Rebecca Plevin|\n|Dailies: 50,001 & over,Dailies: 15,001 - 50,000,Dailies: 15,000 & under||Public Service Journalism||Fourth Place||Los Angeles Times||El Segundo||Our Reckoning With Racism||A massive undertaking with many lengthy articles and exhibits, including an essay from its owner and a history of the paper through the lens of racism. Through it all what stuck out most (and they gave it some prominence) was a headline from 1981:|\n""Marauders From Inner City Prey on L.A\'s Suburbs."" It\'s so highly unusual, strongly positive in many ways but tangled in its mission in some others (there\'s a pervading sense almost of pleading for forgiveness), that it\'s hard to compare properly with any of the other submissions.\n|L.A. Times Staff|\n|Dailies: 50,001 & over,Dailies: 15,001 - 50,000,Dailies: 15,000 & under||Public Service Journalism||Fifth Place||The Tribune||San Luis Obispo||Substandard of living series: Tribune investigation: What it\'s like for SLO County renters stuck in bad housing||Another fine and extensive series on the world as it is for lower-income renters: terrible housing, sky-high prices, little information or recourse. There\'s also useful reader service information. Much of this isn\'t really new, but the detail and the bluntness of the presentation certainly puts it in the reader\'s face, where it belongs.||Lindsey Holden, Cassandra Garibay, Laura Dickinson|', 'The fight for water, health and equity in California’s Central Valley and beyond\nCalifornia’s five-year drought made worldwide headlines as wells went dry and thousands of people were left without water. But many of those whose taps still flow face an even more insidious threat. The state estimates that 1.5 million Californians rely on drinking water that has violated health standards. And there are 200,000 people whose water is chronically contaminated with chemicals linked to cancer and other health effects. Those numbers don’t take into account the 2 million privately owned wells that are not regulated or routinely tested for toxins. Unsafe water systems can be found all over the state, but the largest concentration is in the San Joaquin Valley, where many serve low-income communities of color.\nWater Deeply’s Toxic Taps series looks at the root causes of the safe drinking water crisis in California, how communities are organizing for change and what more needs to be done. We meet families who have had to choose between buying shoes for their children and buying bottled water. We talk to health experts about the risks of common contaminants like nitrates, arsenic and agricultural pesticides. We visit small water systems struggling to pay for technology to treat contaminated water. And we talk to regulators, legislators and organizers who are working toward a future in which clean drinking water is not just legally recognized as a human right, but is readily available to all.\nShare this Post\nWhy Drinking Water in Small California Communities Is Chronically Contaminated\nOperators of water systems in the San Joaquin Valley cannot afford to treat water tainted by toxins.\nCalifornia’s Water Crisis Hits Home in Rural Communities\nMeet residents of San Joaquin Valley towns struggling with contaminated water from community water systems and private wells.\nThe Fight Over Funding for Clean Drinking Water Projects\nSan Joaquin Valley residents rally to support Senate Bill 623, which would create a fund to clean toxic drinking water sources.\nMeet California’s Newest Water Contaminant\nCalifornia has finally regulated a carcinogen, 1,2,3-trichloropropane, that has been contaminating groundwater for decades.\nA Young California Mayor Fights for Clean Water\nJosé Gurrola, the 23-year-old mayor of Arvin, California, talks about the town’s fight for safe drinking water.\nWhat Happened to California’s Chromium 6 Regulation\nA judge in May scrapped a California regulation set in 2014 for hexavalent chromium (otherwise known as chromium-6) in drinking water. Here’s why, what the public health implications are and what happens next.\nMobile Home Communities in the Coachella Valley Face Dire Water, Infrastructure Needs\nThe eastern Coachella Valley in Southern California has a high concentration of mobile home parks that help fill a need for affordable housing. But many are not up to health and safety codes and residents face risks from unsafe water, sanitation and electricity.\nUntreated Canal Water Piped to California Homes\nThousands of rural residents in California’s Imperial Valley rely on untreated canal water for household needs like cooking and bathing.\n1,2,3-TCP in California Water Systems\nData provided by the State Water Resources Control Board.']"	['<urn:uuid:bfff6a8a-a839-42a0-947e-7b00e1831121>', '<urn:uuid:205f2df3-0dbd-4ab3-9bc9-078791b8632d>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T06:04:36.674696	14	69	1123
88	What are the key differences between the environmental approach of urban bioregions and the Sahara Forest Project in terms of their strategies for achieving sustainability?	Urban bioregions and the Sahara Forest Project represent different approaches to sustainability. Urban bioregions focus on creating dense urban cores surrounded by supportive ecosystems, emphasizing the recycling of organic wastes and nutrients back to farms and forests, while reducing dependence on imports. They aim to reconnect citizens with local ecosystems and manage resources within regional carrying capacity. In contrast, the Sahara Forest Project takes inspiration from the Namibian fog basking beetle to create self-sufficient ecosystems in desert environments, using greenhouses that harvest water from air moisture and pair with concentrated solar power to provide renewable energy. The solar panels also produce salt crystals that can be used for building materials, creating a complete restorative ecosystem with no waste.	"['No 1079 Posted by fw, June 20, 2014\nIn Part 1, Rees reviewed what he sees as the most pressing global challenges we face – climate change, ecological overshoot, and rising inequality. Against this background, he clearly defined the goal of his ground-breaking paper: to advance a precautionary, transformational approach to sustainability planning.\nIn Part 2, Dr. Rees alleged that the world community is in deep denial over the global challenges it faces. A return to the “business as usual”, growth-induced response, which got us into this mess in the first place, is clearly unsustainable. In this context, Rees frames an action plan, outlining the rationale and major elements for no-growth, “steady-state sustainability” with justice. He acknowledges that this proposed survival strategy “will seem impossibly extreme” to most capitalists, lists five essential human qualities necessary to get us on a cooperative path to sustainability, and worries that we might not be up to the challenge.\nIn Part 3, William Rees asks readers to imagine a scenario in which a major global catastrophe has precipitated a great awakening among world leaders. Shocked into action, they establish a World Assembly for Mutual Survival. The Assembly formulates an alternative conceptual framework that better models the harsh ‘reality’ they face. Among other actions, a radical shift in social-cultural norms and values is called for. To win global public support and acceptance of the plan, a worldwide social marketing campaign will be absolutely necessary. Rees anticipates objections to centralized decision-making by a World Assembly and the risk that global marketing will be perceived as a form of “brainwashing”. But calamitous times on a global scale call for bold, universal measures.\nIn Part 4, Rees argues that it’s time to end the cult of consumerism, which is ecologically destructive, and to return to policies and programs to strengthen community, cooperation, and common interest. To accomplish this, he says we need to find a new balance between globalization and localization. Towards this end he proposes some new rules and regional trade treaties to give nations more flexibility in reacting to changing circumstances. In his closing thoughts on relocalization, Rees explains, in, at times, challenging technical language, the benefits of reconnecting urban regions to essential, self-reliant local ecosystems, thus ensuring recycling of organic wastes and nutrients back to farms and forests, and simultaneously reducing dependence on imports for life’s necessities.\nAs in previous parts, hanging indented subheadings are used in Part 4 to highlight main ideas and facilitate browsing, endnotes are substituted for footnotes, and text highlighting is added for emphasis. Brackets ( ) are again employed in the body of the text to identify the numerical links to the endnote citations.\nTo access a free PDF copy of Rees’ original 20-page paper, click on the following linked title.\nBASIC POLICIES FOR GLOBAL/LOCAL SUSTAINABILITY\nEnd “cult of consumerism”, which is ecologically destructive\nThe cult of consumerism is not only spiritually empty but also ecologically destructive. To repair the failing ecosystems and life-support functions upon which we all depend, steady-state thinking emphasizes investment and conservation over spending and consumption. It also must work to restore trust in government as needed to mend our social safety nets and cultivate mutually supportive relationships among social groups.\nAbandon neo-liberals’ unbridled confidence in free market ideology\nBoth ecological and social sustainability require that we abandon neo-liberals’ unbridled confidence in markets as the sole wellspring and arbiter of social values. Climate change, fisheries collapses, ecosystems degradation and illegal sweatshops are all examples of gross market failure. The World Assembly must relegitimize national government intervention in markets to protect the common good; the world needs sound planning, selective reregulation and comprehensive extra-market adaptation strategies for global change.\nTo fix broken markets, adopt policies favouring “true-cost economics” — that prices reflect full costs of production and use\nA major goal is to ensure that prices reflect the full costs of production and use. True-cost economics recognizes the need to:\nRewriting the social contract\nIntroduce programs to strengthen community, cooperation, and common interest\nConsistent with the principles of community, co-operation and people’s common interest in an orderly transition, the World Assembly would generate guidelines for individual nations to renew the social contract and repair social safety nets. National plans would include programmatic tax reform based on recognition that taxation is society’s means of pooling resources in service of the common good, particularly in times of widespread threat. Specific elements of the program might include:\nRETHINKING GLOBALIZATION, RESTORING LOCALITY\nFive reasons to rethink globalization and restore locality\nThe global survival plan would also partly unravel today’s increasingly unsustainable eco-economic entanglement of nations. The rationale is clear:\nFirst, the human mind is incapable of adequately understanding, let alone safely controlling, the behaviour of complex global-scale systems under stress. (52) On the other hand, local/regional human communities and ecosystems are more manageable and any negative “surprises” will be confined to the affected region.\nSecond, unfettered trade allows trading regions to exceed their local carrying capacities with short-term impunity while it both depletes remaining reserves of natural capital and accelerates global pollution, increasing the risk to all. Global overshoot would be eliminated if each region were sustainably managed.\nThird, the economic restructuring (e.g. national/regional economic specialization) required for global market efficiency reduces domestic economic diversity and resilience, destroys livelihoods and sometimes whole communities and devalues the skills of local populations. Moreover, because specialization makes people dependent on trade for everything no longer produced locally, it increases their vulnerability to global change — crop failures, energy bottlenecks, geopolitical instability and even changes in market conditions. (53) What will China do when it can no longer feed itself because global surpluses are inaccessible or have disappeared?\nFourth, global economic integration is partially a product of abundant cheap energy. With rising energy costs (the end of the fossil fuel bonanza?) the relocalization of production in heavy-goods sectors affected by rising transportation costs is already occurring and, as energy supplies shrink, the rest of the economy will necessarily follow.\nFifth, unlike capital, many people feel affinity to their home communities (and are not fluidly mobile in any case).\nRevise World Trade Org. rules and regional trade treaties to give nations more flexibility in reacting to changing circumstances\nTo rebalance the tension between the global and local economies, the world community should revise WTO rules and similar regional trade treaties (e.g. NAFTA, the European Union). Nations and regions will be able to adapt creatively to emerging conditions only if they are free to:\nDo not abandon global trade: “Trade if necessary, but not necessarily trade”\nLet’s be clear that rebalancing does not mean abandoning international trade. Trade does provide an important buffer in the event of domestic shortages caused by drought or disaster; it is necessary to acquire vital goods that cannot be produced locally. In any event, some countries and regions with large ecological deficits will remain highly trade-dependent at least until their populations fall to more sustainable levels. The rule for resilient local economies should be: export only true ecological surpluses (no net loss of productive natural capital) and import only important commodities that cannot reasonably be sourced at home. “Trade if necessary, but not necessarily trade” serves as convenient shorthand.\nBringing it back home: relocalization\nThose people…living in relatively self-reliant, organic, village-scale settlements should be able to ride the change with minimal difficulty and will emerge into the post-civilization phase intact. (54)\nReconnect urban regions to essential, self-reliant local ecosystems, thus ensuring recycling of organic wastes and nutrients back to farms and forests, thereby reducing dependence on imports for life’s necessities\nThe uncertainties associated with global change also have important implications for urban form and function. (55) Urban designers and planners should begin now to rethink cities — or rather urban regions — so they function as complete quasi-independent human ecosystems. This is the ultimate form of functional bio-mimicry. (56)\nThe city-as-ecosystem requires the relocalization of many ecological functions. Contemporary urbanization, combined with globalization, has transformed local, integrated, cyclical human ecological production systems into global, horizontally disintegrated, unidirectional throughput systems. (57) Rather than being recycled on the land, essential nutrients contained in grain from the Russian steppes or Canadian prairies wind up in distant oceans, irreversibly discharged from urban sewage outfalls all over the world. The soils of some of the world’s most important breadbaskets have lost half or more of their natural nutrients in just a century of mechanized agriculture.\nThe least vulnerable and most resilient urban system might be a new form of urban-centred bioregion (or eco-city state) in which a densely built-up core is surrounded by essential supportive ecosystems. The goal is to consolidate as much as possible of the human community’s productive hinterland in close proximity to its consumptive centre. Organic “wastes” and nutrients could then be economically recycled back to farms and forests.\nSuch a bioregionalized city would reconnect its human population to “the land.” Citizens would see themselves to be directly dependent on local ecosystems and thus have a strong incentive to manage them sustainably. Less dependent on imports for the necessities of life, bioregionally focused populations would be partly insulated from external climate vagaries, resource shortages and distant conflicts.\nIdeally, regional eco-cities would develop economic and social planning policies to facilitate reducing their residents’ ecological footprints to a globally equitable 1.8 gha per capita. This is technically possible (58) and the implicit greater equity could actually improve individual and community well-being. (59) In any case, footprint contraction is essential to protect the regenerative capacity of nature and, where possible, to maintain populations within regional carrying capacity.\nClearly, the bioregional vision would require new governance structures that devolve significant control over their extended territories and resource hinterlands to eco-city states. These mechanisms would function to manage land, ecosystems and other resources vital to sustaining human life in the long-term collective interests of the entire community. This, in turn may require stinting some customary private property rights. On a planet in overshoot, it is unacceptable for landowners to destroy through “development” the life-support functions required by everyone. Protecting a redefined commons would be a fundamental goal of any socially just and sustainable steady state.\n49 For more on how an equitable and effective carbon tax can be modelled, see Lee, Marc. 2011. Fair and Effective Carbon Pricing: Lessons from BC. Vancouver: Canadian Centre for Policy Alternatives.\n50 The CCPA has produced many reports outlining ideas for progressive tax reform. See for example Lee, Marc and Iglika Ivanova. 2013. Fairness by Design: A Framework for Tax Reform in Canada. Ottawa: Canadian Centre for Policy Alternatives.\n51 For more on the transition to green jobs, see the Climate Justice Report by Lee, Marc and Amanda Card. 2012. A Green Industrial Revolution: Climate Justice, Green Jobs, and Sustainable Production in Canada. Ottawa: Canadian Centre for Policy Alternatives.\n52 This is perhaps the most powerful argument against attempts to “geo-engineer” solutions to climate change. Unintended systems responses are inevitable, unpredictable and most likely to be negative.\n53 Rees, W.E. 2002. Globalization and sustainability: Conflict or convergence? Bulletin of Science, Technology and Society22(4):249-268., // Rees, W.E. 2012. Cities as Dissipative Structures: Global Change and the Vulnerability of Urban Civilization. Chapter in M.P. Weinstein and R.E. Turner (eds.), Sustainability Science: The Emerging Paradigm and the Urban Environment. New York: Springer // Kissinger, M. and W.E. Rees. 2009. Footprints on the Prairies: Degradation and sustainability of Canadian agriculture in a globalizing world. Ecological Economics 68:2309-2315.// Kissinger, M. and W.E. Rees. 2010. Importing terrestrial biocapacity: The U.S. case and global implications. Land Use Policy27:589-599.\n54 Mare, E.C. 2000. Sustainable Cities: An Oxymoron? Seattle: Village Design Institute.\n55 Register, R. 2006. EcoCities: Rebuilding Cities in Balance with Nature (rev. ed.). Gabriola Island, BC: New Society Publishers.\n56 Rees, W.E. 2012. Cities as Dissipative Structures: Global Change and the Vulnerability of Urban Civilization. Chapter in M.P. Weinstein and R.E. Turner (eds.), Sustainability Science: The Emerging Paradigm and the Urban Environment. New York: Springer.\n57 Rees, W.E. 1997. Is “sustainable city” an oxymoron? Local Environment 2:303-310. // Rees, W.E. 2012. Cities as Dissipative Structures: Global Change and the Vulnerability of Urban Civilization. Chapter in M.P. Weinstein and R.E. Turner (eds.), Sustainability Science: The Emerging Paradigm and the Urban Environment. New York: Springer.\n58 von Weizsäcker, E., K. Hargroves, M. Smith et al. 2009. Factor 5: Transforming the Global Economy through 80 per cent Increase in Resource Productivity. UK and Droemer, Germany: Earthscan. ISBN 978-1-84407-591-1.\n59 Wilkinson, R. and K. Pickett. 2010. The Spirit Level: Why Equality Is Better for Everyone. London: Penguin Books.\nFAIR USE NOTICE – Click on above tab for details', ""Biomimicry - How nature can help us build better\n“Nature is like a catalogue of products and all of these have benefited from a 3.8 billion year research and development period - given that level of investment it makes sense to use it.” declared architect Michael Pawlyn at this year’s Annual CE100 summit. He has a point. Biomimicry is not a new idea in the design of buildings (look at Gaudi’s Sagrada Familia), but using it for sustainable design is.\nInterior of the Sagrada Familia, Barcelona (image by SBA73 from Sabadell, Catalunya (Tot conflueix / All's conected) [CC BY-SA 2.0], via Wikimedia Commons)\nBiomimicry can help us build more sustainably, whilst doing less bad and doing more good. By tapping into nature’s natural synergies, optimised shapes and textures, and re-emerging patterns, we can be more resource efficient and streamline building and infrastructure designs.\nPawlyn races through examples of nature’s ability to design incredible systems, from ‘Dog vomit slime mould’ (apparently providing insights to better urban planning) to learning from the structure of bird bones to lightweight a concrete structure (reducing the material quantity required by 50%!).\nThe Sahara Forest Project, which aims to bring vegetation back to the desert, epitomises this approach perfectly. Inspired by the Namibian fog basking beetle – which is able to harvest water from moisture in the air – the design is for a greenhouse which allows plants to grow with minimal water and can actually harvest more water than it needs. The eventual effect is that vegetation starts to grow around the greenhouse, spreading out to the once barren desert. Pairing this design with concentrated solar power (CSP) will provide a constant source of renewable energy, and the mirrored panels will provide shading for vegetation. The CSP can also use waste heat from the greenhouse and the panels collect salt crystals on their surface over time, which can then be harvested to make light weight building blocks. This will create a self-sufficient, restorative ecosystem in the heart of the desert (with no waste… just like nature).\nThe Sahara Forest Project by Exploration Architecture\nAnother example of how biomimicry can help design more sustainably is the work of Japanese architect Shigeru Ban. At this year’s Ecobuild, Ban gave an inspiring talk about some of his projects made out of recycled paper tubes, built into honeycomb roofs. Lightweight, fluid and inspired by nature, he is increasingly designing these stunning sustainable structures for disaster relief projects.\nSeguru Ban's 'Cardboard Cathedral', Christchurch New Zealand (image by Jocelyn Kinghorn (Flickr: Way Up High) [CC BY-SA 2.0], via Wikimedia Commons)\nPerhaps most importantly, designing with nature in mind can help create healthier, happier more productive spaces for us to live and work in. Using passive design to maximise natural light, fresh (un treated) air has been proven to improve health and productivity (see our previous blog on this here).\nBy paying attention to nature’s catalogue, we can create a new language for more resource efficient sustainable design and integrate buildings and infrastructure into a circular economy.\nJesse Putzel is a Senior Sustainability Advisor at BAM\ncomments powered by Disqus Back to Insights""]"	['<urn:uuid:6846befb-89c5-41b4-a8d8-cefa815d61f3>', '<urn:uuid:6ec8d476-a6b0-4193-a976-5d934a20e881>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T06:04:36.674696	25	118	2615
89	What role do specialized breeding techniques and medical knowledge sharing play in developing resilient populations, both in dogs and humans?	In Alaskan huskies, breeding techniques focus on combining different dog breeds to produce offspring that excel at specific tasks, with an emphasis on developing desirable traits and monitoring health conditions through knowledge of bloodlines. Similarly, in human populations, medical knowledge sharing along trade routes, particularly regarding smallpox prevention, led to the development of variolation techniques that spread from China and India to Anatolia, eventually evolving into modern vaccination practices that successfully eradicated smallpox globally.	"[""What Are the Physical Characteristics of the Alaskan Husky?by Betty Lewis\nThe American Kennel Club recognizes the Siberian husky and Alaskan malamute in its working group, but the AKC does not recognize a breed called an Alaskan husky. The fact is, the Alaskan husky is not a breed but is a category of dog, bred for a specific purpose. His physical characteristics depend on his purpose and his parents.\nThe Alaskan husky's past has everything to do with his present -- and his physical presence. He got his start courtesy of Canadian and Alaskan mushers who needed a hardy working dog. His early jobs included delivering supplies to distant locations, pulling logs, ferrying people place to place and racing for money. Dogs in Inuit villages were bred with other dogs to produce offspring that would excel at whatever tasks were deemed necessary. The result was the Alaskan husky.\nBuilt for Work\nWhen it comes to the Alaskan husky, what's on the inside matters more than what's on the outside. He's been made to work, and his physical characteristics reflect what his job is. Desirable traits include speed, size, coat, gait, stamina and good feet. Greyhounds, Eskimo dogs, Siberian huskies, border collies and German shorthaired pointers are among the breeds usually chosen to breed a litter of Alaskan huskies. For example, a greyhound bred to a Siberian husky will likely produce a fast sled-racing dog. Since he's a mixed-breed, no kennel clubs recognize the Alaskan husky as a specific breed.\nOne Dog, Many Looks\nBased on the dogs typically used for breeding an Alaskan husky, a few generalizations can be applied to the dog's physical characteristics. He usually tips the scales between 38 and 50 pounds, making him a medium-size dog. His coat often ranges between short and medium length, coming in a wide variety of colors and patterns. His head shape may be wedge-shaped if he has Eskimo bloodlines, or he may have a longer muzzle, such as a hound-type dog would have. According to VetStreet.com, since most Alaskan huskies have some spitz bloodline in them, pricked ears are one of the few common physical characteristics among Alaskan huskies.\nAnother common physical characteristic of most Alaskan huskies is their need for exercise. Considering his roots lay in the work he was born to do, it's not surprising that he needs to keep busy, even if he doesn't have a formal job to do. Depending on his heritage, he might like pulling carts or going for long runs with his people, as well as performing agility or herding exercises. Regardless of his heritage, it's wise to engage an Alaskan husky -- left alone and to his own devices, without a form of stimulation or exercise, he is prone to destructive behavior.\nBecause they tend to be bred from medium-size dogs, Alaskan huskies tend to have life spans of 10 to 12 years. Alaskan huskies may develop health problems that are common to some purebreds, such as the progressive retinal atrophy that sometimes occurs in Siberian huskies. Knowing your Alaskan husky's bloodlines will help you determine what conditions to watch for.\n- NA/AbleStock.com/Getty Images"", 'This article is the second in a series on the spread of disease along the Silk Roads which examines the ways in which people have historically responded to illness and explores how we might approach newly arising challenges today. It uses the Silk Roads as an instructive example of the benefits of an interconnected world built on collaboration and timely and reliable knowledge sharing. This article details the spread of smallpox along the Silk Roads and the transmission of novel public health measures to combat it, including variolation and, later, vaccines.\nWherever people, animals and goods have moved and brought enriching effects, undesirable phenomena such as disease have also been transmitted on a broad scale. Historically, trade and movement have inevitably played a major role in the spread of infectious disease. In addition to diseases caused by bacteria, such as Plague, many viruses have been transmitted via movement along the Silk Roads. One notable example of a viral disease which has been prevalent throughout much of human history is smallpox. However, just as the disease itself travelled the Silk Roads, so too did a number of public health measures designed to combat it, including an early precursor to vaccinations, a practice known as “variolation”. Indeed, the first ever vaccines produced were used to protect people from catching smallpox, which, due to large scale international vaccination programmes in the 20th century, has since been successfully eradicated worldwide.\nSmallpox is an infectious disease caused by the “Variola” virus characterised by the formation of small sores all over the body. The disease spreads via contact with an infected person or from a contaminated item such as clothing or bedding. Although the exact origins of smallpox are unknown, there is evidence of the disease having been present in Ancient Egypt from as early as the 3rd century BCE. It appears that trade played an early role in spreading smallpox and there is speculation amongst historians that traders from Egypt might have transmitted the disease to the Indian Subcontinent sometime in the 1st millennium BCE. Some of the earliest written descriptions of smallpox date from 4th century CE China and, as trade along the Silk Roads increased in the 6th century CE, the disease spread rapidly to Japan and the Korean Peninsula. Notably, smallpox broke out between 735 – 737 CE in Japan, where it is believed to have killed up to one-third of the population.\nBy the 7th century CE, as trade and travel along the Silk Roads increased, smallpox became “endemic” (outbreaks regularly reoccurring within a given population) in the Indian Subcontinent. Muslim expansion during this time spread smallpox into Northern Africa, Spain and Portugal. In the 9th century CE, the Persian physician Razi, an early proponent of experimental medicine and chief physician of Baghdad and Rey hospitals in the Abbasid Caliphate, produced one of the most definitive descriptions of smallpox and the first account differentiating it from other similar diseases such as measles and chickenpox. By the 10th century smallpox had spread throughout Anatolia, with another wave of increased activity along the Silk Roads in the 13th century CE causing the disease to become endemic in previously unaffected areas such as Central and Northern Europe. In the 15th century, Portuguese expeditions to the West Coast of Africa and the establishment of new trade routes introduced the disease to further previously unaffected areas.\nDespite the fact that the movement of people and goods across vast distances has undoubtedly aided the spread of disease, the medical sciences have been one of the direct beneficiaries of the resulting intercultural exchanges. An excellent example of this is the development and transmission of “variolation”, a practice which was an early precursor to smallpox vaccination. There are early accounts of priests from the Indian Subcontinent travelling the Silk Roads popularising the practice of what they called “tika”, an early effort at inoculation (the introduction of a disease-causing agent in order to produce immunity to a specific disease). This involved taking matter from a smallpox patient’s sores and applying it to a small wound on an uninfected person, the idea being that the uninfected person would develop only a very mild case of the disease and, on recovery, become immune to catching a severe case in the future.\nThis practice may have developed independently in the Indian Subcontinent or, alternatively, practitioners might have learned it from Muslim physicians, who themselves came into contact with the practice via travel and trade with China. As early as the 1400s, medical healers in China had realized that those who survived smallpox did not catch the illness again and inferred that exposure to the illness protected a person from future instances of it. This observation gave rise to a second important public heath measure which was that those who had contracted the disease and survived were able to treat and care for new patients as they had incurred a natural immunity and were unlikely to become ill a second time. In order to transfer this immunity to new patients, Chinese doctors would grind smallpox scabs into a powder and insert it into a person’s nose with a long silver pipe. If only a very small amount of the virus was ingested that person would have a mild experience of the disease and be immunized for life. Similar practices, of “variolation”, were also documented in Africa in accounts from what is today Sudan. By the 16th century, this practice was a widespread public health measure enacted across many regions of the Silk Roads reaching as far west as Anatolia, having been introduced via descriptions from travellers and merchants.\nThroughout history, as we have developed better knowledge of how diseases are transmitted, how they can be treated, and the relevant public health measures that prevent their spread, a major trend for many endemic diseases has been the gradual reduction in their impact over time. In the case of a number of viral diseases, these measures have included the development of vaccinations, which, as in the practice of variolation, have an historic precedent in medicine transmitted along the Silk Roads. In the 18th century, the English physician Edward Jenner built on the idea of variolation and made a major contribution to the development of the modern smallpox vaccine. He observed that those who had contracted cowpox, a similar but milder viral infection, rarely went on to catch smallpox later in life. It is from the disease cowpox, known in Latin as variola vaccina, that we derive the term “vaccine”. Coordinated international vaccination programmes throughout the 20th century led to the eradication of smallpox in 1980, and today outbreaks of the disease no longer occur anywhere in the world. The eradication of smallpox is a testament to the development of the medical sciences over a long period of time, building on and sharing pre-existing medical knowledge and coordinating public health initiatives. A natural precursor to this vaccination dates back many hundreds of years with its origins in the many exchanges in the medical sciences taking place along the Silk Roads.\nThe Spread of Disease along the Silk Roads: Plague\nThis article explores the spread of plague, known as ‘the Black Death’, across the Silk Roads of the 14th Century CE. It examines ways in which people responded to the disease and looks at how we can respond to newly arising challenges today, utilizing the Silk Roads as an instructive example of the benefits of an interconnected world built on collaboration and timely and reliable knowledge sharing.\nThe Spread of Disease along the Silk Roads: The Development of Medical Botany and Pharmacology\nThis article is the third in a series on the spread of disease along the Silk Roads. Its outlines the early development of medical botany and pharmacology during the Middle Ages and identifies the role of the Silk Roads in helping fuel an incredible period of scholarship, particularly within the field of medicine, during the 8th and 9th centuries CE.']"	['<urn:uuid:9135fe69-fb7b-4f82-b4ec-fa780602253a>', '<urn:uuid:f8ffdf4b-360c-495c-a308-faed95336603>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T06:04:36.674696	20	74	1846
90	nutrients in cornmeal calcium magnesium daily value compare recommended salt intake	In 100 grams of whole-grain cornmeal, calcium provides 1% of daily value, while magnesium provides 32% of daily value. Regarding salt intake, the maximum daily recommended amount is 6g per day, though 75% of the population are unaware of this limit.	"[""How many vitamins in Whole-grain Cornmeal\nYour body needs vitamins in adequate intake to work properly.\nNevertheless How many Vitamins in can I find in this food? Discover here the amounts present in each of the listed vitamins and useful facts about them.\nSome of the vitamins found in Whole-grain Cornmeal are: Vitamin A (214 IU), Vitamin B-9 (25 mg) and Vitamin B-3 (3.63 mg).\nVitamin A is a fat-soluble vitamin whose absorption goes through the digestion process. Subsequently, this vitamin can be used for body functions or sent for storage in the liver and fat cells.\n214 IU of Vitamin A can be found on every 100 grams of Whole-grain Cornmeal, the 7% of the total daily recommended Vitamin A intake.\nThe American Heart Association recommends obtaining health benefits of vitamin E antioxidant. Vitamin E is a group of eight compounds called tocopherols and tocotrienols which reduces cholesterol and the risk of developing diabetes, Alzheimer's disease, and cancer.\nIn 100 grams of Whole-grain Cornmeal, you can find 0.42 milligrams of Vitamin E. It provides the 3% of the daily recommended value for the average adult.\nVitamin K, also called Phylloquinone, offer protection against health problems like Osteoporosis, Brain health problems, Arterial calcification, varicose veins, and specifics cancer diseases -Prostate cancer, lung cancer, liver cancer, and leukemia.\n0.3 micrograms of Vitamin K can be found on every 100 grams of Whole-grain Cornmeal, the 0% of the total daily recommended Vitamin K intake.\nVitamin B1 is one of the eight water-soluble B vitamins. it plays an essential role in the production of energy from food, the conduction of nerve impulses and synthesis of nucleic acids.\n100 grams of Whole-grain Cornmeal contains 0.38 milligrams of Vitamin B-1, that’s the 25% of the daily recommended value for an adult.\nThe main functions of vitamin B2 (riboflavin) are connected to its role as a helper the body to convert vitamin B6 and vitamin B9 into active forms, neutralize ‘free radicals’ that can damage cells and produce energy converting food into glucose.\nIn 100 grams of Whole-grain Cornmeal, you can find 0.2 milligrams of Vitamin B-2. It provides the 12% of the daily recommended value for the average adult.\nVitamin B3 is one of the water-soluble B vitamins. It is also known as niacin (nicotinic acid) and plays an important role in the disease risk reduction of diseases like Cancer and Diabetes.\nIn 100 grams of Whole-grain Cornmeal, you can find 3.63 milligrams of Vitamin B-3. It provides the 18% of the daily recommended value for the average adult.\nVitamin B5 is known as pantothenic, is really nice strengthening the immune system, enhance the level of hemoglobin in the human body and assists the liver in metabolizing toxic substances.\n100 grams of Whole-grain Cornmeal contains 0.42 milligrams of Vitamin B-5, that’s the 4% of the daily recommended value for an adult.\nFolic acid (Vitamin B9) is essential for the proper functioning of the body and healthy living. It plays an important role in maintaining healthy digestive system, hair, skin, kidneys and eyes.\n100 grams of Whole-grain Cornmeal contains 25 micrograms of Vitamin B-9, that’s the 6% of the daily recommended value for an adult.\nMinerals in Whole-grain Cornmeal\nMinerals are inorganic substances required in small amounts by the body for a variety of different functions. Your body needs larger amounts of some minerals, such as calcium, to grow and stay healthy. Other minerals like copper or iodine are called trace minerals because you only need very small amounts of them each day.\nSome of the minerals found in Whole-grain Cornmeal are: Potassium (287 mg), Phosphorus (241 mg) and Magnesium (127 mg).\nThis vital mineral is best known to strengthen bones, teeth, the heart, and slash your risk of developing a number of diseases like hypertension or seizures.\nIn 100 grams of Whole-grain Cornmeal, you can find 6 milligrams of calcium. It provides the 1% of the daily recommended value for the average person.\nIron is an essential element for almost all living organisms as it participates in a wide variety of highly complex metabolic processes including deoxyribonucleic acid (DNA) synthesis, and oxygen/electron transport.\n100 grams of Whole-grain Cornmeal contains 3.45 milligrams of iron, that’s the 19% of the daily recommended value for one person.\nAn adequate intake of potassium is important to maintain normal body growth, control the acid-base balance, build proteins, regulate digestive functioning, build muscle, and control the electrical activity of the heart.\n287 milligrams of potassium can be found on every 100 grams of Whole-grain Cornmeal, the 6% of the total daily recommended potassium intake.\nMagnesium is an essential element for energy storage in the body’s cells. This mineral provides energy for almost all metabolic processes, being necessary for more than 300 chemical reactions in the human body.\nIn 100 grams of Whole-grain Cornmeal, you can find 127 milligrams of magnesium. It provides the 32% of the daily recommended value for the average adult.\nNext to calcium, phosphorus is the most abundant mineral in the body and an important role in activities for different body parts like the brain, kidney, heart and blood. Health benefits of phosphorous include cellular repair, protein formation, hormonal balance, improved digestion, proper nutrient utilization, and healthy bone formation.\n241 milligrams of phosphorus can be found on every 100 grams of Whole-grain Cornmeal, the 24% of the total daily recommended phosphorus intake.\nThe optimal sodium intake allows the creation of electrolytes and an essential ion present in the extracellular fluid (ECF). However, high levels of sodium in the body are associated with high blood pressure and hypertension.\n100 grams of Whole-grain Cornmeal contains 35 milligrams of sodium, that’s the 2% of the daily recommended value for one person.\nZinc is an really vital mineral for the human body as it helps in regulation of the cells production in the immune system. The health benefits of Zinc include reduction of stress levels, control of diabetes, digestion, proper functioning of immune system, and energy metabolism.\n100 grams of Whole-grain Cornmeal contains 1.82 milligrams of zinc, that’s the 12% of the daily recommended value for one person.\nCopper is an essential trace mineral present in all body tissues. This Mineral regulate various physiologic pathways, such as iron metabolism, connective tissue maturation, neurotransmission and energy production.\n0.19 milligrams of copper can be found on every 100 grams of Whole-grain Cornmeal, the 10% of the total daily recommended copper intake.\nManganese mineral is important in the healthy bone structure metabolism and formation -helping to create essential enzymes for building bones- play a key role in the proper functioning of the thyroid gland.\n0.49 milligrams of manganese can be found on every 100 grams of Whole-grain Cornmeal, the 25% of the total daily recommended manganese intake.\nSelenium is an essential trace mineral that the body needs to stay healthy. Scientists and researchers suggests that Selenium prevent certain cancers such as stomach, colon, bladder, lung, skin, esophagus, and prostate.\nIn 100 grams of Whole-grain Cornmeal, you can find 15.5 micrograms of selenium. It provides the 22% of the daily recommended value for the average adult.\nCalories in Whole-grain Cornmeal\nWe need an average of 2,000 calories per day to maintain body functions. 100 grams of Whole-grain Cornmeal have 362 calories, the 18% of your total daily calorie needs.\nYounger people generally need more calories than older people. There are various gender and age groups to calculate the average Calories intake per day.\nA Sedentary women aged 14 to 25 years needs between 1,800 and 2,000 calories daily. However, a sedentary women aged 26 to 50 need 1,800 calories, while high active women with the same age need 2,200 calories.\nFats and Cholesterol\n100 grams of Whole-grain Cornmeal contain the 6% of your total daily needs: 3.59 grams of total fat.\nAn average adult needs 65 grams of total fat per day. 65 grams fat equals to the 30% of calories consumed by humans and represents the estimated daily needed for a 133-lb. person to maintain her or his weight. For a 167-lb. person the estimated daily needed are 80 grams fat.\nThe AHA (American Heart Association) recommends limiting your daily cholesterol intake to less than 300 milligrams. Less than 200 if you are at a high risk of heart disease.\nThe AHA (American Heart Association) recommends limiting your daily saturated fat intake to less than 130 milligrams.\n100 grams of Whole-grain Cornmeal contain the3 of your total daily needs saturated fat, exactly 0.5 grams.\nMonounsaturated fatty acids\nPolyunsaturated fatty acids\nData Facts Table of Whole-grain Cornmeal"", 'Five salt myths that could be damaging your health\nMost of us are well aware that high blood pressure is a major risk factor for some of the most common killers, such as stroke and coronary heart disease. But are you aware just how big a role salt consumption can play in developing high blood pressure in the first place?\nEven if you are clued up on this, there’s still a chance you’re not entirely sure how much salt you’re consuming – especially if you regularly eat processed foods (things like ready-made sauces, basically many of the foods you’re not preparing from scratch), which, according to Consensus Action on Salt & Health (CASH), around 75% of the salt in our diets comes from.\nAccording to recent research from low-salt alternative brand LoSalt, nearly two-thirds of people (63%) are not actively reducing their salt intake – which indicates there’s still a lack of awareness around the white stuff and its associated health risks.\nSo what else do you need to know about salt intake?\nWe’ve done some salt myth-busting to point you in the right direction…\nMYTH: You can eat as much salt as you like\nFalse! We need salt to survive, so cutting it out entirely is NOT the goal. However, too much of it can lead to potentially serious problems down the line. As many as three-quarters (75%) of the population don’t realise that 6g is the maximum daily recommended salt intake, according to the LoSalt survey.\nThere’s two components in salt: sodium and chloride, and it’s the sodium that is doing the damage because it can lead to high blood pressure. Although most people with high blood pressure don’t realise they have it, it is responsible for around 50% of heart disease cases and 60% of strokes, according to the World Health Organisation.\nMYTH: Rock salt is healthier than regular salt\nMany people believe trendier sea and rock salts are healthier than regular salts, but this is incorrect.\nNutritionist and dietitian Azmina Govindji says: “They contain the same amount of sodium chloride: 100%! Sea salt may contain traces of other minerals, but the levels are too low to have a health benefit and so it is not healthier for you than any other salt.”\nMYTH: Saltless food is bland\nExtra salt added during cooking makes up 20% of our salt intake, notes Govindji. Granted, this might not sound like much, but it’s the chunk of our intake which we’re most in control of.\nWe might think our dinner will taste bland without a sprinkling of salt, but this isn’t strictly true.\n“By reducing a little here and there, you’ll soon find your taste buds become accustomed to not having such high levels of salt and you’ll have less need for it in cooking and for seasoning food,” says Govindji.\nMYTH: There are no alternatives to salt\nFor those who really can’t go without the taste of salt, brands like LoSalt offer an alternative to the traditional white stuff. It still contains all the flavour of normal salt, but contains 66% less sodium.\nAlso, try herbs and spices, vinegar or a dash of lemon instead to give your food the punch of flavour that salt usually would. Dill tastes great with fish, rosemary is wonderful with meat, and basil will finish off a pasta dish perfectly.\nMYTH: Cutting out salt in cooking is all you need to do\nNot adding salt to cooking is a good start – but it’s important to remember that this is only a small part of the solution, and there’s plenty more we can do.\nThe food we buy fresh in supermarkets can still be salt heavy, so always read the labels. If you can’t always cook from scratch, choose low-salt options and avoid the red traffic light on food labels!\nEat notoriously high-salt foods, like cheese, bacon, ham, salted and roasted nuts and salami, in moderation, and watch out for ready meals, pizzas, pasta sauces and bread. They’re usually far higher in salt than you might think!']"	['<urn:uuid:3e9db625-3a0c-429c-86da-7dd9e5b188b3>', '<urn:uuid:27e2ab2d-22f1-4776-9702-5e6acad94fb8>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T06:04:36.674696	11	41	2095
91	I'm studying cognitive science and I'm curious: how do hand movements help us think and what kind of professional therapy uses physical movement with patients?	Hand movements help us think by leveraging our experience with manipulating objects in space - when we make turning movements with our hands, it helps us mentally rotate and visualize objects because we have extensive experience seeing how objects move when we manipulate them. As for professional therapy using movement, music therapy is an evidence-based health profession that uses movement along with music to help patients with physical rehabilitation, motor function, and overall movement capabilities, particularly for conditions like Parkinson's disease.	"['Language is our most powerful tool for thinking. We talk to other people to solve problems, and we also use internal monologues to guide our thinking. However, there are some things that language is not so good at. One particular limitation of language is in describing and manipulating three-dimensional space. For example, when you want to describe the size of an object, you often resort to comparing an object to something of known size (“That dog was the size of a small car!”) or using gestures to indicate sizes.\nMovements of your hands and arms seem particularly well-suited to thinking about space. We have to configure our hands to grasp objects, and so we must have ways to represent sizes in order to plan those movements. In addition, the movements themselves provide a method for changing the location objects and their orientation. In a store, the price of many objects is printed on the bottom, and so you just pick up the object and turn it over, effortlessly exposing the bottom.\nDoes this ability to move objects around also help you to think about the movements of objects that aren’t physically present? This question was addressed in a paper in the February, 2011 issue of the Journal of Experimental Psychology: General by Mingyuan Chu and Sotaro Kita.\nThese researchers examined people’s performance in a mental rotation task. In mental rotation, people see pictures of two objects. The objects are either identical or one is the mirror image of the other. In addition, the objects are rotated through some angle so that they are each in a different orientation. Examples of the figures used in studies like this are in the figure. The top row shows a pair of identical objects at a moderate degree of rotation. The bottom row shows objects that are mirror images.\nStudies of mental rotation started with the work of Roger Shepard and his students in the 1970s. Participants are asked to press a button as soon as they know whether the objects are identical or are mirror images. The classic finding of this work is that as the difference in angle of rotation between the two objects increases, people take longer to respond and they make more errors. This pattern of findings suggests that people may be mentally rotating the objects to place them in the same mental orientation in order to compare them.\nIn one study, Chu and Kita used this mental rotation task and told people they could take as much time as they needed to solve the problems. They looked at whether people spontaneously made hand movements in which they rotated their hands as if they were rotating the objects. In this study, people were far more likely to make rotating hand movements when there was a big difference in orientation between the objects than when there was a small difference in orientation. That is, when the problems got difficult, people seemed to use hand movements to help them rotate an object that wasn’t actually there.\nIn a second study, participants were either explicitly encouraged to use gestures or they were forbidden to use gestures. In the case where people were forbidden to use gestures, they had to sit on their hands. In this case, people made many fewer errors in their judgments when they were allowed to gesture than when they were not.\nWhy do gestures help you imagine rotations?\nBecause you often use your arms and hands to change the position of objects in space, your visual system has a lot of experience witnessing these changes in position when you make particular kinds of movements. You have picked up lots of objects and turned them over, and so there is a strong association between these movements and the changes in what you can see in the world. This close connection between body movements and the state of the world may come to influence your mental images as well. Turning your hands in a particular direction supports thinking about the object moving in that direction, because these turning movements often led to movements of objects in that direction in the past.\nThat means that when you are trying to solve a difficult spatial problem, it is a good idea to give yourself room to move around. These movements may actually help you think about space and movements through space more easily.', 'What is Music Therapy?\nMusic Therapy is the clinical and evidence-based use of music interventions to accomplish individualized goals within a therapeutic relationship by a credentialed professional who has completed an approved music therapy program.\nMusic Therapy is an established health profession in which music is used within a therapeutic relationship to address physical, emotional, cognitive, and social needs of individuals. After assessing the strengths and needs of each client, the qualified music therapist provides the indicated treatment including creating, singing, moving to, and/or listening to music. Through musical involvement in the therapeutic context, clients\' abilities are strengthened and transferred to other areas of their lives. Music therapy also provides avenues for communication that can be helpful to those who find it difficult to express themselves in words. Research in music therapy supports its effectiveness in many areas such as: overall physical rehabilitation and facilitating movement, increasing people\'s motivation to become engaged in their treatment, providing emotional support for clients and their families, and providing an outlet for expression of feelings.\nWhat Music Therapy Is... and Is Not\nThe American Music Therapy Association (AMTA) supports music for all and applauds the efforts of individuals who share their music-making and time; we say the more music the better! But clinical music therapy is the only professional, research-based discipline that actively applies supportive science to the creative, emotional, and energizing experiences of music for health treatment and educational goals. Below are a few important facts about music therapy and the credentialed music therapists who practice it:\n- Music therapists must have a bachelor’s degree or higher in music therapy from one of AMTA’s 72 approved colleges and universities, including 1200 hours of clinical training.\n- Music therapists must hold the MT-BC credential, issued through the Certification Board for Music Therapists, which protects the public by ensuring competent practice and requiring continuing education. Some states also require licensure for board-certified music therapists.\n- Music Therapy is an evidence-based health profession with a strong research foundation.\n- Music Therapy degrees require knowledge in psychology, medicine, and music.\nThese examples of therapeutic music are noteworthy, but are not clinical music therapy:\n- A person with Alzheimer’s listening to an iPod with headphones of his/her favorite songs\n- Groups such as Bedside Musicians, Musicians on Call, Music Practitioners, Sound Healers, and Music Thanatologists\n- Celebrities performing at hospitals and/or schools\n- A piano player in the lobby of a hospital\n- Nurses playing background music for patients\n- Artists in residence\n- Arts educators\n- A high school student playing guitar in a nursing home\n- A choir singing on the pediatric floor of a hospital\nFinally, here are examples what credentialed music therapists do:\n- Work with Congresswoman Giffords to regain her speech after surviving a bullet wound to her brain.\n- Work with older adults to lessen the effects of dementia.\n- Work with children and adults to reduce asthma episodes.\n- Work with hospitalized patients to reduce pain.\n- Work with children who have autism to improve communication capabilities.\n- Work with premature infants to improve sleep patterns and increase weight gain.\n- Work with people who have Parkinson’s disease to improve motor function.\nAMTA’s mission is to advance public awareness of the benefits of music therapy and increase access to quality music therapy services in a rapidly changing world. In consideration of the diversity of music used in healthcare, special education, and other settings, AMTA unequivocally recommends the unique knowledge and skill of board certified music therapists.\nBradt, J., Magee, W.L., Dileo, C., Wheeler, B.L., & McGilloway, E. (2010). Music therapy for acquired brain injury. Cochrane Database of Systematic Reviews, 2010(7), doi: 10.1002/14651858.CD006787.pub2.\nClair, A. A., Lyons, K., & Hamburg, J. (2012). A feasibility study of the effects of music and movement on physical function, quality of life, depression, and anxiety in patients with Parkinson disease. Music and Medicine, 4 (1), 49-55.\nQuotes about Music Therapy\n- Dr. Sanjay Gupta:\n""On this day, I was playing the patient. An intensive, exhaustive seven-hour schedule was presented, full of physical therapy, speech, recreational, occupational and my personal favorite – music therapy."" - CNN, February 2011\n""Music therapy helps speech, but also motor skills, memory and balance. Also emotionally uplifting."" - Twitter, May 2011\n- Jodi Picoult (Author of the bestselling book Sing You Home):\n""Music therapy, to me, is music performance without the ego. It’s not about entertainment as much as its about empathizing. If you can use music to slip past the pain and gather insight into the workings of someone else’s mind, you can begin to fix a problem. ""\n- Michael Greene, President & CEO of NARAS - 1997 Grammy Awards:\n""When we look at the body of evidence that the arts contribute to our society, it\'s absolutely astounding. Music Therapists are breaking down the walls of silence and affliction of autism, Alzheimer\'s and Parkinson\'s disease.""\n- Sen. Harry Reid (D-Nev.):\n- Sen. Harry Reid:\n""Music helps all types of people to remain forever young."" He noted that Congress had never before ""directly addressed the question of music"" as preventive medicine and as ""a therapeutic tool for those suffering from Alzheimer\'s disease and related dementias, strokes and depression.""\n- SUPERIOR, WI Telegram, Aug. 14, 1991.\n- Mickey Hart (Grateful Dead):\n""(Rhythm) is there in the cycles of the seasons, in the migrations of the birds and animals, in the fruiting and withering of plants, and in the birth, maturation and death of ourselves,"" Hart told a Senate panel studying music therapy.\n- REUTERS, Aug. 1, 1991.\n- Ida Goldman (90-year-old testifying at Senate hearings):\n""Before I had surgery, they told me I could never walk again. But when I sat and listened to music, I forgot all about the pain,"" said Goldman, who walked with assistance during the hearing.\n- REUTERS, Aug. 1, 1991.\n- Sen. Harry Reid:\n- Dr. Oliver Sacks (""Awakenings""):\nDr. Sacks reports that patients with neurological disorders who cannot talk or move are often able to sing, and sometimes even dance, to music. Its advocates say music therapy also can help ease the trauma of grieving, lessen depression and provide an outlet for people who are otherwise withdrawn.\n- ST. Louis Post Dispatch.\n- Dr. Clive Robbins (Nordoff-Robbins Music Therapy Clinic):\n""Almost all children respond to music. Music is an open-sesame, and if you can use it carefully and appropriately, you can reach into that child\'s potential for development."" Nordoff-Robbins uses music therapy to help 100 handicapped children learn and to relate and communicate with others.\n- Barbara Crowe (past president of the National Association for Music Therapy):\n- Oliver Sacks, M.D.:\n- Mathew Lee (Acting Director, Rusk Institute, New York):']"	['<urn:uuid:6cb6c5c5-2ce5-49a2-98c6-338af48cbd51>', '<urn:uuid:5965de03-6b3a-4315-acc6-33d3682be80b>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T06:04:36.674696	25	81	1844
92	ive discovered great guitar tuning want write down but no piano what are fret numbers method for remembering tuning	You can use a method based on fret numbers to write down guitar tunings. Just find which fret on the bottom string gives you the note for the open 5th string, then which fret on the 5th string gives you the note for the open 4th string and so on. This way you can write down the tuning by describing the relationship between strings in terms of frets.	"['by Howard Wright\nInstead of writing out guitar tunings using the note names for each string, Joni herself uses a system of one letter and five numbers to describe each tuning. The letter is the note name of the bottom (lowest pitch) string, and the numbers represent the fret numbers at which you play one string to be able to tune the next open string.\nThis notation may look a little strange at first but is in fact very easy to use.\nIf you have the tunings spelled out with note names, e.g CGDEGC, you need a piano or standard pitch reference (or perfect pitch!) to give you the correct note for each string. Joni\'s tuning notation makes it simpler to tune up - the numbers tell you which frets to use to tune one string to another, so once you have the bottom string at the correct pitch, it is straight forward to tune the other strings.\nTake the example of standard guitar tuning: EADGBE\nMost people learn to tune a guitar by first tuning the bottom string to E (using a standard pitch reference such as a guitar tuner, tuning fork etc), then fretting this string at the 5th fret to tune the open A, then fretting the A string at the 5th fret to tune the open D and so on. The tuning notation used by Joni is a simple way of writing down this tuning process by noting the pitch for the lowest string, and the fret numbers needed to tune successive open strings.\nWriting normal guitar tuning in ""Joni tuning notation"" gives : E 55545\nNote that you only need one note name and five numbers to specify each tuning.\nIf you tune up using this method, you will usually need to make small adjustments to get the tuning just right. Try strumming the open strings, or some simple chords to see if any strings need some fine tuning.\nIt is surprisingly easy to get \'lost\' when you start playing around with different tunings. Imagine you discover a fabulous tuning that you want to remember, but you don\'t have a standard pitch reference (e.g piano) available - you won\'t know the names of the notes of the open strings, so how do you write the tuning down?\nUsing Joni\'s tuning notation, all you need to do is find which fret on the bottom string gives you the note for the open 5th string, then which fret on the 5th string gives you the note for the open 4th string and so on. You\'ll be able to write down the tuning this way by describing the relationship between strings in terms of frets.\nAs well as making it easier to tune up, Joni\'s system of tuning notation makes comparisons between different tunings much easier, and it can also help you to spot ""families"" of tunings. For example, the connection between the following two tunings isn\'t obvious at first glance.\nTuning 1 = C# F# B E G# C#\nTuning 2 = E A D G B E (standard tuning).\nWriting the two in the \'Joni\' system, the connection becomes immediately clear:\nTuning 1 = C# 55545\nTuning 2 = E 55545\nIn other words, the relative tuning of the strings is the same, it\'s just the reference pitch for the lowest string which differs.\nSimilarly, when tunings such as D A E F# A D and B F# C# D# F# B are written in Joni\'s tuning notation, their similarity is revealed: D 77235 and B 77235.\nIf the relative tuning of strings is the same, as in the two examples above, the same chord shapes that are used in one tuning can be used in the second tuning - the only difference is that the chords will be in a different key. This is useful, as it allows you to \'transfer your knowledge\' from one tuning to another. Marian Russell has written a lot more about this and the concept of tuning patterns on this page.\nJoni herself sometimes refers to tuning families:\n(From My Secret Place, an article by Jeffrey Pepper Rogers from Acoustic Guitar, August 1996)\nMitchell has come up with a way to categorize her tunings into families, based on the number of half steps between the notes of adjacent strings. ""Standard tuning\'s numerical system is 5 5 5 4 5, with the knowledge that your bass string is E, right?"" she said. ""Most of my tunings at this point are 7 5 or 7 7, where the 5 5 on the bottom is. The 7 7 and the 7 5 family tunings are where I started from.""\nMitchell continued, ""However, the dreaded 7 9 family - I have about seven songs in 7 9 tunings - are in total conflict with the 7 5 and the 7 7 families. They\'re just outlaws""\nJoni\'s tuning notation also makes it easier to distinguish strings that are tuned in unison from those tuned in octaves. For example, if the tuning for Joni\'s song This Flight Tonight is written as Ab Ab Eb Ab C Eb, it\'s not clear whether the bottom two strings are tuned in unison or in octaves. Specifying the tuning in \'Joni notation\' makes it clear. The tuning is Ab 12 7 5 4 3, so the bottom two strings are in fact tuned an octave (12 frets) apart.\nThe numbers used in this tuning notation make it easy to see what the pitch intervals between adjacent strings are. In the example above (This Flight Tonight) the first number is 12, which means the bottom two strings are 12 frets or one octave apart. The table below shows the musical interval that corresponds to different numbers of frets.\n|Number of frets (semitones)||Interval|\n|1||Semitone, half step or flat 2nd|\n|2||Tone, whole step or 2nd|\n|6||Augmented (sharp) 4th or diminished (flat) 5th|\n|8||Minor 6th or augmented (sharp) 5th|\n|10||Minor 7th or flat 7th|\nYou can use this table to see what the musical intervals are between strings for different Joni tunings. For example, looking up the intervals that correspond to the fret numbers for the tuning C 77235, we find the tuning has the following intervals between strings: perfect 5th, perfect 5th, 2nd, minor 3rd, perfect 4th.\nThe most common intervals for Joni\'s tunings are perfect 5ths, perfect 4ths, major 3rds, minor 3rds and 2nds.']"	['<urn:uuid:196b35fe-5816-48a9-b1ae-7920efdc76f1>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	19	68	1062
93	conservation challenges tulum wildlife habitat destruction climate change tourism impact	Tulum faces significant conservation challenges including rapid urbanization, climate change, and illegal wildlife trade. Climate change particularly impacts turtle nesting sites by altering sand temperatures, which affects hatchling sex ratios. Tourism-related threats include light pollution disrupting nesting behavior, vehicle traffic compacting beach sand, and coastal development destroying natural habitats. To address these challenges, conservation initiatives include reforestation projects, sustainable tourism practices, and educational programs for visitors about respecting local ecosystems and wildlife.	"['Tulum, a picturesque coastal town nestled along Mexico’s Riviera Maya, has long been celebrated for its azure waters, ancient ruins, and bohemian charm. But beyond its postcard-perfect beaches and historic Mayan relics lies a natural paradise brimming with captivating wildlife. From the lush jungles to the crystal-clear cenotes, Tulum offers a sanctuary for an astonishing array of animals that often remain concealed from the casual tourist’s eye.\nIn this article, we embark on a journey into the heart of Animals in Tulum rich biodiversity, uncovering the untold stories of its resident creatures. From the elusive jaguars and vibrant toucans that call the Sian Ka’an Biosphere Reserve home to the mesmerizing marine life that thrives in its pristine Caribbean waters, we’ll introduce you to the fascinating creatures that coexist with Tulum’s vibrant culture and thriving tourism industry.\nPrepare to be enchanted by the ethereal world of Animals in Tulum, where the wonders of nature flourish alongside the allure of this coastal gem.\nBrief Overview of Tulum Natural Beauty and Diverse Ecosystems:\nNestled on the Yucatan Peninsula, Tulum boasts an enchanting blend of natural beauty and diverse ecosystems that make it a truly unique destination. This coastal town is renowned for its postcard-worthy beaches that kiss the crystal-clear waters of the Caribbean Sea.\nBeyond the shoreline, Tulum’s landscapes transform into lush jungles and ancient Mayan ruins, providing a haven for an astonishing variety of wildlife. This article delves deep into the various facets of Tulum’s ecosystems, from its coastal paradises to the thriving jungles, exploring how its biodiversity is intertwined with its rich history and culture. Join us on a journey through Tulum’s hidden treasures, where the beauty of nature merges seamlessly with the allure of a coastal paradise.\nTulum’s Diverse Ecosystems:\nA. Coastal Paradise:\nTulum’s coastline is a true testament to nature’s artistry. Its pristine, white-sand beaches and turquoise waters create a paradise for both visitors and the creatures that inhabit the underwater world. Coral reefs, home to an array of marine life, hug the coast, making it a hotspot for snorkelers and divers. From vibrant corals to playful dolphins, the coastal waters of Tulum are a testament to the town’s commitment to preserving its natural splendor.\nB. Lush Jungles: The Rich Biodiversity of Tulum’s Forests:\nTulum’s jungles are a lush tapestry of life, teeming with biodiversity. These forests are inhabited by a captivating mix of wildlife, including spider monkeys, jaguars, and colorful toucans.\nAs you venture deeper into the heart of Tulum’s jungles, you’ll discover cenotes, underground rivers, and hidden caves that are not only breathtaking but also vital to the region’s delicate ecosystem. The symbiotic relationship between the flora and fauna of these jungles is a testament to the resilience of nature and the need for conservation efforts in this rapidly developing region.\nThe Ancient Guardians: Tulum’s Mayan Connection:\nTulum’s ecological significance is intricately tied to its rich Mayan heritage. The ancient ruins that dot the landscape serve as guardians of the environment, demonstrating how human history can coexist harmoniously with nature. These archaeological wonders, perched on cliffs overlooking the sea, offer a glimpse into the past and a stark reminder of the importance of preserving the natural world. The Mayan culture’s reverence for the environment continues to inspire conservation efforts in the region, fostering a sense of responsibility among both locals and visitors.\nTulum’s pristine coastline serves as a crucial nesting ground for endangered sea turtles, including loggerheads, green turtles, and hawksbills. These ancient mariners return year after year to lay their eggs in the soft sands, connecting the town’s history with its future. Conservation efforts in Tulum are dedicated to protecting these majestic creatures and their hatchlings. Visitors can witness the awe-inspiring sight of baby turtles making their first journey to the sea, underscoring the importance of sustainable tourism in preserving this vital part of Tulum’s wildlife heritage.\nBeneath the Caribbean’s azure waters, rays glide gracefully, captivating divers and snorkelers with their serene elegance. In Tulum, encounters with spotted eagle rays and manta rays are not uncommon. These gentle giants are essential to the region’s marine ecosystem, balancing the delicate harmony of life beneath the waves. Understanding the ecological significance of rays is an integral part of responsible tourism in Tulum, as it highlights the interconnectedness of all marine life and emphasizes the need for preservation.\nTulum’s vibrant coral reefs are alive with a dazzling array of colorful fish. Parrotfish, angelfish, and butterflyfish are just a few of the species that call these reefs home. Snorkelers and divers are treated to a vivid spectacle of colors and shapes, making it a must-see for nature enthusiasts. Tulum’s commitment to marine conservation ensures that these underwater wonders continue to thrive. Sustainable tourism practices and protected marine areas enable visitors to witness the mesmerizing diversity of colorful fish while contributing to the long-term health of Tulum’s underwater ecosystems.\nTulum’s lush jungles are enlivened by the echoing calls of howler monkeys, the spirited acrobats of the treetops. These charismatic creatures are a symbol of the region’s thriving biodiversity. As one explores the jungle trails, the opportunity to encounter these playful primates amidst the dense foliage is a testament to the coexistence of nature and civilization. Tulum’s conservation initiatives recognize the importance of preserving the habitats that sustain howler monkeys and other jungle inhabitants, ensuring that future generations can continue to enjoy these awe-inspiring encounters.\nDeep within Tulum’s wilderness, the enigmatic jaguar, the apex predator of the jungles, prowls with an air of mystery. Their elusive nature and intrinsic connection to Mayan culture make them symbolic guardians of Tulum’s natural heritage. Conservation efforts focus on protecting these big cats and their habitats, as they play a vital role in maintaining the ecological balance of the region. Responsible ecotourism allows lucky visitors to glimpse the jaguar’s tracks, fostering a deeper appreciation for the untamed beauty of Tulum’s jungles.\nThe humble tapir, a gentle giant of Tulum’s forests, often goes unnoticed but plays a significant role in the ecosystem. As seed dispersers, they contribute to the regeneration of the jungle, ensuring its long-term health. Tulum’s commitment to preserving its lush landscapes includes safeguarding tapir habitats. With patience and respect for their natural behaviors, eco-conscious travelers may have the privilege of observing these elusive herbivores, solidifying the vital connection between Tulum’s jungles and its remarkable biodiversity.\nTropical splendor is incomplete without the vibrant presence of toucans, and Tulum’s jungles are home to these emblematic birds. Their striking plumage and distinct calls add a touch of enchantment to the region’s forests. Tulum’s conservation initiatives prioritize protecting the habitats of these charismatic avian inhabitants, ensuring that their colorful presence continues to grace the canopies. Birdwatchers and nature lovers alike can revel in the opportunity to spot toucans in their natural habitat, a testament to Tulum’s commitment to preserving its avian treasures.\nTulum’s coastal lagoons and wetlands are adorned with the graceful presence of flamingos, casting a rosy hue over the serene waters. These striking birds are not only a visual spectacle but also essential to the local ecosystem. Tulum’s conservationists work diligently to protect the fragile habitats that flamingos depend on. Observing flocks of these elegant birds in their natural environment is a reminder of the delicate balance that sustains the region’s diverse wildlife.\nThe jungles and mangroves of Tulum resound with the raucous chatter of parrots, their colorful plumage adding a vivid palette to the natural tapestry. Conservation efforts in Tulum extend to these vibrant birds, as their survival depends on the preservation of their habitats. Birdwatchers are treated to the spectacle of parrot flocks in flight, a testament to Tulum’s commitment to nurturing its avian wonders and the rich biodiversity that thrives within its borders.\nConservation Efforts and Challenges:\nPreserving Tulum’s natural beauty and diverse ecosystems is not without its challenges. Rapid urbanization, climate change, and illegal wildlife trade pose significant threats to the delicate balance that sustains this paradise. Nonetheless, passionate conservationists and local authorities are taking proactive steps to safeguard Tulum’s environment. Initiatives include reforestation projects, sustainable tourism practices, and marine conservation efforts, all aimed at preserving the unique ecosystems that make Tulum a global gem.\nResponsible Tourism and Wildlife Encounters:\nResponsible tourism plays a pivotal role in Tulum’s conservation efforts. Visitors are increasingly educated about the importance of respecting local ecosystems and wildlife. Eco-friendly accommodations, wildlife sanctuaries, and guided tours provide opportunities for travelers to appreciate Tulum’s natural beauty without causing harm. The goal is to foster an ethos of responsible travel, where encounters with animals are both ethical and educational, ensuring that Tulum’s wildlife thrives for generations to come.\nFuture Outlook for Tulum’s Wildlife:\nThe future of Tulum’s wildlife hinges on the collective efforts of conservationists, locals, and responsible tourists. While challenges persist, there is hope on the horizon. With continued dedication to preservation and sustainable development, Tulum’s ecosystems can flourish. The region’s unique blend of natural beauty, cultural heritage, and biodiversity can serve as a model for responsible coexistence between humans and nature in an increasingly interconnected world.\nIn Tulum, nature’s grandeur and cultural history converge, creating a paradise worth cherishing. The coastal paradises, lush jungles, and Mayan guardians all contribute to the town’s allure. Tulum’s journey toward sustainable tourism and conservation is a testament to the power of community and collective responsibility. As we conclude our exploration of Tulum’s ecosystems and wildlife, we find that this enchanting destination serves as a reminder of the delicate balance between human progress and environmental preservation, offering hope for a future where both can thrive harmoniously.\nRahul M Suresh\nVisiting the Zoo can be an exciting and educational experience for all involved. As a guide, I have the privilege of helping students and visitors alike to appreciate these animals in their natural habitat as well as introducing them to the various aspects of zoo life. I provide detailed information about the individual animals and their habitats, giving visitors an opportunity to understand each one more fully and appreciate them in a more intimate way.', ""National Oceanic and Atmospheric Administration (Ocean Today)\nSeven different species of sea (or marine) turtles grace our ocean waters, from the shallow seagrass beds of the Indian Ocean, to the colorful reefs of the Coral Triangle and the sandy beaches of the Eastern Pacific. While these highly migratory species periodically come ashore to either bask or nest, sea turtles spend the bulk of their lives in the ocean.\nOver the last 200 years, human activities have tipped the scales against the survival of these ancient mariners. Slaughtered for their eggs, meat, skin, and shells, sea turtles suffer from poaching and over-exploitation. They also face habitat destruction and accidental capture—known as bycatch—in fishing gear. Climate change has an impact on turtle nesting sites; it alters sand temperatures, which then affects the sex of hatchlings. Nearly all species of sea turtle are now classified as endangered, with three of the seven existing species being critically endangered.\nKemp's Ridley Turtle\nOlive Ridley Turtle\nThreats to Sea Turtles\nSea turtles are a fundamental link in marine ecosystems. They help maintain the health of seagrass beds and coral reefs that benefit commercially valuable species such as shrimp, lobster, and tuna. Sea turtles are the live representatives of a group of reptiles that have existed on Earth and traveled our seas for the last 100 million years. Turtles have major cultural significance and tourism value.\nFive of the seven species are found around the world, mainly in tropical and subtropical waters. The remaining two species, though, have relatively restricted ranges: Kemp's Ridley is found mainly in the Gulf of Mexico and the Flatback turtle around northern Australia and southern Papua New Guinea.\nLoggerhead and hawksbill turtles are particularly vulnerable. Nearly all species of sea turtle are classified as Endangered. They are killed for their eggs, meat, skin and shells. They also face habitat destruction. Climate change has an impact on turtle nesting sites. As fishing activity expands, this threat is more of a problem.\nSea turtles journey between land and sea and swim thousands of ocean miles during their long lifetimes. They wait decades until they can reproduce, returning to the same beaches where they were born to lay their eggs. Females can lay hundreds of eggs in one nesting season, yet few will yield hatchlings that survive their first year of life. Beyond these significant natural challenges, sea turtles face multiple threats caused by humans as noted below.\nFisheries: Sea turtles virtually everywhere are affected by bycatch from fisheries. Worldwide, hundreds of thousands of sea turtles a year are accidentally caught in shrimp trawl nets, on longline hooks and in fishing gill-nets. Sea turtles need to reach the surface to breathe, and therefore many drown once caught.\nDirect Take: Sea turtles and their eggs are killed by people throughout the world for food, and for products including oil, leather and shell.\nCoastal Development: Sea turtle habitats are degraded and destroyed by coastal development. This includes both shoreline and seafloor alterations, such as nesting beach degradation, seafloor dredging, vessel traffic, construction, and alteration of vegetation.\nClimate change: Climate change will increase the frequency of extreme weather events, result in loss of nesting beaches, and cause other alterations to critical sea turtle habitats and basic oceanographic processes. It may impact natural sex ratios of hatchlings and increase the likelihood of disease outbreaks for sea turtles.\nMarine pollution is anything from a discarded plastic to a lost fishing net. Every ocean in the world is littered with some form of debris which resembles food for marine life. Many animals accidentally eat marine debris causing internal injury, intestinal blockage, and starvation. Once ingested, the plastic piles up in the turtles's stomach and can then obstruct bowels, preventing turtles from digesting food and leading them to starve to death. Light pollution disrupts nesting behavior and causes hatchling death by leading them away from the sea. Chemical pollutants can weaken sea turtles’ immune systems, making them susceptible to disease.\nHabitat loss uncontrolled coastal development, vehicle traffic on beaches, and other human activities have directly destroyed or disturbed sea turtle nesting beaches around the world. Sea turtles are dependent on beaches for nesting. Lights from roads and buildings disorient hatchlings away from the sea, and vehicle traffic on beaches compacts the sand, making it impossible for female turtles to dig nests. Turtle feeding grounds such as coral reefs and seagrass beds are damaged and destroyed by activities onshore, including sedimentation from clearing of land and nutrient run-off from agriculture. Beach restoration projects for protecting seaside buildings have also been found to be harmful, through dredging and sand filling.\nHow You Can Help Protect Sea Turtles\nMinimize beachfront lighting during the sea turtle nesting season by turning off, shielding, or redirecting lights.\nClose blinds and draperies in oceanfront rooms at night during the nesting season to keep indoor lighting from reaching the beach.\nDo not construct campfires on the beach. Sea turtle hatchlings are known to be attracted to the light emitted by campfires and crawl into fires and die.\nUse your natural vision when walking on the beach at night. The use of flashlights and flash photography can deter turtles from coming ashore to nest or cause them to abort nesting attempts.\nIf you encounter a turtle on the beach at night, remain quiet, still, and at a distance, otherwise she may become frightened and return to the ocean without nesting.\nLeave the tracks left by turtles undisturbed. Researchers use the tracks to identify the species of turtle that nested and to find and mark the nests for protection.\nProperly dispose of your garbage. Turtles may mistake plastic bags, styrofoam, and trash floating in the water as food and die when this trash blocks their intestines.\nCelebrate events without the use of helium balloon releases. Like plastic trash, balloons end up in the ocean, especially when released near the coast. Sea turtles mistakenly eat the balloons and die.\nRemove recreational equipment, such as lounge chairs, cabanas, umbrellas, and boats, from the beach at night. Their presence can deter nesting attempts and interfere with the seaward journey of hatchlings.\nProtect beach vegetation that stabilizes sand and the natural coastline.\nWhen boating, stay alert and avoid sea turtles. Propeller and collision impacts from boats and ships can result in injury and death of sea turtles. Also, stay in channels and avoid running in seagrass beds to protect this important habitat from prop scarring and damage. Avoid anchoring boats in seagrass beds and coral reefs, which serve as important foraging and resting habitats for sea turtles.""]"	['<urn:uuid:61148393-3b8f-4ba1-ada9-877d301277c4>', '<urn:uuid:46011562-1517-4cbb-a934-3c1378e91e94>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T06:04:36.674696	10	72	2752
94	who brought buddhism bhutan when	Guru Rimpoche brought Buddhism to Bhutan in 650 A.D. He is believed to be the second incarnation of Buddha.	"[""IN THE LAND OF THE THUNDER DRAGON\nA Journey to Bhutan\nby Bruce Meade\nThe small passenger jet dropped out of the clouds, darted through a heart stoppingly narrow pass, then settled onto the single landing strip of the Paro Airport. I stepped out into the sweet smelling spring air. I was in Bhutan, the last Buddhist Kingdom.\nThe main reason for my journey was to experience the culture of this isolated Himalayan realm, but I also had a paper trail to follow.For by coincidence no sooner had I made my travel arrangements than we began to carry handmade paper made in Bhutan here at Hiromi Paper International.\nBhutan is a small Kingdom about the size of Switzerland, with Tibet as its neighbor to the north and India to the south. The current king is a very enlightened ruler, more concerned with his countries Gross National Happiness than its Gross National Product. By his decree the protection of Bhutan's pristine natural environment is the stated policy of the government.\nTo enter Bhutan, whether in a group or by yourself you need to be with a Bhutanese guide and a driver. This, as well as your visa, must be arranged through a licensed Bhutanese tour agency and you must pay $200.00 per day while in the country, which includes food, lodging, and vehicle.\nFor me the opportunity to visit an isolated Buddhist country high in the Himalayas far outweighed any of the aforementioned limitations. As far as the $200.00 per day I figured, hey, that's why credit cards were invented.\nAnd that how I came to be standing in a field of blooming Daphne at a 10,000 foot high pass in Bhutan! To touch this shrub whose bark would eventually become a sheet of paper selling in the store where I worked halfway across the world was an incredible feeling of being part of a full circle.\nI was en route to the Paro festival, an important rite of spring in Bhutan. The festival is part county fair, part religious ceremony. Vendors from all over the Himalayas set up to sell their wares. Colorful masks, beautiful jewelry, antique prayer wheels, and exotic foods were all on display.\nOn its final day the Festival is capped off by the unfurling at dawn of a four story high banner depicting Guru Rimpoche, who brought Buddism to Bhutan in 650 A.D.\nSo blessed is the huge banner (called a Thondrol) that it is believed one achieves liberation simply by viewing it. So there I stood in the cool light of dawn, staring up at the massive image of Rimpoche seated upon his lotus throne. Bhutanese people had come from all over, many walking for days to see the Thondrol. It remains up for only a few hours, on this one day a year, as the monks do not let the 300 year old work of devotional art be touched by direct rays of the sun, least it fade.\nI then made my way to the booth of Mangala Paper, who make the paper we import, and introduced myself to Kezang Udon. Her Eco Friendly hand made paper was doing a brisk business, with many western tourists checking out the beautiful sheets, as well as handsome photo albums and journals crafted from Bhutan paper. I told Kezang how proud I was to be part of the introduction of Bhutan paper to the United States and she graced me with the warm smile that is so typical of the Bhutanese people.\nI promised to stop by her factory on my way back from my journey to the remote central valley, and began an arduous day and a half drive on the only paved road in the country.\nI traveled over passes towering above the clouds, through dense forests punctuated by flaming red rhodendrums. I visited the cave where Guru Rimpoche first meditated when he arrived in Bhutan from Tibet. A temple has been built around the cave and a huge cypress tree stands beside it, supposedly having sprung up from Rimpoche's walking stick. I touched the tree with deep respect for the man who is believed to be the second incarnation of Buddha.\nOn the return journey it began to snow. We were heading for a monastery at the top of a pass, and arrived to the sound of deep guttural chanting of monks, accompanied by the plaintive cry of conch shell horns and thudding bass of drums. A healing ceremony for the Lama (Teacher) was in progress and we were allowed to observe.\nThe interior of the monastery was a treasure trove of wall paintings depicting various spiritual heroes battling demons. Bhutan is a Tantric Buddhist nation, thus visualizations of inner states of mind are constantly on view, serving as signposts on the path towards enlightenment.\nLeaving the monastery I noticed the snow had stopped, and the sky was now a dazzling blue. I stood at the top of the pass gazing out at the Himalayan range in all its monumental glory. Massive snow crowned peaks stood as silent sentinels between the border of Bhutan and Tibet. Bathed in the clear light the mountains seemed to be the liar of Bhutans legendary Thunder Dragons. It was a sight that will linger forever in my memory.\nContinuing westward I entered Thimphu, the only world capitol without a traffic light. I made my way to the paper making facility, which sits above the river at the southern end of the city. Colorful prayer flags fluttered by the entrance, sending hopes for peace out to the world on the breeze.\nOver tea Kezang shared with me the story of how she came to be involved in the making of handmade paper. It was not a family business by any means. She learned about the long tradition of paper making in Bhutan, and was most impressed with the rich character of the paper. Realizing the art was slowly dying she decided to take it up despite the objections raised by her family.\nSince she didn't know very much about papermaking she went to Japan to study their techniques. After returning to Bhutan she set up the Mangala House of Handmade Paper, knowing full well she was bucking all the modern trends. The labor intensive, time consuming handmade methods were being fazed out in search of higher profit margins, but she did not want to see the handmade paper of Bhutan disappear. With lots of imagination and hard work Kezang Udon is helping keep an ancient tradition alive. As I told her, we at Hiromi Paper are happy to be part of this effort, introducing these beautiful sheets to artists all over the world.\nAll too soon it was time for me to climb onto a Druk Airlines jet to fly up and over the Himalayas towards home. I knew how fortunate I was to have been able to visit this magical realm of beauty and insight. And someday I hope to return. Until then I can content myself showing our customers gorgeous sheets of paper that come from the Land of the Thunder Dragon.""]"	['<urn:uuid:d0478ab1-06df-4a29-ab44-293c94d109fc>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	5	19	1185
95	minimum requirements obtain project management certification	For PMP certification, there are two paths based on education level. With a four-year degree, you need 3 years (36 months) of project management experience and 4,500 hours leading and directing projects. With a secondary degree, you need 5 years (60 months) of experience and 7,500 hours leading projects. Both paths require 35 hours of project management education. The experience must be within the last 8 years and include non-overlapping projects across all process groups.	['Project Management Professional (PMP)\nWhat is PMP?\nProject management Professional is a globally acknowledged professional certification that validates a professional’s education and experience in Project Management Professional (PMP®) certification is a qualification program overseen by the Project Management Institute (PMI).\nProject Management Professional (PMP) is an internationally recognized professional designation offered by the Project Management Institute (PMI). As of March 2018, there are 833,025 active PMP certified individuals and 286 chartered chapters across 210 countries and territories worldwide.\nAdvantages of PMP\n- PMI Salary Survey Reveals That Project Management Professionals with the PMP Certification Earn 23% higher on average across the 37 countries.\n- Better Job Opportunities: PMP Certification opens up better career avenues and provides professionals with greater job opportunities in the project management world. According to a Price waterhouse Coopers survey, 80% of high-performing projects use PMP credentialed project managers; according to the PMI Pulse of the Profession study, organizations with more than 35% PMP certified project managers demonstrated much better project performance than those without a certification.\n- Applicable to Most Industries: The PMP certification is an ideal bet for all project managers in various professional fields, including IT, telecom, business processing, commerce, finance, research, and more.\n- Enhance Your Skills: It’s not easy to get the PMP certification and you need to undergo rigorous training for the same. There is a also a significant amount of coursework involved. You get trained and educated in in five project management processes—planning, initiating, implementing, monitoring and controlling, and finally closing. In short, you learn A-Z of project management which you can implement in your company projects for better project execution.\n- Earn More: PMP certified project managers earn more than the non-certified ones. As soon as you get your PMP degree, you can command a higher pay and you can expect an immediate hike. Many surveys have shown that PMP certified project managers earn at least 20 percent more than the non-certified counterparts. Also PMP certified professionals have the capability to earn a six figure income.\n- Greater Visibility To Recruiters: Research across industries suggests that organizations prefer hiring PMP certified project managers rather their non-credentialed peers. The PMP certification is a standard that demonstrates a professional’s expertise in project management and it immediately catches a recruiter’s eye during profile evaluation.\n- Expand Your Marketability: A PMP certification can help reach to global organizations working in different parts of the world. It enhances your professional marketability to a great extent and legitimize your experience as a project manager. You’d also be in touch with individuals aspiring to take up the course and PMP certified professionals as already mentioned, through project management forums and discussion boards, helping you to master your expertise with their tips and knowledge.\n|Eligibility Requirement||Four-Year Degree||Secondary Degree*|\n|Years of Project Management Experience||3 Years (36 Months)||5 Years (60 Months)|\n|Hours Leading & Directing Projects||4,500 Hours||7,500|\n|Hours of Project Management Education||35 Hours||35 Hour|\n- Indicate whether you have secondary diploma or a degree. You will need the name of the institution, the year you graduated, and your major.\nProject Management Experience\n- The experience must be leading or directing projects that are non-overlapping, cumulative across all process groups, and within the last 8 years to be counted on the PMI PMP application. Refer to the above table for full experience requirements.\nProject Management Education\n- You will need to show 35 contact hours of project management education, preferably covering initiating, planning, executing, monitoring and controlling, and closing. PMTI plans ahead for our students and provides an inclusive online course so you qualify for this requirement even prior to attend the class.\nWho should apply?\nAn experienced project manager who meets the following requirements:\n- Perform their duties under general supervision and are responsible for all aspects of the project for the life of the project.\n- Lead and direct cross-functional teams to deliver projects within the constraints of schedule, budget and resources.\n- Demonstrate sufficient knowledge and experience to appropriately apply a methodology to projects that have reasonably well-defined project requirements and deliverables.\nIf you are considering working toward your PMP exam, Process exam offers a variety of project management practice exam that will help you pass, including the PMP Certification practice exam. This Practice exam are conducted by certified, highly experienced professionals with at least ten years of experience.\nRead the following article:Tips and Tricks to pass PMP Certification in first attempt']	['<urn:uuid:093b25ae-2272-46c9-a759-a39636b35c2d>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T06:04:36.674696	6	75	735
96	when was agia sophia church built date	The exact construction date of the Agia Sophia is debated. While some scholars believe it could predate the Istanbul Hagia Sophia from the 6th century based on its masonry and pendentive design, most scholars think it was built in the early 8th century, possibly during Leo III the Isaurian's reign (717-40). The current building was constructed over the remains of both a Roman building and an Early Christian basilica that previously existed on the site.	"['History of Agia Sophia\nThe date of the Agia Sophia is not entirely clear. Based on its masonry and the imperfect design of its pendentives, some scholars think it could predate its more famous namesake in Istanbul, which was built in the 6th century. But most believe it dates from the early 8th century, perhaps during the reign of Leo III the Isaurian (717-40), and represents a transitional style between the domed basilica and the domed cruciform church.\nWhichever date is correct, the present building is not the first to stand on the site: excavations have revealed remains of both a Roman building and an Early Christian basilica beneath the church.\nAfter seven or eight centuries as an Orthodox church, the Agia Sophia was converted into a mosque by the Turks in 1585. After a fire in 1890, it was reconstructed in 1907-10 and rededicated for Christian worship in 1912. The building lost its elegant Turkish portico to an Italian air raid in 1941and was badly damaged by the earthquake of 1978.\nWhat to See at Agia Sophia\nDue to its 20th-century renovations, the Agia Sophia lacks some of the ambiance of Thessaloniki\'s less restored churches. But the historical and artistic importance of this ancient sacred site still make it well worth a visit.\nThe exterior is not especially attractive, but it occupies a fine setting in a garden with palms and pine trees. The west facade is plain, flat, and square, but the east side looks more like a typical Byzantine church. The interior is exceptionally spacious, covered with a dome 10 meters in diameter. Unusually, it rests on a square drum with rounded corners rather than a circle.\nThankfully, some of Agia Sophia\'s original mosaics have survived its turbulent history. Those on the dome date from the 9th or 10th centuries and depict the Ascension, with Christ seated on a rainbow throne occupying the central medallion. Below is the Virgin Mary flanked by angels and the Apostles divided by trees. A Greek inscription quotes the angels\' remark in Acts 1:11: ""Men of Galilee, why do you stand here looking into the sky?""\nThe apse mosaics include a Virgin Enthroned and monograms of Constantine VI and the Empress Irene. Some mosaics survive from the Iconoclastic period, when figural representations were forbidden in religious art, and are dated to 785-97. These mosaics depict crosses, stars, and liturgical texts.\nThe arches on the west wall of the narthex bear fragments of 11th-century frescoes, consisting mainly of full-length and bust portraits of saints.\nOutside the church to the northwest is Agia Sophia Square, one of the most important squares in Thessaloniki. When the city was liberated from the Germans on November 2, 1944, this is where the service of thanksgiving was held. Across the way at 33 Agias Sophias is the Terkenlis and Byzantium pastry shop, a good place for a snack break after visiting the church.\nQuick Facts on Agia Sophia\n|Categories:||churches; change of religion|\n|Visitor and Contact Information|\n|Coordinates:||40.632802° N, 22.947194° E|\n|Address:||Ayia Sofia Sq.|\n|Hours:||Daily 8am-9pm (but often closed about 1-5pm)|\n|Lodging:||View hotels near Agia Sophia|\n- Sherry Marker and James Pettifer, Blue Guide Greece: The Mainland, 7th ed. (W.W. Norton, 2006), 603-04.\n- Ayia Sophia - Frommer\'s Greece\n- Photos of Agia Sophia - here on Sacred Destinations\nMap of Agia Sophia, Thessaloniki\nBelow is a location map and aerial view of Agia Sophia. Using the buttons on the left (or the wheel on your mouse), you can zoom in for a closer look, or zoom out to get your bearings. To move around, click and drag the map with your mouse.']"	['<urn:uuid:7de58e59-c5fc-44ad-803a-c3d7abd25ee3>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-13T06:04:36.674696	7	75	604
97	Which equipment types convert engine exhaust into steam?	For steam generation from engine exhaust, the Exhaust Steam Generator Series 1 (ESG1) heat recovery steam generators are used. These systems transform engine exhaust stream heat into primary or secondary steam production. For industrial combustion exhaust heat conversion to steam, the Exhaust Steam Generator Series (ESG) heat recovery steam generators are utilized.	"[""Exhaust Heat Recovery and Steam Generator Systems/* php echo basename($_SERVER['PHP_SELF']); Returns The Current PHP File Name */ ?>\nCain Industries manufactures exhaust heat recovery and steam generator systems for the engine cogeneration (CHP), boiler economizer, industrial heat transfer and exchanger markets.\nCain Industries is a leading designer and producer of combustion exhaust heat transfer products. Founded in 1978 and celebrating over 4 decades in business, our success in lowering fuel costs and related pollution makes Cain the first choice for both combustion retrofit and OEM exhaust heat recovery applications.\nWe are the only manufacturer producing a complete spectrum of exhaust heat transfer and steam production equipment for the gas and diesel engine cogeneration (CHP), boiler exhaust economizer, industrial combustion heat recovery and spiral finned tubing markets. Cain Industries has developed 16 complete product lines with over 3,450 standard models for the correct choice in recovering BTU from exhaust applications.\nCain Industries is your single source for boiler economizers, condensing economizers, condensing heat exchangers, waste heat recovery systems, waste heat boilers (WHB), industrial heat recovery units (HRU), heat recovery silencers (HRS), feedwater heaters, heat recovery steam generators (HRSG) and exhaust steam boilers.\nExhaust Heat Recovery\nCain Industries Exhaust Heat Recovery Systems\nIt is a fact that a minimum of twenty cents of every fuel dollar is wasted, when instead, much of it can be recovered. Cain Industries recovers the heat and transfers the usable BTUs to water, glycol, special fluids or steam. Listed below are some of the combustion sources that would benefit from a Cain heat recovery system.\nSince 1978, Cain Industries has produced high quality waste heat transfer products. We are dedicated to the reduction of fuel usage and pollution worldwide. Our expertise makes us the natural choice for both the retrofit and OEM client. We set ourselves apart from others by producing products to serve a broad spectrum of markets: the Diesel and Gas Cogeneration market, the Boiler Exhaust market and the Fume Incineration market. As the only manufacturer in all of these markets, Cain Industries has the greatest selection of products and system applications available.\nWe have become leaders in this industry by replacing old technology with the most recent technological advancements. Using elaborate computer programs, Cain Industries has developed and manufactured twelve product lines with over 3,500 dependable heat transfer products. Our unique designs increase efficiency and performance, while making installation, service and maintenance more cost effective.\nWe are also dedicated to a primary investment in our associates, their manufacturing technology, quality improvements and innovative cost reductions to meet the customer’s budget. It is by these means that we will achieve absolute customer satisfaction. The success of Cain Industries is a direct result of our simple philosophy: to produce the highest quality products and provide unmatched customer service.\nThe words “safely and economically recover waste heat” also mean “no-risk return on investment” which is exactly what Cain Industries heat recovery systems represent. By installing a fuel saving economizer on a combustion source, the BTU recovered pays for all the equipment installed, usually in 12 to 18 months (or an equivalent return on investment of 75 to 100% annually). This means recapturing approximately 50% of the wasted $$ for every fuel dollar spent. The exact payback period for your installation will depend on local fuel costs and the number of hours of usage.\nDepending on fuel type, temperatures, flow size requirements, performance and specification, Cain Industries can propose a specific cost effective exchanger to economize your fuel bills. Listed below are just a few design features which clearly speak for themselves and far exceed the capabilities of other economizer manufacturers:\n- Internal stainless steel exhaust bypass for stack corrosion control, tempering exit temperatures and/or protection against exhaust backpressure buildup due to fouling.\n- Stainless steel hinged access doors for ease of routine inspection and/or cleaning.\n- Quick release adjustable tension latches requiring no tools to open access doors.\n- Five types of available fin tube materials:\n- SA249 TP316 /316L Stainless Steel Tube with AL-FUSE™ Aluminum Fin - Metallurgically Bonded\n- SA249 TP316 /316L Stainless Steel Tube with 304 or 316 Stainless Steel Fin - Nickel Braze/Welded*\n- SA249 TP316 /316L Stainless Steel Tube with Carbon Steel Fin - Nickel Braze/Welded*\n- SA789 S31803 Alloy 2205 Duplex Stainless Steel Tube with AL-FUSE™ Aluminum Fin - Metallurgically Bonded\n- SA178 Gr.A Carbon Steel Tube with Carbon Steel Fin - Nickel Braze/Welded*\nClick here for additional fin tubing information.\n- No weld/removable tubes with no pressure welds in the gas stream make for easy tube replacement.\n- Round or rectangular design configurations as standard model selections.\n- Custom computer design for special multiple order OEM requirements.\n- ASME Certificate of Authorization: U Stamp, UM Stamp and S Stamp\n- The National Board Certificate of Authorization: NB Mark and R Stamp\n- Canadian Certification of Registration: CRN\nDepending on the application, Cain Industries offers a variety of ancillary equipment, such as timed automatic sootblowers, factory insulation, circulating pumps, thermometers, remote indicating controllers, modulating damper actuators and stack-to-economizer transitions to meet the needs of each specific installation.\nFree Savings Analysis Study\nUpon review of your application, you can expect our proposal within 48 hours. It will include professionally engineered details showing equipment costs, savings analysis, computer-generated economizer performance, cad dimensional drawings, flow schematics, warranty and performance guarantee.\nEase of Installation\nThe selection of a Cain Industries economizer results in the most economical design to install and maintain. Design advantages such as compactness and lightweight construction allow for installation at the very lowest cost.\nAll economizers are guaranteed to meet or exceed the anticipated performance specification.\nGlobal Heat Recovery Markets and Related Product Lines\nExhaust Heat Recovery for Boilers\nIndustrial heat exchangers that transfer boiler exhaust heat to water, glycol and other thermal fluid heat sinks:\nRectangular Tube Recovery Economizers - RTR | Fin Coil Recovery Economizers - FCR | Energy Manager Economizers - EM | Single Stage Condensing Economizers - CXL | Two Stage Condensing Economizers - DXL\nBoiler Economizer RFQ\nExhaust Heat Recovery for Gas and Diesel Engine Cogeneration (CHP) Systems\nIndustrial heat exchangers that transfer engine exhaust stream heat to water, glycol and other thermal fluid heat sinks:\nHeat Recovery Silencer Radial Exchangers - HRSR | Heat Recovery Silencer Axial Exchangers - HRSA | U-Tube Recovery 1 Exchangers - UTR1 | U-Tube Recovery Exchangers - UTR\nEngine Exhaust Heat Exchanger RFQ\nExhaust Heat Recovery for Incinerators, Thermal Oxidizers, Catalytic Converters, Industrial Ovens, Kilns and Furnaces\nHeat exchangers that transfer industrial combustion exhaust heat to water, ethylene glycol and other thermal fluids heat sinks:\nHeat Recovery Silencer Radial Exchangers - HRSR | Incinerator Tube Recovery Exchangers - ITR | U-Tube Recovery 1 Exchangers - UTR1\nCombustion Exhaust Heat Exchanger RFQ\nHeat Recovery Steam Generators (HRSG) for Gas and Diesel Engines\nHeat recovery steam generators that transform engine exhaust stream heat into primary or secondary steam production:\nExhaust Steam Generator Series 1 - ESG1\nSteam Generation RFQ\nHeat Recovery Steam Generators (HRSG) for Incinerators, Thermal Oxidizers, Catalytic Converters, Industrial Ovens, Kilns and Furnaces\nHeat recovery steam generators that transform industrial combustion exhaust heat into primary or secondary steam production:\nExhaust Steam Generator Series - ESG\nSteam Generation RFQ\nGas & Diesel Cogeneration Systems (.pdf)|\nA PDF brochure of Cain l Gas & Diesel Cogeneration exhaust heat recovery systems that include models: ESG1, HRSR, HRSA, UTR1 and UTR.""]"	['<urn:uuid:1a83f617-c437-4ea0-9cbd-8b72cacb0c56>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T06:04:36.674696	8	52	1226
98	How does historical photo research combine artistic creativity with systematic investigation, and what specific steps are involved in evaluating historical photographs?	Historical photo work combines creativity through colorization, where basic Photoshop tools and extensive research on details like military uniforms, medals, and authentic colors are used to bring historical images to life. The goal is to help people better understand and connect with historical events by seeing them in color as they actually appeared. For systematic investigation, there is a structured 5-step process: 1) Asking detailed questions about the photo's origin and provenance, 2) Focusing analysis by clearly defining research questions using the five Ws, 3) Careful examination of physical details like uniforms, insignias and markings, 4) Conducting exhaustive research using various sources and databases to learn more context, and 5) Explaining and documenting the findings. This methodical approach helps determine a photograph's historical value and authenticity.	['Marina Amaral was studying international relations at the Pontifical Catholic University of Minas Gerais, Brazil, when she first tried her hand at digitally colorizing a historical photograph. She had no formal background in art or photography, but since childhood she had enjoyed working with Photoshop in her free time. One day, while not feeling terribly optimistic about future career prospects, she encountered a collection of colorized World War II-era photographs on the web and decided to reproduce the technique.\nTransfixed, she continued to practice and improve, eventually adopting digital photo restoration and colorization as her profession. In September, she will publish “The Colour of Time: A New History of the World, 1850–1960,” a book of 200 colorized historical photos accompanied by captions by best-selling historian Dan Jones.\nAmaral discovered multiple photos that will appear in the book in the Library of Congress online collections. Here she answers a few questions about her work and her use of photographs from the Library.\nHow do you determine which photos to colorize?\nMy choices are based on two important factors, above all others. First, a photo needs to be in the public domain, and it also needs to be of medium or high quality. In a way, this greatly diminishes my options, but these are two essential factors for me. There are some amazing photos that I would love to colorize, but they are protected by copyright restrictions. Beyond that, I’ll evaluate how interesting a photo is based on its subject, composition and small details that catch my attention, like the face of a person smiling at the camera. All this contributes to my final choice.\nWhat is your goal in colorizing historical photos?\nI strongly believe that colors have this immense power to makes us better understand that historical people and events were real. When colors are applied to the faces of people, to the buildings and streets of cities in photos taken decades ago, I believe that we can create a greater empathy and a deeper connection with what we are seeing. My goal is to offer this perspective, allowing people to see those particular scenes and people in color, as they were actually seen through the photographer’s eyes.\nWhat kind of research do you do before beginning?\nI research everything I can—military uniforms, medals, hair, skin colors. Since I’m not an expert, I count on the help of historians and people who have studied enough to be able to give me a direction to follow. They are definitely essential to my work, and I am very grateful that they are always willing to help me.\nWhat tools and techniques do you use to achieve such life-like results?\nThere’s no big secret. I use basic Photoshop tools and a great deal of patience. But what helps me achieve realistic results is the fact that I’m never satisfied, and I’m always looking for something new to study and learn that might add to what I currently do. I study traditional painting techniques and principles of chemistry and physics, among other things. I’m always trying to improve.\nWhich photographs and collections at the Library of Congress have you used?\nThe Library of Congress is the best source of high-quality photos I know of. Sometimes I spend more time looking at the collections than colorizing the pictures! These collections are an extremely valuable resource for us—artists, photographers, teachers or lovers of history and photography.\nWhich Library photos stand out for you?\nThe entire Civil War Collection is absolutely amazing. I always find incredible photos there. My favorite is an 1865 picture by Alexander Gardner of Lewis Powell, also known as Lewis Payne, a conspirator with John Wilkes Booth, who assassinated President Abraham Lincoln. Another favorite is an 1864 photograph by Timothy O’Sullivan of a Union “war council” meeting in Massaponax Church, Virginia, including Gen. Ulysses Grant, Gen. George Meade, Assistant Secretary of War Charles A. Dana and others.\nWhat has your experience been like working with the Library’s collections?\nIt has been amazing so far. The website is easy to use, and I always have a very enjoyable experience looking at the collections.', 'Updated: Apr 2\nToday I would like to introduce Luke Sprague to our audience. Luke Sprague operates HistoryMint.com, a private historical research firm specializing in military history. Luke Sprague donates time each week to the Latah County Historical Society to assist in reference requests and acquisition appraisals. This is one such acquisition appraisal which Luke Sprague took the reigns on to determine if LCHS should acquire this photograph.\nWhen a member of the community brings in any type of object into the Latah County Historical Society, there is an investigation to determine how that object fits into the collection. Without the right information, those deciding which objects to add to the collection would face an insurmountable task. Therefore, it is crucial to understand the facts, the history, and historical opinions surrounding each object in order to access its historical value to the collection. In this post, we are going to follow the investigation into a photograph donated by a community member. Here are the five steps I use to research an object—often moving back and forth between the steps.\nThis is the image donated by a community member.\nStep 1: Ask questions\nThis may seem like an obvious thing to do, but not everyone does it. When a member of community walks in, ask them what is this thing? Where did you find it? Who found it? When did you discover it? What would you like done with it? What research have you already completed on the object and may we have a copy of it? Often, this series of questions reveals important details that help with later research. In this example, by following up with a phone call, I determined that this framed photograph had been found in a garage wall during a remodel. The community member told me he was not related to anyone in the photograph, nor did he know much beyond what he had told me already.\nStep 2: Focus your analysis\nKey to success in any research is clearly defining the question you are asking. Write down or at least know in your head what question you are trying to answer prior to starting any historical research. Focusing on the question you are trying to answer will determine the speed with that you arrive at the answer. I use the “five Ws” to zoom in on a particular topic: who, what, when, where, and why. In this First World War photograph example the question we are trying to answer is, “Do the men in this picture have any particular relationship with Latah County? Or does the image offer a compelling historic narrative that will improve the Latah County Historical Society collection?” Please note, this does not mean you cannot change your question or revise it, but this technique will tend to focus your efforts and prevent drifting into irrelevant topics.\nStep 3: Examine the object carefully\nCarefully look at the object and see what details it exhibits. In the case of the above photograph, a closer look provides a wealth of information that makes clear its origin. Look at the below close-up photograph and look at the men’s boots in the front row, notice how they are the same except for one?\nIf you look carefully at the center of the photograph with a magnifying glass, you will find a man sitting down in the center of the first row with black boots, he has bright-gold acorns on his hatband, and no one else has them; this is likely their commanding officer.\nAs a standard practice, the commanding and/or senior non-commissioned officer would sit in the center of a photo such as this. The officer also wears rank insignia on his shoulders and the “U.S.” brass on his collar, whereas the enlisted soldiers wear circular brass “U.S.” emblems on their collars.\nThe branch insignia on this officer is compelling evidence. The branch insignia designates what a particular officer specializes in, and in this case, you can just make out the aviation propeller supported by two wings.\nThe branch insignia of the officer combined with where he is sitting indicates that this is some type of aviation unit.\nNext, note the text in the lower left, “Section H, A.S.M.S. Kelly Field, Tex 12-7-18.” This text alone reveals the photographer likely took the photo at Kelly Field, Texas on December 7, 1918 with men wearing First World War United States Army uniforms. For now, set aside the text “Section H, A.S.M.S.,” we will come back to that later.\nThe lower right corner of the photograph shows that this is a “Photo by Stead,” likely an Army photographer or as it turns out in this case, a photographer under contract to the Army.\nColor enhanced image of the back of photograph\nThe back of the photograph shows that, “From Chas A. Stead [illegible] Kampann BLDG San Antonio, Texas, Phone Travis 2028” likely took this photograph.\nAlso on the back of the photo are a negative number, date, price, and significantly, what appears to be someone’s surname, “Zinsser.” There is also something written underneath “Zinsser” but it is not clear what it is, looks like “Cui…mnd.”\nStep 4: Use research to learn more about the object\nAfter examining the object itself, it is time to use research to discover more about the object. Research on each object often is exhaustive in nature to where you reach a point of diminishing returns, limited by budget and available labor. The depth of historical research can potentially be unlimited; however, at some subjective point, you (as the researcher) will know that you have found as much detail as is relevant. In my opinion, the researcher is looking to “paint a picture” of the past that often is only 75 to 85 percent accurate and beyond this return on effort can drop off significantly. This is the art of historical research, finding that “sweet spot,” where a somewhat accurate image of the past is created at a reasonable cost.\nI recommend less experienced researchers start with a structured search plan that leads your research through specific search tools and known resources. After more experience, I think you will find that you will develop a sixth sense or instinct about where specific objects and details can be found. This “gut” instinct can be extremely valuable as it grows over time—listen to it! Organization is fundamental to successful historical research; I strongly suggest a note taking system, a filing system, and a formal documentation system.\nSpecifically for this photograph, with a little bit of research we learn that the Army used Kelly Field Texas as a new Aero Squadron training facility for the First World War. In the First World War, Aero Squadrons were still part of the Army Signal Corps and had just begun to make the transition to a separate branch within the Army. The Signal Corps linkage harkened back to the Army using balloons for reconnaissance, observation, and signaling. Between 1917 and 1918, the Army trained thousands of pilots, mechanics, and support personnel at Kelly Field, Texas. Of particular interest to us, are the Mechanical Service (M.S.) Squadrons that trained mechanics for wartime aircraft. These “school house” units turned out thousands of students each month during the First World War.\nThis is why in the photograph above you see the “A.S.M.S.,” an acronym for Aero Squadron Mechanical Service. The “Section H” stands for the new unit identifier created for the unit in July 1918 replacing from the former unit identity, the 244th (II) Aero Squadron. It is unclear why the photographer labelled this photo “Section H,” though it is in all likelihood a mislabeling of “Squadron H,” given that a United States Army section is much smaller (7 to 9 soldiers) than a squadron (usually hundreds of soldiers). In addition, the United States Army does not designate smaller size units, like sections, with letters, but instead uses numbers.\nInterestingly, this was also likely the demobilization photograph for this unit as the First World War ended and the majority of soldiers returned home. This newly designated Squadron “H” at Kelly Field Texas of the Aero Squadron Mechanical Service remained in active service from July 1918 to November 1918 with our photograph dated December 7, 1918. Squadron “H” at Kelly Field Texas never deployed to Europe but instead served as a training unit stateside.\nConcerning the surname “Zinsser” on the back of the photograph, I had optimism that this would link us back to Latah County, Idaho. In terms of general research techniques, I recommend starting the name search with known and reliable indexes, and then moving on to related organizations. With this photo, the card catalogue revealed a small number of Zinsers in Latah County Idaho, but no Zinssers with two “s.” The Past Perfect database indexing the collection closely mirrored what I found in the card catalogue.\nWe also at one point had hopes that the Zinsser name on the photograph might be an ancestor of Elizabeth Zinser, the former first woman President of the University of Idaho, but unfortunately, her ancestors were from Pennsylvania not Idaho. The birth and death indexes for Latah County Idaho revealed for all practical purposes no Zinssers with either one or two “s” in a lifespan. The obituary files showed no Zinser or Zinsser that would have matched a soldier in the 1918 photograph.\nThe Latah County Historical Society photo collection also showed no connections or linkages to this image. All third-party online databases including the First World War draft records for Idaho and Fold3HistoryMint.com show nothing for a various spellings of Zinsser in Latah or surrounding counties. Also problematic, the address the community member provided made no sense in terms of their explanation of the discovery; therefore, a property deed search based on the street address made no sense.\nStep 5: Explain your findings\nThis step is self-explanatory; explain to your client or in this case Latah County Historical Society what you have found. Often you will find nothing or not what you expect, and though it may not seem like it, this is a result.\nFor today’s example, I find that at this time the “Zinsser” First World War photograph has no linkages to Moscow or Latah County. This is a photograph from an Army training or “school house” unit at Kelly Field Texas. This unit, Squadron “H” existed for five months from July 1918 to December 1918 and likely cycled through numerous trainees during that time. Thousands of American men attended these types of training units during the First World War and the majority of the units took photographs like this one.\nSoldiers who paid for this photograph may have taken it with them as they moved throughout the country during their lifetimes. The discovery of the photo in Moscow does not imply a strong connection to Moscow or Latah County, Idaho. Therefore, I conclude that other than an example of First World War uniforms, this object retains limited value to the Latah County Historical Society.\nCenter of Military History. Order of Battle of the United States Land Forces in the World War, Zone of the Interior: Directory of Troops. Vol. 3, part 3. Washington: Center of Military History, United States Army, 1988.\nLatah County Historical Society, Moscow, Idaho (hereafter LCHS) all photograph credits. LCHS, Archives, Small Collections 92-62. LCHS, Archives, Small Collections 02-40. LCHS, Latah County Birth and Death Indexes. LCHS, Obituary Files, “Z.” LCHS, Photographs, Latah County 30-16. LCHS, Photographs, Moscow 1-7-16. LCHS, Photographs, Moscow 1-11-16.\n“United States World War I Draft Registration Cards, 1917–1918.” Index and images. FamilySearch.org: accessed 2014. Citing NARA microfilm M1509. Washington: National Archives and Records Administration, n.d.\nLuke Sprague, M.A., lives in Moscow, Idaho and helps authors, screenwriters, and other creators with their United States military content for HistoryMint.com. Luke is also a volunteer researcher at the Latah County Historical Society in Moscow, Idaho.']	['<urn:uuid:003d2192-8e24-4785-b7aa-e0b9ffa4e317>', '<urn:uuid:f8856e8a-9078-41f2-bfcc-b07f216c1e48>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T06:04:36.674696	21	126	2667
99	why modern sci fi include cyberpunk elements	Cyberpunk has fundamentally changed how we look at science fiction. Most modern science fiction contains at least some cyberpunk elements because the genre established a more realistic approach to depicting the future. Instead of presenting overly optimistic visions, cyberpunk took humanity as it is and extrapolated a future from that perspective, characterized by the concept of 'High Tech, Low Life.' This influence is so pervasive that it has become difficult to establish clear genre boundaries for cyberpunk.	['Cyberpunk is not and never has been a retrofuturistic genre. Cyberpunk has always been about our future, not a future once imagined. Retrofuturism is defined as “a trend in the creative arts showing the influence of depictions of the future produced in an earlier era.” This definition is easily applied to other “punk” genres such as Steampunk, Atompunk, or Deiselpunk because they are set in alternate histories where futuristic technologies were developed, often as they were imagined at the time the stories are set. Jules Verne, although often referenced as such, is not “steampunk”, although it might be a precursor that inspired the genre. Verne was writing about the future, not someone else’s imagined future. And neither were the original cyberpunks.\nWhy is Cyberpunk confused with Retrofuturism?\nThere are some reasons that cyberpunk is often confused with retrofuturism. First, cyberpunk began as a recognized genre in the 1980s, as a reaction to overly optimistic science fiction that had preceded it. It took humanity as it is, rather than how we would like them to be and extrapolated into the future from that perspective. Humans aren’t great, and neither will the future be. Then nearly 40 years passed, and here we exist very much in this predicted world. People who grew up reading cyberpunk were now writing science fiction, but they weren’t writing 1980s cyberpunk, they were writing 90s, 2000s, or 2010s cyberpunk. They were still writing about the future from their point forward, not something set in the 80s, or specifically about the future as it was imagined in the 80s. For instance, computers turned out quite a bit different than they were imagined in a lot of ways, and so the computers changed to reflect this.\nThere is a lot of nostalgia for the cyberpunk of the 80s. The books were excellent. The movies were excellent. And nobody had seen anything quite like it before, although there were certain threads in other fiction – the proto-cyberpunk if you will. Most movies made in the cyberpunk style today are still extrapolating the future from when they were written and not from an imagined 1980s. There are movies that emulate the style of the 1980s, but they are still about the future beyond us, and they tend to be informed by modern sensibilities.\nNostalgia has also led to a rise in the musical genres of retro-wave, synth-wave, and vapor-wave which all take heavy cues from 1980s cyberpunk movies. Some of the narratives tucked away in these albums have a retro-futuristic tang to them, like Irving Force setting the EP, The Violence Suppressor, in the “distant future of 2015,” with a pointed cyberpunk feel to it. Another example is VHS Glitch’s narrative being set in 1980. This music is all excellent and brings my mind back into the 1980s aesthetic and is as close as we get to seeing people create truly retrofuturistic cyberpunk tales.\nOf course, it doesn’t help that the cyberpunk movement inspired the steampunk movement and its followers. In fact, K. W. Jeter (who wrote Morlock Night and Infernal Devices – steampunk classics), who coined the term steampunk, wrote one of the earliest books to be considered cyberpunk, Dr. Adder. Although it was written in 1974, it wasn’t published until 1984 when it finally fit into a genre that could accept its weirdness. Then, William Gibson and Bruce Sterling wrote The Difference Engine which is solidly steampunk. So it isn’t surprising to me that people might make this mistake, because many of the important figures in the steampunk movement, were also involved in the cyberpunk movement, and steampunk IS a retrofuturistic genre.\nCyberpunk is still being written today. It is still about our dark future. The technology has changed, the authors may have even changed, although many of the original cyberpunks are still writing it, take Bruce Sterling for example. Sterling constantly churns out short stories and novels that still fit the genre. Cyberpunk is everywhere because it changed the way that we look at science fiction. Most modern science fiction has at least a kernel of cyberpunk in it. No wonder it so hard to nail down a genre definition. The important elements are that it is “High Tech, Low Life,” and there is tons of science fiction being written today that fits into this simple definition AND it is still about the future, not a future once imagined.']	['<urn:uuid:7a880ade-3d24-4b8f-bc10-90f5a78fd429>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T06:04:36.674696	7	77	731
100	lubricant requirements honing cylinder versus changing miter saw blade which process needs oil	Only cylinder honing requires lubrication, specifically needing an oil-based lubricant like mineral oil or specialized cylinder honing lubricant applied liberally to the honing device. The miter saw blade changing process does not require any lubrication or oil application.	"[""Honing a cylinder sounds complicated at first, leading many people to take their engine blocks to professionals for a simple hone. In reality, honing is basically just polishing metal, and it is something the average do-it-yourself mechanic can easily do once the engine is stripped apart. The tools required for this project are inexpensive and can easily be found over the Internet or at many automotive stores. A cylinder-honing kit usually attaches to an electric drill and is a simple cylindrical device with an abrasive surface. The spinning device is thrust into the cylinders multiple times until the desired surface texture is achieved. These instructions assume you have already torn the engine down, the cylinders are exposed and you are ready to begin honing.\nSelect the proper-sized honing tool. Keep in mind when making your selection that these are intentionally built slightly oversized. For example, if the cylinder you want to hone is 2 inches wide, there is no need to purchase a honing tool that is larger than 2 inches wide. It will automatically come slightly larger so it applies adequate pressure to polish the cylinder walls. The amount of polishing done is not determined by the width of the tool, but rather by how long it is used inside the cylinder.\nThe desired rotational speed is 1,200 to 1,600rpm, so make sure that speed is within your drill's normal operating range at full power. If your drill has a speed selection switch, set it to around 1,400rpm. Any rotating device that operates within the required rpm range will work, but a drill is usually the most convenient device for this task. If you're using a cordless drill, make sure the battery is fully charged before beginning. A reduced charge could lower the rotational speed and alter the results.\nApply lubricant liberally to the honing device. Any oil-based lubricant will work for this task. Mineral oil or a lubricant specifically made for cylinder honing will work fine. Slowly rotate the honing tool using the drill, and pour oil over the entire contact surface. To ensure proper lubrication, apply more oil than what appears to be necessary. It is better to have too much than not enough.\nHold the trigger on the drill to maximum operating speed, and quickly thrust it in and out of the cylinder several times. This process removes metal rapidly, so don't go more than five thrusts without stopping to check the cylinder walls. The desired finished product will be shiny, without visible nicks, scratches or wear, but you want to remove as little metal as possible. When it appears you are halfway done, reverse the direction of the drill (clockwise instead of counterclockwise, or vice versa) to produce a criss-cross pattern that helps oil stick to the cylinder walls.\nWhen finished, wash the surface area with soapy water. The honing process produces metal dust and fragments in the cylinder that could potentially cause engine damage, so it is important to clean away as much of the metal as possible. When the cylinder has been thoroughly washed and dried with a lint-free cloth, apply a thin layer of motor oil to the freshly polished surface to prevent oxidation.\nOnce you are finished with the honing kit, it is not likely you will use it again in the near future, so consider selling it to recoup your money.\nCheck your progress often to avoid removing more material than needed. Always wear safety glasses when working with power tools.\nTips and warnings\n- Once you are finished with the honing kit, it is not likely you will use it again in the near future, so consider selling it to recoup your money.\n- Check your progress often to avoid removing more material than needed.\n- Always wear safety glasses when working with power tools."", 'How to Change Blade on Craftsman Miter Saw – 9 Simple Steps\nCraftsman is a renowned company for its quality meter saw. They offer durable and sharp blades for effective and efficient cutting. Unfortunately, the blade starts getting dull with time. Changing the blade is the only option to get the sharpness back. Yes, you can sharpen the blade several times before changing, but eventually, you have to change it. However, changing the blade is a tedious and challenging task. Hence, our writings on how to change blade on Craftsman miter saw will make it easier for you to change the blade. You can hire a professional to get the job done for sure. But you don’t have to do that after reading this guide. You can smoothly change the blade by yourself.\nHow to Change Blade on Craftsman Miter Saw – Deep Discussion\nThe process of changing the blade on a Craftsman miter saw is not different from other saws. Just follow the guideline below.\nStep 1: safety\nSafety comes first, always. A silly mistake or laziness can cause you serious damage. Ensuring protection and safety is more important than knowing How to change blade on Craftsman miter saw. For that, make sure you wear\n- Safety goggles for eye protection\n- Rubber hand gloves\n- A helmet\n- Ear protection from sound\n- Full sleeves shirt\n- High-quality boots\nStep 2: Preparation\n“Give me six hours to chop down a tree and I will spend the first four sharpening the axe.”\nThat means good preparation will do half of the job. To ensure perfect preparation, make sure to\n- Remove or unplug the miter saw even after turning off the machine. It will reduce any risk.\n- Wait until the motor and blade become cool. The motor and blade stay hot even after turning off your saw.\n- Keep the area clean so you don’t lose any parts\n- Put all the removed parts aside to find it easily\n- Keep a wrench, screwdriver, and new blade right beside your workstation\nStep 3: Turn off its switch\nFirst, turn off its switch. Simply flip the switch to the off position. Also, unplug the cord since it will prevent any accidental switching on.\nStep 4: Remove blade cover\nSome people change the blade without removing the blade cover or guard. If you want to do that, simply grab it and lock it to its highest position. You will have enough space to work on. However, sometimes, it may come into the line with a blade. In that case, removing the guard would be a better idea. To do that,\n- Set the guard cover to the highest position\n- Take your screwdriver and loosen the guard screw\n- Detach the screw and remove its cover\nStep 5: Lock the blade\nBefore unscrewing the saw blade, it is important to lock it. Press the lock knob in one hand. It will stop the blade from turning. Without locking, it will be a risky task to change your saw blade. Remember to keep pressing the knob until you remove the blade.\nStep 6: Remove the blade\nAfter locking, simply remove the blade. To remove the blade from the saw,\n- Use your other hand and take a wrench. Some people prefer using a standard 3/8” wrench. However, depending on the screw size, pick the most suitable wrench.\n- Then use your wrench and unscrew the blade arbor bolt. Here, make sure to rotate clockwise. Some people find it bread and butter to remove any bolt since all have a common thread. But here, you will face a reverse thread that needs to be rotated clockwise.\n- After removing the arbor bolt, remove the washer.\n- Now unlock the blade\n- Finally, use both of your hands, take the blade, and remove it. Be careful handling the blade.\nSometimes, you may find it stubborn to remove. In that case, tip it slowly forward and back. It will be free and come up easily.\nStep 7: Install a new blade\nYou already have a new blade beside your table. Install the new sharp blade. To do that,\n- Simply slip the new blade onto the arbor\n- Make sure the teeth of the blade are facing downward\n- Now put back the washer where it was\n- Use your wrench and screw the arbor bolt. Tighten properly.\nStep 8: Put back the blade guard\nJust like you remove the blade guard, put it back in the same way. Use the screwdriver to tighten the blade guard screw.\nStep 9: Have a trial\nFinally, plug it on and turn on the switch. Have a trial cut to make sure you have done it correctly. If you follow the steps above, it should work fine.\nWatch the video below if you still have any confusion –\nAlso, read this – How to Use A Sliding Miter Saw Safely!\nFrequently asked questions\nHave you got any questions related to blade changing on Craftsman miter saw? Then quickly have a look at these questions asked often by people.\nHow often should you change a miter saw blade?\nThere is no rule of thumb to change a miter saw blade. Whenever you find out your saw blade is dull, making tears in wood pieces, and making too much sound, you should consider changing the blade.\nCan you sharpen a miter saw blade?\nYes, you can sharpen a miter saw blade when it is blunt. Simply remove the blade, use a clamp to place it on the workbench, and then sharpen your blade.\nCan I put a 10-inch blade on a 12 inch miter saw?\nYou can put a 10-inch blade on a 12-inch miter saw since the arbor hole size is the same. However, it is not recommended to use any size smaller and larger blade for miter saw for better performance and safety.\nChanging your dull, blunt saw blade will bring efficiency to your work. Some people ignore changing the blade for the tediousness and challenges of replacing it. Our guide on “how to change blade on Craftsman miter saw” will make it easier and non-challenging to replace it. You can do it by spending less than an hour.\nWith a new and sharp blade, working becomes easier. Also, it delivers a smooth and straight cut line, unlike the dull blade. The wood pieces stay in shape after cutting and no wear and tear are found there. So make sure to change the blade in time to get the maximum result from it.']"	['<urn:uuid:e8e73a56-c7ef-4866-858e-f42ce10638a9>', '<urn:uuid:65295083-593d-468f-b5bc-cae008cb7601>']	factoid	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-13T06:04:36.674696	13	38	1732
