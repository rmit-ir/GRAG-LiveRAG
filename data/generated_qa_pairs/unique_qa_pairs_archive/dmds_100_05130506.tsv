qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	winner clive churchill medal losing team grand final rugby history	Brad Mackay became one of only four players (along with Bradley Clyde in 1991, Daly Cherry-Evans in 2013 and Jack Wighton in 2019) to win the Clive Churchill Medal from the losing side when the Dragons lost 6-14 in the 1993 Grand Final to the Brisbane Broncos.	"['|Born||7 February 1969|\nSydney, New South Wales, Australia\n|Height||183 cm (6 ft 0 in)|\n|Weight||90 kg (14 st 2 lb)|\nBrad Mackay (born 1969 in Sydney, New South Wales) is an Australian former rugby league footballer who played in the 1980s and 1990s. A versatile lock for the St George Dragons, Illawarra Steelers, Western Reds and the joint-venture of St. George Illawarra Dragons, he also represented New South Wales in the State of Origin and Australia.\nA St George junior from the Brighton Seagulls club, Mackay was graded by the Dragons in 1987 and then played 117 games with club over eight seasons till 1994 scoring 22 tries. Playing all positions in the backline bar halfback, he was most remembered for his lock position in his peak.\nMackay debuted for the New South Wales Blues as a reserve in game II of the 1989 State of Origin series. He went on to play 17 Origin games between 1989 and 1995, scoring three tries at that level.\nMackay had his best season in 1990, making his test début for Australia in the mid-season test against France, becoming only the second player to score a hat trick of tries on début for Australia. A month after the France test, Mackay went on to play in a one off test against New Zealand. Following the season he was selected for the 1990 Kangaroo Tour, coming into the test team for the winning 2nd and 3rd Ashes Tests against Great Britain before playing in the two tests against France.\nAfter failing to hold his test place in 1991, Mackay was selected for the first two tests against Great Britain during the Lions 1992 tour of Australasia but missed the last match of the series through injury. After playing in a World Cup qualifying match against Papua New Guinea in Townsville, Mackay was used as a replacement in the Dragons 8-28 Grand Final loss to the Brisbane Broncos and missed a place in Australia\'s World Cup Final winning team.\n1993 saw Mackay continue his good form and he was the starting lock forward for NSW in all three State of Origin games in the Blues 2-1 series win over Queensland. The return from injury of Bradley Clyde would see Mackay on the bench for Australia in the 1st and 3rd Trans-Tasman Tests against New Zealand.\nLater that year he became one of only four players (the others being Bradley Clyde in 1991, Daly Cherry-Evans in 2013 and Jack Wighton in 2019) to win the Clive Churchill Medal from the losing side when the Dragons went down 6-14 in the Grand Final, again to the Brisbane Broncos.\nSt. George\'s form dropped markedly in 1994 and Mackay could only make the Bench for NSW in their 1994 State of Origin series win over Queensland, with NSW coach Phil Gould preferring Bradley Clyde at lock. He made his final test appearance for Australia from the interchange bench in the 1994 mid-season test against France at Parramatta Stadium, but the Dragons poor form, which saw them fall to 11th on the premiership ladder, saw Mackay overlooked for the 1994 Kangaroo Tour bringing down the curtain on his international career.\nMackay decided to leave the Dragons due to a poor relationship with coach Brian Smith, later saying, ""Brian had signed an extension on his contract and so I felt like I needed a change or I\'d go down the drain as a player.""\nMackay joined the Western Reds in 1995, accepting the foundation captaincy of the Perth-based side, but moved back east when they joined the Super League franchise from 1996.\nHe started playing for the Illawarra Steelers in 1996, staying for three seasons over which he played 56 games. A return to his best form and season-long consistency in 1997 earned him the BHP Medal as the Steelers\' player of the year. Mackay later became part of the new St. George when the joint venture was formed in 1999, playing 24 games, including an appearance from the bench in the 1999 NRL Grand Final loss to the Melbourne Storm.\nMackay had played a total of 248 first grade games over 12 years.\nMackay played out his career in the 2000 season with the Bradford Bulls in the UK, winning the Challenge Cup Final with them.\nBrad Mackay is a committed Christian. In 2003, he was active in fund raising for the Joanne Mackay Helping Hand Foundation following the 2002 passing of wife, Joanne from breast cancer. He lives in the Illawarra area with his second wife, Tracy and is a fireman.\nWendell Jermaine Sailor is an Australian former professional rugby footballer who represented his country in both rugby league and rugby union – a dual code international.\nRod Wishart is an Australian former professional rugby league footballer who played the 1980s and 1990s. A New South Wales State of Origin and Australian international representative goal-kicking winger, he played club football with the Illawarra Steelers and the St. George Illawarra Dragons in the National Rugby League.\nCraig Young is an Australian former representative rugby league footballer for the Australia national rugby league team, the New South Wales Blues and a stalwart player over 11 seasons from 1977 to 1988 with the St. George Dragons in the NSWRL premiership competition. He played as a prop-forward. His nickname was ""Albert"" after his middle name and/or the cartoon character Fat Albert.\nCraig Fitzgibbon is an Australian professional rugby league coach and a former professional rugby league footballer who played in the 1990s, 2000s and 2010s.\nShaun Timmins is an Australian former professional rugby league footballer who played in the 1990s and 2000s. A New South Wales State of Origin and Australian international representative utility, he played his club football in the National Rugby League for the Illawarra Steelers and the St George Illawarra Dragons. Timmins was a versatile player, performing at five-eighth, centre, lock, and second-row at representative and club levels.\nMark Coyne is an Australian former rugby league footballer, a state and international representative player and an Insurance Executive. His football club career was with the St George Dragons and the merged St George Illawarra Dragons - he captained both sides. He played principally at centre but sometimes as a wing in his notable representative career. He was also the brother of another first grade footballer, Peter Coyne.\nBradley Clyde is an Australian former professional rugby league footballer who played in the 1980s, 1990s and 2000s who, at the peak of his playing career was widely acknowledged as the best lock in the game. He represented both New South Wales, and played for the Australian national side, and played his club football in Australia for the Canberra Raiders and Canterbury-Bankstown Bulldogs, and in England for Leeds Rhinos.\nBrett Mullins is an Australian former professional rugby league footballer who played in the 1990s, and early 2000s. A New South Wales State of Origin and Australian international representative back, he played his club football for Australian clubs the Canberra Raiders and Sydney Roosters, and for English club, the Leeds Rhinos. He was described as ""one of the most exciting attacking weapons in rugby league.""\nTimothy Brasher is an Australian former rugby league footballer who primarily played as a fullback in the 1990s and 2000s. He began his career with the Balmain Tigers, where he played the majority of his career. He then moved to the South Sydney Rabbitohs for two seasons, before finishing his career with a season at the North Queensland Cowboys. During his career he also represented New South Wales on 21 occasions and Australia on 16 occasions.\nPaul McGregor is an Australian professional rugby league coach who was until August 2020, the head coach of the St. George Illawarra Dragons in the NRL, and a former professional rugby league footballer who played as a centre in the 1990s and 2000s.\nChris Johns is an Australian former rugby league footballer who played in the 1980s and 1990s. He played in the centres, achieving representative honors for Australia and New South Wales. His club football career was spent with the St. George Dragons and Brisbane Broncos, as well as two spells in England, first with Castleford in 1986-87 and then Barrow in 1989–90. After retiring from the playing field, Johns worked in the administration of the Brisbane Broncos and Melbourne Storm clubs.\nWayne Bartrim, is an Australian former professional rugby league footballer who played in the 1990s and 2000s. He was selected to represent Australia and Queensland during his career, which he spent playing for the Gold Coast Seagulls, St. George Dragons and the St. George Illawarra Dragons in Australia and the Castleford Tigers in England. Bartrim primarily played his club career as a lock, but played his representative career as a hooker.\nTrent Merrin is a former Australian professional rugby league footballer who played as a lock, prop and second-row forward for the St George Illawarra Dragons in the NRL and Australia at international level.\nBoyd Cordner is an Australian former professional rugby league footballer who played as a second-row forward for the Sydney Roosters in the NRL and Australia at international level.\nTyson Lomano David Frizell is a professional rugby league footballer who plays as a second-row and lock for the Newcastle Knights in the NRL. He has played for Wales in the 2011 Four Nations and 2013 Rugby League World Cup and Australia at international level.\nPaul Vaughan is a professional rugby league footballer who last played as a prop for the St. George Illawarra Dragons in the NRL. He has played for both Italy and Australia at international level.\nCameron McInnes is an Australian professional rugby league footballer who captains and plays as a hooker for the St. George Illawarra Dragons in the NRL.\nEtonia Nabuli is a Fijian-born Australian professional rugby footballer who plays rugby league for the Wentworthville Magpies in the Ron Massey Cup. He previously played rugby league for the St. George Illawarra Dragons in the National Rugby League as a wing, also representing Fiji. He also played rugby union for the Queensland Reds.\nHame Sele is an Australian professional rugby league footballer who plays as a lock and second-row forward for the South Sydney Rabbitohs in the NRL.\nBlake Lawrie is an Australian professional rugby league footballer who plays as a prop and lock for the St. George Illawarra Dragons in the NRL.']"	['<urn:uuid:bc5b1ab8-450a-42d9-b1dc-edcfbb4ee263>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	10	47	1728
2	What role do customer preferences and behavior patterns play in shaping both premium airline services and sustainable dining initiatives at airports?	Customer behavior analysis shows that in Portland International Airport's food court, 77% of customers preferred sitting down to eat rather than taking food away, which enabled the successful implementation of a reusable plate program where 45% of orders were served on Green Plates, with all surveyed customers expressing satisfaction with real dishes. Meanwhile, Lufthansa's approach to premium travelers reveals different behavioral considerations, where passengers with tight connections or early flights need quick-service options, leading to the development of their 'Delights to Go' automated service. However, there are questions about whether the offering truly meets premium travelers' expectations, as some view the simple snack box containing items like an apple and nut bar as underwhelming for a premium service.	['If you don’t have time to visit the lounge owing to an early morning flight or a late connection, what can airlines do to keep a premium traveller keen, and keep them coming back? A six-month trial of a new Lufthansa automated self-service space in the departures area — which the airline tells me will start in Munich this week — starts asking that very question.\nThough details are scant, “Delights to Go” seems to be a scan-your-way-in, self-service space that Lufthansa is trialling for passengers cutting it close with a departure or whose ever-tighter connections are stretched. On offer are on-demand snack boxes and hot drinks, dispensed from a multilingual kiosk accessed via boarding pass scans.\nI asked Lufthansa for details and images, but a media representative declined to share any further details or images when I asked, saying that they might be available five days hence. German website Flugrevue, however, seems to have snapped some images of the concept at the reveal earlier in the week, with a single shot of what Lufthansa tells me is “a photo of the mock up poster”.\nPart of me likes the concept and applauds Lufthansa for trying it. I can’t count the number of times I’ve had to hoof it through Frankfurt on a connection stretched to breaking point as the result of Lufthansa operational problems. Being able to stop and grab a to-go box could certainly be handy.\nBut at the same time I have practicality concerns.\nFlugrevue also says that the snack box — rather aspirationally named Genießer Box, which to my beginner’s German using Google Translate comes out as “gourmet” and, er, “box” — will contain an apple, a cereal/nut bar, a piece of cake and a small bottle of water. This seems a little underwhelmingly shelf-stable, and one shudders to think of the packaging waste.\nTo start at the beginning of the Delights to Go experience, Lufthansa will need to look at how it staffs the kiosk. Peering at the details of the Flugrevue snap, the staff member doesn’t seem to be wearing a Lufthansa uniform. As frequent premium class travellers and frequent flyers through Lufthansa’s main airports know, the airline has outsourced many of the “lounge greeter” positions at its lounges to the multi-service staffing organisations best known across Europe for private sector prisons, and indeed the warmth of the welcome seems to carry over.\nI’m also not sure why, if there’s a gate dragon, it’s necessary to complicate matters with an automated vending machine. I can just imagine the thing breaking down during the morning rush and the resulting queue of undercaffeinated, unimpressed frequent flyers. Would a simple soda fridge, snack basket and Nespresso machine perhaps make more sense — or indeed some sort of automated token system handed out by the staffer to prevent the inevitable sweep-it-into-the-cabin-bag troughing of what’s on offer?\nIt also strikes me that the point about stretched connections is really Lufthansa saying it’s trying to solve a problem that it creates itself. To my mind — and the minds of many others — the airline offers many unrealistic minimum connection times, particularly in Frankfurt, especially when crossing the Schengen border, given that it is one of Europe’s worst laid out and least connection-friendly airports.\nAt the end of the day, though, I can’t help wondering whether my threshold for “delight” is a little higher than the airline’s. An apple and nut bar — especially when Lufthansa has for some time offered a free coffee machine in some boarding areas — doesn’t quite qualify for “delight”. Is there a German word for “mild satisfaction following operational disappointment owing to sub-optimal logistical and location planning of premium travel benefits”?\nPerhaps my problem is that the offering reminds me of a watered-down and less impressive version of the Air New Zealand regional lounge space on the turboprop side of Christchurch Airport. There’s a full lounge in the jet section of the terminal, but with minimal service on its turboprops, Air NZ offers its lounge-eligible travellers the chance of a hot bite to eat and a coffee from a semi-private, swipe-to-enter space. It’s actually just a bit of cleverly fenced off terminal area that backs on to a café, whose staff and supplies furnish the lounge offering. Would that be just as “go” but even more “delight”?\n- Five steps airlines should take to improve outstation #PaxEx\n- Passengers wake up, smell the bacon with Ryanair pre-order breakfast\n- Munich Airport’s showpiece new satellite terminal opens\n- Farm-to-terminal trend continues with OTG’s new Newark arrival\n- To Buy. To Serve: British Airways goes full Ryanair on shorthaul food\n- Lufthansa Eurobusiness review: three steps forward, one step back\n- Why crew will determine success of Lufthansa Restaurant Service\n- VIDEO: Lufthansa brings Taste of America aboard\n- Inflight Taste Test: Lufthansa’s new Taste of America menu', 'Pilot Program Reduces To-Go Packaging Waste by 73 percent\nAre single use to-go containers always necessary while eating at a food court?\nTo test out a new “old” way of serving food, the Portland International Airport Waste Management Team offered reusable, green plates at food carts from April to June 2017 in the airport. The pilot project gave diners a chance to use silverware and eat off real plates. It turns out people preferred it.\nAbout half of customers chose the sustainable option. As a result, the Green Plate and Port of Portland Headquarters Box pilot program reduced to-go packaging waste from PDX food carts by a remarkable 73 percent. It also enabled composting of 374 pounds of food waste that otherwise would have gone to the landfill.\nReduce First, Then Recycle\nInsiders know – recycling alone won’t solve the waste problem. Changing demands for collected recyclables in domestic and international markets makes it even more difficult. Given challenges, many are rethinking their approach beyond recycling. Innovative solutions lie upstream, in the “reduce” component of the equation.\nReducing waste at airports has always been tough. Travelers pass through the terminal quickly and often a more sustainable option is not available. Disposables are easy. So, when the waste team looked at new opportunities, they focused on reducing the single use, disposable to-go container.\nThe waste team has always depended on data and audits to provide airport tenants strategic feedback. So naturally, they began to observe and ask the question – where will a reusable alternative work?\nLocation, Location, Location\nThe project took place in the pre-security food cart area where it was observed that nearly 80 percent of customers enjoyed their food sitting down and almost all of those taking food to-go were airport employees. Another ideal feature was the fact that the food carts shared a common kitchen located near a janitorial dishwashing room.\nDurable Dishes, Signs and Staff Make a Great System\nThe Green Plate and Port Headquarters Box pilot program included three food cart restaurants and provided durable dishes for customers dining-in and durable to-go boxes for Port employees. Signage informed customers of a common collection area to return dishes. Janitorial staff were responsible for collecting, washing, sanitizing and returning the dishes to the kitchen as well as composting food waste.\nPilot Program Results\n- 73 percent reduction in to-go packaging waste.\n- 374 pounds of food waste collected and composted.\n- 10,000 dishes reused.\nPartner for Success\nThe Waste Management Team consulted food carts, janitorial staff, and airport operations during program design, and customers were surveyed for feedback. Given positive accolades, PDX is considering program expansion to include more employees and restaurants. Oregon Department of Environmental Quality funded the pilot project through a Materials Management Grant; they will be sharing the project as a case study.\nFor more Information, contact: firstname.lastname@example.org\nWant to reduce waste in your food court? 6 tips to begin a durable dish program:\n- OBSERVE: Look for a location where people sit down to enjoy their food rather than carry it away. We noticed that 77 percent of PDX food cart customers enjoyed their meal sitting down in a pre-security food court, providing an opportunity for a reusable alternative.\n- ENGAGE STAKEHOLDERS: When designing the program be sure to include all people who will be involved in the new system, getting their feedback and perspective early will ensure a smooth launch and increase program support.\n- SET UP A SYSTEM AND TEST: Try out the new system on a limited basis to allow time to work out the obstacles. To test the program, we offered reusable green plates, utilized a nearby dishwashing room, posted signage and engaged staff to collect, wash and return dishes to the food cart kitchen. We found that janitorial staffing levels needed to be adjusted to customer volumes and there was capacity to include more restaurants.\n- PROVIDE CUSTOMER CHOICE: While we offered durable plates, customers still had the choice to order their food “to-go.” To encourage durables, signs shared that to-go packaging was always available if needed. Forty-five percent of PDX Food Cart orders were served on a Green Plate.\n- COMPOST EXTRAS: The new system allowed for new opportunities to minimize waste. While composting wasn’t appropriate for the food court, it was easily managed in the commercial dishwashing room. By adding a dishwashing position, we could collect any food left on plates by customers amounting to 374 pounds.\n- MONITOR AND MEASURE: To understand program impact, conduct pre-and post-waste audits and tracking how many dishes were used. We also surveyed participants with comment cards and interviews to gather their opinions. One-hundred percent of customers indicated they enjoyed eating on a real plate.']	['<urn:uuid:f359207b-95e2-45bb-a147-c2ad1ccb40d4>', '<urn:uuid:ee8795e1-afde-4a62-8781-524c131d3c08>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T05:06:54.527160	21	118	1597
3	real time impurity monitoring challenges continuous manufacturing	Real-time monitoring of product and process-related impurities in continuous manufacturing faces several key challenges. The main issues include limited range of appropriate analytical sensors for continuous monitoring, sensor drift over operating time, ongoing calibration needs, and system suitability testing requirements. Additionally, the turnaround time of typical assays is problematic, and there's a need to increase testing throughput, decrease reliance on heterogeneous reagents, and develop simplified methods or robust systems for complex analyses. Current sensor technology needs further development to become sufficiently diverse, robust, accurate, reliable, and rapid.	"['OR WAIT null SECS\nReal-time monitoring of product- and process-related impurities remains a challenge.\nContinuous bioprocessing provides numerous benefits, from high product quality and consistency to smaller physical footprints, increased flexibility, and potentially lower capital and operating expenses. It also provides the greatest benefits if real-time monitoring of process parameters and product quality, including detection and quantitation of residual impurities, is performed. While advances in analytical methods and automation are occurring, practical implementation of real-time monitoring of product- and process-related impurities has yet to be achieved.\n“In an ideal world,” asserts Byron Kneller, senior director for analytical and formulation development with AGC Biologics, “we would have simple, rapid, physicochemical tests for residual impurities.”\nUnfortunately, there are different types of residual impurities. They can be classified as product- and process-related, and as chemical impurities (typically small-molecule process additives such as isopropyl β-D-1-thiogalactopyranoside, antifoams, antibiotics, etc.) or biological impurities from the host cell or processing steps (host-cell proteins [HCP], host-cell DNA, residual Protein-A, etc.).\n“Technologies that utilize mass spectrometry (MS) capabilities are, at this point, the seemingly best choice for residual impurity detection, characterization, and/or quantitation when coupled with liquid chromatography (LC) systems,” observes Amit Katiyar, director of analytical and formulation sciences for Thermo Fisher Scientific. Multi-attribute methods (MAM) capable of detecting, quantifying, and characterizing multiple product quality attributes are currently being developed.\nA triple-quadrupole system would be best suited for quantitative analysis of small molecules/peptides, with quadrupole time-of-flight systems more appropriate for the characterization and semi-quantitative analysis of larger molecules, according to Charles Heise, senior staff scientist for bioprocess strategy and development at Fujifilm Diosynth Biotechnologies.\nAn amalgamation of both technologies for a single quantitative measurement covering a wide mass range would be an ultimate solution, he adds. A series of standards could be used to generate standard curves for target impurities and the software needed for identification and quantification of peaks. Aspects of high-throughput proteomics analysis may also be applicable to the continuous manufacturing environment.\nOther analytical techniques Heise highlights include in-line spectroscopic measurements such as Raman, Fourier-transform infrared (FTIR), ultraviolet/visible (UV/Vis), and refractive index methods that can give quantitative as well as qualitative, real-time analysis. “The particular technology used will depend on the impurity, its expected concentration range, and whether monitoring is performed during steady-state conditions,” he observes. New methods or advances in current technologies may also become available that allow in-line measurements using nuclear magnetic resonance (NMR) imaging, sensor arrays, or LC systems.\nThe final option, Heise says, is to do no process monitoring and rely solely on the quality-by-design data generated during process development to define the operating ranges and edges of failure.\n“In practice, the choice of method may be constrained by the availability of the instrumentation, turn-around time, analyst training, and potential interference from the complex matrices that may be involved in the downstream purification,” comments Ian Parsons, director of analytical development for biologics at Charles River Laboratories. His company has focused primarily on the use of high-performance LC (HPLC) and the enzyme-linked immunosorbent assay (ELISA), with MS and gas chromatography (GC) also used frequently.\n“The pharmaceutical industry is still heavily utilizing traditional separation techniques such as size-exclusion, ion-exchange, and reversed-phase chromatography in addition to immunoassays such as ELISA and electrophoresis as the primary means for quantifying product- and process-related impurities,” notes Katiyar.\nOften tests for process-related impurities require specialized reagents such as anti-HCP antibodies or natural products like limulus amebocyte lysate and can be more complex, adds Kneller. In addition, many impurities are heterogeneous (e.g., HCP) and thus not amenable to detection and quantitation using simple tests.\nIn practice for continuous processes, in-line confounded (e.g., spectroscopy) or secondary parameter (e.g., off-gas, pH, basic physical property) measurements are most user friendly, but accuracy of residual impurity detection is sacrificed for real-time control, as a result of the heterogeneous nature of the signal, or to allow for detection of a secondary response, according to Heise.\nHe also notes that off-line LC–MS, where mass windows can be limited by buffer solution and scanning rates, or LC–evaporative light scattering detection (ELSD), is typically used for small molecules, but these are not sensitive enough and can take too long to run for large-molecule manufacture.\nThese approaches could potentially be substituted for sensor technologies with equivalent modalities, according to Heise, such as the Octet system from Fortebio, which relies on biolayer interferometry. Similarly, he says that ELISAs for sensitive quantitation of biological impurities that have slow response times (material quarantining needed) could also be replaced with equivalent ‘dip-and-read’ methods.\nThe new technologies (e.g., MAM, MS for HCP analysis, automation for DNA, HCP, and Protein A residuals) are just starting to be introduced in the process development space, according to Katiyar. “The transition from exploratory to fully validated methods will be slower for such methods in a cGMP environment where method performance, equipment validation, and data-integrity measures need to be at the highest level. There will also be a significant cost aspect of not only reconfiguring current laboratories to accommodate the equipment, but also for training and/or hiring new personnel with expertise in these new technologies,” he says.\nAt the process level, identifying the residual impurities present and the appropriate analytical assays, with the required detection limits and sensitivity ranges, to monitor them across the process is critical, stresses Dan Pettit, senior staff scientist for analytical development at Fujifilm Diosynth Biotechnologies. At the operations level, he notes that identifying suitably equipped labs to develop and validate methods for GMP analysis is vital because of the heterogeneity of process impurities and the consequent analytical issues they pose.\nHeterogeneous process-related impurities, specifically HCPs, are often quantified in early-phase projects using generic kits that may result in key impurities going undetected during purification process development. “These impurities have the potential to cause problems at production scale,” Kneller says. Current ELISA techniques lack the ability to identify specific immunogenic proteins, thereby making the process development aspect less strategic in design, agrees Michael Farris, scientific manager of analytical and formulation sciences with Thermo Fisher Scientific. “Building toxicology and immunogenicity databases for various HCPs and other process impurities would help fine-tune process development and, in itself, provide a deeper insight into process performance and robustness,” he asserts.\nIdentifying/developing suitably sensitive methods is also challenging, according to Pettit. “Typically, ppm or ppb levels of analyte in a complex mixture must be detected, and therefore, isolation/derivatization of the analyte may be required. These activities tend to be prohibitively expensive to outsource,” he observes.\nIn addition, isolation of impurities prior to quantitation may introduce artifacts or cause the generation of various altered states during the isolation process. For example, physical manipulation of samples prior to the detection of multiple analytes may alter the HCP antigen profile and relative abundance of the analytes themselves, according to Farris. Additional studies to understand the stability of isolated product(s) may therefore also be needed.\nOther process impurities such as Long R3 IGF-1, which is a component of some cell-culture processes, have a tendency to adhere to certain plastics, thereby making accurate quantitation more difficult, Farris adds. Sampling instructions must be clearly defined so as to not artificially deplete the analyte during sampling and/or storage prior to analysis.\nParsons agrees that the main challenges are focused around developing methods of sufficient sensitivity that also minimize product and matrix-related interference, and thereby accomplish sufficient recovery of the analytes. In numerous instances, Farris points out that the quantitation and/or characterization of impurities is hindered by the API and/or the matrix composition.\n“Immunoassays, such as ELISA, used for quantitation of host-cell proteins and leachables like Protein A are sensitive to extreme pH, high salt concentrations, and certain detergents. The typical means for overcoming the signal suppression or interference associated with their presence is to dilute the sample, which acts to decrease the sensitivity of the method to maintain acceptable precision and accuracy,” Farris explains. As a consequence, sensitivity constraints imposed by matrix/API interference must be a point of consideration when demonstrating that a method is fit for use for process characterization and/or process validation activities.\nFor products with small total batch volumes, the quantities of material required for analytical method development and sample analysis can also present a challenge, Parsons notes. “If method sensitivity is also an issue, an approach can be taken to spike an aliquot of upstream material with a higher known concentration of analyte and then demonstrate fold-removal of the analyte across the downstream purification step(s),” he comments.\nFor continuous processes, the turnaround time of typical assays is a key issue, according to Kneller. “The challenge is to increase our testing throughput, decrease our reliance on heterogeneous reagents, and to find ways to either simplify the method types used or engineer robust, easy-to-use systems for more complicated analyses (e.g., mass spectrometry),” he says.\nReal-time monitoring of representative material for modeling the residence time distribution in continuous processes is also required for process control, according to Pettit. “The range of appropriate analytical sensors for continuous monitoring of product specific critical quality attributes and impurities is limited. Additionally, sensor drift over the operating time, any on-going sensor calibration needs, and system suitability testing for quality critical attribute monitoring during continuous operation must be resolved,” he adds. “Here again, sensor technology could be the way to go (e.g., online Octet dip-and-read-type systems); however, the technology is not sufficiently developed yet. The final technology offerings need to be diverse, robust, accurate, reliable, and rapid,” he concludes.\nFor off-line analysis of residuals from a continuous process, Parsons notes that methods for aliquot/sample withdrawal are needed. In addition, the sampling frequency should be sufficient to provide a statistical sampling of the material flowing through the process in order to account for any transient variation that may occur while avoiding significant depletion of the process flow.\nThe analytical testing strategy associated with continuous processes can, however, be more demanding in terms of the number of sampling time points if in-line or on-line monitoring is not possible, according to Farris. “Process analytical technologies have generally been geared more toward product quality, cell-culture viability, and growth than active monitoring of process impurities. If a deeper understanding of any particular impurity is required, more complicated assays will likely be required,” he observes.\nWhen moving from batch to continuous processing, it is likely that the same analytical methods for residual impurities can be used, but potentially in higher throughput versions. Offline analysis using LC, GC, ELISA, and MS should be possible as long as an appropriate sampling approach can be implemented, Parsons observes.\nThe throughput capability and time-to-result for the analytical method employed are also important aspects if at some point the impurity is deemed process critical, according to Darshini Shah, senior scientist and group lead for downstream process development at Thermo Fisher Scientific. Automation of methods (e.g., liquid-handling or use of systems such as the Octet for HCP analysis) can provide higher throughput both in terms of overall speed and analyst effort, according to Kneller.\nThe analytical target profile must also be evaluated to ensure methods are capable of monitoring fluctuating analyte levels throughout continuous processing. “Retention rates or biomass removal may result in periods of correspondingly lower impurity levels, and the analytical method must be sensitive enough and support a large enough dynamic range to be able to actively monitor the process range without the need to constantly repeat analyses to better target the operating range of the assay,” Shah explains.\nIn addition, the typical approach in batch processing involves analysis of a small discrete sample number, with reference standards bracketing the samples and system suitability tests performed prior to analysis, according to Heise. Rotating through multiple identical analyzers would allow continuous monitoring, but the solution would be costly. “Validation of both system suitability tests and the analytical methods for continuous control/monitoring will need additional work to demonstrate that results do not require bracketing with reference standards and that the detection methods do not drift or get poisoned over time when operating for 90+ days,” he says. Failure mode mitigation strategies will also need to be developed, for example with respect to system suitability failure.\nIn addition, Parsons points out that for in-line and on-line analysis, the inherent requirements of sensitivity and specificity for analysis of process residuals suggests that many of the current and potential online methods would have limitations and may perhaps be able to best provide supportive general information, rather than specific quantitation of residuals. “Offline analysis using traditional validateable techniques might still be required for specific and sensitive quantitation of the process residuals. Nevertheless, if a continuous process can be shown (perhaps by an online monitoring method) to be robust, then reduced sampling of the continuous process for quantitation of residuals may be justifiable,” he notes.\nIn fact, because continuous processes are expected to operate under steady-state conditions, control through predictive modeling or by exception should be possible, Heise adds. “Multi-variant analysis during development may identify simple in-line monitoring techniques that can control the process to ensure residual impurities do not pass through the whole process. However, the dynamics of the process will be different at the outset until the steady state is reached. A testing strategy defining the frequency of measurement pre- and post-steady state will be required to identify when process equilibrium has been reached and when it begins to fail,” he observes.\n“Ideally,” Heise continues, “we don’t really want to be testing anything real-time and on-line. Instead, we want to have confidence that the process will deliver consistently over a defined time scale.”\nSignificant development is still needed before on-line/in-line analysis of residual impurities will be widely implemented. Shah is not aware of any on-line/in-line tools capable of monitoring residual impurities for either quantitation or characterization. “Offline analysis is the predominant means of analysis and the only on-line/in-line tools are related to product quality aspects,” he says.\nCurrent solutions exist around automated sampling for at-line analytics, where process changes occur over a longer time span than the analysis time, such as in the case of mammalian bioreactor metabolite analysis, according to Pettit.\nOn-line process analytical techniques such as near-infrared spectroscopy (NIR) or mid-infrared spectroscopy (mid-IR) and UV may provide some general information on clearance of residuals but are unlikely to be specific or sensitive enough to enable detailed quantitation of analytes, according to Parsons.\nParsons does note that low-field NMR is a potentially promising technique for on-line analysis of process residuals. “The system is simple to operate, potentially applicable to both batch and continuous processes, and could be performed by stop-flow analysis with direct connection of the flow path to the downstream purification process,” he says. Attractive features of NMR in this regard are its inherently quantitative response factor and its relatively high resolving power, and hence higher specificity than a technique such as UV, while drawbacks include barriers to implementation of a conceptually complex analytical technique and limitations on sensitivity.\nVol. 32, No. 7\nWhen referring to this article, please cite it as C. Challener, “Analysis of Residual Impurities in Continuous Manufacturing,"" BioPharm International 32 (7) 2019.']"	['<urn:uuid:4f77253c-863c-4629-a720-5f529738fa22>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T05:06:54.527160	7	87	2501
4	What roles did the spleen and gallbladder serve in humoral theory?	In humoral theory, the spleen and gallbladder served as repositories for specific bodily humors and regulated emotions. The gallbladder stored yellow bile (gall) and affected other body parts through its connections to the heart and brain. The spleen stored black bile, the most noxious substance, and prevented melancholia by containing this fluid. Interestingly, the proper functioning of the spleen was associated with the ability to laugh.	"['A HISTORY OF THE LIVER, SPLEEN, AND GALLBLADDER\n""Now, why is the stomach surrounded by the liver? Is it in order that the liver may warm it and it may in turn warm the food? This is indeed the very reason why it is closely clasped by the lobes of the liver, as if by fingers."" -- Galen, ca. 200 A.D.\nMedical practitioners in antiquity identified the liver as one of the three principal organs of the body, along with the heart and the brain. They differed quite dramatically, however, on the relative importance of each. It was the Roman anatomist Galen who made the liver the principal organ of the human body, arguing that it emerged first of all the organs in the formation of a fetus. ""The liver is the source of the veins and the principal instrument of sanguification,"" he observed in On the Usefulness of the Parts of the Body. For Galen, it was the liver rather than the heart where blood was most actively formed; it was a warm, moist organ. If all veins exchanged fluids through the liver, connecting only tenuously to the heart in order to provide a tiny amount of blood to mix with spirit in the arteries, then the liver was the center of the circulation of material substances in the body.\nGalen identified the gall bladder and spleen as the two crucial subsidiary organs of the liver. All three organs worked together to produce and store three of the four humors of the body: blood (liver), yellow bile (gall bladder) and black bile (spleen). In essence, three-quarters of the substances which established the balance of a healthy body could be found in this region. Both the gall bladder and spleen prevented the blood formed in the liver from being contaminated by denser and more toxic fluids. Galen observed that the ""spleen serves to purify the liver"" and that the ""passages from the gall bladder have evidently been formed for separating the bile."" Puzzling over the location of the spleen, Galen hypothesized that the large size of the stomach and the necessary proximity of the bladder to the liver ""to receive the thin, yellow residue"" had led to its unusual distance from the liver since, logically, nature ""would have greatly preferred to place the spleen, which attracts the thick, muddy residue right below the port where the atrabilious residue, borne down by its own weight, would automatically sink into it."" The medieval illustration to the left suggests the way in which anatomical illustrations helped to depict the fit of different organs to each other. Can you see Galen\'s ideas in the drawing?\nIn the Middle Ages, the strength of the Galenic model of the body is amply apparent in descriptions of all three organs. Calling the liver ""the seat of the nutritive or vegetative faculties,"" the Islamic medical philosopher Avicenna observed, ""Physicians regard the liver as the seat of manufacture of the dense part of the humors."" He further specified the liver\'s uniquely heavy and moist condition as a function of the fact that the organ itself was nothing more than a dense concentration of blood rather than being made of actual tissue. He also noted that the primary function of uroscopy -- reading urine -- was to understand the health of the liver. Thus by the early eleventh century, medical practitioners had consolidated their views of the importance of the liver into an elaborate regime of diagnosis and therapy.\nDespite these strong statements about the centrality of the liver, Avicenna indicated his sympathies to the Greek philosopher Aristotle\'s view of the body when he observed: ""All agree that the brain and the liver each receive their power of life, natural heat, and breath from the heart, and that each of them is also the starting-point of another faculty which it sends out to the other agents."" The fundamentally different accounts of the body offered by Galen and Aristotle proved to be an endless source of confusion for their medieval and Renaissance readers. Other nagging questions remained in regards to what exactly the liver generated of the humors. In the late twelfth century, for example, Master Nicolaus expressed his uncertainty on this question when he wrote: ""Some say that the blood alone is generated, in form as well as substance, in the liver, the other humours are generated there in substance but in other places as to form."" Yet he was nonetheless quite sure of its overall purpose, as ""the spiritual instrument of the second digestion."" Look at the late medieval physiology to the right. How have the illustrators dealt with these problems?\nYet another problem regarded the number of lobes of the liver. Generally ancient and medieval anatomists found five (the number in a dog). Renaissance anatomists were less sure about this number because increased dissection of human cadavers had suggested alternatives. ""It has five lobes, sometimes four and three, sometimes two,"" wrote Jacopo Berengario da Carpi at the end of the fifteenth century. A few decades later, anatomists were even more skeptical of the five-lobed human liver. It is very rarely divided into five lobes; more frequently into four most frequently into three,"" wrote Andres de Laguna in 1535. The number that we count today -- two -- seems to have been quite difficult for them to see. Look at the medieval image of the liver to your left. Then look at a Renaissance image of the liver to your right. What differences do you observe?\nWhile the liver may have been the most importance organ to observe and describe in this sector of the body, the gall bladder and spleen also provided interesting material to contemplate. Humoral theory suggested that each had a profound effect on the emotions. The gall bladder regulated the emotions by being a repository for gall. ""Its substance is slender,"" wrote Berengario, ""because it does not digest anything and hard so that it may resist the sharpness of the gall."" Attached to both the heart and the brain, it was a truly sensitive organ that could affect other parts of the body. Similarly, the spleen, repository of the most noxious substance of the body -- black bile -- prevented the onset of melancholia by containing the bodily fluid that producted this mental state. ""Similarly [according to] other physicians the spleen [is] the receptacle of melancholy as the gall bladder of gall; wherefore the spleen causes one to laugh,"" wrote William Harvey in his Lectures on the Whole of Anatomy (1653). The ability to laugh was a sign that the spleen was working well.\nDespite any lack of knowledge of the lymphatic system, which was not identified until the middle of the seventeenth century, Renaissance physicians nonetheless understood that the spleen played an important role in fighting off disease. ""It helps the entire body by purging the bloody mass from the feces ... and suffers all manner of illnesses,"" wrote Berengario. In this instance, a humoral account of the body did anticipate what further anatomical investigation would reveal.\nRenaissance accounts of all three organs are filled with tiny inconsistencies that reflect the growing amount of information that came as the result of repeated dissections. While some physicians continued to follow the teachings of the ancients, others began to privilege evidence obtained directly from the human body in their practice of medicine. Harvey still spoke of the liver as a ""noble organ"" in 1653 and the spleen as an ""ignoble organ."" But he also went to great pains to offer a physical description of each organ. He described the gall bladder as ""a very long pear compressed from base into neck"" and the spleen as ""like the tongue of an ox [or] the sole of the foot; slightly bowed out on the left side, a little concave on the inner side, toward the stomach. It has an uneven surface and is a little rough with some tubercles."" Even as many aspects of physiology remained uncertain, the anatomy became more precise. Leonardo\'s depiction of the liver, for instance, is a good example of the changes that observation gradually made in accounts of the body, as well as the difficulties of overcoming what one already knew to describe what there was to see.\nBetween the 1640s and 1660s, increased numbers of animal vivisections allowed anatomists to look more closely at the relationship between the liver and the newly discovered lymphatic system. Their research suggested that the all-important chyle, which Galen had describing as being converted into blood after it flowed in the liver, took a different pathway through the body. Johannes van Horn wrote in 1652: ""I categorically deny that even one lactic vessel goes to the liver."" The great Danish anatomist and discoverer of the lymphatic system, Thomas Bartholin, was initially more tentative. That same year he wrote, ""I do not have the courage to doubt the liver which has so long been considered essential."" Yet in his Lymphatic Vessels (1653), he confidently reversed this view, declaring the end of the liver\'s role as ""ruler of the abdomen"" and the death of the ""sanguine empire."" Bartholin ultimately decided that the role of the liver was to form bile rather than blood. In 1654, the English physician Thomas Glisson was the first scholar to publish a book devoted exclusively to the Anatomy of the Liver. He continued to wonder about the connections between the liver and the glands.\nQUESTIONS: WHY WAS THE LIVER SUCH AN IMPORTANT ORGAN? WHY DID PHYSICIANS OFFER SO MANY DIFFERENT DESCRIPTIONS OF THESE THREE ORGANS, BUT ESPECIALLY THE LIVER?\nReturn to History of the Body Home Page\nSome Additional Readings']"	['<urn:uuid:6f4498a5-a9c4-473d-8189-cc0d5f14a530>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:06:54.527160	11	66	1606
5	How can researchers get a glimpse of which proteins are being made in a cell at any given moment?	Ribosome profiling, also known as Ribo-Seq, provides a snapshot of all the ribosomes active in a cell at a specific time point. This information can help researchers determine which proteins are being actively translated in a cell.	['What are the three main steps of transcription in prokaryotes?\nTranscription takes place in three steps: initiation, elongation, and termination. The steps are illustrated in Figure 2.\nHow does Ribo seq work?\nRibosome profiling, also known as Ribo-Seq (ribosome sequencing) or ART-Seq (active mRNA translation sequencing), provides a “snapshot” of all the ribosomes active in a cell at a specific time point. This information can help researchers determine which proteins are being actively translated in a cell.\nWhat is canonical translation?\nDuring canonical translation, the ribosome moves along an mRNA from the start to the stop codon in exact steps of one codon at a time. However, dedicated recoding signals in the mRNA can reprogram the ribosome to read the message in alternative ways.\nWhat is non canonical translation?\nTranslation of cellular mRNAs normally initiates on an AUG start codon, but non-AUG initiation can occur to regulate translation of a subset of mRNAs, for example by establishing an alternative open reading frame.\nWhat are the 4 stages of transcription?\nStages of transcription\n- Initiation. RNA polymerase binds to a sequence of DNA called the promoter, found near the beginning of a gene.\n- Elongation. One strand of DNA, the template strand, acts as a template for RNA polymerase.\n- Termination. Sequences called terminators signal that the RNA transcript is complete.\nWhat are the 3 main steps of transcription?\nTranscription of a gene takes place in three stages: initiation, elongation, and termination.\nHow is translation regulation in prokaryotes?\nTranslation in prokaryotes is usually regulated by blocking access to the initiation site. This is accomplished via base-paired structures (within the mRNA itself, or between the mRNA and a small trans-acting RNA) or via mRNA-binding proteins.\nWhat are the steps of transcription in prokaryotes?\nIn prokaryotic organisms transcription occurs in three phases known as initiation, elongation and termination.\nWhat is the process of transcription in prokaryotes?\nProkaryotic transcription also known as bacterial transcription is the process in which a segment of bacterial DNA is copied into a newly synthesized strand of messenger RNA (mRNA) which is later translated to produce proteins with the use of the enzyme RNA polymerase and other transcription factors.\nWhat is the difference between ribosome profiling and polysome profiling?\nOn the other hand, ribosome profiling captures positional information of ribosome footprints at the subcodon level while polysome profiling does not, and is therefore more suitable for investigating alternative start codons or open reading frames .\nWhat does ATAC seq measure?\nWhat is ATAC-Seq? The assay for transposase-accessible chromatin with sequencing (ATAC-Seq) is a popular method for determining chromatin accessibility across the genome. By sequencing regions of open chromatin, ATAC-Seq can help you uncover how chromatin packaging and other factors affect gene expression.\nWhat is a canonical mutation?\nThe canonical Wnt receptor signaling pathway is a series of molecular events that are initiated by the binding of Wnt proteins to the frizzled family of receptors on the cell surface. This ultimately activates transcription factors and results in changes to the expression of target genes.\nWhat is transcription in prokaryotes?\nTranscription in Prokaryotes. The process of synthesis of RNA by copying the template strand of DNA is called transcription. During replication entire genome is copied but in transcription only the selected portion of genome is copied.\nWhat is the process of transcription?\nThis is a process-a means to an end and to my mind very necessary. I have a DVD titled “The Improviser’s Guide to Transcription” (Caris Music Services) which describes the process in detail with actual demonstrations. Transcribing involves a three part learning process: body, mind and spirit-in that order.\nWhat is a transcription bubble in prokaryotes?\nTranscription in prokaryotes (and in eukaryotes) requires the DNA double helix to partially unwind in the region of mRNA synthesis. The region of unwinding is called a transcription bubble.\nWhat is the role of Tau in transcription termination in prokaryotes?\nSeveral factors like Rho, Tau, and NusA are important in transcription termination in prokaryotic transcription. In roughly half of the cases, a ring shaped protein called Rho (ρ) assists for termination. RNA Polymerase reaches a Rho-utilization (rut) site and transcribes this sequence.']	['<urn:uuid:aa906962-169d-480e-a8c1-84ae98d795af>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:06:54.527160	19	37	689
6	What kind of specialized equipment would you recommend for creating proper three-dimensional illumination when shooting extremely close-up photographs?	For macro photography, ring flash and twin flashes are the most suitable lighting equipment for creating three-dimensional lighting around the object.	['Clever Macro Photography Tips and Tricks\nAmong the popular photographic styles, there are a large number of photographers who are interested in macro photography. Thanks to modern cameras and advanced lenses that a lot of photographers are finding it suitable. But it is also a field where lot beginners struggle to take good shots.\nWe are trying to help all such people with the help of this article. Here, we will mention several macro photography tips. Let us have a look at them and see which one works for you the best.\nBe careful about choosing the subject\nYou will never get good photos if you don’t have the right subject despite having the best gear. And if you think everything looks great in the macro mode, you are wrong. When you start shooting the photos of such things, you will find that most of those photos are non-perceivable when viewed from close. No matter whether you will get the right shot from that or not but you will learn a lot about choosing the right subject.\nIf you are an absolute beginner, starting with small insects, raindrops, water drops, jewellery, other household items would be a great idea. And the biggest challenge that you will face is during shooting the photos of insects. In that case, you should maintain a proper distance so that you avoid scaring them off.\nChoose the right lens\nThis is the first and most important among the macro photography tips to consider here. Though the majority of cameras offer macro photography mode but that is not enough in all cases. You can never get 1:1 magnification with that in-built mode. For such a magnification, you have to buy a macro lens. There are plenty of macro lenses available in the market and that is the root of all confusion.\nIf you want to shoot objects with more flat surfaces, you should use a flat-field macro lens. These lenses will provide you the best edge-to-edge sharpness. Most macro lenses are very expensive but that investment is worth making if you want good quality photos.\nChoosing the right lens depends a lot on the type of subject that you have chosen. Usually, going for the macro lenses having longer focal length are the best options. When you choose a lens of that kind, you can digitally move very close to the object. This facility helps you in getting the perfect shot you don’t disturb the subject and the environment.\nBe ready to use the assistive accessories\nHaving a macro lens is not enough to get the best photo. There are a lot of assistive tools in the market. In this section, we are going to talk about those accessories. All of these accessories can be very easily incorporated into the macro kit.\nYou can use Bellows or tubes as they are very helpful in producing some ultra-tight close-ups. Lens adapters are also very helpful in using the same lens of your camera for different purposes.\nClever macro photography of a fly\nUse customised backgrounds\nIf you are a beginner, you will find that your photos don’t look that cool as the ones on the internet. There is a reason for that and it is the background. The background of a photograph impacts the photo equally as the subject. When you are shooting macro photos, the background becomes even more important.\nIn macro photography, the same subject looks different in different backgrounds. While choosing the background, make sure that the colour and composition of the subject don’t collide with that of the background. Always put your subject against a contrasting background. In that case, the subject is more visible and appealing. Such photographs offer the best background blur.\nIt may be a bit challenging when you are shooting macro photos outdoors. You will not have much control over the background in that case. But you are free to adjust the background by changing the perspective.\nDepth of field plays a very important role\nDepth of field is one of the most crucial things in photography. But when it comes to macro photography, depth of field plays an even more important role. You can do it using the smaller apertures. Small apertures are very helpful in increasing the depth of field. But there are several complications involved in deciding the depth of field in macro photography.\nSmall apertures are recommended very much for this purpose. But when the aperture is too small, you can’t get sharp photos. In such a case, the depth of field is less and as a result of that, some of your subject will be blurred. This is not a good thing especially in the case of macro photography.\nIf you decrease the magnification and then use a small aperture, you will get some nice sharp photos. And after that, you can crop the image to get the desired aperture. But these macro photography tips is only for those who don’t mind cropping the photo.\nAnd many cameras of this age come with a feature called focus stacking. You can use it to balance the depth of field and sharpness of the subject.\nAlways try to create good lighting\nLight plays the most important role in photography. Most of the things and technology that we use in photography are to get the light in proper balance. And when it comes to macro photography, it becomes even more important.\nYou should enhance the lighting situation not only to get better photos but also to support the exposure settings of your camera.\nThe best lighting equipment that you can use for this purpose is ring flash and twin flashes. Both these lights are suitable for creating three-dimensional lighting around the object.\nThere are several macro photography tips that we have mentioned in this photograph. But choosing the right tip is not so easy task and these are not the only tips available for this purpose. You have to keep on experimenting with different ideas and learn.']	['<urn:uuid:30fccc7e-507e-4972-be83-f0e89965d32f>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	18	21	1000
7	I want to start growing my own plants from seeds. What should I do to prepare them for outdoor planting?	To prepare seedlings for outdoor planting, you'll need several steps. First, start them in appropriate containers - you can use plug trays, recycled containers with drainage holes, or soil blocks. Fill containers with lightweight potting mix containing peat and vermiculite/perlite. When seedlings outgrow their initial containers, transplant them to larger 3-4 inch deep containers filled with quality potting soil. Water carefully and provide proper light. About 10 days before outdoor planting, begin 'hardening off' - gradually expose plants to outdoor conditions, starting with a few hours in a protected spot with filtered light, then slowly increase time outdoors and sun exposure. This helps plants develop defenses against wind, temperature changes, and intense sunlight that could otherwise damage tender seedlings.	['To my mind, any vegetable that can be raised from transplant probably should be, especially if your garden is small or your season short. Doing so will greatly improve the productivity of any garden because it allows successions and interplantings to be more closely planned. As soon as one crop is harvested, you’ve got another set of plants ready to make use of the space. Why give each broccoli plant a foot and a half of space when it’s young and only needs a couple of inches?\nNot all vegetables transplant well, though. Root crops like carrots, leafy biennial herbs like dill and fennel, and the heading types of Chinese cabbages are all likely to run to seed in response to the stress of transplanting, instead of yielding a crop. Still others grow so quickly when sown directly in the garden that transplanting isn’t really worth the extra effort. Greens like spinach, mustard, arugula, and cress are good examples. In the end, your own garden plan is going to determine which plants you should start indoors.\nI think one of the biggest mistakes gardeners make when growing seedlings is starting too soon. I know gardeners in Los Angeles who harvest tomatoes in January, but north of USDA Zone Six you shouldn’t even think of starting tomatoes until February. When we lived in Vermont, we started ours in two batches in mid-March and again the first of April. If the weather broke early, it was worth it; but if spring was slow, the early ones just got tossed out as they were too big when planting season arrived. That’s why it’s a good idea to make two sowings of seed, a week or two apart. If something happens to one set, you’ve got the backup. With two plantings, a week before and a week after the theoretically ideal planting date, you’re ready either way. You can always give away the extra plants if things work out.\nKeep in mind that all else being equal, you are better off with young, vigorous plants than older, root-bound ones. The best produce comes from plants that grow quickly, without what the pros call “checks” that is shortages of any nutrient, of water, or of temperatures to their liking. If the plants are a little small, all you’ve lost is a week or two in the garden; that is often made up in the good growing days of early summer. Two weeks in March is only worth a couple of days in May, or so we said in the nursery trade.\nCeleriac, leeks, and parsley are among the first plants to be started, a good twelve weeks before their intended transplant date, which is itself two to four weeks before the last frost. Check the USDA Zone Chart for your location and count back from the frost free date. You should probably just wait to direct-seed them in the garden a month before the last frost, and save the seedling space for other crops. Gardeners in the North should also start seedlings of thyme, sage, rosemary, and other perennial herbs at the same time if they want a decent harvest in the first season. This is true whether you grow your plants in a windowsill or a greenhouse.\nTomatoes, peppers, eggplants, and basil need really warm conditions. But once they germinate they grow faster, and so can be started only eight to ten weeks before the frost-free date. If you have a greenhouse, you can cut another week off that, due to the increasingly great difference spring brings to greenhouses versus windowsills. We have grown great tomato transplants in only six weeks; even a month-old plant will make a good sized bush by mid-summer and yield a lot of fruit.\nThere is a third group of vegetables that most gardeners direct-seed, but that can be started indoors if you have a very short season, or simply can’t wait for spring. All the members of the cabbage family do well grown from transplant, as does lettuce. Start them only six to eight weeks before the last frost, though; because if they get too big in the flat they don’t withstand transplanting as well, and are unlikely to yield a first-class harvest. Melons, cucumbers, summer squash, beans, and even corn and peas can be started in plug flats, peat posts, or large soil blocks a mere two to four weeks before the frost-free date. The plants grow so fast and are so succulent that it’s difficult to hold them in a tray longer than that.\nGardeners use all different kinds of containers to start seedlings: wooden flats, egg boxes, sawed-off milk cartons, recycled garden center six-packs, peat pots, soil blocks, and plug trays. All will work, but whichever you choose remember that the key to growing healthy transplants is consistency: all the plants of a given size and type should get equal treatment, which is difficult using a mishmash of containers in all different sizes and shapes.\nOne of the most common modular systems is peat pots and strips. Many organic gardeners prefer them simply because they want the benefits of individual containers for each plant, but they don’t like to use plastic. Peat containers have the benefit of consistent sizing, but they are messy to work with, and unless you use the strips, their shape makes them top-heavy once the plants have reached a decent size. Once they tip over they are difficult to keep moist as the water runs off instead of sinking in. You can buy peat pots or pellets mounted in special plastic trays, but they are more expensive that way, and if you’re going to use the plastic at all you might as well go with a completely integrated system. And peat itself is not really a renewable resource.\nPerhaps the best system to use, from an organic perspective, is soil blocks formed by compressing a peat-compost mix into cubes of different sizes with a small, hand-held press. These blocks are then placed into special growing trays. All you need for this system is one or more of the presses, or block makers, which cost from $15 to as much as $100 (depending on the size), plus a collection of special three-sided trays. Both are available by mail order. Generally, soil blocks require a bit more practice to make, and skill to maintain, than a plug or peat system, as watering is more critical. Too much water can erode the blocks and make them hard to separate at transplant time; too little water may let them dry out enough so that it is difficult to get them wet again. The time required to make soil blocks also has to be considered.\nWhen we were commercial gardeners we did a detailed analysis of the different systems, and without a doubt the most efficient and least expensive integrated system is plug trays. These are inexpensive plastic inserts for the standard 10 x 20-inch plastic greenhouse tray. They are similar to the six-packs that store-bought transplants are grown in, but are slightly heavier gauge plastic and so, easily reusable. The insert is made up of conical “cells” of different sizes. According to the size of plant that will be grown in them. Commercial growers, with automatic water and fertilizer systems, often start their plants in plug inserts with 288 or even 406 cells per 10 x 20-inch tray. That gives each plant a root mass not much larger than a pencil eraser!\nLow-tech market gardeners and amateurs use trays with 162, 98, 72, 50, or as few as 24 cells per tray. This makes it considerably easier to care for the seedlings, because each has a much larger amount of potting soil to maintain itself. The plant plugs produced by this system are easy to transplant, because the conical shape of the cells directs root growth downward; if set out at the right stage of growth the seedlings take right off once set out in the garden. The trays and inserts are not expensive, and with proper storage in the off-season will last a number of years. The plastic used for these trays is also recyclable, though programs may not be in place everywhere to actually do so.\nWhat you put into the container or block is just as important. Straight compost—what my grandfather used—is fine for open flats, as long as it is fully matured and screened. But the texture of straight compost is too dense for plugs and soil blocks. In plugs it tends to pack down and then become hard to water; in blocks it erodes and soon you have a tray full of compost instead of individual blocks. A mix that is at least half peat will allow the blocks to hold together until the seedling’s roots have a change to spread, binding it into a sturdy and stable package. In plug trays and recycled six-packs the peat’s coarseness helps keep the surface from packing down.\nCommercial potting mixes are available which are mostly peat, and may be a good choice for the beginner, though you should be sure to look for an untreated, unfertilized mix. Also be sure you are buying a seedling mix. What you want will be brown in color, very light by volume, and sprinkled throughout with white or grayish specks—these are vermiculite or perlite (or both), which are added to the mix to improve its aeration and drainage. A loosely packed bag the size of a feed sack or large bag of birdseed should weigh less than ten pounds. What you want to avoid is the small bags of fine-grained, soot-colored potting soil. They are much heavier and usually don’t have the perlite and vermiculite; these mixes have all of the disadvantages of compost, and none of the benefits.\nAn additional benefit of the commercial mixes is that they are, for all intents and purposes, sterile. If your compost was not properly made—that is, the pile did not heat up fully—it may contain weed seeds and disease spores from the plants that were composted. In the incubator-like conditions under which most plants are started, that can be fatal. Of course, you can sterilize your compost by heating it (to 160ºF for four hours) in the oven before mixing it with the peat. Whatever way you choose, remember that the first few days of a seedling’s life make a big difference to its eventual success in the garden, and it is less resistant to problems that it might shrug off outdoors.\nIf you are using a tray system or container to start your seedlings, fill it loosely with potting soil—until it overflows—then scrape off the excess. Don’t pack down the mix, because young plant roots need air. The mix will pack down naturally as it is watered. You should moisten the mix slightly before filling the containers, though, to keep down the dust, and so that the particles will adhere to one another.\nThe process of making soil blocks is a bit more complex, and requires a bit of practice. The mix should be wet enough to mold well in the block-making press; when you squeeze a handful, some water should drip out between the base of your fingers. First, fill a flat-bottomed tray or tub with the mix, then grasp the block maker down around its base and scrape it across the bottom of the tray or tub that contains the mix, forcing the potting soil into it by pressing against the side of the container; it might take a couple of passes to get enough mix into the press. Once the press is full, release the blocks onto a growing tray by pushing down on the plunger. The ideal growing tray for blocks is smooth-bottomed and three-sided, so that the blocks can be moved (if necessary, but try to avoid it) by sliding them out of the tray rather than picking them up. Continue in this fashion until the growing tray is full, set it aside, and start the next tray. Most blockers make a small dimple in the top of the finished block where the seed should be down.\nOutdoors, seeds should be planted three times as deep as they are across, but indoors they need only be half that deep, which means less waiting for them to break ground.Why? Because while depth brings consistent moisture and temperature, seeds also need to be near the surface to get oxygen, and in some cases, light. Indoors you can control temperature, moisture, oxygen, and light, so it’s just a matter of balancing these needs. The first stage of germination sees the seed swell with water, and activate stored enzymes that start digestion of the seed’s food supply. As this process accelerates, oxygen is needed, which is why, although seeds want moist conditions, you shouldn’t drown them. Just soak the medium once after planting, then cover the flat with some sort of moisture barrier until the plants break ground.\nAfter a few hours (or days, depending on the species of plant) the first seedling root emerges tiny feeder roots begin to spread throughout the soil. Soon the plant breaks ground and finds the last vital growth factor it needs: light. If you haven’t added fertilizer to the soil mix, you will need to start both watering and fertilizing. From this point on the seedling becomes dependent on the nutrients its roots can find in the soil, the moisture that makes them available, and the quality of light its leaves receive. Without the right amount of each-in proper proportion to each other and the temperature—no plant will prosper. Optimum temperatures vary from plant to plant, but most vegetables will thrive in temperatures of 65-75ºF. If your house isn’t this warm and you don’t have a greenhouse, you can start your seeds on top of the refrigerator or in the wash room. But keep a close eye on the flats, because the moment they break ground they need all the light they can get.\nSince heat and light fuel plant growth, the relationship between the two is critical to growing healthy, vigorous seedlings. A common mistake among gardeners without a greenhouse is to keep plants at too high a temperature for the amount of light they receive. Not only is the light from a south-facing window more short-lived than it might seem (rarely exceeding eight hours a day), but the glass in house windows screens out some parts of the sunlight that plants need. What often happens is that the gardener tries to compensate for slow growth with more fertilizer and higher temperatures; both of these make the problem worse, since they increase the imbalance between light levels ad the other factors necessary for good growth. The result is limp, leggy seedling that are hard put to cope with outdoor conditions when planting time arrives.\nFertilizer and water also need to be kept in proper proportion. To grow, plants need nutrients, and without enough moisture, they’ll not only be unable to take up those nutrients, they’ll wilt and die. But too much water washes away the nutrients in the tray or pot and the plants will starve. The conventional wisdom holds that you should fertilize every certain number of waterings not every certain number of days. That way the amount of fertilizer is based on the amount of water that the plant has taken up, not some abstract calendar date. One compromise solution is to fertilize every time you water, at one-quarter strength. That way you don’t need to keep track of when you last fertilized, and the plants get an even, constant supply of nutrients.\nKeep an eye out for signs of under- or over-fertilization. Leaves that curl under are a sign of overfeeding; discoloration, though, is usually a sign of underfeeding. If the plant is pale, that is likely a sign of nitrogen deficiency; leaves with purplish undersides indicate a shortage of phosphorus; leaves with bronze edges show a shortage of potassium. Since seaweed and fish fertilizers contain balanced amounts of all these, the solution is just to increase or decrease the strength or frequency of feedings. The mix we’ve used, both for growing bedding plants to sell, and for our own gardens, works out to a tablespoon each of liquid fish fertilizer (or fish emulsion) and liquid seaweed per gallon of water. This is the full-strength formula and should be diluted 4:1 if you fertilize with every watering.\nIt’s not enough to raise vigorous, healthy seedlings if they are so pampered that they can’t survive the sun and wind and rain and the seesaw effect of day and night temperature changes. But frosty nights aren’t the only enemy to tender transplants. Wind can be just as hard on young plants raised in the still air of a greenhouse or windowsill, snapping off brittle stems or flattening them to the ground, where they can fall prey to all kinds of fungi and insects. Even the sun on which they are so dependent can be dangerous; plants grown indoors develop extra photosynthetic cells in the leaves, and a sudden increase in available light can cause leaves to literally overload, then shrivel and fall.\nAll of these problems—changes in temperature, wind, and light—can be solved by hardening off the seedlings. As planting day approaches, help them adjust gradually to outdoor conditions. This will give them a chance to develop their defenses. At first, just move the flats outside for a few hours in the afternoon; then gradually increase the time they spend in the open air, exposed to the sun and wind. Hardening off is one of the best uses of a cold frame: put the plants in the frame after the first few days, and then leave the lid off for longer and longer periods each day until it is no longer needed. This process could last as long as a week, but doesn’t have to if good transplanting weather comes along.', 'Transplanting Seedlings and Getting Them Garden Ready\nSo you got through the first step…you planted your seeds for your spring garden. If you followed the steps in part one of this post, by now your seedlings should be growing and thriving.\nNow it is necessary to transplant them out of their tiny egg carton homes. This is an extremely important step in successfully growing your own seedlings for your garden. The seedlings need nutrients from the soil and small spaces only have so many nutrients! It is time to transplant your plants to a larger container with quality potting soil, so they can grow large and strong enough to survive once planted outside. I have learned (many times the hard way) lots of tips that should help as you transplant your delicate seedlings, so here’s my tried-and-true method. Happy Transplanting!\nStep 1: Thin the seedlings. Gently pull out the longest / leggiest seedlings, leaving 1-2 strong seedlings in each space of the egg carton.\nStep 2: Decide on your new containers. Any recycled pot will do the trick. Look for something at least 3-4 inches deep. I like to reuse yogurt containers, but you can use peat pots, recycled tin cans, etc. Using a hammer and nail, punch up a bunch of holes into the bottom of the container for drainage.\nStep 3: Fill your new containers with moist, quality potting soil about an inch from the top. Use your finger to make a small dent in the soil to fit the single egg carton.\nStep 4: This is usually the most difficult part, but I’ve found that the egg cartons makes this part a lot easier. Separate the egg carton into the individual seedlings. Gently tear away a bit of the bottom of the egg carton, being careful not to disturb the roots of the seedling. Place the entire egg carton into the new container. Cover with soil.\nStep 5: Now that your seedlings are happy in their new home, gently water them in to remove any air pockets in the soil.\nStep 6: Put your seedlings near a sunny window, but give them a few days to recover from their move before placing in direct sunlight. Water as needed, being careful to not over water. I use a small squeeze bottle to get just the right amount of water to the plants.\nStep 7: Hardening Off – a crucial stage! Use a flat from the garden center, a baking tray, or even a boot tray to put all of your seedlings on for easy transport. About ten days before you plan to plant them in the garden, begin the process of hardening them off. You will want to start by bringing them outside on warm, sunny days to get them ready to be outdoors in the garden. In the beginning, place them in a wind-protected spot, in filtered light. Gradually increase the amount of time they spend outdoors and increase the time they spend in direct sunlight each day.']	['<urn:uuid:fc5840e1-592a-4ef0-877c-699134883813>', '<urn:uuid:23f86b60-e6a2-4eec-87ac-2f53b0e7313d>']	open-ended	with-premise	concise-and-natural	distant-from-document	three-doc	novice	2025-05-13T05:06:54.527160	20	119	3495
8	I work in commercial real estate and want to know what types of buildings religious organizations typically start with when expanding to additional locations. What's the most common first step?	Some 50% of multi-campus churches start out utilizing space in a school building for their first remote sites. Schools, along with theaters, are typically low-risk, low-cost alternatives for a church going multi-site.	['Going from a single campus to multiple sites is currently one of the biggest trends going in the worship facilities arena. The numbers are growing, the technology is getting better, and there are a number of resources available to help a church use this method to make it easier for members of the community to be touched by its message.\nThe Big Picture\nOn any given Sunday, some nine million people—about 10% of all churchgoers—attend services in one of the 3,000-plus multi-site churches in North America, according to Jim Tomberlin, former megachurch pastor, pioneer in the multi-site movement, and founder, president and senior strategist of Third Quarter Consulting. Based in Scottsdale, Ariz., the company provides customized multi-site strategies for churches.\nTomberlin advises that going multi-site is not for every church. “It’s not a growth engine, and it is not a turnaround tool for a church in trouble,” says Tomberlin. “But it is a viable tool to help a growing church that wants to reach more people.”\nMultiple sites, he adds, can be viewed as “stepping stones to the greater purpose—reaching more people and having more impact on the community, and having many points of entry for people in the community to be touched and reached by a vibrant church.”\nFrom the Lending Perspective …\nToday’s church lending community recognizes that the multi-site trend is growing—and that from a financial standpoint, multi-site facilities are typically good deals for both lender and borrower.\n“The days of the 4,000- to 5,000-seat sanctuary are far from over, but the trend now is for churches to try to provide big-church services in more intimate environments that create a more meaningful experience of community,” says Scott Rolfs, managing director of the Church and School Financing Division of Ziegler, a Chicago-based financial services firm that specializes in church lending.\nThat kind of experience is difficult to achieve for a parishioner sitting in a sanctuary with 4,000 other people—or that has to drive 40 minutes to get to church in the first place, Rolfs notes. “So why not have two smaller, more intimate venues 15 miles apart, managed at a central location, and still be able to offer the top-shelf ministries of a 4,000-member church?”\nReal estate lenders are typically very supportive of multi-site projects, adds Rolfs.\n“What you are basically doing when you go multi-site is diversifying your collateral base,” he explains. “As a lender, if you get in a situation where you need to foreclose or sell a church facility, the market is such that it is much better to have two, $10 million assets as opposed to one worth $20 million.” Plus, “If your church experiences a downturn in attendance, you can always consolidate back to one location—and perhaps sell off the other location to another church to pay off the debt on your main facility.”\nSome 50% of multi-campus churches start out utilizing space in a school building for their first remote sites, according to Tomberlin. Schools, along with theaters, are typically low-risk, low-cost alternatives for a church going multi-site.\n“A school is typically good for a one-to-three year ‘run’ in terms of energy and volunteer commitment to helping with a portable ‘church on wheels’ that gets torn down every week,” Tomberlin says.\nThe move to multi-site has been an increasing source of business for Portable Church Industries, according to Pete van der Harst, president of the Troy, Mich.-based company. Founded in 1994, the firm specializes in assisting portable churches by supplying them with the entire range of equipment, furnishings and storage necessary to operate a church facility, as well as consultative services that help them get up and running.\nIn addition to providing an additional venue for spreading its ministry, having a portable, off-site facility helps churches be more flexible, according to van der Harst. “You have one main campus with an address, and then you also have the second, portable facility available for events (such as weddings) when needed.”\nThe next step for a church in multi-siting program is typically going to a 24/7 facility, often an existing commercial building such as a former grocery store, Tomberlin continues. “It’s more expensive than a part-time location, and you have to do a lot more buildout,” he notes, “but it is a lot less expensive than adding the same number of seats to an existing campus. Tomberlin also reports that there is also an increasing trend where existing churches are absorbed/adopted under the flag of a healthy, growing multi-campus church, often receiving renovations and upgrades in the process.\nDelivering the Message\nAbout two thirds of multi-site churches use some form of video transmission (as opposed to strictly live, on-site presentation) to deliver their content, according to Tomberlin.\nThe simplest form of “transmission,” of course, is to record a previously held service onto tape, DVD or computer hard drive and deliver it to the remote location to show on Sunday. Satellite feed is yet another way, he notes, “for those wanting live simultaneous experiences, or that want to cover long distances across states.” And in the past year or so, the technology for streaming video across the Internet and projecting it upon large screens has improved, reports Tomberlin.\nStreaming high-definition Internet video onto a large screen is still a costly proposition, notes Barry Brown, director of Fathom Theater Church, a unit of Centennial, Colo.-based National CineMedia LLC, which operates a digital in-theatre network in North America that includes approximately 16,800 screens in more that 1,325 theatres in 46 states.\n“Streaming video in high-definition on a large screen requires a lot of bandwidth,” says Brown. There are an increasing number of technology options becoming available, though, “and for churches with multiple locations, this can become a cost-effective approach.”\nFathom Theater Church provides in-theatre worship facilities solutions packages to faith-based organizations. “Theatres are great places for multi-site churches to launch,” he notes, “because people are already accustomed to coming to a theatre, sitting in a seat directed at a screen, and anticipating an emotional experience.”\n“Our main location is on a 3.5-acre site just south of downtown—with 2,500 attendees at six services every Sunday,” recounts Steve Blount, executive pastor of administration for Discovery Church, a non-denominational church in Orlando, Fla., with three campuses.\nAbout four years ago, Discovery realized that it had to move to a new location to keep growing, says Blount. But new, nearby locations were scarce, which meant a long move was in store, “And we couldn’t forget the old rule of thumb that if you move a church more than a few miles, you can anticipate a significant drop-off in membership,” he says.\nThat’s when the church began researching the multi-site approach, according to Blount.\n“We did some basic mapping of how our members’ residences were dispersed around the Orlando area, and found several concentrations of members in different parts of town, which gave us a good starting point for figuring out where to put additional campuses.”\nDiscover launched its second campus in a rented high school facility in an affluent part of town, “so we were operating in the black very quickly, by around month four or five,” says Blount. It later followed up with a third site.\nThe biggest challenge for Discovery in opening its first multi-site unit, according to Blount, “was taking over some of the things we do at our main campus, like nursery programs.” High schools don’t have nurseries, “so we had to retrofit a couple of classrooms to serve that purpose.”\nAt the same time, there were some aspects of high school facilities that lent themselves to a church operation.\n“What we failed to fully consider in our early years was that these high schools have incredible gymnasiums, which we don’t have on our main campus—so for the first couple of years we under-utilized the gymnasium facility we had available to us.” The lessons learned here, according to Blount, are that when a church goes multi-site, they must adapt to new locations. “You can’t necessarily take what you do at the main campus without adapting/adjusting to limitations of the new facility, and sometimes the secondary locations have physical elements of their own that are superior,” he closes.']	['<urn:uuid:74f53437-adee-4e4a-aa6a-4c5687034086>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	30	32	1360
9	How do traditional and modern Aboriginal art materials compare preservation-wise?	Traditional Aboriginal art materials include natural elements like bark, ochre pigments, and bird egg binders, while modern works may use synthetic paints, canvas, and printmaking materials. From a preservation perspective, all works require protection from direct sunlight and UV damage, but their specific needs differ. Traditional bark paintings need careful upright storage and protection from humidity changes, while modern works on paper or canvas require acid-free matting, appropriate framing under glass, and careful attention to storage conditions using archival materials with neutral pH levels. Temperature and humidity stability are crucial for both traditional and modern pieces.	"[""World of Dreamings\nTraditional and modern art of Australia\nAn exhibition held at the State Hermitage Museum, St Petersburg | 2 February - 9 April 2000\n- Dr Brian Kennedy, Director, National Gallery of Australia\n- An introduction to Aboriginal art by Susan Jenkins and Carly Lane\n- The Aboriginal Memorial We have survived, by Djon Mundine\n- The Aboriginal Memorial 1987-88 A description\n- John Mawurndjul The resonating land by Luke Taylor\n- All the world The paintings of Nym Bandak by Kim Barber\n- 'Who's that bugger who paints like me?' Rover Thomas by Wally Caruana\n- The enigma of Emily Kngwarray by Jenny Green\n- High art and religious intensity. A brief history of Wik sculpture by Peter Sutton\n- Laced flour and tin boxes The art of Fiona Foley by Avril Quaill\n- The memory theatre of Tracey Moffat by Gael Newton\nAn introduction to Aboriginal art\nContemporary Australian Aboriginal Art in Modern Worlds celebrates some of the highest achievements of Aboriginal artists. This exhibition focuses on the work of six artists: Nym Bandak, Fiona Foley, Emily Kam Kngwarray, John Mawurndjul, Tracey Moffatt, and Rover Thomas. It also includes two major collaborative works from Arnhem Land and Cape York: The Aboriginal Memorial, a magnificent set of painted hollow log coffins by 43 artists from Ramingining, and a group of ceremonial sculptures by Wik artists. While the work of these artists is based in age-old traditions, it embodies the realities of Aboriginal Australians living in the modern world: a world that has undergone great social, political and cultural upheavals since Europeans colonised the country over two centuries ago. These upheavals continue to have repercussions today. Aboriginal art is at the very frontier between indigenous and non-indigenous Australia and reflects the contemporary realities of its makers.\nArt, specifically Aboriginal art created in a European-dominated Australia, can be seen as a medium of negotiation between cultures, and of the exchange of ideas and beliefs; exchanges that are not always equal. The circumstances in which the paintings by Bandak and the Wik sculptures were made, involved the active participation of anthropologists and missionaries. Fiona Foley and Rover Thomas question the 'official' histories of Australia. A focus of the films and photographs of Tracey Moffatt is the definition of identity in a multicultural society. Emily Kam Kngwarray and John Mawurndjul, to varying degrees, have become favourites of the marketplace. The Aboriginal Memorial, on the other hand, was made specifically to register an Aboriginal voice in the clamour of Australia's celebration of two centuries of European occupation. These are but some of the contexts in which art by Aboriginal people operates. Simultaneously, this art expresses the complex of the relationships between the individual, the group, the land, the ancestral beings and the spiritual forces which have invigorated Aboriginal life for generations, past and present.\nAboriginal art is the oldest continuing art tradition in the world. While Europeans mark the start of their third millennium, Aboriginal Australians are marking (at least) their fiftieth. Archaeological research indicates that rock paintings in Arnhem Land in northern Australia date back 50,000 years and rock engravings in southern Australia at least 30,000 years, predating Palaeolithic rock paintings of Altamira and Lascaux in Europe. Graphic symbols and designs found in such rock art continue to be used by artists today.\nAboriginal art manifests in a wide range of forms and media: from ancient rock art to ephemeral body decoration, ground paintings and sand sculptures; from bark paintings and sculpture in wood to jewellery and woven fibres. In recent decades, introduced media such as synthetic paints and canvas, print making materials, textiles, photographic film and computer technology have extended the means by which Aboriginal artists express themselves.\nThere is not one but several traditions of indigenous art in Australia, each with a distinct visual language, its own vocabulary of icons and symbols. Designs usually have layers of meanings, determined by the context in which they are used - whether in a ceremony with a restricted audience of initiates, or in the public domain of the uninitiated. Artists distinguish between the different levels of meaning: the 'inside' stories for the initiated and the 'outside' interpretation. This enables them to make sacred images in the public domain without compromising the work's true cultural significance, nor the artist's integrity.\nTraditionally, the great themes of Aboriginal art relate to the Ancestral Realm, commonly known as the Dreaming. The term 'Dreaming', however inadequate, is used by both Aboriginal and non-Aboriginal people alike. It does not mean a state of dreams or unreality, rather a state of reality beyond the mundane, incorporating the spiritual and physical worlds.\nThe Ancestral Realm refers to the genesis of the universe and to the supernatural and ancestral beings who created it - beings whose spiritual powers continue to influence and sustain successive generations of Aboriginal people. In the Dreaming these beings and creator ancestors travelled across the unshaped world in both human and non-human form, creating the landscape and laying down the laws of social and religious behaviour. Much Aboriginal art concerns stories (also known as Dreamings) about the epic deeds and activities of the creator ancestors. The entire Australian continent is covered in an intricate web of ancestral tracks. Some are specific to, and contained within, a region whilst others span across regions, connecting those whose land they cover.\nThe powers of the supernatural and ancestral beings are present in the land, in natural species and within individuals. Activated through ceremony, they continue to sustain their human descendants. An individual's links with the ancestral beings and the land, and his or her spiritual and social identity, are expressed through totems of natural species and phenomena, which feature in ritual songs, dances and through the creation of art.\nBefore European colonisation, more than 200 distinct languages and up to 600 dialects were spoken across the continent. Equally, there is a myriad of cultural practices, spiritual beliefs and art traditions and styles.\nSimply, indigenous Australian art can be categorised according to a number of stylistic regions, in which artists share common visual vocabularies and specific media. Among those represented in this exhibition are the 'Top end' (Arnhem Land and its surrounds), the Kimberley, the desert and Cape York. In addition, artists such as Fiona Foley and Tracey Moffatt have been raised in the cities, outside traditional communities.\nArnhem Land, in the tropical north, is one of the richest art producing regions in the country. Covering some 114,000 square kilometres, it is renowned for hundreds of rock art sites, mighty rivers, freshwater lagoons, jungles, forests, mangrove swamps and sandy beaches.\nAt least 30 distinct languages and dialects are spoken in Arnhem Land, each as different as, say, Russian is to English; yet the region's clans (or extended family groups) have close associations through similar social organisation, spiritual beliefs and ceremonial activity.\nThe major forms of artistic expression are weavings in natural fibres produced mostly by women, sculptures and bark paintings. Painting on sheets of bark of the eucalyptus tree is the most distinctive form of art in the region. The tradition stems from painting on the inside walls of bark shelters, rock art and paintings on the human body for ceremonies (bark paintings are often considered to be reflections of the body). The tradition was stimulated during the twentieth century by the desire of Europeans to collect portable examples of local art.\nThe bark is best stripped from the stringy-bark tree (Eucalyptus tetradonta) in the wet season of October to March, when the rising sap makes it easy to remove. It is then cured either on a fire and flattened under weights. The outer surfaces are cleaned by stripping excess bark, then the inner painting surface is made smooth.\nThe standard colours used in bark paintings are red and yellow ochre, white kaolin or pipe clay, and black charcoal. These powdery pigments are mixed with binders to make paint. Traditionally, birds egg or orchid juice were used but in recent decades artists have favoured commercial wood glues which have similar qualities and are readily available. The most common brush used to execute the fine cross-hatching on barks is a short stemmed brush with only a few long hairs attached. Other brushes are made from such things as the frayed ends of sticks. Today, commercial brushes are also used.\nCross-hatched patterns, referred to as rarrk or miny'tji, are common throughout Arnhem Land. These are achieved by building up layers of lines using the fine hair brush. The patterns are used to give the surface a visual vibrancy, which evokes the power of the ancestors. This can be seen, for example, in John Mawurndjul's paintings, which have direct references to body designs.\nThe Aboriginal Memorial is an installation of 200 hollow log coffins, of the type used in secondary mortuary ceremonies in Arnhem Land to keep the bones of the deceased clan members. This collaborative piece was made on the occasion of the Bicentenary of Australia's white settlement, in remembrance of Aboriginal people who had died protecting their land during conflict, but who were denied proper burial. It was the artists' hope that the bicentennial year would mark a transition of Australia from an unjust and racist past to a more egalitarian future.\nTo the west of Arnhem Land and Darwin lies Wadeye (Port Keats) in the Daly and Fitzmaurice rivers area, home of the Murrinhpatha and related peoples. This coastal area features sandy beaches, mangrove swamps, creeks, rivers and low hills. Bark painting was introduced to the area in the 1950s, following the commercial success of this portable collectable art form in nearby Arnhem Land. Wadeye bark paintings are distinctive, however, as artists commonly fashion them into an oval shape. The style of painting has similarities with the rock art of the area. Also distinctive is the extended palette, which included greens (a mixture of yellow ochre and black), purples (red ochre and black) and pinks (red ochre and white).\nNym Bandak was the pre-eminent artist of the Wadeye community; a prolific bark painter, he lived through a period of great social and cultural upheaval. Bandak produced the paintings in this exhibition in the late 1950s at the request of the anthropologist W.E.H. Stanner, who sought to investigate the artist's conception of his environment. The result was not only a unique group of works but a long encounter and friendship with Stanner, which lead to a better understanding of indigenous culture by European Australians.\nThe Kimberley area in the north west of Australia, ranges from monumental rock formations in the east to where the desert meets the sea in the west. This area has a great diversity of language groups and art styles. The Kimberley is home to the ancestral Wandjina (agents of procreation) whose images first appeared in rock paintings in the region about 3000 years ago.\nThe practice of making headdresses and other paraphernalia for ceremonial dances in the Kimberley lead to the recent use of painted boards which are carried across the dancers' shoulders. In the late 1970s and early 1980s, a school of painting emerged in the eastern Kimberley of which Rover Thomas was the leading figure.\nIn contrast to the lush tropical north of Australia, vast areas of the centre of the continent are desert. This harsh environment of sandhills, mountain ranges, rock formations, plains, saltpan lakes, seasonal waterways and rare permanent waterholes is the country of several distinct but related groups who share systems of graphic representation and iconography.\nThe desert was the birthplace of one of the most important movements in modern Australian art, known as the Papunya movement. In the early 1970s senior men at the government settlement of Papunya, west of Alice Springs, began to make portable paintings in acrylic using the traditional symbols of their ceremonial sand drawings, ground paintings and body painting.\nThe developments at Papunya spread in time to other desert communities, including Utopia, north east of Alice Springs, where the Utopia Women's Batik Group began in the late 1970s. By 1988, the artists at Utopia had begun painting on canvas. They employed a range of styles, including naturalistically rendered landscapes and the ubiquitous symbols of the desert.\nIn the work of Emily Kam Kngwarray, symbols are used sparingly to transcend the narrative aspect of the Dreamings they evoke. Kngwarray's strong gestural marks and fields of colour express the resonance of ancestral power in the landscape, as does the cross-hatching in Arnhem Land bark paintings.\nThe areas of Cape York in far north Queensland and the islands of the Torres Strait, which form a bridge to Papua New Guinea, are home to a number of different groups.\nWest Cape York Peninsula is the country of the Wik people, whose art has some affinities with that of Arnhem Land across the Gulf of Carpentaria to the west. The Wik have intentionally limited their engagement in the public art world -most of their art continues to be made specifically for ceremonies - although in recent times, weavings and sculptures have been made more frequently for the public domain. The sculptures in this exhibition were made for a public performance of an initiation ceremony.\nThe influence of Queensland sculptural traditions is evident in some major 'urban' artists' work. Fiona Foley, for example, a southern Queenslander, references the forked stick form in her installation work.\nAboriginal and Torres Strait Islander people who live in rural and urban areas, away from traditional environments, have played a significant role in the resurgence of indigenous culture. Such people are not concentrated in one cultural bloc and often have grown up away from their homeland or language group, without the traditional upbringing often expected of Aboriginal people.\nAboriginal art practice in areas where indigenous people are a minority dates back to the nineteenth century, with artists such as William Barak (c.1828-1903) and Tommy McRae (1836-1901). Their distinctive drawings give us rare glimpses of life at the time through Aboriginal eyes.\nArt by Aboriginal people in the city was neglected by the wider art world until the 1970s when Aboriginal social and political movements gained public attention. In 1967 citizenship rights were granted to all Aboriginal people and the first acknowledgment of their rights to land occurred in the mid 1970s. Such reforms provided indigenous artists with opportunities that were previously denied them.\nAn entire generation of artists has emerged in the cities and towns. Not restricted to stereotypical notions of Aboriginal art, they are able to freely use a range of materials, media and recent technology. Such artists articulate various subjects - notions of identity, the link to land, and other political issues concerning the indigenous view of Australian history. The so-called 'urban' movement has flourished to the point where such artists are now at the cutting edge of Australian art. The resulting work presents unique perspectives born of distinctive experiences.\nThe work of all the artists in this exhibition is intended to inform and educate its audience about the breadth and scope of art by Aboriginal people in recent years. It is also intended to challenge stereotypical perceptions of 'Aboriginality' based on notions of the 'authentic' and the 'primitive'.\nTracey Moffatt, for example, has consistently taken the stance that the categorisation of 'Aboriginal artist' is limiting. A maker of photographs and films, she regards herself as an Aboriginal person who makes art; she in fact spends most of her working time in New York.\nFrom New York to Milmilngkan in Arnhem Land, this exhibition represents the broad diversity of art by Aboriginal people at the turn of the century.\nIn the nineteenth century, Aboriginal people were classified as primitive and their material culture often shared spaces in museums with 'strange' specimens of Australian flora and fauna. By the twentieth century, with the beginning of serious anthropological interest in Aboriginal culture and society, it became clear that the preconceived ideas were in fact 'primitive' rather than Aboriginal people themselves. Now, at the beginning of the twenty-first century, Aboriginal art in all its styles and media, and from every part of the continent, is recognised as a vital and dynamic expression of the contemporary world. Today, artists are no longer disregarded on the basis of race, gender, geography and the domination of classical European models.\nThe interest in Aboriginal art which has flourished since the 1970s has created new opportunities for indigenous artists, as their work leaves the communities to be shown in museums and galleries around the world. Meanwhile, the imperatives to produce art for traditional purposes continue, and the expanded environment in which indigenous art now operates has created further compelling reasons for artists to continue expressing the values of their culture to the wider world. In the public domain, Aboriginal art can be appreciated for its spirituality and aesthetic qualities, and as a reflection of the social and political achievements and aspirations of the peoples who create it.\nSusan Jenkins and Carly Lane"", 'Preservation tips for paintings and prints\nArt & Antiques by Dr. Lori\nPeople love their collections. No matter the type of object — cookie jars, military memorabilia, fine art posters — collectors want to add to an existing collection, display their assembled objects, and learn more about their cherished treasures. One of the most important and interesting aspects of collecting is preserving art, antiques, and collectibles for the long term. Many collections include family heirlooms or assembled collectibles that will be handed down to younger generations so preserving a collection is very important. Here are some key points about how to protect, preserve, and enjoy your collections.\nLight is the real problem when it comes to the preservation of paintings and works on paper. UV protection using UV-filtered or opaque materials helps prevent fading and light damage. One of the best ways to preserve fine art is investing in quality framing. For an oil on canvas painting, a frame will protect both the stretcher and the canvas as well as give a finished look to the painting once it is on the wall. Paintings exist best when kept out of direct sunlight and hung away from elements that may spark temperature and humidity changes like heaters, radiators, and air conditioners.\nPrints require a different type of protection when it comes to framing and display. Prints and other works on paper like antique maps, historic documents, and the like should be matted and framed under glass using materials that are free of acid. Acid free materials like mats and storage boxes should have a pH level of 7.0 or greater and the adhesives used in the framing of a fine art print should be pH neutral to protect fragile works on paper. Some acid free materials are made free of lignin, which can produce acid and darken paper, this process is known as acid burning or tanning. Avoid acid burning or tanning whenever possible.\nSome of the most critical damage that happens to art and antiques happens when objects are stored. Although it is little known, significant damage can occur during storage. When you first put an antique object away in storage, everything is fine but over time, changes in temperature and humidity can occur. When no one is looking, other affects may take place which will impact the condition and value of an antique or collection. It is important to store objects in archival boxes intended for a certain type and size of collectible. Physical support is necessary for fragile objects and storage containers like archival boxes need to be constructed to stand the test of time. What you put into a storage box like acid free tissue paper along with an antique is as important as the storage container. One size does not fit all when it comes to archival storage.\nLarge paintings should be stored off the floor, preferably hanging up even in storage locations. If there is no room for a hanging storage solution, then store large paintings standing upright in a closet or storage area. While it may seem like a convenient place to store paintings, never lay paintings flat, face up under a bed. This will put stress on the stretchers and the canvas itself. Smaller paintings may be stored upright back to back and face to face on separated shelves. Use acid free foam core dividers to prevent the wire from the back of one painting from scratching the frame or canvas on the front of another painting positioned next to it. There are specific techniques to protecting art, a good rule of thumb is to handle with care, display works of art away from direct sunlight and store works in areas where temperature and humidity fluctuations are minimal.\nDr. Lori Verderame is the award-winning Ph.D. antiques appraiser on History channel’s #1 hit show about the world’s oldest treasure hunt, The Curse of Oak Island. For more information, visit www.DrLoriV.com and www.YouTube.com/DrLoriV.']"	['<urn:uuid:f54ee2fb-0db4-40af-98e7-71e5c6dec018>', '<urn:uuid:1892276b-6ff4-4ac9-b00c-da15321a2cb7>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T05:06:54.527160	10	96	3475
10	machine learning fairness solutions impact on minority communities linguistic patterns	Traditional approaches to AI fairness can actually be problematic for minority communities. For instance, systems can incorrectly flag posts written in African-American English as offensive and LGBTQ+ community content as toxic. To address this, researchers have developed controllable AI models that can detect toxicity while accounting for in-group and cultural linguistic norms, providing fairer classification.	"['Eliminating bias in AI may be impossible—a computer scientist explains how to tame it instead\nWhen I asked ChatGPT for a joke about Sicilians the other day, it implied that Sicilians are stinky.\nAs somebody born and raised in Sicily, I reacted to ChatGPT\'s joke with disgust. But at the same time, my computer scientist brain began spinning around a seemingly simple question: Should ChatGPT and other artificial intelligence systems be allowed to be biased?\nYou might say, ""Of course not!"" And that would be a reasonable response. But there are some researchers, like me, who argue the opposite: AI systems like ChatGPT should indeed be biased—but not in the way you might think.\nUncovering bias in AI\nComputer scientists say an AI model is biased if it unexpectedly produces skewed results. These results could exhibit prejudice against individuals or groups, or otherwise not be in line with positive human values like fairness and truth. Even small divergences from expected behavior can have a ""butterfly effect,"" in which seemingly minor biases can be amplified by generative AI and have far-reaching consequence.\nBias in generative AI systems can come from a variety of sources. Problematic training data can associate certain occupations with specific genders or perpetuate racial biases. Learning algorithms themselves can be biased and then amplify existing biases in the data.\nBut systems could also be biased by design. For example, a company might design its generative AI system to prioritize formal over creative writing, or to specifically serve government industries, thus inadvertently reinforcing existing biases and excluding different views. Other societal factors, like a lack of regulations or misaligned financial incentives, can also lead to AI biases.\nThe challenges of removing bias\nIt\'s not clear whether bias can—or even should—be entirely eliminated from AI systems.\nImagine you\'re an AI engineer and you notice your model produces a stereotypical response, like Sicilians being ""stinky."" You might think that the solution is to remove some bad examples in the training data, maybe jokes about the smell of Sicilian food. Recent research has identified how to perform this kind of ""AI neurosurgery"" to deemphasize associations between certain concepts.\nBut these well-intentioned changes can have unpredictable, and possibly negative, effects. Even small variations in the training data or in an AI model configuration can lead to significantly different system outcomes, and these changes are impossible to predict in advance. You don\'t know what other associations your AI system has learned as a consequence of ""unlearning"" the bias you just addressed.\nOther attempts at bias mitigation run similar risks. An AI system that is trained to completely avoid certain sensitive topics could produce incomplete or misleading responses. Misguided regulations can worsen, rather than improve, issues of AI bias and safety. Bad actors could evade safeguards to elicit malicious AI behaviors—making phishing scams more convincing or using deepfakes to manipulate elections.\nWith these challenges in mind, researchers are working to improve data sampling techniques and algorithmic fairness, especially in settings where certain sensitive data is not available. Some companies, like OpenAI, have opted to have human workers annotate the data.\nOn the one hand, these strategies can help the model better align with human values. However, by implementing any of these approaches, developers also run the risk of introducing new cultural, ideological or political biases.\nThere\'s a trade-off between reducing bias and making sure that the AI system is still useful and accurate. Some researchers, including me, think that generative AI systems should be allowed to be biased—but in a carefully controlled way.\nFor example, my collaborators and I developed techniques that let users specify what level of bias an AI system should tolerate. This model can detect toxicity in written text by accounting for in-group or cultural linguistic norms. While traditional approaches can inaccurately flag some posts or comments written in African-American English as offensive and by LGBTQ+ communities as toxic, this ""controllable"" AI model provides a much fairer classification.\nControllable—and safe—generative AI is important to ensure that AI models produce outputs that align with human values, while still allowing for nuance and flexibility.\nEven if researchers could achieve bias-free generative AI, that would be just one step toward the broader goal of fairness. The pursuit of fairness in generative AI requires a holistic approach—not only better data processing, annotation and debiasing algorithms, but also human collaboration among developers, users and affected communities.\nAs AI technology continues to proliferate, it\'s important to remember that bias removal is not a one-time fix. Rather, it\'s an ongoing process that demands constant monitoring, refinement and adaptation. Although developers might be unable to easily anticipate or contain the butterfly effect, they can continue to be vigilant and thoughtful in their approach to AI bias.']"	['<urn:uuid:520dba0f-e9f6-41d5-a9db-9291d2bd4272>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	10	55	786
11	detailed physical properties pinus radiata sapwood heartwood and biocontrol strategies against fusarium circinatum pathogen affecting pinus seedlings	Pinus radiata has distinct sapwood and heartwood characteristics - the sapwood is pale yellow to white and not susceptible to lyctine borer attack, while the heartwood is reddish-brown to yellow. As for protecting Pinus against Fusarium circinatum pathogen, endophytic bacteria have shown promise as biocontrol agents. Specifically, Bacillus subtilis and Burkholderia sp. strains isolated from Pinus tissues can inhibit F. circinatum growth through direct antagonism and production of thermostable metabolites that reduce fungal growth by over 50%. These bacteria, being naturally present in healthy Pinus tissues, are well-adapted as biocontrol agents for nursery applications.	"[""Pinus radiata: Family: Pinaceae.\n|Description and natural occurrence|\nA medium sized tree attaining a height of 40 to 50 m and a stem diameter of 1 m. Branches are usually large and spreading and pinecones are very conspicuous on the tree. Bark is grey to red-brown in colour, thick, rough, deeply fissured and shed in small flakes.\nRadiata pine is native to a very small area of the west coast of North America but is now a major plantation species throughout the world, especially in New Zealand, Chile, South Africa and Australia. In Australia it is grown in all states and the ACT although commercial plantings in Queensland are confined to the southern highlands.\nSawn timber of this species is readily available.\nColour. Heartwood is reddish-brown varying to shades of yellow. Sapwood is usually pale yellow to white.\nGrain. Generally straight. An often pronounced difference in colour between earlywood and latewood results in a very distinctive figure when backsawn.\nDensity. Australia: 545 kg/m3 at 12% moisture content; approximately 1.8 m3 of seasoned sawn timber per tonne. New Zealand: 490 kg/m3; approximately 2 m3/t.\nStrength groups. Australia: S6 unseasoned; SD6 seasoned. New Zealand: S7 unseasoned; SD6 seasoned.\nStress grades. Australia: F4, F5, F7, F8 (unseasoned), F5, F7, F8, F11, F14 (seasoned); New Zealand: F4, F5, F7 (unseasoned), F5, F7, F8, F11, F14 (seasoned) when visually stress graded in accordance with AS 2858-2001: Timber - softwood - visually stress-graded for structural purposes.\nJoint groups. Australia: J4 unseasoned; JD4 seasoned. New Zealand JD4 seasoned.\nShrinkage to 12% MC. 5.1% (tangential); 3.4% (radial).\nUnit shrinkage. 0.27% (tangential); 0.20% (radial). These values apply to timber reconditioned after seasoning.\nLyctine susceptibility. Sapwood is not susceptible to lyctine borer attack.\nTermite resistance. Not resistant.\nDurability above-ground. Class 4 - life expectancy less than 7 years.\nDurability in-ground. Class 4 - life expectancy less than 5 years.\nPreservation. Plantation grown trees have a high proportion of sapwood, which readily accepts commercial preservative impregnation. The heartwood of radiata pine also accepts some preservative impregnation but for practical purposes it is considered untreatable as results are unreliable.\nSeasoning. To avoid distortion, framing sizes should be high temperature dried. Boards may be air-dried or kiln dried at conventional or high temperatures.\nHardness. Soft (rated 5 on a 6 class scale) in relation to indentation and ease of working with hand tools.\nMachining. Machines and turns well but planer blades should be kept sharp to avoid surface ridging.\nFixing. Nails may occasionally follow the growth rings. Nailing guns give good results.\nGluing. Differential glue absorption can occur between earlywood and latewood but this rarely causes problems.\nFinishing. Will readily accept paint, stain and polish.\nEngineering. Preservative impregnated poles for pole frame construction, transmission poles and land poles.\nConstruction. General purpose softwood used as dressed, seasoned timber in general house framing, flooring, lining, joinery, mouldings and laminated beams. Preservative impregnated in sawn or round form in fencing, pergolas, landscaping, retaining walls, playground equipment. Also used in the manufacture of Scrimber.\nDecorative. Furniture, outdoor furnishings (preservative impregnated), plywood, joinery, turnery, carving.\nOthers. Structural plywood, scaffold planks, wood wool, paper products, particleboard, and medium density fibreboard.\nSapwood. Pale yellow.\nHeartwood. Reddish-brown, varying to shades of yellow.\nTexture. Non-uniform, consisting of alternating bands of earlywood and latewood. Grain straight. Knots usually present in constructional timber grades.\nGrowth rings. Prominent and clearly visible, latewood forming a dense dark band. False rings rare. Transition from earlywood to latewood abrupt.\nResin canals. Numerous, prominent as lines on dressed longitudinal surfaces.\nRays. Fine, visible with a lens.\nOdour. Wood generally has a resinous odour.\nHopewell, G (ed.) 2006, 'Construction timbers in Queensland: properties and specifications for satisfactory performance of construction timbers in Queensland, Class 1 and Class 10 buildings', books 1 and 2, Department of Primary Industries and Fisheries, Brisbane.\nIlic, J 1991, 'CSIRO atlas of hardwoods', Crawford House Press, Bathurst, Australia.\nStandards Australia, 2000, 'AS 2082-2000: Timber - hardwood - visually stress-graded for structural purposes', Standards Australia.\nLast updated 04 August 2010"", 'versión On-line ISSN 0718-5839\nChilean J. Agric. Res. vol.72 no.2 Chillán jun. 2012\nChilean Journal of Agricultural Research 72(2) April - June 2012\nEndophytic Bacteria from Pinus taeda L. as Biocontrol Agents of Fusarium circinatum Nirenberg & O\'Donnell\nBacterias Endófitas de Pinus taeda L. como Agentes de Control Biológico de Fusarium circinatum Nirenberg & O\'Donnell\nSilvina Soria1, Raquel Alonso1*, and Lina Bettucci1\n1Universidad de la República, Facultad de Ciencias-Facultad de Ingeniería, Montev\nideo, Uruguay. ""Corresponding author (firstname.lastname@example.org).\nFusarium circinatum Nirenberg & O\'Donnell, the pitch canker fungus, has been recently reported in Uruguay affecting Pinus taeda L. seedlings. The spread of this pathogen to plantations constitute a risk to forestry production. The aim of this work was to evaluate the inhibitory effect of live bacteria and their thermostable metabolites on F. circinatum growth in vitro. Four Bacillus subtilis strains and one of Burkholderia sp. isolated as P. taeda endophytes were evaluated as biological control agents of F. circinatum. Dual cultures between live bacteria and pathogen were performed. Furthermore, bacteria metabolites obtained from liquid cultures were sterilized and added to the culture media where fungus was grown. In this study all bacteria showed an antagonist effect on the pathogen growth arresting the mycelia at one cm of the edge of the bacteria colony. Bacteria thermostable metabolites reduced over 50% fungal growth. These results demonstrates that endophytic bacteria, well adapted to live in host tissues, constitute a good alternative to control F. circinatum affecting Pinus seedlings.\nKey words: Biological control, Bacillus sp., Burkholderia sp., pitch canker.\nLa presencia de Fusarium circinatum Niremberg & O\'Donnell, agente causal del cancro resinoso en pino, ha sido detectada recientemente en plántulas de Pinus taeda L. en Uruguay. La propagación de este patógeno en las plantaciones constituye un riesgo para la producción forestal. El objetivo de este trabajo fue determinar la capacidad inhibitoria de bacterias vivas y de sus metabolitos termoestables sobre el crecimiento de F. circinatum in vitro. Cuatro cepas de Bacillus subtilis y una de Burkholderia sp. aisladas como endófitas de P. taeda, fueron evaluadas como potenciales agentes de control biológico sobre F. circinatum. Para ello, se realizaron enfrentamientos directos entre las bacterias vivas y el micelio del patógeno. Por otra parte, los metabolitos bacterianos obtenidos de cultivos líquidos fueron esterilizados en autoclave y se incorporaron al medio de cultivo donde se hizo crecer el patógeno. En este estudio todas las bacterias mostraron un efecto antagónico sobre el crecimiento del patógeno, deteniéndose el avance del micelio a 1 cm del borde de la colonia bacteriana. Los metabolitos termoestables de las bacterias produjeron una disminución significativa en la tasa de crecimiento del hongo mayor al 50%. Estos resultados muestran que las bacterias que viven dentro de los tejidos sanos del hospedante son una buena alternativa para el control del patógeno F. circinatum en plántulas de Pinus.\nPalabras clave: control biológico, Bacillus sp., Burkholderia sp., cancro resinoso.\nThe pitch canker fungus Fusarium circinatum Nirenberg and O\'Donnell is a destructive pathogen that affects several Pinus species (Barnard and Blakeslee, 1980; Viljoen et al., 1994). The symptom more frequently associated to this pathogen is the presence of large resinous cankers on the main trunk and lateral branches of trees but it can also be associated to roots, shoots, cones, and seedlings. In plant seedlings aerial symptoms do not appear until the pathogen reach the trunk from lesions at soil level resulting in plant discoloration and needles drying. The disease has been detected in south eastern USA (Kuhlman et al., 1982), Mexico, South Africa, Chile, Japan, and Spain (Kobayashi and Muramoto, 1989; Guerra-Santos, 1999; Wingfield et al., 2002; Perez Sierra et al., 2007). Recently, in Uruguay this pathogen was detected on Pinus taeda L. seedlings from nurseries mainly affecting stem collar (Alonso and Bettucci, 2009).\nAccording to Cook and Baker (1983) biological control can be defined as a reduction of the amount of inoculum or disease produced by the activity of a pathogen, based on the use of natural enemies or the use of compounds derived from its metabolism. Then, the biological control offers an alternative to the chemical products, contributing to minimize the negative consequences for human health and environment (Kim et al., 2003). Fungal diseases are very frequent in nurseries and the chemical control of pathogens is the most common practice.\nBacillus subtilis has been identified as a potent antagonist against several fungal pathogens due to the production of antifungal compounds, antibiotics and proteases, hence is extensively used in agricultural systems (Todorova and Kozhuharova, 2009; Chen et al., 2009; Kinsella et al., 2009).\nBurkholderia sp. is known to have beneficial effect on plant growth through the production of antifungal and other compounds that are able to suppress many soil-borne plant pathogens (Holmes et al., 1998). Burkholderia cepacia is an ubiquitous soil organism that can be easily obtained and it has been studied as biocontrol agent of plant disease (Leisinger and Margraff, 1979). Many of its metabolites have been isolated and identified thus verifying its inhibitory effect on different plant pathogens such as fungus, bacteria and yeasts (Sopheareth et al., 2006), particularly on species of Pythium, Botrytis, Fusarium, and Rhizotocnia (Sijam and Dikin, 2005; Quan et al., 2006).\nThe abuse and misuse of chemical products can cause environmental and human health-related risks. On the other hand, little work has been performed on biological control of forest pathogens. Identification and action mode of antifungal compounds produced by an antagonist need to be studied.\nThe aim of this work was to evaluate the antagonist effect of both live bacteria and their thermostable metabolites of four Bacillus subtilis strains and one of Burkholderia sp. on Fusarium circinatum growth.\nMATERIALS AND METHODS\nFungal and bacteria isolates\nFusarium circinatum strains used in this work were all isolated from symptomatic Pinus taeda seedlings from two Pinus nurseries from Rivera and Florida Departments in Uruguay. The fungal isolates were identified by macro and micromorphological characteristics and verified by molecular analysis using CIRC1A and CIRC4A specific primers for F. circinatum (Schweigkofler et al., 2004). The isolates were maintained in potato dextrose agar (PDA). Bacteria were present as endophytes from Pinus seedlings and were isolated from stem healthy tissues. Those showing inhibitory effect on fungal growth were selected. The identification of bacteria strains were performed by molecular analysis of 16S RNA region. The cultures were maintained on triptone soy agar (TSA).\nBacteria antagonist on F. circinatum growth\nTo evaluate the antagonist effect of different live bacteria, mycelia plugs from the edges of actively growing fungal cultures were placed in the center of Petri dish containing PDA. Four bacteria isolates were streaked on the same plates at equal distance from the fungal inocula. Plates with the fungal plug without bacteria were used as control. Plates were incubated at 25 °C for 5 d to evaluate the inhibition activity of bacteria on the fungus. Each treatment was replicated five times. The fungal strains used were Fc 2052, Fc2053, Fc2054, and Fc2057. Bacteria strains of Bacillus subtilis used were B1, B2, B3, B4, and one strain of Burkholderia sp. (B5).\nObservations of mycelia of the interaction zone between fungi and bacteria were performed under microscope.\nThe activity of bacteria thermostable metabolites was also evaluated. Liquid cultures of bacteria were performed transferring colonies of each bacterium to a 250 mL Erlenmeyer flask containing 100 mL of potato dextrose broth (PDB) and then incubated in a rotary shaker at 27 °C and 180 rpm during 7 d. Ten milliliters of each flask were transferred to a new flask with 90 mL of PDA. These new flasks were sterilized during 16 min at 121 °C and 1 atm. The culture medium plus the metabolites were homogenized and 20 mL were placed on Petri dishes of 9 cm of diameter. Once the medium was solidified a plug of each fungal strain was placed in the centre of a dish. A fungal plug placed on PDA was used for control. Each treatment and control was replicated three times. Both treatments and controls were incubated at 25 °C during 9 d. After incubation for 120 h the diameter of the colonies was measured daily during 4 d and compared with controls. The measures were made from the centre of the fungal plug to the edge of the colony. Two measures were taken which were then averaged. Percentage of inhibition growth and the rate of growth were calculated. To determine if there were differences in the rate of growth between the four fungal strains tested a One Way ANOVA test was made using the Sigma Stat 3.5 program.\nRESULTS AND DISCUSSION\nIsolation of bacteria and screening of antifungal effect The screening of bacteria for antifungal activity against the Pinus pathogen F. circinatum showed that all of them exhibited growth inhibition against the pathogen. All the strains arrested the mycelium growth at 1 cm or more of the fungal colony margin (Figure 1). The development of an inhibition halo was observed between the fungal colonies and the bacteria inocula. This may be due to the production of bacterial metabolites that may diffuse in the culture medium and suppress the growth of F. circinatum. These results are consistent with those obtained by Nourozian et al. (2006) who evaluated the antagonist activity of different bacteria (Bacillus, Pseudomonas) against F. graminearum. They observed, in dual culture experiments, the formation of inhibition zones between bacteria and fungus.\nThe micromorphology of mycelia in the interaction zone showed a change in hyphal mode development, exhibiting empty, vacuolated and swollen hypha and a different ramification pattern.\nEffect of thermostable metabolites\nDespite metabolites of all strains showed an inhibitory effect against the fungus strains tested (Figure 2) all of them had a different incidence on the fungal growth (Figure 3).\nFigure 2. Antagonist effect of thermostable metabolites on Fusarium circinatum growth (±SD).\nGrowth of Fusanum circinatum strains Fc2052 Q, Fc2053 S, Fc2054 PS and Fc2O570 after 5 days on culture media containing thermostable metabolites of Bacillus subtilis strains (B1, B2, B3, B4) and Burkholderia sp. (B5). Control: culture media without metabolites.\nFigure 3. Growth of Fusarium circinatum strain Fc2057 on culture media containing thermostable metabolites of Bacillus subtilis strains (A: strain B1; B: strain B2; C: strain B3; D: strain B4; E: control, culture media without metabolites).\nThere were significant differences among the bacterial strains. Growth inhibition on the strain Fc2052 was greater than to the other strains (p < 0.05) (Figure 2). On the other hand, from all strains of B. subtilis the strain B2 showed the lowest effect on the pathogen growth. Although the fungal strain Fc2053 showed a lesser growth than the control, the difference was not significant. The effect of metabolites of B. subtilis B1 on Fc2054 was greater than to other fungal strains. Metabolites from Burkholderia sp. evidenced a lesser effect on the growth of this pathogen. Metabolites of B1, B3, B4, and B5 reduced the growth of F. circinatum Fc2057over 50% (Figure 2).\nThese results showed that the metabolites of Bacillus and Burkholderia tested, reduced the rate of growth of F. circinatum although some differences among fungal strains were observed. These findings suggest the possibility of using B. subtilis as biocontrol agent of Pinus pathogen F. circinatum, consistently with other studies where Bacillus has inhibitory effect against Fusarium spp. and other plant pathogen fungi (Moita et al., 2005; Kinsella et al., 2009). Recently, Burkholderia spp. have been used as biocontrol agents against fungal disease, including Fusarium spp. (Quan et al., 2006).\nThe bacteria that were present as endophytes of Pinus taeda seedlings were symptomless colonizers and apparently adapted to host tissues. This can constitute an advantage for using them as biocontrol agent of the pitch canker fungus on this host. The biological control could be an alternative to reduce the incidence of the pathogen in nurseries in order to avoid the expansion of the disease to the field. The active thermostable metabolites are also a very interesting alternative to chemical control and to avoid the use of living organisms. Both, B. subtilis and Burkholderia are not frequently used in forest management.\nWe thank Estella Reginensi and Ana Clara Bianchi for the identification of the bacteria used in this study.\nAlonso, R., and L. Bettucci. 2009. First report of the Pitch canker fungus Fusarium circinatum affecting Pinus taeda seedlings in Uruguay. Australasian Plant Disease Notes 4:91-92. [ Links ]\nBarnard, E.L., and G.M. Blakeslee. 1980. Pitch canker of slash pine seedlings: a new disease in forest tree nurseries. Plant Disease 64:695-696. [ Links ]\nCook, R.J., and K.F. Baker. 1983. The nature and practice of biological control of plant pathogens. 593 p. APS Press, St. Paul, Minnesota, USA. [ Links ]\nChen, F., M. Wang, Y. Zheng, J. Luo, X. Yang, and X. Wang. 2009. Quantitative changes of plant defense enzymes and phytohormone in biocontrol of cucumber Fusarium wilt by Bacillus subtilis B579. World Journal of Microbiology and Biotechnology 26:675-684. [ Links ]\nGuerra-Santos, J.J. 1999. Pitch canker on Monterrey pine in Mexico. In Devey, M.E., A.C. Matheson, and T.R Gordon (eds.) Forestry and forest products. Current and potential impacts of pitch canker in Radiata pine. Technical Report Vol. 112. p. 58-61. CSIRO, Canberra, Australia. [ Links ]\nHolmes, A., J. Govan, and R. Goldstein. 1998. Agricultural use of Burkholderia (Pseudomonas) cepacia: A threat to human health? Emerging Infectious Diseases 4:221-227. [ Links ]\nKim, H.S., J. Park, S.W. Choi, K.H. Choi, G.P. Lee, S.J. Ban, et al. 2003. Isolation and characterization of Bacillus strains for biological control. Journal of Microbiology 41:196-201. [ Links ]\nKinsella, K., C.P. Schulthess, T.F. Morris, and J.D. Stuart. 2009. Rapid quantification of Bacillus subtilis antibiotics in the rhizosphere. Soil Biology & Biochemistry 41:374-379. [ Links ]\nKobayashi, T., and M. Muramoto. 1989. Pitch canker of Pinus luchuensis, a new disease of Japanese forests. Forests Pests 40:169-173. [ Links ]\nKuhlman, E.G., S.D. Dianis, and T.K. Smith. 1982. Epidemiology of pitch canker disease in a loblolly pine seed orchard in North Carolina. Phytopathology 72:1212-1216. [ Links ]\nLeisinger, T., and R. Margraff. 1979. Secondary metabolites of fluorescent pseudomonads. Microbiological Reviews 43:422-442. [ Links ]\nMoita, C., S.S. Feio, L. Nunes, M.J.M. Curto, and J.C. Roseiro. 2005. Optimization of physical factors on the production of active metabolites by Bacillus subtilis 355 against wood surface contaminant fungi. International Biodeterioration and Biodegradation 55:261-269. [ Links ]\nNourozian, J., H.R. Etebarian, and G. Khodakaramian. 2006. Biological control of Fusarium graminearum on wheat by antagonistic bacteria. Songklanakarin Journal of Science and Technology 28:29-38. [ Links ]\nPerez Sierra, A., E. Landeras, M. Leon, M. Berbegal, J. García-Jiménez, and J. Armengol. 2007. Characterization of Fusarium circinatum from Pinus spp. in northern Spain. Mycological Research 111:832-839. [ Links ]\nQuan, C.S., W. Zheng, Q. Liu, Y. Otha, and S.D. Fan. 2006. Isolation and characterization of a novel Burkholderia cepacia with strong antifungal activity against Rhizoctonia solani. Applied Microbial and Cell Physiology 72:1276-1284. [ Links ]\nSijam, K., and A. Dikin. 2005. Biochemical and physiological characterization of Burkholderia cepacia as biological control agent. International Journal of Agriculture and Biology 7:385-388. [ Links ]\nSchweigkofler, W., K. O\'Donnell, and M. Garbelotto. 2004. Detection and quantification of airborne conidia of Fusarium circinatum, the causal agent of pine pitch canker, from California sites by using a real-time PCR approach combined with a simple spore trapping method. Applied and Environmental Microbiology 70:3512-3520. [ Links ]\nSopheareth. M., L. Seug-Je, H. Hoon, K. Yong-Wong, P. Keun-Hyung, C. Gyu-Suk, et al. 2006. Isolation and characterization of antifungal substances from Burkholderia sp. culture broth. Current Microbiology 53:358-364. [ Links ]\nTodorova, S., and L. Kozhuharova. 2009. Characteristics and antimicrobial activity of Bacillus subtilis strains isolated from soil. World Journal of Microbiology and Biotechnology 26:1207-1216. [ Links ]\nViljoen, A., M.J. Wingfield, and W.F.O. Marasas. 1994. First report of Fusarium subglutinans f. sp. pini on seedlings in South Africa. Plant Disease 78:309-312. [ Links ]\nWingfield, M.J., A. Jacobs, T.A. Coutinho, R. Ahumada, and B.D. Wingfield. 2002. First report of the pitch canker fungus, Fusarium circinatum, on pines in Chile. Plant Pathology 51:397. [ Links ]\nReceived: 15 September 2011.\nAccepted: 25 May 2012.']"	['<urn:uuid:e6d83854-8a64-44ba-a709-8e3348c4c10c>', '<urn:uuid:b2ae39c2-542e-4f5e-aa2a-2a9ee94ad27a>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T05:06:54.527160	17	94	3333
12	How large are the Wellfield Grassland areas combined?	The grassland areas (Wellfield West, East & South) are roughly 0.9ha (2.2 acres) in size.	['Wellfield Plantation and Grasslands are a set of roughly linked sites that form a complex of secondary woodland and chalk grassland over approximately 2 hectares (5 acres), at the very northern-most part of the North Downs on Upper Chalk deposits.\nIn the early 20th century, the site lay within the extensive grounds of the former Queen Mary’s Hospital, which was built in the early 1890’s. The remnants of Queen Mary’s Hospital are now occupied by Orchard Hill Hospital, which has been owned and managed by the Sutton & Merton Primary Care Trust since 1959. Queen Mary’s Woodland abuts the eastern edge of Wellfield Plantation and Wellfield South.\nIn the late 1990’s, a development of 36 homes was accepted and built on the area of an old Medical Research Council site, just to the southwest of Orchard Hill Hospital. As the development would be within the Green Belt and would affect areas of species-rich chalk grassland, The London Borough of Sutton negotiated for the areas of undeveloped grassland. These are now termed Wellfield Grasslands.\nWildlife & Habitats\nAs the site name explicitly states, the site is a mixture of plantation woodland and grassland. The plantation, as noted above, is secondary woodland that is likely to be self sown, that is, the woodland has arisen as a direct response to lack of management, once the original conifer plantation was finished with. The majority of trees within the plantation woodland are sycamore (Acer pseudoplantanus), with some mature ash (Fraxinus excelsior) forming the canopy, with hawthorn (Crataegus monogyna) and elder (Sambucus nigra) as understorey species. There is a great deal of ivy (Hedera helix) covering a large number of the trees and it dominates the ground flora too.\nThe small ‘grassland’ area within the plantation woodland (referred to as C3) was dominated by large and dense strands of bramble (Rubus fruticosus agg.), with some mature sycamore trees providing a large amount of canopy cover to the area. Some of the sycamores are trees-throws; they were blown over in the storm of 1987 but have continued to grow, throwing up lots of stems from their now horizontal trunk.\nThe grassland areas (Wellfield West, East & South) are roughly 0.9ha (2.2 acres) in size. Wellfield West and East are good examples of chalk grassland with several notable flowering plant species. Wellfield South has not been managed since the housing development in the late 1990’s and consequently is very rough. It is hoped that the London Borough of Sutton will be able to manage this area for nature conservation in the very near future.\nThe chalk underlying the site is particularly valuable habitat for many species of plant; amongst the most notable are kidney vetch (Anthyllis vulneraria), majoram (Origanum vulgare), wild basil (Clinopodium vulgare), meadow vetchling (Lathyrus pratensis) and the occasional pyramidal orchid (Anacamptis pryamidalis). These play host to numerous species of invertebrates, including butterflies, spiders and beetles.\nThe plantation itself is quite good for birds, with several common species breeding there. In late 2007, early 2008, bird and bat boxes, including an owl box, were erected within the woodland.\nIn 2005, work started at Wellfield Plantation to clear the grassland area C3 of bramble. Over the last few years, through a combination of cutting, scrub clearance and grazing with Herdwick sheep, provided by our partners the Downlands Countryside Management Project, we are starting to get larger and larger areas of grass coming through. Within the grassy areas, cowslips (Primula veris) have made a welcome return. It is likely that they have lain dormant for several years, unable to compete with the bramble. By clearing the bramble, we have enabled this species to regain a foothold on the site.\nA major phase of work on the plantation woodland started in 2007. Apart from the dominance of bramble on the grassland, shading from sycamores restricts the spread of grasses. By thinning the multiple stems from the fallen sycamores, we allow more light and heat to the floor, enabling grass (and other plant) species to move in. As at The Warren, the cut branches are used to create compost bins for cut grass and other arisings from the management work. We are also thinning areas of sycamore within the plantation woodland. This helps understorey species, such as hawthorn and elder thrive but also reduces the amount of ivy, allowing other species of plant to establish.\nThe grasslands are either mown or grazed with sheep to reduce the amount of scrub and rank grasses getting establish on site. Grazing creates an uneven sward (height and patchiness of grasses) due to the slow and selective grazing of the sheep. This creates lots of ‘micro-habitats’ that various plant and invertebrate species can use, increasing the amount of biological diversity on the grasslands.']	['<urn:uuid:1dcc65bb-b3e0-4a48-b0f0-4d99cd7fdffa>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	8	15	790
13	How does gene therapy help eye diseases and what's the new approach?	Gene therapy currently treats inherited eye diseases by delivering corrective genes to repair mutations. A revolutionary new approach called the modifier gene platform takes a gene-independent approach by overexpressing regulatory genes instead of correcting individual mutations, potentially offering broader treatment for retinal degeneration.	"[""Rajendra Kumar-Singh, PhD\nNational University of Ireland, B.Sc, Physics and Biology\nUniversity of Dublin, Trinity College, Ireland, Ph.D, Retinitis Pigmentosa\nUniversity of Michigan, Ann Arbor, Adenovirus Vectors\nDr. Kumar-Singh is engaged in groundbreaking research in gene therapy. Specifically, his research interests are focused on development and application of gene therapy vectors for retinal diseases. Dr. Kumar-Singh is expected to lead work on important translational research opportunities within the fields of regenerative medicine and sensory neuroscience, research priorities identified in the strategic plan.\nGene therapy has already been shown to be efficacious in humans and animal models of disease. However, several important hurdles still exist to translate gene therapy from the laboratory to the clinic. These problems include vector toxicity, neoplasia and longevity of transgene expression. Dr. Kumar-Singh’s laboratory addresses each of these problems from several angles including the development of ‘gutted’ or helper-dependent adenovirus vectors that are capable of evading the immune system, pseudotyped (retargeted) adenovirus vectors that are redirected from their natural receptor (CAR, Coxsackie Adenovirus Receptor) to alternative receptors that are abundantly present on photoreceptor neurons and Adenovirus/Adeno Associated Virus hybrids that allow the normally episomal Adenovirus genome to integrate site specifically in human cells. This vector technology is used to rescue retinal degeneration in naturally occurring models of RP including the rd mouse, that has a mutation in the beta subunit of cGMP Phosphodiesterase and is the most common cause of autosomal recessive Retinitis Pigmentosa.\nDr. Kumar-Singh has recently developed a mutation independent approach to treating autosomal dominant RP (adRP) using short hairpin RNA (shRNA). A mutation- independent approach is extremely useful given that there are more than 100 different mutations present in the gene rhodopsin, any one of which can cause adRP. The difficulty of developing mutation specific therapies is hence obviated and makes it more likely to be able to economically develop therapies for adRP patients. Dr. Kumar-Singh hopes to take this technology to clinical trials in the future.\nDr. Kumar-Singh is also working on the development of shRNA’s that target Vascular Endothelial Growth Factor (VEGF) for treatment of macular degeneration. Several clinically approved therapies targeting VEGF are currently available but each one is associated with severe side effects. Dr. Kumar-Singh hopes to develop shRNA based therapies for macular degeneration that are efficacious and long lasting.\nFinally, Dr. Kumar-Singh is working on the development of non-viral vector technology for gene transfer to neurons. Given that neurons are mitotically quiescent, the nuclear membrane does not dissolve and allow easy access to the nucleus. Overcoming these barriers is a key challenge for non viral gene therapy. Dr. Kumar-Singh is working on peptide mediated delivery of DNA to neurons. He believes that ultimately non viral gene transfer will be the most readily acceptable method to safely deliver genes to humans.\n1. Farrar, G.J., Jordan, S.A., Kenna, P., Humphries, M.M., Kumar-Singh, R., McWilliam, P., Allamand, V., Sharp, E. and Humphries, P. Autosomal dominant retinitis pigmentosa: localization of a disease gene (RP6) to the short arm of chromosome 6. Genomics 11:870-874, 1991.\n2. Kumar-Singh, R., Bradley, D.G., Farrar, G.J., Lawler, M., Jordan, S.A. and Humphries, P. Autosomal dominant retinitis pigmentosa: a new multi-allelic marker (D3S621) genetically linked to the disease locus (RP4). Human Genetics 86:502-504, 1991.\n3. Farrar, G.J., Kenna, P., Jordan, S.A., Kumar-Singh, R., Humphries, M.M., Sharp, E.M., Sheils, D.M. and Humphries, P. A three-base-pair deletion in the peripherin-RDS gene in one form of retinitis pigmentosa. Nature 354:478-480, 1991.\n4. Kumar-Singh, R., Jordan, S.A., Farrar, G.J. and Humphries, P. Poly (T/A) polymorphism at the human retinal degeneration slow (RDS) locus. Nucleic Acids Research 19:5800, 1991.\n5. Farrar, G.J., Kenna, P., Redmond, R., Shiels, D., McWilliam, P., Humphries, M.M., Sharp, E.M., Jordan, S., Kumar-Singh, R. and Humphries, P. Autosomal dominant retinitis pigmentosa: a mutation in codon 178 of the rhodopsin gene in two families of Celtic origin. Genomics 11:1170-1171, 1991.\n6. Farrar, G.J., Kenna, P., Jordan, S.A., Kumar-Singh, R. and Humphries, P. A sequence polymorphism in the human peripherin/RDS gene. Nucleic Acids Research 19:6982, 1991.\n7. Bleeker-Wagemakers, L.M., Gal, A., Kumar-Singh, R., Ingeborgh van den Born, L., Li, Y., Schwinger, E., Sandkuijl, L.A., Bergen, A.A., Kenna, P. and Humphries, P. Evidence for nonallelic genetic heterogeneity in autosomal recessive retinitis pigmentosa. Genomics 14:811-812, 1992.\n8. Farrar, G.J., Findlay, J.B., Kumar-Singh, R., Kenna, P., Humphries, M.M., Sharpe, E. and Humphries, P. Autosomal dominant retinitis pigmentosa: a novel mutation in the rhodopsin gene in the original 3q linked family. Human Molecular Genetics 1:769-771, 1992.\n9. Farrar, G.J., Kenna, P., Jordan, S.A., Kumar-Singh, R., Humphries, M.M., Sharp, E.M., Sheils, D. and Humphries, P. Autosomal dominant retinitis pigmentosa: a novel mutation at the peripherin/RDS locus in the original 6p-linked pedigree. Genomics 14:805-807, 1992.\n10. Jordan, S.A., Farrar, G.J., Kumar-Singh, R., Kenna, P., Humphries, M.M., Allamand, V., Sharp, E.M. and Humphries, P. Autosomal dominant retinitis pigmentosa (adRP; RP6): cosegregation of RP6 and the peripherin-RDS locus in a late-onset family of Irish origin.American Journal of Human Genetics 50:634-639, 1992.\n11. Humphries, M.M., Sheils, D.M., Jordan, S.A., Farrar, G.J., Kumar-Singh, R. and Humphries, P. Alu polymorphism in the human type I Keratin (KRT14) gene. Human Molecular Genetics 1:453, 1992.\n12. Farrar, G.J., Kenna, P., Jordan, S.A., Kumar-Singh, R., Humphries, M.M., Sharp, E.M., Sheils, D. and Humphries, P. Autosomal dominant retinitis pigmentosa: a novel mutation at the peripherin/RDS locus in the original 6p-linked pedigree. Genomics 15:466, 1993.\n13. Humphries, M.M., Sheils, D.M., Farrar, G.J., Kumar-Singh, R., Kenna, P.F., Mansergh, F.C., Jordan, S.A., Young, M. and Humphries, P. A mutation (Met-->Arg) in the type I keratin (K14) gene responsible for autosomal dominant epidermolysis bullosa simplex. Human Mutation 2:37-42, 1993.\n14. Jordan, S.A., Farrar, G.J., Kenna, P., Humphries, M.M., Sheils, D.M., Kumar-Singh, R., Sharp, E.M., Soriano, N., Ayuso, C. and Benitez, J. Localization of an autosomal dominant retinitis pigmentosa gene to chromosome 7q . Nature Genetics 4:54-58, 1993.\n15. Kumar-Singh, R., Wang, H., Humphries, P. and Farrar, G.J. Autosomal dominant retinitis pigmentosa: no evidence for nonallelic genetic heterogeneity on 3q. American Journal of Human Genetics 52:319-326, 1993.\n16. Kumar-Singh, R., Farrar, G.J., Mansergh, F., Kenna, P., Bhattacharya, S., Gal, A. and Humphries, P. Exclusion of the involvement of all known retinitis pigmentosa loci in the disease present in a family of Irish origin provides evidence for a sixth autosomal dominant locus (RP8). Human Molecular Genetics 2:875-878, 1993.\n17. Kumar-Singh, R. and Humphries, P. Isolation and genetic mapping of four microsatellite repeats from chromosome 3p21 using 40 CEPH pedigrees. Genomics 18:717-719, 1993.\n18. Kumar-Singh, R., Kenna, P.F., Farrar, G.J. and Humphries, P. Evidence for further genetic heterogeneity in autosomal dominant retinitis pigmentosa. Genomics 15:212-215, 1993.\n19. Mansergh, F.C., Jordan, S.A., Farrar, G.J., Kumar-Singh, R., Gal, A., Bhattacharya, S. and Humphries, P. Three sequence polymorphisms in the PDC gene. Human Molecular Genetics 3:2077, 1994.\n20. Kumar-Singh, R., Wang, H., Carritt, B., Kruse, T.A., McCarthy, T.V., Vergnaud, G. and Humphries, P. The EUROGEM map of human chromosome 1. European Journal of Human Genetics 2:204-205, 1994.\n21. Schurmann, M., Muller, B., Duvigneau, C., Leutelt, J., Krey, S., Kumar-Singh, R., Lush, M., Swallow, D.M., Vergnaud, G. and Bakker, E. The EUROGEM map of human chromosome 3. European Journal of Human Genetics 2:208-209, 1994.\n22. Terrenato, L., Jodice, C., Blasi, P., Loizedda, A., Contu, L., Buard, J., Vergnaud, G., Humphries, P., Kumar-Singh, R. and Massart, C. The EUROGEM map of human chromosome 6. European J of Human Genetics 2:214-215, 1994.\n23. Mansergh, F.C., Kenna, P., Rudolph, G., Meitinger, T., Farrar, G. J., Kumar-Singh, R., Scorer, J., Hally, A.M., Mynett-Johnson, L., Humphries, M.M., Kiang, S., and Humphries, P. Evidence for genetic heterogeneity in Best's vitelliform macular dystrophy. Journal of Medical Genetics 32:855-858, 1995.\n24. Kumar-Singh, R., and Chamberlain, J.S., Encapsidated Adenovirus Minichromosomes allow efficient delivery and expression of a 14Kb dystrophin cDNA to muscle cells Human Molecular Genetics 5:913-921, 1996.\n25. Hauser, M.A., Amalfitano A., Kumar-Singh R., Hauschka S.D., Chamberlain J.S. Improved adenoviral vectors for gene therapy of Duchenne muscular dystrophy. Neuromuscular Disorders 7(5):277-283, 1997\n26. Kumar-Singh, R., and Farber, D.B. Encapsidated adenovirus minichromosome- mediated delivery of genes to the retina: application to the rescue of photoreceptor degeneration. Human Molecular Genetics 7(12):1893-1900, 1998.\n27. Kumar-Singh, R., Tran, K., Yamashita, C. and Farber, D.B. Construction and use of encapsidated adenovirus minichromosomes (gutted vectors) for gene delivery to photoreceptors Methods in Enzymology316; 724-743,2000\n28. Cashman,S., Sadowski, S., Morris D., Frederick, J., and Kumar-Singh, R.Intercellular trafficking of Adenovirus delivered HSV VP22 from the retinal pigment epithelium to the photoreceptors-implications for gene therapy. Molecular Therapy6: 813-823, 2002\n29. Cashman,S., Morris, D., and Kumar-Singh, R. (2003) Evidence of protein transduction but not intercellular transport by proteins fused to HIV Tat in retinal cell culture and in vivo. Molecular Therapy 8: 130-142, 2003\n30. Cashman,S., Morris, D., and Kumar-Singh, R. Adenovirus type 5 pseudotyped with adenovirus type 37 fiber uses sialic acid as a cellular receptor. Virology324: 129-39, 2004\n31. Cashman, S. M., Binkley, E. A. and Kumar-Singh, R. (2005). Towards mutation-independent silencing of genes involved in retinal degeneration by RNA interference. Gene Ther12: 1223-1228.\nNew England Eye Center\n260 Tremont Street\nPhone: 617-636-4600 • Fax: 617-636-4866"", 'It is 21 years since the death of teenager Jesse Gelsinger nearly obliterated the field of gene therapy. In 1999, Jesse received a dose of the ornithine transcarbamylase gene, engineered into a recombinant adenovirus, at the University of Pennsylvania. The idea was for the gene to zero in on liver cells. However, soon after the treatment, he developed jaundice, inflammation, and multiple organ failure. Within four days, Jesse was dead.\nSince then, genetic technology has made gigantic leaps forward. The human genome has been sequenced in its entirety, and the cost of sequencing has plummeted 100,000-fold. CRISPR-Cas9 has emerged as a precise and efficient tool to edit genes. Artificial intelligence and deep learning are being put to use in designing molecules. And gene therapy has morphed from a blacklisted experimental concept to a life-altering practical solution for a growing list of previously incurable diseases.\nTo date, the U.S. Food and Drug Administration (FDA) has approved five gene therapy products for a half-dozen diseases (Table). Earlier this year, the FDA announced that it “anticipates many more approvals in the coming years, as there are more than 900 investigational new drug applications for ongoing clinical studies in this area.”\nThe ideal target for gene therapy is a Mendelian disease caused by defined single-gene mutations, such as hemophilia or Duchenne muscular dystrophy. That has been its initial focus, bolstered by approvals of drugs such as Luxturna (for a type of Leber’s congenital amaurosis, an inherited blindness disorder) and Zolgensma (for spinal muscular atrophy).\nNow, gene therapy is taking on more complex, multigene diseases such as central nervous system diseases, neuropathic pain, sleep apnea, and cancer. Beyond correcting genes, gene therapy is revolutionizing cell-specific delivery of therapeutic proteins and synthetic drugs, as well as inspiring basic research into unanswered questions on immunogenicity. But lest anyone think gene therapy has solved all of its earlier issues, the deaths last summer of two young boys in an experimental trial for a rare muscular disorder remind us that the field still has a lot of work to do to ensure gene therapy can be safely delivered for all patients.\nThe first order of business is to deliver therapeutic gene products into target cells. Vehicles used to transport genes into the cell include genetically modified viruses for long-term genomic integration or lipid nanoparticles for more transient tasks such as expressing genome editing nucleases.\nThe reason for gene therapy’s healthier outlook in 2020 is “improvements in the technology of gene transfer,” says James M. Wilson, MD, PhD, director of the Gene Therapy Program at the University of Pennsylvania. “Our work with adeno-associated viruses and the work of a number of other investigators, including that of Luigi Naldini, MD, PhD, in creating lentiviral vectors, were fundamental advances that paved the way for everything else.” He credits the clinical successes of his Penn colleague Jean Bennett, MD, PhD, on retinal blindness; the work of Nationwide Children’s Hospital’s Jerry R. Mendell, MD, on spinal muscular atrophy; and “the pivotal work on hemophilia out of St. Jude’s.”\nThe former vector favorite, adenovirus, has been largely supplanted by adeno-associated viruses (AAVs), generally used for in vivo gene therapy, and lentiviruses, used to modify cells outside the body, such as chimeric antigen receptor (CAR) T cells in the treatment of cancer. These ex vivo engineered cells are then amplified in tissue culture and returned to the patient.\nAAV-based gene therapy has made great strides in recent years. Pioneering work from the lab of Terence R. Flotte, MD, dean, provost, and executive deputy chancellor of the University of Massachusetts Medical School, has propelled clinical trials for two classic Mendelian disorders, alpha-1 antitrypsin (AAT) deficiency and Tay-Sachs disease. With excellent safety and durability established in a five-year follow-up of its Phase IIa trial, the AAT gene therapy is now in its final investigational new drug–enabling toxicology study stage.\n“AAT trials have shown some unusual tolerogenic properties,” says Flotte. “The study shows patients develop regulatory T cells against AAV coat proteins that prevent them from reacting negatively even without pharmaceutical immune suppression.”\nNearly 75% hemophilia patients take regular intravenous infusions of a recombinant factor VIII (F8) protein coupled with other drugs that prevent bleeding or the breakdown of clots. “Current treatment for hemophilia A is very burdensome to patients,” says Bettina M. Cockroft, MD, chief medical officer at Sangamo Therapeutics.\nA new therapy being developed by Sangamo and Pfizer injects a recombinant AAV6 encoding F8, which could provide a permanent solution. “The gene therapy for F8 shows lasting activity,” Cockroft reports. “Five patients tested in Phase I/II of the clinical trial did not need F8 infusions and had no bleeding events.”\nVectors injected into blood generally don’t get across barriers. Wilson’s lab, in collaboration with Amicus Therapeutics, is focusing on developing penetrating next-generation AAVs that deliver more efficiently to cells in organs such as the muscle, heart, and brain. “With the new AAVs,” notes Wilson, “you can get more delivery with less vector. Less vector would mean less toxicity.”\nPatient and public support for gene therapy is crucial for its advance. “I have a pretty good idea of how stakeholders view gene therapy,” Wilson asserts. “Because gene therapy is primarily focused on treating and potentially curing diseases of significant unmet need that are disabling or lethal, I see incredible enthusiasm for us to move forward.” Public and private ventures have invested heavily in the promise of curing genetic disease by altering or replacing faulty genes.\nThe change in the academic, industrial, and public outlook on gene therapy is readily appreciable in the reaction to the recent tragic deaths of two young boys in a gene therapy trial, sponsored by Audentes Therapeutics, for X-linked myotubular myopathy (XLMTM), despite impressive efficacy in preclinical studies, as discussed in a recent episode of GEN Live (“Gene Therapy: What’s Up With AAV?”). Instead of blacklisting the entire field of gene therapy as was seen decades earlier, the trial has been put on temporary hold while researchers reevaluate data so that such fatalities can be avoided in the future.\n“Audentes has elaborated further on the data surrounding the deaths in a recent letter to Human Gene Therapy,” says Flotte, the journal’s editor-in-chief. The presence of comorbidities, particularly liver disease, and prior exposures to the virus in these patients might have played a role, in addition to technical challenges, such as the lack of standardized assays for measuring levels of preexisting viral antibodies. Ongoing preclinical investigations are focused on establishing safer doses and understanding immune mechanisms in response to AAV that might exacerbate existing liver dysfunctions.\n“In the long term,” says Nicole Paulk, PhD, assistant adjunct professor, biochemistry and biophysics, UCSF, “we need to shift our focus from ‘How can we safely use high doses?’ to ‘How can we design the vector so we don’t have to?’” Limiting the viral dosage necessary for maximal efficacy in gene therapies is being evaluated through experimental approaches, such as engineering more penetrating capsids, introducing regulatory elements for tissue-specific viral expression, developing better manufacturing and purification processes to increase the purity of virus injected, and devising strategies to avoid immune responses.\nMore than correcting genes\nFuture applications of gene therapy technology for more common, polygenic diseases will not necessarily treat defective genetic components directly. Instead, these applications may improve the delivery of proteins that are otherwise used to treat diseases. An example is RGX-314,a developed by RegenXbio, for the treatment of neovascular age-related macular degeneration, where unwanted blood vessels form in the retina that result in blindness. This gene therapy uses a NAV AAV8 vector that encodes a vascular endothelial growth factor (VEGF)-neutralizing monoclonal antibody fragment. A one-time subretinal injection of RGX-314 is enough for the sustained synthesis of anti-VEGF therapeutic protein and circumvents the need for repeated intravitreal injection VEGF-neutralizing proteins.\nModifier gene platform:\nOphthalmologist Mohamed Genead, MD, chair of Ocugen’s retina scientific advisory board, is excited about a revolutionary new approach—the modifier gene platform—that takes a “gene-independent approach” by overexpressing upstream regulatory genes for nuclear hormone receptors (NHRs) in retinal cells instead of attempting to correct mutations in individual genes.\n“Overexpressing NHRs does not correct the gene mutations; instead, it corrects the disease phenotype,” says Genead. “Conventional gene therapy has several limitations. The amount of DNA you can package into the virus is restrictive. Gene therapy can work if you have identified the mutation, but most genetic diseases do not have identified mutations.”\nNHR overexpression regulates endoplasmic reticulum stress to promote normal function in cells with defective genes. “Its efficacy remains to be shown in the clinical trials,” comments Genead, “but if we see in patients what we see in animal models, there will be a significant shift of the treatment paradigm in patients with retinal degeneration.”\nContinuous positive airway pressure machines or surgery are the currently available treatments for obstructive sleep apnea, a condition where the tongue loses muscle tone and blocks the upper airway in sleeping patients. Consequences range from fatigue and metabolic disruptions to stroke and death.\nAn alternative approach is being explored by researchers at Johns Hopkins Medical School. They are evaluating whether DREADDs—designer receptors exclusively activated by designer drugs—can be used to activate the genioglossus muscle, and thereby improve breathing during sleep.\n“We are inserting designer receptors for synthetic drugs in specific brain cells to maintain patency of the airways,” says Thomaz Curado, MD, PhD, research associate at Johns Hopkins. “If translated successfully in human clinical trials, this can mean a one-time injection combined with a nightly pill will eliminate risk of death due to obstructive sleep apnea.”\nConferring resistance to infectious diseases:\nSingle-gene disorders and infectious diseases are being treated using the same tools and concepts. “We are in the midst of a very aggressive program in using AAV as a way to prevent infections of COVID-19,” says Wilson. Essentially, AAV vectors are, as usual, being used to engineer cells in patients to express proteins. In this program, however, the proteins confer resistance to COVID-19.\nAAVCOVID, a gene-based vaccine that will deliver genetic sequences of the SARS-CoV-2 in an AAV capsid, is being developed jointly by the Gene Therapy Program at the University of Pennsylvania and the Massachusetts Eye and Ear and Massachusetts General Hospital, and it is scheduled to enter clinical trials later this year.\nSafety, success, and staying on\nLacunae in the basic understanding of how the immune system responds to foreign vectors are currently at the forefront of investigations on toxicity and inflammation induced by gene therapy.\n“What factors predispose complement activation and the hemolytic-uremic syndrome at high-doses of rAAV? What factors predispose to inflammation in the dorsal root ganglia and spinal cord after direct rAAV-mediated gene delivery into the central nervous system? What is the nature of liver toxicity in patients treated with high doses of rAAV gene therapy? [These] are the frontier questions,” says Flotte. “How significant are immune responses to Cas9 and other nucleases? How important is the toxicity from off-target gene editing effects? [These] are questions facing gene editing in the context of gene therapy.”\nOne of the limitations of the ubiquitous AAV vector has been the mounting of an antibody response against the vector itself, according to Federico Mingozzi, PhD, chief scientific officer at Spark Therapeutics. Hence the interest in imlifidase (IdeS), an endopeptidase that degrades circulating antibodies without disrupting B lymphocytes that produce antibodies. The enzyme, already in the clinics for transplantation patients, has two main benefits for gene therapy. It can be used to treat seropositive patients, and it permits the re-administration of the gene therapy product if the need arises.\n“Once you give IdeS, you see a sharp and immediate drop in circulating antibodies. They stay low for 3–4 days and then go up again,” says Mingozzi. “Of course, it has to be tested in humans in conjunction with gene therapy, but it has a very good safety profile.”\n“The reality is that we are at the beginning of a revolution,” declares Wilson. “The forward move for gene therapy is to make it safer and more effective.” Making it durable depends on the context but depends on three key factors: The genome must be stable, the cell has to persist, and therapeutic gene expression cannot be turned off. “Of these three,” says Wilson, “I think the major challenge is going to be to assure that its expression is ongoing.”\n“Durability is a major consideration for rAAV gene therapy and is intertwined with immune and inflammatory toxicity,” adds Flotte. “It is less of a concern in gene editing, which involves permanent changes in the genome.”\nAccording to Laura Hercher, director of student research, Human Genetics Program, Sarah Lawrence College, “the main roadblock is going to be price and accessibility. Unfortunately, in countries where there isn’t that kind of money to spend on a single patient, it is only going to be available to the very wealthy.”\nA new nonprofit called the Institute for Life Changing Medicines aims to secure, develop, manufacture, and distribute life-changing medicines such as gene therapy at cost. “Ultimately, we want to provide access in countries where the diseases might not be rare, but the governments who are responsible for reimbursements simply won’t pay,” says Wilson. “I’m going to do everything I can to the last day I’m on my job to assure [global access]. What we are trying to do to assure global access is to be very efficient in clinical development and manufacturing to contain cost.”\nFollowing a No Disease Left Behind strategy, the nonprofit is currently committed to providing gene therapy solutions for Lesch-Nyhan syndrome, aromatic L-amino acid decarboxylase deficiency, and Crigler-Najjar syndrome type I—diseases where gene therapy has a proven transformative effect.\naThe original text left the mistaken impression that RegenXBio produces Lucentis. The text has been modified to make clear that RegenXBio produces RGX-314. Lucentis (also known as ranibizumab) is a similar but not identical anti-VEGF produced by Genentech/Roche.']"	['<urn:uuid:c8fceaf0-2ea7-4a26-a336-7685911f8bb4>', '<urn:uuid:21fd3d5e-2d0f-4030-8c00-11cd3b744150>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T05:06:54.527160	12	43	3809
14	What were British Falklands War helicopter challenges and Argentine air response	The British faced severe helicopter limitations in the Falklands War, struggling with logistical transport across rough terrain and losing most of their vertical lift capability when SS Atlantic Conveyor was sunk. In response to these operations, Argentina later developed the A-4AR Fightinghawk program in the 1990s, merging A-4 Skyhawk characteristics with F-16 instrumentation to modernize their air capabilities, though only six of the original 36 planned aircraft remain operational today.	['How does one stop a revisionist autocracy from invading its island neighbor? If the invasion lands on this nation’s shores, how does one force the withdrawal of an adversary with local numerical superiority while operating at hundreds or even thousands of miles from main logistical hubs? In 1982, the British had to solve this exact problem when the military junta of Argentina invaded the Falkland Islands after years of claiming sovereignty over the British Overseas Territory. There are disquieting parallels between Operation Corporate, the British victory in its long-range, expeditionary operation to recapture the Falklands from Argentina and the challenges focused around the defense of Taiwan from China.\nThe U.S. Marine Corps has made it a priority to address the rise of great-power competition in the Indo-Pacific. British forces in the Falklands operated in a similar manner to how the commandant envisions marines operating in the future: small formations distributed across vast expanses of maritime terrain, relatively limited indirect fire support, and limited traditional close air support. Vertical lift aircraft were critical to enabling British maneuver and logistical sustainment in the South Atlantic. But these aircraft are largely absent from new Marine Corps concepts.\nTo address these discrepancies, I offer a brief overview of relevant lessons learned during Operation Corporate. After capturing these lessons learned, I turn to ways to better incorporate them into the Marine concepts, specifically focused on maximizing current and future vertical lift capabilities.\nOn Apr. 2, 1982, the armed forces of Argentina invaded the British territory of the Falkland Islands in the South Atlantic. Successive British administrations had concluded that any landings by Argentina would represent a fait accompli with little room for recourse. The government of Prime Minister Margaret Thatcher, however, quickly committed itself to returning the Falkland Islands to the United Kingdom. Within hours, Operation Corporate was set in motion to drive Argentina from the rocky archipelago. The first naval vessels departed Britain less than three days after the invasion. A rapidly formed, combined task force overcame significant hurdles and transited over 8,000 miles to finally regain possession of the islands by June 14. Victory was not guaranteed. From the moment the task force set sail, a failure to anticipate vertical lift requirements, ambiguous command relationships, and a force design that marginalized amphibious operations threatened ultimate success.\nThe British operation had to overcome vast distances and associated challenges (Source: Department of History, US. Military Academy)\nThe British task force was primarily comprised of a carrier battle group to establish air and sea superiority, and an amphibious assault group, which included 3 Commando Brigade, was tasked with recapturing the islands. The rapid deployment of the British naval task force was impressive, but was initially conducted to force a political settlement with the hope that a military solution would prove unnecessary. As the British task force sailed from the historic ports of Sir Francis Drake and Adm. Lord Horatio Nelson, its logistical supplies and equipment were largely stowed without consideration for an eventual amphibious operation. A significant logistical reorganization was required at Ascension Island, the lone intermediate firm base available to the British. Utilizing surface vessels to reshuffle equipment should have proven a simple task for the Royal Navy, but the steep gradients, soft sand, and heavy surf of the beaches required extensive use of helicopters for ship-to-shore movement. Even after landing equipment at Ascension Island, the rough terrain meant that helicopters were essential to inland transportation. With minimal restow and staff coordination complete, the task force sailed toward their objective where unforgiving climate and terrain would require even more aviation support.\nAir defense proved to be a vital consideration throughout Operation Corporate. The decision to land elements of 3 Commando Brigade only after the carrier battle group established air superiority was ultimately deemed impractical and the risk of Argentine aircraft was accepted for the sake of expediency. Argentina possessed six times the number of aircraft of the British task force and the air defense systems organic to the Royal Navy could not provide adequate force protection to troops after landing ashore. The ground-based Rapier air defense system was intended to mitigate the risk of air attack in the absence of air superiority. The terrain best suited for the Rapier to perform this function was inaccessible by ground vehicles and the system itself was too heavy to be hand-carried. As a result, the commander of the amphibious battle group, Michael Clapp, was compelled to dedicate limited assets “to supply the Rapiers with one Sea King on permanent call for the delivery of stores and petrol for their generators.” For a helicopter force that would become heavily taxed, the reliance on these aircraft only increased as Britain transitioned from the amphibious landing to offensive operations ashore.\nThe geography of the Falkland Islands limited the number of suitable landing locations. San Carlos Water in the northwest of East Falklands was selected as the site for the landing. On May 21, the first elements of 3 Commando Brigade landed, but the buildup of the beachhead was delayed due to unresolved inefficiencies in logistical stores as well as the force protection measure of constantly moving surface vessels in and out of San Carlos Water. The only means to maintain momentum under these circumstances was a constant use of helicopters. From the initial landing until the eventual capitulation of Argentina on June 14, “Helicopters remained vital to logistics operations during the war because of the rough, trackless terrain of East Falkland.” When air-launched Exocet missiles sank SS Atlantic Conveyor on May 25, all but one of its critical cargo of additional helicopters was lost, further challenging the limited mobility of the ground force. The lack of adequate roads to traverse the rocky marshland ensured that the limited helicopter force was occupied externally transporting all artillery and heavy equipment while British marines and soldiers were left to march over the unforgiving landscape. As a stark illustration of the requirements on helicopters, it took 82 Sea King sorties to transport a single battery of six 105-millimeter howitzers and its required ammunition. This reduction in maneuver assets undoubtedly extended the conflict as a majority of helicopters were allocated to transporting equipment and not personnel.\nThe Falklands campaign (Source: Department of History, U.S. Military Academy)\nCommand relationships between inter-service units, and the inadequate allocation of helicopter assets for even a single brigade, were a tangible point of friction as the task force grew. The Royal Marine brigade fielded roughly 4,600 personnel, which the Ministry of Defence determined was too small to face the 10,000 Argentinians on the Falklands. With the addition of 5 Infantry Brigade, Maj. Gen. Jeremy Moore would command a divisional headquarters within the task force. Most of the battalions in this brigade were comprised of soldiers who split time between operational and ceremonial guard duty with no training in amphibious operations. This additional brigade represented an increase in overall numbers but produced confused command relationships that contributed to the single largest loss of British troops since World War II.\nWhen it was still believed a political solution was possible, little effort was made to define the exact role of 5 Infantry Brigade. The question of what to do with these troops was answered when 3 Commando Brigade conducted its first offensive actions after landing at San Carlos Water. The initial success at the settlement of Goose Green prompted 2nd Battalion, Parachute Regiment to bound well forward of logistical support, leaving them vulnerable to enemy attack without the possibility of rapid reinforcement. An ill-constructed support request process and lack of helicopter expertise on the amphibious staff allowed the ambitious battalion to requisition helicopter support that was sorely needed elsewhere. In response, elements of 5 Infantry Brigade would be landed at the settlements of Bluff Cove and Fitzroy to relieve the isolated paratroopers. The misallocation of helicopters required surface connectors to transport these soldiers despite their lack of amphibious experience. The 35-mile movement in open landing craft took several hours as soldiers were exposed to the frigid South Atlantic climate and Argentine air attack. As the sun set on June 8, multiple surface connectors were attacked, LSL Sir Galahad was sunk, and 51 soldiers and sailors were killed with 46 wounded.\nTo better understand how such tragedies happened, it’s important to go back in time to the years immediately following World War II. Inter-service competition hit the Royal Navy particularly hard in this period with the cancellation of an updated carrier program in 1966 and the decommissioning of Britain’s last catapult carrier — HMS Ark Royal — in 1979. This had cascading effects on the amphibious capabilities of the navy in the years to come. The only two remaining carriers capable of fixed-wing operations were HMS Hermes and HMS Invincible, whose small decks utilized vertical take-off and landing Sea Harriers at the expense of helicopter operations.\nThatcher inherited budgetary constraints that contributed to an increasingly niche view of the Royal Navy’s purpose. Her government’s 1981 Defence White Paper recommended the removal of all amphibious vessels by 1984. Britain’s belief that the unlikely requirement for amphibious capabilities would only be used as part of a larger NATO operation degraded the readiness of the Royal Navy. The two fixed-wing carriers were scheduled for sale to foreign militaries with the landing platform dock, HMS Intrepid, already in the process of being decommissioned. Fortunately for Britain, Argentina did not wait for the full impact of the projected changes in British amphibious capabilities. The sale of the carriers was delayed and the decommissioning of HMS Intrepid reversed for the use of its associated landing craft and medium-lift helicopters.\nIn hindsight, the gap between the British and Argentine capabilities appears inevitable. However, no amount of military professionalism could have bridged the 8,000-mile gap between Britain and the Falkland Islands without the appropriate means to not only arrive in theater, but move troops, supplies, and equipment across the inhospitable terrain of the islands. Had the conflict been delayed by a matter of months, a complete lack of amphibious vessels and its associated helicopters would have made repossession of the Falkland Islands impossible. Much has been written regarding the fortuitous timing of the invasion prior to the final transfer of Britain’s aircraft carriers. The carrier battle group, however, failed to accomplish its main task: air superiority. In the end, it was the essential mobility that vertical lift aviation provided that ensured that the Falkland Islands returned to British control.\nImplications for Force Design 2030\nThe commandant of the Marine Corps’ 2019 planning guidance lays out an ambitious, but necessary, plan to mitigate the threat of emerging peer adversaries. In each year since, his annual Force Design 2030 updates have built on this initial guidance, setting the service on a path which is directly in line with strategic guidance from the White House and the Department of Defense. Multiple conceptual documents have informed the commandant’s guidance. The concept of expeditionary advanced base operations seeks to mitigate potential adversary advantages “by improving our own ability to maneuver and exploit control over key maritime terrain.” Incorporating the lessons of the British experience with helicopter utilization in the Falklands is critical to ensuring that the Marine Corps does not repeat similar missteps during the maneuver and sustainment of its own distributed forces.\nIt is critical that vertical lift aviation is better integrated into maritime mobility. Utilizing surface vessels to land personnel and equipment ashore is not guaranteed. Artist’s renderings of conceptual Light Amphibious Warships depict the offloading of equipment on pristine beaches that the example of Ascension Island demonstrates as problematic. The Falklands War also exhibited that the requirement for mobility does not end at the shoreline. Excluding four light tanks, all supplies, artillery, and air defense systems were light enough to be transported by helicopter, demonstrating the rapid mobility that aviation provides. Restrictive inland terrain or a lack of existing airfields remains a consideration to Marine planners, particularly on islands throughout the Indo-Pacific that often lack large airfields and improved road infrastructure. CH-53E/K helicopters provide a unique capability to rapidly transport radars, mobile air defense systems, and ordnance for long-range precision artillery critical to new concepts. The air-refuellable capabilities of these platforms enable operations at distances exponentially greater than those covered by British helicopters. A 100 percent increase in active component KC-130 tanker transport squadrons stationed in the Pacific will provide range extension to CH-53E/K and MV-22 Osprey aircraft that provides the option to self-deploy from outside the First Island Chain to distributed expeditionary bases within it. The ability to overcome the tyranny of distance in vertical lift aircraft is not without precedent. Twice in 1989, MH-53 helicopters utilized aerial refueling to transit nearly 1,400 miles — roughly the distance between Guam and Taiwan — without landing to conduct combat operations in Panama. Operating at such ranges would clearly be the exception, but it provides Marine commanders with the option to rapidly employ vertical lift aviation when the threat, terrain, or alternative asset allocation prove prohibitive.\nProjected Marine command relationships could challenge the service’s utilization of vertical lift aviation, likely in ways that mirror the friction that the British Task Force experienced in 1982. The Marine littoral regiment was developed through the Force Design 2030 process with expeditionary advanced base operations in mind. The commandant has made it clear that this new unit is not the only entity that can conduct such operations, but the basing of current and future Marine littoral regiments within the Pacific ensures that they will conduct a preponderance of the new concept. Tentative doctrine only roughly describes aviation in a general support role to the littoral force without outlining the source of these aircraft. The emphasis of the tentative manual and “A Concept for Stand-in Forces” on aviation fires and intelligence, surveillance, and reconnaissance over vertical lift threatens to replicate Britain’s inefficient utilization of helicopters in 1982. Moreover, with no organic aviation assets capable of vertical lift, a Marine littoral regiment’s reliance on composite squadrons of a Marine expeditionary unit to support their maneuver will prove unsustainable. Not enough vertical lift will exist to support operations of both these units concurrently as the only aircraft between the two elements is deliberately sized to support Marine expeditionary unit operations. Air wings allocating squadrons to current deployment cycles will find it difficult source additional tasking, especially in a CH-53 community reduced by 35 percent. This reduction was consciously designed to coincide with a concurrent reduction of infantry battalions by 13 percent. However, it was not simply the number of vertical lift platforms available, but the ad hoc command relationships that threatened the success of the British task force in 1982. The lack of defined support relationships within any current doctrinal or tentative publications between a commander of the landing forces and a commander of the littoral force would complicate any amalgamation of their respective entities. Wherever the source, clear allocation of aviation support during expeditionary advanced base operations, specifically vertical lift, should be defined moving forward.\nThe overwhelming emphasis on mobility contained in “A Concept for Stand-in Forces” ignores the stark disparity in speed, range, and flexibility of aviation compared to proposed surface vessels that the U.S. Navy is hesitant to fund. In fact, despite the critical role that it anticipates mobility will play within maritime terrain, vertical lift aviation is never once mentioned in this document. Any reliance on nonexistent logistical drones without continuing to integrate currently fielded air-refuellable, long-range vertical lift platforms further risks the viability of these new concepts. Offensive and reconnaissance drones have emphatically demonstrated their utility not only in operational tests conducted by the Marine Corps, but also in combat in Ukraine. The ability for unmanned platforms to logistically sustain marines in an environment envisioned by the commandant is less certain. While more recent unmanned concepts have been proposed, one of the more capable unmanned vertical lift platforms is the Kaman K-MAX helicopter. During a 33-month experiment in Afghanistan in 2011 it proved capable against an insurgent adversary, but its speed of 80 knots, one-way range of 267 nautical miles, and payload of 6,000 pounds is overwhelmingly outperformed by both the MV-22 and CH-53E/K. At a fraction of the price of manned vertical lift, unmanned systems would only provide a fraction of the logistical support. These alternatives to sustain marines throughout the Pacific should continue to be developed, but currently a capabilities gap limits these options to a supporting role.\n“Technological advance is sharply changing the defence environment. The fast-growing power of modern weapons to find targets accurately and hit them hard at long ranges is increasing the vulnerability of major platforms such as aircraft and surface ships.” One might believe this passage describes the current global environment, but this quote from Thatcher’s secretary of state for defence demonstrates that many of the considerations shaping the Marine Corps were relevant forty years ago.\nThe Marine Corps should take concrete steps to learn from the British experience during Operation Corporate. First, the Marine littoral regiment should capitalize on the inherent mobility and flexibility of vertical lift aviation through fully integrated training at long range in maritime terrain. Next, feasible command relationships between aviation and ground forces in distributed operations should be developed and clearly codified in evolving doctrine. Finally, the Marine Corps should continue to develop innovative, unmanned surface and air platforms with the understanding that existing systems must be leveraged until these technologies are fully fielded. Successfully incorporating vertical lift aviation into the commandant’s vision of the Marine Corps will prove challenging, but the negative impact of relying solely on alternative mobility options will prove to be untenable.\nNolan Vihlen is a CH-53E instructor pilot currently assigned to Marine Aviation Weapons and Tactics Squadron One and a recent graduate of Expeditionary Warfare School. The opinions expressed are those of the author alone and do not reflect those of the U.S Marine Corps, the Department of Defense, or any part of the U.S. government.\nImage: IWM FKD 2744', 'After the Falklands War, the Argentine Air Force moved forward with an upgrade program for its main combat aircraft. This allowed the Mirage weapon system, made up of Israeli Mirage IIIs and IAI Daggers (later upgraded to the Finger program), to fly until 2015. However, in the 1990s the Argentine government and the FAA undertook an ambitious project to incorporate a modern fighter-bomber that led to the development of the A-4AR Fightinghawk.\nWe are talking about a truly unique piece of engineering, and for different reasons. In the first place, because it involved merging the characteristics of different existing aircraft into one. But also because it was the last great project of the international arms industry designed specifically for the Argentine Air Force.\nThe A-4AR Fightinghawk has become a cult object among fighter aircraft enthusiasts. Today it is the spearhead of the Argentine military aviation, but unfortunately in recent years the number of operational units has fallen drastically.\nOf the more than 30 that were available in the late 1990s, only six are still active. And while hard work is being done to recover many of those who are out of service, Argentina’s complex economic situation makes the process slower and more difficult.\nA-4AR Fightinghawk, much more than the modernization of an old acquaintance\nAs you may have noticed from the name of the aircraft and its design, the A-4AR Fightinghawk has its origins in the Douglas A-4 which entered service in 1956. However, it is not just a modernized variant like so many others that have been launched since its introduction.\nWe are talking about a development that merged many of the good features of the A-4, an aircraft widely used by both the Air Force and the Argentine Navy, with the modern instrumentation of the F-16.\nIn fact, that the A-4AR bears the name Fightinghawk is not on a whim. In recognition of their “hybrid” nature, it was decided to merge the distinctive Fighting Falcon and Skyhawk designations of the F-16 and Douglas A-4, respectively.\nHowever, the original idea of the Argentine Air Force to modernize its fighter fleet in the 1990s was different. With the implementation of the Convertibility Law that established that 1 Argentine peso was equivalent to 1 dollar, added to the foreign policy of the government of then President Carlos Menem, which proposed a clear rapprochement with the United States, the initial plans were somewhat more grandiose. .\nAccording to records from 1991 and 1992, the initial intention was to add units of the F-16 Fighting Falcon, the F-15 Eagle or even the F-18 Hornet. However, none of this happened.\nThe plan quickly mutated into the acquisition of a batch of A-4M Skyhawk He belonged to the United States Marine Corps. They would be handed over to Lockheed Corporation—later Lockheed Martin—for major rebuilding and modernization, using the F-16’s avionics. Let’s not forget that this company had acquired the aircraft manufacturing business of General Dynamics, so it had been left in charge of the production of that fighter.\nThe original cost of the program, in 1994, was estimated at 365 million pesos/dollars and encompassed the production of 36 A-4AR Fightinghawks. However, this figure was later cut due to economic problems caused by the “Tequila Effect”. This forced to dispense with some characteristics of the final equipment of the fighter-bombers, among other issues.\nIn principle, 18 units would be produced in the United States and as many in Argentina, although finally the North American versions were less. As part of the agreement, it was agreed to the privatization of the Military Aircraft Factory located in Córdobawhich passed into the hands of Lockheed Martin in mid-1995.\nSingle and two-seater models\nAlthough all units of the fighter-bomber produced for the Argentine Air Force are commonly referred to as the A-4AR Fightinghawk, there are actually two versions: a single-seater and a two-seater. Of the first 32 were manufactured, while the remaining four correspond to the second variant. The latter bears the formal name of OA-4ARbecause its development follows from the Douglas OA-4M Skyhawk.\n|Length||12.76 meters (13.17 meters on the OA-4AR Fightinghawk)|\n|Crew||1 (2 on OA-4AR Fightinghawk)|\n|Engine||Pratt & Whitney J52-P-8A (Single Seat)\nPratt & Whitney J52-P-408A (Two-seater)\n|maximum takeoff weight||11,113 kilograms|\n|flight ceiling||13,000 meters|\n|Maximum operating speed||1,300km/h|\n|Armament||Two Colt Mk 12 automatic cannons with a capacity of 200 ammunition each. It also has five hardpoints under the wings and fuselage that allow the use of a varied configuration of bombs of up to 500 kilograms, rocket launchers and AIM-9 Sidewinder air-to-air missiles.|\nSt. Louis, the home of the A-4AR Fightinghawk\nThe delivery process of the A-4AR Fightinghawk began in 1997 and ended in early 2000. Since then, all units of the fighter-bomber have been operated by the Argentine Air Force Fighter Group 5. As it works in the V Air Brigade of Villa Mercedes, in San Luis, this has earned it the nickname “Cradle of Falcons”.\nAll photos courtesy of Martín Otero.\nUnfortunately, despite the fact that they have only been in service for 25 years, their operational capacity has been drastically reduced over time. Four units were completely destroyed in separate incidents — two in 2005, one in 2013 and one in 2020 — while another was damaged when it hit a hangar at its base in 2015.\nArgentina’s recurrent economic problems have also taken a toll on the maintenance of the A-4AR Fightinghawks, which were temporarily inoperative in early 2016. The then Minister of Defense, Julio Martínez, confirmed that the aircraft could not fly due to corrosion problems and lack of vital spare partssuch as cartridges for ejection seats, among others.\nIt was even known that the intention was to deprogram them in 2018, which ultimately did not happen. At present, only six units are active, although a very promising A-4AR Fightinghawk weapon system recovery process has begun, with the appearance of possible new suppliers and the addition of more of its own capabilities.\nAlthough the coronavirus pandemic and the devaluation of the peso have taken their toll, the Río Cuarto Material Area, in Córdoba, is currently working to put a seventh unit in operating condition. However, the plans are much more ambitious. The Argentine Air Force wants to return to having about 12 units in operation in the medium termto later recover six more and reach a fleet of 18 fighter-bombers.\nDespite its operational and budgetary limitations, the A-4AR Fightinghawk is the only “powerful” combat aircraft possessed by the Argentine armed forces. It is true that the Air Force has incorporated units of the IA-63 Pampa IIIproduced in FAdeA —name received by the aircraft factory in Córdoba after being nationalized again at the end of 2009—, but it is an advanced training aircraft with light attack capabilities.\nWhile the Argentine Navy is waiting to be able to fly the Super Étendard Modernise acquired in 2019. They have not yet been able to take off due to the UK veto on the provision of pyrotechnic cartridges for ejection seats.\nWill the A-4AR be able to regain its glory? Today Argentina does not have many alternatives. In recent months, representatives of the Air Force have been in China evaluating the JF-17 Thunder fighter as a possibility to incorporate. However, the possible interest in acquiring said aircraft would be more to regain supersonic capability lost with the loss of the Mirage weapon system, than in getting a direct replacement for the Fightinghawk.']	['<urn:uuid:03369ad7-1ac1-48ba-be1e-6e4b6d71e241>', '<urn:uuid:9d784b73-430e-4991-8c2c-50d5da09f7b1>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T05:06:54.527160	11	70	4223
15	Which plant species requires more frequent watering during their early growth stage: proteas or azaleas planted in sandy soils during warm weather?	Azaleas planted in sandy soils during warm weather require more frequent watering, needing water twice a week during the first year. In contrast, proteas should be watered only once a week for an hour (though more often when young), as shorter, more frequent watering can encourage weak root systems and fungal infection.	"[""Growing Proteas from seed in an eggbox\nProtea Eggbox Starter Kit\nThese instructions are the ones we put into the eggbox starter kits that we sell at Kirstenbosch Gardens. Unfortunately, because this starter kit contains soil, it cannot be imported into other countries. However, if you purchased the envelope-sized Starter Kit (without soil), you can have a look here to see how it might be done once you have made up your own soil mix.\nJulie has grown these seeds on our (south-facing = away from the sun in South Africa) kitchen window sill (inside). There were times when she forgot to water them, and when the cat knocked them out the window for a day or so, but as the photos show, the plants grew.\nPut the Protea Fynbox Mix Soil into the Egg Box\nDistribute the supplied soil mixture evenly into the little cups of the egg box.\nSoak the protea seed in Smoke Seed Primer\nA detailed description of how to treat your fynbos and protea seed with Smoke Primer is described in our Smoke Primer in Protea Seed Germination page.\nPlanting your Protea seed\nPlant both seeds of each species into each eggcup.\nPlant the seed to a depth equal to its size and water well.\nKeep the seedlings in semi-shade, and protect against rodents, birds and large insects (30% shade-cloth works well for outdoors - a window sill works well indoors). Water with a fine rose spray and do not allow the soil to dry out at any time.\nIn climates with harsh winters, you should place the seedlings in a greenhouse during the winter. Keep the temperature at 5-8oC at night and 15-20oC during the day. Humidity must be kept low with circulation of cool air. Extra light may be needed in areas of extreme latitudes.\nThe germination period varies from 1 to 3 months, depending on the species. The cotyledons appear first, then the true leaves. Once the true leaves appear, the seedlings can be exposed to full sunlight.\nPlanting Out your Protea seedlings\nWhen the seedlings are about 1cm tall cut out each cup and plant the cup straight into the ground, a pot or a larger seed bag. To give the roots unhindered access to deeper soil, carefully remove the bottom of the egg cup. The rest of the egg box will decompose naturally, feeding the young plant and allowing the roots to grow through.\nIf both seeds germinate, you should seperate them carefully with minimum root disturbance.\nTake care to leave the roots undisturbed during planting out. If your garden soil is well drained and acidic, and your climate frost-free, choose a sunny spot and plant the proteas out in square holes 500 mm deep. The young plants appreciate some well-matured compost (non animal) mixed into the soil with which you fill the hole.\nMulching with well-rotted compost or wood chips helps keep down the weeds, retain moisture, cool the roots and supply some nutrients.\nIf you need to keep the proteas in a pot, you must make up your own potting medium. This should be similar to the mixture you used for germinating the seeds, but needs more perlite or vermiculite and some well-matured compost. Ensure that the pot's drainage holes are open.\nFeeding your Protea\nSince the proteaceae are adapted to nutrient-poor conditions, chemical fertiliser or manure will burn their sensitive root system.\nThe starter kit contains a small amount of a specially formulated fertilizer that is designed for fynbos and Proteaceae. Sprinkle this around your proteas when they are around 6 to 9 months old and water well. You can buy more of this fertilizer from the Kirstenbosch garden supply shop, or directly from our protea growing accessories store.\nFungus poses a serious threat to proteas, and causes sudden death in some species if it attacks the roots. To avoid exposing the plants to fungal infections, keep the plants dry when it is hot. This means water the plants only early in cool of the morning - never in the evening. Also, water the plants once a week for an hour (more often when they are young). Shorter, more frequent, watering encourages weak root systems and fungal infection."", 'Revised by Joey Williamson, HGIC Horticulture Extension Agent, Clemson University, 09/15. Originally prepared by Debbie Shaughnessy, HGIC Information Specialist; Bob Polomski, Extension Consumer Horticulturist; and Donald L. Ham, Extension Urban Forestry Specialist, Clemson University. New 09/00. Image added 09/15.\nAzaleas are Southern signature plants in South Carolina landscapes. Numerous azalea species, hybrids and cultivars, either native to this area or hailing from the Orient, can be grown here, with bloom times ranging from early spring to midsummer. The numerous cultivars of the Encore™ series of azaleas offer blooms twice a year, in the spring and again in the fall. Azaleas are classified in the genus Rhododendron.\nThe ‘Formosa’ Southern Indica Azalea produces large blooms.\nJoey Williamson, ©2013 HGIC, Clemson Extension\nIn order for azaleas to have healthy, vigorous growth, it is important to understand the impact of cultural and environmental factors on this plant. Choosing the proper planting location and using good planting methods and cultural practices are critical in providing the best conditions for optimum plant growth. For more information on the beginning steps to a healthy azalea, refer to HGIC 1058, Azalea Planting.\nAzaleas prefer cool, partially shaded sites, such as the filtered shade of pine trees. Azaleas planted beneath hardwoods with shallower roots must compete with these trees for nutrients and water. If placed in the right location, however, they can do well on these sites. Although some varieties tolerate sun better than others, they all prefer an area that is not exposed to long periods of hot full sun and drying winds. Flowers last longer when plants are partially shaded. Azaleas exposed to full sun are more susceptible to lace bugs. Early morning sun exposure after a hard freeze may cause cold injury. Do not plant azaleas in heavy shade as poor flowering and weak growth result.\nAzaleas are shallow-rooted plants that are easily damaged by excessive soil moisture. They grow best in acid (4.5 to 6.0 pH), well-drained, organic soils. Before planting, have the soil tested and adjust the pH according to soil test results.\nAzaleas located in poorly drained sites do not receive the oxygen required for healthy growth and often develop root rot diseases. When planting in poorly drained areas, add composted pine bark to as large an area as possible, and plant the root ball higher than ground level.\nA 2- to 3-inch layer of organic mulch is very important. It conserves soil moisture, maintains soil temperature and helps discourage weeds. There are many materials available suitable for mulching. Pine straw, composted pine bark and leaves work very well, enriching the soil with organic matter as they decompose. Keep the mulch a couple of inches away from the main stem to keep the bark dry and extend it beyond the outermost branches.\nAzaleas are shallow-rooted plants and require irrigation during dry periods. This is especially true of those planted in the spring. Azaleas planted in warm weather in sandy soils may require watering of the root mass twice a week during the first year.\nTo determine when to water, pull back a small area of mulch near the base of the plant and check the moisture level of the root ball and surrounding soil. If the top few inches of soil feels dry, wet the soil deeply, to at least a depth of 6 to 8 inches.\nUse soaker hoses or drip irrigation to slowly water the base of the plants. Overhead irrigation may promote disease.\nAzaleas in waterlogged soils will decline and are susceptible to root rot diseases. It is important to reach a balance of regular, deep watering and good drainage to promote a healthy plant.\nAzaleas have low nutritional requirements compared to other shrubs. A soil amended with organic matter prior to planting followed by a mulch of compost, shredded leaves, pine straw or other organic material will usually provide sufficient nutrients for adequate growth.\nBefore fertilizing, have a specific reason for doing so, such as increasing growth rate or correcting a nutrient deficiency. A nutrient deficiency can be exhibited by a number of symptoms including stunted growth, smaller than normal leaves, light green to yellowish leaf color and early leaf drop. Be aware that these same symptoms can be caused by other problems such as heavily compacted soil; stresses from insects, disease organisms and weeds; and excessively wet or dry soil. Fertilization will not correct those problems, so be certain that you know the cause of the symptoms and treat them appropriately.\nHaving your soil tested is one way to determine if applying fertilizer will benefit your azaleas. Information on soil testing is available in HGIC 1652, Soil Testing.\nMost fertilizer recommendations are based on nitrogen, which is an important element in plant growth and often the one that is most likely deficient in the soil. Apply 1 pound of actual nitrogen per thousand square feet of root spread area. Up to 2 pounds can be applied with a slow-release fertilizer. In the absence of a soil test fertilize azaleas lightly in the spring and early summer with a well-balanced, extended-release, acid-forming, azalea fertilizer. By well-balanced, this means to look for nutrients in the ratio of 2-1-1. Good quality azalea fertilizers will also contain necessary trace elements. Never fertilize azaleas in the late fall with these high nitrogen fertilizers, as this may delay dormancy and result in plant injury.\nFertilizer examples are:\nComplete, acid-forming organic fertilizers are also excellent choices for use on azaleas, and these are great to mix into the soil at planting, as well as for use with spring fertilization. They are typically not as nutrient rich, and because of both the low nitrogen content and inability to burn the roots, they can be used to mix lightly into the soil in the fall at planting. Organic acid-forming fertilizer examples are:\nApply fertilizer to the azalea’s root zone area (area occupied by nutrient and water-absorbing roots) which can extend beyond the drip line or outer-most branches. According to research findings, a shrub’s roots can extend three times the distance from its center to the outermost branches. So, if the distance from the center of the azalea to the outer-most branches is 2 feet, the feeder roots can extend an additional 4 feet beyond the drip line. To visualize the area to be fertilized, imagine the azalea as the center point of a circle with a 6-foot radius (the ""root radius""). Trace the outer edge of the root zone area.\nSince most azalea roots are located in the top foot of soil, surface application of the fertilizer is adequate. Evenly broadcast the fertilizer with a handheld spreader or a rotary or cyclone spreader over the root zone area. Sweep any fertilizer off the branches and water afterwards to make the nutrients available to the roots.\nFor azaleas growing in a bed, follow the steps below to determine how much fertilizer to apply over the bed to supply 1 pound of nitrogen per 1,000 square feet. If the shrub’s root zone area is confined by a sidewalk or driveway, reduce the area to be fertilized accordingly.\n1. Measure the area of the bed, making an allowance for the roots that extend beyond the outermost branches. Let’s assume the bed is 30 feet long and 10 feet wide. The bed area (length x width) is 300 square feet.\n2. Determine how much fertilizer to apply over the bed to deliver 1 pound of nitrogen per 1,000 square feet using this equation:\n%N in bag\n|=||Number of pounds of fertilizer required per 1000 square feet in order to apply 1 pound of nitrogen|\nAssuming you have a 12-6-6 fertilizer, the equation for this example would look like this:\n|=||8.33 lbs of fertilizer required per 1000 square ft²|\nSince the root zone is 300 square feet, the actual amount of fertilizer to apply is calculated as follows:\n|Root area ft²\n|x||lbs fertilizer per 1000 ft²||=||fertilizer to apply over root area|\n|x||8.33 lbs fertilizer per 1000 ft²||=||2.5 lbs fertilizer to apply over root area|\nApply 2½ pounds or 5 cups of 12-6-6 evenly over the mulched bed.\nAzaleas do not have to be routinely fertilized during the growing season. Any fertilizer application should be based on their appearance, such as leaf color, growth rate, soil test results and your objectives, such as encouraging growth or correcting a mineral deficiency.\nThe best time to apply fertilizer is when it will be readily absorbed by the roots of the plant and when the soil is moist, which can be any time from late spring (after new growth emerges) up to early summer. Avoid fertilizing plants stressed by drought during the summer months. Without water, plants are unable to absorb nutrients, so it is best not to fertilize if water is unavailable.\nThere are two pruning techniques used for azaleas: thinning and heading. Thinning refers to the removal of branches back to the main trunk or another branch. This method is used to remove leggy branches that extend beyond the canopy of the plant, remove damaged or diseased wood, or reduce the size of the plant. Thinning allows light to penetrate the shrub, encouraging growth on interior branches. You can thin at any time of the year without causing significant impact on flowering, growth or cold hardiness of the plant. How-ever, to reduce the impact on flowers the following year, prune just after flowering in the spring.\nHeading refers to the cutting back of a branch, not necessarily to a side branch. This method is used to reduce the size of a plant, create a hedge or to renew old overgrown plants. Renew overgrown plants by cutting them back to within 6 to 12 inches of ground level. This practice results in abundant new growth by midsummer.\nThe best time to renew azaleas is before spring growth begins. This allows sufficient time for next year’s flower buds to form in midsummer, and for new growth to mature and harden off for winter. Renewal pruning before spring growth, of course, means that flowers are sacrificed for that year.\nAfter renewal pruning, prune the tips of new shoots when they are 6 to 12 inches long, to encourage branching and a full canopy. Thin out new shoots emerging from the old stem. Keep the soil moist during the period after severe pruning.\nThe most common diseases on azaleas in South Carolina include petal blight, leaf gall, leaf spots, dieback, and root and crown rot. The most common insects are lacebug and spider mites. Good cultural practices such as careful plant location, provision of good aeration and drainage, mulching and good watering habits will reduce the incidence of disease and insect damage. For more information on disease and insect problems on azaleas, refer to HGIC 2050, Azalea & Rhododendron Diseases, and HGIC 2051, Azalea & Rhododendron Insect Pests.\nIron is essential for healthy azaleas. Iron is available for uptake by azaleas when the soil pH is low (acidic). When soil pH is too high (alkaline), iron becomes unavailable and chlorosis, or yellowing of the youngest leaves, may occur. A telltale sign of iron chlorosis is when the area between the veins is yellow or light green, while the veins are darker green. Application of iron as a foliage spray will usually give quick, temporary results when applied during the growing season.\nIn low pH soils, such as the clay soils in the Piedmont area, iron is readily available. In these soils, chlorosis is usually due to other causes, such as waterlogged or compacted soil, root rot, or injury caused by nematodes or too much fertilizer. The first step to determine the cause of yellowing is a soil test to determine the soil pH.\nAzaleas are susceptible to cold injury, especially when exposed directly to early morning sun after a hard freeze. The rapid thawing of frozen branches and twigs may result in bark splitting. Sudden early freezes in the fall and late freezes in the spring also cause bark-splitting. It may take several months before the branches die back on winter-injured azaleas with split bark. Prune out affected branches. To reduce chances of bark-splitting, plant only azalea varieties known to be hardy in your area.\nPage maintained by: Home & Garden Information Center\nThis information is supplied with the understanding that no discrimination is intended and no endorsement of brand names or registered trademarks by the Clemson University Cooperative Extension Service is implied, nor is any discrimination intended by the exclusion of products or manufacturers not named. All recommendations are for South Carolina conditions and may not apply to other areas. Use pesticides only according to the directions on the label. All recommendations for pesticide use are for South Carolina only and were legal at the time of publication, but the status of registration and use patterns are subject to change by action of state and federal regulatory agencies. Follow all directions, precautions and restrictions that are listed.']"	['<urn:uuid:19ea5184-c10f-4dc4-807a-1892f3d42512>', '<urn:uuid:cd7263c0-c338-4984-b18c-d46ee4415f5d>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T05:06:54.527160	22	52	2852
16	moisture resistance insulation boards vs plywood panels difference	Both materials offer moisture resistance but through different mechanisms. Plywood achieves moisture resistance through its cross-laminated structure and special adhesives, while phenolic insulation boards have a high closed cell content (>95%) and are highly resistant to moisture penetration with minimal effect on thermal performance. Phenolic boards also feature gas-tight aluminum foil or glass tissue facings that provide additional moisture protection.	"[""Plywood (sometimes referred to simply as ‘ply’) is an engineered sheet timber product that is widely used for construction purposes. It is manufactured from three or more thin layers of wood veneer, or ‘plies’, that are glued together to form a thicker, flat sheet. It is economical, capable of being produced to precise dimensions and is relatively resistant to warping and cracking.\nSome of the most common uses of plywood include:\n- Light partition or external walls.\n- Structural systems.\n- Light doors and shutters.\nTypes of plywood include:\n- Structural plywood: Used in permanent structures where high strength is needed, such as beams, formwork and bracing panels.\n- External plywood: Used on exterior surfaces where a decorative or aesthetic finish is important.\n- Internal plywood: Used for aesthetic finishing of non-structural applications such as wall paneling and ceilings.\n- Marine plywood: Water resistant plywood that is used in shipbuilding and in parts of buildings where there may be high moisture content, such as roofing or bathrooms.\n Manufacturing process\nThe timber used to make plywood is prepared by steaming or dipping in hot water. It is then peeled into thin plies of between 1-4 mm by a lathe machine. It is then formed into large sheets.\nPlywood consists of the face (the surface that is visible after installation), the back, and the core (which lies between the face and the back). The plies are glued together using a strong adhesive, usually a phenol or urea formaldehyde resin.\nEach layer of ply is oriented with its wood grain perpendicular to the adjacent layer, rotated up to 90-degrees to one another. This is called ‘cross-graining’ and it is this that distinguishes plywood from laminated veneer lumber (LVL). In LVL, the direction of the plies is the same, whereas in plywood, the direction of the plies alternates. It is usual to have an odd number of plies so that the sheet is balanced and this helps to reduce warping.\nCross-graining reduces the tendency of the plywood to split when nailed at the edges. It also reduces expansion and shrinkage, which improves its dimensional stability, and it gives panels consistent strength in all directions.\nThe durability of the face and back veneers can be improved by the addition of a thin outer layer that resists moisture, abrasion and corrosion, as well as making it easier to apply paint and dye. Some of the materials that can be used include plastic, resin-impregnated paper, fabric, Formica, and metal.\nPlywood has several properties that make it a useful and popular construction material.\n High strength\nPlywood combines the structural strength of the timber from which it is manufactured, with the properties obtained from its laminated design. Cross-graining allows the sheet to resist splitting and provides uniform strength for increased stability.\n High panel shear\nThe odd number of veneer layers that comprise plywood mean that it is resistant to bending. By increasing the panel shear of plywood, it can be used in bracing panels and fabricated beams.\nPlywood can be manufactured to fit a wider range of requirements than cut timber. Veneer thicknesses can vary from a few millimetres to several inches, and the number of veneers used can be increased as required in terms of strength.\n Moisture resistance\nPlywood is relatively resistant to moisture and humidity due to the type of adhesive that is used in the binding process. This can make it suitable for exterior use such as cladding, sheds, concrete formwork and in marine construction. The veneers are prevented from warping, shrinking or expanding on exposure to water and temperature by the cross lamination.\n Chemical and fire resistance\n Impact resistance\nCross lamination gives plywood high tensile strength which makes it capable of withstanding overloading by up to twice its designated load. This makes it suitable for use in flooring systems and formwork.\n Related articles on Designing Buildings Wiki\n- Confederation of Timber Industries.\n- Cross-laminated timber.\n- Engineered bamboo.\n- Fire resistance.\n- Laminated veneer lumber LVL.\n- Lime wood.\n- Oriented strand board.\n- Sustainable materials.\n- Timber construction for London.\n- Timber preservation.\n- Timber vs wood.\n- Tree rights.\n- Types of timber.\n External references\n- Understand construction - Plywood\nFeatured articles and news\nWhat will the General Data Protection Regulations (GDPR) mean for you when they come into force in May?\nBusiness Secretary chairs a new taskforce to monitor and advise on mitigating the impacts of Carillion’s liquidation.\nSir John Armitt is appointed the new chair of the National Infrastructure Commission.\nHigh quality and high density homes - is it what we need or is it storing up trouble?\nGovernment announces its intention to strengthen planning rules to protect music venues and neighbours.\nNational Audit Office reports that there is little evidence that PFI offers better value than other forms of contracting.\nWhat is liquidation and how does it apply to contractors in the construction industry?\nScrutiny is placed on Carillion's controversial 2013 decision to extend subcontractor payment terms to 120 days.\nRSHP unveil their involvement in a boundary crossing which will provide a new entry point into Hong Kong.\nWith PFI currently under the spotlight due to Carillion, this introductory article explains what they are.\nEstimates suggest that up to 30,000 small firms could be at risk of non-payment as a result of Carillion's collapse."", 'Phenolic Insulation Boards\nPhenolic insulation boards are the most effective type of rigid insulation boards currently available. Phenolic insulation board is manufactured by a process in which a plastic foam forms an insulating core between two flexible facing layers. It has a high closed cell content and fine cell structure. Rigid phenolic insulation is produced by mixing high solids and phenolic resin with a surface acting agent. The heat created by the reaction evaporates a volatile liquid blowing agent in the mixture produces a mass of small bubbles in the material. The foam is then cured and manufactured in a continuous process to create thin sheet material which is laminated with various facings and cut into boards. Phenolic boards are laminated with pure aluminium foil facings, glass tissue facings, plasterboard.\nIn order to comply with EC regulations, blowing gases should be CFC and HCFC free--zero ozone depletion potential--and have low global warming impact. The most common blowing agent used in this type of insulation production is pentane, which is a hydrocarbon with similar properties to those of butane and hexane. The rigid boards are faced with, gas tight, aluminium foil or glass tissue in order to help eliminate escaping of the blowing agent in the manufacturing process.\n- External wall insulation with render\n- Partial fill cavity wall insulation\n- Pitched roofs\n- Soffit insulation\n- Solid and suspended floors\nRigid phenolic insulation products offer very good thermal insulating properties due to the very low thermal conductivity of phenolic foam. Its low thermal conductivity allows specified thermal performance targets to be achieved with minimal thickness of insulation. This is particularly significant where space saving is important.\nThe reflective foil can also act as a radiant barrier that adds to insulating values. The low emissivity facings improve the thermal performance when positioned next to an unventilated space. This makes the phenolic boards particularly effective when used to partially fill cavities in external walls.\n- Has a high closed cell content (>95%) and fine cell structure,\n- Lightweight, easy to transport, handle and install,\n- High density boards have a good compressive strength and are suitable for floors,\n- Resistant to fungus and mould growth and will not sustain vermin,\n- Can withstand continuous temperatures of up to 120 degrees C,\n- Highly resistant to moisture penetration. Moisture has a minimal effect on its thermal performance,\n- Is dimensionally stable and can be cut accurately to achieve a snug fit in situations such as between rafters,\n- Has better fire properties,\n- Has very low thermal conductivity, so can be used where a self extinguishing, low smoke emission, material is required,\n- Meets the requirements of building regulations,\n- Non- biodegradable but waste material can be put back into the manufacturing process for reuse.\n3* Kingspan Kooltherm K3 Floorboard, Kingspan Kooltherm K15 Rainscreen Board, Gyproc Thermaline Super Insulated Plasterboard, Kingspan Kooltherm K12 Framing Board, Kingspan Kooltherm K15 Rainscreen Board, Xtratherm Safe-R Thermal Liner SR/TB-MF Mex-Fix, Xtratherm Safe-R Thermal Liner SR/TB Dot & Dab']"	['<urn:uuid:b111a624-26c1-47c4-acc3-d998d2549cd8>', '<urn:uuid:d429eb9c-0c68-4c6a-9ffd-b92b5ea1944a>']	factoid	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-13T05:06:54.527160	8	60	1385
17	model 1916 gun range effectiveness	The Canon d'Infanterie de 37 modèle 1916 TRP had an effective firing range of 1,500 meters (1,600 yards), with a maximum firing range of 2,400 meters (2,600 yards). It had a sustained rate of fire of 25 rounds per minute and a muzzle velocity of 367 meters per second (1,200 feet per second).	"['Canon d\'Infanterie de 37 modèle 1916 TRP\n|Canon d\'Infanterie de 37 modèle 1916 TRP|\nA 37 mm Infantry gun in the Brussels Army Museum.\n|Type||Infantry support gun\n|Place of origin||France|\n|Wars||World War I\nWorld War II\n|Designer||Atelier de Puteaux|\n|Weight||Combat: 108 kg (238 lbs)\nTravel: 160.5 kg (354 lbs)\n|Barrel length||74 cm (2 ft 5 in)|\n|Caliber||37x94R mm (1.45 in)|\n|Elevation||-8° to 17°|\n|Rate of fire||Sustained: 25 rpm|\n|Muzzle velocity||367 m/s (1,200 ft/s)|\n|Effective firing range||1,500 m (1,600 yd)|\n|Maximum firing range||2,400 m (2,600 yd)|\nThe Canon d\'Infanterie de 37 modèle 1916 TRP (37mm mle.1916) was a French infantry support gun, first used during World War I. TRP stands for tir rapide, Puteaux (fast-firing, designed by the Atelier de Puteaux). The tactical purpose of this gun was the destruction of machine gun nests. It was also used on aircraft such as the Beardmore W.B.V and the Salmson-Moineau. Fighter ace René Fonck used a 37mm mle.1916 on a SPAD S.XII.\nFor transport, this weapon could be broken down into 3 sections. In addition, wheels could be attached for towing. These guns were sometimes equipped with an armoured shield. They were all equipped with a removable APX telescopic sight.\nU.S. high explosive ammunition for the TRP was the Mark II HE shell with a projectile weighing 0.67 Kilograms and a TNT bursting charge of 27.2 grams. The French Army used the Obus explosif Mle1916 HE round with a projectile weighing 0.555 Kilograms and a bursting charge of 30 grams. Captured rounds of this type were designated Sprgr 147(f) by the German military in World War II.\nDuring the First World War, the U.S. acquired a number of these guns, which they designated 37mm M1916. During the interwar years the US Army adopted a .22 caliber device to train with the 37mm cannon as an economic measure that allowed training on indoor ranges. By 1941 the U.S. Army had put most of these into storage, scrapped them, or converted them for use as subcaliber devices for heavy guns. Some were used in the Philippines Campaign in 1941-42 due to shortages of the 37mm M3. The Japanese Type 11 was based on this design.\nThe French Army still had the cannon in service in 1940 as a substitute for the 25 mm Hotchkiss anti-tank gun, which was in short supply. After the defeat of France by Germany, the Wehrmacht began using the TRP under the designation 3.7 cm IG 152(f).\n|Wikimedia Commons has media related to Canon d\'Infanterie de 37 modèle 1916 TRP.|\n- List of U.S. Army weapons by supply catalog designation SNL A-7\n- Infantry support gun\n- List of infantry guns\n- List of aircraft artillery\n- TM 9-2005 volume 3 Ordnance Materiel-General dated 1942\n- FM 23-75\n- TM 9-2300 standard artillery and fire control material. dated 1944\n- SNL A-7\n- SNL C-33\n- (1918) War Department Document No. 758 Provisional Instruction for the 37 Mm Gun Model 1916 R.F.\n- 37-Millimeter Gun Matériel, Model of 1916. In ""Handbook of artillery : including mobile, anti-aircraft and trench matériel"". United States. Army. Ordnance Dept. May 1920']"	['<urn:uuid:5e16b294-2647-479a-ae0f-249513bc3d5f>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-13T05:06:54.527160	5	53	516
18	corn seeds vs tomato seeds storage duration comparison	Corn seeds and tomato seeds have different storage durations when properly stored. Corn seeds can remain viable for up to 4 years or longer, with germination rates decreasing over time (86% at 1 year, dropping to 14% by 6 years). Tomato seeds, on the other hand, can remain viable for up to 6 years when properly stored and dried. Both types require cool, dry storage conditions to maintain viability, with corn seeds needing temperatures between 40-50°F and humidity below 60%, while tomato seeds need to be kept in a cool, dark place like a pantry or refrigerator.	['Corn seed saving is a time-honored tradition that can help you to be self-sufficient and reduce your reliance on big agribusiness. It is also a great way to preserve heirloom corn varieties for future generations. In this guide, we will walk you through the process of harvesting corn seeds, preparing them for storage, and storing them under the right conditions.\nWhen to Harvest Corn Seeds?\nThe best time to harvest corn seeds is when the ears have fully matured and dried. This typically takes place around mid-September, depending on your climate. You should wait until the husks are completely dry and feel brittle to ensure that the kernels inside have also dried out completely.\nOnce you’ve determined that the ear is ready, gently peel back the husks and remove the cob. Place it in a paper bag or other breathable container to allow any remaining moisture to escape before you proceed with seed saving.\nSeed Preparation for Storage\nOnce you have your mature ears of corn, it’s time to prepare your seeds for storage. The following steps will help ensure successful germination in the future:\n- Separate the kernels from the cob – use a corn sheller to remove them easily;\n- Float test – drop your kernels into a container of water; any that float can be discarded as they are likely to be empty or contain insects;\n- Air-dry – spread the kernels in a single layer on a screen or newspaper and allow them to dry for 3-7 days, depending on the humidity;\n- Cleaning – remove any remaining chaff, dust, or debris by sifting through the seeds with your hands before packaging;\n- Heating – heating the kernels to a temperature of 110-115°F before storage kills off any insects or pathogens that may be present;\n- Packaging – store your cleaned and dried seeds in an airtight container such as a Mason jar or vacuum sealer bag to keep out moisture and pests;\n- Storage conditions – ideal storing conditions for corn seeds are in a cool, dry place with temperatures between 40-50°F and humidity levels at or below 60%. Seeds should also be shielded from direct sunlight.\nHow Long Do Corn Seeds Last\nCorn seeds can last for several years if stored correctly. If you take the proper precautions during seed preparation, storage and handling, corn seeds can remain viable for up to four years or longer. Some varieties may even have higher longevity depending on their genetic makeup.\nThe most important factor in determining how long your corn seeds will last is the temperature and humidity levels of the storage conditions. Keeping the temperature in a range between 40-60 degrees Fahrenheit (4-15 Celsius) and humidity below 50% will help ensure that your seeds retain their viability. It is also important to make sure your seed containers are airtight to keep moisture out.\nThe data below showcases how the rate of germination decreases as the time of seed storage increases.\n- 1 year – 86%\n- 2 years – 80%\n- 3 years – 72%\n- 4 years – 62%\n- 5 years – 33%\n- 6 years – 14%', 'Other than buying tomato seeds or seedlings for your garden, there is a more economical and money-saving technique, which is to be harvesting tomato seeds from the bounty of your tomato fruits.\nIt is not complicated to save your own tomato seeds. If you have fully ripe, disease-free tomatoes, then you have the perfect setup to start your own seed collecting.\nWhy Should You Save Tomato Seeds?\nIt saves you money\nIf you collect the seeds from your favorite tomato variety, you can grow them again next season.\nIt makes you self-sufficient. You can grow tomatoes independently year after year without buying seeds or seedlings from a vendor, plus you know how the seeds performed and were cultivated.\nYou can exchange seeds\nAfter saving your own tomato seeds, it’s possible to use the seeds to trade with other gardeners. If you have extras, you can also share them with other family members and friends and help them to get their tomato planting started.\nAllows you to grow lots of crops\nIf you have your own seeds, you can grow as many tomatoes as you like.\nWhen you use the seeds from your best tomato plants you are ensuring that next year’s crop will look the same as this season.\nHarvesting seeds and saving them for the next growing season is easy as long as you know how. It does require waiting time and a few steps, but it’s not difficult. Let’s take a look:\nChoose The Seeds\nBefore you start saving tomato seeds for next year, you should consider which plants to use to cultivate your seeds for the next season.\nWhen choosing seeds, make sure you choose seeds from a tomato plant that are open pollinated varieties. The plants that have been open-pollinated will produce seeds that remain true to type to their parent plant vs seeds from hybrid plants were produced by companies that have combined varieties to make a new one and may not breed properly for you from seed.\nIf you don’t have any open-pollinated tomatoes open-pollinated tomatoes in your garden, you can use those from heirloom tomatoes you can get at the local farmer’s market.\nCollecting Tomato Seeds Preparation\nTo prepare the seeds for saving you’ll need to take them through a process of fermentation, in order to do this, make sure you have the following:\nGlass jar or bowl\nEnvelopes or paper bags for storage\nCheesecloth, paper towel, or plastic wrap\nFermenting Tomato Seeds\nThe fermenting process is easy:\n1. Wash all the dirt and dust off of the tomato before slicing to keep any disease or fungus from being transferred inside the tomato. Using a knife, slice the ripe tomato in half.\n2. Scoop out the insides, including the seeds and the gel surrounding the seeds, with a spoon. The gel sac that surrounds the seed keeps it from germinating, so this process will allow the seeds to germinate.\n3. Place the contents in a clean cup or container.\n4. Add water to the container. The amount of water should be enough to cover the seeds.\n5. Cover the container with cheesecloth, a paper towel, or plastic wrap. Before securing, you should ensure that there is enough room for air circulation around the seeds. If you are using plastic wrap, make sure to poke a few holes in the top.\n6. Place the container in a warm location, but away from direct sunlight.\n7. At least once a day, remove the cover from the container and stir the seed mixture.\n8. Allow the seeds to sit for several days until you see a film form on top of the water and the seeds start to sink to the bottom. The viable seeds are those that sink to the bottom, in most varieties. You know that the seeds are fermented if:\n- The seeds start to separate and they sink to the bottom of the container.\n- There’s a white, foamy mold that has formed on top of the water.\n- Bubbles have started to form at the top of the container.\n9. Using a spoon, remove the moldy film as well as any seeds that float on top of the water, as they aren’t viable, then you’ll want to discard all of that.\n10. Clean the container well and fill it again with fresh, room temperature water.\n11. Wash the seeds by swishing them gently around in the clean water.\n12. Put a cover on top of the container (you can use cheesecloth) and pour off the water.\n13. Place the seeds in a strainer and then rinse them under running room temperature water.\n14. Spread the seeds out on a paper plate, coffee filter, wax paper, or paper towels (warning – the tomato seeds can stick to the paper towels as they dry and be hard to take off). If you’re saving different varieties, label each plate with the seed variety.\n15. Allow the seeds to dry out in direct sunlight completely. Shake or stir them periodically. The drying time may take up to 1 to 2 weeks.\n16. Once the seeds are completely dry, place them in an envelope or paper bag. Don’t forget to label them with the name of the seed variety and the date they were stored.\n17. Place the container in a cool, dark place like your pantry or at the back of your fridge, don’t freeze them. They need to remain in a cool, dry area. If they are subjected to humidity or moisture it can cause them to rot.\nHow Will You Know If The Seeds Are Completely Dry?\n1. Allow the seeds to dry out for at least one to three weeks.\n2. The seeds start to be very hard.\n3. They are difficult to smash or to bite.\n4. The seeds make a faint snapping noise when you try and break them.\nThings To Keep In Mind When Saving Tomato Seeds\nNever use plastic or ceramic plates when drying your tomato seeds. When drying, any water needs to be wicked away from the seeds to allow for proper drying out. Plastic or ceramic plates can retain water and inhibit the process.\nProper storing and drying of the seeds means that they can be kept and will remain viable for up to 6 years.\nYou can store the dry seeds in an envelope or paper bag. Just be sure to label and date them and keep them sealed well.\nIf you store the seeds in the refrigerator, allow the container to reach room temperature before opening it to use for planting. Opening the container before allowing it to reach room temperature will introduce moisture to the seeds which can cause them to rot and fail or harbor fungus or disease.\nBe careful when storing the seeds in a plastic packet. If some seeds still contain moisture, it will transfer to all the seeds in the packet. The moisture will encourage mildew and will cause the seeds to rot.\nDo not try to speed up the process of drying out the seeds by using heat. Heat can destroy the seeds.\nDuring the drying process, shake the plate or stir the seeds to avoid clumping and allowing equal and even drying. Spread the seeds in a single layer. Once the seeds are exposed to air, they can dry out faster.\nSaving your tomato seeds will guarantee that you can grow the best (or your favorite) variety. It also saves you money.\nBeing able to save your own seeds and grow crops from them year after year is very satisfying and allows you to be self-sufficient.\nGood luck with saving your tomato seeds!']	['<urn:uuid:285bfafd-a4f2-458a-ba8c-9de4eae55cea>', '<urn:uuid:6688255e-a425-4cc9-960b-2e639a4f5477>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-13T05:06:54.527160	8	97	1801
19	How does team coordination differ in design-build versus traditional projects?	In design-build projects, there is increased partnering and cooperation compared to traditional design-bid-build scenarios. While traditional projects often lead to adversarial roles between designer, contractor and owner as each entity secures its own interests, design-build offers a more transparent process. The initial selection is value-based, including price, qualifications and experience rather than just low bid. This creates trust and enables collaborative effort among the design-builder, design team and owner, ensuring better focus on project quality, schedule and budget.	"[""POTENTIAL BENEFITS OF DESIGN-BUILD CONSTRUCTION DELIVERY\nSingle point accountability - this means that if the owner has issues with the project he has only one source to go to for resolution (design-builder) instead of two (designer and contractor).\nBetter quality design - in traditional design-bid-build scenarios the design team is designing a project in somewhat of a vacuum, without the benefit of the contractor's perspective on the process. The contractor brings the heightened perspectives of construction feasibility, site access, scheduling sequence and value engineering to the design process. Also, in design-build, the design team typically answers to the builder creating an additional level of accountability for the perspectives referenced above.\nReduced if not eliminated change orders due to errors or ambiguity in design - In traditional design-bid-build, the owner is responsible for the designs their hired consultants produce. If the designs include errors or ambiguities leading to differences of interpretation, the owner may be exposed to change order costs to the contractor. While the owner has the right to go back to the designer for some recovery of these costs, the right is seldom exercised. In design-build, the design-build entity (typically the contractor) is responsible for design, and therefore, assumes the financial risk of any potential design errors or ambiguities. This element of design-build can often lead to reduced overall cost of the project.\nIncreased level of partnering and/or cooperation among the team - The traditional design-bid-build scenario can often lead to adversarial roles between designer, contractor and owner with each entity acting first to secure its own interests. The designer attempts to maintain the integrity of the initial design and the designers' reputation, the contractor attempts to maintain its initial profits from time of bid, its initial interpretation of plans and specifications and uninterrupted sequence of work and the owner attempts to receive its project at a certain level of quality, on time and within budget. With each entity starting with a somewhat different motive, it is easy to see how relationships can deteriorate and focus can be lost. In the design-build scenario, the entire process is much more transparent. The contracting phase and initial selection of the design-build entity is typically more of a value-based selection including overall price, qualifications and experience rather than just low bid. The initial selection criteria forms the basis for the trust necessary in a design-build effort. Each of the phases of design is a much more collaborative effort among the design-builder, its design team and the owner. This collaboration helps insure a more goal centric process relative to project quality, schedule and budget.\nPotential timesavings - the design-build process can be a much quicker process than traditional design-bid-build. First, design-bid-build delivery requires the design team selection phase, several levels of design and approvals, bidding and contracting phases for selection of contractors and finally the construction phase, which often may be extended, based on change order facilitation. In the design-build process, the selection of the design-build entity is similar to the design team selection. Based on the complexity of approvals, design can be completed in phases to allow start of construction well before final documents are complete. Also the bidding and contractor selection and contracting phase are eliminated. Finally, the overall construction period may be condensed or at least maintained due to reduction in change orders.""]"	['<urn:uuid:b1deddbf-2b52-48b1-9443-83305284b0f8>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	10	78	553
20	What happened to intellectual dissidents under Communist rule?	Under Communist rule, intellectual dissidents faced severe consequences - Mindszenty was arrested, tortured and sentenced to life imprisonment in a show trial for opposing the regime, while Milosz was tipped off about potential arrest in Stalinist purges and chose to defect to France rather than return to Communist Poland.	"['Czeslaw Milosz, the Polish émigré writer who won the Nobel Prize in Literature in 1980, in part for a powerful pre-mortem dissection of Communism, in part for tragic, ironic poetry that set a standard for the world, died Saturday at his home in Krakow, his assistant, Agnieszka Kosinska, told The Associated Press. He was 93\nAn artist of extraordinary intellectual energy, Mr. Milosz was also an essayist, literary translator and scholar of the first rank.\nMany of his fellow poets were in awe of his skills. When another Nobel poet and exile from totalitarianism, the Russian Joseph Brodsky, presented Mr. Milosz with the Neustadt International Prize for Literature in 1978, he said, ""I have no hesitation whatsoever in stating that Czeslaw Milosz in one of the greatest poets of our time, perhaps the greatest.""\nMr. Milosz was often described as a poet of memory and a poet of witness.\nTerrence Des Pres, writing in The Nation, said of him: ""In exile from a world which no longer exists, a witness to the Nazi devastation of Poland and the Soviet takeover of Eastern Europe, Milosz deals in his poetry with the central issues of our time: the impact of history upon moral being, the search for ways to survive spiritual ruin in a ruined world.""\nIn 1951, he was in Paris, on duty there as a Polish cultural attaché following elite assignments in the United States at the consulate in New York and the embassy in Washington. An urbane man fluent in Polish, Lithuanian, Russian, English and French, Mr. Milosz had established close associations with leading left-wing intellectuals in Paris.\nThese diplomatic contacts were important to the Warsaw authorities, but Mr. Milosz, a skeptic about Marxist rule, was tipped off that he faced arrest and trial in the Stalinist purges then under way if he returned to Poland. So he denounced the Moscow-dominated system that was tightening its grip on his homeland and took political asylum in France.\nFormulating a New \'New Faith\'\nIn his youth, Mr. Milosz had been drawn to some of the idealized aspects of Marxism but he rejected dictatorship. In large measure, he defected, he explained later, because of damage he saw inflicted on spiritual values and intellect by Communist dogma, which he scorned as the ""New Faith."" For Mr. Milosz, faith was something else, as he made clear in a 1985 poem under that title:\nFaith is in you whenever you look\nAt a dewdrop or a floating leaf\nAnd know that they are because they have to be.\nEven if you close your eyes and dream up things\nThe world will remain as it has always been\nAnd the leaf will be carried by the waters of the river.\nMr. Milosz detested Socialist Realism, the Soviet-contrived literary doctrine that distorted truth into propaganda to promote the political and ideological goals of the Communist Party.\nTwo years after defecting, Czeslaw Milosz, (pronounced CHESS-wahf MEE-wosh) published ""The Captive Mind,"" a searing analysis of Stalinist tactics and their numbing effect on intellectuals. ""The Captive Mind"" was translated and published in many countries, becoming itself a historical document.\nIn it, Mr. Milosz wrote:\n""The philosophy of history emanating from Moscow is not just an abstract theory; it is a material force that uses guns, tanks, planes and all the machines of war and oppression. All the crushing might of an armed state is hurled against any man who refuses to accept the New Faith.\n""At the same time, Stalinism attacks him from within, saying his opposition is caused by his \'class consciousness,\' just as psychoanalysts accuse their foes of wanting to preserve their complexes.""\n""Still,"" he added, prophetically, ""it is not hard to imagine the day when millions of obedient followers of the New Faith may suddenly turn against it.""\n""The Captive Mind"" was among a powerful group of books in the early 1950\'s that condemned Communist ideology and foreshadowed the downfall of Communist power. A similar book was ""The New Class"" by Milovan Djilas, the Yugoslav dissident, which deplored self-aggrandizement and moral rot in the Communist leadership.\nAfter his defection, Mr. Milosz explained in a speech: ""I have rejected the New Faith because the practice of the lie is one of its principal commandments, and Socialist Realism is nothing more than a different name for a lie.""\nIn the same year ""The Captive Mind"" appeared, Mr. Milosz also published ""The Seizure of Power,"" a fictionalized scrutiny of the relationship between Communism and intellectuals.\nBy 1960, Mr. Milosz had tired of his life amid leftist intellectual squabbling in France. Years later he would speak with acerbity of those in Western Europe who continued to regard the Soviet Union as the hope of the future, particularly those ""French intellectuals who considered that only a man who was insane could abandon his position of a writer in a people\'s democracy in order to choose the capitalistic, decadent West."" He accepted a professorship in the Slavic Department at the University of California, Berkeley.\nHe became an American citizen and lived in the Berkeley hills in a modest house with a stunning view of San Francisco Bay. He celebrated that vista in his poetry (""Views From San Francisco Bay"" in 1972), but he also spoke of the alien remoteness of the California landscape.\nMr. Milosz, with his bushy eyebrows, herringbone tweed jacket, wry humor and brilliant lectures was soon a popular figure on campus, especially in his seminars and lectures on Dostoyevsky. He continued to write verse, translated literary masterpieces into Polish and compiled a large volume, ""History of Polish Literature,"" published in 1969.\nThe hardships and dangers in Mr. Milosz\'s life, first under Nazi military terror and then under Communist oppression, followed by long years as an émigré in the West, clearly marked his writing.\n""In both an outward and inward sense he is an exile writer, a stranger for whom physical exile is really a reflection of a metaphysical - or even religious - spiritual exile applying to humanity in general,"" the Nobel Committee observed in 1980. ""The world that Milosz depicts in his poetry, prose and essays is the world in which man lives after having been driven out of Paradise.""\nA Multilingual Boyhood\nCzeslaw Milosz was born June 30, 1911, to a Polish-speaking family in Szetejnie, Lithuania, which together with Poland, Latvia and Estonia was part of the Russian empire at the time. The complex, multiethnic Baltic region was inhabited by communities of Poles, Lithuanians, Jews, Russians and others, all speaking their separate languages and living their own cultures.\nHis family was not rich but it was distinguished and intellectual. He was only 3 when World War I broke out, and his father, a civil engineer, served in the czar\'s army, while his family was kept on the run from advancing German armies.\nFrom his childhood on, Mr. Milosz had a rich inner life, reading widely. He also had a challenging array of talents, interests and skills. As a schoolboy, he was fascinated by the scientific world of animals.\nBut in the end, he enrolled in law school at the University of Vilnius, graduating at the age of 23. He worked several years in radio, and sometimes remarked in interviews that he felt guilty for having abandoned science.\nMr. Milosz traced the distinctive imagery of his poetry to his boyhood experiences in the rural countryside of Lithuania; his childhood is evoked in an autobiographical novel published in the United States as ""The Issa Valley"" (1981) and in ""Native Realm,"" an autobiography. In one of his essays he wrote: ""If I were asked to say where my poetry comes from I would say that its roots are in my childhood in Christmas carols, in the liturgy of Marian and vesper offices, and in the Bible.""\nThe author Eva Hoffman, a native of Poland, said of him: ""He has never been a provincial artist. His writing may bear the marks of his particular Lithuanian-Polish past, but the material of his own life is filtered through a fully cultivated intelligence and probed to those depths at which individual experience becomes universal.""\nHe attended high school in the city of Vilnius, which by then had been transferred from Lithuania to Poland, and later restored to Lithuania, and published his first poem at the age of 15, He studied Latin for seven years in school, and in his Nobel acceptance speech credited that underlying linguistic discipline and classroom translations of poems with helping him to develop his mastery. He also learned Hebrew and Greek well enough to later translate the Bible into Polish.\nPoetic Vision Born of War\nAt the age of 22, while attending law school, Mr. Milosz published his first experimental verse, ""Poem on Time Frozen."" Favorable reaction helped him win a state scholarship to study literature in Paris after he was awarded a law degree in 1934. A relative there, Oscar Milosz, who worked in the Lithuanian legation and wrote poetry in French, helped broaden his world outlook and shape his poetic style.\nHe returned to Vilnius after the publication of a second book of poems called ""Three Winters"" but was fired from his job at a local Polish radio station for being too liberal. Mr. Milosz was working in Warsaw for Polish Radio when the Germans invaded Poland in September 1939.\nDuring the Nazi occupation, he worked in the Warsaw University Library, wrote for the anti-Nazi underground, heard the screams and gunfire in 1943 as Germans killed or captured the remaining Jews in the walled Ghetto and witnessed the razing of nearly all Warsaw after the uprising in 1944.\nOne of his most moving poems, ""A Poor Christian Looks at the Ghetto"" (1943), described the assault on the Jews:\nBees build around red liver,\nAnts around black bone.\nIt has begun: the tearing, the trampling on silks,\nIt has begun: the breaking of glass, wood, copper, nickel,\nOf gypsum, iron sheets, violin strings, trumpets, leaves, balls,\nPoof! Phosphorescent fire from yellow walls\nEngulfs animal and human hair.\nBees build around the honeycomb of lungs,\nAnts build around white bone.\nTorn is paper, rubber, linen, leather, flax,\nFiber, fabrics, cellulose, snakeskin, wire.\nThe roof and the wall collapse in flame and heat seizes the\nNow there is only the earth, sandy, trodden down,\nWith one leafless tree.\nSlowly, boring a tunnel, a guardian mole makes his way,\nWith a small red lamp fastened to his forehead.\nHe touches buried bodies, counts them, pushes on,\nHe distinguishes human ashes by their luminous vapor,\nThe ashes of each man by a different part of the spectrum.\nBees build around a red trace.\nAnts build around the place left by my body.\nI am afraid, so afraid of the guardian mole.\nHe has swollen eyelids, like a Patriarch\nWho has sat much in the light of candles\nReading the great book of the species.\nWhat will I tell him, I, a Jew of the New Testament,\nWaiting two thousand years for the second coming of Jesus?\nMy broken body will deliver me to his sight\nAnd he will count me among the helpers of death:\nAfter the war, a collection of poems called ""Rescue,"" which showed the influence of T. S. Eliot, established him among Poland\'s pre-eminent writers. Although he was not a member of the Communist Party he was accepted into the diplomatic corps in 1946 and began the journey that ended with his defection in 1951 in Paris.\nMr. Milosz chose throughout his life to compose his poetry in the complex but rich Polish language, even after he mastered French and English. Poetry can be true, he said, only if created in one\'s mother tongue.\nAs his work won increasing attention and respect, Mr. Milosz developed close ties to many leading world intellectuals, writers, and political and religious leaders, especially to Pope John Paul II, his countryman and leader of his faith.\nWhen he consulted on his plan to break with Communism, it was with no less a figure than Albert Einstein, who advised him during a talk at Princeton University that he should go home to Poland, not defect to the West to join the sad fate of exiles.\n\'A Poet Remembers\'\nMr. Milosz also knew Lech Walesa, the electrician who led the anti-Communist Solidarity movement and went on to become president of Poland. Lines from a verse by Mr. Milosz were put on a memorial in Gdansk to honor Mr. Walesa\'s fellow shipyard workers who were shot by the police in the early 1970\'s:\n""You who harmed a simple man, do not feel secure: for a poet remembers.""\nWhen Communism was smashed in Poland, Mr. Milosz returned to what he called ""the country of my first immigration."" Arriving in Warsaw after an absence of three decades, he received a hero\'s welcome. Mr. Milosz was regarded as one of the world\'s literary immortals. When he chose, he walked and talked with the great men of his time, but he remained humble.\nHe also had a remarkable memory and could readily recall the names of his early teachers, companions and friends, and he remembered in vivid detail the first books he read, his adventures and mishaps. He demonstrated that acute memory in his 1968 book ""Native Realm, A Search for Self-Definition,"" a compelling and mildly ironic account of his life, work and thoughts in the illuminating context of Baltic and family history.\nMr. Milosz enjoyed pleasures of the body as well as of the mind, as he acknowledged in his 1985 poem, ""A Confession,"" translated by himself and Robert Hass:\nMy Lord, I loved strawberry jam\nAnd the dark sweetness of a woman\'s body.\nAlso, well-chilled vodka, herring in olive oil,\nScents, of cinnamon, of cloves.\nSo what kind of prophet am I? Why should the spirit\nHave visited such a man? Many others\nWere justly called, and trustworthy.\nWho would have trusted me? For they saw\nHow I empty glasses, throw myself on food,\nAnd glance greedily at the waitress\'s neck.\nFlawed and aware of it. Desiring greatness,\nAble to recognize greatness wherever it is,\nAnd yet not quite, only in part, clairvoyant,\nI know what was left for smaller men like me:\nA feast of brief hopes, a rally of the proud.\nA tournament of hunchbacks, literature.\nAt times, Mr. Milosz fell into melancholy, but he firmly fended off any would-be therapists. His early poetry was in what was called the ""Catastrophist"" school of the 1930\'s, which foresaw the annihilation of the principal values of modern culture and a devastating war. His wartime ordeals tended in ways to bear out the forebodings.\nMr. Milosz was a man of quiet manner but strong opinions and he expressed them, sometimes to the distress of his admirers. For example, in a PEN congress talk he reminded his fellow writers, ""Innumerable millions of human beings were killed in this century in the name of utopia - either progressive or reactionary, and always there were writers who provided convincing justifications for massacre.""\nReacting to the atrocities in the struggle between Christians and Muslims in Bosnia in the 1990\'s, Mr. Milosz blamed intellectuals more than politicians and generals.\n""These people who had liberated themselves from Marxist doctrine very quickly became nationalists,"" he said in 1996. ""And we see what happens now in Yugoslavia. In my opinion, intellectuals are responsible for the horrors in Bosnia, for they initiated the new nationalist tendencies there.""\nMr. Milosz was married twice. His first wife, Janina Dluska, shared his ordeals in Warsaw during the Nazi occupation and went into exile with him. She died in 1986. They had two sons, Anthony and John Peter, who survive him. In 1992, Mr. Milosz married Carol Thigpen, a historian. Ms. Thigpen died in 2003, The Associated Press said.\nAfter Mr. Milosz was awarded the Nobel, many of his books were translated into English and published in the United States. Ecco Press gathered a half-century of his work in ""The Collected Poems 1931-1987."" In it is a 1986 poem called ""And Yet the Books,"" which contained these lines:\nI imagine the earth when I am no more:\nNothing happens, no loss, it\'s still a strange pageant,\nWomen\'s dresses, dewy lilacs, a song in the valley.\nYet the books will be there on the shelves, well born,\nDerived from people, but also from radiance, heights.', 'On February 18, 1946, 32 bishops and archbishops from around the world gathered in Rome to receive from Pope Pius XII the red hat—the traditional symbol of elevation to the Sacred College of Cardinals. It was the first such ceremony after World War II, and Pope Pius added to the growing international nature of the College of Cardinals by nominating prelates from Brazil, Lebanon, Australia, the United States, Cuba, Mozambique, and China.\nThe consistory was notable for one other event: As he placed the scarlet hat on one new member’s head, the pope said to him: “Among the 32, you will be the first to suffer the martyrdom whose symbol this red color is.” The words proved prophetic, for within a short time, that cardinal—József Mindszenty, Archbishop of Esztergom, Hungary—endured arrest and horrifying torture, becoming a symbol of Catholic martyrdom for the entire world.\nA Shepherd in Two Wars\nJózsef Pehm was born in Mindszent, Austria-Hungary (modern Hungary) on March 29, 1892, the son of devout Catholic peasant farmers. He later changed his German-sounding surname to Mindszenty to honor his place of birth. Called to the priesthood, he was ordained in 1915—the middle of World War I.\nIn the aftermath of the war and the disintegration of the Austro-Hungarian Empire, Hungary fell briefly under the control of a Soviet Hungarian Republic led by Béla Kun in 1919.\nWhen the short-lived Communist regime proposed massive takeovers of agriculture and then attempted to seize Catholic schools, Mindszenty spoke out against the state. The Communists responded by arresting the young priest. He was released when Kun fell from power in August 1919, but it was not the last time Mindszenty would inhabit a prison cell.\nIn 1941, Hungary’s leaders sided with Hitler and sent thousands of young Hungarians to die in Russia for the Nazi cause. That was not enough for the Nazis, who suspected that the Hungarians might try to negotiate a separate peace with the Allies. Germany therefore launched Operation Margarethe, the occupation of Hungary, in March 1944. The Nazi terror apparatus descended on the country.\nA mere 15 days prior, Fr. Mindszenty had been named Bishop of Veszprém. He was consecrated in the midst of the Nazi seizure of Hungary, and he spent the next months providing spiritual encouragement to his flock and assisting as many Jews as possible to avoid arrest and deportation. His work rescuing Jews and encouraging resistance soon attracted the attention of the Sicherheitsdienst—the Nazi Secret Service—and the Gestapo. He was imprisoned from November 27, 1944 to April 20, 1945, when he was released after the fall of the pro-Nazi regime. By this time, Mindszenty was a national hero to both Catholics and Protestants and was honored especially for his help to the Jews.\nIn recognition of his abilities and his courage, Pope Pius XII named Mindszenty archbishop of Esztergom on October 2, 1945. With the nomination, Mindszenty became the primate of Hungary and thus the symbol of the Church for the nation. Pius wasted little time in making him a cardinal the next year, using the occasion to predict the coming darkness for Hungary.\nThe Soviet Red Army had swept into Hungary from the east and captured Budapest after fierce fighting in April 1945. The Soviet occupation lasted for two years, during which free elections were held in November 1945. But the Soviet occupiers refused to allow the new government to form. Instead, the Communists were forced upon the Hungarians, and the newly formed secret police, with the help of the Soviets, began arresting and exterminating all opposition. The last democratic leaders were driven into exile or sent to Siberia in 1948 as the country sank into the iron grip of Communist oppression.\nFor God and for Hungary\nThe puppet dictators understood clearly that the Church was a huge obstacle to their program. So they first confiscated most of the Church’s land, especially all agricultural holdings. Then they took over all of the nearly 5,000 Catholic schools and installed ardent Marxists as teachers. Catholics who protested the changes were arrested for being counter-revolutionaries and engaging in conspiracy against the government. Of particular concern, of course, was the leader of Hungarian Catholics, Cardinal Mindszenty.\nMindszenty protested. He visited every city and every village and called on the Hungarians to resist the new laws peacefully, to refute government lies, and to be firm in refusing to surrender Church lands and schools. In November 1948, he issued what proved to be his last pastoral letter. Although banned, it still made its way to the Voice of America radio broadcast. The cardinal declared: “I stand for God, for the Church, and for Hungary. . . . Compared with the sufferings of my people, my own fate is of no importance. I do not accuse my accusers. . . . I pray for those who, in the words of Our Lord, ‘know not what they do.’ I forgive them from the bottom of my heart.”\nOn December 26, 1948—St. Stephen’s Day—the cardinal’s residence was surrounded by Hungarian State Security, and Mindszenty was arrested in front of his aged mother and loyal aides. As he was taken away, he implored them not to believe any claim that he had resigned or confessed. He was swept away to the prison of the secret police on Andrassy Street, the same building that had been used as a torture center by the Gestapo during the war. Stripped of his breviary, rosary, and religious habit, he spent the next 16 days being tortured, interrogated, and badgered into signing a confession.\nIn February 1949, Mindszenty’s sham trial began in a Hungarian court. At the trial, he was dressed in clown clothes, mocked by his jailers, and charged with 40 crimes, including treason, conspiracy against the Communist government, and being an enemy of the people.\nThere was little surprise when the trial ended with Mindszenty’s conviction. Although these crimes were usually punished by hanging, he was instead sentenced to life imprisonment because the Communists did not want to create a martyr for the Hungarian people. Still, the conviction elicited worldwide condemnation. Both Winston Churchill and President Harry Truman added their voices to the international outrage, and in protest of the ridiculous “trial,” nine Hungarian diplomats in the United States resigned in shame.\nThe World Watches\nSeven years of imprisonment ensued. In September 1949, he was transferred to Conti Prison in Budapest where he was held in solitary confinement for four years. As Mindszenty recalled later, his cell was “small and crumbling. There was a straw mat to sleep on, a table, a stool, a small bucket for one’s needs and another for water . . . I received no mail, read no newspapers and no books except my breviary and my Bible . . . Each day I said my rosary six times. Much of the time I prayed for strength . . .” (Time Magazine, “The Mindszenty Story,” Dec. 17, 1956.)\nMindszenty’s health finally failed, and the prison doctor threatened that if he were not transferred to a better situation, he would die as the world watched. The government relented. On July 16, 1955, Mindszenty was taken to Castle Puspokszentlaszlo in southern Hungary, the traditional summer residence of the bishops of Pecs. Mindszenty saw sunshine, trees, and flowers for the first time since 1949. He recovered some of his strength over the next few months and was transferred to Felsopeteny Castle in the north.\nMeanwhile, events outside of the prison were overtaking Hungary. In October 1956, a spontaneous revolution against the Hungarian Communists and their Soviet masters erupted after student uprisings in Budapest prompted nationwide protests. The Hungarian government fell, the prisons were emptied, and a new government promised to hold free elections.\nAmong the prisoners granted freedom was Cardinal Mindszenty. He returned to Budapest and his old house on Uri Street. Over the following days, he spoke on the radio calling for moderation in dealing with the fallen Communists and exhorting Hungarians not to exact revenge.\nThe happiness was short-lived. On November 4, 1956, the Soviet government unleashed the Red Army on Hungary. The doomed Hungarian Prime Minister Imre Nagy broadcast an emergency appeal to the West for help, and then advised Mindszenty to take refuge in the U.S. embassy. Mindszenty rolled up his cassock under a long overcoat and made his way on foot to the embassy. From that sanctuary, he watched as the new puppet regime of hardliner János Kádár arrested tens of thousands of Hungarians; many were deported to the Soviet Union, where they died in Siberian labor camps. Some 200,000 Hungarians fled the country while they still could.\nThe new regime promised Mindszenty safe passage to Austria, but he declined. The mere presence of this national hero was a symbol of hope to the Hungarian Catholics—and a great embarrassment to the Hungarian government.\nAdvocate in Exile\nBy the late 1960s, the global situation had changed. Both Vatican and American diplomacy had adopted a different approach to the Soviet Union. For the Vatican, the cardinal was perceived increasingly as a hindrance to the new strategy of Ostpolitik and the efforts to develop a rapprochement with the Soviet bloc that could lead to an easing of relations and the creation of new dioceses in Hungary. As the American government expressed unwillingness to have him remain in the embassy, Mindszenty received instruction from Paul VI to leave Hungary. He departed his native land on September 29, 1971 and settled in Vienna, Austria. Over the next few years, he traveled the world to speak of the plight of Hungarian Catholics. On December 18, 1973, he was retired officially as archbishop of Esztergom by Pope Paul to permit further normalization of relations between the Vatican and Hungary. The archbishop’s vacated see was not filled during his lifetime. He died in Vienna at the age of 83 on May 6, 1975, and was buried there. In 1991 the freely elected government of Hungary repatriated his remains. He is now buried in the basilica at Esztergom.\nMindszenty’s courage was never forgotten, and he remained a symbol for Hungarians—and all Christians—that they and the Church would one day be free.']"	['<urn:uuid:dd47109c-1164-4f32-9c80-3369c4dd5ac6>', '<urn:uuid:0303dd3a-7013-4145-8bb9-0bc8b1a01f87>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T05:06:54.527160	8	49	4410
21	I've been researching supply chain best practices and would like to understand how inventory turnover connects with supplier sustainability - what's the relationship between managing inventory turnover and ensuring supplier sustainability across different tiers?	Inventory turnover rate is crucial for managing costs by preventing over-ordering or under-ordering of inventory, which helps avoid waste and delivery delays. This metric needs to be integrated with supplier sustainability practices across all tiers, as companies need to look beyond just Tier 1 suppliers. Research shows that two-thirds of a company's environmental and social footprint lies with suppliers, making it essential to monitor sustainability throughout the supply chain. Companies need to evaluate ESG credentials of potential suppliers and ensure all tiers adhere to sustainability requirements. Simply focusing on Tier 1 suppliers is insufficient, as issues like labor violations often occur at lower tier levels. Companies that effectively combine inventory management with supplier sustainability can achieve both operational efficiency and environmental responsibility.	['Inventory management is perhaps the most important aspect of any business selling goods to customers.\nProduct quality is important. Shipping and fulfillment are important. But without a proper system for managing inventory, you won’t even get that far. Not to mention the costs you will incur for failing to accurately forecast demand.\nThat’s why proper inventory management not only helps a business to operate more efficiently, it can also drastically reduce costs.\nHere are the top 3 inventory metrics that are most important in reducing costs.\n- Current inventory levels\nCurrent inventory levels are extremely important to know and to know with complete certainty and accuracy. This is because if you do not know what is in your inventory, then you cannot guarantee that orders will be fulfilled on time. If an order is not fulfilled on time, then it can cost your company money. This is because you may lose the customer, possibly lose the inventory (if it is perishable), and you will have to spend more money gaining new replacement clients. So, knowing current inventory levels is crucial.\n- Product procurement cycle\nProduct procurement cycle refers to the amount of time that it takes the suppliers of your inventory to get the materials to you. This metric is extremely important because it lets you know how long after you order materials that they will arrive at your warehouses or retail locations. You need to know this information so that you can guarantee that you have the products available for delivery once they are purchased by your customers. If they are not available for delivery when they need to be, then you can face major problems with your business. Not only can it harm your company’s reputation, but it can cause you to lose customers, both of which can force you to lose money.\n- Inventory turnover rate\nInventory turnover rate is the rate at which your company sells its inventory. There are several different reasons why this is an extremely important metric. First, it lets you know what profits you can expect to make from your inventory in a given amount of time. Second, it lets you know how frequently you will need to reorder inventory. Having as accurate information as possible about inventory turnover rate is crucial for reducing costs.\nThis is because knowing your inventory turnover rate can prevent you from ordering too much or too little inventory in a given amount of time. Doing either one of these things can potentially cause your company to lose money. This could either be through wasted inventory if your materials are perishable or through failing to deliver orders on time. Inventory turnover rate also allows you to carefully monitor how well your company is selling its products.\nIf you use all three of these metrics, then it can help your company to manage its inventory in an optimum way. As with most other aspects of a business, if you maximize efficiency for inventory management, it can help your company to reduce costs and to increase profitability. Keeping track of current inventory levels, product procurement cycles, and inventory turnover rates can all be easily done with inventory management software.\nIf your company does not currently use inventory management software, then you could be missing out on a significant amount of inventory-related savings for your company. If you decide to use inventory management software and keep a close eye on all three of these metrics, then you could soon witness your company reduce savings and increase profitability significantly.', 'TO ACHIEVE ENVIRONMENTAL AND SOCIAL SUSTAINABILITY, PROCUREMENT MUST BE FULLY ON BOARD. ONLY PURSING SUSTAINABILITY INTERNALLY MEANS THE MAIN OPPORTUNITIES FOR SUSTAINABILITY ARE MISSED IN THE SUPPLY CHAIN.\nProcurement is at the core of any business effort to achieve sustainability. Sustainable procurement contributes value in many ways by reducing costs and minimizing the risk of disrupted sources of materials. This is a minimal perspective however, because sustainable procurement is also at the core of the world’s ability to reach environmental and social sustainability. Business leaders are focused on responsibility, resilience, and supply regeneration of critical resources, and for too long, procurement has been relegated to the background when it should have an equal voice at the table. Only this way can supply chain models be reinvented and procurement become a central player in developing sustainable products and services.\nEMBEDDING (REAL) SUSTAINABILITY\nA research project called The Embedding Project is a global public-benefit research project collaborating with companies ready to embed environmental and social factors into their operations and decision-making. The ultimate goal is progress towards a regenerative economy, which requires conscientious procurement. The white paper “Procuring a Regenerative Economy” addresses the fact that “a truly sustainable and impact-free sourcing network can only be implemented by rethinking the overall company business model,” and goes on to say that today’s “sustainable procurement largely entails check-lists pushed onto suppliers, often through third parties, and the results can be overridden if short-term financial conditions make it seem preferable to do so.”\nThe future of sustainable procurement is marked by necessity. Without sustainable procurement, a regenerative economy is not possible, but the question is how to get buyers and suppliers focused on sourcing sustainable products and services. The sustainability efforts going on right now are not producing the desired results, and one of the biggest challenges is connecting the strategic plan to actual procurement processes and systems. Regenerative sourcing has challenging goals which includegenerating net-zero GHG emissions, restoring ecosystems, and providing inclusive, fair and safe livelihoods for people. As the white paper points out, sustainable procurement is about more than material flows; it recognizes purchasing decisions made as the business will impact a multitude of organizations in terms of community, climate, resources, and the environment.\nEVERY STEP OF PRODUCT DEVELOPMENT AND PRODUCTION\nSustainable procurement is not altruistic. It is crucial to business reputation, minimizing the risk of scarcity of supply in the future, and maintaining competitiveness by managing costs through energy consumption and waste reduction. It is a risk management strategy for competitive endurance as much as an environmental and social strategy. True sustainable procurement requires deep analysis, and a commitment to ensuring suppliers are adhering to sustainability values as much as the buyer.\nCosmetic companies are expected to act responsibly in many areas – product testing, diversity, environmental protection, fair labor practices, packaging and logistics. The RBI was founded by L’Oréal, Coty, Groupe Rocher, and Clarins to promote sustainability best practices and processes. The mission statement provides insight into what sustainable procurement is really all about, saying, “Responsibility is spread all along the value chain” from the small-holder producer to the final consumer, including of course all the manufacturers and intermediaries that enable them to produce and distribute a good.” RBI established sustainability ratings to measure and benchmark Corporate Social Responsibility and engaging suppliers more effectively is a core strategy. L’Occitane is a French cosmetics company that signed onto the RBI, which was a public commitment to sustainable supply chains.\nA company like Elate Beauty is a good example of a business that has innovatively developed products based on sourcing sustainable ingredients. This company looks at every step of the supply chain and product design to ensure maximum sustainability is achieved. The company has an impressive procurement process, in which ingredients sourced are 75% organic, but they will choose fair trade over organic if unable to have both. All products are 100% vegan and cruelty free. Bamboo used in products comes from suppliers with Forest Stewardship Council Certification, mica comes from suppliers who are members of the Responsible Mica Initiative, and wood for pencils comes from suppliers who can demonstrate the Californian cedar wood is certified by the Program for Endorsement of Forest Certification. As the company says, innovation and ethics in sustainability are core values.\nOPTIMIZING SUSTAINABILITY INTERNALLY AND EXTERNALLY\nElate Beauty is offered as an example of procurement innovation in sustainability because the company has considered every stage of its procurement and production processes. McKinsey research has found that two-thirds of a company’s ESG (environment, social, and governance footprint) is with suppliers. ESG can lead to higher business performance too. Companies focused on ESG experience faster growth and higher valuation compared to the competition by 10-20%. However, optimizing resource consumption internally is not enough, and that is where procurement becomes so important.\nPer McKinsey, as much as 80-90% of greenhouse-gas emissions of products are indirect emissions in procured goods and services, travel, commuting, and the use of end-of-product life cycle treatment. Two-thirds of the emissions are from the upstream supply chain. Procurement can have an enormous influence on ESG by influencing product design and purchasing decisions. McKinsey suggest building sustainability as a procurement standard, expanding collaboration with suppliers, evaluating ESG credentials of potential suppliers, and employing carbon accounting principles. This requires procurement having access to the right tools, resources, and data sources.\nIt is no secret that lower levels of suppliers in the supply chain often violate sustainability principles, yet most companies concentrate on Tier 1 suppliers. One of the procurement challenges is developing supplier networks that are sustainable down to the lower levels, which is not only the right thing to do, but can save a company from reputational damage. A good example is when the media reported Nestle was using forced labor in its supply chain. A long investigation uncovered the slave labor was actually in the Fancy Feast supply chain.\nEvery supplier should adhere to procurement policies and requirements for sustainability, meaning all tiers of suppliers should be included in the procurement sustainability strategy. Tier 1 suppliers can play a major role here, because they have the ability to assess their own suppliers.\nCompanies are adopting sustainable procurement best practices because environmental and social sustainability is directly connected to business sustainability. The future of sustainable procurement depends on the honest, transparent, and sincere effort of businesses.']	['<urn:uuid:d715d5a8-f16b-4b6e-9fef-99a47a18070b>', '<urn:uuid:e581a43e-6de5-4a11-a4e2-b3b3a08951e9>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T05:06:54.527160	34	122	1645
22	What are the main configuration parameters required for establishing a baseband channel with S-Parameters, and what physical connections are needed for DVI implementation?	For S-Parameter configuration, key parameters include FileName for S-Parameter import, PortOrder for S-Parameter ports, MaxNumberOfPoles for fit output, ErrorTolerance in dB, SampleInterval in seconds, StopTime for Impulse Response duration, TxR and RxR for impedance values, and TxC and RxC for capacitance values. For DVI implementation, physical connections require TMDS transmitter on the computer (implemented either on motherboard, ADD card, or graphics card) and TMDS receiver on the monitor, with single-link DVI supporting up to 165 MHz bandwidth or dual-link DVI using 2 TMDS transmitters and receivers for up to 330 MHz.	"['Convert Scattering Parameter to Impulse Response for SerDes System\nThis example shows how to find an Impulse Response by combining a Scattering-Parameter (S-Parameter) model of a baseband communication channel along with a transmitter and receiver represented by their analog characteristic impedance values. You will see how to find an Impulse Response of this network using the\nsParameterFitter app to create an\nSParameterChannel (hyperlink to sParameterChannel in doc) object in SerDes Toolbox™, which also uses some supporting functions from RF Toolbox™ such as\nrational (RF Toolbox) and\nimpulse (RF Toolbox).\nThe S-Parameter file representing the baseband channel should be a Touchstone 1.0 (.sNp) file. Typically these are extracted from a software EDA tool or laboratory VNA with a port reference impedance (50-Ohms is recommended). The following properties are the main settings you would use to extract an impulse response of the concatenated Transmitter-Channel-Receiver circuit network:\nFileName - Filename of the S-Parameter to be imported.\nPortOrder - Port order for the S-Parameter.\nMaxNumberOfPoles - Maximum number of poles to use for a fit output by the\nErrorTolerance - Desired error tolerance in dB for a fit output by the\nSampleInterval - Sample interval in units of seconds.\nStopTime - Desired duration of the Impulse Response in units of seconds.\nTxR - Single-ended impedance value in Ohms of the analog TX IO structure.\nTxC - Capacitance value in Farads of the analog TX IO structure.\nTxAmplitude - Stimulus amplitude of the Tx output rising waveform.\nTxRiseTime - The 20-80% rise time of the Tx stimulus waveform.\nTxRTFactor - The conversion factor from 20-80% or 10-90% to 0-100% risetime, default is 20-80%.\nRxR - Single-ended resistance value in Ohms of the analog RX IO structure.\nRxC - Single-ended capacitance value in Farads of the Rx structure.\nSignalling - Specify signaling as \'single-ended\' or \'differential\'.\nAggressorDefinition - Method of acquiring aggressor behavior. For \'same-source\' the stimulus is applied to victim input and probed at aggressor output and for \'same-load\' (default) the stimulus is applied to each aggressor input and probed at the victim output. While the same load definition is more direct, the same source definition is used by methodologies that rely on time domain excitation (like HSPICE) to stimulate the system with a single pulse or step response. Consider the following 4-port single-ended system:\nThrough-Ports: (1) -- (3)\nThrough-Ports: (2) -- (4)\nThe victim line is S31, the same-source crosstalk aggressor is S41 and the same-load crosstalk aggressor is S32.\nAutoDetectPortOrder - Boolean option to force auto-detect of port order.\nNote: defaults are provided for all settings if no entries are made when calling\nCreate the S-Parameter Channel Object:\nYou create an\nSParameterChannel object by launching the\nsParamterFitter app from the base workspace.\nThis will allow you to create an impulse response from a Touchstone S-Parameter data file. The app loads with default parameters for an\nSParameterChannel object. The equivalent command would be as follows:\nobj = SParameterChannel(\'FileName\',\'default.s4p\',... \'PortOrder\', [1 2 3 4],... \'MaxNumberOfPoles\', 1000,... \'ErrorTolerance\', -40,... \'SampleInterval\', 6.25e-12,... \'StopTime\', 20e-9,... \'TxR\',50,... \'TxC\',0.1e-12,... \'TxAmplitude\', 1.0,... \'TxRiseTime\', 40e-12,... \'TxRTFactor\', 0.6,... \'RxR\',50,... \'RxC\',0.2e-12,... \'Signaling\',\'differential\',... \'AggressorDefinition\',\'same-load\');\nIt is important to also ensure\nStopTime are set appropriately for the Impulse Response calculation (in this case, 6.25e-12 seconds and 20e-9 seconds, respectively), as well as the stimulus represented by\nTxRiseTime. Understanding S-Parameters is beyond the scope of this example, but it is important to remember the bandwidth of an S-Parameter must be sufficient to model a channel according to the Nyquist-Shannon sampling theorem.\nVisualize a Plot of the Impulse Response:\nYou can plot the impulse response in the\nsParamterFitter app and export the object\nsParameterFit and the impulse response\nsParameterFitImpulse to the base workspace.\nUse Impulse Response for Channel Model in Serdes Designer\nYou can use the impulse response of the baseband channel model within SerDes Designer by selecting ""Impulse response"" for channel model and enter the base workspace variable\nsParameterFitImpulse that you created with the\nSimulate Channel Network with Transmitter and Receiver in Serdes Designer\nYou can use the impulse response of the baseband channel model within Serdes Designer in concert with TX and RX topologies to simulate an eye diagram.\nImpulse Response for Channel Model in Simulink\nYou can use the impulse response of the baseband channel model within Simulink by opening the Analog Channel block and clicking on the option ""Impulse Response"" under Channel Model. This will launch the', 'This action might not be possible to undo. Are you sure you want to continue?\nconnecting the DVI ports on the computer and monitor and the underlying protocol used for the transmission of video signals from the computer to the monitor. In recent years the CRT monitor is being rapidly replaced by the LCD monitor. CRTs have analog inputs, a PC must take an additional step and convert the digital data, essentially a string of digital ones and zeros, into analog voltages before it can be transmitted to a CRT monitor. This is done through a digital to analog converter (RAMDAC) in the PC. LCD’s are inherently digital devices, and for the display on an LCD using the VGA port, the PC must convert the data to analog format, as is the case with a CRT monitor. Then, the LCD monitor must have an analog to digital converter (ADC) and associated logic to convert the signal back to digital ones and zeros before it can be shown on the display. This is as shown below.\nLCD to VGA interface\nThis digital to analog conversion and subsequent analog to digital conversion results in signal degradation and delay during transmission and conversion\nThe implementation using DVI Standard is as shown below. ADD Card . as such DVI uses a TMDS transmitter on the computer and TMDS receiver on the monitor. The TMDS transmitter can be implemented – 1) On the motherboard directly 2) As an ADD card which plugs into the PCIe slot. LCD to DVI interface The underlying protocol used for DVI is TMDS or transition minimized differential signaling.\nMultiplying all three we get 115200000 or 115. TMDS protocol is capable of transferring up to 225 Mhz while the DVI standard based on it can support up to 165 Mhz in a single-link configuration. whereas the add card just consists of the TMDS transmitter and is generally cheaper.3) As a graphics card which plugs into the PCIe slot. HDMI and many other digital standards. . The rest of the bandwidth is used for clock and associated signaling. The 165 MHz bandwidth can support a resolution of 1600x1200 at a 60 Hz refresh rate.2 Mhz. Graphics Card The difference between a graphics card and an add card is – the graphics card has graphics processor unit useful for application which require accelerated graphics like games. TMDS serves as the underlying protocol for the DVI.\nthree color components are required: Red. Transition minimization is achieved by carrying out an XNOR operation on the sequence and selecting the sequence which has minimum transitions.A single link DVI using TMDS protocol is as shown below. Single Link DVI To drive an LCD and light up a pixel. TMDS then uses an advanced encoding algorithm to minimize transitions and at the same time generate a DC Balanced Sequence. The graphics controller outputs 24 bits in parallel with 8 bits representing each color component to the TMDS transmitter. The TMDS transmitter converts 8 parallel bits representing a color component into 8serial bits. Properties of TMDS 1) Transition Minimization Transition Minimization is carried out to prevent the generation of radio frequencies (which may affect other electronic devices) due to frequent 1-0 and 0-1 transitions. A 9th bit is added to indicate whether an XNOR operation was carried out or not. Each color component has 8 bits that allow 256 different color shades to be selected. For example. Using a combination of 256 shades for each color (RGB) allows up to 16 million colors to be displayed. Green and Blue (RGB). consider the sequence – The first bit is retained as it is and is XNORed with the second bit to generate the transition minimized 2nd bit. This 2nd bit is again XNORed with the 3rd bit of the original .\nThe method used to overcome this is known as DC Balancing and involves inverting some of the bits in the sequence and marking them as inverted. The generated sequence is – A 9th bit is added as a 1 to indicate transition minimization encoding has been carried out. the charge on the cable tends to resist the subsequent change of data to the opposite state and will cause data errors.sequence to generate the 3rd transition minimized bit and so on. giving . Consider a long sequence of 1’s as shown below. resulting in the sequence- 2) Dc Balanced Sequence Transmitting a long series of 1’s or 0’s at high speeds results in the charging of the line. The 10th bit is set to indicate polarity reversal. We then pick one word (8 bits) from the series of words that have the same state and add the 9th bit as 0 as no transition minimization is required.\nDifferential Signaling uses two wires with the second wire transmitting the opposite value of the first as shown below.3) Differential Signaling TMDS also uses differential signaling for immunity to radio interference and other noise. At the receiver one signal is subtracted from the other cancelling out the noise. .\nA dual link DVI can be implemented using 2 TMDS transmitters and 2 receivers providing a bandwidth upto 330 MHz. Dual Link DVI . This is as shown below.\nand vertical and horizontal sync pins associated with it. The advantage of this is the same connector or port can be used for both DVI and VGA. and DVI-A. but the analog signals needs to supplied to the analog pins on the connector.DVI-D. A dual link DVI-I connector and its pins are as shown below. 2 for green and 2 for blue giving a dual link. ie DVI-I connector does not perform an digital to analog conversion. ie TMDS based DVI signals. but only one at a time. But the analog signals to the corresponding pins in the connector needs to be given by the board or computer. DVI-D connector provides support only for digital signals. There are 6 data channels associated with the connector ie 2 channels for red. ie it provides pins through which analog signals or VGA signals can be transmitted. Plug and display is another standard for transmission of digital data which is no longer being used because of its expensive . The connector can also be of type single link or dual link to support a single or dual link DVI.There are three basic types of DVI connectors. DVI-I connector provides support for digital as well as analog signals. DVI-A connector is rare and used to connect to a DVI-I socket so as to tap only the analog signals and map the signals onto a VGA port to support a CRT monitor. DVI-I. You also pins for transmitting analog data.\nSignals for this standard can be passed through a DVI connector with Plug and Display connector at the other end. .connectors.']"	['<urn:uuid:9a1758bc-0849-41b6-b277-bfd046e8a187>', '<urn:uuid:a220f502-9b43-42a3-8796-1add71ab2254>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T05:06:54.527160	23	91	1854
23	millennium seed bank partnership how many seeds stored purpose of facility	The Millennium Seed Bank Partnership currently houses over 2.3 billion seeds at Kew's purpose-built facility in Wakehurst Place. The bank has successfully stored more than 12.5 percent of all wild plant species, focusing on endangered, endemic, and economically important species. These seeds aren't just passive reserves - they are actively used in projects to restore degraded habitats, increase ecosystem resilience, recover species in the wild, and can be used to breed adaptations into crops to help food systems adapt to climate change.	['23 September 2019\nPlants and policy: Tackling global challenges\nBiodiversity loss, climate change and sustainable development are urgent global issues. 2020 is a critical year to address all three - and plants are at the core of many solutions.\nIn October 2020, governments from 196 countries will gather at the UN Convention on Biological Diversity (CBD) in China, to set ambitious new global targets that aim to reverse biodiversity loss. This Post-2020 Global Biodiversity Framework will provide a roadmap for conservation efforts everywhere, going forward.\nThis is occurring at a time when the scale of biodiversity loss is staggering. Plants are intricately linked to human life, yet one in five plants are threatened with extinction. Drawing on over 250 years of plant and fungal expertise, Kew’s scientific research is at the forefront of informing policy - and contributing to biodiversity targets. From accelerating assessments of plants most at risk of extinction, to increasing food security in a changing climate, to protecting vulnerable ecosystems, Kew’s work is contributing plant and fungal based solutions to some of the world’s most pressing challenges.\nWhere to start in a biodiversity crisis?\nWith at least 390,000 plant species across the planet, and new species being discovered every year, there is a serious shortage of conservation resources to understand and monitor every single species. There is a risk that many species may go extinct before scientists can assess them, or before they are even discovered – potentially losing irreplaceable medicines, food sources, habitats, and climate regulators.\nIUCN Red List Assessments\nOver the past three years, Kew has been accelerating the number of plants being assessed for the IUCN Red List – the global source of information on species extinction risk. From 126 species of coffee (Coffea), to a plant genus that forms the basis of the Amazon rainforest (Myrcia), the status of many plant species has been captured for the first time. As well as continuing to assess new species, Kew’s Plant Assessment Unit will also be conducting further assessments – creating more ‘snapshots’ for species, so that not only the threat status of species can be determined, but also trends over time.\nThis is a particularly important indicator for measuring progress towards conservation targets - it allows us to answer key questions such as how many species are threatened with extinction, and are conservation efforts reversing these trends? These form an important component of our understanding of the current biodiversity crisis and help us measure progress towards global conservation targets.\nKew is working to champion plants and fungi, and increase their representation in these metrics, which have historically been under-represented in these assessments.\nSeeds as back-ups, and solutions\nWith land use change and exploitation of species accelerating, and climate change amplifying the challenges for species, the list of species in the biodiversity crisis is growing. Plants are fundamental to all life on Earth and seed banking is playing an essential role in creating a safety-net against extinction.\nThe Millennium Seed Bank\nWith a focus on endangered, endemic and economically important species, Kew is working with a network of partners to bank over a tenth of all wild plants in the Millennium Seed Bank Partnership. More than 12.5 per cent of wild plant species have been banked so far, with over 2.3 billion seeds currently housed at Kew’s purpose-built facility in Wakehurst Place.\nYet these seeds are not just passive ‘reserves’, locked away in a vault - they are also active solutions. The genetics of these seeds contain tools that can be applied to some of the world’s most urgent global challenges. They can be actively used in projects to restore degraded habitats, increase the resilience of ecosystems and recover species in the wild. Banking the wild relatives of crop species can reveal a suite of adaptations that can be bred into crops – enabling food systems around the world to adapt to a changing climate.\nWith a growing network of partners across the globe, this project meets all three objectives of the CBD: to conserve, sustainably use, and equitably share the benefits arising from the use of biodiversity.\nThe multiple value(s) of plants\nConservation work always requires a combination of approaches – and conserving species in their native habitats is crucial. An important limitation of seed banks is that not all seeds can be banked. This is particularly true of many tropical plant species – the very same species that form the basis of ecosystems that are globally relevant for climate regulation, and critical for millions upon millions of people locally, who depend on forests for food, medicine and livelihoods.\nThe tropics are therefore a major priority for biodiversity conservation at a global level - nowhere are the intricate links between biodiversity, sustainable development and climate change clearer.\nTropical Important Plant Areas (TIPAs)\nBuilding on an approach that has secured habitats across Europe, the Mediterranean and North Africa, Kew has carefully adapted Important Plant Area (IPA) criteria to suit the paradox of the tropics: a relative lack of plant data, but urgent need for action.\nWorking closely with partners in-country, Tropical Important Plant Areas (TIPAs) are identified within each country according to a range of criteria – from areas of unrivalled botanical richness, to concentrations of threatened species, to areas that are important locally because of plants that are used.\nFormally recognising these areas can help make a compelling case for protecting habitats, in regions that are trying to balance development with preserving irreplaceable biodiversity. TIPAs can help with the planning of protected areas, contributing to global goals to protect 17 per cent of land, or lead to legal recognition for community areas, contributing to the UN Sustainable Development Goals (SDGs). They can also put plants on the radar in the planning stages of development projects, allowing short-term and long-term trade-offs to be more accurately considered.\nTIPAs therefore act as a tool to empower local decision making, whilst simultaneously highlighting how national plant conservation fits within an international picture – bridging gaps between global policy and national context.\nA critical opportunity\nBending the curve of biodiversity loss is a momentous task. It requires a combination of approaches that include all aspects of biodiversity: from species, to genetics, to ecosystems.\nThe upcoming UN Convention on Biological Diversity represents a critical opportunity to galvanise the support and resources needed at a global level. The decisions made in these rooms will filter down to plants, people and habitats in every corner of the planet.\nKew will be striving to ensure plants and fungi are at the forefront of this. And our science continues to reveal the same message loud and clear - biodiversity loss is a major global problem, but conserving biodiversity offers major global solutions too.\nCorlett, R.T. (2016). Plant diversity in a changing world: status, trends, and conservation needs. Plant Diversity, 38(1), 10-16.\nDarbyshire, I., Anderson, S., Asatryan, A., Byfield, A., Cheek, M., Clubbe, C., Ghrabi, Z., Harris, T., Heatubun, C.D., Kalema, J. & Magassouba, S. (2017). Important Plant Areas: revised selection criteria for a global approach to plant conservation. Biodiversity and Conservation, 26(8), 1767-1800.\nMace, G.M., Barrett, M., Burgess, N.D., Cornell, S.E., Freeman, R., Grooten, M. & Purvis, A. (2018). Aiming higher to bend the curve of biodiversity loss. Nature Sustainability, 1(9), 448-451.\nWillis, K. J. (ed.) 2017. State of the World’s Plants 2017. Report. Royal Botanic Gardens, Kew.\nRead & watch\n16 January 2019\nA high extinction threat for wild coffee could rattle the sector\n6 November 2018\nPlant cryobiotechnology at Kew: past, present and future\n6 July 2018\nThe critically endangered Madagascar Banana\n22 May 2019\nOur food, our health\n17 May 2019\nSaving endangered species\n23 July 2019\nWe can see beyond the trees']	['<urn:uuid:2e4dd975-72b3-4fb8-b7c8-d0ba8f0a9056>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:06:54.527160	11	82	1282
24	when does company lose right buy back restricted stock employee	The company's buy-back right lapses progressively over time as shares vest. The right to buy back shares is lost gradually, typically over a 48-month period, with portions of shares becoming fully owned by the recipient each month of their service tenure.	"['Restricted stock is the main mechanism whereby a founding team will make sure that its members earn their sweat fairness. Being fundamental to startups, it is worth understanding. Let\'s see what it will be.\nRestricted stock is stock that is owned but could be forfeited if a founder leaves a company before it has vested.\nThe startup will typically grant such stock to a founder and develop the right to purchase it back at cost if the service relationship between the company and the founder should end. This arrangement can double whether the founder is an employee or contractor in relation to services performed.\nWith a typical restricted stock grant, if a founder pays $.001 per share for restricted stock, the company can buy it back at bucks.001 per share.\nBut not realistic.\nThe buy-back right lapses progressively occasion.\nFor example, Founder A is granted 1 million shares of restricted stock at cash.001 per share, or $1,000 total, with the startup retaining a buy-back right at $.001 per share that lapses as to 1/48th of the shares terrible month of Founder A\'s service tenure. The buy-back right initially holds true for 100% on the shares earned in the scholarship. If Founder A ceased employed for the startup the next day of getting the grant, the startup could buy all of the stock back at $.001 per share, or $1,000 finish. After one month of service by Founder A, the buy-back right would lapse as to 1/48th of your shares (i.e., as to 20,833 shares). If Founder A left at that time, the could buy back basically the 20,833 vested gives you. And so lets start work on each month of service tenure just before 1 million shares are fully vested at the finish of 48 months of service.\nIn technical legal terms, this isn\'t strictly dress yourself in as ""vesting."" Technically, the stock is owned but sometimes be forfeited by what called a ""repurchase option"" held the particular company.\nThe repurchase option could be triggered by any event that causes the service relationship in between your founder and the company to end. The founder might be fired. Or quit. Or why not be forced give up. Or collapse. Whatever the cause (depending, of course, from the wording of your stock purchase agreement), the startup can normally exercise its option to obtain back any shares that are unvested associated with the date of termination.\nWhen stock tied a new continuing service relationship might be forfeited in this manner, an 83(b) election normally in order to be be filed to avoid adverse tax consequences down the road for that founder.\nHow Is restricted Stock Applied in a Startup?\nWe are usually using entitlement to live ""Co Founder Collaboration Agreement India"" to relate to the recipient of restricted stock. Such stock grants can be generated to any person, even though a author. Normally, startups reserve such grants for founders and very key people. Why? Because anyone who gets restricted stock (in contrast for you to some stock option grant) immediately becomes a shareholder and have all the rights of an shareholder. Startups should stop being too loose about providing people with this stature.\nRestricted stock usually cannot make sense for every solo founder unless a team will shortly be brought .\nFor a team of founders, though, it will be the rule pertaining to which lot only occasional exceptions.\nEven if founders do not use restricted stock, VCs will impose vesting to them at first funding, perhaps not if you wish to all their stock but as to several. Investors can\'t legally force this on founders but will insist on it as a complaint that to funding. If founders bypass the VCs, this needless to say is not an issue.\nRestricted stock can be used as to a new founders and not others. Is actually no legal rule saying each founder must create the same vesting requirements. One can be granted stock without restrictions any kind of kind (100% vested), another can be granted stock that is, say, 20% immediately vested with the remaining 80% depending upon vesting, so next on. The is negotiable among creators.\nVesting do not have to necessarily be over a 4-year age. It can be 2, 3, 5, one more number which renders sense to your founders.\nThe rate of vesting can vary as well. It can be monthly, quarterly, annually, or other increment. Annual vesting for founders is relatively rare nearly all founders will not want a one-year delay between vesting points as they build value in the organization. In this sense, restricted stock grants differ significantly from stock option grants, which face longer vesting gaps or initial ""cliffs."" But, again, this almost all negotiable and arrangements alter.\nFounders may also attempt to barter acceleration provisions if termination of their service relationship is without cause or if perhaps they resign for grounds. If they include such clauses involving their documentation, ""cause"" normally always be defined to utilise to reasonable cases certainly where an founder isn\'t performing proper duties. Otherwise, it becomes nearly impossible to get rid associated with an non-performing founder without running the risk of a court case.\nAll service relationships within a startup context should normally be terminable at will, whether not really a no-cause termination triggers a stock acceleration.\nVCs typically resist acceleration provisions. When agree these in any form, it will likely remain in a narrower form than founders would prefer, items example by saying in which a founder are able to get accelerated vesting only should a founder is fired from a stated period after an alteration of control (""double-trigger"" acceleration).\nRestricted stock is used by startups organized as corporations. It may possibly be done via ""restricted units"" in an LLC membership context but this a lot more unusual. The LLC a excellent vehicle for many small company purposes, and also for startups in the right cases, but tends turn out to be a clumsy vehicle to handle the rights of a founding team that in order to put strings on equity grants. be done in an LLC but only by injecting into them the very complexity that a majority of people who flock with regard to an LLC seek to avoid. The hho booster is to be able to be complex anyway, will be normally advisable to use the corporation format.\nAll in all, restricted stock is often a valuable tool for startups to utilization in setting up important founder incentives. Founders should take advantage of this tool wisely under the guidance from the good business lawyer.']"	['<urn:uuid:c3242822-3b3e-47db-8abd-af0faed8ac6b>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T05:06:54.527160	10	41	1096
25	I'm worried about my online privacy - what are the different types of tracking that websites use and how can I protect myself from being monitored?	Websites use several types of cookies to track users: Strictly Necessary Cookies for basic website navigation, Performance Cookies that anonymously collect usage information, Functional Cookies that remember user preferences, and Advertising Cookies used for targeted marketing. There are also Third-Party Cookies from external sites like Facebook or Twitter, and profiling cookies that monitor user behavior to create customized ads based on browsing habits. To protect your privacy, you can customize your browser's cookie settings to refuse cookies, though this may limit website functionality. Different browsers have different procedures for clearing cookies, and you can find specific instructions for browsers like Google Chrome, Firefox, Internet Explorer, Opera, and Safari at www.allaboutcookies.org. For mobile devices, you'll need to consult your device's instruction manual to remove cookies.	['What are cookies?\nCookies, also known as tracking cookies or browser cookies are a small set of encrypted text files or information that is being stored in your device or computer when you visit a website. The web developers are using cookies to help the users navigate the websites and perform specific functions. It is created when a browser loads a certain website. The website will then send the information to the browser and create a small text file. Each time the user loads the same website, the browser will retrieve and send the file to the server. Cookies are capable of carrying information from one website to another related site so disabling it may prevent the users from using a certain website.\nWhich cookies are being used on our website?\nWhen you visit our website, we collect common internet log information and the details about your behaviour patterns. The Crazy Food Masala is doing this so we can learn things like the number of visitors to the different parts of our website. We are collecting such information yet we never attempt to find out or disclose the identities of those visiting our website. We will never associate the data we gathered from our website to any personally identifying information from any source. If we want to collect personally identifiable information from our users, we will inform you and explain our intentions for doing it.\nWe have listed the types of cookies we are using on our website. Please refer to the information below:\n- Strictly Necessary Cookies\nStrictly Necessary Cookies are important in order to allow the visitors to move around our website and use the features in it. Without this, we cannot provide you with the services you are asking. We often collect cookies like:\n- NET SessionID\nThis cookie is used when you load a web browser that uses ASP.NET session state. This type of cookie will be deleted once you close your browser.\n- CMSPreferred Culture\nThis cookie is used to identify the browser language of the visitor. It normally expires within 24-hours after you first access the website.\nThis cookie is used to identify the visitor’s preferred colour scheme for our website. It normally expires within 24-hours after you first access the website.\n- Performance Cookies\nPerformance Cookies will allow the website to collect the information about how our visitors use a website, such as the pages that they often visit and if they receive error messages from web pages. This type of cookie doesn’t collect any information that identifies our visitors. The cookies collected are gathered and therefore anonymous. This cookie is only used to improve our website.\n- Functional Cookies\nFunctional Cookies allow our website to remember the choices that our visitors are making (e.g. the language or region they are currently in). It also provides more personal and enhanced features. These types of cookies are also used to remember such changes you have made on our website such as the text size, fonts, and other parts of our website that are customizable. We also used these cookies to provide you with the services you are asking such as commenting to a food blog or article or watching a video. The information that was collected by these cookies is in anonymous form and it cannot track the visitor’s browsing activity on other websites.\n- Advertising Cookies\nAdvertising Cookies such as the DoubleClick DART is the type of cookie that is being used by Google to provide Internet ad serving services to its partners. Our website may display AdSense ads or participate in the certified ad networks of Google. The DoubleClick DART cookie is being used to help the marketers determine how well their paid search listings and advertising campaigns work. Different websites use the DART technology to manage their paid search listings. The information that this cookie is collecting helps the marketers to know how many users clicked on their paid listings or internet ads and which paid listings or internet ads they clicked on.\n- Third-Party Cookies\nWe use Google Analytics to improve the way we deliver our website. Google Analytics is used for tracking our website visitors. It collects information on how the visitors are using our website. We use that information to compile reports, which can help us improve our website. The cookies are collecting information anonymously, including the number of our website’s visitors and the pages they visited.\nWhen a visitor loads a page that contains content embedded from sites like Facebook or Twitter, the visitor will be presented with cookies from those websites. The Crazy Masala Food does not control the distribution of those cookies. You should check similar third party website for more information and you should also review these since they will govern the use of the information you submit or the information collected by cookies when you visit these websites.\nHow to remove the cookies?\nVisitors may refuse to accept cookies by customizing your browser’s cookie settings. Although cookies are vital in navigating a website, some users preferred to protect their privacy online. There are no standardized procedures for removing the cookies because different browsers have different procedures in clearing cookies. For more information on how to remove cookies using browsers like Google Chrome, Firefox, Internet Explorer, Opera, and Safari, please visit https://www.allaboutcookies.org/manage-cookies/clear-cookies-installed.html.', 'WHAT ARE COOKIES?\nCookies are pieces of information that are input onto your browser whenever visiting a website or using a social network with a computer, smartphone or tablet. Each cookie contains different data, such as the source server name, a number identifier, etc. . Cookies may remain stored into the system either for the duration of a single session (i.e. up to the moment when the web-surfing browser is closed) or longer time frames. They might also contain a single id code.\nWHAT ARE COOKIES FOR?\nSome cookies are used for log-in purposes, session monitoring and for memorizing relevant information on the users who visit specific web pages. Such cookies, known as technical, are oftentimes more useful, as they can speed up the web-surfing process to a large extent, for instance by facilitating online-purchasing procedures, as well as other ones pertaining logging into reserved access areas or detecting the most used language while surfing. A peculiar type of cookies, known as analytics, are used by website owners to retrieve overall information on the number of users and how they surf through websites, thus allowing to draft general statistics on a given service and its usage. Other cookies may be used to monitor and profile users instead, namely while they are surfing, allowing to study their movements, web-surfing or usage-related habits (e.g. what they buy, read, etc.), with the goal of sending specifically-aimed and customized service-related ads (i.e. Behavioural Advertising). This is the case of profiling cookies. This process occurs because those web spaces are designed in order to recognize your pc or another terminal that you use for web-surfing purposes (smartphone, tablet), and to eventually send “profiled” promotional messages according to your researches and web usage. It might happen that a web page contain cookies coming from other websites and contained in various host components of the page itself, such as advertising banners, pictures, videos, etc. . In these cases, we may refer to them as Third-Party Cookies, which are usually intended for profiling purposes. Hence all the cookies downloaded on a pc, smartphone or tablet can be read by third parties who are not necessarily website owners.\nHOW TO OPERATE AND DELETE COOKIES\nThe functioning modes as well as the available options to reduce or block cookies can be changed by editing the settings of your own browser. Furthermore, you can visit the website ww.aboutcookies.org for info on how to handle/delete cookies according to the different browsers used. In order to delete cookies from your smartphone/tablet browser, you will need to consult the instructions manual of your device.\nLIST OF COOKIES ON THE WEBSITE\nThe following bullet-point template reports the non-technical cookies that are present on the website. Third-party cookies are an exclusive and direct liability of the owner and are subdivided into the following major categories:\n- Analytics. Such cookies are used to collect and analyse statistical information on accessing/visiting websites. Further information on privacy and their usage can be found directly on the websites of the respective owners.\n- Widgets. All the graphic components belonging to a user interface of a program fall under this category, which has the aim of facilitating the user while interacting with the program itself. A relevant instance are widgets, Facebook, Google+ and Twitter cookies. Further information on privacy and their usage can be found directly on the websites of the respective owners.\n- Advertsing. Cookies used for advertisements on websites fall under this category. Google, Tradedoubler fall under this category. Further information on privacy and their usage can be found directly on the websites of the respective owners.\n(https://tools.google.com/dlpage/gaoptout). The non acceptance of such cookies may compromise some of the website’s functions.\nUSING FLASH COOKIES.\nThe website may use Adobe Flash Player in order to provide some multimedia content. As far as the vast majority of computers are concerned, this program is installed by default. While viewing such type of content, Google Analytics will store additional data on the computer, also known as Flash Cookies (or Local Share Object), through which the owner can be informed regarding the total number of times that a certain Audio/video file is opened up, the number of people who use it until the end and those who chose to close it beforehand instead. Adobe’s website provides information on how to remove or disable Flash Cookies (see http://www.adobe.com/products/flashplayer/security). This bears reminding that limiting and/or deleting this type of flash cookies may jeopardize the available functions for all those apps relying on Flash technology.']	['<urn:uuid:4f126b87-7ab9-4deb-919a-260877698685>', '<urn:uuid:ba97dd5c-c563-4032-beab-0c0d3ceadf9e>']	open-ended	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-13T05:06:54.527160	26	124	1636
26	I'm worried about wasting food and climate change. Looking at Norway, how much food do consumers throw away yearly, and how does food waste impact global warming?	Norwegian consumers throw away 42 kg of edible food annually. Regarding climate impact, food waste is a significant contributor to global warming - if food waste were a country, it would have the world's third largest emissions of climate change gases. When food is sent to landfills, it releases methane, a greenhouse gas up to 30 times more potent than CO2, which has contributed to 30-50% of current temperature rises.	['A third of all food produced globally is destroyed or thrown away. In Norway, the average consumer throws away 42 kg of food annually that could have been eaten. Food waste in the food supply chain equals 68 kg per person per year.\n– Throwing away food is a waste of resources and good produce. If we manage to reduce food waste, we will be able to save both money and the environment. This is not just the responsibility of each consumer but for food industry as a whole, says Minister for climate and environment, Vidar Helgesen.\nHalving the food waste\nMany measures are already being implemented to prevent food waste. The supermarkets reduce the price of food nearing its sell-by date, smaller loaves of bread are available, and there are campaigns for “buy one, pay for one” instead of “take three, pay for one” as we have been used to quoting.\n– In order to incentivise the reduction of food waste, the government has removed VAT on food that is donated to charity. I would like to challenge the food industry to be innovative and creative in the coming years. The food industry’s actions will be the deciding factor in halve food waste by 2030, says Minister for Children and Equality, Solveig Horne.\nThe parties agree to support the United Nations (UN) sustainable development target to halve food waste at the retail and consumer level, and reduce food losses along production and supply chains by 2030. Food waste is both an environmental issue and a challenge for combating climate change. Estimates show that if food waste were a country, it would have the world’s third largest emissions of climate change gasses.\n– This agreement to reduce food waste is a new way of working where we set ambitious goals. I have great faith that this agreement will contribute to reducing food waste in Norway, says Minister for Climate and Environment, Vidar Helgesen.\n350 000 tonnes of fully usable food are thrown away in Norway every year. Well over half of this is wasted by consumers.\n– The most important actions that consumers can take are to plan food shopping and preparation better, as well as to look at, smell and taste food before it is thrown away. We can also be more aware of sell-by-date labelling, says Minister for children and equality, Solveige Horne.\nAttracting international interest\nThe good co-operation between industry and authorities on food waste has attracted international interest. This work was presented only last week at a meeting in EUs platform for food waste in Brussels. We are now talking about the “Norwegian Model”. Perhaps our experiences with this agreement format can inspire similar activity in other countries to adopt new ways of working.\n– It is paramount that the whole production and supply chain from primary producers to consumers, must take responsibility to reduce food waste. I am committed to reducing food waste in Norway and would like to pursue the possibility of a Nordic co-operation under the Nordic Council of Ministers in Ålesund next week, says Minister for Agriculture and Food, Jon Georg Dale.\n– Norway is the 2nd largest exporter of seafood in the World and we have a responsibility to not waste our valuable natural resources, says Minister for Fisheries, Per Sandberg.\nAccording to this agreement, the authorities will carry out mapping of food waste from consumers whilst the authorities and food industry together will carry out measures resulting in reduced food waste by consumers. The food industry will co-ordinate mapping and reporting from their commercial activities whilst the authorities will compile national statistics.\n– It is important that we have the entire food industry co-operates in this work. Mapping and data collection are necessary in order to implement effective measures, says Vidar Helgesen.\nThe signed agreement replaces the statement of intent that was signed on the 7th of March 2015.\n– The Ministry of Health and Care Services has had positive experiences regarding industry agreements on nutrition. I am therefore glad that we can sign a similar agreement on food waste, says State Secretary Frode G. Hestnes.\nThe Ministers who have signed the agreement are the Minister for Climate and Environment, Vidar Helgesen, and the Minister for Children and Equality, Solveig Horne. In addition the State Secretary to the Ministry of Agriculture and Food, Hanne Maren Blåfjelldal, State Secretary for trade and Industry, Roy Angelvik and State Secretary for Health and Care Services, Frode Hestnes. Representatives from the breadth of the food industry also signed the agreement. These include the Grocery Sector’s Environmental Forum, The Grocery Suppliers of Norway, Food and Drink Norway, the Norwegian Hospitality Association, National Federation of Service Industries, The Norwegian Agrarian Association, The Norwegian Seafood Association, Norwegian Farmers and Smallholders Union, The Norwegian Fishermen’s Association, Confederation of Norwegian Enterprise, Norwegian Seafood Federation and the Enterprise Federation of Norway.', '“There is a lot to unpack in the latest IPCC climate report, and we know that all of this new information – in addition to witnessing this summer’s unprecedented extreme weather events – can feel pretty overwhelming.\nWe’re here to help break down some of the key takeaways and explain the science behind how fighting food waste can contribute to positive change.\nWhat is the IPCC Report?\nUpdated every six-seven years to reflect new evidence on climate change, the report pulls together the findings from more than 14,000 peer-reviewed studies on climate science. For the first time, the latest report (released this month) included a chapter dedicated to weather extremes as well as a statement that human activities are causing climate change.\nBelow are a few topics that the report touched on, and why they matter.\nExtreme Weather Events\nHuman-caused greenhouse gas emissions have led to extreme weather events, which are linked to rising temperatures. We’ve seen this from the recent heavy flooding across Europe, China, Nigeria, and South Sudan – to the wildfires on the West Coast, Greece, Turkey, and Canada – which just had a record heatwave.\nWith our planet heating up, more warm air is trapped in our atmosphere causing the moisture level to increase and rainfall to become more extreme – leading to catastrophic flooding. Around the world we are also seeing warmer and drier conditions making forests more susceptible to wildfires – wildfire seasons getting longer and more severe.\nThe Impact on Our Planet\nWe’re seeing “widespread and rapid” changes in the oceans, ice and land surfaces of every inhabited region on earth. Though there is still time for us to create change, we are getting closer to crossing more of Earth’s tipping points.\nAll About Methane\nMethane is a greenhouse gas that’s up to 30 times more potent than CO2. As a Waste Warrior, you may know that when food gets sent to the landfill, this dangerous gas is emitted as the food waste rots.\nEmissions of methane have made a huge contribution to the current warming of our planet (30-50% of the current rise in temperatures). Experts believe that if methane emissions were cut by 40-45% over the next decade, we could reduce the increase in global temperatures by 0.3C by the year 2040.\nFIGHTING FOOD WASTE MATTERS!\nThe good news is that being in the fight against food waste means we’re in the fight against climate change as well. Reducing food waste is a key solution to cutting down on methane, which is responsible for so much of the findings in this latest report.\nBy reducing food waste, we actively ensure that food does not end up in landfills, and that it wasn’t produced in vain. Wasting food means wasting resources, and also all the emissions that it took to produce it in the first place.\nEach and every one of us can make a difference in the fight against food waste, and therefore the fight against climate change. “\nCEOforLIFE – We promote life. We support the SDGs.']	['<urn:uuid:f76a799f-b72c-4906-99a9-ec4d1d38e35d>', '<urn:uuid:b7861525-17e1-4458-a8ad-bdfa6ea557cc>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:06:54.527160	27	70	1318
27	What makes hesitance problematic during service receive?	Hesitance is a major problem in service receive because table tennis is a touchy game with high speed and difficult-to-control spin. A soft or uncertain return often plays into the opponent's hands. Making an assertive stroke with full confidence, even if the shot selection isn't perfect, gives better chances of making a quality receive.	"[""|Tips for Effective Service Receiving|\nLearn to Read The Ball Contact: Reading service will ultimately provide you with the best idea of how to return the ball most effectively. Watching the moment of ball contact and the opponent's service action will give you a lot of information as to what they are serving. Also remember the first bounce on the table will give you information on the length of the serve, more often than not a bounce closer to the net is characteristic of a short serve and closer to the server, a longer serve.\nAlso pay special note as to how much contact the opponent makes on the ball, you can observe their brushing action on the ball, or the 'dwell time' which indicates the amount of spin on the ball. Including service receive against different service types in training is very important as it is one of the 4 key skills to master in order to become a well rounded player and to win matches.\nBe Decisive: Hesitance is often your worst enemy when it comes to service receive. Table Tennis can be a very touchy game, alongside it's high speed reputation. Spin is difficult to control and often a soft or uncertain return of service will play right into the hands of your opponent. The best thing to do when receiving serve is to make an assertive stroke once you have decided how to receive serve. Even if it is not the correct shot selection, your chances are much better of making a quality receive of serve if you are making a shot with full confidence. This also includes having decisive placement, this is a crucial part of your return as a well placed and high quality receive can be devastating to your opponent.\nBe Assertive: In table tennis the first attacker usually holds an advantage. Taking the initiative where opportunities lie is a crucial step in taking control of a point. When returning serve it is important not to be too passive when the chance arises to make an offensive play. Often we see players tripping up over medium long serves and pushing the ball back long (as a medium long serve can be difficult to play short), this gives their opponent the first chance to attack. In reality if a serve will only bounce once, it is important to develop the mentality to make an attacking stroke. This often requires a player to have an attacking mindset. Be assertive, recognise each and every chance to take the initiative.\nAvoid Big Stroke Actions: Service receive is often well controlled with concise strokes. A large swing can be inaccurate or ineffective. Keeping shorter and more controlled strokes is often a better option for efficiently returning most serves. This minimises the level of risk involved, this is important as service receive is your first foot in the door into a point.\nHopefully these key pointers are enough for you to work with and focus on for increasing the effectiveness of your service receiving, even though it was quite a brief outline of the large topic of service receive. Thanks for your question Arjun! :)""]"	['<urn:uuid:0cbc93ba-3c91-4c36-93b4-310b0ae05c1c>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:06:54.527160	7	54	526
28	safest way protect small company online	The best way to protect a small business is to establish basic cybersecurity defense measures, educate employees, follow best practices, and invest in cyber security insurance services. It's important to stay informed, avoid suspicious services or material, and report data breaches quickly. Nearly half of small businesses suffered cyberattacks last year, with average losses in the tens of thousands of dollars.	['We’d all like to believe we are immune to cybersecurity threats, but the truth is that anyone could be hacked at any time. It doesn’t matter if you own a large enterprise company or are an individual doing a little online shopping, hackers are hungry for your personal information.\nThis is especially true for small businesses. In fact, nearly half of small businesses suffered cyberattacks last year, and the average loss of business was in the tens of thousands of dollars!\nWhile cybersecurity efforts have improved in the past decade, so have the threats.\nContinuously evolving technologies and risks have locked us in a digital arms race, and cybercriminals are not showing any sign of slowing down.\nDigital Arms Race\nIt seems that every time we discover and patch a network vulnerability, another one is exploited. It can be enough to make your head spin. Tech news source ZDNet reports that hackers, looking to combat our digital solutions, have begun grouping together to cause more damage: “Cybercrime gangs are now almost as sophisticated as the big businesses they are trying to steal from, leading to a new security arms race that companies are losing.”\nThat’s right. Hacking has matured beyond lone-wolf attacks and is regularly taking advantage of automated or brute force attacks to infiltrate organizational data.\nCredential stuffing, for example, uses large scale automated attacks to test stolen login credentials until gaining access to financial assets. Hackers then use these accounts to transfer money to themselves or hold it for ransom until the rightful owner pays a lump sum.\nHacking software like this can be easily obtained on the dark net and used against your small business. That’s scary stuff! And, according to the OWASP Automated Threat Handbook, there is no shortage of these kind of attacks.\nOf course, it’s not just company desktops and laptops being targeted. Cybercriminals are moving to mobile as more workplaces adopt bring-your-own-device (BYOD) policies. While BYOD is incredibly useful in improving employee productivity, it has made it even easier for hackers to exploit your data through SMS phishing scams, unsecured Wi-Fi connections or unencrypted mobile data transfers.\nAnd it doesn’t stop there. Evidence shows that hackers are already gunning for the burgeoning Internet of Things in hopes of discovering security gaps before the market even takes off.\nWhile the most adept hackers are developing more advanced ways to attack state-of-the-art security solutions, others prefer to infiltrate your small business through methods tried and true.\nPhishing scams and spam email attachments are still some of the primary methods cybercriminals use to access your most valuable data. That’s because these techniques prey on the weakest link in the cybersecurity chain – your employees.\nHuman beings aren’t perfect. We are often tricked by phony sign-in pages or sketchy download offers. Hackers figure that if they cast out a large enough net of fraudulent emails, someone is bound to click it. Then boom! Your data is under their control.\nOther security gaps include old software or outdate web browsers that have not patched existing security flaws. While your employees may not realize it, ignoring regular update prompts from these services could put your whole organization at risk.\nIn the end, educating your employees, following best practices and investing in cyber security insurance services can greatly improve your chances of fending off a data breach.\nThe Best Offense Is a Good Defense\nWhile the news about data breaches can be alarming, the truth is that establishing basic cybersecurity defense measures is enough to convince many hackers to look for an easier target.\nEven newer, more high-tech threats can be foiled by following a few basic rules: stay informed, avoid suspicious services or material and report data breaches quickly before things get out of hand.\nMany people ignore the reality of cybersecurity risks until it’s too late. Don’t be one of them!']	['<urn:uuid:0b7c4aca-ea39-46ab-9a7b-945e621de14d>']	open-ended	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T05:06:54.527160	6	61	643
29	impact museums economic contribution united states taxes jobs revenue statistics	Museums have a significant economic impact in the United States. According to data, museums generate more than $12 billion per year in tax revenue for federal, state and local governments. In terms of employment, arts institutions support 726,000 jobs and directly employ 372,100 people in the US, which is more than double the employment in the professional sports industry.	['Blockbuster exhibitions are often a break-even prospect for museums, with any boost to revenue from ticket sales, memberships, retail purchases and other visitor spending offset by the costs of production, installation and operations. But what impact do the thousands of cultural consumers who turn up for those exhibitions have on local economies?\nBased on US government statistics and tourism reports, blockbuster shows can create significant gains for their host cities. “We know that a vibrant arts community is good for local businesses,” says Randy Cohen, the vice president of research and policy at the art lobbying organisation Americans for the Arts. “And a blockbuster brings people from out of town, out of the country, and they provide business for local merchants.”\nThe larger arts industry contributed $764bn to the US economy—more than agriculture or transportation—according to data gathered by the US Bureau of Economic Analysis. A study released in 2018 by the American Alliance of Museums (AAM), using information from the Bureau of Labor Statistics, states that arts institutions support 726,000 jobs in the US and directly employ 372,100 people—more than double that of the professional sports industry. The AAMs report also showed that US museums generate more than $12bn per year in tax revenue to federal, state and local governments.\nWere not investing in a frill but in an industry that draws to the community people who spend money there\nSurveys conducted by Americans for the Arts found that each art event attendee spent around $31.47, on top of admission costs, which equated to an estimated $102.5bn in additional spending during 2015 (the year of its most recent surveys). “What this shows is that when we invest in the arts, were not investing in a frill but in an industry that draws to the community people who spend money there, and it keeps our neighbours and their hard-earned discretionary dollars right there in town,” Cohen says.\nOne recent example is the hugely popular touring exhibition Yayoi Kusama: Infinity Mirrors, which quickly became a “city phenomenon” when it first opened at the Hirshhorn Museum and Sculpture Garden in Washington, DC, in 2017, according to the organising institutions director Melissa Chiu. Kusamas work “attracted visitors that had never come to the museum before” and “had a lasting effect on public awareness of the museum”, Chiu says. The exhibition also created jobs and generated extra revenue in many of the cities it later travelled to. The show contributed $5.5m to the Cuyahoga County economy when it was shown at the Cleveland Museum of Art (9 July-30 September 2018), according to a study by the local research firm Kleinhenz & Associates. The museum hired 120 extra employees for visitors services and other exhibition-related work, and while most were temporary, some stayed on to support other exhibitions, a spokeswoman says.\nThe High Museum of Art hosted a blockbuster Yayoi Kusama show\nCourtesy of High Museum of Art\nSimilarly, the High Museum of Art in Atlanta held a special job fair to hire 60 part-time, temporary workers in guest relations, the museum sRead More – Source']	['<urn:uuid:4fdfe119-d1ed-468d-ae2f-7af0f6686849>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:06:54.527160	10	59	511
30	How do treatments differ between athlete's foot and psoriasis?	Athlete's foot can be treated with over-the-counter antifungal medicines like creams, sprays, or powders, which typically take a few weeks to work. In contrast, psoriasis, being a genetic autoimmune condition where skin cells are produced too quickly, requires different treatment - it won't respond to antifungal treatments but instead needs systemic medications that slow down skin cell production. While both conditions can cause red, itchy skin, psoriasis produces thick scales and white/silver patches that may bleed, and can affect multiple body areas like knees and elbows, whereas athlete's foot primarily affects the feet. For severe cases of athlete's foot, a doctor may prescribe steroid creams or antifungal tablets.	"[""General signs and symptoms which are most common in athlete’s foot infections include:\n- Skin which is red (rash-like and raw), dry or moist and / or flaky, cracked (fissuring) and scaly lesions (peeling) – especially in the toe webbing and / or soles\n- Soggy skin\n- Itchy skin (or varying degrees of stinging or burning sensations) – that often worsens when shoes and socks are removed\n- Small blisters (which may be painful) or ulcers (which may leak fluid)\nAthletes foot may affect one or both feet at one time. A ‘two feet, one hand’ pattern is fairly common with fungal growth spreading to the hands via direct contact (i.e. scratching or picking) with infected feet.\nHand fungal infections are known as tinea manuum. Infection can spread to the toenails as well. Some fungal infections result in rash-like, red, raw and blistery symptoms, while others produce thick, dry and scaly types of effects.\nTypes of Athlete's Foot\nThe different types of athlete’s foot are5:\n- Toe web infections: Infections are initially seen between the fourth and fifth toes, which appear scaly (or peeling) and cracked. Infection can spread to the rest of the foot, particularly the soles. Also known as an interdigital infection, the skin becomes red (inflamed), rash-like, scaly or peeling and may give off a smelly discharge. In severe instances, the skin may turn green in colour.\n- Moccasin infections: The first signs of infection may be a little soreness on the sole of the foot, and skin along the heel may become thick, itchy and cracked (dry). Dryness and scaling (which can resemble eczema) can extend from the soles up the sides of the feet. Toenails can also discolour and thicken when infected, and begin to crumble. Toenails may also pull away from the nail bed, and in some cases, loosen and fall out. If toenails are infected, combination treatment may be required (toenails are required to be treated separately).\n- Vesicular infections: This type of infection may begin with an outbreak of tiny fluid-filled blisters (which may be itchy and / or painful), usually along the soles of the feet. Blisters can surface anywhere on the foot, however, and can sometimes be accompanied by a bacterial infection as well (if blisters burst open). Open sores or ulcers can also develop causing a bacterial ulcerative infection. These can become inflamed, painful and may ooze. Signs of a bacterial infection can include a honey-yellow crust on the foot, redness, swelling, warmth and pain.\nHow to tell the difference between athlete’s foot and psoriasis\nPsoriasis is a genetic autoimmune condition6 which is not as a result of a fungal infection, and is not contagious. In the case of psoriasis, skin cells are produced at a faster rate than is normal, accumulating on the surface of the skin. The skin cells do not fall off naturally and instead develop into thick scales and / or white/silver patches. The result is dry and itchy skin which can also be painful. The condition can either affect just a small area of the body or in some patients, cover large areas.\nSymptoms between the two skin conditions are a little similar, but there are a few differences:\n- Psoriasis: Skin is covered by scales and red patches, which may be painful. Dry cracked skin may also bleed. Joints may also become swollen and painful, and nails may thicken or become pitted.\n- Athlete’s foot: Skin may be red and scaly (peeling).\n- Both: Itching or burning sensations on or around red, rash-like areas of skin.\nLocation on the body may also be a tell-tale sign\nThe feet are the main area of the body affected by athlete’s foot, with the potential for the infection to spread elsewhere. Patches of skin irritation that occur around the knees, elbows, back, trunk etc. are more typical of psoriasis than a fungal infection (athlete’s foot).\nAthlete’s foot may be easily treated with anti-fungal ointments and creams which can be bought over-the-counter. Psoriasis is not likely to clear with the use of the same ointments and works better with the use of systemic medications which slow down the production of skin cells.\nWhen in doubt, a consultation with a medical doctor can help to clarify which of the two conditions a person may have. This can be done with a physical inspection of the affected areas of the body (physical examination) and a laboratory test (using a sample).\n5. HealthLinkBC (British Columbia). October 2017. Athlete's Foot: https://www.healthlinkbc.ca/health-topics/hw28392 [Accessed 29.08.2018]\n6. National Center for Advancing Translational Sciences. Psoriasis: https://rarediseases.info.nih.gov/diseases/10262/psoriasis [Accessed 29.08.2018]"", ""Athlete's foot is a common fungal infection that affects the feet. You can usually treat it with creams, sprays or powders from a pharmacy, but it can keep coming back.\nCheck if you have athlete’s foot\nSymptoms of athlete's foot include:\nIt can also affect your soles or sides of your feet. If it's not treated, it can spread to your toenails and cause a fungal nail infection.\nAthlete's foot sometimes causes fluid-filled blisters.\nA pharmacist can help with athlete’s foot\nAthlete's foot is unlikely to get better on its own, but you can buy antifungal medicines for it from a pharmacy. They usually take a few weeks to work.\nAthlete's foot treatments are available as:\nThey're not all suitable for everyone – for example, some are only for adults. Always check the packet or ask a pharmacist.\nYou might need to try a few treatments to find one that works best for you.\nHow you can help treat and prevent athlete’s foot yourself\nYou can keep using some pharmacy treatments to stop athlete's foot coming back.\nIt's also important to keep your feet clean and dry. You don't need to stay off work or school.\ndry your feet after washing them, particularly between your toes – dab them dry rather than rubbing them\nuse a separate towel for your feet and wash it regularly\ntake your shoes off when at home\nwear clean socks every day – cotton socks are best\ndo not scratch affected skin – this can spread it to other parts of your body\ndo not walk around barefoot – wear flip-flops in places like changing rooms and showers\ndo not share towels, socks or shoes with other people\ndo not wear the same pair of shoes for more than 2 days in a row\ndo not wear shoes that make your feet hot and sweaty\nKeep following this advice after finishing treatment to help stop athlete's foot coming back.\nSee a GP if:\n- treatments from a pharmacy do not work\n- you're in a lot of discomfort\n- your foot is red, hot and painful – this could be a more serious infection\n- you have diabetes – foot problems can be more serious if you have diabetes\n- you have a weakened immune system – for example, you have had an organ transplant or are having chemotherapy\nTreatment for athlete’s foot from a GP\nYour GP may:\n- send a small scraping of skin from your feet to a laboratory to check you have athlete's foot\n- prescribe a steroid cream to use alongside antifungal cream\n- prescribe antifungal tablets – you might need to take these for several weeks\n- refer you to a specialist called a dermatologist for more tests and treatment if needed\nHow you get athlete’s foot\nYou can catch athlete's foot from other people with the infection.\nYou can get it by:\n- walking barefoot in places where someone else has athlete's foot – especially changing rooms and showers\n- touching the affected skin of someone with athlete's foot\nYou're more likely to get it if you have wet or sweaty feet, or if the skin on your feet is damaged.""]"	['<urn:uuid:6a2cb445-049f-4df9-a169-e9fb3911e1d0>', '<urn:uuid:92a59323-9db1-4e7c-b3ff-09510639e437>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:06:54.527160	9	108	1297
31	What role did cultural identity and civilization play in the development of both Petra and the Parthenon during their respective historical periods?	The Parthenon embodied Athenian cultural identity and their view of themselves as civilized people among barbarians, expressing their democratic ideals and achievements through its refined architecture. For Petra, while its exact cultural origins are unclear, the city reflected a unique blend of cultural influences - the Nabataean civilization combined Greek, Egyptian and Syrian elements in their architecture, particularly evident in their tomb designs which evolved from simple pylon-tombs to elaborate temple facades incorporating multiple cultural styles during the Ptolemaic period.	"['The Treasury – Petra.\nIn order to get to this hidden place you have to walk down the beautiful canyon of Sik. You loose a breath when you get to the end of this narrow passage, between two high colorful sand rock walls, and you see this amazing temple.\nPetra is one of the world heritage sites.\nSome history of Petra (from Wikipedia):\nSo far, no method has been found to determine when the history of Petra began. Evidence suggests that the city was founded relatively late, though a sanctuary may have existed there since very ancient times. This part of the country was traditionally assigned to the Horites, probably cave-dwellers, the predecessors of the Edomites. The habits of the original natives may have influenced the Nabataean custom of burying the dead and offering worship in half-excavated caves. However, the fact that Petra is mentioned by name in the Old Testament cannot be verified. Although Petra is usually identified with Sela which also means a rock, the Biblical references are not clear. 2 Kings xiv. 7 seems to be more specific. In the parallel passage, however, Sela is understood to mean simply ""the rock"" (2 Chr. xxv. 12, see LXX). As a result, many authorities doubt whether any town named Sela is mentioned in the Old Testament.\nIt is unclear exactly what Semitic inhabitants called their city. Apparently on the authority of Josephus, Eusebius and Jerome, assert that Rekem was the native name and Rekem appears in the Dead Sea scrolls as a prominent Edom site most closely describing Petra. But in the Aramaic versions Rekem is the name of Kadesh, implying that Josephus may have confused the two places. Sometimes the Aramaic versions give the form Rekem-Geya which recalls the name of the village El-ji, southeast of Petra. The capital, however, would hardly be defined by the name of a neighboring village. The Semitic name of the city, if not Sela, remains unknown. The passage in Diodorus Siculus (xix. 94–97) which describes the expeditions which Antigonus sent against the Nabataeans in 312 BC is understood to throw some light upon the history of Petra, but the ""petra"" referred to as a natural fortress and place of refuge cannot be a proper name and the description implies that the town was not yet in existence. Brünnow thinks that ""the rock"" in question was the sacred mountain en-Nejr (above). But Buhl suggests a conspicuous height about 16 miles north of Petra, Shobak, the Mont-royal of the Crusaders.\nMore satisfactory evidence of the date of the earliest Nabataean settlement may be obtained from an examination of the tombs. Two types may be distinguished—the Nabataean and the Greco-Roman. The Nabataean type starts from the simple pylon-tomb with a door set in a tower crowned by a parapet ornament, in imitation of the front of a dwelling-house. Then, after passing through various stages, the full Nabataean type is reached, retaining all the native features and at the same time exhibiting characteristics which are partly Egyptian and partly Greek. Of this type there exist close parallels in the tomb-towers in north Arabia, which bear long Nabataean inscriptions and supply a date for the corresponding monuments at Petra. Then comes a series of tombfronts which terminate in a semicircular arch, a feature derived from north Syria. Finally come the elaborate façades copied from the front of a Roman temple. However, all traces of native style have vanished. The exact dates of the stages in this development cannot be fixed. Strangely, few inscriptions of any length have been found at Petra, perhaps because they have perished with the stucco or cement which was used upon many of the buildings. The simple pylon-tombs which belong to the pre-Hellenic age serve as evidence for the earliest period. It is not known how far back in this stage the Nabataean settlement goes, but it does not go back farther than the 6th century BC.\nA period follows in which the dominant civilization combines Greek, Egyptian and Syrian elements, clearly pointing to the age of the Ptolemies. Towards the close of the 2nd century BC, when the Ptolemaic and Seleucid kingdoms were equally depressed, the Nabataean kingdom came to the front. Under Aretas III Philhellene, (c.85–60 BC), the royal coins begin. The theatre was probably excavated at that time, and Petra must have assumed the aspect of a Hellenistic city. In the reign of Aretas IV Philopatris, (9 BC–AD 40), the fine tombs of the el-I~ejr [?] type may be dated, and perhaps also the great High-place.\nCritiques | Translate\nkhalij_khazar (182) 2008-02-13 15:12\nI\'ve seen countless photos of this place and it never gets old.\nthe framing is outstanding, and i\'m assuming you didn\'t have a tripod. otherwise it probably would have been nice to up the F to increase the sharpness. the facade also seems to be a little over exposed, perhaps playing with the curves might have helped\nmeme (273) 2008-02-13 17:54\nAlthough there were many pictures of that place it impresses me !Well done\njmcl (14535) 2008-02-13 18:41\nReally unique image of this unique place .. I love the sense of emerging into this wonder .. beautiful work with the range of dark to light. I wonder how it would be if you leveled the temple .. would it create a really strange line for the opening?\ngolden (2289) 2010-03-18 12:31\nwonderful shot!It\'s really interesting your POV and framing..I\'ve seen recently a documentary about Petra and how it was built..So interesting!\n- Copyright: Yitzhak Avigur (avigur_11) (22081)\n- Genre: Places\n- Medium: Color\n- Date Taken: 2008-02-09\n- Categories: Ruins\n- Camera: Canon EOS 350D/Rebel XT, 18-55 Canon II EF-S f/3.5-5.5\n- Exposure: f/5.6, 1/30 seconds\n- More Photo Info: view\n- Photo Version: Original Version\n- Date Submitted: 2008-02-13 15:08', ""Continued from page 1\nAs a post and lintel temple, the Parthenon presents no engineering breakthrough in building construction. However its stylistic conventions have become the paradigm of Classical architecture, and its style has influenced architecture for many centuries after it was built.\nThe Parthenon is a large temple, but it is by no means the largest one in Greece. Its aesthetic appeal emanates from the refinement of many established norms of Greek architecture, and from the quality of its sculptural decoration. The Parthenon epitomizes all the ideals of Greek thought during the apogee of the Classical era through artistic means. The idealism of the Greek way of living, the attention to detail, as well as the understanding of a mathematically explained harmony in the natural world, were concepts that in every Athenian’s eyes set them apart from the barbarians. These ideals are represented in the perfect proportions of the building, in its intricate architectural elements, and in the anthropomorphic statues that adorned it.\nSome of these details were found in other Greek temples while some were unique to the Parthenon. The temple owes its refined appeal to the subtle details that were built into the architectural elements to accommodate practical needs or to enhance the building’s visual appeal.\nThe fact that there are no absolute straight lines on the Parthenon bestows a subtle organic character to an obvious geometric structure. The columns of the peristyle taper on a slight arc as they reach the top of the building giving the impression that they are swollen from entasis (tension) - as if they were burdened by the weight of the roof; a subtle feature that allots anthropomorphic metaphors to other wise inanimate objects.\nThe peristyle columns are over ten meters tall, and incline slightly towards the center of the building at the top (about 7 cm), while the platform upon which they rest bows on a gentle arc which brings the corners about 12 cm closer to the ground that the middle.\nThe architects of the Parthenon appear to be excellent scholars of visual illusion, an attribute undoubtedly sharpened by years of architectural refinement and observation of the natural world. They designed the columns that appear at the corners of the temple to be 1/40th (about 6 cm) larger in diameter than all the other columns, while they made the space around them smaller than the rest of the columns by about 25 cm. The reason for this slight adaptation of the corner columns is due to the fact that they are set against the bright sky, which would make them appear a little thinner and a little further apart than the columns set against the darker background of the building wall. The increase in size and decrease of space thus compensates for the illusion that the bright background would normally cause.\nThese subtle features set the Parthenon apart from all other Greek temples because the overall effect is a departure from the static Doric structures of the past, towards a more dynamic form of architectural expression. Moreover, the intricate refinements of the forms required unprecedented precision that would be challenging to achieve even in our time. But it was not mere grandeur through subtlety that the Athenians desired. It is evident that they sought to out-shine all other temples of the time through the lavish sculptural decoration of the Parthenon, and its imposing dimensions. The doors that lead to the cella were abundantly decorated with relief sculptures of gorgons, lion heads and other bronze relief ornaments.\nThe Athenian citizens were proud of their cultural identity, and conscious of the historical magnitude of their ideas. They believed that they were civilized among barbarians, and that their cultural and political achievements were bound to alter the history of all civilized people. The catalyst for all their accomplishments was the development of a system of governance the likes of which the world had never seen: Democracy.\nDemocracy, arguably the epitome of the Athenian way of thinking, was at center stage while the Parthenon was built. This was a direct democracy where every citizen had a voice in the common issues through the Assembly that met on the Pnyx hill next to the Acropolis forty times per year to decide on all matters of policy, domestic or foreign.\nThe fact that common people are depicted as individuals for the first time at the Parthenon frieze was owed to the fact that for the first time in history every citizen of a city was recognized as a significant entity and a considerable moving force in the polis and the observable universe.\n- Year Built: 447-432 BCE\n- Precise Dimensions:\n- Width East: 30.875 m\n- Width West: 30.8835 m\n- Length North: 69.5151 m\n- Length South: 69.5115 m\n- Width to Ratio: 9:4\n- Width to height Ratio (without the Pediments): 9:4\n- Number of stones used to built the Parthenon: Approximated at 13400 stones.\n- Architects: Iktinos and Kallikrates\n- Parthenon Cost: 469 talents\n- Coordinates (of Plaka area just below the Acropolis): 37° 58'N, 23° 43'E""]"	['<urn:uuid:0203ef1d-7d91-4b8e-ace9-9a581e4d7e1b>', '<urn:uuid:a30cc534-bfb4-40d9-8f76-2d126973f0d1>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T05:06:54.527160	22	80	1809
32	what muscles work during single arm cable pull exercises	Single arm cable exercises work multiple muscle groups. The single arm high rotation works the abdominal muscles, shoulders and chest muscles. The single arm low row targets the abdominal muscles while rotating the body.	['A cable tower exercise machine has weights and cables that can be adjusted to different heights. Women can strengthen their abdominal muscles, including the obliques and lats, because of the twisting and pulling motions of cable rotations. The arms, chest and back muscles also get a good workout from this machine. Start with light weights and focus on proper form to prevent injury and get the most benefit from the workout.\nSet up the cable machine at the highest setting and attach a rope to the cable pulley. Grasp the rope with both hands and stand about one arm-length from the machine. Turn your right side toward the machine and place your feet apart, a little wider than your hips. Turn your torso toward the machine and extend your arms with elbows slightly bent over your right shoulder toward the machine. Squeeze your abdominal muscles to brace for the rotation. Straighten your arms and pull the rope down and across the front of your body as you rotate your torso toward the left. Pull the rope until your hands are down and pointing at the floor on your left side. Hold the position for two to five seconds and then slowly return to the start position. Do 10 repetitions and then switch sides.\nSingle Arm High Rotation\nThe single arm high rotation works the abdominal muscles, the shoulders and the chest muscles. Attach a single hand grip to a cable machine. Set the machine at the highest setting. Stand with your left side next to the machine. Place your feet hip-width apart on the floor. Grasp the handle of the cable rope with your left hand so that your palm is facing forward. Step forward with your left foot. Place your right hand on your right hip for balance and then squeeze your abdominal muscles. Pull the cable with your left hand drawing it straight across your chest toward your right side. Keep your elbow slightly bent as you pull the cable. Do 10 repetitions with each arm.\nSingle Arm Low Row\nAdjust the cable machine to the lowest level. Attach a single-hand handle to the cable. Stand to the right side of the machine and hold the cable handle with your right hand. Turn your body to the left and bend your knees slightly as your body pivots toward the left. The right arm should extend toward the cable machine across your torso. Squeeze your abdominal muscles and pull the cable across your body toward your right hip. Rotate your body until you are standing straight up with the cable pulled across your torso. Do 10 repetitions and then repeat on the other side.\nThe Judo flip cable rotation exercise targets the upper abdominal muscles and the obliques. Connect a rope to the cable machine and set the machine at the lowest level. Stand with your left side toward the machine and spread your legs into a wide stance. Grasp the rope with both hands and pull it over your right shoulder. Hold the rope with both hands at shoulder level. Twist your body toward the left, away from the pulley and down toward the floor as if you are trying to Judo flip the machine over your right shoulder. Return to the start position and do 10 repetitions. Then switch sides.\n- Full-body Cable Workout\n- What Machines Do Women Use for Arms to Lose Weight at the Gym?\n- Easy Triceps & Chest Machine Workouts\n- The Average Salary of Cable TV Techs\n- Effective Cable Cross Exercises for Abs\n- Subscapularis Exercises\n- Exercises for Core Strength & Trunk Rotation\n- Rope Pulldown Exercises to Build the Serratus Muscles']	['<urn:uuid:e89c5a7d-1c8b-4c3a-913c-05d840ab15a1>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T05:06:54.527160	9	34	613
33	What makes ISO 22000 different from regular HACCP systems?	ISO 22000 differs from regular HACCP systems by providing more flexibility while requiring more discipline. It lets companies develop customized food safety programs that fit their specific needs, rather than following prescriptive audit requirements. The standard mandates organizations to establish, implement, and verify effective prerequisite programs, while also requiring specific elements like Food Safety Teams, process flow diagrams, and comprehensive hazard analysis.	['ISO 22000 Certification in Manila (FSMS) – Requirements\nNew users are recommended to refer to the Food Safety Management System Standard to learn the specific requirements of ISO 22000 Certification in Manila (FSMS). As a new user guideline, we are addressing all of the most crucial needs of the ISO 22000 standard so that users can better grasp FSMS.\n- Develop the organization’s Food Safety Team and Team Leader as Food Safety Team Leader (FSTL)\n- Develop the Process Flow Diagram alongside Process Descriptions.\n- Construct the Product Descriptions (Raw Material, Ingredients, product-Contact, End Products, etc.)\n- Develop the plant’s layout and surrounding area\n- Identify the Potential Food Safety Hazards and conduct the Hazard Analysis, where the Food Safety Hazards are a substance included in Food -which could be a biological, chemical, or physical substance with a negative effect on health. Analysis of Food Safety Hazards is a crucial requirement for Food Safety; therefore, organizations must identify the Potential or present contaminated Food Safety Hazards from Food, conduct the appropriate analysis to determine the sources of Food Safety Hazards contamination, and take the necessary steps to prevent contamination in Food Products.\n- Identification of Internal and External Issues of the Organization Affecting the FSMS of the Organization, Risk Analysis, and Implementation of Appropriate Mitigation Measures.\n- Mention the requirements and anticipations of the Interested Party (such as customer, regulatory Body, etc., and so on)\n- Construct the Food Safety Policy and Objective and disseminate it inside the organization for awareness and make it accessible to interested parties (where possible). Periodically examine the Food Safety Policy and goal.\n- Define the Roles and Responsibilities of All Organizational Team Members Regarding Food Safety and Ensure at Least One Organizational Member is Responsible as Food Safety Team Leader (FSTL)\n- Ensure that the organization’s infrastructure and work environment are suitable for supporting food safety and capable of preventing food safety contamination.\n- Ensure that the organization’s personnel are sufficiently knowledgeable about Food Safety, Food Safety Hazards, Personal Hygiene, Sanitation, Health concerns, HACCP, CCP, PRP, OPRP, Customer requirements, Potential Emergency Situations, Food Safety legal and regulatory requirements, Product Handling, and Disposal, etc. Additionally, timely training is provided to them to maintain currency.\n- Timely Monitor and promote the prevention and reduction of pollutants (including food safety risks) in products, product processes, and the workplace.\n- Develop the Traceability System – which may include Product name /Number, Date of Manufacture, Batch no / Lot Number, Best before Use, etc. – so that it may be recalled/withdrawn if there are any problems with the product and so that it can assist in taking appropriate action as required.\n- Identify the Potential Food Safety Emergency and its preparedness and periodically test the readiness to ensure that it is serving its intended purpose.\n- Develop the HACCP plan, monitor the CCP and OPRP, and periodically assess and update the HACCP plan.\n- Create a strategy for Handling potentially hazardous goods and Disposal of nonconforming goods.\n- As required, develop a system for the withdrawal/recall of products.\n- List the appropriate legal and regulatory requirements and assure compliance with them.\n- Monitor the organization’s overall Food Safety Performance.\n- Develop the system for Timely Internal Audit of Food Safety Management System Implementation.\n- Develop the Timely Management Review System for the adopted Food Safety Management System\nThese are the summarised requirements of ISO 22000, which will assist the company in developing a deeper understanding of the ISO 22000 ISO 22000:2018 Standard and obtaining ISO 22000 Certification in Manila. In addition to these requirements, the ISO 22000 Certification in Manila standard should be consulted for further specific FSMS criteria.', 'The food safety program known as Hazard Analysis and Critical Control Point (HACCP) is a half-century old. For all of you who have taken a HACCP class, you most likely remember how the instructors talked of it being a collaboration of the National Aeronautics and Space Administration (NASA), the US Army Laboratories at Natick, MA, and Pillsbury.\nPaul Lachance, PhD, a retired nutritionist from Rutgers University, was one of the principals with NASA at the time HACCP was created to ensure that foods used in the space program were safe. He said he wanted to be assured that NASA’s chief medical officer would not be calling him to say that “his astronaut or astronauts were sick and had stomach problems.” It was this fear and a systems evaluation that indicated the traditional quality control method — end product testing — was not adequate to ensure production of safe food. This same concern is why such measures continue to be used today. No one wants to hear that its products have made someone sick.\nHowever, the original HACCP program is not the program used today. Its focus was to control salmonella in space foods. The original system that was created by Pillsbury in 1973 and used as a training tool for Food and Drug Administration people had only three principles:\n• The Identification and assessment of food hazards\n• Documentation of critical control points to control identified hazards\n• Establishment of a system to monitor the critical control points.\nSince its inception, HACCP’s basic goal has never changed. However, it has undergone a gradual evolution, which has helped ensure food processors have a tool to develop strong and robust food safety management systems.\nIn the US, HACCP received its biggest boost in 1985 from the National Academy of Sciences (NAS) report “An Evaluation of the Role of Microbiological Criteria for Foods and Food Ingredients.” This report stated that HACCP “provides a more specific and critical approach to the control of microbiological hazards than that achievable by traditional inspection and quality control procedures.”\nThe report also concluded that testing of finished products was not an effective strategy to protect customers and assure that food was free of pathogens. This is something that many people seem to forget today.\nThis report helped jump-start HACCP systems in the US and around the world. The National Advisory Committee for Microbiological Criteria for Foods (NACMCF) issued the seven principles of HACCP in 1992. This prompted prerequisite programs in a HACCP regulation in 1995, which was the impetus behind the principles including the five preliminary steps that were harmonized in 1997 by NACMCF and Codex Alimentarius (See “HACCP Preliminary Steps and Principles” on Page 106). And this eventually led to the creation of ISO 22000 in 2005.\nMany people believe that more regulations are the answer to everything. In 2007, the US Congress enacted legislation that resulted in the Reportable Food Registry, which is now part of doing business for the food industry. Pending legislation also is targeted at further enhancing food safety. The question is, “Will more regulations make our food supply safer?” HACCP is mandatory for the US meat and poultry industry, and the plants are under continuous inspection. Yet that industry continues to have issues and recalls.\nWhile there is talk about mandating HACCP for all food and ingredient processors, the reality is, that is almost the case now. Regulations have not driven food safety and HACCP in food sectors such as the baking and snack industry, but food safety is motivated by a desire to produce safe food and current market requirements.\nThe basic tenet in today’s market is rather simple: If you wish to compete and sell foods and ingredients, your company must have a HACCP plan. The program is mandated by almost every buyer throughout the country. In other words, get with the program or lose your customers.\nThe bottom line is that economic incentives are a more powerful and effective tool for change than mandating food safety programs through the regulatory process.\nMost products manufactured in the baking and snack industry are considered quite safe. Fried snacks, cookies, cakes, pies and breads are not the foods one thinks about when hazardous products are discussed.\nHowever, the industry has encountered problems through the years. An unexpected outbreak occurred in Germany in 1993. More than 1,000 persons developed salmonellosis from consuming potato chips. Potato chips are fried in hot oil at temperatures in excess of 300°F. How could these be a source of a foodborne outbreak? It turned out that the chips were seasoned with paprika found to be contaminated with the pathogen. The spice came from South America and was distributed by a manufacturer in Germany as a seasoning for fried snacks.\nIn 2007, Veggie Booty snacks were recalled due to salmonella contamination attributed to the spice blend. Spices are a ready-to-eat product in most cases, so buyers should purchase products not only from reputable suppliers but should also consider mandating that they be processed in some way to reduce microbial risk.\nThe US also experienced outbreaks that were traced to a breakfast cereal. Malt-O-Meal conducted a recall in 1998 when epidemiological evaluations determined that its cereals were causing salmonellosis. Again, how does a toasted breakfast cereal end up contaminated? Like the potato chip issue, post-process contamination was the source.\nWhat these outbreaks say is that anything can happen. This is why HACCP is a system that includes prerequisite programs, emphasizes issues such as vendor quality and stresses the importance of properly assessing potential hazards. Failing to maintain these programs in actual processing operations is what can lead to a failure of the system and perhaps foodborne illness.\nPeople have always assumed that bakery and snack foods are safe. Most of the processes to make these products have been used for many years. Given the events of the past few years, processors are seriously re-evaluating or revalidating their cooking processes, whether they are oven baked, dry roasted or oil roasted/fried. Are these heating processes adequate to kill potential pathogens?\nOne of the basic principles of thermobacteriology is that the heat resistance of microorganisms increases as the water activity of a product decreases. In addition, materials such as fats and oils can protect microorganisms and further increase their heat resistance. This is why it would be almost impossible to impart enough heat to peanut butter to kill a pathogen like salmonella. If it is not destroyed on the raw peanut, it will not be inactivated in the butter.\nThe cooking process for baked and fried foods is essentially a dehydration operation. As the product is heated and cooks with accompanying drying, the water activity drops, which could adversely affect the lethality imparted to the process. In addition, validation of thermal processes used as kill step or critical control point (CCP) is an integral part of a company’s HACCP plan.\nValidation of cooking processes could be a challenge for the baking and snack industry. Other challenges also include validating cleaning processes and prerequisite programs deemed essential for food safety.\nSeveral issues must be addressed in the area of process validation: People who are process authorities, a need for surrogate microorganisms and development of methods to complete these studies. The low-acid canned food regulations found in 21 CFR Part 113 mention “competent process authorities” but really do not define what skills and knowledge such a person should have. The Almond Board of California (ABC) initiated a program to approve process authorities as part of its program to ensure that all almonds sold are given a process sufficient to ensure microbiological safety. ABC had to write its own definition of a process authority so that it would have guidelines to evaluate prospective applicants.\nSurrogates, the nonpathogenic microorganisms whose characteristics mimic that of target organisms, were also mentioned. In the canning industry, nontoxic Clostridial or bacillus sp. are used in lieu of Clostridium botulinum. The reason surrogates are so important is that one never wants to bring a known pathogen into a food processing environment. As an example, the surrogate organism for Salmonella enteriditis specified by the ABC, Enterococcus faecium NRRL B-2354, for almond roaster validation is also being used for peanut and other tree nut validations. The questions being, Is this the best organism to use as a surrogate for Salmonella for validating all processes for low-moisture products?\nISO 22000 EMERGES\nThird-party audits are a fact of life in today’s food industry. Processors must meet audit requirements mandated by their customers. HACCP is an integral part of all audits, including SQF (Safe Quality Foods), BRC (British Retail Consortium), IFS (International Food Standard) and Dutch HACCP. These audit formats have been approved by the Global Food Safety Initiative (GFSI) and FSCC 22005 (ISO 22000).\nUnfortunately, third-party audits are not standardized. In fact, some processors have separate manuals for different audit firms. More companies, however, are moving toward ISO 22000. This is an auditable food safety standard that was issued on Sept. 1, 2005, and it was reviewed this past September by the technical committee that developed the standard. Processors, especially multinationals, like the discipline of the standard and have been developing their food safety programs using this document. What many people do not realize is that one can use a standard yet not spend the time and money required for a certification. Among the firms that are moving toward adoption of ISO 22000 are Cargill and Nestle.\nISO 22000 defines what is expected to ensure the production of safe food. It is up to the company that implements the system to properly define the prerequisite programs and the HACCP plan.\nISO 22000 mandates that organizations “establish, implement and maintain prerequisite programs (PRPs) to minimize the potential of introducing hazards into the processing environment, control potential food hazards and reduce the potential of biological, chemical and physical hazards being introduced into the food or beverage. The processor must verify that the PRPs are effective as part of the food safety management system and modify them as needed.” This kind of discipline is implied but not mandated in many HACCP programs. It gives processors the flexibility to develop programs that directly meet their needs, rather than implement programs to meet the requirements of a more proscriptive audit like SQF. The key word is effective. ISO 22000 mandates that the systems that are developed, implemented and maintained be effective.\nThe ISO standard provides reference material to assist a company in the proper selection, implantation and documentation of the prerequisite programs. This strategy was intentional. In addition, ISO and the International Trade Centre published an additional reference that further elaborates on the standard. It allows the development of a food safety management system that is customized to the culture and operations of the food processing company. ?']	['<urn:uuid:ca8d414b-4156-4c5a-a011-9487034032aa>', '<urn:uuid:801c1499-bfe5-48b2-9e28-39ba5f716253>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T05:06:54.527160	9	62	2407
34	As an audiology student, how exactly does the middle ear match sound impedance?	The middle ear acts as an impedance matching network by converting sound from air to liquid through the difference in area between the ear drum and oval window. The ear drum has an area of about 60 square millimeters, while the oval window has about 4 square millimeters. This difference increases the sound wave pressure by approximately 15 times, allowing more sound energy to enter the liquid of the inner ear rather than being reflected at the air/liquid interface.	"[""The human ear is an exceedingly complex organ. To make matters even more difficult, the information from two ears is combined in a perplexing neural network, the human brain. Keep in mind that the following is only a brief overview; there are many subtle effects and poorly understood phenomena related to human hearing.\nFigure 22-1 illustrates the major structures and processes that comprise the human ear. The outer ear is composed of two parts, the visible flap of skin and cartilage attached to the side of the head, and the ear canal, a tube about 0.5 cm in diameter extending about 3 cm into the head. These structures direct environmental sounds to the sensitive middle and inner ear organs located safely inside of the skull bones. Stretched across the end of the ear canal is a thin sheet of tissue called the tympanic membrane or ear drum. Sound waves striking the tympanic membrane cause it to vibrate. The middle ear is a set of small bones that transfer this vibration to the cochlea (inner ear) where it is converted to neural impulses. The cochlea is a liquid filled tube roughly 2 mm in diameter and 3 cm in length. Although shown straight in Fig. 22-1, the cochlea is curled up and looks like a small snail shell. In fact, cochlea is derived from the Greek word for snail.\nWhen a sound wave tries to pass from air into liquid, only a small fraction of the sound is transmitted through the interface, while the remainder of the energy is reflected. This is because air has a low mechanical impedance (low acoustic pressure and high particle velocity resulting from low density and high compressibility), while liquid has a high mechanical impedance. In less technical terms, it requires more effort to wave your hand in water than it does to wave it in air. This difference in mechanical impedance results in most of the sound being reflected at an air/liquid interface.\nThe middle ear is an impedance matching network that increases the fraction of sound energy entering the liquid of the inner ear. For example, fish do not have an ear drum or middle ear, because they have no need to hear in air. Most of the impedance conversion results from the difference in area between the ear drum (receiving sound from the air) and the oval window (transmitting sound into the liquid, see Fig. 22-1). The ear drum has an area of about 60 (mm)2, while the oval window has an area of roughly 4 (mm)2. Since pressure is equal to force divided by area, this difference in area increases the sound wave pressure by about 15 times.\nContained within the cochlea is the basilar membrane, the supporting structure for about 12,000 sensory cells forming the cochlear nerve. The basilar membrane is stiffest near the oval window, and becomes more flexible toward the opposite end, allowing it to act as a frequency spectrum analyzer. When exposed to a high frequency signal, the basilar membrane resonates where it is stiff, resulting in the excitation of nerve cells close to the oval window. Likewise, low frequency sounds excite nerve cells at the far end of the basilar membrane. This makes specific fibers in the cochlear nerve respond to specific frequencies. This organization is called the place principle, and is preserved throughout the auditory pathway into the brain.\nAnother information encoding scheme is also used in human hearing, called the volley principle. Nerve cells transmit information by generating brief electrical pulses called action potentials. A nerve cell on the basilar membrane can encode audio information by producing an action potential in response to each cycle of the vibration. For example, a 200 hertz sound wave can be represented by a neuron producing 200 action potentials per second. However, this only works at frequencies below about 500 hertz, the maximum rate that neurons can produce action potentials. The human ear overcomes this problem by allowing several nerve cells to take turns performing this single task. For example, a 3000 hertz tone might be represented by ten nerve cells alternately firing at 300 times per second. This extends the range of the volley principle to about 4 kHz, above which the place principle is exclusively used.\nTable 22-1 shows the relationship between sound intensity and perceived loudness. It is common to express sound intensity on a logarithmic scale, called decibel SPL (Sound Power Level). On this scale, 0 dB SPL is a sound wave power of 10-16 watts/cm2, about the weakest sound detectable by the human ear. Normal speech is at about 60 dB SPL, while painful damage to the ear occurs at about 140 dB SPL.\nThe difference between the loudest and faintest sounds that humans can hear is about 120 dB, a range of one-million in amplitude. Listeners can detect a change in loudness when the signal is altered by about 1 dB (a 12% change in amplitude). In other words, there are only about 120 levels of loudness that can be perceived from the faintest whisper to the loudest thunder. The sensitivity of the ear is amazing; when listening to very weak sounds, the ear drum vibrates less than the diameter of a single molecule!\nThe perception of loudness relates roughly to the sound power to an exponent of 1/3. For example, if you increase the sound power by a factor of ten, listeners will report that the loudness has increased by a factor of about two (101/3 ≈ 2). This is a major problem for eliminating undesirable environmental sounds, for instance, the beefed-up stereo in the next door apartment. Suppose you diligently cover 99% of your wall with a perfect soundproof material, missing only 1% of the surface area due to doors, corners, vents, etc. Even though the sound power has been reduced to only 1% of its former value, the perceived loudness has only dropped to about 0.011/3 ≈ 0.2, or 20%.\nThe range of human hearing is generally considered to be 20 Hz to 20 kHz, but it is far more sensitive to sounds between 1 kHz and 4 kHz. For example, listeners can detect sounds as low as 0 dB SPL at 3 kHz, but require 40 dB SPL at 100 hertz (an amplitude increase of 100). Listeners can tell that two tones are different if their frequencies differ by more than about 0.3% at 3 kHz. This increases to 3% at 100 hertz. For comparison, adjacent keys on a piano differ by about 6% in frequency.\nThe primary advantage of having two ears is the ability to identify the direction of the sound. Human listeners can detect the difference between two sound sources that are placed as little as three degrees apart, about the width of a person at 10 meters. This directional information is obtained in two separate ways. First, frequencies above about 1 kHz are strongly shadowed by the head. In other words, the ear nearest the sound receives a stronger signal than the ear on the opposite side of the head. The second clue to directionality is that the ear on the far side of the head hears the sound slightly later than the near ear, due to its greater distance from the source. Based on a typical head size (about 22 cm) and the speed of sound (about 340 meters per second), an angular discrimination of three degrees requires a timing precision of about 30 microseconds. Since this timing requires the volley principle, this clue to directionality is predominately used for sounds less than about 1 kHz.\nBoth these sources of directional information are greatly aided by the ability to turn the head and observe the change in the signals. An interesting sensation occurs when a listener is presented with exactly the same sounds to both ears, such as listening to monaural sound through headphones. The brain concludes that the sound is coming from the center of the listener's head!\nWhile human hearing can determine the direction a sound is from, it does poorly in identifying the distance to the sound source. This is because there are few clues available in a sound wave that can provide this information. Human hearing weakly perceives that high frequency sounds are nearby, while low frequency sounds are distant. This is because sound waves dissipate their higher frequencies as they propagate long distances. Echo content is another weak clue to distance, providing a perception of the room size. For example, sounds in a large auditorium will contain echoes at about 100 millisecond intervals, while 10 milliseconds is typical for a small office. Some species have solved this ranging problem by using active sonar. For example, bats and dolphins produce clicks and squeaks that reflect from nearby objects. By measuring the interval between transmission and echo, these animals can locate objects with about 1 cm resolution. Experiments have shown that some humans, particularly the blind, can also use active echo localization to a small extent.""]"	['<urn:uuid:74397cf9-2813-42b1-b3a8-2727800c93fc>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:06:54.527160	13	79	1495
35	I've heard some jet fighters use something called afterburners for extra power. How does the fuel usage compare between normal engine operation and when afterburners are on?	When afterburners are engaged, fuel consumption increases dramatically - up to three times higher than during normal engine operation. For example, in the F-16 fighter jet, fuel consumption with afterburners can exceed 64,000 pounds per hour at low altitudes. This is why pilots use afterburners very sparingly, typically for just a few minutes during a mission. The afterburner works by injecting and burning additional fuel between the engine turbine and jet pipe nozzle, using unburned oxygen in the exhaust gas. While this provides a significant thrust boost, it is very inefficient in terms of fuel usage compared to normal engine operation.	"[""Jet Engine Operation\nmain function of any aeroplane propulsion system is to provide a force to\novercome the aircraft drag, this force is called thrust. Both propeller\ndriven aircraft and jet engines derive their thrust from accelerating a\nstream of air - the main difference between the two is the amount of air\naccelerated. A propeller accelerates a large volume of air by a small\namount, whereas a jet engine accelerates a small volume of air by a large\namount. This can be understood by Newton's 2nd law of motion which is\nsummarized by the equation F=ma (force = mass x acceleration).\nBasically the force or thrust (F) is created by accelerating the mass of\nair (m) by the acceleration (a).\nA propeller accelerates a large volume of air by a small amount\nA jet engine accelerates a small volume of air by a large amount\nthat thrust is proportional to airflow rate and that engines must be\ndesigned to give large thrust per unit engine size, it follows that the\njet engine designer will generally attempt to maximize the airflow per\nunit size of the engine. This means maximizing the speed at which the air\ncan enter the engine, and the fraction of the inlet area that can be\ndevoted to airflow. Gas turbine engines are generally far superior to\npiston engines in these respects, therefore piston-type jet engines have\nnot been developed.\nThe operation cycle of a gas\ngas turbine engine is essentially a heat engine using air as a working\nfluid to provide thrust. To achieve this, the air passing through the\nengine has to be accelerated; this means that the velocity or kinetic\nenergy of the air must be increased. First, the pressure energy is raised,\nfollowed by the addition of heat energy, before final conversion back to\nkinetic energy in the form of a high\nThe basic mechanical arrangement of a gas turbine is relatively simple. It\nconsists of only four parts:\nThe compressor which is used to\nincrease the pressure (and temperature) of the inlet air.\n2. One or a number of\ncombustion chambers in which fuel is injected into the high-pressure air\nas a fine spray, and burned, thereby heating the air. The pressure remains\n(nearly) constant during combustion, but as the temperature rises, each\nkilogram of hot air needs to occupy a larger volume than it did when cold\nand therefore expands through the turbine.\nThe turbine which converts some of this temperature rise to rotational\nenergy. This energy is used to drive the compressor.\nThe exhaust nozzle which accelerates the air\nusing the remainder of the energy added in the combustor, producing a high\nvelocity jet exhaust.\nA schematic of a gas-turbine\ngeneralization, however, does not extend to the detailed design of the\nengine components, where account has to be taken of the high operating\ntemperatures of the combustion chambers and turbine; the effects of\nvarying flows across the compressor and turbine blades; and the design of\nthe exhaust system through which the gases are ejected to form the\nIn the gas turbine\nengine, compression of the air is effected by one of two basic types of\ncompressor, one giving centrifugal flow and the other axial flow. Both\ntypes are driven by the engine turbine and are usually coupled direct to\nthe turbine shaft.\nA centrifugal impeller\nThe centrifugal flow compressor employs an\nto accelerate the air and a diffuser to produce the required pressure\nrise. Flow exit's a centrifugal compressor radially (at 90° to the flight\ndirection) and it must therefore be redirected back towards the combustion\nchamber, resulting in a drop in efficiency. The axial flow compressor\nemploys alternate rows of rotating (rotor) blades, to accelerate the air,\nand stationary (stator) vanes ,to diffuse the air, until the required\npressure rise is obtained.\nThe pressure rise that\nmay be obtained in a single stage of an axial compressor is far less than\nthe pressure rise achievable in a single centrifugal stage. This means\nthat for the same pressure rise, an axial compressor needs many stages,\nbut a centrifugal compressor may need only one or two.\nAn axial flow compressor\n(stators omitted for clarity). This is the high pressure compressor from a\nGeneral Electric F404 engine\ncombustion chamber has the difficult task of burning large quantities of\nfuel, supplied through fuel spray nozzles, with extensive volumes of air,\nsupplied by the compressor, and releasing the resulting heat in such a\nmanner that the air is expanded and accelerated to give a smooth stream of\nuniformly heated gas. This task must be accomplished with the minimum loss\nin pressure and with the maximum heat release within the limited space\nThe amount of fuel added to the air will depend upon the temperature rise\nrequired. However, the maximum temperature is limited to within the range\nof 850 to 1700 °C by the materials from which the turbine blades and\nnozzles are made. The air has already been heated to between 200 and\n550 °C by the work done in the compressor, giving a temperature rise\nrequirement of 650 to 1150 °C from the combustion process. Since the gas\ntemperature determines the engine thrust, the combustion chamber must be\ncapable of maintaining stable and efficient combustion over a wide range\nof engine operating conditions.\nThe temperature of the gas after combustion is about 1800 to 2000 °C,\nwhich is far too hot for entry to the nozzle guide vanes of the turbine.\nThe air not used for combustion, which amounts to about 60 percent of the\ntotal airflow, is therefore introduced progressively into the flame tube.\nApproximately one third of this gas is used to lower the temperature\ninside the combustor; the remainder is used for cooling the walls of the\nare three main types of combustion chamber in use for gas turbine engines.\nThese are the the multiple chamber, the can-annular chamber and the\nThis type of combustion chamber is used on\ncentrifugal compressor engines and the earlier types of axial flow\ncompressor engines. It is a direct development of the early type of Whittle engine\ncombustion chamber. Chambers are disposed radially around the engine and\ncompressor delivery air is directed by ducts into the individual chambers.\nEach chamber has an inner flame tube around which there is an air casing.\nThe separate flame tubes are all interconnected. This allows each tube to\noperate at the same pressure and also allows combustion to propagate\naround the flame tubes during engine starting.\nThis type of combustion\nchamber bridges the evolutionary gap between multiple and annular types. A\nnumber of flame tubes are fitted inside a common air casing. The airflow\nis similar to that already described. This arrangement combines the ease\nof overhaul and testing of the multiple system with the compactness of the\nThis type of combustion\nchamber consists of a single flame tube, completely annular in form, which\nis contained in an inner and outer casing. The main advantage of the\nannular combustion chamber is that for the same power output, the length\nof the chamber is only 75 per cent of that of a can-annular system of the\nsame diameter, resulting in a considerable saving in weight and cost.\nAnother advantage is the elimination of combustion propagation problems\nfrom chamber to chamber.\nThe turbine has the task of providing power to drive the compressor and\naccessories. It does this by extracting energy from the hot gases released\nfrom the combustion system and expanding them to a lower pressure and\ntemperature. The continuous flow of gas to which the turbine is exposed\nmay enter the turbine at a temperature between 850 and 1700 °C which is\nfar above the melting point of current materials technology.\nA high-pressure turbine stage from a\nCFM56 turbofan engine\nTo produce the driving torque, the turbine may consist of several stages,\neach employing one row of stationary guide vanes, and one row of moving\nblades. The number of stages depends on the relationship between the power\nrequired from the gas flow, the rotational speed at which it must be\nproduced, and the diameter of turbine permitted. The design of the nozzle\nguide vanes and turbine blade passages is broadly based on aerodynamic\nconsiderations, and to obtain optimum efficiency, compatible with\ncompressor and combustor design, the nozzle guide vanes and turbine blades\nare of a basic aerofoil shape.\nA turbine blade with cooling holes\ndesire to produce a high engine efficiency demands a high turbine inlet\ntemperature, but this causes problems as the turbine blades would be\nrequired to perform and survive long operating periods at temperatures\nabove their melting point. These blades, while glowing red-hot, must be\nstrong enough to carry the centrifugal loads due to rotation at high\nTo operate under these conditions, cool air is forced out of many small\nholes in the blade. This air remains close to the blade, preventing it\nfrom melting, but not detracting significantly from the engine's overall\nperformance. Nickel alloys are used to construct the turbine blades and\nthe nozzle guide vanes because these materials demonstrate good properties\nat high temperatures.\nGas turbine engines\nfor aircraft have an exhaust system which passes the turbine discharge\ngases to atmosphere at a velocity in the required direction, to provide\nthe necessary thrust. The design of the exhaust system, therefore, exerts\na considerable influence on the performance of the engine. The cross\nsectional areas of the jet pipe and propelling or outlet nozzle affect\nturbine entry temperature, the mass flow rate, and the velocity and\npressure of the exhaust jet.\nA basic exhaust system function is to form the correct outlet area and to\nprevent heat conduction to the rest of the aircraft. The use of a thrust\nreverser (to help slow the aircraft on landing), a noise suppresser (to\nquieten the noisy exhaust jet) or a variable area outlet (to improve the\nefficiency of the engine over a wider range of operating conditions)\nproduces a more complex exhaust system.\nA basic exhaust system\nA more complex exhaust system with two\nand noise suppresser\naddition to the basic components of a gas turbine engine, one other\nprocess is occasionally employed to increase the thrust of a given engine.\nAfterburning (or reheat) is a method of augmenting the basic thrust of an\nengine to improve the aircraft takeoff, climb and (for military aircraft)\nAfterburning consists of the introduction and burning of raw fuel between\nthe engine turbine and the jet pipe propelling nozzle, utilizing the\nunburned oxygen in the exhaust gas to support combustion. The resultant\nincrease in the temperature of the exhaust gas increases the velocity of\nthe jet leaving the propelling nozzle and therefore increases the engine\nthrust. This increased thrust could be obtained by the use of a larger\nengine, but this would increase the weight, frontal area and overall fuel\nconsumption. Afterburning provides the best method of thrust augmentation\nfor short periods.\nAfterburners are very inefficient as they require a disproportionate\nincrease in fuel consumption for the extra thrust they produce.\nAfterburning is used in cases where fuel efficiency is not critical, such\nas when aircraft take off from short runways, and in combat, where a rapid\nincrease in speed may occasionally be required.\nTypical afterburning jet pipe equipment"", 'An afterburner, a distinctive feature of certain engines, is renowned for it’s remarkable inefficiency, as it voraciously devours fuel at a staggering rate of up to three times more than conventional engines. Consequently, pilots conscientiously restrict it’s usage during missions, usually reserving it for mere moments. Although the mechanism behind an afterburner may appear uncomplicated, it’s functionality relies on an intricate balance of delicately calibrated tolerances, demanding meticulous precision and skill.\nHow Long Can Fighter Jets Use Afterburners?\nFighter jets equipped with afterburners have the ability to tap into an immense power source, but it comes at a cost. The afterburner, a key feature of these engines, is notorious for it’s inefficiency. When engaged, it consumes fuel at an accelerated rate, up to three times higher than regular engine operation. As a result, pilots are careful to employ this feature sparingly, usually restricting it’s use to a matter of minutes during a mission.\nThe concept underlying an afterburner may be relatively straightforward, but it’s operation is anything but. The afterburner system introduces additional fuel into the exhaust stream, which then ignites, resulting in a sudden surge of power. This process demands intricate engineering and meticulous maintenance to ensure it’s reliability and accuracy. Even slight deviations in the afterburners delicate tolerances can cause a cascade of issues, potentially compromising the engines overall performance.\nFurthermore, prolonging afterburner usage can impose excessive strain on various components, such as turbine blades and exhaust nozzles. These parts are designed to withstand the intense forces generated under normal operating conditions. However, when subjected to prolonged afterburner use, they may succumb to the increased heat and stresses, leading to potentially catastrophic failures.\nThey reserve it’s deployment for critical moments or situations, where maximum thrust is necessary to achieve specific objectives. By doing so, they strike a balance between harnessing the incredible power of the afterburner and conserving precious fuel.\nMoving on to the next topic, it’s fascinating to explore the endurance of the F-16 in full afterburner. At low altitudes, the F-16 is capable of burning an astonishing amount of fuel, exceeding 64,000 pounds per hour. When operating at maximum capacity with external fuel stores, the U.S.-variant F-16 can sustain this level of performance for approximately 20 minutes before being forced to rely on emergency reserves, which would only provide an extra minute or so of full afterburner capability.\nHow Long Can F-16 Use Afterburner?\nThe F-16, renowned for it’s exceptional performance and versatility, is no stranger to the use of afterburners. These powerful engines enable the aircraft to achieve impressive speeds and climb rates, making it a force to be reckoned with in the skies. However, the duration for which an F-16 can utilize afterburners isn’t indefinite.\nWith maximum external fuel stores, a U.S.-variant F-16 can sustain full throttle operations for roughly 20 minutes. Subsequently, the aircraft would rely solely on emergency reserves, which would provide an additional minute or so under such intense conditions. This limited timeline highlights the need for prudent fuel management during missions requiring extensive afterburner usage.\nIt’s worth noting that the duration of afterburner usage can also be impacted by altitude. Conversely, at low altitudes, where the atmosphere is denser, the afterburners consume fuel at a faster rate, limiting the duration they can be utilized.\nStrategies for Fuel Management During Missions Requiring Extensive Afterburner Usage\n- Optimize fuel consumption\n- Monitor fuel levels constantly\n- Use afterburner sparingly\n- Implement precise throttle control\n- Consider pre-flight weight reduction\n- Incorporate efficient flight planning\n- Perform regular engine maintenance\n- Utilize air-to-air refueling if available\n- Employ tactics to minimize time spent in afterburner\n- Train pilots on fuel-efficient flying techniques\nWatch this video on YouTube:\nThe Boeing F/A-18E and F/A-18F Super Hornet, derived from the McDonnell Douglas F/A-18 Hornet series, are renowned carrier-capable, multirole fighter aircraft. The F/A-18E variant, designed as a single-seater, and the F/A-18F variant, built as a tandem-seater, are upgraded versions of their predecessors. These impressive aircraft can internally store up to 14,400 lbs of fuel. In terms of fuel consumption, the powerful GE F414 engine can burn around 36,000 lbs/hr of fuel in full afterburner mode. Therefore, it’s typical for an F/A-18E/F Super Hornet to have a total fuel burn of approximately 72,000 lbs/hr in such conditions.\nHow Much Fuel Does a F18 Burn Afterburner?\nThe F/A-18E Super Hornet, equipped with the GE F414 engine, is a formidable aircraft in terms of fuel consumption. It possesses a substantial fuel capacity, with 14,400 lbs of fuel stored internally. However, when engaging the afterburner, the fuel consumption reaches another level entirely.\nThe afterburner is a significant feature of the F/A-18E, as it provides an immense boost of propulsion. With the afterburner activated, the GE F414 engine burns approximately 36,000 lbs/hr of fuel. This extensive fuel burn showcases the immense power output of the afterburner and it’s impact on the aircrafts overall fuel consumption.\nHowever, it’s important to note that fuel consumption may vary depending on factors such as mission requirements, aircraft configurations, and flight parameters.\nWhen the afterburner is engaged, the engine burns approximately 36,000 lbs/hr of fuel, resulting in a typical total fuel burn of 72,000 lb/hr under maximum power output conditions. However, it’s crucial to consider various factors that can affect fuel consumption when assessing the overall efficiency of this advanced multirole fighter aircraft.\nIn conclusion, the utilization of an afterburner in an engine comes at a significant cost in terms of fuel consumption. Consequently, pilots are cautious and restrict it’s use to only a few minutes during a mission to mitigate the fuel consumption impact. Balancing the benefits of enhanced thrust with the inherent cost of increased fuel consumption remains a crucial consideration in the design and operation of afterburning engines.']"	['<urn:uuid:799d245a-5664-4f15-aad8-a890abfd325c>', '<urn:uuid:5e11055f-6713-4483-a2eb-970fbb674bda>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T05:06:54.527160	27	101	2826
36	who replaced bon scott acdc singer	After Bon Scott's tragic death in 1980, AC/DC hired Brian Johnson as their new singer. With Johnson, they went on to record Back in Black, which became one of the top five biggest-selling albums in music history.	['AC/DC FAQ author Susan Masino shared her account of her three decade friendship with the band on The Slacker Morning Show (101 The Fox, Kansas City) listeners. With AC/DC playing in the background, this fast paced interview took listeners along with Susan on a stroll down memory lane. Take a listen below.\nAC/DC FAQ captures that danger and that insanity. Rock journalist and author Susan Masino spans AC/DC’s 40-year career, starting from the band’s inception in 1973 and covering everything from their earliest days in Australia to their first tour of England and the United States. It also includes personal experiences, stories, conversations, and interviews by Masino, who has known the band since 1977.\nSusan Masino was one of the first American journalist to interview AC/DC during their first U.S. Tour. She met them before their stardom when they were playing in clubs. She watched their evolution and transformation in rock music as they moved up the ranks eventually opening for KISS and Aerosmith.\nFeaturing 37 chapters, AC/DC FAQ chronicles the personal history of each of the band members, all their albums, tours, and various anecdotes. Rebounding from the tragic loss of their singer Bon Scott in 1980, AC/DC hired Brian Johnson and went on to record Back in Black, which is now one of the top five biggest-selling albums in music history.\n“It was a horrible tragedy for the band. It’s just an amazing rock and roll miracle that they found Bryan Johnson, continued the recording of Back in Black, and as they say the rest is history.\nTaking a seven-year break after their album Stiff Upper Lip, the band came back in the fall of 2008 with a new album, Black Ice, and a tour that ran from 2008 through the summer of 2010. Once again breaking records, AC/DC saw the Black Ice Tour become the second-highest-grossing tour in history.\nSusan Masino has been a rock journalist for more than 30 years and has written six books, including Family Tradition: Three Generations of Hank Williams, published by Backbeat Books, and The Story of AC/DC: Let There Be Rock, which has now been published in 11 languages. She also appears in the Van Halen DVD The Early Years and the movie AC/DC: Let There Be Rock.\nDave Thompson, author of The Rocky Horror Picture Show FAQ,spoke with Dale Johnson of Lincoln Live. They spoke about the writer of the play Richard O’Brien, what drew Dave Thompson to writing the book, and more! Listen to what they had to say in the podcast below!\nWhen assessing the cultural impact of The Rocky Horror Picture Show, author Dave Thompson does not pull his punches: “Forty-plus years on from its debut in a tiny London theater; four decades, too, from its transition to the silver screen, Rocky Horror stands among the 1970s’ most lasting, and successful, contributions to modern culture.”\nThompson’s latest contribution to the Applause Books FAQ series, The Rocky Horror Picture Show FAQ (April 2016, Applause Books, $19.99) is the in-depth story of not only the legendary stage show and movie, but of a unique period in theatrical history, in both the movie’s UK homeland and overseas.\nInside these pages, we see Rocky Horror as sexual cabaret and political subversion, as modern mega-hit and Broadway disaster. At the movie house, we learn when to shout, what to throw, and why people even do those things. Here is the full story of the play’s original creation; its forebears and its influences are laid out in loving detail, together with both the triumphs and tragedies that attended it across the next 40 years.\nPacked with anecdotes, The Rocky Horror Picture Show FAQ is the story of dozens of worldwide performances and the myriad stars who have been featured in them. From Tim Curry to Anthony Head, from Reg Livermore to Gary Glitter, from Daniel Abineri to Tom Hewitt, the lives and careers of the greatest ever Frank N. Furters stalk the pages, joined by the Riff-Raffs, Magentas, Columbias, and all the rest.\nThe book also includes the largest and most in-depth Rocky Horror discography ever published, plus a unique timeline – The Ultimate Rocky Horror Chronology – detailing the who, what, where, and when of absolute pleasure.\nThe Rocky Horror Picture Show FAQ will have you doing the Time Warp again!\nTom DeMichael author of Baseball FAQ, helped host Ed Randall get ready for the upcoming baseball season when he was a guest WFAN’s Talking Baseball. Click on the link below to hear the full interview!\nFor 10 years, the Backbeat Books FAQ Series has been a one-stop source for information, history, and minutiae on the world of music and pop culture. The Beatles and Bruce Springsteen, The Doors and Johnny Cash, Dracula and the Beats any many more all have gone under the FAQ microscope. Now the FAQ Series has turned its focus to America’s Pastime.\nWas Abner Doubleday the architect of baseball? What exactly did it mean to be a “professional” baseball player in the 1870s? What goes on in the front office? How do you throw a slider? Readers will find the answers to these questions – and many others – in the pages Baseball FAQ: All That’s Left to Know About America’s Pastime (March 2016, Backbeat Books, $19.99) by Tom DeMichael.\nPart history book, part instructional guide, and part reference manual, Baseball FAQ covers all the bases – from the rules of the game to the ballparks of yesterday and today, from the minor leagues to the majors, from the stats to the food. This engaging, compulsively readable tome offers baseball fans of all ages a wealth of fun facts and anecdotes on America’s favorite pastime, including sections on the All-American Girls Professional Ball League, the Negro Leagues, the basic skills of baseball, baseball in the movies, the scandals, and the Hall of Famers.\nDeMichael, a member of SABR, the Society for American Baseball Research, also digs to into the sport’s seemingly inexhaustible fascination with numbers. While the 19th-century journalist Henry Chadwick was the father of baseball statistics, it was Bill James who coined the term “Sabermetrics” in 1980 and ushered in the era of modern statistical analysis. DeMichael defines Sabermetrics as “an accurate and balanced method by which we can compare players from different eras,” and Baseball FAQ looks at the latest wave of statistical acronyms, including OPS, WHIP, FIP, and WAR.\nLooking beyond the wins and losses and the runs, hits, and errors, Baseball FAQ is a remarkable baseball reference and fun-filled reading for fans of the game.\nElliott Landy author of The Band Photographs 1968-1969, spoke with Paul Metsa, host of the Wall of Power Radio Hour about how he approached his work, his respect for the Band, and how he became the official photographer of the Woodstock Music Festival. Click on the link below to hear what they had to say!\nOn rare exalted occasions, a photographer gains the trust of a performer or band, and his work fuses with theirs in such a way that the two entities become “married” in the public consciousness. One can think of David Duncan’s pictures of Picasso at work or Alfred Wertheimer’s pictures of Elvis backstage in 1956.\nThe Band Photographs, 1968-1969 (December 2015, Backbeat Books, $44.99), Elliott Landy’s chronicle of the Band from 1968 to 1969, is of such importance. The mutual trust and collaborative partnership was so deep that this collection of photographs forms an intimate portrait of a group of musicians not only engaged in their craft, but captured as they created a new genre of music.\nOriginally crowdfunded by what would become Kickstarter’s highest funded campaign for a photography book, The Band Photographs, 1968-1969 features more than 200 photographs documenting the making of the Band’s first two albums, Music from Big Pink and The Band. More than half of the photos, drawn from Landy’s archive of more than 12,000 images, have never been published before.\n“I designed and created this book entirely in my own studio, with complete creative control,” Landy explained. “Because of this, I was able to lay out the photos as I wanted, in order to create the most harmonious visual book experience and communicate what was going on in front of the camera.\nThe book also features commentary from John Simon, who produced the Band’s first two albums and was considered the Band’s sixth member, and an introduction by Jonathan Taplin, tour manager for the Band from 1969 to 1972. As Taplin writes in his foreword, “In a sense, these pictures are the photographic analogue of The Band’s song, ‘The Night They Drove Old Dixie Down’—harkening back to the formal portraiture of Matthew Brady and other late 19th Century photographers. But these pictures are honest and true. They live in the photographic tradition of Robert Frank’s The Americans. Elliott’s images are a record of a wonderfully creative period in America that won’t come again.”']	['<urn:uuid:11c86953-9d7b-4468-9df9-2a389639a66a>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-13T05:06:54.527160	6	37	1485
37	How do medical lab technologists handle time pressure in emergency situations, and what biosafety protocols must they follow during urgent testing?	Medical lab technologists must work quickly to provide test results to physicians in emergencies to save patients' lives, prioritizing urgent tasks while putting other testing aside. However, even under time pressure, they must strictly adhere to biosafety protocols based on the risk level of the infectious agents they're handling. These protocols include using appropriate protective equipment, following proper containment procedures, and maintaining standard laboratory practices like careful fluid handling and proper disinfection. This ensures both quick results delivery and prevention of exposure to dangerous pathogens that could harm the technologist or the community.	"[""What is a Medical Lab Technician?\nClinical laboratory technologists perform testing that is crucial to detecting and diagnosing diseases. They examine and analyze body fluids, tissues and cells and look for bacteria, parasites and other microorganisms. People in this field use microscopes and high tech equipment that is automated to perform many tests at once. Medical Lab technologists have knowledge in biology, chemistry and medicine. They also use mathematics to perform tests and must accurately document their findings. When there is an emergency, the technician has to work quickly to provide physicians with the results of their tests to save patients lives.\nReal People Profile\n||Vincent M. Carraway\n||Palmetto Health Richland\n||Bachelor of Science in biology with a minor in chemistry\nMasters in medical technology\nMasters in public health\n|Biology and organic chemistry\n|First Real Job:\n||Investigation of disease outbreaks\nFishing, canoeing, building/woodworking, gardening\nReal People Q&A\nAdvice for students interested in my job:\nDo your best in all subjects, especially science and math, but also those related to computers and communication.\nWhat subjects in school will students need to do my job?\nBiology, chemistry, physics, math, computer technology\nWhat I like most about my job?\nWhat I like best is investigating the unknown. Basically what we do is take a specimen that was collected from that patient and it's unknown what is causing that infection. We have to do various testing procedures to determine what's happening, if anything, to cause the patient's infection. It's like playing Sherlock Holmes sometimes because we don't know what we're expected to find. Sometimes the doctor can tell us what he suspects but often times he doesn't. He gives us the specimens and says I need all of the cultures and testing done to it, and we have to narrow it down to what probable cause for the infection, and then we have to do more testing to determine what antibiotics to use to treat that infection. It's interesting being part of the healthcare team because the doctors and nurses need that vital information to properly care for the patient.\nWhat is the biggest challenge in my job?\nWhile some of our testing may take hours, days or even weeks, it's important to remember that some of that information has to be relayed back to the physician very quickly. We have to prioritize what we are doing just like a child may have to prioritize their schoolwork. We have to put some things aside so we can do some testing quickly and get those results back to the patients and get back to the other testing after that. I think that is challenging.\nHow do you get a job like mine?\nStudy and keep up with your schoolwork because you are going to need all of that, math and science and computer skills, when you get to college, you're going to need that. When you are in college you are going to need to study the sciences. Most medical technologists get a bachelor's degree in one of the science fields and then they are going to study medical technology for a year after that. You are going to have to have good grades to get into college and that medical technology program. Good grades are going to be important to get that first job."", 'What are the Biosafety Concerns with Infectious Disease?\nInfectious diseases have long been known for their capacity to devastate populations. They can be so deadly and pervasive that they are sometimes even weaponized – using microorganisms to pollute water sources was a known warfare tactic in the American Civil War, many European wars, and has even been documented as early as 600 BC. However, they can be defeated.\nWe can study them to understand how they work, how to prevent them, and how to treat them with the help of a flow cytometer. Since this research is typically done in a laboratory setting with potentially infectious disease components though, there are a number of biosafety concerns to be addressed including occupational exposure.\nSo, what might we study and what concerns might arise within the laboratory?\nStudying an Infectious Disease in the Lab\nImagine studying Mycobacterium tuberculosis—the bacteria responsible for tuberculosis—in your laboratory practice. Tuberculosis (TB) is highly contagious and can be transferred through airborne particles. Thus, you’d need safety precautions to contain the TB cells in a way that prevents them from mixing with your oxygen supply. You’d also need a backup precaution – a personal air supply – in case that containment is faulty.\nFurthermore, you’d need to wear personal protective equipment that can be removed, in order to avoid contacting the disease or carrying it on your clothing. Finally, there must be adequate security within the facility itself to restrict access to only authorized laboratory employees.\nTuberculosis is a prime example of why biosafety measures in the laboratory work area must be addressed:\n- TB is a highly contagious infectious agent and deadly when left untreated.\n- Should this bacteria make it into the public, this could rapidly turn into an outbreak—the US does not vaccinate people for TB due to the low number of cases in the last 30 years.\nSo, Who is Affected by the Biosafety Risks in this Example?\nThere are two parties with major biosafety concerns here:\n- The researcher\n- The community, if the researcher is infected or carries the bacteria outside the laboratory work area\nPreventative Measures for Biosafety Concerns\nTo operate under conditions of severe biological consequences requires implementing all the necessary laboratory safety precautions. There are biosafety concerns over:\n- Safety equipment\n- Laboratory protocols\n- Facility construction\nThese concerns are addressed differently according to the risk assessment that the disease poses. In the US, the Center for Disease Control and Prevention (CDC) recognizes four levels of biosafety:\n- BSL-1 – Low-risk microbes that do not regularly cause illness (like the probiotic lactobacillus acidophilus) are studied in this biosafety category.\n- BSL-2 – Microbes studied in this category result in infectious diseases of varied severity. One example is Staph infection.\n- BSL-3 – These microbes are contagious through respiratory transmission and highly dangerous, potentially deadly. Tuberculosis would fall under this biosafety category.\n- BSL-4 – These are highly contagious microbes that are generally lethal with no known treatments. Only a few labs in the world work with these diseases, such as Ebola and Marburg.\nMoving up from BSL-1 to BSL-4, the safety equipment becomes more extensive. In BSL-1, gloves and eye protection are sufficient. In BSL-4, a full body, positive-pressure suit with its own air supply is required. There are more stringent laboratory safety protocols within the lab and the facility is constructed to handle more dangerous samples.\nIdentifying and Addressing Biohazards\nIn 1989, the National Research Council Committee on Hazardous Biological Substances developed a set of practical guidelines surrounding biosafety issues and concerns. Biosafety in the Laboratory establishes the major sources of microbes of varying levels (BSL-1 to BSL-4) and how to handle them:\n1. Working With Organisms that Pose a Risk\nNot all microbes pose a risk. In fact, the human body plays host to trillions of microorganisms.\nSome microbes, on the other hand, do cause harm. These are the ones that get more attention—they need to be understood to be treated. These organisms are categorized as:\nThe level of risk posed to lab personnel determines the biosafety rating. As reported in the Biosafety in the Laboratory guidelines:\n“Among the agents that have been identified in recent years as posing the greatest risk of infection to laboratory and ancillary personnel of diagnostic laboratories are the virus of hepatitis B, Mycobacterium tuberculosis, and Shigella spp.”\nThese would be studied in BSL-3 or BSL-4 approved labs. However, a virus like human immunodeficiency virus (HIV), with a lower risk to personnel and more limited transmission, would be handled in BSL-2.\n2. Hazards of Animals and Insects\nResearching infectious diseases that arise in vertebrate animals and insects have a particular set of biosafety guidelines and concerns. Some infectious diseases can transmit from animals to humans, and vice versa. Additionally, their fecal matter can be a breeding ground of protozoan cysts and helminths, which if not handled properly, can infect researchers.\nInsects have long been known to carry diseases and transmit them through bites. Additionally, working with insects exposes the researcher to hazardous materials like:\n- Waste products\n- Harmful ingredients in insect diets\n- Mold spores\n- Bacteria that contaminate larval diets1\n3. Cell and Tissue Cultures\nCell cultures provide incredible research value and are used widely throughout the microbiological and medical fields in studying infectious diseases. However, they do come with biosafety concerns.\nThis was evidenced by a 1967 case in which laboratory workers in both Germany and Yugoslavia were exposed to an infectious disease after handling tissue cultures from an African green monkey. There were 31 documented cases of an unknown case—later named Marburg virus—which resulted in seven deaths.1\nThough this was an extreme case and modern precautionary protocols have limited the number of infectious diseases resulting from cell cultures, they can still pose a threat.\n4. Necropsy Pathogens\nHealthcare workers are exposed to a wide variety of infectious agents when handling, performing an autopsy on, and disposing of a human body. There are two primary biosafety concerns:\n- Creation of air droplets with the infectious agent – When performing an autopsy of an infected body, hand saws are needed to open certain areas of the head and chest. The problem is that microbial droplets may become airborne and can be inhaled. For this reason, all healthcare and laboratory workers should wear head coverings, goggles, face masks, as well as long sleeve gowns, gloves, and shoe coverings.\n- Physical contamination via cuts, breaks in the skin, and mucous membrane – Because autopsies involve plenty of sharp objects, sufficient care should be taken in handling all needles, scissors, saws, and scalpels. Additionally, autopsies should be performed with only the necessary personnel present.\nEven if no known disease is present, all cases of autopsy should be assumed potentially infectious since asymptomatic carriers of hepatitis B and HIV are prevalent in many hospitals.\nStandard Laboratory Practices to Follow\nBiosafety measures and guidelines ensure researchers use the necessary precautions when working with infectious diseases. As a rule of thumb, there are seven basic guidelines to practice biosafety when working with any biohazards:1\n- Do not pipette solutions with the mouth. Though this may seem obvious, this was a common laboratory practice prior to the invention of modern pipettes.\n- Handle infectious fluids carefully. Avoid small containers that encourage spills, and always mix, move, or stir gently as to avoid the production of airborne particles.\n- Limit needle and syringe usage to necessary circumstances. If there are no alternatives, use needles and other sharp objects carefully, avoiding puncture. If used, dispose of them properly in puncture-resistant containers.\n- Use the protective equipment designated by the biosafety level. As mentioned above, BSL-1 may only require goggles and gloves, while other biosafety levels require further protection.\n- Wash hands and follow disinfectant protocols following work in the lab. Once you remove your protective equipment, avoid contact with any surface or individual until hands are washed.\n- Disinfect the laboratory workstation after each use. Should a spill occur, or an object be dropped, clean the area immediately.\n- Avoid eating, drinking, smoking, or having digestible products of any kind in the lab.\nUse Equipment That Fits in the Biosafety Space\nWhen working with highly infectious agents, it’s important to set up a station that contains the culture or sample. In these cases, the biosafety space is usually limited—which means it’s necessary to use equipment that fits in a small space. If you have to remove the contagious sample to perform a test outside of this space, you dramatically increase the risk of exposure.\nThis issue is one reason NanoCellect created the WOLF Cell Sorter.\nAt under 2 cubic feet, the WOLF is the space-efficient answer to any infectious disease cell sorting and flow cytometry in medicine needs your lab might have. Plus, NanoCellect provides disposable, sterile microfluidic cartridges. This reduces the risk of exposure, eliminates cleaning hazards, and prevents cross-contamination between samples.\nNanoCellect: Supporting Your Lab’s Biosafety Concerns\nStudying infectious diseases comes with inherent risks and biosafety concerns. To minimize exposure, all labs need the right equipment, proper safety protocols, and a facility that ensures protection.\nTo support your lab with the right equipment, there’s NanoCellect.\nNCBI. Biological warfare and bioterrorism: a historical review. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1200679/\nCDC. Recognizing the Biosafety Levels. https://www.cdc.gov/training/quicklearns/biosafety/\nCDC. Trends in Tuberculosis, 2018. https://www.cdc.gov/tb/publications/factsheets/statistics/tbtrends.htm\nNIH. NIH Human Microbiome Project defines normal bacterial makeup of the body. https://www.nih.gov/news-events/news-releases/nih-human-microbiome-project-defines-normal-bacterial-makeup-body\nNational Research Council Committee on Hazardous Biological Substances. Biosafety In The Laboratory: Prudent Practices for the Handling and Disposal of Infectious Materials. https://www.ncbi.nlm.nih.gov/books/NBK218635/']"	['<urn:uuid:b4c1d48e-4232-46f3-be90-0fba3d402ac0>', '<urn:uuid:0c8b569c-a7a2-4bf8-ac4f-bff551583b72>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T05:06:54.527160	21	93	2135
38	How do modern libraries and digital platforms integrate geographic information systems and spatial analysis capabilities to support both research and practical applications?	Modern libraries and digital platforms offer comprehensive integration of geographic information systems. Fondren Library's GIS/Data Center provides training and access to geographic information systems software and geospatial data sets, while the Digital Media Commons supports the creation and use of digital media. In terms of practical applications, GIS and spatial analysis software can examine locations, objects, attributes, and relationships in spatial data using various analytical techniques to provide valuable insights. These systems allow users to capture, store, analyze, visualize, and manage geographic information. The technology is being applied widely, from academic research to public services, as seen in applications like government open data websites that provide information for public transportation, housing, and city planning. Location intelligence software is specifically used to discover meaningful insights from geographical relationships found in most information.	"['Welcome to Fondren!\nOur collection includes books, e-books, audiobooks, leisure books, journals, CDs, DVDs, streaming audio and video, sheet music, maps, digital data, manuscripts, government publications and patents.\n- The Fondren Library website has a single search box that searches our catalog, journals, databases, online articles, archives, digital scholarship, and our handy research guides. library.rice.edu\n- The Rice University Digital Scholarship Archive contains online version of theses and dissertations, historical images, the student newspaper, manuscript collections, and rare and archival materials. scholarship.rice.edu\n- Off-Campus access to all online library content is available to Rice NetID holders only through the Rice EZproxy service. library.rice.edu/offcampus\n- The Rice Alumni Digital Library provides online access to a subset of the library’s online collections licensed for remote use by Rice alumni. alumni.rice.edu\n- Library Research Guides - libguides.rice.edu\n- Subject Specialists - library.rice.edu/librarians\n- Need Help? - library.rice.edu/help\nTEACHING AND LEARNING SUPPORT\n- Course Reserves - library.rice.edu/course-reserves\n- Short courses on research and technology are offered regularly throughout the year. Customized instruction is also available to faculty members upon request. library.rice.edu/events\n- Library tours for high school students and community groups are available upon prior request library.rice.edu/tour-request\n- The Center for Written Oral and Visual Communication, on the second-floor, offers appointments with peer consultants who provide feedback on papers, oral presentations, research posters, theses and dissertations. cwovc.rice.edu\n- The Brown Fine Arts Library, on the third-floor, provides materials related to art, architecture, classical archeology, and music. library.rice.edu/brown-fine-arts-library\n- The Gilbert and Ruth Whitaker Business Information Center (BIC) (McNair Hall 270) is primarily for the support of the Graduate School of Business and provides materials such as current business journals, books, directories and corporate annual reports. library.rice.edu/bic\n- The Kelley Center for Government Information, Data and Geospatial Services, in Fondren’s basement, provides U.S. Government and Texas state documents patents and trademarks. library.rice.edu/gov\n- The GIS/Data Center (GDC) (Fondren B40) provides training and access to geographic information systems software and geospatial data sets. library.rice.edu/gdc\n- The Digital Media Commons (DMC) (Fondren B42) provides tools and training to support the creation and use of digital media, including audio, video, scanning and printing services. library.rice.edu/dmc\n- The Woodson Research Center (Fondren 160) contains Rice University’s archives and rare book and manuscript collections library.rice.edu/woodson\n- Most specialized study spaces and rooms are available only to Rice faculty, staff and students. library.rice.edu/maps\n- Group study rooms can be reserved online by students for up to four hours per day. rooms.library.rice.edu\n- Designated quiet spaces are available on the second, third and sixth floors. library.rice.edu/content/quiet-study-spaces\n- Study carrels and lockers may be reserved by faculty and eligible graduate students on an annual basis.\n- The student lounge on the fourth floor offers vending machines.\n- The Kyle Morrow Room (Fondren 308) is an exhibit and event space. library.rice.edu/requests/kyle-morrow-room\n- The music study practice room (Fondren 332) has an electronic keyboard for practicing sheet music.\n- Public computers and printers (first floor near Reference Desk) require a visitor access code to initiate a user session, which is limited to two hours. Access codes can be requested upon Non-Rice visitor sign-in at the East Entrance.\n- Rice Network computers and printers (first floor and second floor mezzanine) require a Rice NetID for access for students and staff.\n- Public photocopiers (basement through fourth floors)\n- Public scanning kiosks (first floor near Reference Desk, Woodson Research Center and Brown Fine Arts Library)\n- Public stereos and video viewing stations (first floor)\n- Headphones are available at the Circulation Desk for the public\n- Audio visual and digital equipment is available for checkout by faculty, staff and students at the Digital Media Commons (B42)\n- Wi-Fi is available to the public on the Rice Visitor network\n- ATM (first floor near the East Entrance)\n- Working with Rice’s Disability Support Services, Fondren Library provides accessible facilities, adaptive equipment and material retrieval assistance to those with access needs. library.rice.edu/accessibility\n- Rice Students, Faculty and Staff privileges - library.rice.edu/borrow-request\n- The Friends of Fondren Library organization supports the library by raising funds and sponsoring special programs. Membership provides circulation privileges. library.rice.edu/membership\n- Fondren Library is open to anyone 18 year of age or older with a valid photo ID. Those under 18 must be accompanied by an adult. Visitors should check in at the East Entrance. library.rice.edu/visitors\nPARKING AND TRANSPORTATION\n- Visitor parking is available with different rates and proximity to Fondren Library. The closest parking lot is the Central Campus Garage. Free shuttle service is available from distant parking lots. park-trans.rice.edu\nNews & Features\nVoter Registration Events - Sep. 21, 25, 27 & 28\nRegister to vote! Deputy Voter Registrars will be in Fondren Library near the circulation desk to help you register to vote and answer any questions you may have about registration or the voting process before the October 9 registration deadline.\nTRIAL: The American Slavery Collection, 1820-1922: From the American Antiquarian Society.\nThis collection contains ""the American Antiquarian Society\'s extraordinary holdings of slavery and abolition materials [that] delivers more than 3,500 works published over the course of more than 100 years."" This database ""addresses every facet of American slavery - one of the most important and', 'ProgrammableWeb recently published an article featuring the top 10 mapping category APIs. Most of the APIs featured in the article are for accessing map libraries or geographic data/content. However, the mapping category includes APIs for programmatically accessing map libraries, geographic data sources, geographic information system and spatial analysis software, location intelligence solutions, indoor venue maps and more.\nMany companies not only provide APIs for developers, but also provide easy-to-use GIS and mapping solutions that make it possible for users with very little technical knowledge to create beautifully designed static, interactive or animated maps. No matter where you go on the Web these days you’re bound to find a map, all thanks to the availability of easy-to-use GIS solutions and APIs.\nYou can find maps on government open data websites like the DublinDashboard, which features a variety of Web maps that provide information for public transportation, housing, city planning and more. The city of Boston website features a nice selection of city data Web maps and a mapping application that citizens can use to estimate the cost of installing rooftop solar panels.\nYou can find maps in applications for calculating commuting distances like the EV Explorer Web app, which uses maps to help calculate how much money people would save if they drove an electronic vehicle. ORBIS: The Stanford Geospatial Network Model of the Roman World Web app uses maps to provide the costs of different types of travel in antiquity across the Roman Empire.\nMaps are everywhere and not just all over the Web. Maps can be found on smartphones, smartwatches, automobile dashboards, indoor kiosks, outdoor kiosks and more. Maps are being used by millions of people around the world every day.\nThis article highlights different types of GIS, mapping and location-based solutions. This article does not highlight low-level libraries such as D3.js and Three.js, which can also be used for map visualization.\nMost map libraries have a basic display model of one basemap with options that can include map layers, cartographic styles, map controls, customization via CSS and zoom animation. Map layers can include vector objects, image overlays, tiles, markers pop-ups and other features.\nMany companies provide map libraries that can be used to add interactive maps to Web pages and mobile applications. Popular map libraries include amMap, ArcGIS, CartoDB, GIS Cloud, Google Maps, HERE, Leaflet, Mapbox, MapQuest and OpenLayers.\nGeographic Data/Content and Services\nMany companies provide geocoding services, routing services and/or geographic content. Geocoding takes location information like a street address and converts it into spatial data that can be displayed as a feature on a map. Routing services are used primarily for turn-by-turn directions and sometimes include detailed road attributes.\nContent can include ready-to-use maps and data sets containing various types of information such as demographic, transportation, terrain and imagery.\nGIS and Spatial Analysis\nGeographic information system software allows users to capture, store, analyze, visualize and manage geographic information. Most GIS solutions feature spatial analysis capabilities and are able to analyze spatial data to uncover patterns, relationships, trends and other new information. GIS and spatial analysis software examine locations, objects, object attributes and relationships of features in spatial data using a variety of analytical techniques in order to provide valuable insights or address specific questions.\nLocation Intelligence and Analytics\nLocation intelligence software is used to discover meaningful insights from the geographical relationships found in most information. Objects such as businesses, points of interest and geographic regions are analyzed along with their spatial attributes.']"	['<urn:uuid:0ba21e91-b30e-44fc-9caf-b7373d5c66d6>', '<urn:uuid:d1d9a550-051e-42a2-9d00-3a31eb2a8623>']	open-ended	direct	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-13T05:06:54.527160	22	131	1439
39	As an ecology student, what are the main branches of ecological study?	There are four major areas in ecology: organismal ecology, population-evolutionary ecology, community ecology and systems ecology.	['Conservation biology is the application of science to conserve the earth’s imperiled species and ecosystems. The field is a relatively young one that is growing rapidly in response to the biodiversity crisis, perhaps the most critical environmental issue of our time. Conservation biologists view all of nature’s diversity as important and having inherent value. This diversity spans the biological hierarchy and includes variation at the level of genes, populations, communities, ecosystems, and biomes. A focus on biological diversity and an intrinsic valuation of nature is what distinguishes conservation biology from wildlife management (with its somewhat more utilitarian perspective and a focus on populations of birds and large mammals) and from general environmental biology (with a broad focus on environmental issues).\nWildlife science is the application of ecological knowledge in a manner that strikes a balance between the needs of wildlife populations and the needs of people. Research and teaching in wildlife science began at ESF in 1914, one of the first such programs in the U.S., and was quickly followed by establishment of the Roosevelt Wild Life Station in 1919. Today, our program is recognized nationally and internationally, and our graduates are employed worldwide. The focus is applied ecology, and students engage the environmental challenges associated with managing wildlife, ranging from endangered species to overabundant populations. The program recognizes and accommodates the fact that wildlife scientists increasingly must deal with on all forms of wildlife, including plants and invertebrates, and the scope is becoming more international.\nAquatic and Fisheries Science\nAquatic and fisheries science is the study of aquatic ecosystems to increase scientific understanding and to apply basic ecological principles to their management, thereby sustaining them for multiple uses. Aquatic ecosystems include wetlands, streams, lakes, estuaries and oceans. Aquatic science professionals study and manage valued natural systems for seafoods, drinking water, recreation, transportation and aesthetics. This field of study has a long history; for example, the American Fisheries Society was founded in 1870 and the American Society of Limnology and Oceanography in 1948. Aquatic systems and their organisms are sufficiently distinct from terrestrial systems that numerous professional organizations and scientific journals have been founded specifically to foster communication among aquatic science professionals.\nFish and Wildlife Biology and Management\nFish and Wildlife Science at the SUNY College of Environmental Science and Forestry (ESF) forms a broad undergraduate and graduate concentration recognized nationally and internationally; our graduates are employed worldwide. These disciplines function within the Department of Environmental and Forest Biology, a group of over 30 scientists from a broad array of ecological and biological disciplines. Fish and Wildlife Science at ESF includes about 250 undergraduate and 60 graduate students.\nThis area entails study and maintenance of biological diversity at the level of genes, populations, communities, ecosystems and biomes; intellectual underpinnings include evolutionary theory, systematic biology, population biology and ecosystem science. Conservation biology seeks ways to integrate biological principles with social, economic and political perspectives to achieve conservation goals. The field is a response of the scientific community to the biodiversity crisis. Conservation biologists view nature’s diversity as important and having inherent value. Training in this field includes experience with the fundamental disciplines and theory of conservation biology, as well as specialization in conservation issues.\nEnvironmental interpretation sharpens the cutting edge of communication among scientists and various public sectors. Graduate study enables students to explore interpretation/ conservation education processes through application to specific projects in the natural sciences and science education. Students pursue career pathways in natural resource agencies, in nature centers, museums, aquaria, botanical gardens and especially in the science classroom. The environmental interpretation program incorporates a 15,000-acre reserve in the heart of the Adirondack Park and an associate Visitor Interpretative Center with trail system. Internships and partnerships with a variety of conservation-based programs are vital to the program. Students develop their course of study from a large palette of graduate courses in Environmental and Forest Biology.\nThis area of study in the M.P.S. degree is designed for students who desire to solidify their background in applied ecology and professionals who would return for “retooling”; suitable for careers in environmental oversight, policy, planning, law, and education. This program begins with a three-day orientation in August at one or more of the ESF field facilities. Coursework requirements include three credit hours each from five of the seven focus areas: GIS tools, Statistical Tools, Specialty Tools, Ecosystem Ecology, Organismal Ecology, Human Dimensions in Ecology, and Communications in Ecology; two credit hours in graduate seminars (EFB 797) and additional 19 credit hours of graduate coursework for a total of 36 credit hours.\nThis integrative study area allows students to investigate the relationships of organisms to their environment and those factors which affect their distribution and abundance. Both the practical and theoretical applications of ecology are emphasized through courses and research. There are four major areas in ecology: organismal ecology, population-evolutionary ecology, community ecology and systems ecology. In consultation with the student’s steering committee, courses are chosen from these areas, as well as other disciplines. Specific research may encompass any of the four major areas of ecology and entail the study of the distribution and abundance of organisms, community structure including trophic relationships, diversity, succession and ecosystem properties, such as patterns of energy transfer and biogeochemical cycling.\nGraduate study opportunities prepare students in the basic aspects of insect life and the role of insects in relation to humans and their environment. The wide range of effects stemming from insect activity, from the beneficial to the deleterious, allows for a variety of research subjects in which insects play a major role. Thesis topics may concern insects that affect forests, shade trees and wood products, those relating to the health and well-being of humans, those playing key roles as parasites and predators of pest species, and those serving as food for many birds and vertebrate animals. Current research areas include population dynamics of forest defoliators, pheromone communications in beetles and moths, evolution of chemical communication, effects of forest practices on stream benthic insects, natural control of insects in forest systems and biochemistry of insect detoxification mechanisms.']	['<urn:uuid:565e0f78-e6e2-4810-8887-cd23c057342f>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	12	16	1009
40	difference between musician and jewelry wearer allergy reactions symptoms	The musician (clarinet player) developed coughing and wheezing symptoms due to black molds in his uncleaned instrument, which didn't respond to various treatments including inhaled corticosteroids and antibiotics. In contrast, jewelry wearers experience eczema-type symptoms when wearing jewelry, with about 36% of allergy sufferers reporting this reaction according to a South African study.	"['This article is a collaboration between MedPage Today® and:\nBALTIMORE -- From saxophone lung to the strange affair of the gummy bear allergy, case reports take center stage at the annual meeting of the American College of Allergy, Asthma, and Immunology here.\nThat\'s partly because case reports help illuminate the variety of things that can affect the immune system, according to Jay Portnoy, MD, of Children\'s Mercy Hospital in Kansas City, Mo.\n""Immunology is replete with very unusual presentations,"" Portnoy, who was chair of the meeting\'s abstract committee, told MedPage Today.\n""It\'s not uncommon that a single case report can spark interest and perhaps lead to more reports and better medicine,"" he said.\nThe meeting attracts many younger physicians who might not yet have had time to take part in major studies, but who have had unusual clinical experiences as new allergy fellows, Portnoy added.\nPresenting those cases ""gets them into the scientific swing,"" he said. ""It helps them get their feet wet.""\nAmong the unusual cases this year:\nA musician -- a clarinet player, in fact -- had not cleaned his instrument in more than 30 years, according to Marissa Shams, MD, and colleagues at Emory University School of Medicine in Atlanta. He presented with a 1-year history of coughing and wheezing that didn\'t respond to inhaled corticosteroids, bronchodilators, or oral antibiotics.\nBoth the patient -- originally diagnosed with allergic bronchopulmonary aspergillosis (ABPA) -- and the instrument turned out to be positive for black molds. The combination of oral steroids and cleaning the licorice stick improved the man\'s condition. Take-home message: Not everything that looks like ABPA is.\nCat Saliva Attack\nAn HIV-positive man tried to flick some of his pet cat\'s saliva off a couch, according to Kathleen Dass, MD, and colleagues at Beaumont Hospital in Royal Oak, Mich. In the process, he developed a splinter hemorrhage in his finger, which became ""pale, swollen, and numb,"" the investigators reported.\nThe man went on to develop pruritus on his hands that spread to his arms, urticaria on his chest and trunk, and generalized swelling. After initial improvement with antihistamines and corticosteroids, he began wheezing, with hypotension and increasing tachycardia, and was rescued with epinephrine and famotidine.\nTake-home message: Anaphylaxis to animal secretions is rare and has not been reported with a splinter hemorrhage as the trigger.\nBlame the Lobster\nA 35-year-old woman on a south Pacific vacation had shrimp salad in the morning, lobster bisque in the afternoon, and traces of lobster later -- followed by anaphylaxis. She had not previously been sensitive to shellfish, according to Anil Patel, MD, and colleagues at West Virginia University in Morgantown.\nSix months later, after accidentally eating some crab, she was rushed to emergency with shortness of breath and a closed-throat feeling and spent 4 days in intensive care getting epinephrine every 4 hours. After release and while on oral corticosteroids, she had several other bouts of anaphylaxis.\nTake-home message: It took only a day for sensitization, but the anaphylaxis was protracted.\nDon\'t Breathe Easy\nA 71-year-old man with interstitial pulmonary fibrosis received new lungs, from a 15-year-old who had died of anaphylaxis after eating peanuts, according to Priyanka Lall, MD, and colleagues at Emory University. But no one told the recipient and he had two bouts of anaphylaxis after transplant -- both times after eating a dessert with nuts in it.\nThe man\'s post-transplant serum specific IgE levels were elevated to walnut, hazelnut, pistachio and cashews, although he had no history of nut allergy.\nTake-home message: The allergic profile of the donor should be on the donor card and the recipient should be cautioned, if necessary, about food allergies.\nGummy Bear Threat\nGelatin allergy is rare, but if gummy bears make you itch, you might want to be careful about getting a flu shot, according to Stephanie Albin, MD, and a colleague at Icahn School of Medicine at Mount Sinai in New York City. They reported the case of a 4-year-old boy who was found to have gelatin sensitivity after he developed diffuse hives, watery eyes, sneezing, and vomiting within 15 minutes of receiving his flu shot.\nHe had previously been immunized without a problem, and his parents worried he had an egg allergy. But they also reported he had displayed watery mouth, abdominal pain, and weakness after eating gummy candies, gummy vitamins, and marshmallows.\nTake-home message: People with gelatin allergies need to be careful when getting vaccines, and might consider having an allergist perform the immunization.\nBut all physicians involved in vaccinations should be aware of the possibility of gelatin allergies, Portnoy told MedPage Today. While a great deal of attention has been paid to egg allergies, he said, gelatin is actually more of a problem with certain vaccines.', 'While around 17 million South Africans suffer from seasonal hay fever every year, there are some unusual allergies that could surface at any time. We take a closer look at six unusual allergies that could affect you at any time.\nWhat is an allergy?\nWhether you have a common or unusual allergy, the body reacts in a similar way. According to the Allergy Foundation South Africa, “Allergies occur in people who are more sensitive than ‘normal’ people to innocent substances in their environment. These environmental factors that cause the symptoms are called allergens and many things in the environment can be allergens: some indoors, some outdoors and some taken in as foods or medicines.”\nHow are allergies triggered?\nAllergens differ from patient to patient. There is no ‘one size fits all’ list of allergens. Your symptoms may be caused by your specific allergy triggers. To find out what triggers your unique symptoms, your doctor needs to find out where and when the symptoms are worst, and then do skin or blood tests to look for ‘the allergy antibody’ called IgE.\n“Even if you’ve never suffered from an allergy before, it’s entirely possible to become allergic later in life. Although we don’t have substantial data in South Africa, it’s clear that allergy rates are climbing and are likely to continue to rise in the future, therefore it’s best to know your allergy status and to carry emergency medication with you.\nUnusual allergies to watch out for\n“You should never ignore the symptoms of an allergic reaction. If left untreated, it can quickly worsen, especially in the case of anaphylaxis, where emergency treatment is required,” advises Nicole Jennings, a spokesperson at Pharma Dynamics, one of South Africa’s leading providers of allergy treatments. “And even though scientists are still grappling with the why or how people become allergic to certain allergens, thankfully there are ways to treat them.”\nThe company recently conducted a nationwide poll of 1 772 allergy sufferers to find out to what degree South Africans are affected by unusual allergies, and came up with some surprising numbers.\nWhile a whopping 17 million South Africans (around 30% of the population) suffer from hay fever every spring, some unusual allergies include;\n- 36% experience eczema type symptoms when wearing jewellery\n- 25% are allergic to certain fruits\n- 21% break out in a rash when spending time in the sun (different to sunburn)\n- 15% are allergic to alcohol\n- 7% experience either hives, swelling of the mouth and throat or wheezing after eating chocolate\n- 6% complain of headaches, heart palpitations or skin problems when drinking coffee\n“Fruit such as strawberries, kiwi, peach, apple, bananas and citrus are known to cause allergies, which can prove fatal in severe cases,” says Nicole. “Most often, symptoms include skin reactions, swelling of the oral cavity, asthma or rhinitis.\nSimilar signs of alarm are also common in those who are allergic to alcohol. While some may be allergic to alcohol itself, others may be sensitive to certain allergens in alcoholic drinks, such as wheat, barley, yeast, grapes etc. However, if you are allergic to alcohol, fruit or any other type of food, rather steer clear of these allergens.\nCoffee and chocolate allergies\n“Two sinful, but everyday indulgences, coffee and chocolate, could trigger allergy symptoms in some, but it’s important to note the distinction between allergies and sensitivities,” remarks Nicole. “While both are problematic, a sensitivity to something isn’t life-threatening, while an allergy can be.\nSymptoms of a sensitivity could include stomach cramps, feeling bloated or jittery, anxious and could even elevate your heartbeat and blood pressure, especially if you’re sensitive to coffee, while an allergy can cause more serious symptoms, such as nausea, vomiting, trouble swallowing, shortness of breath, wheezing, a sudden drop in blood pressure and dizziness.”\nSigns of a severe allergic reaction\n“Anaphylactic reactions usually happen fast. Symptoms often become the most serious within three to 30 minutes of exposure to the allergy trigger,” say doctors at WebMD. “Quicker reactions are usually more severe.”\nStudies have also shown that most anaphylactic reactions have symptoms in two or more areas of the body and you should take the following symptoms seriously:\n- Trouble breathing or noisy breathing\n- Coughing or wheezing\n- Sneezing and Congestion\n- Tightness in the lungs\n- Chest pain\n- Low blood pressure\n- A weak, rapid pulse\n- Dizziness and fainting\n- Pale or flushed skin\n- Hives or itchy skin\n- Swelling of the throat, face lips or tongue\n- Abdominal pain, nausea, vomiting or diarrhea.\nBy Health Editor, Belinda Dos Santos']"	['<urn:uuid:cb9cc1a6-ea10-413a-bb5f-e87fff86cba4>', '<urn:uuid:6cd19d5f-b802-4e7e-a9ee-f86b9ebe7aed>']	factoid	direct	long-search-query	distant-from-document	comparison	novice	2025-05-13T05:06:54.527160	9	53	1553
41	I'm studying to work on ships that use LNG fuel. What kind of training do crew members need according to safety regulations?	According to IMO's IGF Code of January 1st, 2017, all personnel working on vessels using gases or low-flashpoint fuels must be properly trained. Basic training is required for seafarers responsible for safety duties related to LNG fuel, while advanced training is required for masters, engineer officers, and personnel directly responsible for the care and use of fuels and fuel systems on the ships.	['LNG-FUELED MARINE VESSEL TRAINING SYSTEM\nAccording to IMO’s IGF Code of January 1st, 2017, personnel working on vessels using gases or other lowflashpoint fuels must be properly trained. These training requirements are further addressed through STCW regulations. Kongsberg Digital’s latest K-Sim Engine model, Diesel Electric Dual Fuel Cruise Ferry, enablessafe, realistic and efficient training on LNG-bunkering procedures for engineering students and crew. It isspecially designed to support IMO’s IGF Code and STCW’s new requirements for basic and advanced training\nIGF Code and STCW regulations\nBecause of stricter environmental requirements for sulphur emissionin shipping, LNG is increasingly used as a clean fuel alternative, specifically for vessels operating in Emission Control Areas (ECA), suchas the North Sea, Baltic Sea and designated coastal areas in theUnited States and Caribbean Sea. The 2020 global sulphur limit isset at 0.5%, which will increase the use of LNG as a fuel worldwide.\nBased on IMO’s IGF Code the STCW requirement raises the bar forthe education of students and seafarers responsible for safety duties,as well as masters, engineer officers and other personnel with directresponsibility for the care and use of LNG-fuelled systems on a vessel.\nIn order to satisfy section A-V/3 paragraph 1 of the STCW requirements, basic training is required for all seafarers responsible for designated safety duties related to the care and use of LNG fuel in additionto emergency response on board ships subject to the IGF code.Whilst for section A-V/3 paragraph 2, advanced training is requiredfor masters, engineer officers and personnel with immediate responsibility for the care and use of fuels and fuel systems on the ships.\nSophisticated LNG-bunkering simulator fulfils the trainingrequirement\nKongsberg Digital provides a range of K-Sim Engine simulator models enabling engineers to learn and practice daily procedures while gaining vital skills crucial for operational safety. To support proper training regarding sulphur emissions, the latest engine simulation model, K-Sim Engine DEDF42 Cruise Ferry, provides both basicand advanced training for LNG bunkering operations. It provides a cost-efficient and safe way to build knowledge and test procedures,while fulfilling IMO’s IGF Code and the STCW requirements in addition to being certified for training by DNV GL.\nLNG BUNKERING TRAINING SYSTEM\nThe new DEDF42 CF model is provided as a PC desktop systemfor student training. It is based on a dual fuel diesel electric configuration with the engine room systems on board a cruise ferry. In addition to training on operating a medium speed, dual fuel (gas and diesel oil) engine, the K-Sim Engine DEDF42 CF model’s main purposeis to facilitate training courses that comply with STCW.\nMain training elements:\n- Characteristics of LNG (low flashpoint fuels quality)\n- Safe working practices with LNG equipment\n- LNG bunkering operation procedures\n- Fire prevention\n- Emergency procedures\n- Operational principles of dual fuel generator installations\n- Fuel oil and LNG supply system for diesel generators\nIn addition, the simulator includes sufficient control relevant for the particular ship type enabling the realistic simulation of the ship/truck/shore to ship interface and the required checks and operations.\n- Shore side mimic: selection of barge, tank and truck, including fuel quality, methane number, Wobbe Index and density\n- Connection/disconnection of shore/ship/truck interface (LNG)\n- Flow rate control\n- Emergency shutdown implementation\n- Purging control\n- Capabilities to simulate effects of excess line pressures and resulting action\n- Onboard LNG storage and bunkering system\n- LNG monitoring system for bunker operation\n- LNG emergency shutdown\n- Gas heating\nDependent on background knowledge and experience of thetrainee, the simulator provides training for system familiarization,operations and procedures for:\n- Propulsion plant Integrated Automation System (IAS)\n- Alarm and Safety Warning System (ASWS)\n- Power Management System (PMS)\n- Propulsion Control System (PCS)\n- Dual fuel diesel generator sets and support systems\n- Electric power supply incl. switchboards and distribution centre\n- Ventilation control system in machinery space\nINSTRUCTOR CONTROL& ASSESSMENT\nKongsberg Digital Engine Room and Cargo Handling simulators havea state-of-the-art Instructor, Monitoring and Assessment system. In close cooperation with experienced worldwide instructors, the Norwegian Maritime Directorate and DNV GL, Kongsberg Digital has designed and developed a user-friendly and efficient approach that enables instructors to optimize the pedagogical value of exercises for students.\nThe Instructor can easily configure each Student Station to define what information shall be accessible and visible, including defining which subsystems apply to each station, and indicators that should be viewed. Instructors have access to a unique tool that allows the assessment of students on all levels, from support to management.This allows the Instructor to monitor, and use for assessment, not only alarms, but any of the available variables in the simulation model.\nThe assessment can be performed during simulations where theInstructor is absent. The instructor has the possibility to print out assessment reports for each individual student reflecting his performance, including pass or fail. The Instructor, Monitoring and Assessment system also has an intuitive and user-friendly feature for Recording and Replay, allowing full debrief after simulator exercises.\nWe are pleased with the LNG bunkering solution delivered by Kongsberg Digital. The system enables us to offer advanced training for ships subject to the new IGF code, including simulated training scenarios. The simulation training is provided in a safe and realistic environment, where students and professionals can learn to better understand the complexities, risks and hazards of working with LNG\nDNV GL COMPLIANCE\nIn addition to meeting the STCW requirements based on the IMO IGF Code, the K-Sim Engine DEDF42 Cruise Ferry model has achieved DNV GL Statement of Compliance as a Class S simulator.This is based on latest DNV GL Standards for Certification of Maritime Simulators that addresses specific requirements for simulatorsused for training ship`s officers using LNG as fuel.The DNV standard is based on the requirements of the STCW convention which requires the approval of simulators when used for mandatory simulator-based training, or when used to demonstrate competence.']	['<urn:uuid:b6ebcf6e-8a3d-4f0a-ad9a-39610305e24a>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:06:54.527160	22	63	968
42	I'm developing treatment protocols for full-mouth reconstructions. Between treating a dog's entire maxilla with 3D printed implants and replacing a human's full dental arch, which approach requires more individual attachment points?	Replacing a human's full dental arch using conventional individual implants would require up to 14 implants per arch, while the All-On-4 system needs only 4 implants. In contrast, for the dog's maxillofacial reconstruction case, a single custom 3D-printed implant was sufficient to restore the entire affected area of the maxilla, making it a more streamlined approach compared to human full arch restoration.	['Humans are the only animals that take on pets and the profound connections that we develop with our animal companions mean that we want to do everything in our power to keep them happy and healthy. We do this not just because we want it for them, but also because we want them to be with us, for as along as possible. The great tragedy of the profound love that we feel for our pets is that their lives are so much shorter than ours. Each loss is a tragedy and yet, as all pet owners know, we can’t help but put ourselves through it over and over again. The responsibility that comes with having that special kitty or pupper – or even ball python – is ensuring that it receives love, comfort, and appropriate medical care.\nTaking proper care of a pet may mean only checkups and maintenance, but sometimes something extraordinary happens and for that, we need extraordinary veterinarians. Such was the case when a Canadian family’s seven-year-old Bernese Mountain Dog developed a significant tumor that required removal. The development of tumors in dogs as they age is relatively common, with more than half having one after reaching the age of 10. Unfortunately, the location of this tumor on the dog’s muzzle was going to required an excision that would remove a large portion of tissue and could significantly affect the dog’s quality of life. This required that the veterinarian develop a plan for addressing the affected area so that the dog would be able continue to lead a good life.\nAs such, veterninarian Julius Liptak of Alta Vista Animal Hospital in Ottawa, Canada worked with Germany-based Voxelmed to create a patient-specific maxillofacial implant. The implant itself was then fabricated at the Additive Design in Surgical Solutions (ADEISS) center in London, Ontario, a center arising from a partnership between Western University, the London Medical Network and Renishaw. Working back and forth between 3D model and data about the patient provided through 3D scans, an implant that met the surgeon’s specifications was created. This type of modeling and preparation also gave Dr. Liptak the ability to create a detailed plan for the procedure before having to actually undertake it, thereby reducing the chance of errors and decreasing the time necessary for the dog to remain under anesthesia while the operation took place. The creation of this implant meant that the dog’s nose structure did not have to be altered and the difference in his before and after appearance would be minimal. This was important not for the dog’s vanity, but rather in terms of his ability to breathe easily, eat well, and play comfortably.\nNot only the ability to create a prosthetic that was precisely right for this individual patient, but also the fact that 3D printing allows one to be fabricated so quickly was key to the success of the implant. As the tumor was continuing to grow, the area affected by it changed the topography of the affected area, so had the implant taken too long, it would have no longer been a proper fit. The ability of 3D printing to map complex geometries and do so very precisely is also part of what makes this kind of implant possible, as Voxelmed’s CEO and veterinary surgeon Jan Klasen explained:\n“Without additive manufacturing technology, it would have been almost impossible to reconstruct the dog’s maxilla after tumor removal, because the area was extremely complex in geometry. The implant had to have similar shape and functionality as the dog’s existing bone structure. Using additive manufacturing to maintain the original shape and function of the oral and nasal cavity ensures a high quality of life for the dog, just as a naturally shaped skull and maxilla helps the dog to breathe and eat easily.”\nHe continued, “To my knowledge, this is the first implant of its kind. Prior to this case, the majority of veterinary surgeons were unaware that the technology was even available. Similar reconstructions are now being planned in Germany and there is ongoing research into how animals can benefit from this procedure.”\nThe doggo was quickly able to breathe easily through his nose and after a couple of days of pain medicine, appeared to be making a full and complete recovery, going home after only one day in the hospital. The success of this medical intervention is just one more example of the inroads that 3D printing is making into medicine, whether human or animal, and the wider the realization of the benefits this type of manufacturing can provide. What was once out of reach for all but the most extreme of cases, is now becoming a routine part of the practice of medicine, both changing the face of medical practice in general and touching the lives of each patient individually.\nWhat do you think of this news? Let us know your thoughts; join the discussion of this and other 3D printing topics at 3DPrintBoard.com or share your thoughts in the comments below.[Source/Images: Renishaw]\nYou May Also Like\n3DPOD Episode 93: Bound Metal 3D Printing with Mantle CEO Ted Sorom\nTed Sorom, CEO and co-founder of Mantle, is looking to revolutionize metal 3D printing. Mantle has a paste extrusion method that features a post-machining step to mill unfinished parts and...\nBig and Tall Metal 3D Printer Heralds Rocket Future for China’s EPlus 3D\nUntil recently, Chinese 3D printer manufacturers either stuck to selling in China, made inexpensive 3D printers, made copies of Western printers, or did some combination of all of the above....\nDesigning and Metal 3D Printing a Dental Implant\nLes Kalman is Assistant Professor of Restorative Dentistry and Academic Lead for Continuing Dental Education at Western University’s Schulich School of Medicine & Dentistry. He will be participating in Additive...\n3D Printing Webinar and Event Roundup: January 23, 2022\nWe’ve got plenty of webinars and events to tell you about in this week’s roundup: NAMIC and CASTOR are talking 3D printed parts identification, Carbon has a major announcement, HP...\nView our broad assortment of in house and third party products.', 'What if you could replace all your teeth with just four implants per arch?\nMillions of American adults struggle with missing, broken, or otherwise unsightly teeth. Often, these imperfect teeth can be treated with a variety of cosmetic procedures from dental implants to crowns and bridges.\nHowever, what happens when you need to replace an entire arch?\nThat’s where fixed full arch dental implants or All On 4 dental implants come in.\nWhat Are All On 4 Dental Implants?\nTo understand what All On 4 dental implants are, it helps to know about dental implants. All dental implants require the placement of tiny titanium posts, or screws, embedded in your jaw. These implants provide the foundation of various cosmetic prosthetics such as crowns or bridges.\nImplants are by far the preferred choice for replacing missing teeth. Implants replace dental roots and provide a strong, integrated foundation for prosthetic attachments.\nAll On 4 dental implants are a type of fixed full-arch replacement treatment. All On 4 implants are designed to replace an entire upper or lower arch. The All On 4 approach utilizes just four strategically located implants to support a fixed bridge, or denture prosthetic. This arch replacement strategy has numerous advantages over other teeth replacement treatments.\nWhat Are the Benefits & Advantages of All On 4 Dental Implants?\nAll On 4 dental implants have many advantages over conventional dental implants and traditional dentures.\n- Fixed in place\n- Natural mouth feel\n- Natural aesthetic qualities\n- Biologically integrated (osseointegration)\n- Prevents bone resorption\n- Minimal jaw bone depth requirements\n- Potentially lower lifetime costs\n- New teeth in a single appointment\nChallenges of All On 4 Dental Implants\nAll On 4 dental implants are an excellent choice for full arch replacement patients. Additionally, All On 4 dental implants are suitable for a wide range of patients.\nHowever, All On 4 dental implants aren’t for everyone.\nChallenges associated with All On 4 dental implants include:\n- Unsuitable for single tooth replacements\n- Requires a minimum jaw bone depth\n- Higher upfront costs than alternatives\n- Device failures can require extensive repairs\nAll On 4 Dental Implants vs. Dentures\nTraditional dentures are a popular alternative to dental implants. Prior to the invention and widespread use of modern dental implants, dentures were the preferred treatment for missing teeth. While affordable and easy to replace, dentures have significant drawbacks when compared to implant-based solutions.\nMajor differences between dentures and All On 4 dental implants include:\n- Cost (initial)\nIn general, dentures are much less expensive than All On 4 dental implants. However, when lifetime costs are taken into consideration, the total cost differences are less clear-cut. Traditional dentures must be replaced as frequently as every few years. In comparison, All On 4 dental implants can last many years or longer. Well-designed, expertly fitted, and well-maintained, it’s not uncommon for All On 4 dental implants to last decades or even a lifetime.\nThis brings us to the question of longevity. All On 4 dental implants can be expected to last much longer than a set of dentures. Dentures must be replaced frequently. Furthermore, removable dentures are prone to breakage from being repeatedly removed for cleaning.\nAll On 4 implants function much like your natural teeth. On the other hand, dentures can sometimes fall out or limit what you can eat and do. One of the best arguments for All On 4 dental implants is the high level of functionality they provide.\nWhile both All On 4 dental implants and traditional dentures can look great, All On 4 dental implants tend to look more natural and aesthetically pleasing. All On 4’s must be carefully tailored and fitted for each person’s individual smile. As a result, All On 4 custom solutions tend to look better than mass-produced dentures.\nFinally, there is no question that All On 4’s feel much better in the mouth. They are functional extensions of your jaw and feel much like your natural teeth would. Dentures simply cannot compare to implant-based solutions in this important regard.\nAll On 4 Dental Implants vs Indivudual Dental Implants\nWhile similar in some ways, All On 4 dental implants should not be confused with conventional individual dental implants.\nIn order to replace a full arch of teeth, All On 4 dental implants require four strategically placed dental implants per arch. In contrast, a conventional individual implant approach would require as many as 14 implants for a single arch or 28 implants for a full mouth replacement. The significant difference in the number of implants required translates to reduced costs, reduced surgical time, reduced recovery time, and reduced overall treatment time in favor of All On 4 dental implants.\nThe major difference between All On 4 dental implants and conventional implants include:\n- Cost (initial)\n- Surgical scope\n- Recovery time\n- Required pre-existing bone depth']	['<urn:uuid:202954c9-3087-4985-b894-55e18d13c2fd>', '<urn:uuid:ea322958-9be0-46fb-bb75-6f483ff0b239>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T05:06:54.527160	31	62	1822
43	san andreas fault creep rate vs earthquake hazard modeling ucerf3 comparison	In the creeping section of the San Andreas Fault between Los Angeles and San Francisco, the fault creeps at about 20 millimeters per year. The UCERF3 model accounts for this type of fault behavior in its hazard assessment, showing that compared to previous models, the likelihood of moderate-sized earthquakes (magnitude 6.5 to 7.5) is lower, while the likelihood of larger events is higher. This is particularly important because even creeping faults can accumulate some stress and potentially generate large earthquakes.	"['- About SCEC\n- Learn & Prepare\nHome / Third Uniform California Earthquake Rupture Forecast (UCERF3)\nThird Uniform California Earthquake Rupture Forecast (UCERF3)\n|Documents and Resources|\nWith innovations, fresh data, and lessons learned from recent earthquakes, scientists have developed a new earthquake forecast model for California, a region under constant threat from potentially damaging events. The new model, referred to as the third Uniform California Earthquake Rupture Forecast, or ""UCERF3"" (http://www.WGCEP.org/UCERF3), provides authoritative estimates of the magnitude, location, and likelihood of earthquake fault rupture throughout the state. Overall the results confirm previous findings, but with some significant changes because of model improvements. For example, compared to the previous forecast (UCERF2), the likelihood of moderate-sized earthquakes (magnitude 6.5 to 7.5) is lower, whereas that of larger events is higher. This is because of the inclusion of multifault ruptures, where earthquakes are no longer confined to separate, individual faults, but can occasionally rupture multiple faults simultaneously. The public-safety implications of this and other model improvements depend on several factors, including site location and type of structure (for example, family dwelling compared to a long-span bridge). Building codes, earthquake insurance products, emergency plans, and other risk-mitigation efforts will be updated accordingly. This model also serves as a reminder that damaging earthquakes are inevitable for California. Fortunately, there are many simple steps residents can take to protect lives and property.\n|This research was supported by:|\nWhat is UCERF3?\n- The Uniform California Earthquake Rupture Forecast, Version 3 (UCERF3) is a comprehensive model of earthquake occurrence for California. It represents the best available science for authoritative estimates of the magnitude, location, and likelihood of potentially damaging earthquakes in California.\n- Two kinds of scientific models inform decisions of how to safeguard against earthquake losses: (1) an earthquake rupture forecast tells us where and when the Earth might slip along the state’s many faults, and (2) a ground motion prediction model estimates the ensuing shaking from a fault rupture. UCERF3 is the latest earthquake rupture forecast for California.\nWho developed UCERF3?\n- UCERF3 was developed by the 2014 Working Group on California Earthquake Probabilities (WGCEP), a multi-disciplinary collaboration of leading experts in seismology, geology, geodesy, paleoseismology, earthquake physics, and earthquake engineering. The development of UCERF3 involved numerous public meetings and included formal evaluations by five separate review panels.\n- The study was led by the U.S. Geological Survey, the Southern California Earthquake Center, and the California Geological Survey, with partial financial support from the California Earthquake Authority, which is the largest provider of homeowner earthquake insurance in California.\n- The development of earthquake rupture forecasts by the WGCEP (in 1988, 1990, 1995, 2003, and 2007) shows progress towards more accurate representations of the very complex California fault system.\nWhat is improved or innovative about UCERF3?\n- UCERF3 includes more than 350 fault sections, compared to the 16 and 200 faults considered by the WGCEP in 1988 and 2007, respectively. As detailed geologic data about the history of earthquakes and locations of faults is limited in many parts of California, UCERF3 has incorporated analysis of the gradual movement of hundreds of locations throughout California using space-based geodesy (GPS data) in order to estimate rates of deformation for faults lacking geologic data.\n- Recent events demonstrate that earthquakes can rupture beyond previously understood fault boundaries, resulting in a much larger fault-rupture area and magnitude than expected (e.g., 2010 M7.2 El Mayor-Cucapah; 2011 M9 Tohoku, Japan; 2012 M8.6 Sumatra). Scientists now recognize that faults are not isolated, but instead are often interconnected in a broad network or “fault system”, where multiple nearby faults can occasionally rupture simultaneously. Unlike previous models, UCERF3 accounts for the possibility of these multi-fault ruptures.\n- A “grand inversion” was developed for UCERF3 to solve the rate of all possible earthquake ruptures in the interconnected California fault system. This new system-level approach also draws upon a broader range of observations to arrive at the solution.\n- By employing the “grand inversion” technique, the resulting UCERF3 forecast rate for moderate-sized earthquakes (magnitude 6.5 to 7.5) is more in line with the observed rate in nature. All previous earthquake rupture forecast studies for California estimated a much higher rate for these moderate-sized events than compared to the observed rate.\n- Calculating the UCERF3 “grand inversion” results required the use of supercomputers to cover a broad range of models (>5,000), each considering more than 250,000 fault-based earthquake possibilities (including multi-fault ruptures) throughout California. The same calculation would take more than 8 years on a single desktop computer.\n- After a fault ruptures, it takes time for tectonic stress to re-accumulate on that fault – a concept known as Reid’s elastic rebound theory (Reid, 1911). UCERF3 includes this notion of fault “readiness” in the earthquake rupture forecast. Faults are more likely to rupture (more ready) where tectonic stress has built up over many years without an earthquake. Conversely, faults are less likely to rupture (less ready) when and where a recent earthquake has occurred.\n- We may not know when the most recent earthquake occurred on most faults in California, but we can assume it was before reliable recordkeeping began (in 1875). By accounting for this “historic open interval” for events prior to 1875, we can quantify fault readiness throughout the entire California fault system, unlike previous studies that were limited to only a subset of faults with well-known histories.\nWhat are the major findings of UCERF3?\n- UCERF3 confirms many previous findings, but it sheds new light on how future earthquakes will be distributed across California and how big those earthquakes might be.\n- Greater California. UCERF3 shows the likelihood of moderate-sized earthquakes (magnitude 6.5 to 7.5) is lower, while the likelihood of larger events is higher when compared to the UCERF2 results.\n- This is because UCERF3 includes the possibility that multiple nearby faults may occasional rupture together to cause larger earthquakes. The increased rate of these larger earthquakes must be balanced by a decreased rate of lower-magnitude events in order to maintain the overall plate tectonic, fault system budget.\n- The estimated rate of earthquakes around magnitude 6.7 (the size of the destructive 1994 Northridge earthquake) has gone down by about 30 percent. The expected frequency of such events statewide has dropped from an average of one per 4.8 years (in UCERF2) to about one per 6.3 years (in UCERF3).\n- The likelihood that California will experience a magnitude 8 or larger earthquake in the next 30 years increased from about 4.7% in UCERF2 to about 7% in UCERF3.\n- Southern California / Los Angeles Region.\n- The UCERF3 model estimates a greater increase in the likelihood of larger earthquakes in the Los Angeles Region compared to most of California, because the region has more faults that can host multi-fault ruptures.\n- UCERF3 concurs with previous studies that consider the Southern San Andreas Fault the most likely to host a large earthquake.\n- Compared to UCERF2, the likelihood of M≥6.7 earthquakes on the San Jacinto Fault decreases three-fold in UCERF3, but is balanced by an equivalent increase in likelihood of M≥8 earthquakes on that fault.\n- Northern California / San Francisco Region.\n- The Northern San Andreas Fault has a lower likelihood of hosting an earthquake (compared to the Southern San Andreas) partly because of the relatively recent 1906 earthquake on that fault.\n- The Hayward-Rodgers Creek and Calaveras Faults are more likely to rupture (compared to the Northern San Andreas) because it has been a long time since the last earthquakes occurred on these faults.\n- Compared to UCERF2, the Calaveras Fault shows a three-fold increase in M≥6.7 earthquake likelihoods, but no compensating decrease in rate at higher magnitudes. This is because most events on the Calaveras in UCERF2 were well below magnitude 6.7, so the inclusion of multi-fault ruptures in UCERF3 increases the likelihood of all M≥6.7 earthquakes.\nHow is UCERF3 used?\n- UCERF3 can be combined with ground motion prediction models to ascertain the seismic hazard, or ground shaking generated by each possible earthquake rupture. These seismic hazard estimates, coupled with engineering models of the built environment, are used to ascertain seismic risk, or the probability that humans will incur loss or damage to their built environment if exposed to a seismic hazard.\n- UCERF3 has been used for the 2014 update of the U.S. Geological Survey National Seismic Hazard Maps.\n- The California Earthquake Authority will use UCERF3 to evaluate insurance premiums charged to customers, as well as their own level of reinsurance.\n- UCERF3 will be used for other risk mitigation efforts for years to come, including engineering design of building and lifelines, loss estimation for catastrophic bonds and other risk-linked securities, and emergency preparedness – all to increase public safety and community resilience.\nWhat are the limitations of UCERF3?\n- Although UCERF3 is a significant improvement over the previous model (UCERF2), it is still an approximation of the natural system. For example, it does not model the earthquake-triggering process that produces aftershocks, even though we know such events can be large and damaging.\n- It is difficult to make generalizations about the hazard or risk implications of UCERF3 without first specifying both asset types and their locations. Conclusions will vary depending on whether you are designing a single family dwelling, retrofitting a bridge, considering the location of a power plant, laying pipeline across the San Andreas Fault, or considering aggregate losses over a large insurance portfolio. The practical implications will need to be considered on a case-by-case basis.\nHow can we prepare for future earthquakes?\n- Through the National Earthquake Hazard Reduction Program (www.nehrp.gov), the U.S. Geological Survey and its partners will continue to conduct research aimed at improving our understanding of fault behavior and estimates of earthquake hazard in the future.\n- UCERF3 should also serve as a reminder that California is earthquake country, and residents should always be prepared. Simple safeguards include practicing “drop, cover, and hold on,” securing items in your home and workplace that could fall during an earthquake, and storing seven days’ worth of food and water. Homeowners can also consider structural retrofits, such as bolting the house to its foundation, as well as earthquake insurance options. For further guidance on how to prepare for, survive, and recover after big earthquakes, follow the Seven Steps to Earthquake Safety (www.earthquakecountry.org/sevensteps).\nWhere can I find additional information about UCERF3?\n|San Francisco Bay Area\nLeslie C. Gordon\nGeologist/Public Affairs Specialist\nU.S. Geological Survey (Menlo Park)\nDirector of Communication, Education & Outreach\nSouthern California Earthquake Center (Los Angeles)\nPublic Information Officer\nCalifornia Geological Survey (Sacramento)\n|Greater Denver Area\nPublic Affairs Specialist\nU.S. Geological Survey (Golden)', 'Between Los Angeles and San Francisco, the San Andreas Fault releases stress by gradual movement. Scientists calculate fault slip from offset topography and discuss seismic hazard.\nBy Chelsea Scott, Ph.D., Assistant Research Scientist, Arizona State University (@ChelseaPScott)\nCitation: Scott, C., 2021, The Central San Andreas creeps along without a major earthquake, Temblor, http://doi.org/10.32858/temblor.152\nThe San Andreas Fault in California is well known for generating large and damaging earthquakes — including the 1906 magnitude 7.8 San Francisco earthquake. Perhaps less well known, is the fact that some portions of the San Andreas Fault rarely, if ever, rupture in large earthquakes. Between Los Angeles and San Francisco, for example, a portion of the San Andreas Fault continuously slips, or “creeps” at a rate of approximately three-quarters of an inch (20 millimeters) per year. A creeping fault can generate small earthquakes, but shaking is generally imperceptible or felt only near the fault. Occasionally, a creeping fault can suddenly slip in a large earthquake.\nIn a new study published in Geophysical Research Letters, my colleagues and I show where and how much this fault is creeping. We use airborne instruments to measure changes in topography that are too small and occur too slowly for our eyes to see. These new observations reveal the active trace of the creeping San Andreas Fault and show the extent to which the active motion hugs a narrow fault trace. These constraints are critical for scientists to understand the processes that control fault creep in the crust and to determine seismic hazard.\nCreeping faults are a hazard\nBecause creeping faults slip gradually and nearly continuously, they do not build up as much stress as locked faults, which slip infrequently and release a majority of stress in large earthquakes. However, creeping faults do build some stress, which is often not fully released, meaning that these faults can host large earthquakes and pose a hazard. Measuring fault activity or the creep rate is essential for knowing how much stress is built-up along these faults and the likelihood of a moderate to large earthquake in the future.\nMeasuring fault motion from a shifting landscape\nMeasuring a fault slip rate is challenging. Offset curbs like the one pictured above in Hollister, California, give scientists a way to directly observe how much slip has occurred in a given location, but these types of structures do not cross the fault at the spacing required to accurately measure large-scale fault motion. There are a variety of approaches for measuring fault slip rate, but most do not capture a spatially dense slip rate that is critical to assessing the fault’s overall activity.\nLight detection and ranging, known as lidar, is a technique used to create a three-dimensional representation of a target based on a laser signal. Although well-known for its use in self-driving cars, geologists use this technique to measure elevation across a landscape. When the elevation surrounding a fault is measured at two different times — often months to years apart — comparing the two topography datasets reveals how the has fault moved (Nissen et al., 2012; Oskin et al., 2012).\nMy colleagues and I were the first to measure the fault creep rate along the entire creeping portion of the Central San Andreas Fault from airborne lidar datasets. This work resulted in the most spatially dense slip rate measurements along the fault available. We measured the right-lateral creep rate every 1300 feet (400 meters) along the fault from airborne lidar topography datasets acquired just over a decade apart. Applying this method to a creeping fault was challenging because the decade of fault creep produced a relatively low amount of slip relative to the noise in the data. To decrease the noise and better resolve the creep, we removed vegetation from the lidar data because changes in vegetation contribute significantly to the noise.\nNot all tectonic motion occurs along a discrete fault\nThe motion between two sides of a fault can either be localized to a narrow surface or distributed over a broader region in the Earth’s shallow crust. Research on earthquakes has shown that older faults that have accommodated more slip tend to localize motion to a narrow fault. The San Andreas Fault is very old and should localize roughly 90% of the motion to the fault trace (Dolan & Haravitch, 2014). However, we show that only 50-80% of the motion along the creeping San Andreas Fault occurs on a narrow fault trace. We propose that this unexpected behavior reflects the much slower slip rate during creep events relative to earthquakes.\nWhich fault is active?\nOur results show a picture of motion within a mile surrounding the fault trace that allows scientists to clearly see which faults in the Earth’s shallow crust accommodate motion. In the right figure above, we trace out the active San Andreas fault in black based on the fault’s motion over the past decade. This active fault trace is approximately half of a mile away from where the US Geological Survey mapped the fault based on the shape of hills and valleys that indicate an older fault location. The different fault locations show that the active trace of the San Andreas fault has shifted over time. Critically, our result reveals the location of the fault active over the past decade, which is an important input into seismic hazard assessment.\nThe stress buildup along a fault depends on the difference between the long-term tectonic plate boundary rate and the creep rate. One of the major challenges in measuring the seismic risk is knowing how fault slip rates change from the surface where they can be measured to the deeper portions of the fault where slip rates are much more challenging to measure. To infer the slip rate at depth, scientists look at surface motion over a larger area, which indicates deeper fault slip. These displacements measure fault slip at a scale between very near-field and far-field instruments. In the future, we plan to use surface motion measurements over a range of areas to infer fault slip over the entire subsurface extent of the fault. This analysis will indicate the stress buildup and the likelihood of a future earthquake along the fault.\nDolan, J. F., & Haravitch, B. D. (2014). How well do surface slip measurements track slip at depth in large strike-slip earthquakes? The importance of fault structural maturity in controlling on-fault slip versus off-fault surface deformation. Earth and Planetary Science Letters, 388, 38–47. https://doi.org/10.1016/j.epsl.2013.11.043\nEarthScope (2008). EarthScope Northern California LiDAR Project. NSF OpenTopography Facility. https://doi.org/10.5069/G9057CV2\nNissen, E., Krishnan, A. K., Arrowsmith, J. R., & Saripalli, S. (2012). Three-dimensional surface displacements and rotations from differencing pre- and post-earthquake LiDAR point clouds. Geophysical Research Letters, 39(16). https://doi.org/10.1029/2012GL052460\nOskin, M. E., Arrowsmith, J. R., Corona, A. H., Elliott, A. J., Fletcher, J. M., Fielding, E. J., et al. (2012). Near-Field Deformation from the El Mayor-Cucapah Earthquake Revealed by Differential LIDAR. Science, 335(6069), 702–705. https://doi.org/10.1126/science.1213778\nScott, C. P., DeLong, S. B., & Arrowsmith, J. R. (2020). Distribution of Aseismic Deformation Along the Central San Andreas and Calaveras Faults from Differencing Repeat Airborne Lidar. Geophysical Research Letters. https://doi.org/10.1029/2020GL090628\n- Latest Philippine earthquake reveals tectonic complexity - July 27, 2021\n- Injecting CO2 into depleted oil fields may not cause quakes - July 26, 2021\n- Here’s what high-flying balloons can tell us about earthquakes - July 22, 2021']"	['<urn:uuid:f0bf24b2-cab5-4bc7-9f3b-902e187e421c>', '<urn:uuid:f81d5466-5c92-4fdf-8fa6-e98f0cab922d>']	factoid	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-13T05:06:54.527160	11	80	2988
44	machinability rating comparison a572 hardox 450	A572 steel has machinability ratings varying by grade, with Grade 42 having better machinability than higher grades. In comparison, Hardox 450 with its 450 HBW hardness and martensitic structure requires more careful machining. Standard machining methods like HSS or cemented carbide tools can be used for both steels, but Hardox 450 may require adjusted cutting parameters due to its higher hardness.	"[""A high-strength low-alloy steel plate that is utilized in a variety of structural applications.This specification is produced in Grades 42,50,55,60 65,the grades representing the yield strength.Plates that are 4 and thicker are made to a 42 KSI yield,although material can be modified to reach the 50 KSI min yield. results for this questionWhat are the mechanical properties of A572?What are the mechanical properties of A572?Mechanical Properties A572 Gr 65 Tensile 80 ksi min.Yield 65 ksi min.Elongation 12% min in 8 14% min in 2 (for plates wider than 24)A572 Structural,Carbon HSLA Steel Plate - Chapel Steel\nASTM A572 Carbon Steel Plate.ASTM A572 Specification refers to High-Strength Low-Alloy Steel Plate for structural building,among the 5 steel grades of 42,50,55,60 and 65,Grade 50 is one most common steel in applications where need more strength and less weight,use steel plate of this grade other than ordinary carbon steel will make the steel structure with same steel strength but A36 vs A572 machinability - Machines Machining Apr 05,2007·Ed has got it.572 has a tensile strength of about 10-15% higher than a36.This will allow better cuts,nice curled chips.Run more feed and less speed in gummy materials.But this 1045 steel will probably machine the best of the three.john\nASTM A572 Grade 42|A572 Gr.42|A BBNSTEEL is specialized in supplying ASTM steel plate in A572Gr42A572 Gr.42A572 Grade 42 cutting parts,ASTM A572 Grade 60,A572 Gr.60,A BBNSTEEL is specialized in supplying ASTM steel plate in ASTM A572Gr60 stock steel plates,A572 Gr.60 cutting p ; ASTM A572 Grade 65,A572 Gr.65,AA516GR70N Cutting Parts|S355J2+N Machining parts_Steel ASTM A572 Grade 42|A572 Gr.42|A BBNSTEEL is specialized in supplying ASTM steel plate in A572Gr42A572 Gr.42A572 Grade 42 cutting parts,ASTM A572 Grade 60,A572 Gr.60,A BBNSTEEL is specialized in supplying ASTM steel plate in ASTM A572Gr60 stock steel plates,A572 Gr.60 cutting p ; ASTM A572 Grade 65,A572 Gr.65,A\nBeing one of the noted manufacturer,stockists and suppliers,weoffer A572 Carbon Steel Round Bar.Available at economical rates,the offered A572 Carbon Steel Round Bar are widely used in metallurgical,mechanical,electrical construction field,ships,military uses and automobile rear of products.We are a prominent name in manufacturing and supplying a wide range of A572 Carbon Steel Round A572 Grade 42,50,55,60,65 Steel Plates A572 Steel Plate ASTM A572 is used in a variety of structural applications,and is available in five grades 42,50,55,60 65.Overview Popular steel plate grade used for a variety of structural applications including bridges,buildings,construction equipment,freight cars,machinery,truck parts and transmission towers.\nASTM A572 Grade 42/50 Carbon Steel Plates is first Bended,ASTM A572 Grade 42/50 Carbon Steel Plates then goes through the process called curling to get the form a edge on a ring.then ASTM A572 Grade 42/50 Carbon Steel Plates is pass through the process called Decambering so that it removes the camber.then ASTM A572 Grade 42/50 Carbon Steel Plates is stretched over a form of die this process is called Deep drawing.then ASTM A572 Grade 42/50 Carbon SteelAPPENDIX F GUIDELINES FOR WELDING STRUCTURALa.Welding A852 steel .C.AASHTO Guide Specification for Highway Bridge Fabrication with .HPS70W Steel.1.For welding HPS70W steel.II.Determine Weldability or Special Welding Concerns for the Base Metal.A.ASTM A7 (33 ksi min yield) 1.Carbon Steel Plates,Shapes and Bars .2.Found on most existing (1950's) NYSTA bridges .3.Weldability\nMay 30,2020·Both ASTM A36 carbon steel and ASTM A572 grade 50 are iron alloys.Both are furnished in the as-fabricated (no temper or treatment) condition.They have a very high 99% of their average alloy composition in common.For each property being compared,the top bar is ASTM A36 carbon steel and the bottom bar is ASTM A572 grade 50.ASTM A572 Grade50,55,60 HSLA Structural Steel Plate-AGICOASTM A572 Specification refers to High-Strength Low-Alloy Steel Plate for structural building,among the 5 steel grades of 42,50,55,60 and 65,Grade 50 is one most common steel in applications where need more strength and less weight,use steel plate of this grade other than ordinary carbon steel will make the steel structure with same steel strength but lighter weight.\nASTM A572 Steel,grade 42,High-strength low-alloy steel,structural quality The following are suggested consumables for arc welding processes For grades 42 and 50 - Manual Shielded Metal-Arc (AWS A5.5) Low Hydrogen Electrodes E7015,E7016,E7018ASTM Weathering Steel Specifications Central Steel ServiceASTM A242.ASTM A242 is a high strength low-alloy structural steel specification with improved atmospheric corrosion resistance.This specification is normally applicable to steel plates through one-half inch in thickness.The atmospheric corrosion resistance of A242 is substantially better than that of carbon steels such as A36 and A572-50 with or without copper addition.\nAbrasion resistant (AR) steel plate is a high-carbon alloy steel designed specifically to have greater hardness properties than low-carbon steel.Hardness typically comes at the expense of strength,making AR wear-resistant steel an ideal material for harsh,high-abrasion conditions,andCold Rolled 1018 vs A572 gr 50 - American Welding SocietyJan 31,2005·Hello All,I'm in the middle of purchasing materials for use in making built-up beams in which the end service is frames for a metal building.These frames are engineered to be made from carbon steel with a minimum yield strength of 50 KSI.We typically use A572 grade 50 and sometimes use A36 modified or A36 material that has a 50 KSI min yield or greater.\nMATERIAL Hardness BHN Machinability Rating Alloy Steels,Cast Material Grade Range Range Low Carbon 1320,2315,2320 150-300.52 - .32 4110,4120,4320,8020 MECHANICAL PROPERTIES GRADE 65 STEELmachine and the instruments used.After a general discussion of the mechanical properties of steel,results of fifty-two tension specimens from plates and shapes of A572 (Grade 65) Steel are summarized.This report constitutes the most complete study to date of the properties of higher grade of steel.The strain-hardeningrange of the\nCarbon steels 1015 72% 1018 78% 1020 72% 1022 78% 1030 70% 1040 64% 1042 64% 1050 54% 1095 42% 1117 91% 1137 72% 1141 70% 1141 annealed 81% Material selection and product - Steel ConstructionSteel material is supplied in two product forms flat products (steel plate and strip) and long products (rolled sections,either open beams,angles,etc or hollow sections).For structural use in bridges these products are inevitably cut (to size and shape) and welded,one component to another.In the structure,the material is subject to tensile and compressive forces.\nThe American Iron and Steel Institute (AISI) has determined AISI No.1112 carbon steel a machinability rating of 100%.Machinability of some common materials related to AISI No.1112 are indicated in the table below Carbon Steels.1015 - 72% ; 1018 - 78%; 1020 - 72%; 1022 - 78%; 1030 - 70%; 1040 - 64%; 1042 - 64%; 1050 - 54% ; 1095 - 42%; 1117 People also askWhat grade is A572 steel?What grade is A572 steel?A572 Steel Plate ASTM A572 is used in a variety of structural applications,and is available in five grades 42,50,55,60 65.ASTM A572 Grade 50 - A572 Grade 42,50,60,65 Steel\nUse this free tool to easily estimate the weight of carbon steel plates.Whether your plates are round or square,it will automatically account for plates with or without holes.Simply enter the dimensions and quantity to get the total weight.Contact us for a quick quote on fabricated plates for bolted applications.Related searches for steel a572 grade 42 jordan machininga572 42 steelsteel a572 grade 50astm a572 steel grade 50a572 grade 50 machinabilitya572 grade 50 steel suppliersa572 steel machinabilityastm a572 grade 50 vs a36 steelastm a572 steel\na572 42 steelsteel a572 grade 50astm a572 steel grade 50a572 grade 50 machinabilitya572 grade 50 steel suppliersa572 steel machinabilityastm a572 grade 50 vs a36 steelastm a572 steelPrevious123456NextASTM A572 Baden Steelbar Bolt Corp.Speedy Metals Information for ASTM A572 Grade 50 PlateASTM A572 Grade 50 is considered a workhorse grade and is widely used in many applications.Steel mills produce channel and heavy beams with Grade 50.It is commonly used in structural applications,heavy construction equipment,building structures,heavy duty anchoring systems,truck frames,poles,liners,conveyors,boom sections\nSteel Import / Export Statistics Steel Import / Export Statistics.Steel Trade Graphs Steel Trade Graphs.EU Quota Tracking EU Quota Tracking.Events Events Events .Events Events Our major market-leading conferences and events offer optimum networking opportunities to all participants while adding great value to their business.Steel TV Steel TVSteel Plate Sheet - Alro SteelSteel Plate and Sheet ASTM A-36 Hot Rolled Plate A-36 is a structural quality carbon steel used in a variety of general construction applications including; bolted,riveted,or welded construction of bridges and buildings.Typical Analysis* ASTM A-36 Carbon (C) max % 0.25 0.25 0.26 0.27 0.29\nA572-42 A572-50 A572 grade 50 steel plate is a high-strength,low-alloy plate,specified for bridges,higher strength structural duties,and light pressure vessel requirements.Recommended for riveted,bolted,or welded structures,such as Gussets and Bridge Plates,when intended for construction of bridges.At 50ksi,a higher strength steel than A36.Steel Plate Grades - Continental Steel Tube CompanySizes 3/4-1 1/2 the carbon range is (0.15-0.25) which gives improved machining,while retaining its forming and welding properties.Plates over 1 1/2 have carbon increasing to (0.20-0.33),this is a killed steel that provides the best combination of strength,weldability,and structural soundness.MEDIUM CARBON PLATE STEEL\nSizes 3/4-1 1/2 the carbon range is (0.15-0.25) which gives improved machining,while retaining its forming and welding properties.Plates over 1 1/2 have carbon increasing to (0.20-0.33),this is a killed steel that provides the best combination of strength,weldability,and structural soundness.MEDIUM CARBON PLATE STEELSteel Round Bar - RyersonSteel Round Bar.Ryersons carbon steel round bar offering includes both Special Bar Quality (SBQ) and Merchant Bar Quality ( MBQ).SBQ grades include 1018,1045,1117,1141,1144,1215,12L14,Stressproof and Fatigueproof.\nChemical composition A36 - 04b A36 0.26 0.05 0.04 0.40 A572 - 04 Grade 42 0.21 1.35 0.05 0.04 0.40 Grade 50 0.23 1.35 0.05 0.04 0.40Steel grades according to American standards - A36,Chemical composition A36 - 04b A36 0.26 0.05 0.04 0.40 A572 - 04 Grade 42 0.21 1.35 0.05 0.04 0.40 Grade 50 0.23 1.35 0.05 0.04 0.40\nThe A588 A572 Steel Company is a long established service center offering the best quality system to provide completely mill traceable A588 and A572 grade of domesticangles,channels,square bars,flat bars,round bars,beams,plates and sheets.We offer over 50Turning inserts and grades - Sandvik CoromantTurning inserts and grades.Sandvik Coromant insert grade developments for turning have pioneered the manufacturing world for the last 70 years,and we at Sandvik Coromant promise to continue bringing you cutting edges that keep you ahead of the competition.\nThe ASTM A572 specification is the Standard Specification for High-Strength Low-Alloy Columbium-Vanadium Structural Steel for plates used in general construction and structural applications.ASTM A572 includes five grades with specified minimum yield strengths of 42,50,55,60,and 65 ksi,respectively.For applications where notch toughness is important,consult SSAB for specific Charpy VWorld Material - Free Online Material Information ResourceASTM A572 Grade 50 Steel ASTM A572 grade 50 steel is a low-alloy high-strength structural steel with a yield strength of 50 ksi (345 MPa) and a tensile strength of 65 ksi (450 MPa).It has better mechanical properties than ASTM A36 steel.\nPlease Click here"", ""In the article,the structure and change in hardness of the welded Hardox 400 and Hardox 500 steels have been presented.It has been shown that structures of lower wear resistance are being created as a result of welding those materials in the as delivered state (i.e.with the tempered martensite structure) within the heat-affected zones. results for this questionFeedbackHS 450 - HIGH STRENGTH - High Strength and Abrasion steel is an innovative solution for mining,transportation,forestry and construction applications.HS 450 steel is weldable using standard SMAW (stick),SAW,GMAW (MIG),FCAW,or GTAW (TIG) methods and low hydrogen practices.Preheat temperature depends on plate thickness.HS 450 steel requires a minimum top round die radius of 5T (where T is the thickness of the plate) results for this questionWhat ' s The difference between Hardox 500 and Hardox 600?What ' s The difference between Hardox 500 and Hardox 600?Hardox 450 Hardox 500 Hardox 550 Hardox 600 Hardox Extreme Strenx performance steel and Hardox wear plate are steel grades that can be machined with high speed steel (HSS) or cemented carbide (CC) tools.This brochure includes our suggestions for cutting data (feeds and speeds) and the selection of Machining recommendations for Strenx and Hardox\nApplication of the High Strength Steel HARDOX 450 for Manufacturing of Assemblies in the Military Industry Download full-text PDF Read and prescribing the technology for welding the high\nDownload Free PDF.Download Free PDF.STRENX,HARDOX AND DOCOL -BENDING OF HIGH STRENGTH STEEL 860 Docol 1000 DP 1090 Docol 1200 M 1300 Docol 1300 M 1400 Docol 1400 M 1510 Docol 1500 M 1600 Hardox 400 1250 Hardox 450 1400 Hardox 500 1650 TABLE 11Typical tensile strength values to calculate bend force.P = Bend force,tons (metric) t = Plate 168 Hardox 450 uk structural 20140618 - mtlHardox 450 is an abrasion resistant steel with a nominal hardness of 450 HBW.Typical applications are components and structures subject to wear.For more information on applications see ssab.Available dimensions Hardox 450 and Hardox 450 Tuf are available in thicknesses of 3-130 mm.Both grades are available in widths up to 3350 mm and lengths up to 14630 mm.AR400 plate - American Welding SocietyFeb 12,2013Welding Abrasion Resistant SteelAug 06,2008See more results012 Bending UK - AE MachHigh strength steels can also be sheared.As a general rule,the higher the tensile strength,the higher the shearing force needed.Tool wear also increases with increasing tensile strength,and shearing WELDOX 1100,HARDOX 450 and higher strength steels is therefore inadvisable.Satisfactory results in shearing high strength plate pre\nAR450 Steel is a type of abrasion resistant steel that has a surface hardness of 420-470 Hardness Brinell.It is a high-carbon steel alloy composed of several different elements,such as carbon and boron.Its unique composition and treatment allow it to provide more surface hardness than AR400,while maintaining good ductility,formability,and impact resistance.Abrasion Resistant SSAB Plates - csteelindiaConquest Steel and Alloys is India Based Leading Supplier and Stockist of High Quality and Speciality Steel Such as Wear Resistant Steel&SSAB Plates,SSAB Abrasion Resistant Steel Plates,BHN SSAB PLATES,HB SSAB Plates,SSAB High Strength Plates Available in Ready Stock of Wear Resistant Steel SSAB Hot Rolled Steel Plates Exporter located in Mumbai,India.Abrasion-Resistant Steel Properties and ApplicationsAR400 vs.Ar450 vs.AR500 Understanding Abrasion-Resistant SteelWhat Is Abrasion-Resistant Steel Plate?How The Quenched and Tempered Process Creates AR PlateWhats with The F?What Is Through-Hardening?AR400 Versus Ar450 Versus Ar500+In fabrication and construction,the composition and grade of the steel plate materials used have a tremendous effect on the end product.Abrasion-resistant steel plate is a normal steel plate that possesses a tougher,harder quality that lasts about four times longer when compared to a common high-strength structural steel plate.However,what makes it tougher,and how doe you know when a project needs abrasion-resistant steelSee more on azomPublished ASTM Weathering Steel Specifications Central Steel ServiceASTM A242.ASTM A242 is a high strength low-alloy structural steel specification with improved atmospheric corrosion resistance.This specification is normally applicable to steel plates through one-half inch in thickness.The atmospheric corrosion resistance of A242 is substantially better than that of carbon steels such as A36 and A572-50 with or without steel addition.\nThe paper presents an analysis of possibilities and prescribing the technology for welding the high strength steel HARDOX 450.The methodology for estimate the weldability of this steel was established in the theoretical part of the paper,as well as calculations of the welding parameters,while the hardness was investigated in details and the macro and micro structures of the individual welded joint zonesCited by 3Publish Year 2017Author Vuki Lazi,Duan Arsi,Ruica R.Nikoli,Dejan Djordjevi,Radica Proki-Cvetkovi,Olivera Popo(PDF) A REVIEW OF HIGH-STRENGTH WEAR-RESISTANTThe aim of this work is to present the most important information about Hardox-the high-strength wear-resistant martensitic steel.This type of steel cuts down on weight and extends the service DATA SHEET Version Hardox 450 - High Strength PlatesHardox 450 is an abrasion resistant plate with a hardness of 450 HBW,intended for applications where demands are imposed on abrasion resistance in combination with good cold bending properties.Hardox 450 offers very good weldability.\nData Sheet HARDOX 400 ABRASION RESISTANT PLATE HARDOX 400 is an abrasion resistant plate with a hardness of 400 HBW,intended for applications where demands are imposed on abrasion resistance in combination with good cold bending properties.HARDOX 400 offers very good weldability.File Size 100KBPage Count 2High-Strength Low-Alloy Steels - ASM Internationalbeams and panels are additional uses of these steels.The choice of a specific high-strength steel depends on a number of application requirements including thickness reduction,corrosion resist-ance,formability,and weldability.For many applications,the most impor-tant factor in the steel selection process is the favorable strength-to-weightFile Size 116KBPage Count 2Relia&400,450,500 IndusteelRelia&400,450,500 Quality Wear Resistant Plates.Relia&is a range of high hardness,conventional low-alloyed martensitic steels,which obtain their hardness through intense water quenching during plate manufacturing.Relia&steel plates offer outstanding resistance to abrasion typically 3 to 6 times higher than 355MPa-class construction steel,although the actual performance may vary\nHardox&450.A premium and trademarked brand of formable abrasion resistant plates produced by SSAB with a nominal Brinell hardness of 450.Hardox&450 steel is intended for applications where demands are imposed on abrasion resistance with good cold bending applications.Hardox&450 is supplied in a primed and blasted condition,therefore having a superior surface condition whenFile Size 372KBPage Count 3General Product Information Weldox,Hardox,ArmoxHardox,Weldox,Armox and Toolox are registered trademarks that are owned by SSAB Technology AB.600 500 450 400 550 350 300 400 450 500 600 600T 560T 500T 440T 370T HiTuf 350 550 Hardox wear plate Armox protection plate HBW Commercial and high strength plate 250 500 750 1000 1250 Re MPa (N/mm2) 700 900 960 1100 1300 Weldox high strength steel File Size 903KBPage Count 24DOCOL,DOmex,HarDOx anD WeLDOx - Bending ofThis brochure deals with bending of high strength steel for the trademarks Hardox,Weldox,Domex and Docol.The content is intended as a guide,and contains general suggestions for how to achieve the best results in bending.Bending high strength steel sheet and plate rarely proves to be difficult,\nHardox 400 is an all-around wear resistant steel.Thanks to its high toughness,good bendability and weldability,this steel can be used in structures with moderate wear.Dimension Range Hardox 400 Sheet is available in thicknesses between 2 - 8 mm.Hardox 400 Sheet is available in widths up to 1750 mm and lengths up to 16000 mm.HARDOX 450 SHEET - oblibeneThe products can be used in many different components and structures that are subject to wear.Hardox 450,with an extra 50 Brinell hardness over our 400 grade,provides better dent and abrasion resistance as well as longer wear life,so you can achieve even greater savings.Dimension Range Hardox 450 Sheet is available in thicknesses between 2.5 - 8 mm.Hardox 450 Sheet is available in widths up toHARDOX- Processing of wear plates at RimeCharacteristics Hardox Wear Plate is characterized by an even flatness,high hardness and high strength.Due to these important properties,thinner plate thicknesses can be processed for the manufacture of components made of wear plates with higher hardness levels.The weight of the constructions made from them can be reduced by a significant amount.\nMar 30,2006·These joints are full pen.butt welds (hardox 450 to 450) with a carbon steel backing bar and other assembly fillet welds.Yes,the weld is way under the tensile of the 450 plate.But in this application,the welds never sees such stress anyway.Hardox 450 - The most popular structural wear plate - SSABHardox&450 combines good bendability and weldability with an option for guaranteed impact toughness (Hardox&450 Tuf).The products can be used in many different components and structures that are subject to wear.Hardox &450,with an extra 50 Brinell hardness over our 400 grade,provides better dent and abrasion resistance as well as longer wear life,so you can achieve even greater savings.Hardox Steel Providers MTL AdvancedHardox 400,Hardox 450,Hardox 500,Hardox 550,Hardox 600,Hardox HiTuf, The UKs leading supplier to original equipment manufacturers end users of Hardox wear plate and high strength steel.Learn More > Rail.Providing train,freight wagon and infrastructure fabrications to the rail industry.\nSSAB has continuously improved Hardox®,adding new specialty grades,and making new stock shapes available for purchase.And while it was designed for abrasion resistance,Hardox&has also become known for its toughness,lightness,and yield strength,making it appropriate for an array of applications from modular containers to truck parts.Hardox Wearparts Fabrication - G.E.MATHIS COMPANYHardox is a wear-resistant steel,manufactured and exported worldwide by SSAB,a highly specialized global steel company that develops high-strength steels for better performance and sustainability.Hardox wear-resistant components feature high-strength steel construction that offers a proprietary combination of hardness and toughness,rendering them exceptionally wear and impactHardox&HiAce - SSAB high-strength steelIt features the same excellent properties as Hardox &450,with a Brinell hardness of 425 - 475 HBW and a minimum Charpy impact test value of 27 J at -20°C (20 ft-lb at -4°F).Hardox &HiAce is a true wear fighter,especially in acidic corrosive environments that threaten to eat away at your equipment.This corrosion-resistant steel plate helps to meet the challenges of corrosive wear environments found in\nWhy Use Hardox,Strenx,Domex Or Laser?StrenxDomex Mechanical Properties ApplicationsSSAB Laser Mechanical Properties ApplicationsSpecial steels have a vital role in manufacturing.Where durability,wear resistance or bending qualities are essential,those metals come to the rescue.Hardox,Strenx,Domex and Laser are all SSABs steel grades that fill those needs.You may think that a thicker construction steel does the job when strength is needed.Actually,calculations show it does.But the construction will weigh more.SSABs homepage says that you can reducSee more on fractoryWelding Hardox and Weldox - AE MachThe extreme performance of Weldox high strength steel and Hardox wear plate is combined with exceptional weldability.Any conventional welding method can be used for welding these steels to any type of weldable steel.This brochure is aimed at simplifying,improving and boosting the efficiency of the welding process.Impact Resistance of High Strength Steels - ScienceDirectJan 01,2016·It is a factor deciding,among others,about the use of fuel,or the size of the transported cargo.Extra high strength steels are used in structures such as truck chassis,cranes and excavators.In these applications,high strength steel is used in order to reduce weight while * Magdalena Mazur Tel.+48 034 3250399; fax +48 034 3613 876.Machining recommendations for Strenx and HardoxHardox HiTuf Hardox 400 Hardox 450 Hardox 500 Hardox 550 Hardox 600 Hardox Extreme Strenx performance steel and Hardox wear plate are steel grades that can be machined with high speed steel (HSS) or cemented carbide (CC) tools.This brochure includes our suggestions for cutting data (feeds and speeds) and the selection of tools.\nSSAB,Swedish steel manufacturer of high-strength wear and structural steel.Hot and cold rolled,sheets,plates and coils,steel tubes and profiles,steel piles and more. Hardox HiAce Hardox HiTuf Hardox 400 Hardox 450 Hardox 500 Hardox 500 Tuf Hardox 550 Hardox 600 Hardox Extreme Hardox Round Bars Hardox Tubes and Pipes Hardox HiTemp.People also askWhat do you need to know about HARDOX 450 steel?What do you need to know about HARDOX 450 steel?Hardox&450 steel is intended for applications where demands are imposed on abrasion resistance with good cold bending applications.Hardox&450 is supplied in a primed and blasted condition,therefore having a superior surface condition when compared to other abrasion resistant plate products in the marketplace.Hardox 450 Steel Supplier Central Steel ServiceREFERENCE CATALOGUE - Brunswick SteelYield Strength 130,000 PSI minimum HARDOX 450 Wear plate manufactured by SSAB intended for applications requiring high abrasion resistance.Due to its high toughness,bending and welding properties,it can also be used as a load bearing plate.This replaces similar products produced by various mills under a variety of names including""]"	['<urn:uuid:f7da809b-c538-4cc1-bad5-ccaa30b4c341>', '<urn:uuid:09da9156-ac8e-40a1-a135-19d72d055cc2>']	factoid	direct	short-search-query	distant-from-document	three-doc	expert	2025-05-13T05:06:54.527160	6	61	3743
45	What diagnostic tests determine tennis elbow severity, and which self-treatment methods provide immediate relief?	Diagnosis involves identifying pain with one finger on the outer elbow prominence, which worsens when extending the wrist against resistance. Additional tests may include X-rays to rule out arthritis and MRI if neck problems are suspected. For immediate relief, several self-treatment methods are effective: applying ice wrapped in a thin towel for 20 minutes several times daily, taking over-the-counter anti-inflammatory medications, and using compression braces that fit snugly but not too tightly.	['Tennis elbow is an inflammation of the tendons that join the forearm muscles on the outside of the elbow. The forearm muscles and tendons become damaged from overuse — repeating the same motions again and again. This leads to pain and tenderness on the outside of the elbow. Tennis Elbow is a common cause of lateral sided elbow pain. Contrary to the given name, many sufferers of tennis elbow do not necessarily play tennis.\nThe diagnosis is usually straight forward. The area of the tennis elbow pain can be identified with one finger and it is usually on the outer bony prominence of the elbow. This pain is worsened by extending the wrist against resistance with the elbow straight.The person suffering from tennis elbow will have difficulty lifting heavy loads and many simple activities of daily living can potentially trigger the pain as the wrist joint usually need to be extended during most activities.\nThe symptoms of tennis elbow develop gradually. In most cases, the pain begins as mild and slowly worsens over weeks and months. There is usually no specific injury associated with the start of symptoms.\nCommon signs and symptoms of tennis elbow include:\n- Pain or burning on the outer part of your elbow\n- Weak grip strength\nThe symptoms are often worsened with forearm activity, such as holding a racquet, turning a wrench, or shaking hands. Your dominant arm is most often affected; however both arms can be affected.\nYour doctor may recommend additional tests to rule out other causes of your problem.\nThese may be taken to rule out arthritis of the elbow.\nMagnetic Resonance Imaging (MRI)\nIf your doctor thinks your symptoms are related to a neck problem, an MRI scan may be ordered. This will help your doctor see if you have a possible herniated disk or arthritis in your neck. Both of these conditions often produce arm pain.\nThe traditional surgery for tennis elbow involves a 3 to 4 cm incision over the outer aspect of the elbow centred over the painful area and the diseased tendon causing the pain (called the ECRB tendon) is found after cutting through normal ECRL muscle and tendon.\nThe elbow will need to be rested in a splint for about 2 to 3 weeks. The person is unable to work for about 2 to 3 weeks.\nI prefer a minimally invasive key-hole surgery approach.\nArthroscopic Treatment of Tennis Elbow\nThis procedure is done under general anaesthesia and it takes about 25 to 30 minutes to complete. It involves making 2 small puncture holes (about 3mm in size) on either side of the elbow joint. A small TV camera (called the arthroscope) is inserted into the elbow joint from the inner aspect. A 2nd small hole is made on the outer aspect of the elbow joint to allow a shaver device to be inserted into the elbow joint.\nThis shaver allows me to remove a small portion of the joint capsule to expose the diseased ECRB tendon near the insertion into the lateral epicondyle. The diseased ECRB tendon is carefully removed with the shaver. The elbow can be moved almost immediately after this surgery. The time off from work is markedly reduced from a couple of weeks to just a few days.\nFollowing surgery, your arm may be immobilized temporarily with a splint. About 1 week later, the sutures and splint are removed. After the splint is removed, exercises are started to stretch the elbow and restore flexibility. Light, gradual strengthening exercises are started about 2 months after surgery. Your doctor will tell you when you can return to athletic activity.', 'Are you suffering from tennis elbow, also known as lateral epicondylitis, and looking for ways to fix it without surgery? You’re in luck! There are several non-surgical treatments that can help reduce pain and inflammation. These include resting the elbow, taking anti-inflammatory medication, applying ice to the affected area, performing stretching and strengthening exercises, and using compression and braces. With these simple steps you can get relief from your condition without going under the knife. Read on to learn more about each of these methods so you can start finding relief today!\nRest the Elbow\nResting the affected area is a key part of alleviating pain and allowing the injury to heal. This can include avoiding activities that aggravate the elbow, such as tennis or any other type of repetitive motions. It also means taking regular breaks from strenuous activities to give your elbow time to recover. While rest is essential for recovery, it should not be overdone: too much inactivity can weaken muscles and slow healing. Massage therapy can help reduce inflammation and improve range of motion, while physical therapy can target specific exercises to help strengthen weakened muscles around the elbow joint. These treatments can help speed up recovery time and reduce the risk of re-injury by teaching proper technique for future use.\nThe best way to treat tennis elbow is with a combination of rest and targeted exercises that focus on strengthening weak areas around the elbow joint without overworking them. Stretching exercises are an important part of this treatment plan; they can increase flexibility in your forearm muscles and reduce tightness in your tendons which will help reduce pain when using your arm regularly again. Additionally, be sure to avoid heavy gripping or lifting objects until you have been cleared by a doctor as these types of activities may exacerbate symptoms even further.\nBy following these steps – resting, massaging, physical therapy, stretching – you’ll be able to manage pain associated with tennis elbow while working towards full rehabilitation without having to resort to surgery. With diligent effort you’ll be back on court in no time!\nTake Anti-Inflammatory Medication\nBattling the aches and pains of an irritating injury is no fun, so anti-inflammatory meds can help reduce discomfort. Anti-inflammatory medications are used to treat tennis elbow and other chronic pain conditions, such as bursitis or tendinitis. The goal of taking these medications is to reduce inflammation, which in turn decreases pain and stiffness. While there are alternative therapies that can be used for treating tennis elbow such as massage therapy, many people find that anti-inflammatory meds provide a more effective relief from the discomfort caused by this condition.\nWhen taking over-the-counter (OTC) anti-inflammatories like ibuprofen or naproxen sodium, it’s important to follow the instructions on the package for proper dosing and use. If you’re still experiencing severe pain despite taking OTC meds, your doctor may prescribe a stronger dose of an anti-inflammatory medication specifically tailored to your needs. In certain cases, corticosteroids may be prescribed as a short term solution for reducing inflammation in order to provide relief from acute bouts of tendonitis or bursitis.\nIt’s important to remember that while they can be helpful in relieving symptoms associated with tennis elbow and other inflammatory conditions, medications should always be taken as directed by your doctor or pharmacist. Taking too much medication could lead to harmful side effects such as stomach upset or kidney damage; therefore it’s important not to exceed any recommended dosage amounts without consulting with a medical professional first.\nApply Ice to the Affected Area\nSoothe your aching elbow with icy relief – applying ice can help reduce inflammation and ease the pain. Ice is often used in combination with massage therapy to reduce swelling and discomfort caused by tennis elbow. Wrap an ice pack or bag of frozen vegetables in a thin towel and apply it to the affected area for no more than 20 minutes at a time, several times each day. The cold helps numb the area, reducing pain and inflammation. Be sure not to put direct pressure on your skin as this could cause further damage or injury.\nAfter applying ice, you may want to use heat instead; however, be sure to wait 24 hours before switching from one method of treatment to another. Heat is useful for loosening tight muscles and tendons around the elbow joint that can contribute to pain caused by tennis elbow. Use moist heat such as hot towels soaked in warm water or take a warm bath or shower; alternatively, electric heating pads are also helpful. Apply gentle heat for 15-20 minutes at a time every few hours throughout the day as needed for symptom relief.\nTreating your tennis elbow with both ice and heat can help promote healing without surgical intervention. However, if you do not find any benefit after several weeks of home treatments like these, it’s important to speak with your doctor about other treatments that may be available that could provide more effective relief from your symptoms.\nPerform Stretching and Strengthening Exercises\nTo help you get back in the game, strengthening and stretching exercises can provide relief from your elbow discomfort—without having to resort to surgery. Stretching and strengthening exercises are designed to reduce tension on the affected area and repair any damage that has been done. To perform these exercises correctly, it is important to modify your posture appropriately when performing them so as not to aggravate the condition further. In addition, using massage therapy for a few minutes before or after each exercise can help increase circulation and ensure better results.\nThe most common type of strengthening exercises for tennis elbow include wrist curls, reverse wrist curls, wrist extensions, reverse wrist extensions and forearm pronation/supination. These exercises should be performed slowly with light weights initially; gradually increasing the weight as your strength increases. Additionally, stretches such as forearm flexion/extension stretch or finger flexor stretch are recommended to loosen up tight muscles and improve range of motion in the area.\nIn order to achieve optimal results from these exercises it is important to perform them consistently over time while listening carefully to your body’s signals so that you do not overdo it or suffer any unnecessary pain or strain. With proper form and regular practice of both stretching and strengthening activities you can expect an improvement in your elbow discomfort without undergoing surgery.\nUse Compression and Braces\nCompression and bracing can help reduce your elbow pain, allowing you to get back in the game without having to go under the knife. Compression helps improve blood flow by providing gentle compression, while bracing provides support to prevent further injury. For optimal results, it is recommended that you combine compression and bracing with heat therapy and massage therapy. Heat therapy increases circulation, which reduces swelling and inflammation, while massage therapy helps relax the muscles around your elbow joint.\nWhen using a brace or wrap for your tennis elbow condition, make sure that it fits snugly but not too tightly – you don’t want to cut off circulation! It is also important to remember that wearing a brace or wrap should only be part of your treatment plan; performing stretching exercises can help increase flexibility and range of motion in your elbow joint. You should also consider taking anti-inflammatory medications as needed for additional pain relief.\nIt’s essential that you take regular breaks from activities that cause pain or discomfort in order to allow time for healing. If after several weeks of trying these methods there is still no improvement in symptoms then follow up with a medical professional for further diagnosis and treatment options like surgery or physical therapy .\nFrequently Asked Questions\nHow long does it take to recover from tennis elbow without surgery?\nIf you have tennis elbow and are looking to avoid surgery, recovery can take some time. Depending on the severity of the injury, it could be anywhere from a few weeks to several months before your elbow is back up to full strength. To speed up the process, make sure you’re doing regular stretching exercises and physical therapy as instructed by your doctor. These activities will reduce inflammation, improve range of motion, and strengthen the muscles around your elbow. With patience and dedication to treatment, you should be able to recover from tennis elbow without surgery.\nIs it safe to take anti-inflammatory medication for tennis elbow?\nTaking anti-inflammatory medication for tennis elbow is generally a safe option, but it may not be the most effective. Stretching and physical therapy are often recommended as these can help reduce inflammation and strengthen the muscles around your elbow joint. If you choose to take medications, be sure to follow your doctor’s instructions carefully and always check with them before taking any new supplements or medications. Remember that it is important to stay active while treating tennis elbow – rest alone will not cure the condition.\nAre there any alternative treatments for tennis elbow?\nIf you have tennis elbow, there are alternative treatments available besides surgery. Strengthening exercises and stretching routines can help alleviate the pain associated with this condition. A physical therapist will be able to work with you to customize a plan specifically for your needs that includes these activities. Additionally, massage therapy or ultrasound therapy may also be beneficial in relieving symptoms of tennis elbow. You should always consult your physician before beginning any new treatment plan to ensure it is right for you.\nHow often should I apply ice to the affected area?\nYou should apply ice to the affected area for at least 15 minutes, 3 times a day. You can also do some stretching exercises and physical therapy to help alleviate any discomfort. Ice is especially helpful if you have recently experienced pain or soreness in your elbow. Make sure to give yourself enough time between icing sessions so that your muscles have time to relax and repair. Ice may not be a permanent solution, but it can provide fast-acting relief from tennis elbow pain.\nWhat type of braces should I use for tennis elbow?\nIf you’ve been diagnosed with tennis elbow, braces can help you manage your condition. The best type of brace for this condition is one that provides support in a resting stance and allows for strength training. Look for an elbow brace that has straps or supports the forearm and helps to reduce the load on the tendons around the elbow joint. Additionally, it should be comfortable to wear and provide adjustable compression levels to increase support when needed. Remember to consult with your doctor before using any kind of bracing, as they will be able to recommend what’s most suitable for you.\nYou don’t have to resort to surgery in order to fix tennis elbow. With the right treatment and lifestyle changes, you can get relief from your symptoms. Resting the elbow, taking anti-inflammatory medication, using ice and compression, and performing stretching and strengthening exercises can help relieve pain. If these treatments don’t work or if your pain persists for more than a few weeks, it’s best to consult with a doctor or physical therapist for further guidance. With the right combination of treatments, you can find relief from tennis elbow without surgery.']	['<urn:uuid:4989c1b4-3420-4f00-b564-70d43744d7ef>', '<urn:uuid:8308d8f9-5f8a-4278-b2a4-d4661dba3664>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T05:06:54.527160	14	72	2478
46	warm water lake powell effects endangered fish species grand canyon	The warming water flowing from the shrinking Lake Powell into the Grand Canyon is creating problems for the Humpback Chub, a threatened fish species. As Lake Powell's water surface drops closer to the turbine intakes on Glen Canyon Dam, the water flowing through is getting warmer. This warmer water allows predator fish like smallmouth bass and green sunfish to survive downstream, threatening the Humpback Chub population. Previously, the cold water from deep in the reservoir helped keep these predator fish away from the Grand Canyon.	['Crisis is brewing on the Colorado River, the result of decades of water use exceeding the river’s water supply, leaving some communities and ecosystems vulnerable—including the Grand Canyon. Climate change is making things worse, shrinking the river. NASA recently released satellite imagery documenting the decline of water stored in Lake Mead, a visual reminder that water users have drained nearly all their reserves and have no choice but to reduce demands. It’s not yet clear whether the seven Colorado River basin states and water users will reach collaborative agreements for voluntary and compensated reductions, whether federal leaders will have to make the hard choices, or whether courts will be the ultimate arbiters deciding who gets to use the water that’s left. It is clear the impact will be a significant reduction in water use, because water to supply all historic uses does not exist.\nThe Colorado River isn’t just a plumbing system for municipalities and agricultural economies, it’s an ecosystem, a living river home to myriad species. Before the reservoirs started to dry up, the clearest indication that supply and demand were out of balance was the elimination of the Colorado River Delta ecosystem, 1.5 million acres of wetlands where the river is supposed to meet the Gulf of California. That story is now a half century old, and Raise the River, a binational coalition of NGOs, has partnered with the United States and Mexican federal governments to restore a small fragment of that ecosystem to improve outcomes for birds and other wildlife.\nThis summer we learned that another Colorado River ecosystem is in trouble. The Grand Canyon sits between the two largest Colorado River reservoirs (in fact, the two largest in the country), uniquely exposed to the water supply crisis. The Grand Canyon may be one of the most iconic of the United States national parks, but it too is vulnerable to the brewing crisis.\nThe Colorado River within the Grand Canyon is managed under the 1992 Grand Canyon Protection Act. The U.S. Bureau of Reclamation and National Park Service, working with partner and stakeholder agencies, have collaborated for decades. Results of this collaboration include improved sediment flows that help maintain sandy beaches used by plants and animals that dwell in the floodplain, as well as by people traveling the canyon by boat. Results also include creation of in-river conditions conducive to maintaining the largest remaining population of Humpback Chub, a fish endemic to the whitewater reaches of deep canyons in the Colorado River Basin. In 1967 the Humpback Chub was listed as an endangered species, and in 2021 it was downlisted to threatened status, indicative of the remarkable efforts to help the species recover.\nYet the Humpback Chub may be in trouble again, vulnerable to predators including smallmouth bass and green sunfish. Water flowing from the shrinking Lake Powell into the Grand Canyon is getting warmer as the reservoir’s water surface drops closer to the turbine intakes on the face of the Glen Canyon Dam. Until recently the water flowing through those intakes, buried deep below the reservoir’s surface, was cold enough to keep predator fish away from passage into the Grand Canyon. In June 2022 research teams first reported detections of the predator fish downstream from Glen Canyon Dam.\nIf you have read this far and are asking yourself why you should be concerned about an endangered fish in the Grand Canyon while water supply challenges threaten drinking water and agricultural production at an unprecedented scale, consider this: as the water surface at Lake Powell continues to drop, there’s a mounting risk that it gets too low for any water to pass through the turbine intakes. Several bypass tubes sit below the turbine intakes, but Reclamation has expressed uncertainty about their ability to pass Colorado River water downstream on an extended basis. Left unchecked, the crisis brewing on the Colorado River won’t just threaten the Humpback Chub, it will threaten every living thing in the Grand Canyon. That includes not only fish, but also a diversity of wildlife including California Condors, Southwestern Willow Flycatchers, bald eagles, and herons, none of which will survive in the Grand Canyon if the river disappears. Already, Colorado River Basin States and Reclamation have agreed to emergency provisions to raise Lake Powell’s elevation through emergency releases from upstream reservoirs and reduced releases from the Glen Canyon Dam, but these measures are temporary and do nothing to balance supply and demand on the Colorado River.\nConsider this: if the Colorado River disappears in the Grand Canyon, there won’t be any float trips through those hallowed red rock walls.\nConsider this: if the Colorado River disappears in the Grand Canyon, there won’t be a Colorado River flowing into Lake Mead, and soon there won’t be a river flowing downstream from Hoover Dam either. Then the crisis is “day zero” with no Colorado River supply downstream in Arizona, California and Mexico, threatening every water user regardless of the seniority of their water right, the economic value of their water use, or their willingness to pay for water.\nHow we adapt to climate change in the Colorado River Basin is of enormous consequence to everyone and everything that depends on the river:\nIn the near term, Reclamation and the National Park Service should develop strategies to protect the incredible progress of the Humpback Chub, relying on physical removal of predator fish, and develop longer-term plans for mechanical strategies such as an engineered temperature control device. While the volume of water to be released from the Glen Canyon Dam cannot be modified for fish, and this year’s crisis situation at the Glen Canyon Dam precludes the flexibility to modify the timing of water releases (the hydrograph), the agencies have an opportunity to explore how 2023 operations could help limit predator fish migration into the territory of the Humpback Chub.\nIn the near term, states and water users should find ways to reduce water use that minimizes harm to vulnerable communities and ecosystems, and develop longer-term strategies that increase operational flexibility to respond to changing conditions in consideration of the full range of values supported by the Colorado River. There is also ample opportunity to use resources provided in the 2021 Bipartisan Infrastructure Law to increase system resilience, including improvements to reduce consumptive uses of water like irrigation infrastructure improvements that help farmers and ranchers and investments in urban conservation and reuse, and investments in native habitat.\nConsider this: we have no choice at the moment but to adapt to the reduced water supply in the Colorado River Basin, as climate change impacts are here for the foreseeable future. But we do have an opportunity to limit how much climate changes in the future, and that requires all of us to insist that our elected leaders not just provide the resources to adapt to this terrible crisis on the Colorado River, but to enact comprehensive policy approaches to reduce carbon emissions and begin the hard work of climate change mitigation.']	['<urn:uuid:122a4733-090d-4d39-8c59-cc13819e0755>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	10	85	1162
47	how long average bicycle trip istanbul	According to surveys, cyclists in Istanbul prefer to ride short distances (5-6 km), with the majority stating their bicycle trips were no longer than 60 minutes and typically end within the same district or an adjacent district.	"[""Istanbul to Build 1,050km of Cycle Lanes to Beat Congestion and Poor Air\nIstanbul is looking to expand its network of bike lanes and promote cycling to improve public health and well-being. Biking on the Asian side of Istanbul. Photo by bicyclemark.\nGrowing physical inactivity at a global scale is causing more people to suffer from chronic diseases every day. According to the World Health Organization (WHO), 31 percent of adults 15 years old and older were insufficiently active in 2008, leading to 3.2 million deaths worldwide. Furthermore, the top five leading chronic diseases linked to physical inactivity cost the global economy $6.2 trillion in 2010.\nHowever, an active design approach to architecture and urban planning has the potential to make daily physical activity an ingrained feature of city life. By focusing on the role that parks, sidewalks, and walkable public spaces play in healthy communities, active design encourages physical activity. Cities that adopt active design have been shown to increase residents' physical activity and improve public health.\nIstanbul Embraces Cycling for an Active City\nMany cities have recently focused on integrated transport planning, walking, and cycling as elements of active design. In Turkey, both the central government and local governments have been supporting cycling culture and infrastructure. For example, in recent years, cycling projects have become more important and popular in Istanbul.\nWith 14 million people living in dense communities, the city has faced intense traffic congestion and low air quality. To improve livability and public health, the Istanbul Metropolitan Municipality (IMM)—the agency responsible for cycling projects— is turning to active design targets, pledging to build 1,050 km of cycle lanes in Istanbul by 2023.\nA Comprehensive Manual for Decision Makers\nEMBARQ Turkey's Safe Cycling Design Manual for Istanbul addresses the challenges of cycling planning in Istanbul and provides guidelines for improvements. The report gathers input from NGOs and cyclist associations, creates standardized tools for developing safe and accessible cycling infrastructure, and provides recommendations for how district agencies can improve cross-coordination.\nOne challenge of cycling in Istanbul is a lack of integration with other modes of transport. While there is bicycle parking at several ferry ports, subway, and bus rapid transit (BRT) stations, they are not adequate, given Istanbul's size. Furthermore, overcrowding makes it prohibitively difficult to carry a bike on public transport. There are 20 buses currently equipped with bike racks, but bike space needs to be expanded across all modes of transportation.\nThe exact number of cyclists in Istanbul is not known, and collision data is unreliable. This can make planning difficult. To decide on the route of a bike lane, planners and designs need know about the estimated number of cyclists in a given area, their preferences and safety expectations, as well as the slope, width, and conditions of the street.\nSince the IMM rarely has this granular information, district agencies and NGOs need to come together to work on cycling projects, as they are more familiar with the experiences of local cyclists.\nAccording to surveys in the Manual, cyclists in Istanbul prefer to ride short distances (5-6 km). The majority of respondents stated that their bicycle trips were no longer than 60 minutes and that their trips generally end within the same district or an adjacent district. 90 percent of respondents believe that there are major problems with cycling infrastructure. 50 percent feel unsafe, as unconnected bike lanes can lead to dangerous contact between cyclists and cars. Lastly, 35 percent of cyclists think that there is a lack of signs on the road, making it difficult to navigate the city safely.\nBike lanes should be designed with this data in mind. They should both serve neighborhood life and integrate with public transit systems. Additionally, routes for cyclists should be coherent, direct, and continuous. Improvements to current roads and safe bike parking are necessary to ensure convenience and safety.\nMaking Istanbul a City Designed for Cycling\nIn order to combat a growing rate of physical inactivity, local decision makers need to raise awareness about cycling as a viable transport option and implement accessible infrastructure across Istanbul. This will require better coordination between the IMM, district authorities, NGOs, and cyclist associations.\nThe manual, which was awarded an Excellence Honorable Mention from the Center for Active Design in New York, provides valuable guidance for local authorities, planners, and designers to create integrated, connected, and accessible bike infrastructure throughout the city. With strong management and active design, Istanbul can make cycling safer and more convenient for all residents.""]"	['<urn:uuid:33fe2995-f489-4ffe-a612-b73d0a613aa2>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	6	37	749
48	best lighting setup for senior home safety prevent accidents	For optimal lighting safety, install night lights in all bathrooms, ensure bright lighting in areas where medication is stored, use proper light bulbs and wattage throughout the home, and make sure there is adequate lighting in the kitchen, especially around the stove and oven area and where sharp appliances are used.	['Where are the HIDDEN HAZARDS for at home seniors?\nThere could be hidden hazards to your aging loved one in their home. It is essential to recognize and remedy these hazards to keep your senior safe at home.\nHome is not always a sweet home. There could be hidden hazards to your aging loved one in their very own home. It is essential that these hazards are recognized and remedied to keep your senior safe and sound at home.\nStairs can pose a dangerous risk if they aren’t carefully evaluated. Falls are the most common cause of injury and death in individuals over age 65. There are several factors that can contribute to a fall on a stairway including steep stairs, lack of railing and stairs in poor condition.\nTips to minimize risk on stairs:\nAdd railings to both sides of stairways\nMake sure railings are secure and in good condition\nEnsure stairway is well lit with light switches at the top and bottom\nHaving a well-lit home can help seniors get around better and prevent them from falling, reading labels incorrectly and tripping over items in the bathroom or walkways.\nTips to minimize risk with lighting:\nHave a night light for all bathrooms\nEnsure there is bright lighting in areas where medication is stored, so the label can be read easily\nCheck that the proper light bulbs and wattage are used throughout the home\nMake sure there is adequate lighting in the kitchen, especially around the stove and oven area and where knives or sharp appliances may be used\nSlippery floors or worn out rugs can cause an older individual to slip and fall or prevent them from moving around in a walker or wheelchair. Keep floors in good repair to prevent injury and allow them to maintain their mobility.\nTips to minimize risk with flooring:\nKeep floors clean and dry\nUse nonskid mats on the bathroom floor\nUpdate carpets if needed; low pile carpet is best\nUse rugs sparingly and properly secure them to the floor\n4. Bathtub and Shower\nGetting in and out of the bathtub or shower can be tricky and create a potential safety hazard. Water adds an extra slippery element and steps in showers or tubs can make this an especially challenging area.\nTips to minimize risk with bathtubs and showers:\nSkid-proof the bathtub and make sure bath mats have a non-slip bottom\nAdd grab bars to the bathtub and/or shower\nUse a shower seat or a bathtub transfer bench if necessary\nHave hot and cold faucets clearly marked\nGetting on and off the toilet can become increasingly difficult as mobility declines. Make sure your loved one can easily sit on the toilet and then stand up to prevent injuries.\nTips to minimize risk with toilets:\nAdjust the seat height if necessary so it isn’t too low or too high\nAdd grab bars near the toilet so your senior can grab onto them as they go on and off the seat\n6. Obstructed Walkways\nKeeping walkways and living spaces clean and clutter-free can help keep individuals from tripping and falling and can help mobility, especially if the adult is in a wheelchair or walker.\nTips to minimize risk with walkways:\nMake sure furniture is not too large that it is obstructing walking paths\nKeep walkways and general areas clean and free from clutter\nHaving improper seating makes it more difficult for seniors to stand up from a seated position and increases the chances of falling or straining muscles.\nTips to minimize risk with seating:\nHave chairs with armrests so seniors can utilize armrests to help them up\nAdjust seat height if necessary to make it easier to sit and stand up\nCheck that all chairs and tables are sturdy and stable\n8. Electrical Cords and Appliances\nElectrical cords peaking out can create a potential tripping and falling risk. Leaving appliances on or having extension cords can create hazards in the home.\nTips to minimize risk with cords and appliances:\nKeep electrical cords hidden and out of walkways\nClearly label “on” and “off” switches for small appliances\nRemove electric and telephone cords from high-traffic areas\nEnsure there aren’t any electrical cords near sinks\nKeep appliances close to wall outlets\nMany seniors are on multiple medications which assist with daily activities and ultimately help them achieve a better quality of life. Precautions should be taken to make sure medication is properly stored and administered, ensuring they do more good than harm.\nTips to minimize risk with medication:\nMake sure medications are clearly labeled and easy to read\nDispose of any outdated medications properly\nNever use another person’s prescription, even if it’s for the same ailment\nKeep medicine cabinets well lit and read medicine labels in good lighting\n10. Smoke and Fire\nFires can happen in any home due to misuse or just simply a faulty appliance or accident. Set your loved one’s home up so they are alerted and prepared if a fire were to happen in their home.\nTips to minimize risk with smoke and fire:\nMake sure smoke detectors are installed in every bedroom and in all levels of the home\nCheck and replace batteries in smoke detectors every 6 months\nKeep a fire extinguisher on every floor\nDon’t smoke in bed or alone in the home']	['<urn:uuid:8ef80bde-8916-4876-a969-eff2862bc29b>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:06:54.527160	9	51	883
49	calculate seer rating versus actual energy usage	While SEER ratings indicate an air conditioner's efficiency, the actual energy usage can differ from the rated efficiency. SEER ratings are determined under specific test conditions: 82°F outdoor temperature, 80°F indoor temperature, and 50% indoor humidity. For example, a 24,000 BTU/h unit with SEER 16 running 8 hours daily for 125 summer days would theoretically use 1,500 kWh of electricity. However, real-world conditions, such as higher outdoor temperatures (like Las Vegas' average July maximum of 106°F), home insulation quality, and system leaks, can significantly affect actual energy consumption. Additionally, air conditioners become less efficient over time, leading to decreased SEER ratings, which can be maintained through regular professional maintenance.	['In air conditioning systems, the efficiency is determined by the SEER (Seasonal Energy Efficient Ratio); the annual efficiency of a central air conditioner or air conditioner heat pump.\nThe higher the SEER, the more energy efficient the system is, and the lower the operating costs.\nAir conditioning systems offer comfort and relaxation during record-busting summer months. These systems play a key role in keeping your home cozy and utility expenses low. When installing an air conditioning system, it is important to estimate your cooling needs and the size of the living space accurately for optimal cooling. Listed below are the most commonly used air conditioning systems to help you make an informed decision.\nDifferent Types of Air Conditioning Systems\n- Zoned HVAC Allow you to control the temperature in every room for optimal comfort\n- Are compatible with supplemental HVAC sources\n- Help save on energy costs\n- Geothermal heat pumps are efficient, long-lasting, relatively silent, and need little maintenance\n- Come with a high upfront cost but make an excellent investment when it comes to saving energy\n- A geothermal air conditioning system installation depends on various factors like the size of your home, the landscape, your subsoil, and ground temperature.\n- Features multiple small units for heating and cooling separate zones\n- A good choice for homes where distribution ductwork is not a viable and practical option\n- Mini-splits are compact in size and highly efficient for conditioning individual rooms\n- These units offer complete flexibility and optimal comfort\n- Reduce utility costs\n- Radiant systems are an ideal solution for homes with small cooling needs\n- These systems employ floor tubing and are appropriate for installation only in dry climates\n- Offer natural cooling and constantly circulate fresh air in the house\n- Perfect for places with a hot and dry climate\n- Cost half the price of a central air conditioner and use one-quarter of the energy.\nComponents of Air Conditioning Systems\nAir ducts form a critical component of your HVAC system as they run throughout the living space to offer uniform heating and cooling. Poorly installed air ducts can bring down the efficiency of your system and boost your energy bills. We can help you determine the right configuration that is best for your house.\nAir Conditioner Compressors\nThe compressor’s function is to constrict the refrigerant and raise the temperature till it turns into a high-pressure gas which heads to the evaporator coil where the cooling process takes place. It is a separate unit that is located outside the house and is mainly responsible for cooling your home.\nIt is the evaporator coils that extract the heat and lower the temperature throughout your home by allowing the cool liquid to evaporate. It is important to clean the evaporator coils periodically and change the filter frequently if you want your HVAC system working efficiently.\nThis is the actual remote control of your air conditioning system and is available in two variants:\nA digital thermostat is accurate, efficient and can be programmed to run and shut down according to your convenience. This allows you to customize and control your system and lowers your energy consumption.\nThis is the traditional variant that employs metallic coils and mercury tubing to signal the system to turn on or shut down.\nThis is the indoor unit that circulates cool air throughout your home by means of a blower. It contains filter racks that connect to the air ducts for uniform distribution. A standalone air handler blows cool air or hot air from your air conditioning system. For an efficiently working air conditioner, it is important that you have it maintained by a professional and avoid potential repairs and replacements.\nBenefits of Installing Air Conditioning Systems\n- Increased work efficiency\nThe working capacity of human beings reduces drastically under high temperatures due to discomfort and irritation. An unfavorable atmosphere blocks the mind and makes it difficult to concentrate. This leads to stress and fatigue which interfere with the performance. Research suggests that an air conditioned environment eliminates all these problems promotes efficiency and increases productivity.\n- Promotes health\nIt creates a healthy environment and keeps you protected from the allergies, respiratory infections, asthma, and other illnesses that accompany seasonal changes.\n- Keeps the air fresh\nAn air conditioner circulates clean filtered air that is free from dust particles and disease-causing bacteria.\n- Keeps sleep disorders away\nSleep deprivation has serious health implications that lead to physical and mental fatigue, stress, and restlessness. Lack of adequate sleep due to a hot, stuffy environment interferes with your ability to focus and adversely impacts your immune system. Investing in an efficient air conditioning system will eliminate all these problems and leave you feeling energetic, refreshed, and rejuvenated.\n- Offers a comfortable surrounding\nToday’s on-the-go lifestyle is fraught with difficulties and challenges on every front, so it is important to make our surroundings as comfortable as possible. Air conditioners have come a long way from a luxury to a necessity and today they form an important aspect of our lifestyle just like laptops and smartphones.', 'Air Conditioning SEER Ratings Explained: 17+ Things to Know\nby The Cooling Company, on Jun 6, 2017 4:16:00 PM\nIn 2008, the Air Conditioning,\nWhat is SEER?\nSEER (Seasonal Energy Efficiency Ratio) is the ratio of the total cooling output of an air conditioner over the length of a season to the total amount of energy consumed during that period. The InspectAPedia website gives a good account of the SEER definition and its meaning in practical terms.\nThe cooling power of an air conditioner is normally defined in terms of BTU (British Thermal Units) per hour. A few decades ago, when energy was relatively cheap, buyers chose models for their cooling power alone. These days, with the ever-rising costs of energy, buyers are more concerned with the cost to produce the level of cooling power they want.\nIn other words, the operating cost.\nThe SEER rating of AC units allows you to compare operating costs between one model and the next. Many years ago, it wouldn’t have been uncommon to find an air conditioner with a SEER of 6 to 10.\nHowever, from January 2015, the U.S. Department of Energy laid down minimum regional standards for split-system central air conditioners. In the Southwestern region of the country, which includes Nevada, the minimum SEER rating must be 14.\nHowever, there are HVAC units that can reach upwards of 26 SEERs.\nThe Facts About SEER Ratings\nHow SEER Ratings Are Determined\nAll air conditioners are rated according to efficiency tests stipulated by the U.S. Department of Energy (DOE). Tests assume an outdoor temperature of 82˚F, an indoor temperature of 80˚F and an indoor relative humidity of 50%.\nThis rating system implies that an AC unit with a SEER rating of 16 is 60% more efficient than a 10 SEER unit. Of course, this is only relevant if the conditions in your home match those of the test. It would be impossible for the DOE to rate the efficiency according to the conditions in each home.\nIn reality, climate varies greatly across the country. In Las Vegas, the average July maximum temperature is 106˚F, way above the 82˚F used to rate the air conditioner. This will affect the rated efficiency of the unit.\nThere are other factors that will affect the efficiency of an air conditioning system. These include the quality of the home insulation and the presence of any leaks in the system or associated ducting.\nYou Can Use SEER to Calculate Your Energy Consumption\nWikipedia gives a good breakdown of how to do this calculation. You can calculate your energy consumption just by using the ratings of your air conditioner.\nAs an example, let’s take a 24,000 BTU/h air conditioner unit with a SEER rating of 16 BTU/Wh operating 8 hours a day, for 125 days over the summer season.\nThe total cooling output over this period would be: 24000 x 8 x 125 = 24,000,000 BTU.\nWith a SEER rating of 16, the electrical energy usage would be: 24,000,000 / 16 = 1,500,000 Wh = 1500 kWh.\nIf your electricity cost is 12c/kWh, then your total electricity cost over this period would be: 1500 x 0.12 = $180.\nRemember, this is based on conditions used for the SEER rating, which are an outdoor temperature of 82˚F, an indoor temperature of 80˚F and humidity of 50%.\nWith higher outdoor temperatures experienced in the Las Vegas summer, you would use more electricity to cool the air down.\nThe Minimum SEER Rating\nThe DOE stipulates minimum standards for split air conditioning systems in each region of the U.S. These regulations came into effect as of January 2015. Nevada falls within the Southwestern region where a minimum SEER rating of 14 is mandatory.\nThe Payback from Higher SEER Rated Air Conditioners\nThere is quite a jump in price from SEER 12 to the higher rated air conditioners. However, they typically pay for themselves through energy savings within a few years. The exact break-even point will depend largely on how many hours a day your AC unit runs.\nSEER Rating in Terms of Energy\nThe Seasonal Energy Efficiency Ratio (SEER) is defined as the ratio of the total cooling output of an air conditioner over a season (BTU), to the total amount of energy consumed by the unit during that period (Wh). An air conditioner with a SEER of 14 will use 1Wh of electricity to extract 14 BTUs of heat from the air in your home.\nHigher SEER Ratings Help the Environment\nBy reducing the amount of electricity consumed, higher SEER rated air conditioners can help in the global drive to save the environment. The energy savings equate to taking a couple of vehicles off the road over the life of the AC unit.\nSEER Ratings Decrease Over Time\nAir conditioners become less efficient as time goes on, with a corresponding decrease in their SEER rating. The best way to prolong the lifespan of your AC unit is to schedule regular air conditioning maintenance with a professional HVAC company.\nSEER Rating Myths Dispelled\nThere have been a number of myths regarding SEER ratings, some of which still persist. The Environmental and Energy Study Institute has a report on some of these myths.\nMyth #1: The Government issued SEER 13 Standard Harms Low-Income Families\nMost low-income families rent homes with air conditioning, so very few of them will have to purchase an air conditioner unit. They will enjoy the savings on electricity from day one.\nMyth #2: The Cost of Moving to the SEER 13 Standard Is Not Covered by the Savings on Utility Bills\nTaking the air conditioner use of most families into account, the extra cost for a SEER 13 unit will be made up through lower electricity bills within 4 years. Central air conditioners have a lifespan of about 18 years.\nConsumers will have the benefit of many years of energy savings after they have made up that extra cost. These calculations on based on present electricity costs.\nPrices for power are likely to go up, which would mean increased savings and a shorter payback period.\nMyth#3: The Higher Cost of the SEER 13 Will Prevent People from Replacing Their Air Conditioners\nIt is unlikely that consumers will keep their old air conditioners when they realize the potential savings in electricity costs. The higher efficiency of the SEER 13 rated units will give them an ongoing reduction in their utility bill over the life of the new air conditioner.\nMyth #4: SEER 13 Units Are Much Bigger than Older Units, Requiring Major Renovations to Existing Homes to Accommodate Them\nSome of the new units are bigger, but the majority are not. Manufacturers are using latest design technology which means that, in many cases, the AC units can be made smaller.\nMyth #5: The SEER 13 Standard Places a Burden on Small Manufacturers\nThe fact is, SEER technology has been made available to all manufacturers. There is no reason why this should place an extra burden on the smaller companies.\nMyth #6: Over 75 Percent of the Models Will Be Eradicated to Accommodate SEER 13\nWell, this may be true, but it is the price of progress in any sector of industry. As new technology is introduced, models using older technology will be phased out.\nThis doesn’t mean that your choice is limited. With SEER 13 being the minimum rating, other models with higher SEER ratings are being manufactured to meet the demand for better efficiencies.\nMyth #7: Units With Higher SEER Ratings are Too Expensive and Won’t Save Me Any Money\nAir conditioners with higher ratings cost more, but they pay for themselves in a few years with energy savings. You get the benefit of a lower electricity bill for the rest of the AC unit’s life.\nMyth #8: Utility Bill Savings Do Not Really Cover the Cost of Air Conditioners with the Minimum SEER Rating 14\nElectricity is a utility that is in greater demand and costs more to produce every year. Even industry experts are loath to predict what electricity costs will be in the years ahead.\nEven though you may have to fork out the extra cost to purchase a 14 SEER air conditioner, this will be recovered within the first few years. You can then enjoy energy savings for the remainder of the AC unit’s life, which could be up to 18 years.\nMyth #9: You Need a Total Home Makeover to Install a High SEER Rated Air Conditioner\nThe reality is that SEER 14 air conditioners are manufactured in a range of sizes. There is no appreciable difference in size, and this is made possible through advances in technology.\nMyth #10: Purchasing an Energy Efficient Air Conditioner Will Automatically Reduce Your Utility Bill\nThis is not true. The efficiency of any AC system doesn’t only depend on the SEER rating of the unit. There are more factors that have to be taken into consideration like the size of the space to be cooled, quality of the home insulation and leak proofing of ducting.\nIf you’re interested in purchasing a new air conditioning unit, read up on the subject and weigh up the pros and cons. Your next step would be to call in an air conditioning professional to give you the correct advice.']	['<urn:uuid:faad6026-840f-47dc-b0a3-842f75685003>', '<urn:uuid:6180b887-3f59-42af-99a2-12d0a961d69b>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-13T05:06:54.527160	7	109	2395
50	What is the impact of math in science, and how does residue theory help?	Mathematics has shown 'unreasonable effectiveness' in science, with diverse applications across fields like regional economics, physics, and electrical engineering. The residue theory exemplifies this effectiveness by providing a systematic method for computing complex integrals. It works by reducing difficult integral calculations to finding residues at singular points, which is particularly useful for problems involving analytic functions. This theory has found numerous important applications in analytic continuation, decomposition of meromorphic functions, and power series summation.	"['The Cauchy Method of Residues: Theory and Applications\nSpringer Science & Business Media, Apr 30, 1984 - Mathematics - 361 pages\nGrowing specialization and diversification have brought a host of monographs and textbooks on increasingly specialized topics. However, the ""tree"" of knowledge of mathematics and related fields does not\' grow only by putting forth new branches. It also happens, quite often in fact, that branches which were thought to be completely disparate are suddenly seen to be related. Further, the kind and level of sophistication of mathematics applied in various sciences has changed drastically in recent years: measure theory is used (non-trivially) in regional and theoretical economics; algebraic geometry interacts with physics; the Minkowsky lemma, coding theory arid the struc ture of water meet one another in packing and covering theory; quantum fields, crystal defects and mathematical programming profit from homotopy theory; lie algebras are relevant to filtering; and prediction and electrical engineering can use Stein spaces. And in addition to this there are such new emerging subdisciplines as ""completely integrable systems"", ""chaos, synergetics and large-5cale order"", which are almost impossible to fit into the existing classification schemes. They draw upon widely different sections of mathematics. This program, Mathematics and Its Applications, is devoted to such (new) interrelations as exampla gratia: - a central concept which plays an important role in several different mathe matical and/or scientific specialized areas; - new applications of the results and ideas from one area of scientific en deavor into another; - influences which the results, problems and concepts of one field of enquiry have and have had on the development of another.\nWhat people are saying - Write a review\nWe haven\'t found any reviews in the usual places.\nOther editions - View all\nThe Cauchy Method of Residues: Volume 2: Theory and Applications\nDragoslav S. Mitrinovic,J.D. Keckic\nNo preview available - 2012\nAcad analytic function applications arbitrary becomes belong bounded calculus of residues called Cauchy circle closed contour coefficients complex Compt conclude considered constant contains continuous Contour Integration convergent defined Definite Integrals denoted determine difference equal equation evaluated EXAMPLE exists expansion expressed f is regular Figure formula function f Furthermore give given hand Hence implies important inside integral intégrales Let f limit Math Mathematical means mentioned method neighbourhood obtain Oeuvres origin Paris particular period plane polynomial positive problem Proof proved published real axis REFERENCE region region G regular function REMARK Rend replaced Res f(z respect result Russian satisfies side simple poles singularities solution Suppose taken Theorem theory transform variables written zeros\nPage 7 - ... into another ; - influences which the results, problems and concepts of one field of enquiry have and have had on the development of another. The Mathematics and Its Applications programme tries to make available a careful selection of books which fit the philosophy outlined above. With such books, which are stimulating rather than definitive, intriguing rather than encyclopaedic, we hope to contribute something towards better communication among the practitioners in diversified fields.\nPage 8 - ... available a careful selection of books which fit the philosophy outlined above. With such books, which are stimulating rather than definitive, intriguing rather than encyclopaedic, we hope to contribute something towards better communication among the practitioners in diversified fields. Because of the wealth of scholarly research being undertaken in the Soviet Union, Eastern Europe, and Japan, it was decided to devote special attention to the work emanating from these particular regions. Thus...\nPage 8 - The unreasonable effectiveness of mathe- As long as algebra and geometry promatics in science ... ceeded along separate paths, their advance was slow and their applications limited.', 'Residue of an analytic function\nof one complex variable at a finite isolated singular point of unique character\nThe coefficient of in the Laurent expansion of the function (cf. Laurent series) in a neighbourhood of , or the integral\nwhere is a circle of sufficiently small radius with centre at , which is equal to it. The residue is denoted by .\nThe theory of residues is based on the Cauchy integral theorem. The residue theorem is fundamental in this theory. Let be a single-valued analytic function everywhere in a simply-connected domain , except for isolated singular points; then the integral of over any simple closed rectifiable curve lying in and not passing through the singular points of can be computed by the formula\nwhere , , are the singular points of inside .\nThe residue of a function at the point at infinity , for a function which is single-valued and analytic in a neighbourhood of that point, is defined by the formula\nwhere is a circle of sufficiently large radius, oriented clockwise, while is the coefficient of in the Laurent expansion of in a neighbourhood of the point at infinity. The residue theorem implies the theorem on the total sum of residues: If is a single-valued analytic function in the extended complex plane, except for a finite number of singular points, then the sum of all residues of , including the residue at the point at infinity, is zero.\nThus, the computation of integrals of analytic functions along closed curves (contour integrals) is reduced to the computation of residues, which is particularly simple in the case of finite poles. Let be a pole of order of the function (cf. Pole (of a function)); then\nIf (a simple pole), the formula becomes\nif , where and are regular in a neighbourhood of , and if is a simple zero for , then\nThe application of the residue theorem to the logarithmic derivative yields the important theorem on logarithmic residues: If a function is meromorphic in a simply-connected domain , while the simple closed curve lies in and does not pass through zeros or poles of , then\nwhere is the number of zeros and is the number of poles of inside counted with multiplicities. The expression on the left-hand side of the formula is called the logarithmic residue of the function with respect to the curve (see also Argument, principle of the).\nResidues are employed in computing certain integrals of real-valued functions, such as\nwhere is a rational function of , which is continuous if , and is a continuous function if , where is the imaginary part of , and is analytic if except for a finite number of singular points. By substituting , is reduced to the contour integral\ni.e. to the computation of the residues;\nif as , , ; and\nif satisfies the conditions of the Jordan lemma.\nResidues have found numerous important applications in problems of analytic continuation, decomposition of meromorphic functions into partial fractions, summation of power series, asymptotic estimation, and many other problems of theoretical and applied analysis –.\nThe theory of residues in one variable was mostly developed by A.L. Cauchy in 1825–1829. A number of results concerning the generalizations of the theory were obtained by Ch. Hermite (a theorem on the sum of the residues of doubly-periodic functions), P. Laurent, Yu.V. Sokhotskii, E. Lindelöf, and others.\nResidues of analytic differentials rather than residues of analytic functions are studied on Riemann surfaces  (cf. also Differential on a Riemann surface). The residue of an analytic differential in a neighbourhood of (one of) its isolated singular points is defined as the coefficient of in the Laurent expansion of the function , where is a uniformizing parameter (cf. Uniformization) in a neighbourhood of this point. The integral of along any closed curve on the Riemann surface can be expressed in terms of the residues of the differential and its cyclic periods (the integrals of along canonical cuts, cf. Canonical sections). The theorem on the total sum of residues is applicable to Riemann surfaces: The sum of all residues of a meromorphic differential on a compact Riemann surface is zero.\nThe theory of residues of analytic functions of several complex variables.\nSee –, , . This theory is based on the integral theorems of Stokes and Cauchy–Poincaré, which make it possible to replace the integral of a closed form along one cycle by an integral of this form along another cycle which is homologous to the former. The foundations of the theory of residues of functions of several variables were laid by H. Poincaré , who was the first (1887) to generalize Cauchy\'s integral theorem and the concept of a residue to functions of two complex variables; he showed, in particular, that the integral of a rational function of two complex variables along a two-dimensional cycle which does not pass through the singularities of the integrand can be reduced to the periods of Abelian integrals (cf. Abelian integral), and employed double residues as the basis of a two-dimensional analogue of Lagrange series.\nJ. Leray  (see also , ) developed the general theory of residues on a complex-analytic manifold . Leray\'s residue theory describes, in particular, a method of computing integrals along certain cycles on of closed exterior differential forms with singularities on analytic submanifolds. He introduced the concept of a residue form, which generalizes the concept of a residue of an analytic function of a single variable; the residue formula thus obtained makes it possible to reduce the computation of the integral of a form with a first-order polar singularity on a complex-analytic submanifold along a given cycle in to the computation of an integral of the residue form along a cycle on of one dimension lower. In calculating integrals of closed forms with arbitrary singularities on , the important concepts are those of a residue class (cf. Residue form) and the Leray theorem, according to which any closed form has a corresponding cohomologous form with a first-order polar singularity on . For a form with a singularity on several submanifolds one uses the composite residue form\nthe residue class\nand the residue formula\nwhere is the composite Leray coboundary operator associated to the Leray coboundary operator and is a cycle in .\nThere exists another approach to the theory of residues of functions of several complex variables — the method of distinguishing a homology basis, based on an idea of E. Martinelli and involving the use of Alexander duality . Let , , be a holomorphic function in a domain , and let be an -dimensional cycle in . If is a basis of the -dimensional homology space of the domain and\nis the expansion of with respect to this basis, a generalization of the residue theorem has the form\nis an -dimensional analogue of the residue and is called the residue of the function with respect to the basic cycle . As distinct from the case of one variable, it is very difficult to find both a homology basis and the coefficients . In several cases (for example, when , where is a polynomial) these problems may be solved with the aid of Alexander–Pontryagin duality. The coefficients are found as the linking coefficients of the cycle with the cycles on the set (compactified in a certain manner) which are dual to the cycles . The residues can in some cases be found as the respective coefficients of the Laurent expansion of the function .\nMulti-dimensional analogues of logarithmic residues , – express the number of common zeros (counted with multiplicities) of a system of holomorphic functions in a domain by means of the integrals\n|||A.I. Markushevich, ""Theory of functions of a complex variable"" , 1 , Chelsea (1977) (Translated from Russian) MR0444912 Zbl 0357.30002|\n|||M.A. Evgrafov, ""Analytic functions"" , Saunders , Philadelphia (1966) (Translated from Russian) MR0197686 Zbl 0147.32605|\n|||I.I. [I.I. Privalov] Priwalow, ""Einführung in die Funktionentheorie"" , 1–3 , Teubner (1958–1959) (Translated from Russian) MR0342680 MR0264037 MR0264036 MR0264038 MR0123686 MR0123685 MR0098843 Zbl 0177.33401 Zbl 0141.26003 Zbl 0141.26002 Zbl 0082.28802|\n|||B.V. Shabat, ""Introduction of complex analysis"" , 1–2 , Moscow (1985) (In Russian) Zbl 0578.32001 Zbl 0574.30001|\n|||G. Springer, ""Introduction to Riemann surfaces"" , Addison-Wesley (1957) pp. Chapt.10 MR0092855 Zbl 0078.06602|\n|||H. Poincaré, ""Sur les résidues des intégrales doubles"" Acta Math. , 9 (1887) pp. 321–380 Zbl 19.0275.01|\n|||J. Leray, ""Le calcule différentiel et intégral sur une variété analytique complexe (Problème de Cauchy, III)"" Bull. Soc. Math. France , 87 (1959) pp. 81–180|\n|||L.A. Aizenberg, A.P. Yuzhakov, ""Integral representations and residues in multidimensional complex analysis"" , Transl. Math. Monogr. , 58 , Amer. Math. Soc. (1983) (Translated from Russian) MR0735793|\n|||A.K. Tsikh, ""Multidimensional residues and its applications"" , Amer. Math. Soc. (Forthcoming) (Translated from Russian)|\n|||P.A. Griffiths, ""On the periods of certain rational integrals I"" Ann. of Math. (2) , 90 : 3 (1969) pp. 460–495 MR0260733 Zbl 0215.08103|\n|||G.P. Egorichev, ""Integral representation and the computation of combinatorial sums"" , Amer. Math. Soc. (1984) (Translated from Russian)|\n|||P.A. Griffiths, J.E. Harris, ""Principles of algebraic geometry"" , Wiley (Interscience) (1978) MR0507725 Zbl 0408.14001|\n|||W.R. Coleff, M.F. Herrera, ""Les courants residuals associés à une forme meromorphe"" , Lect. notes in math. , 633 , Springer (1978)|\nSee also the comments and references to Residue form.\n|[a1]||D.S. Mitrinović, J.D. Kečkić, ""The Cauchy method of residues: theory and applications"" , Reidel (1984) MR754560 Zbl 0546.30004|\nResidue of an analytic function. Encyclopedia of Mathematics. URL: http://www.encyclopediaofmath.org/index.php?title=Residue_of_an_analytic_function&oldid=23957']"	['<urn:uuid:cf36118b-f483-4fa8-8ce9-9773cb4d9a7f>', '<urn:uuid:0e8e26e8-41b2-48a1-bc9e-93f3c86b7dc5>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:06:54.527160	14	74	2179
51	What does the Mukenga mask represent in Kuba culture?	The Mukenga mask represents an elephant and is characterized by a beaded trunk and two small tusks. The tusks symbolize wealth and fertility, and the mask is used in funeral ceremonies of high-status Kuba titleholders.	"['|A Kuba Mask\nBonnie E. Weston\n|The Kuba live in the Lower Kasai region of central Zaire in a rich environment of dense forest and savanna.\nOrganized into a federation of chiefdoms, the almost 200,000 Kuba are a diverse group of over eighteen different\npeoples unified under the Bushong king. They share a single economy and, to varying degrees, common cultural\nand historical traditions. Agriculture is the main occupation, supplemented by hunting, fishing, and trading. The\nname ""Kuba"" comes from the Luba people to the southeast. The Kuba call themselves ""the children of Woot""—\nafter their founding ancestor (Vansina 1964:6;1078:4).\nPraised as ""God on Earth,"" the king, nyim, is a divine ruler who controls fertility and communicates with the creator,\nMboom. The royal court at Nsheng is a hierarchical complex of councils and titled officials who advise the king and\nbalance his power. Outlying Kuba chiefdoms are largely autonomous, organized on models analogous to those of\nthe capital but on a lesser scale (Vansina 1964:98-99; 1978:216). Kuba society parallels governmental\norganization in that it is stratified. Yet the Kuba people prize hard work and achievement, and while position of birth\nmay secure advantage, it is not binding (Vansina 1964:188;1968:13,15).\nKuba religion, however, is not highly organized. The creator, Mfcoom, is recognized but is not formally worshiped.\nMore considera¬tion is given to Woot, who led the Kuba migration ""up river"" and established matrilineal descent,\nmale initiation, and kingship. Local nature spirits, tended by priests and priestesses, are actively involved in\npeople\'s lives, notably in matters of fertility, health, and hunting. The Kuba have no ancestor cult but do believe in\nreincarnation (Vansina 1964:9-10).\nKuba arts primarily address status, prestige, and the court; they are manifestations of social and political hierarchy.\nRank and wealth are expressed in extensive displays of regalia: jewelry, rich garments of embroidered raffia cloth,\nceremonial knives, swords, drums, and elaborated utilitarian items. Valuable imported cowrie shells and beads\nemblellish garments, furniture, baskets, and masks.\nThe outstanding Kuba style diagnostic is geometric patterning used to embellish the surfaces of many objects.\nThese designs are woven into raffia textiles and mats, plaited in walls, executed in shell and bead decoration, and\nincised on bowls, cups, boxes, pipes, staffs, and other forms including masks. All art forms and designs are laden\nwith symbolic and iconographic meaning, and the same is true of the rich Kuba masquerades.\nMasking was first introduced by a woman who carved a face on a calabash, the original model for initiation masks.\nThe invention was taken over by men, incorporated into initiation, and remains a male privilege. Once Bushong\nboys move into the nkan initiation shelter, they can wear masks and make excursions into the village frightening\nwomen and small children. More powerful masks are worn by initiation officials. The masked Kuba dancer is, in\nevery instance, a spirit manifestation (Torday 1910:250; Vansina 1955:140).\nThree royal mask types exist: the tailored Mwaash aMbooy, representing Woot and the king; the wooden face\nmask, Ngady Mwaash aMbooy, the incestuous sister-wife of Woot; and the wooden helmet mask, Bwoom ,the\ncommoner. These characters appear in a variety of contexts including public ceremonies, rites involving the king,\nand initiations. Although their dances are generally solo, together the three royal masks reenact Kuba myths of\norigin (Cornet 1982:254,256; Roy 1979:170).\nBwoom is a wooden helmet mask elucidated by varied oral traditions. The Kuba feel that one "" \'understands\' the\nwhy of something if one knows how it \'began\'; something is known if it is explained"" (Vansina 1978:15). Thus\nBwoom is the spirit first seen by nkan initiates; he is a hydrocephalic prince, a commoner, a pygmy, or one who\nopposes the king\'s authority. Two traditions trace Bwoom\'s origin to the reign of King Miko mi-Mbul, who had gone\nmad after killing the children of his precedessor. Although he finally became sane, Miko would lapse into madness\neach time he wore Mwaash aMbooy, the most important royal mask and until then the only one worn by the king\nhimself. A pygmy offered the king Bwoom as an alternative. Suffering no ill effects with the new mask, Miko\naccepted it. A less dramatic version is that Miko, known as a great dancer, was simply seduced by the pygmy\'s\ncreation and adopted it despite its humble character. In both cases the King is credited with improvements to the\nmask that justify its inclusion in the royal repertoire (Cornet 1982:269).\nAs inconsistent as they may seem, each account expresses an aspect of the mask or its character. The\nidentification of Bwoom as a pygmy or a hydrocephalic man is often cited to explain the mask\'s enlarged forehead\nand broad nose. Bwoom appears in initiation and is always considered a spirit. The lowly origin of the character is\nreflected in its description: ""a person of low standing scarcely worthy of being embodied by the king"" (Cornet 1975:\n89) and conversely in its defiant performance opposite the regal Mwaash aMbooy. The two may act out a\ncompetition for the affections of the one female in the royal mask trio, Ngady mwaash aMbooy (Cornet 1982:255).\nMwaash aM-booy\'s dance is calm and stately, while Bwoom acts with pride and aggression (Cornet 1982:255). The\nmasks are easily differentiated by material, for Bwoom is carved from a single piece of wood and Mwaash aMbooy\nis made from cloth and raffia textiles.\nBwoom appears on the nkan ""initiation fence"" of the Bushong (Vansina 1955:150-151) and in other initiation\ncontexts. Little is known of this mask (or indeed most Kuba arts) outside of the royal Nsheng tradition.\nA royal mask, Bwoom is sometimes worn by the king. Yet unlike Mwaash aMbooy, Bwoom does not appear at\nfunerals, and it is never interred with the king or other dignitaries (Cornet 1982:270). The costume is similar to that\nof Mwaash aMbooy: heavy with profuse layers of raffia-cloth, bead and cowrie decoration, leopard skins, anklets,\narmlets, and fresh leaves. Eagle feathers or other prestigious media are added to the crown of the head when the\nmask is danced.\nDespite regional variations, the Bwoom mask conforms to a distinct type. All styles feature strongly rendered\nproportions dominated by an enlarged brow, broad nose, and usually naturalistic ears. Typical features include the\nmetal work on the forehead, cheeks, and mouth, bands of beads that embellish the face, and an expanse of\nbeadwork at the temples and back of the head. Plate 8 has these plus patterned raffia-cloth covering the top of the\nhead, with a fringe of hair. The blue beads set into the white band at the temples imitate ethnic tattoo patterns\n(Cornet 1982:266), and the design at the back of the head is one associated with royalty.\n|Mukenga / Mukyeem\nA non-royal variant of the Mwaash aMbooy type mask is the Mukenga mask, representing an elephant, is\ncharacterized by a beaded trunk and two small tusks protruding from the base of the trunk. As in other\nAfrican kingdoms, such as the Asante and those of the Cameroon grassfields, elephants are associated\nwith royal power. According to the Kuba proverb, ""an animal, even if it is large, does not surpass the\nelephant. A man, even if he has authority, does not surpass the king (Binkley 1992: 277). The small beaded\ntusks flanking the trunk symbolize wealth and fertility (Binkley 1992: 288). Mukenga masqueraders perform\nat the funeral ceremonies of high-status Kuba titleholders.\n|THE OBJECTS BELOW ARE NOT IN MY COLLECTION, THEY ARE\nFOR REFERENCE PURPOSES ONLY\nThere are a LOT of great photos in the Eliot Elisofon Photographic Archives\nProperty from a New York Private Collection\nA FINE KUBA MASK\nLOCATION ESTIMATE AUCTION DATE\nNew York 7,000—10,000 USD Session 1\n15 Nov 02 10:15 AM\nLot Sold. Hammer Price with Buyer\'s Premium: 10,755 USD\nheight 21 1/2in. 54.6cm\nmukyeem, of helmet-like form and large proportions a raffia attachment at the base, composed of a flat metal\nfacial plane pierced through for attachment of the expressive beaded facial features, and wearing an\nelaborate headdress composed of blue, white, black, pink and red beads and cowrie shell, a conical projection\nat the crown in the form of an arching elephant trunk framed by two tusks.\nJ. J. Klejman, New York\nSotheby\'s, New York, November 15, 1988, lot 124\n|Elephant Mukenga Mask\nCowrie Shells, Beads, Raffia, Fur, Cloth\n|Mask (mukyeem), 19th–20th century\nDemocratic Republic of the Congo, Kuba peoples\nWood, beads, fiber, hair, cowrie shells, and cloth; H. 18 in. (45.7 cm)\nSource - http://www.metmuseum.org/special/Genesis/10.L.htm\nKuba Culture, Democratic Republic of\nCongo (formerly Zaire) 1800s-1900s\nWood, animal fur, raffia cloth, cowrie\nshells, glass beads, string\n19 1/2 inches high\nVirginia Museum of Fine Arts\nPurchase, The Arthur and Margaret\nGlasgow Fund, 87.82\n|C. Pollzzie Collection\nAcquired from the McDonald-Levy Collection\nHeight: 33 in. (84cm)\nWidth: 20 in. (51cm)\nDepth: 18 in. (46cm)']"	['<urn:uuid:096f18f2-5ab4-4989-bed2-3920c8470c65>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:06:54.527160	9	35	1460
52	universal design elderly care benefits privacy risks	Universal design in elderly care offers significant benefits while raising important privacy concerns. On the benefits side, it enables aging-in-place through features like curb-less European showers and step-less grade changes, making homes more functional and sustainable for elderly residents while maintaining an attractive appearance. However, there are serious privacy risks associated with elderly care technology, including potential data breaches of health monitoring systems, unauthorized access to biometric data that could lead to fraud, and security vulnerabilities in medical devices like pacemakers. These risks make it crucial to implement proper data protection measures like federated learning to maintain privacy while still providing effective care solutions.	['From ReNews, NAHB Remodelers\nA remodeling project that combines universal design and green remodeling can be a strong seller in today’s weakened market, particularly with baby boomers “coming of age,” according to Mike Vowels, of Stewardship Remodeling in Seattle.\nVowels sees a strong link between the two remodeling concepts because both involve consumers planning for their futures and incorporating sustainability in a home.\nBut Vowels also cautions that designing the remodeling solution offered to potential clients has to be “seamless and invisible,” or consumers won’t find it appealing.\n“You need to be careful when presenting the subject of ‘aging-in-place’ to prospective clients because some people are uncomfortable with the language,” Vowel says. “People don’t want to envision themselves getting old or becoming less capable.”\nInstead of using the term, “roll-in shower,” for example, Vowels talks to his prospects about curb-less European showers. Instead of ramps, he discusses step-less grade changes leading to the front entrance or back patio.\nVowels markets and sells universal design and green remodeling as a total home remodeling solution rather than as two compatible concepts.\n“You wouldn’t recognize the differences if you didn’t point them out,” he says. “You have to be able to demonstrate that the remodeled home would have all these tasteful changes without anyone being aware that anything is different or out of the ordinary. None of the changes should look temporary, generic or institutional.”\nThe most effective way to accomplish such a seamless remodel, Vowels says, is to anticipate future needs, plan accordingly and integrate the universal design and green solutions.\n“It’s about how smart your house can be,” says Vowels. For example, a design that plans for future changes can include stacked closets that are properly sized so that they can be converted into an elevator shaft later, if needed. Such pre-planning meets the home owner’s needs now and their changing needs in the future.\nThe approach makes the whole remodeling project much more marketable and easier to sell because there are more features and benefits to sell — and because they work together, he says.\n“Unlike a carton of milk or a steak, the function, safety and comfort of your home should not have an expiration date on it,” Vowels says.\n- Economic Sustainability — An energy-efficient home will have lower operating costs (e.g., utilities) and coupled with universal design, the home will be more marketable to a broader population. Long term, a home with green features and universal design is a good investment.\n- Environmental Sustainability — A home incorporating universal design is remodeled to anticipate the transitions linked to aging. This lessens the need for ad hoc changes in the future that are age related and less seamless.\n- Social Sustainability — A home incorporating universal design provides visitability for people of varied abilities and enables home owners — and sometimes whole families — to stay in their same home (aging-in-place) and continue living in their same community.\nThe overall combination of benefits that result from combining universal design and green remodeling into one seamless remodeling solution is helping Vowels differentiate his company from his competition.\n“We’re trying to distinguish ourselves on universal design by showing the attractive side of a very prudent choice for our customers to consider,” Vowels says.\nThe Stewardship Remodeling Web site, www.universalandgreen.com, and all the company’s marketing materials help focus its branding and reinforce the reason to integrate the two remodeling concepts.\nUniversal design and green remodeling, Vowels says, answer the current and future needs of prospective home owners by creating a finished product that is timeless in its use, contributes positively to the environment and is sustainable.', 'Do not let Big Bro in! – Security and privacy in elderly carepolaris_stable_admin\nIn our previous post we talked about how elderly care is becoming one of the most fundamental challenges across the world. It is clear that we will need all the tricks IT can offer, be it cloud computing, edge devices or AI. However, these shiny, new technologies are not without serious risks regarding privacy, material loss or even immediate danger of life. As elderly care regards, these risks are even more pronounced.\nPicture this. You think your grandma is fine as her well-being is monitored indoor (using cameras, lidars, etc) and outdoor (via wearable devices, etc). But what if someone can tap into the data communication and can see when the apartment is empty. Or by stealing the biometric data, it is a piece of cake to steal money or commit fraud. What is even worse, what if someone can fiddle with the smart pacemaker or the insulin pump remotely? Well, actually, it did already happen (shorturl.at/bkrX2).\nWhile all these hybrid (physical and cyber security) issues would worth a separate post on their own, we now want to introduce you to another aspect of privacy concerns: learning from highly sensitive data.\nAs we have talked about how data is important for learning complex patterns of the world, it is no surprise that health care monitoring or modeling behavioural patterns need lots of patient data. Those data can be as simple as the number of doctor-patient contacts a month or as complex as heart rate variation on a second by second basis. The problem is that we need to make sure that no personal information (“meta data”) gets mingled with the data needed to train the AI models. Why is that? Well, making such sensitive data open can pose direct threat to the participants. What is more, there is an indirect risk that can hurt even those who are not providing data to the training process, but are somewhat related to the patients.\nFor health monitoring, the problem is not limited to the model training phase. Continuous monitoring of the participants requires to maintain contact and repeated access to sensitive data. This data is then used to provide predictions as well as useful information to update (fine tune) the learning models (continual learning scenario). So how can we secure the flow of sensitive data? And how can we make sure that personal information is not getting into the wrong hands?\nThere are existing solutions that either try to hide or erase sensitive information (various kinds of anonymization) or try to deeply encrypt the communication channel.\nHowever, there is another smart idea that is designed to render the communication of sensitive info unnecessary. This approach is called federated learning. Let us see what this is all about.\nFederated Learning (FL)\nAccording to WIKI: “Federated learning is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them. Federated learning enables multiple actors to build a common, robust machine learning model without sharing data, thus allowing to address critical issues such as data privacy, data security, data access rights and access to heterogeneous data. Its applications are spread over a number of industries including defense, telecommunications, IoT, and pharmaceutics.”\nThe term was coined by Google (https://arxiv.org/abs/1602.05629v3) back at 2017.\nLet us dive into this complex definition. The first interesting technology involved is called distributed learning. To brush up our knowledge, let us talk a bit about machine learning, in particular, supervised learning. Here the task is to learn to associate labels with data. Machine learning algorithms learn the association by incrementally tuning parameters that define the chain of transformations that make up the algorithm. Now we talk about millions or billions of those parameters! That explains why training is so tedious in most cases. However, if several machines can parallel work on different bunch of data, then training becomes much faster if the trained model variations are properly combined into one single solution. The other thing that pops up is that FL is ideal when privacy preserving is of central importance. The whole idea is about minimizing data exchange between the clients (unit that can train a model on local data) and the server (a unit that aggregates local model updates, organizes parameter exchange, but does not have access to data). This particular issue is getting so important that it makes FL a central part of all AI applications across various indsutries and business: Google, IBM, Intel, Baidu or Nvidia have all come up with their enterprise grade FL frameworks (shorturl.at/dgiy7)!\nThe original idea was based on the assumption that edge devices (like smart phones) can both collect and process data locally. In turn, if models can fit into the phone’s memory, than it is enough to exchange local updates with a central model. This concept is called cross-device FL. Personalized texting like Gboard is using this approach.\nWell, texting is fine on mobiles, but measuring blood sugar? So there is another real-life scenario. You already have shared your medical data with your doctor so as have all her other patients. In turn, the health care center can tune its own model using all the available data. Centers can exchange the model parameters without exposing their own patient data. This approach is called cross-silo FL.\nSee the picture below! Normally data are collected and aggregated across the different locations and a central unit trains a model using all the data collected. This setup definitely raises the red flag as sensitive medical records are moving around. But here comes cross-silo FL to the rescue! Privacy is preserved, well done!\nClearly, cross-device and cross-silo FL types define the scaling dimension of FL. In the last few years many new ideas have been discussed and now there are at least 6 factors that are needed to differentiate between the various solutions.\nData partitioning is about how participant and their features (data records) are treated across the different local models. While the original idea assumed that each client node has the very same representation on the participants, there are reak life scenarios where data gets partitioned by feature sets and not by user id. As an example, a bank and an insurance company may have access to different data on the very same user, yet they can mutually improve each other’s models.\nMachine Learning modeling is about the core model applied within FL. The more complex the model, the more update exchange is needed. As FL regards, the most important question is how to aggregate the local updates when facing reliability and communication bandwidth issues.\nPrivacy Mechanism is a core component of the FL frameworks. The basic idea is to avoid information leakage amongst clients. Differential privacy (that is to separate user specific and generally relevant information) and cryptography are two frequently used approaches, but this is a constantly evolving field.\nCommunication Architecture. The original idea suggested an orchestrated approach to model training where the central server holds the aggregated model that is mirrored in the local units. More recent solutions drop centrality and suggest various decentralized updating mechanisms. In these solutions client nodes communicate with a few peers and there is a particular policy on update propagation.\nWe talked about Scaling, and the last point is about the Motivating Factors for applying FL. In some cases, stringent regulations force us to turn to FL (consider GDPR in Europe, CCPA in the US or PIPL in China). In other cases shared cost and increased reliability could be the main driving forces.\nIf you wonder why we have so many factors to check just think about the immensely different challenges in e-commerce (personalized ad), finance (fraud detection) or healthcare (remote diagnostics, etc, see https://www.nature.com/articles/s41746-020-00323-1). Different requirements require different solutions.\nSo what are the main challenges that FL solutions meet?\nUpdating large models requires sending large messages. Another problem is limited bandwidth: when a large number of clients try to send data, many will fail. The solution for the first problem involves a form of compression, while the second one is addressed by the introduction of decentralized (peer2peer and gossip) networks, when updates are exchanged locally. One example solution is depicted in the figure next:\nPrivacy and data protection\nWhile raw data stay where it was generated, model updates can be attacked and reveal private information. Some solutions are built around differential privacy, where only statistical (general) data are extracted and used for model training (https://privacytools.seas.harvard.edu/differential-privacy). Another interesting idea is to perform computation on encrypted data only (“homomorphic encryption” for those who like scientific terms). Yet another idea goes to the opposite direction: let us spread the sensitive data across many data owners, but computations can only be done in a collaborative fashion. Cool, isn’t it?\nFor really large FL systems, nodes are most likely quite different in terms of storage capacity, computing power, and communication bandwidth. And only a handful of them participates in the update at a given time, resulting in biased training. Solution? Asynchronous communication, sampling of active devices and increased fault tolerance.\nClients may get different data in terms of quality (noisy, missing info, etc) and statistical properties (difference in distribution). That is big one and it is not easy to fix or even to detect. What is even worse, nodes with their local models can be compromised to enable a “model poisoning” attack (https://proceedings.mlr.press/v108/bagdasaryan20a.html): specially crafted data and local model updates drag the aggregated model toward an unwanted state causing erratic behavior and damage.\nIf you have read this far, you must share our enthusiasm for FL. If you are willing to get your hands dirty, here are some open-source FL frameworks to play with:\n- FATE (https://github.com/FederatedAI/FATE) supported by the Linux Foundation.\n- Substra: https://docs.substra.org/en/stable/\n- PySyft + PyGrid: https://blog.openmined.org/tag/pysyft/\n- Nvidia’s Clara: https://developer.nvidia.com/industries/healthcare\n- IBM’s solution: https://ibmfl.mybluemix.net/\n- OpenFL by Intel: https://openfl.readthedocs.io/en/latest/index.html\n- and the very user friendly Flower: https://flower.dev/\nIf you have any questions, have interesting ideas, or just want to talk about FL, just drop a mail!']	['<urn:uuid:29a79a2c-3a33-4d7a-a4fe-739c0d2b2274>', '<urn:uuid:74a87e46-c2b9-4423-a149-5b29495074be>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T05:06:54.527160	7	104	2286
53	need info about hydrops fetalis and madelung deformity in turner syndrome which condition is more common	Both conditions can occur in Turner syndrome, but hydrops fetalis is more common. According to the documents, hydrops fetalis is frequent in Turner syndrome and is one of its commonest causes. In contrast, Madelung deformity is simply listed as one possible condition that can occur in Turner syndrome patients, without any indication of high frequency.	"['Madelung’s deformity is an abnormality of the wrist caused by a growth disturbance that retards development of the ulnar and volar portions of the distal radial physis.\nThe eponym gives credit to Otto W. Madelung, who described this entity. Carpus curvus, radius curvus, and progressive subluxation of the wrist, manus valgus, and manus furca are other terms used for this condition.\nThe primary deformity is bowing of the distal end of the radius, which in the most typical form curves in a volar diorection while the ulna continues to grow in a straight line. The distal ends of the radius and ulna are at different levels in the lateral plane. That of the ulna has maintained its original normal position, while that of the radius has curved down to a volar level.\nIt is the distal end of the radius that is displaced. Becaue of its curvature and growth disturbance, the radius has become short while the ulna has ocntineus to grow normally and has become relatively longer.\nEtiology of Madelung Deformity\nThere are four categories of Madelung deformity\nFollowing trauma that disrupts growth of the distal radial ulnar-volar physis.\nAssociated with bone dysplasias like multiple hereditary osteochondromatosis, Ollier disease, achondroplasia, multiple epiphysial dysplasias, and the mucopolysaccharidoses . This type can also be seen secondary to sickle-cell disease, infection, tumor, and rickets.\nThe most important dysplasia associated with Madelung deformity, however, is Leri-Weill dyschondrosteosis.\nAs in Turner syndrome\nWhere no cause or association can be found\nThe exact nature of the pathologic process that causes the disturbance in the growth of the distal radial physis is unknown.\nWhen Madelung deformity is a hereditary disorder, it is transmitted as an autosomal dominant trait with incomplete penetrance.\nSporadic forms do occur.\nIt is more common in the females and involvement is frequently bilateral.\nNormally, the distal articular surface of the radius is tilted 5 degrees toward its volar surface and 25 degrees toward the ulna, with its dorsal surface and radial margin convex and its volar surface and ulnar border concave.\nThere are two types of Madelung’s deformity.\n- Typical, or regular\n- Atypical, or reverse.\nIn the typical form, the distal articular surface of the radius may tilt toward its palmar surface as much as 80 degrees and ulnarward as much as 90 degrees. In the normal wrist, the proximal row of the carpal bones is arranged in an arc, with its proximal surface forming a convex dome.\nIn Madelung deformity, this dome becomes peaked, its apex resting on the lunate bone. The radius and ulna are separated, with the peak of the carpal bones wedged into the interosseous space. The entire carpus is shifted toward the ulnar and volar side of the wrist. Coalition of carpal bones may be present.\nIn reverse, or atypical, Madelung deformity (it is rarer form), the distal end of the radius is tilted dorsally, reversing the plane of the distal end of the articular surface with a shift of the carpus toward the dorsal side. The distal end of the ulna then appears to be displaced volarly instead of dorsally.\nPathophysiology of Madelung Deformity\nPrimary chromosomal association with Madelung deformity has been observed in patients with Turner syndrome (Patients having only one X chromosome). Within families affected by a short stature dysplasia, a mutation has been found in short stature homeobox-containing gene, SHOX, present on X chromosome.\nBut families with this mutation and individuals with Turner syndrome and families with a history of MD have been shown to exhibit a variable expression of MD and dyschondrosteosis. This raises a possibility of a modifier gene on another area of the X chromosome or on an autosomal gene may be involved.\nDeformity of the wrist is the initial presenting complaint; it usually becomes obvious in late childhood or early adolescence, between the ages of 8 and 12 years.\nIn typical Madelung’s deformity the distal end of the ulna remains in its normal anatomic position and grows distally, causing a visible prominence on the dorsal and ulnar aspects of the wrist.\nNormally, the radial styloid process is long and is located 1 cm distal to the ulnar styloid. In Madelung’s deformity, the radius is shortened at the wrist; the radial styloid process may be on the same horizontal line as the ulnar styloid or may reach a point proximal to it.\nThe range of motion of the wrist is limited, especially in dorsal extension and ulnar deviation. Because of the diasthesis between the distal radius and ulna and the displacement of the carpus between the two separated bones of the forearm, pronation and supination of the forearm are also limited; as a rule, supination is definitely decreased, and pronation is impaired to a slight degree.\nIn reverse Madelung’s deformity palmar flexion of the wrist is decreased, while dorsiflexion is increased. Range of rotatio in of the forearm, especially pronation, is decreased. When it is minimal, madelung’s deformity may be asymptomatic. In moderate or severe deformity, however, pain develops insidiously at the wrist.\nInitially it is minimal, disappearing on rest. With progression of the deformity and impingement of the displaced carpus on the distal ulna, the pain increases. Volar displacement of the carpus may cause discomfort in the region of the median nerve and flexor tendons. Weakness of the wrist may result from progressive instability of the joint.\nCharacteristic radiographic findings include dorsal and radial curvature of the distal radius; exaggerated palmar and ulnar tilt of the distal articular surface of the radius; pyramiding of the carpal bones; greater length of the ulna as compared with the radius; wide interosseous space; and assumption of a relatively dorsal position by the ulnar head, which appears to be enlarged.\nCT scan provides better details of three dimensional deformity though CT scans and 3-dimensional imaging are not necessary for routine treatment.\nTrauma (dislocation of the distal radioulnar joint), rickets, inflammatory conditions of the wrist such as rheumatoid arthritis, and infection involving the ulnar half of the distal radial physis.\nTreatment is primarily directed toward the relief of pain and the restoration of function, with cosmetic improvement as a secondary consideration.\nThe majority of patients with Madelung’s deformity do not require surgical treatment.\nConservative measures consist of curtailing physical activities that may cause forced dorsiflexion of the wrist and wearing a plastic wrist splint to provide support and relieve symptoms.\nSurgical treatment options are\n- Shortening the ulna – Milch’s cuff resection in children or Darrach’s resection.\n- Correcting the bowing deformity of the distal radius by wedge osteotomy\n- Sstabilization of the carpus\n- Prevention of recurrence of deformity by controlling the asymmetrical growth of the distal radius.\nDeformity correction is achieved by either a closing wedge or an opening wedge osteotomy of the distal radius at its metaphyseal-diaphyseal junction.\nFusion of the radial half of the distal radial physis will prevent recurrence of deformity.\nIncoming search terms:\n- madelung deformity (95)\n- madelungs deformity (32)\n- reverse Madelung deformity (9)\n- madelung\\s deformity (9)\n- madelung deformität (6)\n- madelung bone deformity (5)\n- madelung deformity and tennis elbow (4)\n- madelung deformity rheumatoid arthritis (4)\n- madelung disorder (3)\n- madelungs (2)', ""Dr. Anil B. Jalan\nIntrodution :- Hydrops Fetalis is a diagnosis that in the past , was made after delivery and was described as excess collection of fluid in several neonatal body cavities .\nBallantyne described the first case of Hydrops Fetalis 100 yrs. ago , and 50 yrs. later ,Potter described nonimmune hydrops fetalis .\nNonimmune Hydrops Fetalis is responsible for 3 % of overall perinatal mortality .\nIn 1970 , MaCaffe et.al. reported that 82 % of hydrops cases were caused by immune diseases ; by 1992 , 87 % were caused by nonimmune conditions.\nDefinition :- The presence of excess extracellular fluid in two or more sites without any identifiable circulating antibody to red blood cell antigens .\nIncidence :- Most of the large serieses have reported an incidence of 1 : 2000 to 1 : 3000. Almost 85 - 90 % of these hydrops fetalis incidents are due to nonimuune causes .\nSince the advent of ultrasonography more and more cases of fetal hydrops are being detected by gynecologists & sonologists. At times it becomes diffcult to predict the outcome of such hydropic fetuses . The causes of hydrops fetalis are many , but few common ones must be remembered at the time of sonography , so that we look for other evidence which will give support to our provisional diagnosis .\nSeveral large serieses have suggested that cardiac malformations are amongst the most common causes , because they are often found in hydropic fetuses . Yet , cardiac malformations are one of the most common defects and only seldom are associated with nonimmune hydrops fetalis . Thus , the presence of a heart defect doesnot prove that hydrops results from heart failure .\nAt present three main hypotheses have been proposed for the pathophysiological mechanisms underlying hydrops :-\n1 ] Inadequate cardiac output due mainly to\na. Obstructive out-flow\nb. Diverted blood flow\nc. Inadequate blood return\nd. Inadequate ventricualr filling\ne. Inadequate inotropic force\nf. Regurgitation of blood across Cardiac valves .\ng. Reversal of blood flow in I.V.C.\nAll the above mechanisms lead to cardiac failure , cardiomegaly , and elevated umbilical venous pressure , also seen in Rh immune hydrops , due to portal hypertension .\nPortal hypertension in immune hydrops is due to hypertrophy of Hepatic Erythropoeitic tissue .\n2 ] Lymphatic Abnormalities Found in association with :-\na. Cystic hygroma\nb. Noonan syndrome\nc. Turner's syndrome\nd. Pulmonary or peritoneal lymphangiectasia .\ne. Lymphatic venous anastomosis .\nf. Connective tissue malformation e.g. - skeletal dysplasia .\ng. S.O.L. in thorax e.g. -Cystic adenamatoid malformation of lung & Diaphragmatic hernia .\nPlease note that in the above situation the umbilical venous pressure is normal .\n3 ] Reduced osmotic pressure :- Hypoproteinemia .\nThe fetal outcome & the genetic counselling entierly depends upon the exact etiology of such hydrops . Without repeated USG & colour doppler it is impossible to judge the progress of the anasarca . It is quite evident in our cases . Over a period of 4 wks. we could demonstrate progressive anasarca & suspicion of chromosomal anomaly . This helped tremendously to gynecologists & parents in decision making , which was MTP . However in certain cases especially in the immune hydrops cases , in-utero intervention is possible but the decision of undertaking such heroic measures in the yet unborn patient entirely depends on the parental willingness . The causes of hydrops fetalis are given below .\nFollowing is a list of few common causes of hydrops fetalis .\nCauses of hydrops fetalis :-\na. Rhesus hemolytic anemia\nb. Alfa thalassemia\nc. Fetal erythro - leukemia\nb. CMV infection\ne. Other fetal infections .\na. Anagioma of palcenta or fetus\nb. Renal or umbilical vein thrombosis\nc. Cardiovascular malformation .\nd. Fetal tachycardia\ne. Cardiac rhabdomyoma ( Tuberous sclerosis )\na. Fetal haemorrhage\nb. Twin to twin tranfussion syndrome\na. Fetal red cell enzyme defects\nb. Lysosomal enzyme defects\na. Cystic adenomatoid malformation of the lung\nb. Extralobar pulmonary sequestration\nc. Pulmonary hypoplasia\nd. Pulmonary lymphangiectasia\ne. Tracheal atresia\nf. Diaphragmatic hernia\na. Hepatitis or hepatic necrosis\n8 . Lower urinary tract abnormalities\n9 . Congenital neuroblastoma , teratoma, glioma\n10. Some types of short limbed dwarfism\n11. Noonan syndrome\n12. Nuchal bleb syndrome\n13. Optiz frias syndrome\n14. Turner syndrome\n15. Trisomy 18 and 21\n17. Maternal nephrotic syndrome\nSuggested investigations :-\nMaternal tests with live fetus inutero :\n1. BL.Gr. Serology and typing including titre for anti-d antibodies.\n2 . Serum tests for Syphilis ( VDRL ) & TORCH titre .\n3 . Kleihauer test for feto maternal haemorrhage .\n4 . Fetal USG for cardiac anomalies and tumours .\n5 . U.S.G. for placental hemangioma\nCord blood at delivery - irrespective of outcome .\n1 . Blood Group Serology and tying\n2 . Chromosome analysis.\n3 . Total protein and albumin\n4 . TORCH titre\n5 . Hb and Hb - EPP . ( H.P.L.C. )\n6 . Vacuolated lymphocytes\n7 . Enzymology ( red cell and lysosomal enzyme ).\nIn the event of infant death detailed antopsy and H.P. is indicated especially to look for anomalies :-\n1 . Cardiovascular system .\n2 . Kidneys .\n3 . Central nervous system .\nTissues from all usual organs must be taken for H.P.\nIf possible viral culture and chromosome analysis is indicated .\nFrom the above discussion it appears that the list of causes is extensive . However it will be helpful to know that hydrops fetalis is common in certain conditions and only occasionally seen in others .\nHydrops fetalis is frequent in :-\n1 ) Achondrogenesis , type I .\n2 ) Fibro chondro genesis .\n3 ) Monozygotic twinning and structural defects .\n4 ) Osteogenesis Imperfecta syndrome type II .\n5 ) XO - syndrome ( Turner syndrome ).\nOccasional in :-\n1 . Achondrogenesis - Hypochandrogeneis , type II\n2 . Chondrodysplasia punctata - X Linked Dominant type .\n3 . Down - syndrome .\n4 . Generalised gangliosidosis , type I & severe infantile type .\n5 . Lethal Multiple Pterygium syndrome .\n6 . Morquio syndrome .\n7 . Mucopolysacchroidosis type VII\n8 . Short Rib Polydactyly syndrome type II ( Majewski type SRP )\nIt is impossible to discuss all the causes and pathologies of hydrops fetalis but we would like to highlight few conditions of clinical interest , especially syphilis and parvo - virus infection .\nHydrops Fetalis in Turner?Syndrome :- The Turner?syndrome is one of the commonest cause for fetal hydrops especially in the Indian situation . The frequency of Turner?syndrome amongst liveborn infants is 1 in 2,000 females according to D.W.Smith and 1 in 10,000 females according to J.L.Simpson & M.S.Golbus . There are not many studies available for the incidence amongst spontaneous abortions , but one by DR. J.L.Simpson gives 8.6 % due to Turner ( 45 , OX ) in spontaneous abortion cases .\nMonosomy X ( i.e. Turner ) is the signal most common chromosomal abnormality in spontaneous abortions , accounting for 20 - 25 % of abnormal specimens . There are no definite figures available for hydrops fetalis cases .\nEtiology :- Faulty chromosomal distribution leading to XO individual with 45 chromosomes . The paternal chromosome is the one more likely to be missing . There has been no significant older age factor for this neuploidy . On the contatry it is more common in the young mothers ( Warburton et. al. 1980 ) .This has been our observation also, especially amongst the hydropic fatuses and in the first trimester abortion cases . It is generally a sporadic event in a family , although there are as yet no adequate data on risk of recurrence . Mosaicism doesnot ensure survival till term . However the incidence of sex chromosome mosaiciam is higher in liveborns than in aborted 45 XO Fetuses .\nPatients who had one abortion with 45 XO karyotype doesnot mean that we will have only the similar type of recurrence . Wharton et. al. 1987 , has showed that the mothers who had 45 XO fetuses also had normal Fetuses , Monosomies , Triploidies in future .\nThe above discussion definitely emphasises one point that we must advise prenatal diagnosis during the next pregnancy .\nThe clinical features of Turner syndrome are summarised below . The interested readers are referred to the classical text book - Smith?recognizable patterns of human malformation , 5 th Edn.\nClinical Features of Turner?Syndrome :-\nCommon Features :-\n1 . Short stature\n2 . Edema of fingers and toes\n3 . Mild pectus excavatum\n4 . Anomalous auricles , mostly prominent .\n5 . Narrow maxilla and palate .\n6 . Relatively small mandible .\n7 . Inner canthal folds .\n8 . Short neck and low posterior hair line .\n9 . Webbed posterior neck .\n10. Cubitus valgus.\n11. Medial Tibial exostosis .\n12. Broad chest and widely spaced nipples .\n13. Short 4 th metacarpals/ metatarsals\n14. Bone dysplasia with coarse tubular pattern .\n15. Narrow hyperconvex or deep set nails .\n16. Excessive pigmented naevi .\n17. Distal A.T.D. angle .\n18. Loose skin about neck in infancy\nOther anomalies :-\n1 . Horse shoe kidney .\n2 . Double or cleft renal pelvis .\n3 . Cardiac defects - V.S.D. , Coarctation of aorta etc.\n4 . Perceptive hearing deficits.\n5 . Abnormal angulation of radius to carpal bones .\n6 . Short mid phalanx of the 5 the finger ( cleinodactyly ).\n7 . Scoliosis , kyphosis .\n8 . Spina bifida .\n9 . Vertebral fusion .\n10. Cervical rib.\n11. Anomalous sell turcica .\n12. Ptosis .\n13. Strabismus .\n14. Blue sclerae .\n15. Catarct .\n16. Mental Retardation .\n17. Haemangiomata of intestine .\n18. Idiopathic hypertension .\nFetal Changes :-\n1 . Hydrops fetalis .\n2 . Cystic Hygroma .\n3 . Single Umbilical artery .\n4 . I.U.G.R.\n5 . Congenital Heart Defects .\nAdult Turner?Syndrome with additional features :-\n1 . Hypertension .\n2 . Diabetes Mellitus .\n3 . Delayed Puberty , Amenorrhea ,\nMenstrual disturbances and\nHydrops in other Cytogenetic abnormalities :\nDR. L. P. Shulman from Memphis\n( U.S.A. ) published his data of 18 cases of hydrops in 1998 where hydrops was detected as space suit hydrops in the first trimester of the pregnancy and C.V.Bx or Amniotic Fluid karyotyping was done on all the patients . His results are given below :\nShulman et. al. :-\nTotal No. of cases 18\nChromosome abnormalities 15\n( 83.3 % )\nSex chromosome abnormal 7\nNormal Chromosomes 3 ( 16.7 % )\nTurner Syndrome ; 45 , XO - classical 6 / 15 ( 40.0 % )\nTurner Syndrome Mosaic ; 45 , XO / 46 , XY 1 / 15 ( 6.67 % )\nTrisomy - 21 ; 47 , XX or XY + 21 5 / 15 ( 33.33 % )\nTrisomy 18 ; 47 , XX or XY + 18 3 / 15 ( 20.00 % )\nThe ratio of 7 : 8 of sex chromosome to autosome abnormalities is greater than that observed ( approximately 1 : 4 ) among fetuses with isolated prominent nuchal translucency .\nHydrops in Syphilis :- Hydrops is quite variable in its frequency . Tan et. al. Found it in 40 % of his series , while any number of others failed to mention it at all ! The edema may be so severe that infant is thought be the product of a severely Rh sensitized multiparous mother . It is mandatory that congenital syphilis be considered as the cause in every instance of perinatal non immune hydrops .\nParvovirus Infection :- This is one of the most important viruses responsible for fetal hydrops . Human Parvovirus was first discovered in 1975 and it was first associated with adverse pregnancy outcome in 1984 . This virus is a small single stranded DNA virus , singularly dependent on host cell function . Autonomous parvoviruses including the human parvovirus B . 19 are able to replicate without the help from another virus , but only in the proliferating cells . In general , mammalian parvoviruses are species specific . To date the only human parvovirus identified is B - 19 .\nThe diseases caused by Parvoviruses are :-\n1 . Aplastic crisis in children with sickle cell disease .\n2 . Erythema Infectiosum . ( E.I. )\n3 . Febrile illness with arthargia and arthritis .\n4 . Chronic anemia in immune compromised host .\n5 . Haemo-phagocytic syndrome .\nInfection with B - 19 is relatively common , seroprevalence studies indicate that 30 % to 60 % of adults have been infected . Modes of transmission of the virus have not been completely defined but appear to include respiratory secretions . Viremia occurs approximately one week after inoculation and is accompanied by rash and joint symptoms , approximately 10 days later . There is transient reticulocytopenia . There may be slapped cheek appearance of EI .\nLaboratory Investigations :-\n1 . Serologic diagnosis - Ig G & Ig M - B - 19 specific antibodies .\n2 . Detection of DNA by P.C.R. appears to be the most sensitive test for detection of this virus .\nEffects on pregnancy and fetus . :-\nFetal infection at all stages of pregnancy has been documented with a spectrum of consequences -\n1 . Spontaneous abortions .\n2 . Still birth .\n3 . Severe non immune hydrops .\nFetal damage is not inevitable following maternal infection , and infact , a healthy infant is the most common product of a pregnancy complicated by parvovirus infection .\nThe risk of unfavourable outcome varies from less than 10 % to 38 % in different serieses published .\nThe evidence for treatogenicity is weak and till to date there is only one report of a congenital anomaly in a B - 19 infected fetus .\nIn the infected fetuses , the principal organ affected is the bone marrow . The red cell survival in fetuses is reduced from normal 120 days to between 45 and 70 days and profound anemia results from B - 19 induced erythroid bone marrow aplasia . Fetal blood sampling in one affected fetus revealed a Hb of 1.8 Gm / dL and reticulo-cytopenia . Usually the bone marrow recovers in 7 - 10 days .In immuno-immature infants prolonged infection may occur . Transfusion of infected fetuses with packed red blood cells has been successfully used to treat the intrauterine hydrops but experience is limited .\nLimited data suggests that elevation of M.S.A.F.P. values may be a marker for the development of hydrops fetalis .\nA possible immunologic origin of idiopathic nonimmune hydrops fetalis has been suggested . Of 324 cases of prenatally diagnosed NIHF , 49 (15.1 % ) could be classified as idiopathic . The proportion of parents sharing 4 or 5 H.L.A. antigens was increased significantly in the 38 patients of the idiopathic group as compared to 38 age and parity paired controls . Besides in 8 patients , an increased paternal histo-compatibility and a decreased incidence and percentage of lympho-cytotoxic antibodies was observed .\nMaternal complications of nonimmune hydrops fetalis :-\n1 . Poly-hydramnios :- commonest\n2 . Anemia\n3 . Preterm labour\n4 . Pregnancy induced hypertension\n5 . Post partum haeorrhage .\nProtocol for management of nonimmune hydrops :-\nMaternal causes :-\n1. Obstetric history\n2. Past medical history\n3. Family H/O genetic disorders\n4. Recent infections\n5. Fetal activity\na. Blood group and antibodies\nb. Viral screen / TORCH titre\nd. G.T.T / Glycosylated Hb\ne. Auto-antibody screen\nf. Kleihauer - Betke count\ng. Full blood count\nh. Hb Electrophoresis\nFetal causes :-\nU.S.G. Evaluation :\nDoppler Blood Flow Studies :\na. Umbilical artery\nb. Middle cerebral artery\nd. Umbilical artery pressure\nFetal Blood Sample :\n1. Full blood count\n2. Bl.Gr. & Coombs test\n3. Fetal blood abnormal Hb studies\n5. Blood gases\n6. Serum protein\n7. Viral screen / TORCH titre\n8. Metabolic disorders study\nAborted Fetus Or Dead Fetus :\n1 . Complete autopsy of fetus\n( if parents are willing ) .\n2 . Clinical photographs of the fetus .\n3 . X ray - complete babaygram.\n4 . Collect blood as follows -\n( Use Vacutainer tubes only ) .\n5 ml in E.D.T.A. tubes\n5 ml in heparinised tube\n10 ml in plain tube .\n5 . Two pieces of placenta of 1?1?\neach in normal saline & formalin\n( two separate sterile containers ).\nTo keep all the material at 2 - 8 centigrade temp. in fridge .\nTo inform geneticist within 24 hrs. or send the samples as eraly as possible to lab along with X-ray & clinical photographs of the fetus .\n1 . Avron Y. Sweet & Edwin G. Brown , Fetal & Neonatal Effects of Maternal Diseases , 1991 , Page no. 162 - 163 .\n2 . Duru Sushil Shah , An Introduction To Genetics and Fetal Medicine. Editor : Dr. Kamini Rao , Page no. 130 - 136 .\n3 . J.L. Simpson & Golbus , Genetics in Obstetrics and Gynecology , 2nd Edition.Page no. 61 & 187 .\n4 . L.P. Shulman & O.P. Phillips , Prenatal Diagnosis and Therapy , 1998 ,Proceedings of 8th international Conference on prenatal Diagnosis of genetic disorders ., Page no. 22 - 24 .\n5 . Mary L. Kumar M.D., Fetal & Neonatal Effects of Maternal Disease by Avron Y. Sweet & Edwin G. Brown ( 1991 ) , Page no. 39 to 44.\n6 . Smith's Recognisable Patterns of Human Malformation , 5 th edition , 1997 Page 843 - 844 .""]"	['<urn:uuid:427b6282-ec45-4eca-9fe9-ab67a4ed89f1>', '<urn:uuid:825fc625-73d2-400f-a4c1-2c832bd5f387>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-13T05:06:54.527160	16	55	4162
54	I recently learned that my friend has a weakened immune system and got some kind of brain infection from a fungus. What exactly happens when this fungus attacks someone with a weak immune system?	In immunocompromised hosts, Cryptococcus neoformans can spread through the blood to infect the central nervous system. When it reaches this stage, it causes meningitis that is uniformly fatal if untreated. In contrast, healthy individuals usually control the infection and remain asymptomatic.	"['Cryptococcus neoformans var. grubii H99 Database\nCryptococcus neoformans is an opportunistic fungal pathogen that may cause meningitis in immunocompromised individuals. Often found in soils contaminated with bird feces, C. neoformans enters its host through the lungs via inhalation of spores. In healthy individuals the infection is usually controlled and asymptomatic, but in immunocompromised hosts the fungus can spread via the blood to infect the central nervous system and cause meningitis that is uniformly fatal if untreated. Few antifungal agents exist and drug-resistant strains are emerging.\nC. neoformans, a member of the Basidiomycota phylum of the fungi, is more closely related to mushrooms than to ascomycete fungi like Saccharomyces cerevisiae or Candida albicans or filamentous fungi such as Neurospora crassa or Aspergillus nidulans.\nThe March 2013 release of gene predictions for the serotype A isolate Cryptococcus neoformans var. grubii H99 is based on a chromosome-based genome assembly provided by Dr. Fred Dietrich at the Duke Center for Genome Technology. Using RNA-seq from strand-specific and non-strand-specific libraries, we have updated the set of 6,962 predicted proteins in collaboration with Dr Guilhem Janbon at the Pasteur Institute. This work was supported by NIAID and by a grant from ANR (2010-BLAN-1620-01 program YeastIntrons) to GJ. A file mapping annotations between the current and previous versions may be downloaded here.\nSequencing Cryptococcus neoformans Serotype A\nThe Cryptococcus neoformans Serotype A sequencing project reflects a collaboration between the Fungal Genome Initiative at the Broad Institute and Fred Dietrich at the Duke Center for Genome Technology.\nThe Center for Genome Technology at Duke University Medical Center contributed ~2X coverage in plasmid reads. The Broad Institute of MIT and Harvard produced additional whole-genome shotgun sequence from 4 kb & 10 kb plasmids, 40 kb Fosmids and 110 kb BACs. The H99 BAC library was constructed by Klaus Lengeler and Joseph Heitman at Duke University, finger printed and end sequenced by Jim Kronstad and the Vancouver Genome Sequence Centre, and provided by these investigators to the Broad Institute. Genomic DNA for the H99 libraries was provided by James Fraser in the Heitman lab at Duke University. All the reads were assembled with Arachne.\ncDNA was provided by Dr. Doris Kupfer at University of Oklahoma. RNA of samples for strand-specific library construction and sequencing were provided by Yuan Chen at Duke University\nWhat is Cryptococcus neoformans?\nCryptococcus neoformans is an encapsulated fungal pathogen causing fatal meningitis in humans. The infection, initiated by inhalation into the lungs, occurs mainly in immunocompromised individuals, but can also occur in healthy individuals. Cryptococcus neoformans is usually found in tissues in the yeast form. Infection of the brain and meninges is the most common clinical manifestation. In immunocompetent individuals, the initial infection is usually controlled and asymptomatic and the organism remains dormant in a lymph node complex, much like tuberculosis. Reactivation occurs in immunocompromised hosts where the fungus can spread via the blood to infect the central nervous system. Once C. neoformans reaches this stage it can cause meningitis that is uniformly fatal if untreated. Few antifungal agents exist and drug-resistant strains are emerging.\nThere are four serotypes of C. neoformans. The serotype D was the first serotype chosen for sequencing studies at The Institute for Genome Research and Stanford Genome Technology Center because of its advanced genetic tools. However, more than 90% of clinical isolates and more than 99% of isolates from AIDS patients are of the more divergent serotype A strains. Sequencing a serotype A strain will increase our understanding of this disease and comparative studies between the serotypes A and D will help us define the genome structure and the development of pathogenicity.\nCryptococcus neoformans is unique among the most common human fungal pathogens in that it is a basidiomycete, thus it is evolutionarily divergent from the more common pathogenic ascomycetes (e.g., Candida albicans) and more closely related to wood rotting fungi (e.g., Phanerochaete chrysosporium), mushrooms (e.g., Coprinus cinereus), and plant pathogens (e.g., Ustilago maydis). Cryptococcus neoformans elaborates two specialized virulence factors, the polysaccharide capsule, which inhibits phagocytosis, and melanin, which serves as an antioxidant. The typical vegetative form of C. neoformans is the yeast form. The organism can also undergo sexual reproduction and form basidiospores. Sexual reproduction appears to occur much less frequently in nature than asexual or vegetative reproduction.\nMost isolates of C. neoformans are haploid. The size of the genome is approximately 19 Mb with 14 chromosomes. Cryptococcus neoformans has a defined sexual cycle involving mating between cells of the MATalpha and MATa types. Thus, classical genetic approaches can be applied to study this organism.\nHome page images and credits\nFrom left to right:\n- Encapsulated yeast cells\n- Sexual spores from Serotype A mating\n- Encapsulated yeast and sexual spores on defined media\n- Basidia head bearing four spores at distinct positions\n- Hyphae with a clamp cell, yeast cells and sexual spore chains\nSource: Rajesh Velagapudi, and Joseph Heitman, M.D, PhD., Duke University\nScanning Electron Micrographs taken at ""Center for Electron Microscopy"" with the help of Valerie Knowlton (NCSU).\nThe phylogenetic tree was provided by Dr. Jason Stajich, UC Berkeley. For details please see ""A fungal phylogeny based on 42 complete genomes derived from supertree and combined gene analysis"" David A. Fitzpatrick, Mary E. Logue, Jason E. Stajich and Geraldine Butler. BMC Evolutionary Biology 2006, 6:99\nFunding support for genome sequencing of H99 was provided by the National Human Genome Research Institute, the National Institute of Allergy and Infectious Disease, and by the French National Research Agency (ANR) (2010-BLAN-1620-01 program YeastIntrons).']"	['<urn:uuid:df2769c5-64f3-424e-9519-c719526f3d15>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:06:54.527160	34	41	914
55	How do Building Information Modeling and sustainable construction practices work together to improve energy efficiency and reduce environmental impact in modern building projects?	BIM and sustainable construction work together through BIM's energy modeling capabilities and sustainable construction planning. BIM helps analyze energy consumption and reduce carbon emissions through virtual modeling, allowing accurate studies of natural light penetration and energy usage. This technology aids in transforming spaces and saving energy, while sustainable construction practices focus on addressing pollution control, climate change, and resource conservation. The integration of sustainability principles into every phase of construction, combined with BIM's virtual modeling capabilities, helps improve building efficiency and reliability while meeting environmental goals.	"[""BIM 'Building Information Modelling'\n- For Residential Projects\n9 February 2022\nMaking an educated guess is not always in your best interest, seeing is believing, or the proof is in the pudding we would say. This is how we approach our architectural design processes and Design Led building/construction phases.\nWith the use of ‘BIM’ – Building Information Modeling, we can, with great results hit that nail on the head, metaphorically speaking of course.\nFirstly, let’s understand what is ‘BIM’ – Building Information Modeling and then we can move to what we use it for.\nBIM– is software, widely used in the Architectural, Engineering, and Construction industry.\nBIM– is visualization, a detailed representation of your building and all aspects, including its environment, is created\nBIM– is a management tool, using virtual building modeling to navigate all parties through the entire life cycle of your project, and beyond.\nBIM– is a database, everything regarding the building structure is shown in a virtual model, this creates the database which is used in all sectors of design, construction, operations, maintenance, renovations, alterations, and beyond.\nKnowing what BIM is, is an advantage, even though by keeping the ‘KISS’ – Keeping it simple stupid method going, we can now easily move into how we can use BIM for your residential building projects.\nThe conventional drawing methods of putting it all down on paper have their limits, although you could with somewhat accuracy complete your project with or without any problems, it was and still is very rare that you don’t have any hick-ups down the road. They come out of nowhere and especially when you least expect it. With the first implementation of using the virtual building model, that hick-ups started to become less and less.\nThe key lies with the ‘I’ in Building Information Modelling – the virtual model supplies all parties throughout the entire life cycle with the required information needed to understand and ultimately make the right decisions early, I mean before the building actually exists, in a virtual representation – not like before when you unexpectedly found surprises in your design being implemented on-site, as this is true in 99% of all architects, designers, engineers, etc. – If they agree and accept it or not, it still stands true.\nThe journey to – was and never is a straight highway to your destination, it has many deviations and detours. Having as much information at your disposal as possible makes for fewer detours and deviations (Revisions).\nNow let's look at what BIM can do for your next architectural project…\nADVANTAGES OF BIM FOR RESIDENTIAL DESIGNS\nBIM is suitable for all types of projects, of any scale and it can successfully solve the unique complications of each building sector, with BIM in residential construction and BIM in residential architecture.\nResidential owners and developers have realized that BIM, beyond a tool, is a method, which can be used throughout the practice for all types of projects of any size and design. As a result, BIM is used in more and more custom green homes projects.\nTHE SEVERAL ADVANTAGEOUS FACTORS OF BIM FOR RESIDENTIAL DESIGN:\n- BIM increases productivity with structured information\n- It improves collective understanding of design intent\n- Provides accurate working drawings\n- BIM is capable of analysis and simulation of energy, quantity, and cost (Bill of Quantities)\n- It enables better project planning and thereby ensures accuracy\n- Ensures minimum errors on-site as it uses virtual reality to model the structure\n- Central registry of all building components are available in a virtual model of your building\n- It offers cost estimates for early project certainty\n- BIM enhances the design, construction, operations, maintenance, renovations, alterations, and facility management\n- A useful tool for scheduling project construction activities\nEFFECTIVE COLLABORATION TOOL\nBIM enhances the collaborative effort of all members in a construction project including owners, architects, contractors, sub-contractors, engineers, suppliers, trade professionals, and building operations/facilities personnel. So, the right information is accessible in the right place at the right time. Therefore, all projects of any type and size -from complex commercial building structures to small residential projects, get the benefit of BIM.\nORGANIZED FLOW OF INFORMATION\nThe main feature of BIM - 3D Modeling has a significant role in improving the business and so residential owners and developers depend on visual 3D than 2D. Most of the clients find it difficult to understand the concept of a flat 2-D world, we can now view the project even before it is built. This technology also helps to avoid miscommunication, frustration, and waste.\nBIM ENERGY MODELING\nIncreasing energy prices, unpredictable climate change, and occupant health consciousness are matters of high concern around the globe, the high consumption of energy and carbon emission worldwide are the threats to our world. So, people all over the world are forced to take necessary actions to sustain the society green. As all these phenomena matter to residential, institutional, and commercial building projects, the construction industry is under great pressure for changing the current design and construction approaches.\nHere comes the importance of BIM. As this technology helps to reduce the waste and carbon emissions to the atmosphere, several construction firms found it compulsory to use the BIM tools in the construction of residential architecture projects and custom green homes.\nVirtual Building allows for an accurate study of Natural light;\nAs natural light is one of the most essential elements in architectural design, which helps us transform spaces and save energy. With every project we undertake, we strive to find ways for natural light to permeate into the interior of buildings. BIM (Building Information Modeling) allows us to accurately transform our design with the help of the software, this brings the building to life in a virtual model, we know and can show detailed models of how the natural light transforms your building's spaces. Knowing exactly what to expect and to do the necessary alterations to get the most out of the building design.\nFUTURE OF BIM\nAs BIM improves the broad-scale project ecosystem and enhances project outputs for all parties, it has growing implications in the construction industry. BIM can satisfy the unique workflow, expectations, and values proposition of each player on a construction project, as it can be applied in every situation. Integrated Project Delivery (IPD), the result of BIM application, makes a significant change in the world of the construction industry.\nWe all know that only big enterprises realized the usefulness of computers when it is introduced to the market first. But, now even small children are experts on the computer. Similarly, general contractors and sub-contractors as well as being increasingly using BIM to provide drawings and other disciplines in BIM. Both new projects and renovation projects start to apply BIM for better results.\nBIM has proved to the industry, the owners, and developers of the world that its implementation is a needed one, not only has the benefits already been showcased throughout thousands of projects globally, within all major sectors across the field – BIM is still continuously looking for new ways to improve the AEC industry environment.\nAs industry professionals, we actively encourage BIM adoption in the AEC industry for both new construction and renovation projects."", 'Benefits of Project Management to Realizing Sustainable Buildings\nSustainable materials are becoming popular worldwide in terms of ecologically friendly structures that deal with pollution controls, climate change, global temperature increase, and resource conservation issues. Consequently, the researchers believe that simply using sustainable materials to design and create a building is insufficient.\nThis article discusses several elements of sustainable construction planning including site choice mostly through the proposed project, material classification mostly through the life cycle, analysis time, cost, and commodity controls, reliability and stability, occupant health, manufacturing methodologies and procedures, and design concepts based on advanced ideas. The article argues that by integrating the choice of building components and construction strategic planning practices into the project, the productivity and consequently the reliability of the building could be significantly improved.\nThe article also discusses the advantages and requirements of sustainable construction, the managerial roles in construction procedure, management processes, and a correlation of conventional construction planning and sustainable project management. The research paper indicates that the objective of constructing an environmentally friendly building is simply fully done when the notion of sustainability is integrated into every phase of the construction approach and that the procedures are not restricted to using recycled practices.\nRobichaud, L. B. & Anantatmula, V. S. (2011). Greening project management practices for sustainable construction. Journal of Management in Engineering, 27(1), 48-57.\nMcLennan, J. F. (2004). The philosophy of sustainable design: The future of architecture. Ecotone Publishing.\nCassidy, R. (2003). White paper on sustainability. Building Design and Construction, 10.\nMcGraw-Hill Construction. (2006). Green building smart market report: Design and construction intelligence.\nAnderson, J., & Shiers, D. (2009). The green guide to specification. John Wiley & Sons.\nLee, Y. S. & Guerin, D. A. (2010). Indoor environmental quality differences between office types in LEED-certified buildings in the US. Building and Environment, 45(5), 1104-1112.\nChau, C. K., Tse, M. S., & Chung, K. Y. (2010). A choice experiment to estimate the effect of green experience on preferences and willingness-to-pay for green building attributes. Building and Environment, 45(11), 2553-2561.\nKibert, C. J. (2016). Sustainable construction: green building design and delivery. John Wiley & Sons.\nBockrath, J. T., & Plotnick, F. L. (2000). Contracts and the legal environment for engineers and architects. New York: McGraw-Hill.\nHaghighat, F. & Donnini, G. (1999). Impact of psycho-social factors on perception of the indoor air environment studies in 12 office buildings. Building and Environment, 34(4), 479-503.\nJönsson, Å. (2000). Tools and methods for environmental assessment of building products—methodological analysis of six selected approaches. Building and Environment, 35(3), 223-238.\nLee, W. L. (2012). Benchmarking energy use of building environmental assessment schemes. Energy and Buildings, 45, 326-334.\nLi, X., Zhu, Y., & Zhang, Z. (2010). An LCA-based environmental impact assessment model for construction processes. Building and Environment, 45(3), 766-775.\nZhang, T., Siebers, P.O., & Aickelin, U.S. (2011). Modelling electricity consumption in ofﬁce buildings: An agent-based approach. Energy and Buildings, 43(10), 2882–2892.\nEvins, R. (2013). A review of computational optimization methods applied to sustainable building design. Renewable and Sustainable Energy Reviews, 22, 230–245.\nKubicki, S., Bignon, J.C., & Halin, G. (2005). Assistance to cooperation during building construction stage. proposition of a model and a tool. In: International Conference on Industrial Engineering and Systems Management, Marrakech (Morocco).\nCopyright (c) 2021 International Journal of Engineering and Management Research\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.']"	['<urn:uuid:ceab8b82-e36e-4461-95e7-8b5fd8e2e66b>', '<urn:uuid:7a622597-f136-48a5-b8c1-bb1405c629dd>']	factoid	direct	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-13T05:06:54.527160	23	86	1760
56	What makes natural dwellings environmentally sustainable?	Both tipis and cob buildings are highly sustainable. Tipis use natural, biodegradable materials and have a small eco-footprint, being easily repairable and mobile. Cob construction consumes virtually no energy, produces zero CO2 emissions, and uses material that can often be dug from the building site, eliminating transportation. The materials are also completely recyclable.	"[""“Their tipis were built upon the earth and their altars were made of earth. The birds that flew into the air came to rest upon the earth and it was the final abiding place of all things that lived and grew.” – Luther Standing Bear\nWhat are tipis?\nTipis (or tepees, or teepees) are conical-shaped tents made using long poles with traditionally a skin covering. Tipis are the traditional Native American nomadic dwellings. Originally made from buffalo hide, they were quick and easy to pitch/take down and easily transportable, allowing the tribe to follow their main source of food and skins – the buffalo.\nA tipi consists of between 11 and 20 poles, depending on size, tied at the top to form the famous conical shape – but with an oval rather than circular base. It has an outer cover, traditionally skins, and now mainly canvas, which covers the whole of the outside of the frame, but with a gap of a few cm at the bottom. On the inside is a canvas liner that goes from the ground to about 2m up the inside walls. The gap allows an upward airflow, which means there can be an open fire in the middle of the tipi, and the smoke is carried up and out of the smoke flaps at the top.\nThere are many different kinds of tipi, all with minor variations according to the tribe that made them. Most are 3-pole tipis (Sioux, Cheyenne); this refers to the number of poles initially used to get the ground plan right, then more poles are added later. There are also 4-pole tipis (Blackfoot, Crow). Other variations are primarily to do with the smoke flaps – being longer/shorter or narrower/wider. The most commonly-produced tipi today is the Sioux, but often with the modification of an extended smoke flap similar to that of the Cheyenne.\nA tipi is not the same as a wigwam by the way – a wigwam is more like a bender.\nTipis are the traditional dwellings of some Native American tribes, in particular the sometimes nomadic tribes of the Great Plains and Canadian Prairies. It is worth noting that tipis are incorrectly attributed to all Native American tribes, when actually there was a wide variety of dwelling structures. A tribe could quickly dismantle their tipi when leaving, and raise a tipi at their new site. Often the women would erect the tipi, and could do so in around half an hour.\nTipis are surprisingly versatile. By adjusting the canvas, they can either keep the space cool by creating an airflow from below, or can be warm in winter. Traditionally they would be insulated in winter and are dry in heavy downpours (if put up correctly). There would be an open fire in the centre of the tipi, for keeping warm and cooking.\nWhat are the benefits of tipis?\n- It’s a very cheap form of shelter if you’re tough enough (heating and cooking is via an open fire, collecting wood, creepy crawlies, and in this country, rain and mud). A good-quality, well-looked-after tipi could easily be in constant use for 10 years before the canvas needs to be replaced (and maybe some of the poles). As you’re looking at £1500-2500 for a basic 5½m tipi (average size), that’s very cheap accommodation – even cheaper if you make your own.\n- Can be part of a very simple, back-to-nature lifestyle, with a very small eco-footprint.\n- Natural, biodegradable materials.\n- You can be mobile: tipis are easily transportable and can be put up or taken down in less than an hour.\n- Easily repairable.\n- No mortgage required.\n- They are beautiful spaces to be inside and can make a wonderful gathering space around a central hearth.\nWhat can I do?\nOnly the hard-core will attempt to live in a tipi full-time, and if so, the canvas has to be good quality and treated to be waterproof; plus the canvas could need re-proofing every few years. Most people will choose to live in them temporarily, and probably in the summer, or use them for holidays.\nThere are a number of campsites and venues offering tipi holidays. If you want one of your own, you can buy a tipi or you can have a go at building your own.\nBuilding a tipi\nYou can cut your own poles, from softwood such as spruce. The poles will taper – from around 10cm at the bottom, to almost nothing at the top. The most common tipi size is 5½m – that’s the distance from front to back at ground level, and also roughly the height the canvas reaches from the ground. The poles for a 5½m tipi will be 7-8m though – 14-17 of them. You will have to take the bark off with a draw-knife, otherwise any water that touches the poles will drip into the tipi; plus it looks nicer.\nYou can also cut and stitch your own canvas. The book The Indian Tipi explains how to make different types of tipi, including patterns for cutting canvas. Or you could make your own poles and buy the canvas.\nRain hat: stops rain from entering the tipi. It consists of a large square of canvas with a pocket in one corner, and loops to attach ropes to. It is fixed to the rain hat pole in one corner, and pegged down on the other 3 corners. You have to cut the poles down so that they are only sticking out c. 50cm over the top of the canvas; it takes away some of the beauty, but it stops you getting wet.\nSmoke flaps: flaps of canvas at the top of the tipi; they can open and close, and with a rain hat, you can have the smoke flaps wide open to let the smoke out, without allowing rain in.\nDoor: a piece of canvas held on by lacing pins.\nFloor: some people prefer the natural grass/earth, or you can use wooden decking, ground sheets, sheepskins or other skins (but be careful of the fire).\nLining: essential for insulation, and to create an updraught to take away the smoke from the fire.\nRopes & pegs: the tipi is pegged down all the way round, and the rope is for tying the poles together at the top, and can be used in conjunction with poles for opening and closing the smoke flaps.\nOzan: a partial internal ceiling, covering the back part of the tipi, to stop any raindrops falling on the sleeping area; with a good rain hat, this is unnecessary.\nPainting: Traditionally, the majority of tipis would be left plain. Some tipis were painted with designs. These designs might depict achievements, battles, hunts, or dreams and visions. Each mark had symbolic meaning and different tribes had their own styles.\nIf you are wanting to live in your tipi\nToilet: compost loo or tree bog?\nPlanning permission: you do need planning permission for a tipi if you intend to live in it; talk to your local planning officer or don’t get caught. Of course tipis are temporary dwellings – but they are tall, and visual impact is high, and likely to annoy neighbours. However, the kind of person wanting to live in a tipi is unlikely to want to do it in an urban area. If it’s on your land or you have the permission of the owner, it’s fine for 28 days in any one year, and if you manage to live in a tipi for 10 years, and apply for permission retrospectively, you’re likely to get it. As you won’t be receiving mail though, you’d have to prove it via aerial photographs or Google Earth, maybe?\nFinally, as mentioned before, tipi living is tough – think washing yourself and your clothes – stream? heating water over the fire? launderette? But hey, life is short – and what an incredible thing to do.\nThanks to David Field of World Tents for information.\nWhilst you’re here, why not take a look at the other 30+ shelter topics available? And don’t forget to visit our main topics page to explore over 200 aspects of low-impact living and our homepage to learn more about why we do what we do.\nThe specialist(s) below will respond to queries on this topic. Please comment in the box at the bottom of the page.\nDavid Field lives at Redfield Community where he runs World Tents. ‘Everything we make or sell is either natural or made from recycled materials. Some of the wood we use comes from as close as 200 metres away. All of it comes from sustainable woodland. Whilst we do offer a polyester mixed weave canvas we prefer to sell and recommend the use of the natural 100% cotton canvas.’\nThe views expressed here are those of the author and not necessarily lowimpact.org's"", 'Devon Earth Building FAQ\n1 Q. How are earth walls constructed?\nA. There are three principal building methods, all of which are based upon traditional construction techniques. Because DEBA is primarily concerned with the technique of earth building called ""cob"" which was commonly used in the past in the South West of England, this website concerns itself largely with this technique. For the sake of comparison the other two principal earth building techniques are also described below, but detailed information on these must be sought elsewhere.\n1. Cob, in which subsoil containing ideally approximately equal proportions of clay and silt, sand and fine gravel is thoroughly mixed with water and straw to a fairly stiff but malleable consistency, then built up off a masonry plinth, or underpin course, in successive layers about 600 mm high, allowing each one to dry out before the next one is laid, until eaves level is reached. Walls are normally 450 to 600 mm thick. The Cob Building tradition in Devon is described in the leaflet The Cob Buildings of Devon - [ see reading list ] on this site.\n2. Rammed earth, or pisť de terre , is formed by compacting moist subsoil inside temporary formwork (shuttering). Loose, damp (rather than wet) soil, containing no straw, is placed in layers 100 to 150 mm deep and compacted using either manual or pneumatically powered rammers. When the soil has been adequately compacted the formwork is removed, usually straight away. Construction can proceed continuously, without waiting for successive lifts to dry out. Wall thickness varies, typically, from 300 to 450 mm.\n3. Adobe or clay lump is an earth/straw mix similar to that suitable for cob construction which is placed into timber or steel moulds to form bricks or blocks which, when dried out, can be used to construct solid or cavity walls. Normal bricklaying techniques are employed but with the blocks being laid in either an earth or composite earth/lime mortar.\n2 Q. Can you mend cob?\nA. A. Certainly. Various techniques are set out in the DHBT leaflet The Cob Buildings of Devon 2 and in Earth Building by Laurence Keefe – see the reading list on this site.\n3 Q. How much does it cost to build in cob?\nA. The cost is very variable and depends largely on the labour price, as the materials are inexpensive, especially if they can be sourced on site. If they have to be brought in from elsewhere, then the cost rises proportionately.\n4 Q. How do I know if my soil is suitable for cob building? Can I get my soil tested to see if it is suitable?\nA. If you live in an area where there are existing cob buildings and your soil looks similar to that used in these, then it is likely that it will be suitable. You can always experiment to see how well it works. If however you want to embark on an ambitious project then you should get your soil tested - the following links are suggested www.buildsomethingbeautiful.com or email David Clark at Plymouth University: email@example.com\n5 Q. I have a plot of land -what would I need to do to be able to build upon it? What sort of problems are likely to arise when Planning Permission and Building Regulations approval is being sought for a new earth-walled building?\nA. With regard to Planning Permission, earth-walled buildings would be treated in exactly the same way as those with walls of masonry or timber-framed construction. Any objections raised would normally be concerned only with the appropriateness of location, design and finishes, not with construction materials and methods. Compliance with the Building Regulations can be achieved by supplying sufficient data and adequate constructional details to enable the Building Control Officer to make an informed assessment of the proposed building (over the past 15 years, numerous earth-walled buildings have been built in Britain with full Building Regulations approval). Compliance with Building Regulation Approved Documents is not necessarily mandatory if another method can be shown to be as effective in achieving the desired end. The recently revised requirements relating the thermal performance of external walls (Approved Document L1) can cause problems. However, there are various ways in which compliance can be achieved, by reducing the \'k\' value (thermal conductivity) of earth walls and/or by enhancing the insulation of other building elements. Similarly UK Building Regulation also require that:"" The walls, floors and roof of the building shall resist the passage of moisture to the inside of the building"" and this may be able to be achieved by allowing effective evaporation of ground moisture from the plinth. The various ways in which compliance with Building Regulations can be achieved are fully described in Earth Building by Laurence Keefe-see the reading list on this site\n6 Q. Where are cob buildings found?\nA. Cob buildings are found throughout the world except perhaps in the Americas. In the UK the strongest tradition of cob building is in the South West, particularly in Devon where many thousands of cob houses and farm buildings survive dating from the 14th century to the later 19th century. Cumberland also had an ancient tradition of cob building. Throughout the rest of the UK cob buildings are less common but are also to be found alongside other vernacular building techniques.\n7 Q. How much straw do you put in a cob mix?\nA. This cannot be specified precisely as it depends on circumstances. Typically it will be 2.0 to 3.0% by dry weight according to nature of the soil used. This is roughly one [small] bale of straw to a cubic metre of soil.\n8 Q. Do you use shuttering to build in cob?\nA. Shuttering can be used to build cob but it is not necessary and makes the construction process more complicated.\n9 Q. Do you add cow dung to cob?\nA. No! This is a common but untrue belief.\n10 Q. Do you know of any practical courses in the UK on cob building?\nA. Various people and organisations\nhold courses. In Devon the following contacts are suggested:\nKevin McCabe - Tel: 01404 814270 www.buildsomethingbeautiful.com\nJ & J Sharpe - Tel: 01805 603587 www.jjsharpe.co.uk\nMike Wye - Tel: 01409 281644 www.buildingconservation.com\nAbey Smallcombe - Tel: 01647 281282 www.abeysmallcombe.com\nYarner Trust - Tel: 01288 331692 www.yarnertrust.co.uk\nCentre For Earthen Architecture, Plymouth School of Architecture - Tel: 01752 233630 www.tech.plym.ac.uk/soa/arch/earth.htm\nDevon Rural Skills Trust - Tel: 01803 615634\nCentre For Alternative Technology -Tel: 01654 702400. www.cat.org.uk\n11 Q. What books should I read on cob building?\nSee the reading list on this site. There is a much more extensive reading list in Earth Building  by Laurence Keefe.\n12 Q. What are the environmental benefits of using raw (unfired) earth as a building material?\nA. The construction process consumes virtually no energy and produces no environmental pollution (zero CO2 emissions). The material is eminently recyclable and is thermally efficient. It can often be dug from the building site eliminating transportation.\n13 Q. Are earth walls really durable, and as structurally sound as those of conventional masonry construction?\nA. Yes. Provided they are properly maintained, and protected from the effects of rising and penetrating dampness, well-built earth walls can last for hundreds of years (there are, in south west England and Cumbria, cob buildings that are known to date from the 14th and 15th centuries).\n14 Q. What are the appropriate external and internal finishes for cob buildings?\nA. This topic is covered fully in the ""Appropriate Plasters and Renders etc"" publication which can be found on this website. The fundamental principle is to achieve vapour permeability in finishes on earth buildings to avoid saturation. DEBA for this reason advises strongly against the use of renders and plasters which incorporate Portland cement as this heavily reduces or prevents such permeability. Similarly the use of traditional finishes such as limewash or distemper is to be preferred.\n15 Q. Would a damp-proof course need to be incorporated in a new earth wall? Are damp-proof courses appropriate in existing cob walls?\nA. Traditionally, earth walls relied on the fact that, being of ""breathable"" construction, rising dampness was drawn out by evaporation through external wall faces. However, compliance with Approved Document C of the UK Building Regulations would probably require the installation of a dpc in a new masonry plinth (underpin course). Chemical injection could be appropriate in the very rare occasion where there is a solid brick plinth [although there is doubt about its efficacy even in these] under a cob wall but it is not appropriate for stone plinths in which voids make the injection application patchy and ineffective. Moreover specialist dpc installers also require the plinth to be rendered with a 3:1 sand:cement mix after they have carried out the injection and this drives the rising dampness of the failed areas into the base of the cob. Modern tanking systems can also cause damp to rise into the cob if the plinth is low.\n16 Q. Do earth walls need to be protected from the effects of weathering and erosion?\nA. The essential protection for earth walls is provided by the building’s roof and there is no absolute necessity to protect the walls. In the past, cob farm buildings in particular were often not rendered externally. However, other than in cases where, for example, buildings are of only one storey and have deeply over-hanging eaves together with a high masonry plinth (in excess of 600 mm), some form of protective wall covering is normally recommended. It is essential to use renders, plasters and paints that are vapour-permeable. Advice concerning appropriate finishes for earth walls can be found in DEBA leaflet ""Appropriate Plasters, Renders etc"" on this website.\n17 Q. Are cob buildings more prone to structural failure than other types of traditional construction?\nA. Cob buildings are not more prone to structural failure unless they are misunderstood and in consequence not looked after properly. Because they are susceptible to the effects of excessive moisture, it is essential to ensure that correct finishes are used, as when cement rich renders are applied, rising dampness can be driven into the base of the cob, especially where the plinth is low and sheer failures may then occur as the cob becomes saturated and loses its coherence. Again, when gutters and downpipes are not maintained and water flushes down cement-rich renders, penetrating cracks, moisture is trapped and failures occur.\n18 Q. What are the thermal/insulation properties of cob?\nA. Cob is well know for being a very comfortable material in which to live; in part this is due to its excellent thermal properties, being a combination of high Thermal Resistance and Thermal Storage and good humidity regulation.\nHowever, from a building regulation point of view (in the UK) only Thermal Resistance is taken into account. An 860mm thick cob wall, lime plastered and rendered, bringing the total thickness to 900mm, [which is much thicker than a traditional cob wall] will give a thermal resistance of 0.45w/m/k. In practice in a new build situation it is usually necessary to put extra insulation in the plinth to compensate for this perceived inadequate value.']"	['<urn:uuid:deb135df-a49a-4457-8f7c-d051b0e5e770>', '<urn:uuid:2f42827f-23d6-4132-87c1-241d69b02f5b>']	factoid	direct	concise-and-natural	distant-from-document	three-doc	expert	2025-05-13T05:06:54.527160	6	53	3339
57	music expert here explain bass line glory box unique	The bass line in 'Glory Box' is notable for its chromatic movement while maintaining static harmony around it. It descends stepwise from the I- chord to ♭VIMaj7, creating a progression that combines with minor chord voicings from the electric piano and bluesy string riffs. The bass line employs a variation of a line cliché technique, where chromatic movement occurs against stationary chords. What makes it particularly evocative is how the B♭ and G♭ notes remain present in all four chords while the bass moves, creating a metaphorical effect of one element staying in place while another moves away.	['+ This is an excerpt from Soundfly’s online course, The Creative Power of Advanced Harmony. To access the rest of this lesson, plus hundreds of videos and tutorials on theory, songwriting, composing, beat making, mixing and more, subscribe here.\nIn diatonic harmony, we usually only have seven different notes to choose from in our progressions. In some ways, that can be like painting using only primary colors.\nEmbracing chromaticism gives us the full keyboard at our disposal. But having that much choice can also drive you crazy. You can literally play any note together; so, how will you ever choose?\nOne answer to that question is to see how other creative artists approach chromatic movement in their music, and today we’re zooming in on Portishead. Let’s check it out.\nChromatic Voice Leading\nThere are a lot of cool sounds you can get from chromatic melodies and bass lines, and one way to bring that chromatic movement into stark relief is to keep much of your harmony around them the same. This is something that Portishead does masterfully in their track “Glory Box,” off of Dummy.\nThis song is all about that bass line, which descends stepwise down from the I– chord to ♭VIMaj7, a progression that would be pretty standard except for that one little chromatic twist.\nThe inherent minor-ness of the song is immediately apparent and invokes sorrow and sadness. The tragedy of the bass line alone transcends its musical context to tell its own story. However, combined with the minor chord voicings of the electric piano and the bluesy riffs of the strings and vocals, a story unfolds around this semi-chromatic riff with a blues as deep as any other.\nThis is a super classic and deeply explored but never exhausted riff. It builds on the idea of something called a line cliché, when you ascend or descend chromatically against a single stationary chord, like in the “James Bond Theme,” for example.\nThe main harmony stays the same, but a line on top or below moves up or down chromatically.\n+ Read more on Flypaper: “How John Williams Implies Modality Within Diatonic Harmony in ‘Yoda’s Theme’”\nHow This All Works\nWhen we add the chords atop that bass line, notice one other evocative quality of this progression — the B♭ (which is the top-note voicing on three of the four chords) and the G♭ is present here in all four chords. Metaphorically, it’s as if someone is staying in one place while the other is walking away. The motion of the voices mimics the widening abyss between the two of them.\nDownload: “Glory Box” Bass + Chords MIDI\nThere are a few cool moves happening here that all stack up to make this bass line and top-note fit:\nDescending Into Inversion\nThe E♭ minor falls down to G♭/ D♭, keeping that B♭ in the top note, but allowing the bass line to descend mournfully away.\nThe Modal Interchange Moment\n(Modal interchange is when you temporarily borrow a chord from a different key or mode. It’s something we go into much more detail about in the course. To learn more, subscribe to the full advanced harmony course here).\nThat C–7♭5 chord is borrowed from the key of D♭ major, or, in this case, the mode E♭ Dorian. Given the fact that the chord is dictated by the chromatic motion of the bass, this chord feels almost like accidental interchange. We call it such in analysis, but it’s simply responding to the chromatic riff. It allows for a beautifully tense moment, almost as if a question is poised in the air.\nThe Chromatic Modal Interchange Resolution\nFrom the C–7♭5 chord, only the bass line moves down to the B natural, building that beautifully coarse BMaj7 chord. In this key, we analyze it with the enharmonic (same-sounding note, spelled a different way) root note of C♭.\nAll of this beautiful dissonance, contrast, and tension is built around that one, simple, chromatic bass motion.\nChromaticism as Passing Chords\nThis song has introduced us to one of the most common uses of chromaticism you’ll hear in the world, as passing chords between two diatonic chords. Basically, a passing chord is any non-diatonic chord that connects two diatonic chords. In this case, all the chords of the progression are diatonic to E♭ natural minor except for that C-7(♭5).\nThe simple reason we mention this is because passing chords gives us another tool to complicate our progressions. We can take any classic diatonic progression and add chromatic passing chords between the chords to complicate it, keeping our voice leading in mind as we do so.\nDon’t stop here!\nContinue learning about music theory, composition, arrangement, harmony and chord progressions with Soundfly’s in-depth online courses, like Unlocking the Emotional Power of Chords, Introduction to the Composer’s Craft, and The Creative Power of Advanced Harmony. Subscribe for unlimited access here.']	['<urn:uuid:24f97815-1546-4f7b-aaca-2591f5a0b6c5>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	9	98	813
58	historic idaho theater preservation awards recognition	The Panida theater has received several awards including the 'Orchid Award' for historic preservation from the Idaho Historic Preservation Council, 'Take Pride in Idaho' and 'America' awards, and special recognition from the governor of Idaho, the Idaho Commission of the Arts, the Idaho Centennial Commission and the U.S. Department of the Interior.	['The Panida theater opened as a vaudeville and movie house in 1927. Then, as now, its name reflected its mission: to showcase great performers and performances for audiences of the PANhandle of IDAho.\nFeaturing local and global film festivals, movies and presentations, local dance recitals and professional dance performances, comedy, live theatre, educational and informative speakers and presentations, local and touring artists for music concerts, and more.\nOpening night patrons marveled at both the distinctive architecture and lush interior furnishings of the Panida, and the press praised its Spanish Mission styling for beauty which is unique to any other building In the city of Sandpoint, and which can be equaled by few theaters in the west. Decades of glory faded into years of neglect before, major fundraising and restoration efforts by the Sandpoint community repopened the Panida In 1985. Today, with projects to preserve and improve the theater ongoing, the Panida is on the National Register of Historic Places and has received special recognition from the governor of Idaho, the Idaho Commission of the Arts, the Idaho Centennial Commission and the U.S. Department of the Interior. Honored with both “Take Pride in Idaho” and “America” awards, the Panida has also received the coveted “Orchid Award” for historic preservation from the Idaho Historic Preservation Council.\nThe Panida is the cornerstone of cultural activities for the entire Sandpoint community, and its successful restoration has inspired similar projects throughout the region. Whether hosting local performers, recording stars such as Bonnie Raitt, Arlo Guthrie, Wynton Marsalis and Mitch Miller, or internationally renowned artists like Gunther Schuller, the Spokane Symphony, San Francisco Opera, the Cavini String Quartet or the Paris Chamber Ensemble . . . the Panida theater continues to bring the best in entertainment to the people of North Idaho. What lured audiences to the Panida for vaudeville shows and the “talkies” still brings crowds in today: the promise of performance. For nearly three-quarters of a century, the Panida has made good on that promise. Its easy to see – and hear – why the Panida is so treasured by Sandpoint. There’s truly not a bad seat in this house that can hold 500. And its acoustic perfection makes the Panida a delight for performers and audience-goers alike. From intimate recital to Broadway-size musical, a 50’ movie screen and updated sound board make for epic film presentations, educational talks and presentations in a grand setting, dance recitals to master ballet performance, it’s an ideal venue. That’s why so many touring troupes go out of their way to book the Panida, bringing arts and entertainment to Sandpoint that would otherwise be available no closer than Spokane.\nAs the area’s only full feature performing arts facility, the Panida enjoys an exceptionally high audience rate for its audience base: in a community of just 7900, the house size averages nearly 200 at each of over 362 available calendar days for performances scheduled annually. With an emphasis on bringing in a broad variety of arts and entertainment the Panida continues to be the heart of our downtown, our community and the gem of north Idaho. A 501c3 nonprofit they rely heavily on in-kind and financial support to keep the heart beating to benefit all.\nThank you! Your submission has been received!\nOops! Something went wrong while submitting the form']	['<urn:uuid:4a138b73-c899-4d26-8003-fb41fd69e86e>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	6	52	551
59	safer gene therapy methods animal studies and sports doping risks	Gene therapy has evolved to become safer through improvements in technology, particularly with adeno-associated viruses (AAVs) replacing earlier adenovirus vectors. Animal studies have shown successful results, such as improved muscle mass and strength through IGF-1 gene transfer. However, gene doping in sports poses significant risks, including dangerous side effects like insertional mutagenesis, tumor development, and immune responses. Additionally, gene doping is particularly concerning because it is virtually undetectable in standard tests, making it a major challenge for sports organizations to protect athletes' health and maintain fair competition.	"['It is 21 years since the death of teenager Jesse Gelsinger nearly obliterated the field of gene therapy. In 1999, Jesse received a dose of the ornithine transcarbamylase gene, engineered into a recombinant adenovirus, at the University of Pennsylvania. The idea was for the gene to zero in on liver cells. However, soon after the treatment, he developed jaundice, inflammation, and multiple organ failure. Within four days, Jesse was dead.\nSince then, genetic technology has made gigantic leaps forward. The human genome has been sequenced in its entirety, and the cost of sequencing has plummeted 100,000-fold. CRISPR-Cas9 has emerged as a precise and efficient tool to edit genes. Artificial intelligence and deep learning are being put to use in designing molecules. And gene therapy has morphed from a blacklisted experimental concept to a life-altering practical solution for a growing list of previously incurable diseases.\nTo date, the U.S. Food and Drug Administration (FDA) has approved five gene therapy products for a half-dozen diseases (Table). Earlier this year, the FDA announced that it “anticipates many more approvals in the coming years, as there are more than 900 investigational new drug applications for ongoing clinical studies in this area.”\nThe ideal target for gene therapy is a Mendelian disease caused by defined single-gene mutations, such as hemophilia or Duchenne muscular dystrophy. That has been its initial focus, bolstered by approvals of drugs such as Luxturna (for a type of Leber’s congenital amaurosis, an inherited blindness disorder) and Zolgensma (for spinal muscular atrophy).\nNow, gene therapy is taking on more complex, multigene diseases such as central nervous system diseases, neuropathic pain, sleep apnea, and cancer. Beyond correcting genes, gene therapy is revolutionizing cell-specific delivery of therapeutic proteins and synthetic drugs, as well as inspiring basic research into unanswered questions on immunogenicity. But lest anyone think gene therapy has solved all of its earlier issues, the deaths last summer of two young boys in an experimental trial for a rare muscular disorder remind us that the field still has a lot of work to do to ensure gene therapy can be safely delivered for all patients.\nThe first order of business is to deliver therapeutic gene products into target cells. Vehicles used to transport genes into the cell include genetically modified viruses for long-term genomic integration or lipid nanoparticles for more transient tasks such as expressing genome editing nucleases.\nThe reason for gene therapy’s healthier outlook in 2020 is “improvements in the technology of gene transfer,” says James M. Wilson, MD, PhD, director of the Gene Therapy Program at the University of Pennsylvania. “Our work with adeno-associated viruses and the work of a number of other investigators, including that of Luigi Naldini, MD, PhD, in creating lentiviral vectors, were fundamental advances that paved the way for everything else.” He credits the clinical successes of his Penn colleague Jean Bennett, MD, PhD, on retinal blindness; the work of Nationwide Children’s Hospital’s Jerry R. Mendell, MD, on spinal muscular atrophy; and “the pivotal work on hemophilia out of St. Jude’s.”\nThe former vector favorite, adenovirus, has been largely supplanted by adeno-associated viruses (AAVs), generally used for in vivo gene therapy, and lentiviruses, used to modify cells outside the body, such as chimeric antigen receptor (CAR) T cells in the treatment of cancer. These ex vivo engineered cells are then amplified in tissue culture and returned to the patient.\nAAV-based gene therapy has made great strides in recent years. Pioneering work from the lab of Terence R. Flotte, MD, dean, provost, and executive deputy chancellor of the University of Massachusetts Medical School, has propelled clinical trials for two classic Mendelian disorders, alpha-1 antitrypsin (AAT) deficiency and Tay-Sachs disease. With excellent safety and durability established in a five-year follow-up of its Phase IIa trial, the AAT gene therapy is now in its final investigational new drug–enabling toxicology study stage.\n“AAT trials have shown some unusual tolerogenic properties,” says Flotte. “The study shows patients develop regulatory T cells against AAV coat proteins that prevent them from reacting negatively even without pharmaceutical immune suppression.”\nNearly 75% hemophilia patients take regular intravenous infusions of a recombinant factor VIII (F8) protein coupled with other drugs that prevent bleeding or the breakdown of clots. “Current treatment for hemophilia A is very burdensome to patients,” says Bettina M. Cockroft, MD, chief medical officer at Sangamo Therapeutics.\nA new therapy being developed by Sangamo and Pfizer injects a recombinant AAV6 encoding F8, which could provide a permanent solution. “The gene therapy for F8 shows lasting activity,” Cockroft reports. “Five patients tested in Phase I/II of the clinical trial did not need F8 infusions and had no bleeding events.”\nVectors injected into blood generally don’t get across barriers. Wilson’s lab, in collaboration with Amicus Therapeutics, is focusing on developing penetrating next-generation AAVs that deliver more efficiently to cells in organs such as the muscle, heart, and brain. “With the new AAVs,” notes Wilson, “you can get more delivery with less vector. Less vector would mean less toxicity.”\nPatient and public support for gene therapy is crucial for its advance. “I have a pretty good idea of how stakeholders view gene therapy,” Wilson asserts. “Because gene therapy is primarily focused on treating and potentially curing diseases of significant unmet need that are disabling or lethal, I see incredible enthusiasm for us to move forward.” Public and private ventures have invested heavily in the promise of curing genetic disease by altering or replacing faulty genes.\nThe change in the academic, industrial, and public outlook on gene therapy is readily appreciable in the reaction to the recent tragic deaths of two young boys in a gene therapy trial, sponsored by Audentes Therapeutics, for X-linked myotubular myopathy (XLMTM), despite impressive efficacy in preclinical studies, as discussed in a recent episode of GEN Live (“Gene Therapy: What’s Up With AAV?”). Instead of blacklisting the entire field of gene therapy as was seen decades earlier, the trial has been put on temporary hold while researchers reevaluate data so that such fatalities can be avoided in the future.\n“Audentes has elaborated further on the data surrounding the deaths in a recent letter to Human Gene Therapy,” says Flotte, the journal’s editor-in-chief. The presence of comorbidities, particularly liver disease, and prior exposures to the virus in these patients might have played a role, in addition to technical challenges, such as the lack of standardized assays for measuring levels of preexisting viral antibodies. Ongoing preclinical investigations are focused on establishing safer doses and understanding immune mechanisms in response to AAV that might exacerbate existing liver dysfunctions.\n“In the long term,” says Nicole Paulk, PhD, assistant adjunct professor, biochemistry and biophysics, UCSF, “we need to shift our focus from ‘How can we safely use high doses?’ to ‘How can we design the vector so we don’t have to?’” Limiting the viral dosage necessary for maximal efficacy in gene therapies is being evaluated through experimental approaches, such as engineering more penetrating capsids, introducing regulatory elements for tissue-specific viral expression, developing better manufacturing and purification processes to increase the purity of virus injected, and devising strategies to avoid immune responses.\nMore than correcting genes\nFuture applications of gene therapy technology for more common, polygenic diseases will not necessarily treat defective genetic components directly. Instead, these applications may improve the delivery of proteins that are otherwise used to treat diseases. An example is RGX-314,a developed by RegenXbio, for the treatment of neovascular age-related macular degeneration, where unwanted blood vessels form in the retina that result in blindness. This gene therapy uses a NAV AAV8 vector that encodes a vascular endothelial growth factor (VEGF)-neutralizing monoclonal antibody fragment. A one-time subretinal injection of RGX-314 is enough for the sustained synthesis of anti-VEGF therapeutic protein and circumvents the need for repeated intravitreal injection VEGF-neutralizing proteins.\nModifier gene platform:\nOphthalmologist Mohamed Genead, MD, chair of Ocugen’s retina scientific advisory board, is excited about a revolutionary new approach—the modifier gene platform—that takes a “gene-independent approach” by overexpressing upstream regulatory genes for nuclear hormone receptors (NHRs) in retinal cells instead of attempting to correct mutations in individual genes.\n“Overexpressing NHRs does not correct the gene mutations; instead, it corrects the disease phenotype,” says Genead. “Conventional gene therapy has several limitations. The amount of DNA you can package into the virus is restrictive. Gene therapy can work if you have identified the mutation, but most genetic diseases do not have identified mutations.”\nNHR overexpression regulates endoplasmic reticulum stress to promote normal function in cells with defective genes. “Its efficacy remains to be shown in the clinical trials,” comments Genead, “but if we see in patients what we see in animal models, there will be a significant shift of the treatment paradigm in patients with retinal degeneration.”\nContinuous positive airway pressure machines or surgery are the currently available treatments for obstructive sleep apnea, a condition where the tongue loses muscle tone and blocks the upper airway in sleeping patients. Consequences range from fatigue and metabolic disruptions to stroke and death.\nAn alternative approach is being explored by researchers at Johns Hopkins Medical School. They are evaluating whether DREADDs—designer receptors exclusively activated by designer drugs—can be used to activate the genioglossus muscle, and thereby improve breathing during sleep.\n“We are inserting designer receptors for synthetic drugs in specific brain cells to maintain patency of the airways,” says Thomaz Curado, MD, PhD, research associate at Johns Hopkins. “If translated successfully in human clinical trials, this can mean a one-time injection combined with a nightly pill will eliminate risk of death due to obstructive sleep apnea.”\nConferring resistance to infectious diseases:\nSingle-gene disorders and infectious diseases are being treated using the same tools and concepts. “We are in the midst of a very aggressive program in using AAV as a way to prevent infections of COVID-19,” says Wilson. Essentially, AAV vectors are, as usual, being used to engineer cells in patients to express proteins. In this program, however, the proteins confer resistance to COVID-19.\nAAVCOVID, a gene-based vaccine that will deliver genetic sequences of the SARS-CoV-2 in an AAV capsid, is being developed jointly by the Gene Therapy Program at the University of Pennsylvania and the Massachusetts Eye and Ear and Massachusetts General Hospital, and it is scheduled to enter clinical trials later this year.\nSafety, success, and staying on\nLacunae in the basic understanding of how the immune system responds to foreign vectors are currently at the forefront of investigations on toxicity and inflammation induced by gene therapy.\n“What factors predispose complement activation and the hemolytic-uremic syndrome at high-doses of rAAV? What factors predispose to inflammation in the dorsal root ganglia and spinal cord after direct rAAV-mediated gene delivery into the central nervous system? What is the nature of liver toxicity in patients treated with high doses of rAAV gene therapy? [These] are the frontier questions,” says Flotte. “How significant are immune responses to Cas9 and other nucleases? How important is the toxicity from off-target gene editing effects? [These] are questions facing gene editing in the context of gene therapy.”\nOne of the limitations of the ubiquitous AAV vector has been the mounting of an antibody response against the vector itself, according to Federico Mingozzi, PhD, chief scientific officer at Spark Therapeutics. Hence the interest in imlifidase (IdeS), an endopeptidase that degrades circulating antibodies without disrupting B lymphocytes that produce antibodies. The enzyme, already in the clinics for transplantation patients, has two main benefits for gene therapy. It can be used to treat seropositive patients, and it permits the re-administration of the gene therapy product if the need arises.\n“Once you give IdeS, you see a sharp and immediate drop in circulating antibodies. They stay low for 3–4 days and then go up again,” says Mingozzi. “Of course, it has to be tested in humans in conjunction with gene therapy, but it has a very good safety profile.”\n“The reality is that we are at the beginning of a revolution,” declares Wilson. “The forward move for gene therapy is to make it safer and more effective.” Making it durable depends on the context but depends on three key factors: The genome must be stable, the cell has to persist, and therapeutic gene expression cannot be turned off. “Of these three,” says Wilson, “I think the major challenge is going to be to assure that its expression is ongoing.”\n“Durability is a major consideration for rAAV gene therapy and is intertwined with immune and inflammatory toxicity,” adds Flotte. “It is less of a concern in gene editing, which involves permanent changes in the genome.”\nAccording to Laura Hercher, director of student research, Human Genetics Program, Sarah Lawrence College, “the main roadblock is going to be price and accessibility. Unfortunately, in countries where there isn’t that kind of money to spend on a single patient, it is only going to be available to the very wealthy.”\nA new nonprofit called the Institute for Life Changing Medicines aims to secure, develop, manufacture, and distribute life-changing medicines such as gene therapy at cost. “Ultimately, we want to provide access in countries where the diseases might not be rare, but the governments who are responsible for reimbursements simply won’t pay,” says Wilson. “I’m going to do everything I can to the last day I’m on my job to assure [global access]. What we are trying to do to assure global access is to be very efficient in clinical development and manufacturing to contain cost.”\nFollowing a No Disease Left Behind strategy, the nonprofit is currently committed to providing gene therapy solutions for Lesch-Nyhan syndrome, aromatic L-amino acid decarboxylase deficiency, and Crigler-Najjar syndrome type I—diseases where gene therapy has a proven transformative effect.\naThe original text left the mistaken impression that RegenXBio produces Lucentis. The text has been modified to make clear that RegenXBio produces RGX-314. Lucentis (also known as ranibizumab) is a similar but not identical anti-VEGF produced by Genentech/Roche.', 'E-mail: andrebairros yahoo. Gene doping is characterized by non-therapeutic use of cells, genes and genetic elements, or modulation of gene expression with the aim to increase sports performance. This can only be accomplished through gene manipulation. This doping practice is characterized as virtually ""undetectable"", which represents new challenges for analytical detection.\n|Published (Last):||20 January 2009|\n|PDF File Size:||16.74 Mb|\n|ePub File Size:||20.16 Mb|\n|Price:||Free* [*Free Regsitration Required]|\nIn the past few years considerable progress regarding the knowledge of the human genome map has been achieved. In vitro studies improve the production of human recombinant proteins, such as insulin INS , growth hormone GH , insulin-like growth factor-1 IGF-1 and erythropoietin EPO , which could have therapeutic application.\nUnfortunately, genetic methods developed for therapeutic purposes are increasingly being used in competitive sports. Some new substances e. The use of these substances may cause an increase of body weight and muscle mass and a significant improvement of muscle strength.\nAt the same time, anti-doping research is undertaken in many laboratories around the world to try to develop and refine ever newer techniques for gene doping detection in sport. Thanks to the World Anti-Doping Agency WADA and other sports organizations there is a hope for real protection of athletes from adverse health effects of gene doping, which at the same time gives a chance to sustain the idea of fair play in sport.\nWith the development of science, athletes enjoy the more modern methods and pharmacological agents supporting their physical fitness, muscle strength and improving athletic skills. Now, with the completion of the Human Genome Project HUGO Project and the development of gene therapy in medicine, there has been dynamic progress of research on gene doping and gene delivery technologies to improve athletic performance in various sports.\nIn WADA clarified the type of manipulation of genetic material prohibited in sport as the transfer of nucleic acids or their analogues into cells and the use of genetically modified cells [ 9 ].\nGenetic material can be introduced into a cell either in vivo or ex vivo. The in vivo strategy is direct gene delivery into the human body, i. In indirect DNA transfer strategy, i. In gene therapy and, similarly, in gene doping the genetic material is delivered into cells and tissues using various carriers that can be viral or non-viral [ 10 ]. Using viral vectors attenuated retroviruses, adenoviruses or lentiviruses a transgene is released in target cells and is expressed using cell replication machinery.\nSome of these viruses, such as retroviruses, integrate their genetic material with chromosomes of a human cell. Other viruses, such as adenoviruses, introduce the transgene into the cell nucleus without chromosomal integration [ 11 ].\nIn some cases, however, irreversible side effects, such as unexpected endogenous virus recombination, may occur. It leads to the rapid transformation of normal cells in vitro as well as initiating tumours in vivo via amplification of the host proto-oncogene sequences in the viral genome [ 10 — 13 ]. Additionally, viral vectors can be recognized by the host immune system, resulting in an increased immune response.\nThis effect reduces the effectiveness of the transfection efficiency by reducing the efficiency of the subsequent transgene delivery. The most important biological properties of the viral vectors used in gene therapy, including the treatment of sports injuries, are shown in Table 1.\nNon-viral gene delivery techniques are less effective methods of introducing genetic material into human cells, though characterized by low cytotoxicity. Non-viral gene delivery systems may cause an increased immune response [ 19 — 20 ]. Physical methods of gene delivery allow DNA transfer into the cell cytoplasm or nucleus, through local and reversible damage of the cell membrane.\nThe most common physical technique is electroporation, based on the application of a high voltage electrical pulse to the cells, leading to the formation of hydrophilic pores in the cell membrane, of several nanometres in diameter [ 15 ]. Electroporation is a very effective method, and one of its strengths is the protection of cells against the introduction of undesirable substances during the transgene delivery.\nNowadays, electroporation is the most frequently used method to introduce DNA into skin cells or liver cells. Biochemical methods involve the use of chemical carriers, which form complexes with nucleic acids to neutralize their negative charge.\nSuch complexes are introduced into the cell by phagocytosis, and less frequently by fusion with the cell membrane. Some of the chemical carriers facilitate the release of nucleic acid into the cytoplasm from the endosome, and protect it from cellular nucleases [ 15 ].\nThe main difficulty in the application of gene transfer in gene doping is to achieve a long-lasting effect, as well as monitoring the changes induced in the genome. A long-lasting effect can be achieved by multiple repeated gene doping applications or by the integration of a transgene into the chromosome.\nHowever, it should be emphasized that the integration of gene transfer vectors is associated with a risk of undesirable side effects, including insertional mutagenesis. Integration of the transgene at the wrong site may lead to the development of cancerous cells [ 21 ]. Research on gene doping, which has been carried out mainly in animal models, but also more and more often as gene therapy in humans, has brought many successes. It was reported that injection of a plasmid with a vascular endothelial growth factor VEGFA gene into the muscle of patients with chronic critical limb ischaemia led to improved distal flow [ 22 ].\nIn rats, the introduction of the insulin-like growth factor-1 Igf1 gene in a recombinant viral vector led to an increase in muscle mass and strength and to increase in endurance [ 24 ].\nTransfer of the phosphoenolpyruvate carboxykinase Pck1 gene resulted in increased activity of the transgenic mice, increased strength and speed during a race, and additionally, the mice were characterized by lower mass and fat content as compared to control mice [ 25 ]. In other studies on animals, gene therapy was used to increase the production of growth hormone GH.\nBy intramuscular injection of a plasmid containing the somatoliberin Ghrh gene, under the control of a muscle-specific gene promoter, increased concentrations of GH and IGF-1, and improvement of anabolic and haematological parameters were achieved. Moreover, the obtained results persisted for over one year [ 26 ]. The results of such studies are a major cause for concern over the direct threat of the spread of gene doping in competitive sports. Another problem — which is a priority for sport organizations — is the difficulty in detecting gene doping.\nSo far, the attempts to standardize the ideal test that could be used to detect gene doping have failed [ 5 — 6 ]. It should be emphasized, however, that several intensive studies on a number of promising strategies are being carried out e.\nLack of tests to detect gene doping is associated with the fact that the protein produced by the foreign gene or genetically manipulated cells will be structurally and functionally very similar to the endogenous proteins. Most transgenic proteins, especially those that enhance muscle strength, are produced locally in the injected muscle and may be undetectable in blood or urine.\nThe only reliable method would require a muscle biopsy, but such an approach is virtually impossible to use in sport. Furthermore, gene expression can be modulated as desired using the appropriate pharmacotherapy.\nIt is also associated with a high risk of danger to the health of athletes. Of course, the full list is much longer. Functional protein products of those genes are related to specific increase of endurance, physical strength, redistribution of fat or increase of muscle mass.\nIn addition, gene doping takes into account the genes encoding the peptides that relieve pain e. The EPO gene encodes a glycoprotein hormone that increases the number of red blood cells and the amount of oxygen in the blood, thereby increasing the oxygen supply to the muscles [ 29 , 43 ].\nThe expected effect of the physiological expression of the EPO gene would be increased endurance. For gene doping, an additional copy of the EPO gene may be introduced into the athlete\'s body using a viral vector, thus leading to the overexpression of EPO , increased production of red blood cells in the liver and kidneys, and to increased oxygen binding capacity of the blood. Physiologically dangerous side effects of doping with EPO transfer are primarily an increase in haematocrit, which may enhance the likelihood of stroke, myocardial infarction, thrombosis and an increase in total peripheral vascular resistance [ 29 ].\nIn , the British pharmaceutical company Oxford BioMedica developed Repoxygen as a potential drug for the treatment of anaemia associated with chemotherapy used in kidney cancer. The drug is administered intramuscularly, and consists of a viral vector transferring the modified human EPO gene under the control of genes encoding proteins of oxygen homeostasis e. EPO transgene is expressed in response to low levels of oxygen, and is turned off when the oxygen concentration reaches the correct value.\nIn , Repoxygen attracted the attention of the world of sports, when in Germany it began to be administered to young female runners to maintain constant expression of EPO in muscle cells.\nErythropoietin was the first recombinant haematopoietic growth factor produced and available commercially as a recombinant protein drug [ 30 ]. The Sydney Olympics marked the beginning of the use of effective methods to detect injected rEPO. This method would be relatively safe, because the effect of its actions would be limited to the target muscles.\nIt has been shown that overexpression of IGF1 and its protein product combined with increased resistance training induced greater muscle hypertrophy [ 24 ].\nAdditionally, studies have shown that IGF1 gene transfer enabled the regeneration of skeletal muscle following injury and was more efficient than systemic administration of its protein product [ 33 ].\nIGF1 expression is associated with increased muscle size and weight, thereby increasing muscle strength [ 2 ]. However, IGF1 delivery may lead to profound hypoglycaemia, similar to the administration of insulin. IGF1 protein, also known as somatomedin C, belongs to the group of polypeptide hormones, which are essential for proper development of the fetus. In the mature organism it is involved in the regeneration of tissues, especially connective tissue, and also exhibits insulin-like activity, e.\nIGF1 mediates some anabolic processes of growth hormone. One of the main functions of GH — mediated also by IGF1 — is the stimulation of body growth and body weight.\nGH also affects carbohydrate metabolism stimulation of glycogenolysis and increased glucose release from the liver , fat metabolism increased lipolysis and decreased lipogenesis and protein metabolism increased protein synthesis [ 38 ].\nThere are only a few published reports confirming the enhancing effects of GH on muscle strength and cardiovascular and respiratory functions in trained healthy individuals [ 39 — 40 ]. On the other hand, evidence of the health risks associated with the use of GH e.\nGH overexpression is associated with intracranial hypertension, headache, peripheral oedema, carpal tunnel syndrome, joint and muscle pain, or cardiomegaly in trained persons [ 41 ]. However, there is anecdotal evidence that recombinant GH rGH is commonly abused by athletes. The HIF-1 gene encodes proteins involved in the process of hypoxia, angiogenesis and erythropoiesis activation or regulation of glucose metabolism.\nDoping associated with stimulation of HIF-1 expression under normal conditions of oxygen supply, e. On the other hand, it affects mitochondrial oxygen metabolism, while also stimulating genes associated with metabolic adaptation of cells e. These molecular changes in the cells may result in myocardial infarction, stroke or cancer.\nHIF-1 regulates oxygen homeostasis, thus facilitating the cell\'s adaptation to low oxygen conditions. HIF-1 also affects erythropoiesis, iron metabolism, pH regulation, apoptosis, cell proliferation and intracellular interactions. Hypoxia itself regulates the expression of genes involved, among others, in cell energy metabolism, glucose transport and angiogenesis [ 42 ]. Thus, gene therapy using the HIF-1 gene or protein may result in physiological changes at many levels in the body.\nResearch is being conducted with the use of HIF-1 in the treatment of cardiovascular diseases. Based on the animal studies and early clinical trials in humans, it is believed that HIF-1 administered as gene therapy effectively induces neovascularization in ischaemic tissues [ 43 — 45 ]. Experiments have shown that the activation of PPARD reduces weight gain, increases skeletal muscle metabolic rate and endurance, and improves insulin sensitivity. It was further found that the increase in PPARD expression suppresses atherogenic inflammation [ 47 ].\nNuclear hormone receptor protein is associated with de novo formation of skeletal muscle fibres of type I slow-twitch fibres and their transformation from type II fibres fast-twitch fibres , which determine the athlete\'s endurance and speed. It controls the body\'s energy balance. Thus, this protein plays an important role in the control of body weight [ 50 ]. It is recognized that GW improves the exercise capacity of trained animals [ 51 ].\nHowever, there are no published data on the ergogenic effects of GW in healthy and trained people. GW is an experimental drug that has been used in the treatment of obesity, metabolic syndrome, and type 2 diabetes in some clinical trials [ 52 ]. This molecule is included in the WADA prohibition list and there are reports that some athletes have already been caught using such doping.\nStudies have shown that activated AMPK enzyme may reduce the level of anabolic processes, including synthesis of fatty acids and proteins, and increase the level of catabolic pathways such as glycolysis and fatty acid oxidation [ 53 ]. It is believed that the ergogenic effect is achieved by the mutual interaction between AICAR and training, and their effect on the activation of many genes, determining the exertion efficiency [ 51 , 54 ].\nDopaje genético: transferencia génica y su posible detección molecular\nIn the past few years considerable progress regarding the knowledge of the human genome map has been achieved. In vitro studies improve the production of human recombinant proteins, such as insulin INS , growth hormone GH , insulin-like growth factor-1 IGF-1 and erythropoietin EPO , which could have therapeutic application. Unfortunately, genetic methods developed for therapeutic purposes are increasingly being used in competitive sports. Some new substances e. The use of these substances may cause an increase of body weight and muscle mass and a significant improvement of muscle strength.\n2007, Number 2\nISSN Genetic engineering has brought possibilities before unimaginable, in which not long ago was only seen in movies. Of gene therapy, aiming at a correction or cure a disease, pass the possibility of genetic improvement, currently glimpsed in the sports world with gene doping. But, gene doping would not be violating the right to genetic patrimony unmodified?']"	['<urn:uuid:21fd3d5e-2d0f-4030-8c00-11cd3b744150>', '<urn:uuid:295d74e2-9ee2-4199-bdfe-a03a4c8be911>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T05:06:54.527160	10	87	4764
60	midi sync configuration options local control signal routing explained	MIDI synchronization can be configured with Local On/Off control, where Local On enables a keyboard to play its own sounds while Local Off restricts output to MIDI Out port only. Signal routing can be achieved through MIDI Merge, combining multiple sources into a single MIDI connection, or through MIDI Thru ports that mirror MIDI In data to chain devices. The master clock source provides timing to other devices, though in modern systems like Ableton Link, any device can control tempo and synchronization.	"['I’ve been thinking quite a bit lately about tempo synchronization between Audulus and other external apps. There have been a number of posts related to Ableton Link and other synchronization technologies, and I thought I would share my thoughts on how these technologies might be implemented in the Audulus environment.\nThere are three primary tempo syncing methods in common use today. The oldest of the three is part of the MIDI standard and is commonly used to sync tempos between multiple hardware devices and software controlling hardware devices. There are two MIDI clocks in use. The first is MTC or Midi Time Code which encodes the absolute time and is sent with a rate of between 24 and 30 frames per second to align with commonly used video frame rates. The second is the Midi Beat Clock which is tempo dependent and and divides time into 24 pulses per quarter note. There is also a Song Position Pointer that specifies how many 16th notes have passed since the start of the song. With MIDI sync, one device is typically designated as the master clock source and other devices sync to the clock provided by the master.\nThe second method is used with both AU and VST audio plug-ins and is also supported in IAA (Inter-App Audio). In this case the plug-in can retrieve both a raw “transport” time and tempo information in the form of time signature, tempo (BPM), and current position relative to beat and measure. The host software is the master and provides timing to the plug-in. Audulus already provides access to the raw transport clock via the Time node.\nThe third mechanism which has gained in popularity in the last few years is Ableton Link. This is a protocol and accompanying software developed by the Ableton company to allow various musical applications running on the same or different platforms but connected by a common network to synchronize with each other. Unlike MIDI sync or traditional plug-in sync, Link does not have a single master clock. Each device maintains its own timebase and any device can request an alteration of the tempo or request to start and stop the session. Similar to the other methods, Ableton Link provides an application with tempo, beat position and position within a “measure”.\nBecause the three technologies are fairly similar in concept, with Ableton being the most complex from an implementation perspective, I believe that the strategy used to implement Ableton Link should be able to encompass the other methods as well.\nThe first question is what new functionality would be added to Audulus by implementing external synchronization? It’s already possible to synchronize various modules within a patch by using a shared clock and clock signals can be received from and sent to external sources via the existing I/O capabilities. However existing capabilities do not allow one to easily sync tempos with other applications, DAW software or MIDI connected hardware. With plans in place to expand the MIDI capabilities of Audulus and an AUv3 in development, I thought now would be a good time to consider how sync might look in Audulus.\nAbleton Link has implementation requirements that will dictate some elements of a potential sync node. Enabling/disabling Link (and presumably MIDI) will most likely need to be done within the settings menu rather than at the patch level. Assuming that Link is enabled and a session is available on the network, what inputs and outputs should we expect? Link provides beat, time and tempo to session participants and participants can send tempo change requests, a synchronization quantum (roughly time signature), and start/stop requests. One consideration of Link implementation is that when a client joins an existing session it must adopt the current tempo and should only request a change in response to user action. This precludes using a tempo input on the node attached to a knob or something else that would request an initial tempo value on loading a patch. Rather the sync node should align itself with the current tempo received from Link. It would be useful to have some mechanism for requesting a tempo change which is accessible in a performance environment, perhaps up down buttons on the sync node or located elsewhere on the UI. The same considerations apply to stop/start requests.\nWhat outputs would be useful? I would suggest that a clock at the beat frequency, one at 24 per quarter note and one at the sync quantum should be provided. Additionally the current tempo, beat, time signature and position within the measure should be available as outputs, as well as a run/stop gate.\nThere are two existing nodes which would primarily be involved in synchronization, the delay node and the phasor node. The delay node is the basis for most time related audio effects and the phasor is the fundamental building block for most LFO’s. Ideally there would be some mechanism to optionally synchronize these nodes to some division of the beat clock. Perhaps an input which if not zero, would represent some number of 24 ppqn clock pulses. This input would override the Hz and sync inputs if non-zero and a sync source was available.\nSimilar timing data is available from MIDI and plug-in environments which should lend itself to using a common approach for all three methods. With MIDI, Audulus could potentially act as a sync source (given appropriate MIDI out) as well as a client. Plug-ins only act as a clients and so would have no active inputs to the sync node. I might note that the sync methods are typically mutually exclusive. Ableton strongly recommends that enabling Link should disable other sync methods to avoid conflicts and provides mechanisms to operate within an AUv3/IAA/Audiobus environment where it supersedes other sync mechanisms.\nThe final consideration is what to do when no sync source is available. I would suggest that the sync node remain functional and and operate as a free-running oscillator. This would provide a convenient “master” beat clock with subdivided outputs for a patch if desired. It would be necessary in any case with Link enabled since Audulus could potentially be the first device to start a Link session. A default tempo could be established for the app or possibly even saved with each patch.\nI would like to stress that these are solely my opinions and do not represent any intent on the part of the Audulus developers. I have no knowledge of any future development plans other than those that have been made public on the user forum.', 'On this page you can find frequently asked questions on: ""MIDI interfaces"".\n- Active Sensing\n- Bank Select\n- Bulk Dump\n- Device ID\n- Event or MIDI Event\n- General MIDI (GM)\n- Local On/Off\n- MIDI Cable\n- MIDI Channel\n- MIDI Clock\n- MIDI Filter\n- MIDI In\n- MIDI Merge\n- MIDI Out\n- MIDI Thru\n- MIDI Time Code (MTC)\n- Program Change (Patch Change)\n- System Exclusive (SysEx)\nA MIDI event that some devices send out at regular intervals to determine whether there are any other MIDI devices connected. Some devices do not respond well to it if you are encountering problems with MIDI communication it can be a good idea to filter this out with your sequencer.\nA MIDI control function found on some keyboards thats activated by pushing down on the keys after they have been struck initially. The function usually controls parameters such as vibrato or filter cutoff.\nMIDI control instruction for switching between different sound banks on a MIDI sound device.\nSending or storing all of a MIDI devices parameters. Used for backing up user data in case of power failure etc.\nIf you have several devices of the same type, you can assign different Device IDs to them in order to address them independently. Normally only applicable to System Exclusive data.\nEvent or MIDI Event\nThe pressing of a key, the releasing of a key, the velocity of the stroke, the changing of volume etc all of these things are individual MIDI events, as are any other commands sent over MIDI.\nGeneral MIDI (GM)\nA standard which regulates the allocation of sounds and MIDI controller numbers. GM specifies 128 of the most frequently used sounds as well as controller numbers for effects and patch changes. The idea is that any MIDI file (a set of MIDI song data) that conforms to the GM standard will sound correct when played through any GM compatible device.\nSwitches off the connection between a MIDI synthesisers keyboard and the sound generating engine of the same device. Local On enables the keyboard to play its own sounds, whereas with Local Off, the keyboard only sends MIDI out of the instruments MIDI Out port. This enables a keyboard to play other sound modules or software instruments without playing its own sounds at the same time.\nA 5 pin cable up to 25 meters long for connecting between MIDI devices MIDI amplifiers are available if you need a longer cable run that this.\nThe MIDI standard dictates 16 different channels, over which different sounds and/or MIDI devices can be accessed independently. In order to be able to use a certain channel, the respective MIDI controller (e.g. keyboard or sequencer) must use the same channel as the sound module.\nA regular MIDI event used in conjunction with MIDI Song Pointers to provide synchronisation between MIDI devices. Typically used between two tempo-dependent devices such as a sequencer and a drum machine. Now largely superseded by the more flexible MIDI Time Code (MTC).\nRemoves unwanted MIDI events. This is often used to block events from reaching a sound module when using a number of devices in a chain, such as patch changes or pitch bend. Alternatively it is used by sequencers simply to filter the display of information, rather than the actual events themselves.\nPort through which MIDI data is received.\nDescribes the combining of MIDI information from two or more sources into a single MIDI connection. For instance if you wanted to play a sound module from a keyboard and a MIDI drum pad simultaneously (perhaps on different channels), you could plug both of them into a MIDI Merge box, and then plug the boxs output into the sound modules MIDI In port. As this involves merging data, it cannot be done simply by joining two cables together!\nPort through which MIDI data is transmitted.\nPort that mirrors the MIDI In in order to route MIDI data in a chain to a second device.\nMIDI Time Code (MTC)\nA regular clock signal that improves over MIDI Clock by stamping each event with an absolute time, thereby allowing devices to be restarted anywhere in a song and immediately synchronise.\nA function that clears all sounding notes by sending out a note off command for every possible MIDI note on all channels.\nProgram Change (Patch Change)\nThe standard MIDI instruction for changing a sound.\nA piece of hardware or software for the recording, editing and playback of MIDI information.\nSystem Exclusive (SysEx)\nMIDI data that is only understood by one particular model of MIDI device. Often used for transferring patch information via Bulk Dumps.']"	['<urn:uuid:6c94a759-682b-4652-93e7-35249192b4f4>', '<urn:uuid:1b5f8fcc-4f75-44ad-bcdd-8b3e2d61bdfb>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T05:06:54.527160	9	82	1863
61	trochoidal milling tool path classical shape what curve pattern looks like when tool returns initial point	In classical trochoidal milling, the tool path looks like the letter 'D'. The cutting tool moves along a circular arc (semicircle) and returns to the initial point by the arc chord, then repeats this path with a small stepover.	"['Several decades ago, the introduction of machine tools with significantly increased rotary and linear velocities was the success to efficient high-speed machining (HSM) methods. Peel milling, also known as slicing, was one of these methods. The main principle of peel milling is its high depth of cut (usually, no more than five-tool diameters) when coupled with a low width of cut (typically, up to 0.2 of a tool diameter). This combination features significant advantages.\nDecreasing the width of cut reduces heat load on a cutting edge and allows increasing cutting speed. In peel milling, the cutting speed can be higher when compared with traditional milling methods. The low width of cut significantly diminishes the radial component of a cutting force, which causes mill bending and vibrations. This ensures high operational stability and facilitates an increased depth of cut.\nRadial chip thinning enables higher feeds to maintain the required accurate chip thickness. Therefore, milling with a small radial engagement and a substantial depth of cut performed at high cutting speeds and feed rates is a good cause for improving machining productivity. Moreover, such a machining method provides gradual, uniformly distributed wear along the whole cutting edge, thus increasing tool life.\nPeel milling has proven to be productive in milling deep shoulders and wide edges. The slicing technique is successfully applied to rest milling – a machining process where a small diameter tool cuts various hard-to-reach areas, such as cavity corners.\nThe advance of computer numerical control (CNC) and computer-aided manufacturing (CAM) systems have generated further improvement: trochoidal milling with a complicated tool trajectory instead of a linear feed motion - suitable for peel milling. In mathematics, a trochoid is the curve, generated by the point of a circle rolling along a guide without sliding. In trochoidal milling, a cutting tool moves along a curve slicing thin and slim material layers. Commonly, the curve is a circular arc (semicircle) and the tool returns to the initial point by the arc chord and then repeats the path with a small stepover. In this case, the tool path looks like the letter ""D"". Milling along the curvilinear trajectory facilitates constant loading of a cutting edge and eliminates a sharp increase in load when entering the material.\nIn addition to the D-shaped path that is now considered ""classical"", today, most advanced machines with high-end control systems are much more complex. Trochoidal tool trajectories minimize non-cutting time and optimize machine unit motions.\nTrochoidal milling is known to be very effective in machining deep slots, pockets, and cavities and is also a very promising method to mill hard and difficult-to-cut materials, in particular titanium and high-temperature superalloys (HTSA). In addition, trochoidal milling is extremely useful for improving performance when cutting in unstable conditions: non-rigid workpieces, thin-wall areas, poor work holding devices, etc. And even more so, uniform and considerably reduced tool loading makes trochoidal milling efficient and applicable in micromachining.\nThe major challenges in trochoidal milling are machine tool control and intelligent path programming. However, when solving these challenges, another important factor – the cutting tool – is often overlooked. Without the right tool, all efforts to design the trajectory and maintain uniform loading on the cutting edge reduces the expected results, which creates a challenge for tool manufacturers to produce an optimal tool to meet trochoidal milling requirements.\nWhat are the features of a high-performance trochoidal milling cutter?\nTo begin with, the trochoidal milling cutter must be suitable for high-speed machining. This relates to appropriate accuracy parameters, balancing, safety when operated at considerable rotational speeds, and more. Milling with high depths of cut increases the tool’s overhang while the dynamical behavior of a cutter is crucial to ensure machining stability. When milling with a low width of cut, only one tooth engages the workpiece material at any given time. Optimizing a contact area along the tooth is an important factor for stable milling, and the cutter with the most favorable tool cutting edge inclination is a principal part for finding the best solution. The effective evacuation of the thin chips, which are generated when trochoidal milling, does not require a large chip gullet in the cutter.\nEven a brief examination of the above shows that multi-flute solid carbide endmills (SCEM) or assembled mills carrying replaceable solid carbide heads comply with the requirements in the best way. Indeed, SCEM represents most trochoidal milling tools today.\nUnderstandably, these endmills have their own design features that can be identified in the latest, innovative ISCAR product line. Several features characterize these products: a) different helix and variable angular tooth pitch that provide a vibration-resistant design to improve stability in HSM with high overhang, b) a specially shaped flute that results in an increased core diameter to improve dynamic strength, and c) enough space for chip gullet to ensure smooth chip flow. These products maintain high accuracy and deliver maximum metal removal rates when machining the main types of engineering materials. The diameter range of ISCAR’s solid carbide endmills (SCEM) for trochoidal milling is 2-25 mm.\nCHATTERFREE EC-E7/H7-CF solid carbide endmills have 7 flutes and a variety of corner radii. They are available in a series of two, three, four and six cutting length-to-diameter ratios (Fig. 1). The endmills are produced from PVD coated ultra-fine carbide grade IC902.\nThe key distinctive feature of 7 flute endmills ECP-H7-CF (Fig. 2) is the chip-splitting geometry of a cutting edge. Introducing this geometry provides increased performance at high overhang and significantly improves chip evacuation in machining deep pockets and cavities. It is important to note that the chip-splitting edge ensures a satisfactory surface finish for most operational requirements.\nTi-TURBO 7- and 9-flute solid carbide endmills ECK-H7/9-CFR, which were designed especially for high-speed machining titanium alloys, have a cutting length of around two-tool diameters. Due to remarkable chatter dampening and an optimized-edge geometry, these solid carbide endmills show good results in trochoidal milling of various aircraft components, including machining curved slots in titanium bladed rotors (blisks).\nIf trochoidal milling is applied to shallow slots or corners of titanium parts, 6 flute MULTI-MASTER exchangeable heads that were recently introduced may be a more suitable solution (Fig. 3). The head design incorporated ISCAR\'s competence and experience in this field and has enabled a robust product for efficient machining hard-to-cut titanium grades such as Ti-10V-2Fe-3Al and Ti-5Al-5Mo-5V-3Cr.\nThis review would be incomplete without some notes about tool holders, which are essential to the success of trochoidal milling. Machining practice shows that the best results are reached when the milling cutters are mounted in hydraulic or heat shrink chucks (Fig. 4).\nFigures for the article and their captions.\nFig. 1 – CHATTERFREE multi-flute solid carbide endmills produced in a series of different cutting length-to- diameter ratios.\nFig. 2 – A chip-splitting cutting edge design significantly improves chip evacuation and provides satisfactory surface finish.\nFig. 3 – ISCAR\'s MULTI-MASTER exchangeable head for trochoidal milling hard-to-cut titanium alloys.\nFig. 4 – Trochoidal rough milling of a blisk airfoil by a multi-flute solid carbide endmill, which is mounted in a heat shrink chuck.']"	['<urn:uuid:7cb1f51f-ebf6-419c-b77b-fb0918b10365>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T05:06:54.527160	16	39	1171
62	automotive supply chain changes electric vehicles impact	The automotive supply chain is experiencing significant changes due to electric vehicles, impacting both logistics and revenue. From a logistics perspective, companies like BlueGrace are opening new offices and providing managed services to help automakers adapt to these changes. The industry is facing revenue losses of over $100 billion in 2021. In terms of market growth, electric vehicles are seeing substantial expansion, particularly in Europe with a projected growth rate of 39.30% during 2023-2028, driven by improved battery technology, environmental concerns, and government support. These changes are reshaping both manufacturing needs and supply chain operations.	"['Changing automotive supply chains continue to request strategic help from logistics providers\nGrace Sharkey, Reporter\nOct 26, 2021\nBetween customer demand for electric vehicles, chip shortages, and environmental, social and governance initiatives, the already complicated automotive supply chain has gone through a whirlwind of changes — changes likely to cost the industry north of $100 billion in revenue in 2021.\nFor third-party logistics provider BlueGrace Logistics, these generational changes to the automotive supply chain influenced the decision to open its 11th office last week in Detroit to serve as a “control tower” for its managed services operations.\nTodd Trompeter, BlueGrace’s vice president of logistics operations, explained to FreightWaves that a number of the company’s preexisting managed logistics customers are in the Detroit area, helping service automakers and their manufacturing plants.\nOur current customers have been extremely happy and just asking for more of our help as they’ve grown.\n“We have seen incredible growth in managed services during the last 24 months,” said Trompeter. “Our current customers have been extremely happy and just asking for more of our help as they’ve grown. We have really great relationships with carriers in the Detroit area, and that has given us the ability to provide some very unique solutions for clients.”\nWhile BlueGrace does offer an array of logistics services, Trompeter explained that in the auto manufacturing sector, managed logistics services requests have seen an uptick in demand as companies are looking to experienced organizations to outsource their entire logistics operations and make strategic changes to help with the long-term scaling of new automotive manufacturing needs.\n“We train our managed services people to be innovative in this type of environment,” said Trompeter. “If you can’t get a certain product, what else can you do? Can you shift operations and look for different ways of fixing that problem with a little bit more creativity?”\nMoving the managed services teams to Detroit, with on-location guidance from Kerwin Gordon, senior director of logistics operations, Trompeter expects these solutions to grow regional opportunities for BlueGrace.\nBlueGrace has been so innovative because of our company culture. It’s a great place to work and this gives us the ability to be closer to our clients as well.\n“BlueGrace has been so innovative because of our company culture. It’s a great place to work and this gives us the ability to be closer to our clients as well. Our top-tier talent loves to get on-site with our clients, walk the docks and try to truly understand how to optimize your supply chain using different tools we have available,” he said.\nStrategic hiring within Michigan\nAs Trompeter described to FreightWaves the growth the managed services team was experiencing, he stressed the importance of being able to pull talent from two main groups of individuals in the region: university students and experienced automotive workers.\nWhile, strategically, the former is the most sought after in the logistics community as a whole, Trompeter believes that the latter, those familiar with OEMs, have just as much expertise and hands-on experience in creating innovative solutions.\nYou have so much of that available talent in the Detroit area and it will add a lot of value to our new office.\n“We have had as much success with people coming directly from the automotive sector with an understanding of how they think and what outcome they are looking for from BlueGrace,” he said. “You have so much of that available talent in the Detroit area and it will add a lot of value to our new office.”\nTrompeter was equally thrilled to continue building relationships with local colleges with supply chain accolades, including Central Michigan and Wayne State universities and his alma mater, Michigan State University.\n“As we grow, we really need to look at how we are going to scale BlueGrace’s operations,” said Trompeter. “How do we staff appropriately as we experience this hypergrowth? Detroit just made a ton of sense for that, especially when you look at the supply chain programs at Michigan State, Central Michigan and the graduate programs available at Wayne State and Oakland University. There really is just so much talent in that area.”', ""The Europe electric vehicles market is projected to exhibit a growth rate (CAGR) of 39.30% during 2023-2028. The increasing government support and incentives, stringent emission regulations, advancements in battery technology, rising awareness about environmental concerns and changing consumer attitudes, rapid urbanization and air quality concerns, and significant technological innovation and investment represent some of the key factors driving the market.\nAn electric vehicle (EV) is an automobile powered by electricity, utilizing one or more electric motors for propulsion instead of traditional internal combustion engines that rely on fossil fuels. These cutting-edge vehicles have gained substantial popularity in recent years due to their environmental benefits, improved technology, and increased accessibility. Electric vehicles operate on the principle of storing electrical energy in rechargeable batteries, which power the electric motor to generate the necessary torque and rotation for movement. The elimination of tailpipe emissions makes EVs significantly more environment-friendly than their gasoline-powered counterparts, contributing to reduced air pollution and a decreased carbon footprint. The range of electric vehicles varies depending on battery capacity and advancements in battery technology, however it has been steadily improving, making them more practical for everyday use. Charging infrastructure is also expanding rapidly, with numerous charging stations becoming available in urban areas, making it easier for EV owners to recharge their vehicles. Besides the environmental advantages, electric vehicles also offer lower operating costs due to the reduced reliance on expensive fossil fuels and simplified maintenance, as they have fewer moving parts than traditional vehicles. As governments and societies worldwide prioritize sustainability and seek ways to combat climate change, electric vehicles play a crucial role in shaping a cleaner and more sustainable transportation landscape for the future.\nEurope Electric Vehicles Market Trends:\nThe growth of EVs heavily relies on a robust charging infrastructure. European countries have been investing in the development of public charging networks, making it more convenient for EV owners to recharge their vehicles, especially in urban areas and along major highways. Additionally, the increasing awareness regarding environmental issues and the need to reduce carbon emissions has led to changing consumer attitudes toward EVs. Consumers are now opting for electric vehicles as a conscious choice to lower their carbon footprint and contribute more to a sustainable future. Other than this, Europe's growing urbanization has increased concerns about air quality and pollution in cities. Due to this, electric vehicles are becoming a popular choice among the masses as they offer a cleaner and quieter alternative to traditional internal combustion engine vehicles, addressing these environmental concerns. Besides this, the integration of electric vehicles into ride-hailing and car-sharing services has played a role in promoting their adoption. Many ride-hailing companies are increasingly incorporating electric vehicles into their fleets, exposing more people to the benefits of EVs. In line with this, the rise of electric vehicles has attracted significant investment from both established automakers and new players in the automotive industry. This surge in investment has led to the development of innovative EV models with improved features and functionalities, making them more appealing to consumers. In line with this, European governments have been proactive in supporting the transition to electric mobility. They offer various incentives such as financial subsidies, tax benefits, reduced registration fees, and access to bus lanes and toll-free roads for EV owners. These incentives aim to make electric vehicles more affordable and attractive to consumers, encouraging higher adoption rates. Furthermore, Europe has some of the most stringent emission regulations in the world. The European Union's CO2 emission targets for car manufacturers have put pressure on automakers to produce low or zero-emission vehicles. To avoid heavy fines, manufacturers are increasingly investing in electric vehicle production to meet these strict regulatory standards. Moreover, technological advancements in battery technology have improved the range and performance of electric vehicles. Lithium-ion batteries have become more efficient, offering longer driving ranges and faster charging times, addressing one of the major concerns of potential EV buyers.\nEurope Electric Vehicles Market Segmentation:\nIMARC Group provides an analysis of the key trends in each segment of the Europe electric vehicles market report, along with forecasts at the regional and country levels for 2023-2028. Our report has categorized the market based on component, charging type, propulsion type, and vehicle type.\n- Battery cells and packs\n- On-board charger\n- Fuel stack\nThe report has provided a detailed breakup and analysis of the market based on the component This includes battery cell and packs, on-board charger, and fuel-stack.\nCharging Type Insights:\n- Slow charging\n- Fast charging\nA detailed breakup and analysis of the market based on the charging type has also been provided in the report. This includes slow charging and fast charging.\nPropulsion Type Insights:\n- Battery electric vehicle (BEV)\n- Fuel cell electric vehicle (FCEV)\n- Plug-in hybrid vehicle (PHEV)\n- Hybrid electric vehicle (HEV)\nThe report has provided a detailed breakup and analysis of the market based on the propulsion type This includes battery electric vehicle (BEV), fuel cell electric vehicle (FCEV), plug-in hybrid vehicle (PHEV), and hybrid electric vehicle (HEV).\nVehicle Type Insights:\n- Passenger vehicles\n- Commercial vehicles\nA detailed breakup and analysis of the market based on the vehicle type has also been provided in the report. This includes passenger vehicles, commercial vehicles, and others.\n- United Kingdom\nThe report has also provided a comprehensive analysis of all the major regional markets, which include Germany, France, United Kingdom, Italy, Spain, and others.\nThe report has also provided a comprehensive analysis of the competitive landscape in the Europe electric vehicles market. Competitive analysis such as market structure, key player positioning, top winning strategies, competitive dashboard, and company evaluation quadrant has been covered in the report. Also, detailed profiles of all major companies have been provided.\nEurope Electric Vehicles Market Report Coverage:\n|Base Year of the Analysis\n| Historical Period\n|Scope of the Report\n||Exploration of Historical and Forecast Trends, Industry Catalysts and Challenges, Segment-Wise Historical and Predictive Market Assessment:\n- Charging Type\n- Propulsion Type\n- Vehicle Type\n||Battery Cell and Packs, On-Board Charger, Fuel-Stack\n|Charging Types Covered\n||Slow Charging, Fast Charging\n|Propulsion Types Covered\n||Battery Electric Vehicle (BEV), Fuel Cell Electric Vehicle (FCEV), Plug-In Hybrid Vehicle (PHEV), Hybrid Electric Vehicle (HEV)\n|Vehicle Types Covered\n||Passenger Vehicles, Commercial Vehicles, Others\n||Germany, France, United Kingdom, Italy, Spain, Others\n||10% Free Customization\n|Report Price and Purchase Option\n||Single User License: US$ 2699\nFive User License: US$ 3699\nCorporate License: US$ 4699\n|Post-Sale Analyst Support\n||PDF and Excel through Email (We can also provide the editable version of the report in PPT/Word format on special request)\nKey Questions Answered in This Report:\n- How has the Europe electric vehicle market performed so far and how will it perform in the coming years?\n- What has been the impact of COVID-19 on the Europe electric vehicle market?\n- What is the breakup of the Europe electric market on the basis of component?\n- What is the breakup of the Europe electric vehicle market on the basis of charging type?\n- What is the breakup of the Europe electric vehicle market on the basis of propulsion type?\n- What is the breakup of the Europe electric vehicle market on the basis of vehicle type?\n- What are the various stages in the value chain of the Europe electric vehicles market?\n- What are the key driving factors and challenges in the Europe electric vehicles market?\n- What is the structure of the Europe electric vehicles market and who are the key players?\n- What is the degree of competition in the Europe electric vehicles market?\nKey Benefits for Stakeholders:\n- IMARC’s report offers a comprehensive quantitative analysis of various market segments, historical and current market trends, market forecasts, and dynamics of the Europe electric vehicles market from 2017-2028.\n- The research study provides the latest information on the market drivers, challenges, and opportunities in the Europe electric vehicles market.\n- Porter's five forces analysis assist stakeholders in assessing the impact of new entrants, competitive rivalry, supplier power, buyer power, and the threat of substitution. It helps stakeholders to analyze the level of competition within the Europe electric vehicles industry and its attractiveness.\n- Competitive landscape allows stakeholders to understand their competitive environment and provides an insight into the current positions of key players in the market.""]"	['<urn:uuid:685b0e17-2515-4164-b39c-16cad729562d>', '<urn:uuid:b7467957-f944-4d46-8681-dc3eebb65bfb>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T05:06:54.527160	7	95	2067
63	What solution is proposed for protecting juror privacy?	The proposed solution is a system where prospective jurors could recuse themselves from specific cases if they have a legitimate reason, without having to divulge personal information publicly. They would remain in the overall jury pool for other cases. Judges would need to determine what constitutes a legitimate excuse, similar to how they currently assess who is trying to avoid jury duty.	['A Kansas University law professor says personal questions asked in the jury selection process have led potential jurors to lie or withhold important information, undercutting an attempt to give the accused a fair trial.\n“To date, the history tends to be it’s an unfortunate fact of life that we have to expose these people,” said Melanie Wilson, who is a former federal prosecutor in Georgia. “That’s just part of your civic duty. The focus tends to be that we need to do more to protect jurors, but we really can’t because the defendant’s rights are paramount.”\nWilson, who is also the KU School of Law’s associate dean for academic affairs, argues for changes to the jury selection process in an article that will appear in the Utah Law Review later this year. Less protection of jurors’ privacy does affect the rights of the accused as well, she said. For example, if potential jurors for a rape trial — out of embarrassment for mentioning it during questioning in front of a roomful of people — withhold information that a family member was a victim of a sexual assault, the case could become a mistrial or be overturned on appeal, creating the need for a new trial if the information is discovered or disclosed later. It can defeat the purpose of attorneys asking probing questions if jurors don’t tell the whole truth, she said.\nWilson said scholars are seeing juror dishonesty occurring in a growing number of cases, but she believes it’s mostly due to the fact most people are unaware of the consequences because they are not familiar with that part of the system. “The traditional thought has been maybe jurors have rights,” she said. “But we’re unclear where their rights start and end.”\nWilson suggests a main remedy could be a system where a prospective juror could essentially recuse himself or herself from jury selection in certain cases if they have a legitimate reason. That way the juror wouldn’t have to divulge potential personal information in front of the rest of the potential jurors or the judge, attorneys, defendant and court reporter. The jurors would then remain in the overall jury pool, so they could be called for a different case.\nWilson admits there would need to be a way for judges to be able to determine what is a legitimate excuse. “Judges are good at determining who is trying to avoid jury duty. I’m not sure it would be different,” she said. “What it would do is shift at least some right to the juror right now. They really don’t have any right to opt out. This way there would be some leeway.”\nDouglas County District Attorney Charles Branson said that jurors can get grilled on some occasions during jury selection but that judges are often quick to intervene if they believe an attorney is going too far. “In most cases where we have to ask those very personal questions, we offer to the jurors that if they have something that they think is important they can always ask to meet privately with the judge and attorneys out of the presence of other jurors,” he said.\nWilson said because the Supreme Court has ruled the voir dire, or juror selection, process is part of the trial, a transcript of those questions would still be part of the court record and is available to the public and media.\nDouglas County District Chief Judge Robert Fairchild said that is correct, although rare. “I have never known of an instance in which the answers of a juror excused for such a reason have been published outside of the appellate courts, but it could happen,” he said. Fairchild said Douglas County judges try to make sure jurors are treated with respect by everyone and they don’t allow an attorney to conduct questioning “in an offensive or abusive manner.”\nHe also said he has never had a sense that jurors lie about prior experiences out of concern for having to relate an embarrassing experience. “I think our judges tend to be pretty liberal in excusing jurors who have suffered traumatic experiences that are related to the subject matter of the trial without requiring detailed information about the incident,” he said.\nFairchild and Branson said a system that allows jurors to excuse themselves does bring up other issues. “In order to pass constitutional scrutiny the process cannot systematically exclude any portion of the population and potentially prejudice a defendant,” Fairchild said. “(Wilson) is correct that judges have a great deal of discretion in setting the manner in which voir dire is conducted. However, we are limited by the constraint that the Constitution requires that the selection process must result in a jury that is a cross-section of the community. We cannot intentionally exclude a specific group of individuals.”\nBut Wilson said under her proposal a judge is not excluding jurors, just overseeing the juror’s own strike. She argues that system would result in more qualified juries because it would give attorneys more of an opportunity to ask probing questions.“If jurors are lying to protect their privacy, then the balance is shifted,” Wilson said. “Then we’re not protecting Sixth Amendment rights. It is a constitutional flaw, if you will.”']	['<urn:uuid:5c0b66a5-18cb-4ffe-ae4b-0b66e017d50c>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:06:54.527160	8	62	873
64	What layers protect touchscreen displays from damage?	3M's antireflective film consists of multiple protective layers: an antifingerprint top layer for easy wiping, an antiglare layer that diffuses light reflection, a scratch protection layer, and a nontacky adhesive layer that allows for easy removal and replacement if damaged.	"[""Types of Transmissive Film\n3M designs and manufactures several different types of transmissive films that can be used alone or combined together within the same product to enhance the performance of the LCD display.\nLet's talk about brightness first. 3M uses two different kinds of brightness enhancement films for the LCD displays in the BMW concept car. The first is a single sheet of polymer imprinted with a prismatic surface pattern. The pattern looks like the teeth of a gear or a series of triangles pointing out from the flat film surface.\nThe prisms on the brightness enhancement film redirect light from the backlight toward the viewer. Without these prisms, a lot of the light from the backlight would be wasted -- shining too high or too low for the viewer to see. Think of a non-LCD television set whose light reflects off the ceiling and floor. With brightness enhancement film, all of the light is directed straight out toward the viewer, with any excess light reabsorbed by the back panel of the display.\nThe second transmissive film used to increase brightness is called dual brightness enhancement film. Its name comes from the fact that the film is not only mounted with prisms, but with an extra reflective sheet for recycling any light that doesn't hit the prism at the correct angle. In this configuration, light continues to bounce around the back of the LCD display until it strikes the brightness enhancement film at the optimal angle to be directed out toward the viewer.\nAntireflection transmissive films can be applied to the surface of an LCD display to reduce glare and increase clarity. 3M's antireflective film is actually several layers of film in one. The top layer is an antifingerprint film that makes it easy to wipe fingerprints from PDA or computer screens. The second layer is antiglare film, which reflects light at multiple angles, diffusing the glare effect. A third layer protects PDA screen from scratches. And a fourth layer is a nontacky adhesive that allows the film to be easily removed and replaced in case of scratches or other damage [source: 3M].\nFor privacy and security issues, 3M has developed light control films that use microlouvers to block screen images from anyone who isn't directly in front of the LCD display. Microlouvers are like microscopic Venetian blinds that point straight out toward the viewer. Anyone to the left or the right of the screen will only see black. 3M sells a special product for laptop and other LCD computer monitors called a Privacy Filter. The privacy filter is essentially a light control film that can be adhered directly to a computer screen.\nAnother type of transmissive film called enhanced specular reflector film can be applied behind the backlight to maximize the distribution of light from of a single light source. The enhanced specular reflector film is actually made up of hundreds of microscopically thin polymer layers of alternating high and low frequency reflectors. The result is a reflection rate of more than 98 percent, greater than the normal silver mirror you have in your bathroom [source: 3M].\nFor more information about LEDs, LCDs and related technologies, shift your eyes to the links on the next page.""]"	['<urn:uuid:d06d1aef-355a-45b6-9708-8ab344efd3b3>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	7	40	538
65	what signs mean septic tank needs pumping and how often should it be done	Signs that a septic tank needs pumping include gurgling or slow drains and waste backups throughout the home. As for frequency, it depends on household size, tank size, and drain field condition. Some systems need annual pumping, while others can go ten years. It should be done at least every few years. If unsure, it's recommended to have the tank pumped and ask the provider to recommend an appropriate interval based on your system's characteristics and household usage.	['On this page:\n- Estimating the Cost of Septic Tank Pumping\n- Do I Need to Have My Septic Tank Pumped?\n- What Factors Affect the Cost of Septic Services?\n- What Will Influence the Price of Pumping My Septic Tank?\n- Preparing to Have Your Septic Tank Pumped\n- Cost Comparisons\n- Level of Difficulty\nRegular pumping is a basic maintenance task for homeowners whose homes are serviced by a private septic system, rather than by public sewers. Though the task is fairly straightforward, there are a few factors that can influence the cost of septic tank pumping.\nEstimating the Cost of Septic Tank Pumping\nHow is the Service Priced?\nSeptic service pros generally base their charges on their own costs, such as transportation and disposal, which can vary from place to place. It is common for providers to present their prices on a flat fee basis or per gallon of material removed from the tank.\nIn addition to the basic cost of emptying the tank, extra services such as cleaning or inspection are charged on a fee basis or for the actual labor time spent on the job. It is common for basic pumping service to cost between $200 and $400 throughout the U.S., but when comparing estimates from local contractors, be sure to compare the services that are included.\nDo I Need to Have My Septic Tank Pumped?\nSeptic tanks collect solid waste from a home’s drains, and while it does decompose to some degree inside the tank, matter will accumulate and can create drainage problems if it is not removed periodically. The frequency with which a septic tank should be pumped really depends on the size and habits of each household, along with the size of the tank and the condition of the drain field, but it is something that should be done at least every few years.\nIn some cases it may need to be done annually, while other systems won’t need attention for ten years or more. It won’t cause any harm to do the job more often than necessary, so if you aren’t sure what is best for your system or don’t know when the tank was last emptied, err on the side of caution; have the tank pumped and ask your provider to recommend an appropriate interval based on the characteristics of your system and your household’s usage. To learn more, see our guide, How Often Should My Septic Tank be Pumped?\nPumping a septic tank is definitely not a DIY-friendly project. In addition to being unsavory, at best, the work must be carried out according to specific regulations that address handling waste material.\nIn many locations, professionals must be licensed and comply with strict permitting requirements in order to properly dispose of the waste removed from residential tanks. The cost of service often reflects specific fees that providers are charged for disposal, permits, and licenses as well as typical cost of operation.\nWhat Factors Affect the Price of Septic Tank Pumping?\nLocation typically affects virtually any type of professional home maintenance service, since the cost of living has direct influence on the costs of labor and operating a business. When it comes to septic tank pumping your location may have additional influence on pricing, since local regulations and even your home’s proximity to a waste treatment facility can affect the cost of getting the work done.\nThe cost for septic system service may be affected by the type and availability of providers in a given area. In places with a number of independent pumping contractors to choose from, rates are likely to be lower than in places where the number of pros is limited or where regulations require pumping or septic service at specific intervals.\nWhat Will Influence the Price of Pumping My Septic Tank?\nSeptic tanks are generally sized between 1,000 and 1,500 gallons. Whether pumping is priced by the trip or by the gallon, the price will be higher to pump a larger tank than a smaller one. Even though septic systems are designed to store solid waste in the tank and disperse liquids through a drain field, it is normal for the tank to remain filled with liquid, and the entire contents of the tank are removed when it is pumped, not only the solids.\nSeptic pumping can usually be scheduled with a short turnaround time, but if service is needed on an emergency basis, it is common to pay an extra fee of $100 or more. The first sign of an overfilled septic tank is often backups inside a home’s drains, and the best way to avoid both unpleasant drainage problems and expensive emergency service calls is to plan ahead for pumping before the tank exceeds its capacity.\nContents of the Tank\nAlthough a septic pro may leave a small amount of material at the bottom of the tank after pumping to jumpstart bacteria growth, the septic tank must be essentially emptied during a service call. If the tank has not been serviced regularly, or if the bacterial balance in the tank is unhealthy, excessive sludge can build up on the bottom of the tank. It can take extra time to remove the material, which may result in higher charges.\nBasic septic pumping service usually includes emptying the tank with the use of a specialized truck, but extra services may carry their own fees. If your provider needs to locate and dig out the tank lid, wash down the walls of the tank, or inspect and test the system, the job may cost more than if you simply need the tank emptied. Extensive inspections and tests of a septic system are often done in conjunction with a basic service call, but they can range in cost from $150 to $600 or more over the cost of pumping.\nMaking repairs to your septic system can add to the cost of pumping if the work is done at the time of service. Since the tank needs to be emptied before some types of repairs can be made, it’s often the best time to get the work done, rather than scheduling an additional service call. The cost of septic tank repairs will depend on the nature of the problem, but repair work is generally charged on a “time and materials” basis.\nPreparing to Have Your Septic Tank Pumped\nSince you may pay a premium for an emergency service call, it’s best to plan ahead for septic tank service. Be sure to keep track of when the tank is emptied so you can schedule pumping at a practical interval, but always be aware of potential troubles so you can act before you have a mess to deal with. Gurgling or slow drains and waste backups throughout the home are often signs the tank is at its capacity.\nThe job can be done at any time of year, but the most practical time may depend on the climate where you live. It can be difficult to find and uncover a tank lid when the ground is frozen in the winter, and wet ground conditions in the spring can lead to damage from having a heavy truck parked in your yard or driveway, so summer and fall are often the best seasons to schedule the project.\nMost localities have a number of pumping contractors to choose from, so if you have time to plan ahead you can shop around for the best price and services. In order to get the most accurate estimates, let contractors know the size of the tank if you can, and whether or not you will have the site prepared for them. Be sure to compare estimates on an equal basis, so check to find out what services are included in each pro’s price and what the additional fees are for recommended services.\nPrepare the Site\nPumping a septic tank really isn’t a job you can help out on, but if your tank is not fitted with risers that make the lid accessible from the surface, you can pitch in and probably save on the cost of the job if you can find and dig out the lid before your service appointment. Most pros charge extra for the time it takes them to locate and access the tank lid, and depending on how much information they have to go on, that part of the job could cost as much as the pumping itself.\nIf you aren’t sure where your tank lid is located, check with your local code enforcement office or building authority to find the septic design on file for your property, which should illustrate or describe the tank’s location.\n|$200 - $300||$250 - $450||$350 - $600|\nA one thousand gallon septic tank is typical for most two- and three-bedroom homes, but there are a few variables that can affect the cost of having a tank that size pumped.\nBasic Service: $200 - $300\n- • Prep Work: By finding and uncovering the septic tank lid before the pumping truck arrives, a technician can get straight to work and homeowners can save up to $100 in extra labor charges.\n- • Pumping: As long as no damage or other issues are discovered, pumping should go quickly and according to plan.\n- • Extra Services: A basic check of the tank’s condition and the drainage from the main waste line are standard services, so keeping the work limited to those tasks will keep costs down.\nA Little Detective Work: $250 - $450\n- • Prep Work: With the location of the tank lid a mystery, it can take your provider a little time to track down and dig out the cover. The extra work can add $50 or more to the cost of the job.\n- • Pumping: The unknown location of the tank lid is evidence of a prolonged period between pumpings. The added time to clear the tank of sludge could add $50 to $100 to the tab.\n- • Extra Services: Since it has been a while since this tank was last checked out, it’s worth paying an extra $120 or more to wash down the tank, give it a closer-than-typical look, and make a basic check of the function of the main line and drain field.\nA Bit of Fixing: $350 - $600\n- • Prep Work: Leaving the digging to the pros adds a little to the bottom line, but at least marking the location for them can make it quick work.\n- • Pumping: A regularly maintained septic tank makes for pretty simple pumping, keeping the basis of the service call pretty straightforward.\n- • Extra Services: Some minor damage found during a basic check of the system may be able to be repaired at the time of service. Replacing a damaged baffle or filter could add $200 to $300 to the job, but now is the most cost-effective time to get the job done.\nLevel of Difficulty\nPumping out a septic tank is definitely a job for a pro, but there may be ways for a DIYer to save on the cost of the job. Marking the location of a buried tank cover, or going a step farther and digging the soil off the top, is a simple way to keep costs down by limiting the scope of the job the septic service technician needs to do. Septic tank lids are generally buried within 12” of the surface, so uncovering them requires a little muscle, but only a basic level of skill.']	['<urn:uuid:c88734a2-591f-4f40-889d-3ebb5c844b77>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:06:54.527160	14	78	1920
66	For indoor growing, what's the harvest time for microgreens?	Microgreens should be harvested when very young, up to 14 days old and about an inch tall.	['Consider this the first of many posts about plants in urban areas and the benefits that plants can bring to these locations. As an example, a group of people in Vancouver, B.C. developed an amazing green (or living) roof that incorporates plants native to the coastal grasslands found in that region. Watch this video to see how this project is helping to turn a landscape dominated by concrete and asphalt into a thriving and diverse ecosystem.\nMicrogreens are quite popular these days. They are larger than sprouts and smaller than baby greens, and new research has verified that they are packed with nutrients. Microgreens are easily grown year-round on a countertop or windowsill, even if the lighting situation is less than ideal for growing other plants. I am growing some now in clear, plastic, salad mix containers. I punched a few holes in the bottoms of the containers for drainage, filled them with moist potting soil, scattered seeds on top of the soil, covered the seeds with a bit more potting soil, and placed them outside in a small cold frame. I planted two with lettuce mix and one with radishes and arugula. The plants are ready to cut in a week or two and can be eaten in salads, sandwiches, stir-fries, etc.\nTo be considered true microgreens, the plants should be harvested very young (up to 14 days old and about an inch tall). After they are harvested, they will need to be replanted – unlike baby greens and typical salad mix which will produce multiple harvests – because they will not be large to enough to recover from being chopped down.\nA wide variety of seeds can be grown as microgreens, including lettuces and other salad greens, brassicas (radishes, mustards, arugulas, etc.), and herbs. You can select a pre-packaged lettuce mix, or you can make a special mix of your own. Microgreens are great for people who want to grow some of their own food but have little or no space for a traditional garden because they are easily grown in containers indoors. They can also be grown throughout the winter when outdoor gardens have been put to bed for the season.\nLast week I enjoyed a solitary walk through Powder Valley Conservation Nature Center and Reserve near Kirkwood, Missouri. All was fairly quiet. A gentle, cool breeze swept through the bare tree branches and rattled the brown leaves still clinging to a few stubborn oaks and maples. Leaf litter decorates the entire forest floor; it crackles as I step through it. While the day is still cool (mid 40’s), the Sun’s energy, like the breeze, sweeps past the trees and warms me as I walk.\nAll is quiet. But I can feel the change in season coming. Some trees are beginning to show their plump buds, ready to spring into action. In just a few weeks, I can imagine the wonderful bluebells (Mertensia virginica –previous post) and wood sorrel (Oxalis violacea – previous post) emerging again and filling the brown forest floor with brilliant color.\nView original post 68 more words\nSpring is finally arriving in the Treasure Valley. Evidence can be found at Idaho Botanical Garden.\nHamamelis x intermedia ‘Dianne’ – Witch Hazel\nAlnus viridis ssp. sinuata – catkins on Sitka Alder\nDwarf Iris (photo credit: Ann DeBolt)\nSeeds don’t remain viable forever. However, each species is different – the seeds of some species can remain viable for many years (decades even), while some species have seeds that will no longer be viable after a single year. This, of course, is something to keep in mind when planting seeds.\nRecently I planted some onion seeds. I was curious to see if they would germinate because they were a few years old – collected in 2007. My experience with onion seeds is that they germinate fairly quickly, within a week or so. However, three weeks have passed and my seeds have not yet germinated, despite being kept in moist potting soil in a sunny, warm corner of the house. This experience has led to me to think about seed viability.\nLike I said, seeds of different species remain viable for different lengths of time. For vegetable gardeners, there are a variety of places to go to learn more about seed viability. Iowa State University Extension has a great chart which shows the number of years that the seeds of popular vegetable crops should remain viable. It is interesting to note that onion seeds only remain viable for one year. As it turns out, my seeds were far past their prime.\nSeed storage can make a huge difference, though. Ideally, seeds should be stored in a cool, dry location. If they are exposed to too much heat or moisture, their metabolism will increase and their viability will decrease. This is because seeds are living organisms, despite appearing dead or dormant. Their metabolic processes are proceeding at an extremely slow rate, but they are still proceeding. If metabolism increases (due to excessive heat or moisture, for example), the embryonic resources of seeds can become depleted, and viability (or germination potential) decreases.\nSome sources recommend that you keep your seeds in the refrigerator, provided that they are sealed in plastic to keep them dry. Regardless, the ideal conditions for seed storage are cool and dry. I have always kept my seeds in a shoebox at room temperature (which isn’t always that cool because in the summer I refrain from using the air conditioner as much as possible). Thus, the viability of my seeds may in fact be reduced simply due to the conditions in which they are being stored.\nThere is a way to determine the viability of your seeds if you are curious. Just place some seeds on a moist paper towel, roll up the paper towel, and place it inside of a plastic bag. Wait a few days and then remove the paper towel from the plastic bag. Count the number of germinated seeds and divide that number by the total number of seeds originally placed on the paper towel. This will give you the germination percentage and will help you determine how many seeds to place in each hole or pot when you are planting them.\nThis is the seed packet for the onion seeds that I planted. 2/15/2013 is the date that I planted them. After three weeks they had not germinated. I guess I’ll have to try some newer seeds.\nThe latest issue of the magazine, Heirloom Gardener, has a great article on assessing your garden soil to be sure that it is ready for the coming growing season. The article addresses four main points that every gardener should be thinking about at the beginning of each growing season.\n- Soil pH. This is a measure of the acidity of your soil. A pH of 7 is neutral – anything below that is acidic and anything above that is alkaline. Soil pH is important because it affects nutrient availability. The ideal soil pH for a vegetable garden (depending on what you read) is somewhere between 5.5 and 7.5 – if a soil has a pH above or below this range, certain essential plant nutrients will become less available, affecting the growth of plants in your garden and their potential yield.\n- Soil Test. Determining your soil pH can be done by doing a simple soil test. The soil test will also let you know what nutrients are available in your soil and to what extent. Knowing the fertility of your soil will help you decide what steps to take in terms of adding organic matter and fertilizer to your soil. What amendments are needed will also be determined by what plants you are planning to grow, but having that soil test will at least give you a baseline to work from. Check with your local county extension agent for more information on how to take a soil sample and where to send it for analysis.\n- Soil Amendments. The spring is a good time to add amendments to your soil. The ideal thing to add is mature compost. The best soil for a productive vegetable garden is one that is loamy (referring to a mixture of sand, silt, and clay particles) and contains a large amount of organic matter. The organic matter (especially when highly decomposed) provides structure, drainage, fertility, and a flourishing microbial population to the soil. I have to emphasize “highly decomposed” because organic matter that is not well decomposed could end up being detrimental to your plants. This is because soil microbes, whose job it is to decompose organic matter, need nitrogen to do their job and can “rob” available nitrogen from nearby plants, resulting in a temporary nitrogen deficiency and stunted plant growth.\n- Soil Drainage. The water-holding capacity of your soil is incredibly important and is something you should think about addressing in the spring. Soil that drains too quickly or not quickly enough are both scenarios that are not ideal for a vegetable garden. To test soil drainage in your garden, dig several holes that are at least 2 feet deep and fill them with water. After the holes have drained completely, fill them with water again and keep track of how long they take to drain. A rate of 1-2 inches per hour is ideal. If the test results from your garden are more or less than this standard, the soil should be amended. Adding lots of compost to the soil should address the problem whether it is slow or fast drainage.\nThe health (or condition) of the soil in your vegetable garden is hugely important and will have a large influence on the success and productivity of this year’s crops. So while you’re thinking about all of the things you want to grow this year, take a little time and think about the soil that they’ll be growing in. While it may not seem as interesting as the plants that will be growing in it, good soil will certainly make a huge difference in the long run.\nRead the article, “Is Your Soil Ready for Spring?”, in the Spring 2013 issue of Heirloom Gardener for more detailed information.\nPhoto credit: wikimedia commons']	['<urn:uuid:ab52e6b2-e80f-4dec-9b8e-c288323b31d9>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:06:54.527160	9	17	1705
67	what year national water academy established ministry water resources which organizations help setup	The National Water Academy was established in 1988 by the Ministry of Water Resources, RD and GR, Government of India. It was set up under USAID assistance and was later strengthened with assistance from the World Bank.	"[""National Water Academy\nNational Water Academy (formerly known as Central Training Unit) was set up in Central Water Commission by the Ministry of Water Resources, RD and GR, Govt. of India in the year 1988, to impart training to the in-service engineers of various Central/State organizations involved in the Development & Management of Water Resources. It was established under USAID assistance and strengthened with the subsequent assistance received from the World Bank.\nThe NWA is envisaged to function as a 'Center of Excellence' in training water resources personnel. It is addressing the wider training needs of water resources engineers of States and Central Agencies in the fields of planning, design, evaluation, construction, operation and monitoring of water resources projects. In its national role, the NWA is concentrating on conducting training courses for all water sector personnel, in the specialized and emerging areas, for which the existing State or other institutes are not adequately equipped to meet the needs. Apart from this, in regard to the Central Water Commission, other offices of the Ministry of Water Resources, and for States not having adequate training facilities, it is conducting induction and refresher courses in all relevant areas of the water sector.\nThe regular training programs of NWA are also open to participants from recognized academic institutions, NGOs, Central/State PSUs, private companies, individuals & foreign nationals on payment basis. Click here for more details.\nObjectives and Roles\nThe NWA is envisaged to function as 'Center of Excellence' for in-service training of water resources engineering personnel. The broad objectives of NWA are as under:\n- To organize specialized courses for Group 'A' and 'B' officers of Central and State agencies\n- To arrange National / Regional Seminars and Workshops on key issues of water resources development / subject areas for the benefit of Senior level officers (CE's / E-in-C / Secretary) of State / Central agencies\n- To provide assistance to Central and State Government organizations and their training institutes on their specific training needs.\n- To develop and maintain linkages with leading institutions in India and abroad dealing with training related activities in water resources sector for sharing the expertise.\n- To conduct training in advanced methods of structural analysis and design.\n- To develop training modules / case studies on new emerging technology like GIS applications in water resources.\n- To organize Induction Training for newly appointed Assistant Directors / Assistant Executive Engineers of Central Water Engineering (Group 'A') Services.\n- To organize induction / orientation training to newly appointed / promoted Group 'B' engineering officers of CWC.\n- To organize Re-orientation Courses for officers promoted from Group 'B' to Group 'A' of Central Water Engineering Services.\n- To undertake Induction Training of newly appointed Group 'B' / Group 'A' officers of state water resources department / Central agencies on their request, where such training facilities are not available.\nThe NWA is headed by a Chief Engineer and has eight other core faculty members. The core faculty comprises of CWES (Central Water Engineering Services-Group 'A') officers who have long practical experience in Water Resources Development and Management. The guest faculty comprises of academicians and scientists of eminence from premier Research Centers and Universities in India, as well as practicing professionals and specialists drawn from other organizations and agencies.""]"	['<urn:uuid:352f9e1f-11e9-4111-869a-55a617cdcd67>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-13T05:06:54.527160	13	37	546
68	Which famous visual artists collaborated with the Ballets Russes?	The Ballets Russes collaborated with numerous renowned visual artists and designers including Pablo Picasso, Salvador Dalí, Coco Chanel, Henri Matisse, Joan Miró, Natalia Goncharova, André Derain, Léon Bakst, and Georges Braque, who created works for the company's stages.	['The Legacy of the Ballets Russes by FIRE AND AIR Assistant Director Sophie Andreassi\nSerge Diaghilev’s Ballets Russes existed for a mere twenty years, and yet it is difficult to overstate the company’s influence on dance, art, music, and design in Western Europe and beyond. From its inception in 1909 to Diaghilev’s death and its dissolution in 1929, the company drew throngs of spectators, eager to witness the product of Diaghilev’s latest, carefully-orchestrated collaborations among promising artists of different disciplines. The itinerant company’s experiments galvanized the art world in their time, and forever altered the trajectory of art and culture in the 21st century.\nDiaghilev capitalized on a vibrant Paris arts scene and engaged his talented circle of Russian émigrés. Artists who secured Diaghilev’s approval were poised to take on a near-cult following. Visual artists and designers such as Pablo Picasso, Salvador Dalì, Coco Chanel, Henri Matisse, Joan Miró, Natalia Goncharova, André Derain, Léon Bakst and Georges Braque enjoyed the exposure afforded them by creating works for the company’s stages. The company’s composers included Claude Debussy, Francis Poulenc, Sergei Prokofiev, Erik Satie, Igor Stravinsky, and Richard Strauss. If not already prominent in their field, these and other composers achieved legendary status, especially after the creation of such enduring works as Stravinsky’s Rite of Spring and Debussy’s L’après-midi d’un faune.\nDiaghilev’s choreographic protégés, including Michel Fokine, Vaslav Nijinsky, Bronislava Nijinska, Léonide Massine, Serge Lifar, and a young George Balanchine, tested the viability of currents within modernism at the level of the body while exploring the limits of the classical ballet vocabulary. Significantly, the Ballets Russes challenged the centrality of the idealized female body to ballet. It was the male body was of special importance to Diaghilev’s aesthetic.\nFollowing Diaghilev’s death, offshoots of the company formed to continue to tour the company’s existing repertoire internationally. Over the course of the 1930s and 1940s, national companies on both sides of the Atlantic grew in prominence, often under the purview of Ballets Russes disciples. On the American side, Fokine, Massine, and Nijinska were some of the first choreographers to be engaged by the newly-formed Ballet Theater, now known as American Ballet Theater, in New York in 1939. Balanchine, too, moved to the States and, with Lincoln Kirstein, created the School of American Ballet in 1934 and the New York City Ballet in 1948.\nIn the 1970s, Robert Joffrey, director of the Joffrey Ballet, began a project of reconstructing select pieces from the Ballets Russes canon, inviting Massine to oversee some of his and Fokine’s ballets. Later, Joffrey set about the ambitious project of reviving works by Nijinsky, including Rite of Spring, relying on extensive research by dance historians to attempt to capture the spirit of the piece in the absence of any films.\nUnder Diaghilev’s ingenious, if heavy-handed, supervision, the Ballets Russes reimagined ballet as the fusion of movement, art, and music. The product of these experiments was a rich legacy of innovation and an addicting history that continued to inspire artists, scholars, and the popular imagination through to the present day.\nPhoto: Alexandra Danilova and Serge Lifar in the Ballets Russes’ Apollo (originally Apollon musagète) in 1928.']	['<urn:uuid:13fabf4d-0b4c-48f1-85e4-2eb99468e986>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:06:54.527160	9	38	524
69	compare civilian military lawyer rights warning	For civilians, Miranda warnings must inform them that they have the right to consult with a lawyer and have one present during questioning, and if they cannot afford one, a lawyer will be appointed. In the military system under Article 31, suspects must be advised of the nature of accusations and their right to remain silent, with their statements potentially being used as evidence. This was highlighted in a recent case where a Navy member argued his Article 31 rights form was inadequate for not mentioning the right to an appointed civilian attorney, though the court found it adequately communicated his rights even without explicitly stating this.	"[""Criminal barricade solicitors can be often sought after just by consumers irrespective of whether expenses alongside them all may well possibly be sacked given that legislations enforcement officers performed not even aid these products about most of the protection under the law well before arresting these.\nUsing your newly released Country wide claim when an example of this, the actual functionality involving the posting is to evaluate the classification in between any civilian Miranda dire warnings and even a good armed forces suspect’s rights less than Page 31(b) regarding the Even Computer code of Marine The legal (UCMJ).\nMany are actually accustomed through Tv on pc transgression dramas or perhaps videos utilizing the actual well known Miranda dire warnings.\nWhatever is certainly never at all times perceived around typically the procedure first publicized during 1966 by simply any Huge The courtroom is usually that constraints in typically the tip and additionally the actual possible treat should truth be told there is without a doubt some sort of breach from that rule from a law enforcement officials. Furthermore there tend to be at the same time major disparities around Miranda plus that rights an important army suppose is known as to according to Guide 31, some control which predates your Miranda determination by means of above some sort of era.\nTypically the steam steam reduction relating to a Miranda tip might be that will your think must get with guardianship that will cause the condition in order to counsel these individuals of their particular proper rights. hr management product scenario study\nThe “Public Safety” exception\nA fabulous man or woman might be regarded in order to become throughout custody while a fabulous reasonable man with this identical situation would probably never look and feel who they ended up complimentary towards go away.\nSuppose a fabulous civilian opponent was first during child custody and additionally was initially open in order to pondering on by regularions enforcement lacking very first getting well-advised about most of the proper rights, the actual treatment is actually any exclusion associated with that arxiv remove duplicate content article assertions with free trial.\nSome sort of abuse, even so, can certainly not entail this some sort of lawbreaker criminal prosecution could not keep going in cases where at this time there might be other sorts of facts an important law-breaking seemed to be committed.\nBy comparing, Article Thirty-one necessitates which usually someone kaddish ginsberg might be theme to the UCMJ, no matter whether or simply not even they usually are a designated regulations enforcement officer, counsel a fabulous military suppose regarding a mother nature herself regarding typically the accusations, who all the man or woman report Thirty-one the law alerting assertion a new most suitable that will continue being calm, along with which everything they express might possibly be made use of as the evidence against them all whatever the case about no matter whether a suppose is actually in custody.\nA recent released option coming from any 11th Routine Legal of Is attractive shows the variation for the actual circumstance about the children pornography requested by prosecution regarding a strong dynamic work Fast representative in Country wide the courtroom.\nAll the accused seemed to be billed within Country wide courtroom through violating national young child porn procedures just after his particular ex-wife identified visuals from young child porn about his / her own computer system which usually your sweetheart had taken any time your woman went available connected with the actual home.\nYour woman made this home pc above so that you can solutions by this Naval Unlawful Investigative Services (NCIS) so directed an professional for you to his work place to make sure you meeting her. a defendant was explained to of which he / she ended up being not really within guardianship as well as the person had been not even handcuffed and retrained.\nPreceding to be able to curious about, the actual adviser encouraged typically the defendant in his or her Piece of writing Thirty-one rights working with your traditional service shape.\nSubsequently after admitting his protection under the law, the actual opposition then simply developed incriminating promises the fact that were definitely down the road unveiled through facts immediately after her action to help suppresses that statement ended up being denied.\nThe rationale associated with typically the defendant’s movement to suppress his particular incriminating claims has been which usually the waiver type the guy ok'd does not likely match the Third Variation and Miranda since your armed forces shape was false through meals her perfect in order to some sort of appointed civilian attorney at law in cases where he / she can never find the money for a single.\nThis court had possibly not correct the particular concern associated with no matter whether he had been for legal care considering the software seen which possibly even if this individual was initially in no way complimentary to be able to give, that service develop realistically and additionally adequately communicated all the Third Variation not to mention Miranda liberties still in the event that the software made not really point out those people rights simply by case learn interview techniques"", 'How to Survive Your Criminal Case in Florida (part 5 of a 9 part series)\nHow to Survive Your Criminal Case in Florida\n(part 5 of a 9 part series)\nBy: Carlton “Duke” Fagan, Esq.\n101 Century 21 Drive\nJacksonville, Florida 32216\n(904) 733-1234 – Telephone\n“BUT THEY DIDN’T READ ME MY RIGHTS!”\nWhen someone says this, they are usually referring to what is known as their “Miranda Rights”. These rights came from the United States Supreme Court case, Miranda v. Arizona. Because of television, most people have a pretty good idea of what their Miranda Rights are. Here is the way the Supreme Court stated them in Miranda:\nAt the outset, if a person in custody is to be subjected to interrogation, he must first be informed in clear and unequivocal terms that he has the right to remain silent.\nThe warning of the right to remain silent must be accompanied by the explanation that anything said can and will be used against the individual in court.\nAccordingly, we hold that an individual held for interrogation must be clearly informed that he has the right to consult with a lawyer and to have the lawyer with him during interrogation under the system for protecting the privilege we delineate today.\nWhile authorities are not required to relieve the accused of his poverty, they have the obligation not to take advantage of indigence in the administration of justice… In order to fully apprise a person interrogated of the extent of his rights under this system then, it is necessary to warn him not only that he has the right to consult with an attorney, but also that if he is indigent a lawyer will be appointed to represent him.\nThe reading of these rights by the police does not have to be word-for-word as they are written above. Any recitation of them that is substantially similar and clearly informs the person in custody of these rights will do.\nAgain, due to television, most people mistakenly believe that the police must read them their rights if they are arrested. This is not true. There is no requirement for the police to read a person their rights when they are taken into custody. However, if the police fail to read the Miranda Rights to someone who is in custody AND fail to obtain a waiver of those rights, then any statement by the arrestee is NOT admissible against the arrestee in court. REMEMBER, Miranda only applies when someone is in the custody of law enforcement. Any statement made by someone who is not in custody is admissible against them in court.\nThe moral of the story is – DO NOT talk to the police without your lawyer present. If you are questioned by the police, you should politely tell them that you want to talk to your lawyer. No matter what they offer to do for you or threaten to do to you, politely (even if they are not polite to you) tell them only your name, address, date of birth and that you want to talk to a lawyer. Say nothing else. This is your constitutional right. Never feel awkward about asserting it. Would you feel awkward about asserting your right to vote, to go to church wherever you choose, to speak freely, to vote, to own property, to own a gun? Your right to remain silent is just as important and precious a constitutional right as any of these. Do not hesitate to insist upon it. Our forefathers demanded it and the patriots who came before us purchased our rights with their lives. Never give them up.\nLeave a ReplyWant to join the discussion?\nFeel free to contribute!']"	['<urn:uuid:b03ca1c5-d15e-4e89-a4bb-b7c861fea486>', '<urn:uuid:c3947fd1-ee59-45a0-945c-b3cb6ac2ec93>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-13T05:06:54.527160	6	107	1482
70	Can you compare RePower South's approaches in Montgomery versus their general fuel technology?	In Montgomery, RePower South operates a city-owned facility where they process residential waste through high-tech machinery to sort and recycle, creating a low-carbon clean fuel from non-recyclable paper and plastic, investing over $10 million in capital. Their general fuel technology, known as ReEngineered Feedstock (ReEF), is specifically designed to replace at least 50% of fossil fuel in industrial and utility boilers, utilizing a distinctive rocket mill that grinds, dries, and cleans the material into a cotton-like fluff for consistent quality.	['At a ribbon-cutting ceremony last week, the city of Montgomery, Ala., RePower South (RPS) and the Montgomery Area Chamber of Commerce announced that the Montgomery Recycling and Recovery Facility has resumed advanced residential recycling operations.\n“This ribbon cutting speaks to our commitment to do what’s right by becoming a green city and a destination known nationally as a leader in recycling, innovation and technology,” said Montgomery Mayor Todd Strange in a statement. “Partnerships mean progress in Montgomery, and thanks to our partnership with RePower South, we are bringing a cost effective, environmentally friendly and seamless sanitation solution to our residents. Not to mention, this facility will save taxpayer dollars by extending the life of our landfill at no additional cost to our city.”\nAll Montgomery residential sanitation customers will participate in the program without any extra effort or sorting. Sanitation crews will collect garbage from residents’ green cans—just like the current system—and deliver to the Montgomery Recycling and Recovery Facility where high-tech machinery will sort and recycle. At a 100 percent participation rate, Montgomery will lead the state in recycling.\n“Our platform enables greater recycling recovery and does so across the entire Montgomery waste stream,” said RPS President Scott Montgomery in a statement. “RePower South creates a low-carbon, clean fuel to help reduce the consumption of coal. Greater recycling, less landfilling and cleaner air at lower cost is a true win-win for the city of Montgomery. We know there’s a better way for our nation to manage garbage, and we are excited that the city of Montgomery agrees. We look forward to this facility serving as a model for the world to move toward a more sustainable waste and energy future.”\nRPS reached an agreement with the city in June 2018 to operate the city-owned facility. In the months since, RPS invested more than $10 million in capital and new additions to the facility, spending more than $12 million total on the project. Among the new additions is the latest in advanced recycling machinery essential to turning traditionally landfilled waste, like non-recyclable paper and plastic, into low-carbon, clean fuel. This alternative to coal consumption will be sold to industrial customers and utilities, which will eliminate RPS’ dependence on a volatile commodities market, a weakness in the facility’s previous operator’s strategy.\n“Montgomery has a long history of trusted public-private partnerships, and we are thrilled to add RePower South to our growing business ecosystem,” said Willie Durham, Montgomery Chamber of Commerce board chairman, in a statement. “This move is just one of many smart city solutions coming to Montgomery’s waste management sector, and we look forward to continuing this momentum into other areas, including access to free Wi-Fi. This type of innovation is just one more benefit to living and working in Montgomery.”\nThe private-public partnership includes a revenue sharing provision, which means the city could receive up to $200,000 annually from RPS, if sales of recovered materials increase. Big savings will also be realized for the city by reducing maintenance and capital costs for sanitation equipment and extending the life of the landfill through the diversion of household garbage and recoverable materials. Construction of a new landfill cells generally cost approximately $2.8 million. RPS Montgomery will create more than 65 new green jobs in the River Region at an average rate of $16 an hour.\nRPS development partners include Barnhill Contracting Company as general contractor; Bulk Handling Systems to refurbish and upgrade the waste processing system; and Loesche Energy Systems to provide the fuel manufacturing system. RPS licenses the fuel technology (ReEngineered Feedstock) from Accordant Energy.', 'As the concept of renewable alternative fuel has evolved, so have several terms classifying these products. Some of those terms are “refuse-derived fuel” (RDF), “engineered fuel” and “solid recovered fuel.” On top of these designations, companies coin their own brands to try and distinguish themselves. But how do you differentiate between products? Which fuel type makes sense for what application? What are the regulations around each? And what are associated costs?\nRDF actually has been around for nearly 30 years. The term was created to try and set the product apart from fuel created via mass burn technology.\n“Mass burn was leveraged by big power plants where trash was burned as it came off the truck. RDF plants shredded and mixed it. It was not necessarily cleaner, but you could get more energy value by doing this preparation to make low-grade fuel,” says Steve Simmons, president of Gershman, Brickner & Bratton.\nRDF is only suitable in a few applications and has limited fuel replacement capabilities, which include applications that are not very sensitive to fuel quality fluctuations and/or where there would be low fossil fuel substitution rates. A common example would be fluidized bed borders with heat generated from combustion used to make steam. Otherwise, introducing RDF can upset a plant’s process, says Matt Allers, owner’s representative for RePower South.\nSo, along came a more processed product called engineered fuel with less moisture, higher heat value and more consistent quality. Also known as solid recovered fuel (SRF), depending on quality and process, it can replace up to 100 percent of fossil fuels to generate electricity or heat. Though it is more often in the range of 30 to 60 percent. End users are typically operators of cement kilns and boilers.\nRePower South bought the license to a SRF that’s branded as ReEngineered Feedstock (ReEF) that replaces at least 50 percent of fossil fuel in industrial and utility boilers.\nLike other engineered fuels, one of ReEF’s main sells is it contains less carbon than fossil fuels. It’s made mainly from fibers from paper and cardboard as well as polymers.\n“We did a successful trial with ReEF at a coal-fired power plant in operation on the grid. They can’t afford to upset their boilers, so there are stringent quality criteria,” says Allers.\nSRF technologies require a high investment in equipment, such as dryers to remove moisture and optical sorters to remove high-chlorine plastics. RePower South’s technology includes a shredder, advanced sorting and the most distinguishing feature is the rocket mill that grinds, dries and cleans the material, which is beaten into a fluff.\n“It’s like cotton and burns better. The rocket mill also homogenizes the fuel to make the quality more consistent,” says Allers.\nRDF is generally less expensive to produce than SRF. But there is no open market for it in the U.S. And no one is developing new projects, says Simmons.\nThere are only a handful of plants left, including in Orrington, Maine; West Wareham, Mass.; Hartford, Conn.; Portsmouth, Va.; and a couple in Florida. One in Detroit and another in Elk River, Minn., are closing.\nMeanwhile, SRF projects are gaining traction. Entsorga’s West Virginia plant, RePower’s South Carolina and Alabama plants and an operation at Wasatch Integrated Waste Management District in Utah are among them.\nSimmons knows of others he can’t name because they are under nondisclosure agreements.\nThese cleaner fuels meet less strict regulations than their minimally processed predecessors.\n“The U.S. Environmental Protection Agency classifies SRF as non-hazardous secondary material. If it’s burned in a kiln, it’s regulated as any material burned in a kiln. But if RDF were burned in the same environment, it would be regulated as a cement kiln and as a waste-to-energy plant or incinerator. So, you would have to comply with two levels of regulations,” says Simmons.\nEntsorga makes an engineered fuel it calls Prometheus. Prometheus can be produced according to customer’s specifications by biodrying municipal solid waste, mechanically refining it and possibly adding selected commercial and industrial waste. This is done to homogenize the chemical composition and raise British thermal unit content, says Paolo Carollo, vice president of operations for North America at Entsorga.\nHe believes engineered fuels will help solve problems of thermal technologies like gasification and pyrolysis.\n“Some of the recent failures in this space were mostly due to the non-homogeneity of feedstock, resulting in difficult-to-control process conditions … Considering that the quality of processed engineered fuels is homogenous and they can be produced according to a specification, we believe there is great opportunity for them to improve advanced thermal treatment technologies,” says Carollo.\nStill, there are issues to iron out such as securing enough feedstock to run these plants. And a need for a new recycling and recovery model.\n“Future opportunities will be based on a model where processing facilities will recycle/upcycle high-value waste streams and where unsorted municipal waste and residual, nonmarketable commodities will be used to produce this low-carbon alternative fuel,” says Carollo.']	['<urn:uuid:f013aafd-f93f-4f05-a2fe-b3ae87548e1a>', '<urn:uuid:36a37709-a42c-4858-ab24-d157257ab6e3>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T05:06:54.527160	13	80	1421
71	moving food storage transport rules restrictions	Perishable foods, including frozen and refrigerated items, cannot be transported in moving containers or by moving companies. Even pantry items and pet food should not be stored or moved as they may attract insects or rodents, or develop mold that can damage other belongings. The recommendation is to donate food to local food banks or neighbors before moving. For canned goods specifically, if they must be transported, they should be carried in open containers to prevent explosion from uneven air pressure.	"['Furniture, kitchen appliances, hobby equipment, clothes, toys, books – the options seem pretty much endless for the type of items you can pack or store in a portable moving container. So while most things can be put into storage or transported to your new location, it is important to remember that not all of your belongings should be packed. There are some things that you never want to pack or store in a moving container and it’s important that you set those things aside well before your move. Some of these items you will want to handle personally and some you may want to dispose of or gift to a friend.\nImportant Personal Documents – Birth certificates, social security information, passports, wills, tax returns, financial statements, health insurance, medical records, employment documents, mortgage or rental contracts, and other personal documents should stay with you throughout your move. Place these important documents into a locked box or briefcase and transport them with you.\nHazardous Materials – There are certain things in your home or garage that are considered hazardous when moving and should not be placed in a portable storage container. This list includes items such as aerosol cans, fertilizer, pesticides, charcoal, disinfectants, ammunition, fireworks, pool cleaning chemicals, paint thinner, fire extinguishers, car batteries, and gas cans. This also includes anything that is considered an illegal substance. Always consult with the experts at Zippy Shell if you are unsure about a particular item. You can consider giving these away to neighbors before moving or properly disposing of them.\nPerishable Items – Frozen and refrigerated foods can quickly spoil inside a moving container. Even packing pantry items or pet food in a moving container is a bad idea if it will be stored for any length of time. It may attract insects or rodents or grow mold that will cause significant damage to the rest of your belongings. You can donate food to a local food bank to help those in your community before you move.\nPlants and Flowers – Your delicate, leafy friends will almost certainly not survive the trip in a portable storage container. You will be better off moving your plants yourself or gifting them to a friend before moving.\nDirty or Wet Items – Clean everything before it goes into a storage container, especially if it will stay in there for any length of time before being unpacked. If you store something while it is dirty, it’s only going to be dirtier when you unpack it one, two, or twelve months later. Clothes, furniture, and all appliances should be as clean and dry as possible before storing them away during your move. Anything that is wet or damp will start producing and spreading mold and mildew while in storage.\nTo keep your belongings safe and secure while you move, Zippy Shell storage centers are equipped with climate control and added security. When you are ready, we will redeliver your storage container or move it to your next destination.', ""Relocation day is here, and everything’s sealed and ready. But then, movers come and tell you they can’t transport some of the boxes. To avoid getting surprised (and upset,) learn about the items movers won’t move. Some of them aren’t transported for a good reason, so start taking notes and prepare well.\nMake sure you have a list of prohibited items for transport, so nothing gets left behind\nBefore Packing, Ask Your Long-Distance Moving Company for a List of Items Movers Won’t Move\nOne of the most common relocation mistakes is not learning what to get rid of when relocating. That doesn’t just mean things you don’t need anymore, but also those that cannot be transported even with professional long-distance moving services. They classify them into prohibited and restricted items.\nThere is a comprehensive list of substances and materials that none of the cross-country moving companies wish to deal with. Packing them doesn’t count into the hourly rate of a cross-country moving company because they’ll tell you even before providing packing services what they don’t transport.\nThis shouldn’t change your reasons to move. It could just affect how and what you pack. The sooner you know what not to bring, the more efficient you’ll be in preparing for relocation day.\nDeclutter to Reduce the Amount of Cargo and the Cost of Moving\nThe cheapest way to move out of state is to pack the bare necessities. Additionally, consider your losses. What do you think is worth packing and moving out of your home? For most people, that’d be something hard to replace or expensive to buy new.\nOn the other hand, look at the stuff in your home that’s easily replaced and available everywhere. There’s no need to make room for it in the boxes if it’ll just be replaced or bought new when you move. You’ll see how the relocation budget will start to look more generous the less you pack.\nSome things don't have to come with you to the new home. It's better to replace them if that's easy to do.\n1 So, What Movers Won’t Move? Hazardous Materials Are Prohibited for Transport\nDo you know what’s considered hazardous? It could come as a surprise to you, but things that are prohibited from ever being transported by professional moving companies are some of the most common daily stuff. Cleaning products, batteries, even wax candles are just some of the stuff on the list.\nYou can reduce costs by not packing anything that cleans the drains and bathroom. After all, relocating to a smaller home will require less cleaning, which will subsequently require fewer chemicals laying around.\nAdditionally, if you ignore the warning of a relocation company about prohibited items and pack them anyway, any damage that the chemicals might cause wouldn’t be covered by them. Chemicals react to changes in environment, temperature, and movement. If they experience all three, you’ll have bigger problems later on.\nExamples of Hazardous Materials and Items\nHere are some of the most common hazardous and damaging materials that moving companies don’t transport:\n- Motor oil,\n- Cleaning supplies,\n- Pool chemicals,\n- Nail polish remover,\n- Car batteries,\n- Lighter fluid,\nAnything that falls under these categories won’t be accepted for transport by professional movers. Prohibited and restricted items for transport come in other types, too, which you can read more about below.\nGas tanks are extremely dangerous for transport, and they're always shipped by trucks specially made just for that. Moving trucks aren't one of them.\n2 Flammable and Explosive Things Have to Be Left Behind\nIf you plan to relocate to the suburbs, you’ll probably want to have a nice barbecue set in the backyard when the family gathers on the weekends. However, it’s best to avoid bringing the gas bottle, the coal, and the fire extinguisher for the set and simply buy them after relocating.\nWhether you hire cross-country movers or try to move alone, transporting hazardous and flammable materials is not recommended. They can have severe reactions to changes in temperature and environment, which could cause them to explode and wreak havoc on other cargo.\nMore examples of flammable and explosive materials prohibited for transport are:\n- Scuba tanks,\n- Propane tanks,\n- Guns and ammunition.\nIf you’re not even sure how to store some of these substances properly, the short animated video below explains where to look when storing them and what happens when they’re not correctly placed.\n3 Perishable Foods Can’t Come Along\nDon’t pack any food when you prepare for long-distance movers to collect your stuff and load it into the truck. This is especially true for perishable ingredients like meat, fruits, and vegetables. They can go bad quickly if they’re out of a fridge or at a non-ideal temperature.\nRotting produce and meat can cause all kinds of molds and bacteria to fester. It’s not enough to say that it smells terrible; mold is actually dangerous for people. Additionally, it can severely damage furniture if uncontrolled. There’s the other issue of rats; if they smell food in the truck, they can crawl in and wreak havoc on anything and everything lying around.\nIf you’re moving cross-country alone, it will be easier to pack a couple of cans of food and bring them in an essentials box with you. Canned goods are not easily perishable but should be transported in open containers. That way, they can’t explode from uneven air pressure. Otherwise, maybe you can skip bringing the food and just order in for a few days until you unpack.\nLeave Fresh Ingredients You Have to Someone Who Needs Them More\nIn the case of a last-minute relocation, you may not know what exactly to do with the perishable foods left in the fridge. This is an excellent time to donate unwanted things. Food banks and soup kitchens always accept donations. Even your neighbor might benefit from some of those fresh tomatoes sitting in the fridge drawer.\nAsk around if anyone needs or wants to accept extra food, so you don’t have to throw it in the trash. Helping someone in need is guaranteed to make you feel better and relieve the stress of relocating.\nYou can donate any fresh produce you have leftover before relocating. Maybe someone in the neighborhood would accept it, too.\n4 Living Things Like Plants and Pets Are Your Responsibility\nBeing responsible for other living beings and things can cause a lot of relocation stress. Relocating with kids is hard enough, especially when they’re toddlers or just babies, but creating a safe environment when relocating with pets and packing plants can get equally stressful if not done right.\nProfessional relocation companies aren’t allowed or required to handle plants and pets during transport. While this should make sense, we’ll still talk about why that’s so. Many people call their pets their ‘babies,’ and many do the same for plants. Both require regular care and attention, and being locked inside a relocation truck isn’t the equivalent of that.\nYour pets should stay in their carriers and travel with you in the car. Keep their treats and some water nearby so they can handle the trip nourished and taken care of. Pets can feel relocation depression as much as humans, so being there for them is more than vital in this period.\nHow to Pack Plants for Transport\nIf relocating to a big city from a smaller place, you’ll have to figure out how to keep and grow greenery in a different environment. Before that, however, you need to learn how to prepare them for relocation properly. While transporting flowers across the country isn’t recommended, there are ways to do it without harming them too much.\nTry following these tips for relocation day if you want to move plants:\n- Pack the smaller ones into a shallow container, like a cardboard six-pack holder or a wooden crate that you can get from a store. Keep them in a well-lit place in the car,\n- If some of them don’t like the sun too much, they might be able to brave the trip in the trunk,\n- Water them on relocation day and wait for the water to drain before placing them in the car,\n- Ensure to let fresh air in to give the flowers (and yourself) some oxygen during the drive.\nThese are simple ways to transport any plant. However, there may be more arguments for leaving them behind instead. You’ll rarely hear from any mover or plant lover that relocating them across long distances is safe. Besides, if you decide to give your car to the relocation company for interstate vehicle shipping, it’ll be hard to bring any delicate cargo.\nConsider these options besides transport:\n- Give away each plant to friends, neighbors, and family members,\n- Sell them at a garage sale or even online,\n- Donate them to local organizations or facilities like nearby schools, care homes, or even restaurants and bars if they’d like them,\n- Leave them where they are and contact the new residents to give them care instructions.\nIt can be easy with pets, but plants are tough to transport. Consider leaving them behind to keep the move as simple as possible\n5 Personal Valuables and Documents Should Only Be by Your Side\nThe upside of cross-country moving services is that some things can stay in the company storage units if it comes to that. Maybe some furniture in the new home has to be moved out before yours can be placed. However, you wouldn’t wish your social security card or mortgage papers to be locked away in some storage by accident.\nDocuments and valuables are part of the relocation essentials list. These are best transported close to you, in a special box or bag, so you know exactly where to find them later. It is up to you to organize them, but the first thing to do after relocating is to ensure all the valuables and papers made it home with you.\nRelocation companies don’t accept transporting personal belongings and documentation because they don’t want any repercussions in case of damage or disappearance. It makes sense because valuable stuff should be kept close by, so why risk it? It’s best if you determine what’s valuable and pack it accordingly.\nPack the essential documents in a binder or bag, and keep them by your side during the move.\nPreparing Well for a Move Means No Damage or Trouble Afterward\nMaking a household inventory list and double-checking it with movers is an excellent first step when relocating. They will immediately tell you what’s not possible to transport. We hope you understand the importance of leaving some stuff behind when relocating. That way, damage and trouble will be avoided, and the move will be successful.\nMaybe it is clearer now how important it is to be well-organized and prepared for relocation. Preparation is beneficial in many aspects, from saving money and time to reducing stress and anxiety about relocating. Ensure this time goes well for you by being ready.\nWhat Are Some Items That Movers Won’t Move?\nThere are some items that long-distance movers typically won’t move, including:\n- Hazardous materials: Cross-country movers are not allowed to transport hazardous materials, such as propane tanks, chemicals, and gasoline.\n- Perishable items: Movers may not transport perishable items, such as food, plants, or living creatures.\n- Valuable items: Movers may not transport high-value items, such as cash, jewelry, or important documents.\n- Restricted items: Movers may not transport items that are restricted by law, such as firearms or explosives.\nWhy Do Movers Have Restrictions on Certain Items?\nSome items are too dangerous to transport, such as hazardous materials or explosives. Movers must prioritize the safety of their employees and other customers by avoiding these types of items. There are certain items that are restricted by law, such as firearms or drugs. Cross-country movers must comply with these regulations to avoid legal consequences.\nHow Can I Dispose of Items That Movers Won’t Move?\nIf your movers won’t move some items, but they are still in good condition, you can try selling them or donating them to a local charity or non-profit organization. Websites like Craigslist or Facebook Marketplace can be a great place to sell items, while charities like Goodwill or the Salvation Army will accept donations of gently used items.\nWhat Are the Risks of Moving Restricted Items With a Moving Company?\nCross-country moving companies often have a list of restricted items that they cannot transport due to safety, legal, or ethical reasons. If you move restricted items without obtaining the necessary permissions, you could face legal consequences, fines, or even imprisonment. Moving hazardous materials or chemicals can pose serious safety risks to the movers, your property, and the environment. These items can be flammable, toxic, or corrosive, and mishandling them could result in fire, explosions, or chemical spills.\nCan I Transport Restricted Items Myself, or Should I Hire a Specialized Carrier?\nTransporting restricted items yourself can be risky, and it may be safer to hire a specialized carrier with the necessary licenses and permits to transport these items. Certain items, such as firearms, explosives, or hazardous materials, require special permits and licenses to transport. It is important to research and comply with all relevant laws and regulations to avoid any consequences. Long-distance moving companies and specialized carriers typically carry insurance that can cover damages or losses during transport. If you transport restricted items yourself, you may not be covered by insurance if something goes wrong.\nHow Can I Prepare and Label Restricted Items That I Plan to Move On My Own?\nBefore you begin, research the legal requirements for transporting the specific restricted items you plan to move. This may include obtaining the necessary permits, licenses, and insurance coverage. Use appropriate containers for the items to ensure their safety during transport. For example, hazardous materials should be stored in leak-proof containers that are compatible with the material. Make sure that all containers are securely closed, sealed, and stored to prevent spills or leaks during transport.\nAre There Any Additional Costs Associated With Moving Restricted Items?\nYes, there may be additional costs associated with moving restricted items, depending on the nature of the items and the regulations governing their transportation.\nFor example, if you are moving hazardous materials such as chemicals or flammable substances, you may need to pay additional fees for specialized handling and transportation, as well as for any necessary permits or licenses.\nSimilarly, if you are moving items such as firearms, alcohol, or tobacco, there may be additional fees and requirements for ensuring that these items are transported safely and in compliance with local laws and regulations.\nWhat Should I Do if I Accidentally Pack or Forget to Mention a Restricted Item?\nIf you accidentally pack or forget to mention a restricted item during the moving process, it’s important to take action as soon as possible. Contact your long-distance moving company and inform them about the restricted item. They may have procedures in place for dealing with such situations and can provide guidance on the next steps.\nCan I Purchase Additional Insurance Coverage for Restricted Items During a Move?\nIt depends on the cross-country moving company and the nature of the restricted item. Some moving companies may offer additional insurance coverage for certain types of restricted items, such as valuable or fragile items, but may not offer coverage for hazardous materials, firearms, or other items that are prohibited by law.\nHow Can I Determine if an Item Is Restricted or Prohibited From Moving, and Where Can I Find a Comprehensive List of Restricted Items?\nTo determine if an item is restricted or prohibited from moving, you can start by checking with the relevant regulatory agencies and reviewing their guidelines and requirements. The specific regulations that apply to your items will depend on a variety of factors, such as the type of item, its value, and its potential hazards.""]"	['<urn:uuid:02a19893-563c-4e81-a452-12b031b8ef0f>', '<urn:uuid:a670baba-56ee-4e92-a8d2-b5032659b9e2>']	factoid	with-premise	short-search-query	distant-from-document	three-doc	expert	2025-05-13T05:06:54.527160	6	81	3162
72	what happened to us president health during world war 1 summer 1918 secret treatment	During the summer of 1918, while U.S. troops were fighting in World War I, the President underwent a secret medical procedure, likely to remove polyps from his nose. This treatment was kept extremely quiet, known only to the White House doctor Cary T. Grayson, the president's wife Edith Wilson, a nurse, and a White House usher. The White House went to great lengths to conceal the president's frail condition during this important period of world events.	"['During Woodrow Wilson\'s presidency...\n- Women received the right to vote with the passing of the 19th Ammendment.\n- The Federal Reserve System was established.\n- Income tax was initiated with the birth of the Internal Revenue System.\n- World War I broke out in Europe between 1914 and 1918.\n- Sheep grazed on the White House lawn to help the Red Cross raise wool for the war effort.\n- The national observance of Mother\'s Day was established.\n- Image is on the $100,000 bill although it is no longer in circulation\nWoodrow Wilson was the first president to...\n- Personally deliver what is known today as the ""State of the Union Address"".\n- Hold regular news conferences.\n- Cross the Atlantic Ocean while in office.\n- Was the first president to attend the Major League Baseball Fall Classic. He saw the debut of a young 20 year old pitcher by the name of George Herman ""Babe"" Ruth.\nWoodrow Wilson and the Great War\nWhen Europe was thrust into war in 1914, Woodrow Wilson, who believed in neutrality, saw America\'s role as that of peace broker. ""The Great War"", as contemporaries called it, was without precedent, involving many countries in a vast and gruesome battlefield. The resumption of unrestricted submarine warfare by Germany and the news of the Zimmermann telegram were part of a long process that persuaded Wilson to ask for a declaration of war. Rarely does one event precipitate a decisive reaction from a thoughtful and deliberate individual such as Wilson. The Russian Revolution and the French Army mutiny convinced the United States that Russia and France would pull out of the war leaving the way open for a German victory. This was unacceptable to the Wilson administration. These two events, combined with the numerous sinking of ships, the increasingly belligerent communications from the Germans, the resumption of unrestricted submarine warfare, and the Zimmermann telegram, drove Wilson’s decision. In April 1917, Wilson asked Congress to declare war, only the second declaration of war in U.S. history. President Wilson was best remembered for his leadership during World War I and his vigorous attempt to establish the League of Nations. The war came to an end on November 11, 1918. At the Paris Peace Conference, Wilson proposed ""Fourteen Points"" as the basis for the peace treaty. The final Treaty of Versailles included many of Wilson\'s ideas. Unfortunately, the U.S. Congress did not support the Treaty. Consequently, the United States never joined the League of Nations. In 1920, Wilson was awarded the Nobel Peace Prize for his efforts on behalf of the League of Nations.\nVisit the links listed below for an insightful journey into the history of The Great War.\nWorld War I Links\nWorld War I Document Archives\nAddress to Senate - Appealing to Europeans for Peace\nWilson\'s Declaration of War Message to Congress\nWilson\'s Fourteen Points\nWilson\'s Address in Favour of the League of Nations\nA Call to Service\nWilson asks soldiers to undertake a great duty\nOn May 4, 2007 the 116th Infantry Brigade Combat Team, historically known as the Stonewall Brigade, mustered and shipped out from the Thomas D. Howie Memorial Armory in Staunton, Virginia for service in Iraq. This unit traces its lineage back to 1742 but became the 116th Infantry Regiment when it was called upon to join the fight on the Western Front in World War I.\nTwo days after the soldiers of the Stonewall Brigade were drafted into Federal Service and only weeks before they would become part of the 29th ""Blue and Gray"" Division that saw battle in the trenches of France, President Wilson sent this message to the soldiers he had charged ""to show all. . . not only what good soldiers you are, but also what good men you are. . . and set a standard so high that it will be a glory to live up to it.""\nWilson and Censorship of the Press: President Wilson hopes the press will observe a ""patriotic reticence""\nWilson\'s position on censorship following the United States\' entrance into the Great War was a complicated one. He could ""imagine no greater disservice to the country than to establish a system of censorship that would deny to the people . . . their indisputable right to criticize their own public officials,"" but he also felt strongly that his administration should have the power to censor information in the interest of national security. The letter to Representative Webb represents Wilson\'s argument for the inclusion of a censorship provision in the Espionage Act that would prohibit the dissemination of information deemed ""to be useful to the enemy"" in times of national emergency. An amendment to that effect passed in the House but did not make it into the Espionage Act voted into law on June 15, 1917.\nIn May of 1918, Wilson was successful in getting the Sedition Act passed as an amendment to the Espionage Act. It became a crime to ""utter, print, write, or publish any disloyal, profane...or abusive language"" about the United States government or to disagree with its actions abroad. The act was repealed in 1921.\nA President\'s Illness Kept Under Wraps: Woodrow Wilson\'s Deteriorating Health Detailed in Doctor\'s Correspondence\nBy Michael Alison Chandler\nWashington Post Staff Writer\nWashington Post Photo\nSaturday, February 3, 2007\nSTAUNTON, Va. While U.S. troops were fighting in World War I in the summer of 1918, President Woodrow Wilson underwent treatment for a breathing problem in a hushed episode that foreshadowed worse health troubles to come.\nThe White House doctor, Cary T. Grayson, later recounted the incident to his wife in one of a slew of newly public documents that show how far Wilson\'s innermost circle went to conceal his frail condition amid major world events.\n""The patient is progressing most satisfactorily, so far, and I have good reasons to hope for a most beneficial result. It has been a big undertaking. . . . No one knows anything about it except Miss E., Miss Harkins, Hoover -- It is one secret that has been kept quiet, so far, and I think it is safe all right now,"" the doctor wrote Alice Grayson in a July 16 telegram.\nThe episode, which caused Grayson great anxiety, most likely involved an operation to remove polyps from the president\'s nose, according to Michael Dickens, a Charlottesville physician familiar with Wilson\'s medical history and the telegram. Historians say the telegram indicates that the procedure was then known only to Grayson; the president\'s wife, Edith Wilson; a nurse; and a White House usher.\nGrayson\'s account was revealed to the public recently through a cache of personal files his family donated to the Woodrow Wilson Presidential Library here in the Shenandoah Valley. The letters, photographs and other documents had been stored for decades at a family home in Fauquier County.\nThe papers offer new insight into the 28th president\'s fragile health and how those around him tried to keep it quiet. Although historians have known for years about Wilson\'s debilitating stroke in late 1919 and his poor health beforehand, including previous possible strokes, the estimated 10,000 documents in the Grayson collection offer fresh and intimate details about the president\'s mental and physical condition from a man whom several Wilson biographers call an extraordinary eyewitness: his personal physician.\n""He literally had his finger on the pulse of the president,"" said historian A. Scott Berg, who has been studying the Grayson papers for a forthcoming Wilson biography. ""Dr. Grayson enjoyed a unique position in Woodrow Wilson\'s life as both his personal physician and a personal friend.""\nThe Grayson documents are the first major donation to the eight - year-old Wilson library, which unveiled them in late December on C-SPAN. The documents are drawing historians to this city two hours southwest of the Washington area where Wilson was born to a Presbyterian minister and his wife 150 years ago. Researchers say about 15 percent of the documents have been explored so far.\nGrayson, born in Culpeper County in 1878 to a well-connected Virginia family, was a naval surgeon who rose to the rank of rear admiral.\nAt the family home in Upperville, Cary T. Grayson Jr. is surrounded by black-and-white photos of his father in an impeccable Navy uniform posing with his ""number one patient,"" as the elder Grayson sometimes called the president. The younger Grayson, now 87, lives there with his wife, Priscilla. He said the papers reveal that the doctor-patient relationship was about much more than health.\nIn the doctor\'s weekly planners, now at the library, Grayson chronicled frequent golf games, automobile rides and nights of theater with the president. From inauguration day in 1913, when they met, to the Sunday morning in February 1924 when Wilson died, Grayson was an almost constant companion. He stayed often at the White House, traveled with Wilson to the Paris Peace Conference in 1919 and was one of a few people who saw him regularly after his stroke that October.\nThe stroke was a turning point for Wilson\'s presidency and, many argue, the world. Wilson collapsed Oct. 2 in the White House after a national tour seeking support for the Treaty of Versailles and America\'s entrance into the League of Nations. He went into seclusion for the remainder of his presidency. The treaty he had so strongly championed was rejected by the Senate in March 1920.\n""This is the worst instance of presidential disability we\'ve ever had,"" said John Milton Cooper, a Wilson scholar at the University of Wisconsin. ""We stumbled along . . . without a fully functioning president"" for a year and a half, he said.\nThe public was largely left in the dark about Wilson\'s condition. The official White House line was that the president was suffering from ""nervous exhaustion."" Other presidents have also concealed health problems, historians say, but the secrecy that enveloped Wilson\'s illness seems difficult to imagine today.\nGrayson died in 1938. Years ago, his family gave Arthur S. Link, a preeminent Wilson scholar, access to his diary. Much of it was published in Link\'s collection of Wilson papers in the late 1980s and early 1990s, offering historians an in-depth look at the troubled end of Wilson\'s presidency and how his health could have affected major events.\nLink\'s grandson is one of the historians now sifting through the Grayson papers to see what else they reveal about the president. Arthur Link III and other researchers were struck by this letter from Grayson to his wife dated Aug. 19, 1915: ""\nMy number one patient in this house had an accident last night with one of his eyes -- the good one, which is bad now. I am hurrying off to Philadelphia with him at six o\'clock tomorrow morning to consult an eye specialist. We are going by motor. I think we can make the trip less noticeable in this way -- . . . [T]he papers will read something like this: The President made his annual visit to the oculist etc etc.""\nThe letter was written during Wilson\'s first term at a time when many historians believe he was relatively healthy. During Wilson\'s second term, Grayson wrote another letter to his wife Sept. 7, 1918, that may be of historical note. It appears to refer to a second operation, similar to the previous one that Grayson probably performed that year on Wilson\'s nose.\nResearchers were also struck by two photographs that experts say show Wilson at his last Cabinet meeting in 1921. In one photo, he is holding a cane; in the other, the cane appears to have been erased.\n""They were flying by the seat of their pants, doing damage control,"" Link said. ""It\'s fun to watch.""\nHistorians say the documents help shed light on what can happen when a president falls ill. The 25th Amendment spells out how a vice president can become acting president in the event of a presidential disability. But that amendment was ratified in 1967, nearly a half-century after Wilson left office.\nA letter Grayson sent from Europe to his friend Samuel Ross on April 14, 1919, shows that Wilson\'s doctor knew the stakes:\n""The president was suddenly taken violently sick with the influenza at a time when the whole of civilization seemed to be in the balance. And without him and his guidance Europe would certainly have turned to Bolshevism and anarchy. From your side of the water you can not realize on what thin ice European civilization has been skating. I just wish you could spend a day with me behind the scenes here. Some day perhaps I may be able to tell the world what a close call we had.""\nNow the papers are gone from the Grayson family basement, attic and closets. But Cary Grayson Jr. still has family stories to share. He recalled how his older brother Gordon went with their father to the White House to cheer the president after the 1919 stroke, sitting at Wilson\'s feet while they watched Western movies. Sometimes they went for drives with him.\nGrayson himself had a stroke in August. He used a cane as he walked around the wooded hills where his father used to go fox hunting. He said he is relieved that the papers have a place in history.\nView a Grayson letter from the eLibrary detailing secret. Click here.']"	['<urn:uuid:c4349d01-bd62-4efe-9ba2-f8287b743dc1>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:06:54.527160	14	76	2220
73	decorating christmas tree minimum safe distance from heaters prevent fire hazard	The Christmas tree should be set up at least one metre from heat sources, including baseboard heaters, other heaters, and fireplaces.	['Tips for a safe Holiday Season\nThe end-of-year holiday season is a time for merriment. But misfortunes such as fire, theft, or vandalism can quickly dampen the festive spirit.\nChristmas shopping precautions\nAs you to and fro picking up Christmas gifts, stow your purchases out of sight in the trunk of your car to avoid break-ins and vandalism. If you can’t hide items in the trunk, cover them up so as not to tempt thieves. Items showcased on the back seat are not safe, even if the car is locked and parked in the mall’s underground parking lot. The constant coming and going in the lot will not dissuade the nimble criminal.\nIf you’re carrying a purse or backpack, don’t forget to check it’s properly closed and keep an eye on it. Pickpockets need only seconds!\nMake sure you tuck your credit card safely away after each purchase; don’t let yourself feel hurried by the line-up behind you. Everyone’s turn will come.\nO Christmas tree, O Christmas tree\nIf you’re opting for a real Christmas tree, make sure it’s a fresh one.\n- The needles should be supple and not drop easily.\n- Leave the tree outdoors, sheltered from the wind and sun, until it’s time to bring it indoors to set up.\n- Put it in a bucket and top up the water every day.\n- Take the tree down as soon as the needles start to drop and dispose of it in an eco- responsible way.\nPutting it up:\n- First cut a slice off the trunk; the new cut will help the tree take up water.\n- Use a solid stand that can contain a lot of water and install the tree somewhere it won’t get in people’s way and cause accidents.\n- Set it up at least one metre from heat sources (baseboard heaters, other heaters, fireplaces, etc.).\n- Make sure you use only certified (CSA, ULC or UL) indoor lights, extension cords and power bars.\n- Do not overload power outlets. If the total watts used by your light sets is more than 1,500, use more than one outlet.\n- Avoid running any extension cables along doorsills or under carpets.\n- Turn off the lights when you’re out or when you go to bed.\nThe same safety recommendations about locating the tree and about tree lights apply if you are putting up an artificial tree.\nUse the special outdoor tree lights if you are decorating an outdoor Christmas tree.\nDon’t play with fire\nMake absolutely sure your fireplace is in perfect working order. Check that the chimney does not need sweeping. If in doubt, call in the specialists. There is no chance worth taking.\nAsh and embers can stay hot for 72 hours. Put the ashes in a metal container with a raised bottom, such as a large can or ash bucket, which you should then store outside, away from all combustible materials.\nUse candle holders that are:\n- Made of non-combustible material such as metal rather than wood or plastic\n- Big enough to catch hot dripping wax\n- Capable of holding the candle securely\nPlace the candle holders on flat surfaces where they cannot be knocked over.\nBuy minimal or non-drip candles.\nPlace candles away from any materials likely to catch fire. Keep your eye on lit candles at all times and away from children and pets – accidents happen so fast.\nDon’t forget to keep matches and lighters out of reach too.\nKeep your kitchen work space clear, especially around the stove. If too many cooks spoil the broth, then too many cooks can also accidentally start a fire.\nDon’t allow children to play where you’re working. A dropped knife, a spilled pan or spitting hot fat can easily cause injuries.\nTake special care with electrical devices such as raclette grills and fondue sets:\n- Make sure the machines are certified.\n- If the appliance is on the dining table, attach the electrical cord to one of the table legs so that no one trips over it.\n- Allow the appliance to cool down before moving it.\nDon’t invite thieves in\nDon’t advertize your plans for the holidays on social media.\nKeep exits clear\nMake sure you clear the entrances and exits to and from your house: basement windows, front and back doors, patio doors, etc. In the event of an emergency, you’ll be glad you did.\nAlso make sure that the access routes to your home are unobstructed and de-iced. The last thing you want is to spend New Year’s Eve in hospital.\nRelax this holiday season, have a great time with your family and, above all, celebrate in safety.']	['<urn:uuid:269c39af-72ec-47ca-a06a-dc49ebe41ee5>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T05:06:54.527160	11	21	779
74	beginner advanced scuba course length difference	The PADI Open Water Diver certification requires completing 5 classroom sessions, 5 confined water dives, and 4 open water dives, while the PADI Advanced Open Water Diver course involves completing 5 open water dives total (including mandatory Deep and Underwater Navigation dives) and approximately 12-15 hours of eLearning.	['PADI Open Water Diver\nExperience another world\nThe PADI Open Water Dive Certification is the most recognized dive certification in the industry. Valid in over 200 countries, no other certification gives you the flexibility and freedom to dive safely with accredited shops worldwide.\nPADI Junior Open Water Diver Course\n- Minimum age 10 years old\n- Maximum depth for 10 to 14 year olds is 40 feet\n- All elements of the Open Water Diver course must be completed to certify\n10 & 11-year olds and parent or guardian must watch “Youth Diving: Responsibilities & Risks” before starting training\n- Children under 18 must have PADI release forms signed by a parent or guardian before starting training.\nCosta Rica Open Water Diver\nThe intensive and fun educational process and the skills you’ll learn will serve you for the rest of your life and is one of the most important steps towards greater freedom you can make in life, as we really only get to see a third of the world without being able to go beneath the waves. The PADI OWD Certification allows you to dive anywhere in the world to a depth of 60 feet or 18 meters, independent of professional supervision.\nThe PADI Open Water Diver Certification is composed of three distinct parts, each designed to build your knowledge and skill base until you are comfortable and confident diving on your own.\n- Knowledge Development – There are several ways to complete the Knowledge Development portion of the course work, which consists of 5 classroom sessions. Students may also opt to study online or independently on their tablet or laptop using PADI’s e-learning application. Learning the basics of scuba before you hit the water is essential and the subjects covered will enable you to build your confidence before you enter the water as well as dispel many common myths and preconceptions about scuba. Once you have finished your coursework, you will be tested on the principles of scuba. When you can demonstrate that you have retained enough to score a 75 percent or higher on an administered scuba test, you can move on to the second portion of the course.\n- Confined Water Dives – Now that you have a background in the basics of scuba and have demonstrated competency of what you have learned, the next step is to get in the water in a confined area such as a pool. The Confined Water portion of the course consists of 5 dives where the student is able to put into practice the skills and knowledge they gained during the study sessions. Under the supervision of your instructor, you will familiarize yourself with many common situations likely to encounter during your dives, such as resetting your mask or breathing off another diver’s tank. These common skills are essential to becoming a safe and confident scuba diver and will form the foundation of your skill base moving forward to the final portion of the course.\n- Open Water Dives – The final step in receiving your certification as a PADI Open Water Diver is to complete 4 Open Water dives in the majestic Pacific Ocean. Now that you have demonstrated your proficiency in the classroom and practiced and honed your skills in a limited environment, it’s time to put all that knowledge and skills to good use to explore the marine world that has been off limits previously. There is little that compares to the 360 degrees of freedom of movement that you have while scuba diving, and the marine wildlife that you will encounter will take your breath away. Once you have completed your open water dives and receive your certification, you will be free to dive anywhere in the world and experience the true variety and diversity of our amazing planet, both above and below the waves.', 'Go beyond basic knowledge and become a PADI Advanced Open Water diver\nMay till December\n2 Days in Cabo:\n- 5 Open Water Dives\n- Physically Fit\n- Be at least 15 years old\n- Be PADI Open Water Certified\nWhy become Advanced:\n- Gain more experience\n- Go deeper 30m (100ft)\n- Make diving more interesting\n- 183 USD E-Learning Theory\n- 375 USD Practical Part in Cabo\nIncluded: Boat , PADI Instructor, All necessary Equipment, Water, Snacks.\nWhat to Bring: Sweater, Camera, Sunscreen, Hat, Towel\nBest time to go:\n- May 70% 70%\n- June 80% 80%\n- July 90% 90%\n- August 100% 100%\n- September 80% 80%\n- October 100% 100%\n- November 100% 100%\n- December 100% 100%\nPADI Advanced Open Water E-Learning\nThat’s what the PADI Advanced Open Water Diver course is all about. You don’t have to be “advanced” to take it – it’s designed to advanceyour diving, so you can start right after earning your PADI Open Water Diver certification. The course helps build confidence and expand your scuba skills through different Adventure Dives. You try out different specialties while gaining experience under the supervision of your PADI Instructor. You log dives and develop capabilities as you find new ways to have fun scuba diving.\nYou’ll plan your learning path with your instructor by choosing from a long list of Adventure Dives. There are two required dives – Deep and Underwater Navigation – and you choose the other three, for a total of five dives.\nDuring the Deep Adventure Dive, you learn how to plan dives to deal with the physiological effects and challenges of deeper scuba diving. The Underwater Navigation Adventure Dive refines your compass navigation skills and helps you better navigate using kick-cycles, visual landmarks and time.\nThe other knowledge and skills you get vary with your interest and the adventures you have – photography, buoyancy control, exploring wrecks and many more.\nGet started NOW:\nSign up for PADI eLearning: click here\nChose option: Continue Education –> Advanced Open Water\nCabo Trek PADI Store Number is: 25979\nContact us and let us know when you’ll be in Cabo and if you have any questions or doubts.\nReserve your spots now by clicking the “Book Now” button below.\nAs easy as that! You’re on the best trek to become a PADI Advanced Open Water Diver!\nWhat are the course prerequisites?\nTo be certified as a PADI Advanced Open Water Diver, you need to be at least 15 years old, medically fit and be PADI Open Water Certified. Also you will have access to a tablet.\nWhat’s so great about PADI Advanced Open Water Diver eLearning?\nAs you progress through the material, you’ll be presented with interactive presentations including videos, audio, graphics and text. Short quizzes gauge your progress, and review and correct anything you might miss. This allows you to progress efficiently and at your own pace. In order to complete the five required Knowledge Reviews, you must be connected to the internet and logged in. After completing each Knowledge Review, you’ll receive an email from PADI with the results, so please check your spam filter or junk mail folder in case you don’t see one.\nWhat else is required to complete the course?\nPrior to certification as a PADI Advanced Open Water Diver, you’ll visit your PADI Dive Center or Resort to complete your training. You’ll take a short Quick Review to confirm your understanding of safety-related material from the course, and you must successfully complete five open water training dives with your PADI Instructor. You learn and master each of the required skills and show your instructor that you can comfortably repeat those skills in open water. As a PADI Advanced Open Water Diver, you will be able to dive up to 30 meters (90 feet).\nWhat does PADI Advanced Open Water Diver eLearning cost and cover?\nThe cost of PADI Advanced Open Water Diver eLearning varies by region and is nonrefundable. The fee covers your knowledge development training and unlimited access to PADI Advanced Open Water Diver eLearning via the PADI Digital Library. Your PADI Dive Center or Resort will charge an additional fee for the in-water portion of your course.\nHow long do I have to complete the program? How long does it take?\nPADI Advanced Open Water Diver eLearning will be available for one year from the time of program registration. Though you must finish the online portion of the course within that time frame, you will have perpetual access to an online version of the PADI Advanced Open Water Diver Manual through the PADI Digital Library app. The PADI Advanced Open Water Diver eLearning should take approximately 12 to 15 hours to complete.\nHow do I document that I’ve completed the knowledge development portion of the program?\nOnce you finish PADI Advanced Open Water Diver eLearning, your dive center is notified that you have completed it and that you are ready for the practical portion of the course. At the end of the Advanced Open Water Diver eLearning program, you will need to print out your eRecord and bring a copy of it with you to your selected PADI Dive Center or Resort, or email it to them.\nIs PADI Advanced Open Water Diver eLearning available for Apple? Android? Windows?\nPADI Advanced Open Water Diver eLearning is currently available for Apple and Android operating systems only.\n- Day 1: Open Water Dive 1 & 2\n- Day 2: Open Water Dive 1, 2 & 3\nActivity is lead by highly qualified PADI professional Scuba Diving Instructors who will be next to you at all time during this experience.\nContact us if you need more information.\nTOLL FREE CALL NOW:\n1 844 373 3931']	['<urn:uuid:59c227bb-a0f7-42de-8c18-a4df55703b22>', '<urn:uuid:60a7f656-db7d-4a0f-8734-e7f78e5081e8>']	open-ended	direct	short-search-query	distant-from-document	comparison	novice	2025-05-13T05:06:54.527160	6	48	1598
75	I heard Michigan coney sauce has a weird ingredient - what is it?	Michigan coney sauces traditionally contain beef heart as an ingredient. This is true for multiple styles - Koegel's Detroit-style chili contains beef heart, National and Leo's Coney Island use beef heart products, and Detroit Chili Co. (used by American and Lafayette coneys) lists beef heart as their first ingredient. Even Jackson coneys began using ground heart during World War II because it was easier to obtain than regular ground beef.	['Hot dogs, and similarly coneys, are ubiquotous across the United States. There isn’t a state that doesn’t have one or the other, and in most cases they have both.\nBut there’s something a bit odd about them. Something we’ll use in the overall discussion of Cuisinology.\nThere are too many hot dog styles to count. Wikipedia’s page on hot dog variations lists probably close to 100 variations across the U.S. and around the world. We know from a quick glance that’s not a complete list: Nothing is listed for Hawaii, where the Puka Dog is a popular favorite on Kauai. But the Puka Dog, it appears, is also strikingly similar to the “párek v rohlíku” in the Czech Republic.\nWhat’s particularly odd about hot dogs, coneys, and similar foods such as Kielbasa, Hungarian sausages, Pennsylvania Dutch sausages, etc., is the depth of regionality they enjoy. Cincinnati has their Chili Cheese Dog, Toledo has the Hungarian sausages at Tony Packo’s, Chicago is adamant about their all-beef Chicago Dog with either its neon-green sweet relish or the spicier Giardiniera, pickled in vinegar or oil, and countless others across the country.\nRegional pizza styles also enjoy this kind of popularity, but few other foods do. Hamburgers don’t, nor do most others.\nArguing about regional, or even personal, food favorites of any kind is quite honestly a complete waste of time and energy. It stands to reason that you will like the foods you grew up with. Regional, cultural, religious and family-specific preferences will always be a factor in what kind of foods you will enjoy or even prefer. If you’re from Detroit, you might like American or Lafayette Detroit Style coneys, while thinking a friend who likes Flint or Jackson styles is crazy. A person standing by from Chicago will tell them they’re both nuts since they’ve never had a Vienna Beef dog with Giardiniera, while the Hawaiian resident and the West Virginian will be arguing Puka vs. Sam’s Hot Dog for an hour. Similarly though, a person from the deep south will avoid Zehnder’s fried chicken like the plague since it “will never be like my mama’s”, Chesapeake Bay crab lovers will always be at odds with those who love Bering Sea ophelia, and a new Chinese visitor to the U.S. will always have a difficult time figuring out why a so-called Chinese restaurant serves that incredibly popular General Tso’s thing he’s never heard of.\nIn all of this, the arguing always seems to get worse when it comes to coneys or pizzas. Here though we’ll focus on hot dogs and their counterparts.\nMany people will look at a proper coney in Michigan and think it’s just a hot dog on a bun topped with a “beanless chili”. If you know someone from Michigan and you offer them a chili dog, you might hear a response similar to “Well it’s not a coney, but alright.” And if you’re the person from Michigan in that conversation, you’ll know there’s nothing that can replace the coney you know and love. It has to have the “snap” of the natural lamb or sheep casing on a properly-grilled “coney dog”. The coney has to be topped with a flavorful meat topping having the memorable flavor of ground beef heart (although they might not have known this is the source of that flavor), and maybe ground beef, along with some other organ meats (depending on your style and/or restaurant of choice), minced onion, cumin, chili powder, and paprika. Coney “purists” will then be sure to always have that meat topping garnished with a couple lines of good yellow mustard, topped with more onions that have been minced to be silky smooth.\nIf you’re the kind of person who gets deeper into this sort of thing, you’ll know that in Michigan there are four basic coney varieties: The dry-sauced Jackson Coney, developed by George Todoroff in 1914, another dry sauce that’s served on a Koegel’s Skinless Frankurter at the circa 1915 Kalamazoo’s Original Coney Island, the juicier-sauced Detroit Coney, developed by someone at either of the side-by-side American or Lafayette Coney Islands (there are too many versions of the story to sort out) in 1917, and the Flint Coney, with Simion Brayan’s Flint Coney Island restaurant opening just south of the Flint River on Saginaw St. in 1924.\nThere is serious debate about which coney in Michigan is “best”. Even between the two Detroit Coney shops that sit next to each other, American and Lafeyette, there are “turf wars”. But honestly, you’ll like what you grew up with, and what you like will always be “best” for you. It really is that simple. None are “best”, they’re all good, and they all have their followers and detractors. That will never change.\nThere is also regular discussion about which of these three styles evolved into the other two. But common sense dictates that neither the Jackson nor the Detroit Coney shops had any bearing on the development of the Flint Coney. The originators of all three Coneys likely had no clue what the others were doing until long afterward. In fact, Ft. Wayne Famous Coney Island also opened in 1914, the same year as Todoroff’s in Jackson, and there is no known connection between those two whatsoever.\nIn Flint, as is elsewhere, some restaurants that serve the local coney style are decidedly more popular than others. Certain Flint Coney restaurants might also have a group of local followers, or might be on an interstate or older tourist travel route with visitors from out-of-town viewing it as a “destination”. Ask anyone who’s been through town, grown up there recently, or seen recent news reports where to get a real Flint Coney and they’ll likely mention Angelo’s, Starlite, Palace, or Tom Z’s.\nUnfortunately, the true story of the Flint Coney has become shrouded in a sort of fog created by decades of rumors, folklore and serious misinformation. So many restaurants have used the word “original” in their menus or signage that the actual original has been lost in the shuffle. And out-of-towners are regularly taken to Angelo’s where their local “guides” will likely tell them that’s where the Coney was invented, perpetuating the myths of multiple “original” locations. (Angelo’s food is excellent of course, and this writer has enjoyed many Flint Coneys there.) The fact is, Flint Original Coney, originator Simion Brayan’s shop, closed in 1979.\nIn early 2012 journalists from MLive put together what they called the Michigan Coney Dog Project, resulting in what they determined to be Michigan’s Top 10 Coney Dogs. That they put the Flint Style Coney further down on the list (at position #4) than the Detroit Style Coney (at positions #1 and #2) is not at all surprising since only one of their members is from Flint. That “Coney Detroit” co-author Joe Grimm was along for the ride is even more telling as a partial reason for those results. And a brief look at the more-than 75 comments below that article will show proof of liking what you grew up with.\nThe arguments in those comments regarding who was first in coney development is interesting. The Jackson Style was supposedly in 1914, the Detroit Style was somewhere between 1914 and 1917, and the completed Flint Style was in 1924. Whether or not those people even knew what the others were doing or how they were doing it will never be known, making any argument relatively pointless. The simple fact that these developments occured within 100 miles of each other is what matters, as it puts the development of the Coney itself squarely in southeastern Michigan. That’s something to be proud of.\nThe Squeamishness Of Westerners\nBear with me a moment here so we can look at something people don’t seem to want to think about.\nAmericans, and others in western cultures, generally have no idea where their food comes from. This is particularly true about meat, poultry, fish, and other proteins. They see it in stores, throw it in their cart, take it home and cook it, and think nothing else about it. They’ll see cows and chickens in their travels, they’ll go to farms with petting zoos … and think nothing of the connection between what they’re seeing and what they’ll eat when they get home.\nIt’s been less than a century since butchering was taught in public schools. In their 1922 “Home Economics Cook Book for Elementary Grades” the teachers of Domestic Science in the Department of Home Economics in the Toledo Public Schools in Toledo, Ohio, included cut diagrams for various cattle and hogs. The chapter on poultry also included detailed instructions for cleaning chickens:\nSinge by holding on a flame of any kind to remove long hairs. Cut off the head and draw out the pin feathers with a small pointed knife.\nBy putting the first two fingers under the skin close to the neck, the wind pipe may be easily found and removed; also the crop, which is found fastened to the skin close to the breast. Draw down the neck skin and cut off neck close to the body, leaving skin enough to fasten under the back.\nCut through the skin around the leg an inch and a half below the leg joint. Be careful not to cut the tendons; place the leg at this cut on the edge of the board and snap the bone and pull off the foot with the tendons. In an old bird the tendons will have to be drawn out separately.\nCut through the skin below the breast bone large enough to admit the hand. Begin at the top and with the hand loosen the intestinal organs, keeping the hand close to the side, being careful not to break the gall bladder which is removed with the liver, being near it. Remove the lungs which are enclosed by the ribs on either side of the back bone; the kidneys in the hollow near the end of the back bone; the heart found near the lungs; and eggs if any. Remove the oil bag near the tail and wash the fowl thoroughly by letting water run through it.\nIf there is a disagreeable odor, wash in soda water. [Boardman, Mary; Duncan, Katherine; Ellis, Mary B.; Fisher, Lila; Hoyt, Louise; Knights, Gertrude; McGinnis, Marguerite; Mallory, Effie; Malone, Elizabeth; Sanger, Ruth; Semple, Margaret; Watson, Della Marie; Weeks, Harriet G.; Wylie, Helen. Home Economics Cook Book for Elementary Grades, Toledo, Ohio. Toledo Public Schools, 1922.]\nThe people of today’s western civilizations, with their sanitized supermarkets and complete disconnect between farm and table, need to realize this is how many of their neighbors, many of their countrymen, and the rest of the world, prepare their meals. They need to stop being squeamish about what they eat and where their food comes from. When it comes to food, they need to return to reality.\nWhy This Matters In This Conversation\nIn a Metro Times interview with John Koegel published on June 27, 2007, Mr. Koegel specifically stated “Beef hearts are in our Koegel Detroit-style chili. National and Leo’s Coney Island and Kerby’s Coney Island use beef-heart products, though not ours.” [Broder, Jeff. The Daily Grind. Metro Times. [Online] June 27, 2007. [Cited: June 19, 2016.] http://www.metrotimes.com/detroit/the-daily-grind/Content?oid=2187641.] In another Metro Times article by Michael Jackman dated February 18, 2014, Grace Keros, owner of the American Coney Island downtown and Canton locations, stated she owns the Detroit Chili Co. [Jackman, Michael. American Coney Island owner sets us straight. Metro Times. [Online] February 19, 2014. [Cited: June 19, 2016.] http://www.metrotimes.com/detroit/american-coney-island-owner-sets-us-straight/Content?oid=2144081.] The Detroit Chili Co. sauce is available at GFS Marketplace stores, and the first ingredient listed is beef heart, meaning both American and Lafayette use the meat in their sauces. Of course there is another Michigan style of Coney Dog in Jackson, developed by George Todoroff in 1914. In a piece by MLive’s Brad Flory on June 4, 2014, Richard Todoroff stated beef heart wasn’t in the original recipe. However, he followed this by saying “Coney restaurants in Jackson began using ground heart during World War II because it was easier to obtain than regular ground beef.” [Flory, Brad. Brad Flory column: Feeding Jackson’s astonishing appetite for ground beef heart. MLive.com. [Online] June 4, 2014. [Cited: June 19, 2016.] http://www.mlive.com/opinion/jackson/index.ssf/2014/06/brad_flory_column_feeding_jack.html]\nIn explaining the Flint Coney to interested individuals, and in talking about coneys in Michigan, the reaction when they learn the toppings contain beef heart is generally one of four things:\n1. If this is regarding the Flint Coney and they believe the topping contains ground hot dogs, the conversation may last a while until they understand Flint Coney sauce doesn’t include such a thing.\n2. They knew already and have simply had the information verified again.\n3. They had no idea, but the information just makes the coney that much more interesting.\n4. They will never eat a coney in Michigan again.\nThis last one is a particular pet peeve of mine. My own rule for my own kids has always been “You have to try a food at least once before you can say you don’t like it.” If you merely look at a food or hear about it and decide then that it’s not for you, you’re likely missing out on a lot of possible favorites. It’s because of my rule that my kids enjoy foods many of their peers refuse to try. Foods such as alligator, squid, clams and mussels, German blood tongue sausage, grilled liver, liverwurst, head cheese, and countless other items too many are simply squeamish about and refuse to even try. My daughter was 12 when, at a dinner after a Daddy-Daughter Dance, I’d ordered a fried alligator appetizer at a friend’s restaurant. At first she refused, until I reminded her of my rule. She tried it … and we ended up fighting over each and every piece of meat.\nThat she doesn’t like cornbread is something I can’t figure out.\nSo when someone tells me they’ll never again eat a coney in Michigan because of the beef heart in the sauce, I have some solid questions. What changed? Did the taste change? Maybe the texture? No? So only your perception changed? Normally at that point I hear “I just can’t eat that.” Well … you have most certainly eaten that, and enjoyed it more times than you can probably recall. Therefore, your perception and level of squeamishness require adjustment.\nBeing a cuisinologist, amateur or otherwise, means not only being more accepting of flavors and textures outside your comfort zone, and being willing to try them, but also acknowledging our differences in food likes and dislikes, and celebrating those differences even though we may not agree for whatever reason. Saying a town “Doesn’t know how to do a hot dog” isn’t true. They don’t do your hot dog. They do theirs just fine. When you’re in their town, you’re actually the one who’s nuts. So try theirs. And remember to keep your mind and your taste buds open.']	['<urn:uuid:8bb4430a-b5b6-42a3-9883-4d1f6bfa910c>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:06:54.527160	13	70	2503
76	What's the cooking temperature and duration for duck confit preparation?	Duck confit is cooked at a low temperature between 76-135°C (170-275°F) for 4 to 10 hours until meltingly tender. In the specific recipe, it's cooked at 200°F for about 2 hours until the meat falls from the bones.	"['Roasted Duck Breasts with Farro Risotto and Duck Confit\nBon Appétit | September 2003\nLe Cirque 2000, New York, NY\nMakes 4 servings\nIngredientsadd to shopping list\n- 1 onion, chopped\n- 1 carrot, chopped\n- 4 garlic cloves, chopped\n- 2 fresh thyme sprigs\n- 1 teaspoon sea salt\n- 2 ducks, legs, thighs, and breasts cut from carcasses; carcasses quartered\n- 4 cups rendered duck fat\n- 3 tablespoons chopped leek (white and pale green parts)\n- 2 tablespoons chopped celery\n- 2 tablespoons chopped onion\n- 2 tablespoons chopped carrot\n- 1/2 cup ruby Port\n- 6 cups water\n- 10 whole black peppercorns\n- 1 tablespoon butter\n- 1 cup farro*\n- 8 cups water\n- 1/2 cup plus 1 tablespoon olive oil\n- 2 tablespoons (1/4 stick) butter\n- 1/4 cup chopped shallots\n- 2/3 cup dry red wine\n- 2 cups low-salt chicken broth\n- 2 tablespoons freshly grated Parmesan cheese\n- 1 teaspoon vegetable oil\n- 1/2 cup honey\n- 3/4 cup assorted chopped nuts (such as pistachios, hazelnuts, and pecans)\n- 1 whole star anise**\nMix first 5 ingredients and duck legs and thighs in bowl. Cover leg-thigh mixture and breasts and carcasses separately; chill overnight.\nPreheat oven to 200°F. Heat duck fat in medium ovenproof pot over medium heat to 200°F. Add duck leg-thigh mixture; transfer to oven and cook until meat falls from bones, about 2 hours. Allow duck confit to cool 2 hours.\nPreheat oven to 400°F. Place duck carcasses in large ovenproof pot; roast uncovered until lightly browned, about 30 minutes. Transfer pot to stove top. Add leek and next 3 ingredients and sauté over medium-high heat until vegetables are lightly browned, about 3 minutes. Add Port; boil until liquid is reduced by half, about 2 minutes. Add 6 cups water; boil gently for 30 minutes. Add peppercorns; boil broth 10 minutes. Strain broth; skim fat from surface. Boil broth in medium saucepan until reduced to 1/2 cup, about 30 minutes. Whisk butter into sauce. (Can be made 1 day ahead. Chill confit and sauce separately.)\nRewarm duck confit to melt fat; remove duck from fat. Cut meat from bones and add to sauce.\nFor farro risotto:\nSoak farro in cold water 20 minutes. Drain; rinse. Bring 8 cups water to boil in medium saucepan. Add 1/2 cup oil and farro. Simmer 20 minutes. Drain in strainer and rinse. Melt 1 tablespoon butter with 1 tablespoon oil in medium saucepan over medium heat. Add shallots; sauté 1 minute. Add farro and wine. Simmer until almost all liquid evaporates, stirring frequently, about 5 minutes. Add chicken broth 1 cup at a time and simmer until liquid is absorbed and farro is just tender, stirring frequently, about 14 minutes total. Stir in cheese and 1 tablespoon butter. Season with salt and pepper.\nMeanwhile, prepare duck breasts:\nPreheat oven to 450°F. Sprinkle duck breasts with salt and pepper. Heat oil in large ovenproof skillet over medium-high heat. Add duck breasts, skin side down; cook until skin is crisp, about 5 minutes. Turn duck over. Cook 1 minute; remove from heat.\nBring honey to boil in medium saucepan over medium-high heat. Add nuts and star anise; boil until honey is reduced to thick syrup, stirring constantly, about 2 minutes. Spoon honey-nut mixture onto duck breasts. Cook in oven about 4 minutes for medium-rare.\nRewarm duck confit sauce over low heat. Divide risotto among 4 plates. Using slotted spoon, remove duck meat from sauce and spoon over risotto. Slice breasts diagonally; arrange around risotto. Drizzle duck with confit sauce.\n*Farro is sold at some Italian markets and natural foods stores.\n**A brown star-shaped seed pod available at Asian markets and specialty foods stores and in the spice section of some supermarkets.\nNutrition Informationper serving (4 servings) POWERED BY Edamam\n- Carbohydrates92 g (31%)\n- Fat313 g (481%)\n- Protein37 g (73%)\n- Saturated Fat94 g (471%)\n- Sodium943 mg (39%)\n- Polyunsaturated Fat36 g\n- Fiber11 g (44%)\n- Monounsaturated Fat142 g\n- Cholesterol351 mg (117%)\nrecipe featured in\nPoultry: How to Brine a Turkey\nPoultry: Injecting a Turkey with Olive Oil\nPoultry: Stuffing and Trussing a Turkey\nPoultry: Dry Herb Rub for a Turkey\nPoultry: Butter Marinade for a Turkey\nPoultry: Basting a Turkey\nPoultry: Tenting a Turkey\nPoultry: Checking Doneness of a Turkey', 'This article\'s lead section may be too long for the length of the article. (March 2018)\nThis article needs additional citations for verification. (February 2013) (Learn how and when to remove this template message)\nDuck confit (French: confit de canard [kɔ̃.fi d(ə) ka.naʁ]) is a French dish made with the whole duck. In Gascony, according to the families perpetuating the tradition of duck confit, all the pieces of duck are used to produce the meal. Each part can have a specific destination in traditional cooking, the neck being used for example in an invigorating soup, the garbure. Duck confit is considered one of the finest French dishes.\nWhile it is made across France, it is seen as a specialty of Gascony. The confit is prepared in a centuries-old process of preservation that consists of salt curing a piece of meat (generally goose, duck, or pork) and then cooking it in its own fat.\nTo prepare a confit, the meat is rubbed with salt, garlic, and sometimes herbs such as thyme, then covered and refrigerated for up to 36 hours. Salt-curing the meat acts as a preservative.\nPrior to cooking, the spices are rinsed from the meat, which is then patted dry. The meat is placed in a cooking dish deep enough to contain the meat and the rendered fat, and placed in an oven at a low temperature (76 – 135 degrees Celsius/170 – 275 Fahrenheit). The meat is slowly poached at least until cooked, or until meltingly tender, generally four to ten hours.\nThe meat and fat are then removed from the oven and left to cool. When cool, the meat can be transferred to a canning jar or other container and completely submerged in the fat. A sealed jar of duck confit may be kept in the refrigerator for up to six months, or several weeks if kept in a reusable plastic container. To maximize preservation if canning, the fat should top the meat by at least one inch. The cooking fat acts as both a seal and preservative and results in a very rich taste. Skipping the salt curing stage greatly reduces the shelf life of the confit.\nConfit is also sold in cans, which can be kept for several years. The flavourful fat from the confit may also be used in many other ways, as a frying medium for sautéed vegetables (e.g., green beans and garlic, wild or cultivated mushrooms), savory toasts, scrambled eggs or omelettes, and as an addition to shortcrust pastry for tarts and quiches.\nA classic recipe is to fry or grill the legs in a bit of the fat until they are well-browned and crisp, and use more of the fat to roast some potatoes and garlic as an accompaniment. The potatoes roasted in duck fat to accompany the crisped-up confit is called pommes de terre à la sarladaise. Another accompaniment is red cabbage slow-braised with apples and red wine.\nUse in other dishes\nDuck confit is also a traditional ingredient in many versions of cassoulet.\n- Times-Picayune, Ann Maloney NOLA com | The. ""How to make duck confit: A simple, if time-consuming dish"". NOLA.com. Retrieved 2018-12-07.\n- How to make duck confit\n- Duck confit\n- Kerry Saretsky. ""Potatoes Sarladaises Recipe"". Serious Eats. Retrieved 10 June 2018.\n|Wikimedia Commons has media related to Confit de canard.|\n|Wikibooks Cookbook has a recipe/module on|']"	['<urn:uuid:04ac5e05-7c1a-429b-80b4-c0c94e1487ee>', '<urn:uuid:34e9608d-4d13-4f30-ad1a-4ee147a8bb39>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T05:06:54.527160	10	38	1283
77	what medicines exist to treat cancer by changing dna structure	There are epigenetic drugs (epi-drugs) that can modify DNA structure to treat cancer. By 2016, six of these drugs had received FDA approval. Some key types include DNA methylation inhibitors and histone deacetylase enzyme (HDAC) inhibitors. More recently, acetyltransferase histone inhibitors (HATi) have shown promising results in treating solid tumors. These drugs work by targeting epigenetic enzymes to correct marks on the genome. However, these compounds are non-specific and can affect any type of tissue and organ, which may lead to adverse reactions.	['Epigenome editing, a new anti-aging technology for longevity\nEpigenetics includes all the mechanisms that regulate gene expression, namely DNA methylation, histone modifications and chromatin remodelling. These mechanisms are introduced into the genome by non-coding enzymes and RNAs. With age, epigenetic errors occur, causing many diseases such as cancers. However, epigenetic changes are reversible and it is possible to target their origins to reverse them. Handling these errors makes it possible to understand their role in aging and the development of pathologies, but also to develop new therapies. Recently, the epigenome editing technology, literally meaning “genome editing technology“, has been developed, whose tools target the non-coding enzymes and RNAs responsible for the modifications, thus allowing the epigenome to be manipulated.\nEpigenome Editing : what is that?\nIn the term “epigenome editing”, the epigenome refers to the epigenetic state of a cell, while the term “editing” refers to the manipulation of this state to modify the epigenetic marks present. Epigenetic editing is therefore a technology for revising the epigenetic state of the genome, thus modifying the expression profile of a cell’s genes. It is currently the only technology that can modulate the epigenome.\nThe tools used to modify epigenetic marks are chimeric proteins, composed of two parts: a DNA binding domain, allowing proteins to associate with DNA, as its name suggests, and a domain comprising an epigenetic factor (enzyme or other) that will modify DNA. The DNA binding domains are of three different types: zinc finger proteins (or ZFP for Zinc Finger Protein), TALE effectors (Transcription Activator-Like Effectors) and the CRISPR-Cas9 system. These chimeric proteins are powerful tools for creating isogenic cells (“clones”) and transgenic animals (which may or may not express a specific gene) making epigenetic publishing a powerful tool for research, biotechnology and medicine.\nRevising epigenetic changes which can lead to age-related diseases offers great potential for therapeutic development against these diseases. Epigenetic editing is a technology that is very applicable to anti-aging research. It allows the introduction of epigenetic variations in vitro or in vivo, useful to understand and explain the development of pathologies, the severity of certain symptoms and the role of the environment. This approach complements the knowledge on the development of pathologies that cannot be explained by an approach based solely on the study of the DNA sequence. Epigenome editing can induce an epigenetic mark, but it can also remove it, as is the case, for example, in the study of cancers. Indeed, cancer cells can be induced in vitro by introducing epigenetic errors into the DNA of normal cells, and then these errors can be removed from cancer cells.\nEpigenome editing: what does it target?\nThis technology targets molecular effectors that produce epigenetic marks. Action is achieved by inhibiting enzymes that bring or suppress epigenetic changes, or by preventing the binding of non-coding RNAs with their targets. DNA methyltransferases (DNMT) are the enzymes responsible for establishing and maintaining DNA methylation, and one of the main targets of epigenetic editing. DNMT inhibitors are the most advanced in clinical applications of epigenome editing. At the same time, there are currently at least 232 enzymes capable of modulating post-translational changes in histones, and epigenetic therapies based on HDAC (Histone deacetylase: an enzyme that suppresses histone acetylation) are already in the clinical phase of testing. By targeting the enzymes responsible for epigenetic markers, epigenome editing makes it possible to target different types of epigenetic mutations such as loss of function mutations (where a gene is no longer expressed) and function gain mutations (where a gene is too expressed).\nEpigenome editing : epigenetic alterations and epi-medicine\nEpigenetic alterations are one of the major causes of aging and many pathologies. This is particularly the case for cancers, whose malignancy often results from epigenetic errors leading to the expression of pro-cancer genes, called oncogenes. Cancer epigenetics is one of the most important research topics in this field and the mechanisms of carcinogenesis are now better understood and controlled. It is now possible to induce or suppress cancer by epigenetic editing in cellular and animal models, with the hope of transferring this to humans in the next few years. An example of these mechanisms is the enzyme G9a, a methylase that will modify a specific histone, and, depending on the methylation state of this histone, will either induce or suppress cancer in an in vitro model. Another example is DNMT3A, a methyltransferase responsible for modifying DNA on a particular gene, called SOX2, and whose absence of expression (no protein production from the gene) will lead to suppression of tumor growth.\nEpi-medicine (or epigenetic drugs) rely on these mechanisms, sometimes beneficial, sometimes deleterious, to modify them in the right direction. They are compounds capable of targeting epigenetic enzymes to correct marks on the genome. Interest in these epigenetic modulators is growing, particularly in the treatment of cancers. By 2016, six epi-drugs had received FDA (Food and Drug Administration) approval. In addition, other drugs are in clinical development, including DNA methylation inhibitors and histone deacetylase enzyme (HDAC) inhibitors, overexpression of which is a common feature of human pathologies. Inhibiting HDACs would address some of the characteristics of cancers such as cell proliferation, angiogenesis and cell differentiation. More recently, it is the acetyltransferase histone inhibitors (HATi), the enzymes responsible for post-translational histone acetylation, that have gained interest thanks to promising results on solid tumours. Oncology is not the only field that can benefit from epigenome editing technology: the administration of a non-specific HDAC inhibitor, trichostatin, for example, has reversed neurodevelopmental disturbances in adult rats.\nFinally, there is a class of compounds capable of preventing the binding of non-coding RNAs to their targets: antagomirs. They are used to inhibit the action of non-coding RNAs in epigenetics. Treatment with the microRNA 181a antagomir following a stroke has been shown to reduce stroke severity, neural deficits, inflammation and provide neuroprotective effects in mice. In addition, the study demonstrated that reducing microRNA 181a before a stroke protects brain cells from ischemic damage in vivo and in vitro. However, epic drugs and antagonists are non-specific compounds that can affect any type of tissue and organ. This characteristic highlights the risks of more or less serious adverse reactions that can be observed with such treatments.\nEpigenome editing is an innovative technology whose powerful tools have revolutionized biomedical research. It offers unprecedented opportunities to study the epigenetics of aging and develop therapies against related diseases. Although methodological and conceptual breakthroughs are still needed to apply epigenome editing in routine and clinical settings, a large number of publications have already demonstrated the great potential of this technology.\nSee all our articles on the epigenetics of aging and longevity\nWith epigenetics, a new technology has come to light: epigenome editing, made possible by technologies such as the use of CRISPR-Cas9.\n Shota Nakade, Takashi Yamamoto, Tetsushi Sakuma, Cancer induction and suppression with transcriptional control and epigenome editing technologies, Journal of Human Genetics (2018) 63:187–194. https://doi.org/10.1038/s10038-017-0377-8\n Pratiksha I. Thakore, Joshua B. Black, Isaac B. Hilton and Charles A. Gersbach. Editing the Epigenome: Technologies for Programmable Transcriptional Modulation and Epigenetic Regulation, Nat Methods. 2016 February ; 13(2): 127–137. doi:10.1038/nmeth.3733.\n Cia-Hin Lau and Yousin Suh. Genome and Epigenome Editing in Mechanistic Studies of Human Aging and Aging-Related Disease, Gerontology. 2017 ; 63(2): 103–117. doi:10.1159/000452972.\n Graça et al. Clinical Epigenetics (2016) 8:98. DOI 10.1186/s13148-016-0264-8\n Tim J. Wigle, Promoting Illiteracy in Epigenetics: An Emerging Therapeutic Strategy, Current Chemical Genomics, 2011, 5, (Suppl 1-M1) 48-5.\n Sophia Xiao Pfister and Alan Ashworth. Marked for death: targeting epigenetic changes in cancer, Nature Reviews, Drug Discovery, Volume 16 (April 2017), 241-263.\n Carlos Lopez-Otin, Maria A. Blasco, Linda Partridge, Manuel Serrano and Guido Kroemer. The Hallmarks of Aging, Cell 153, June 2013, 1194-1217.\n Alfonso Dueñas-González, J. Jesús Naveja, José L. Medina-Franco, Introduction of Epigenetic Targets in Drug Discovery and Current Status of Epi-Drugs and Epi-Probes, Epi-Informatics. http://dx.doi.org/10.1016/B978-0-12-802808-7.00001-0\n Moshe Szyf. Prospects for the development of epigenetic drugs for CNS conditions, Nature Reviews, Drug Discovery Volume 14 (July 2015), 461-474.\n Li-Jun Xu, Yi-Bing Ouyang, Xiaoxing Xiong, Creed M Stary, and Rona G Giffard. Post-stroke treatment with miR-181 antagomir reduces injury and improves long-term behavioral recovery in mice after focal cerebral ischemia. Exp Neurol. 2015 February ; 264: 1–7. doi:10.1016/j.expneurol.2014.11.007.\nAnne is studying medicine science at the Institute of Pharmaceutical and Biological Science in Lyon and she has graduated with a Bachelor’s degree in molecular and cellular biology at the University of Strasbourg.\nMore about the Long Long Life team\nAnne étudie les sciences du médicament à l’Institut des Sciences Pharmaceutiques et Biologiques de Lyon. Elle est titulaire d’une licence en biologie moléculaire et cellulaire de l’Université de Strasbourg.\nEn savoir plus sur l’équipe de Long Long Life']	['<urn:uuid:7a1dfcae-e6d5-4cfa-a2c7-d24edf91fd49>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T05:06:54.527160	10	83	1439
78	How long did World War 2 last?	World War 2 was a global war that lasted from 1939 to 1945.	"[""This World War 2 Colouring Map for kids will challenge them to work out which country was on the side of the allies, axis or neutral. World War 2 was a global war from 1939-1945 between the Allied powers and Axis powers.World War 2 started after Germany, led by dictator Adolf Hitler, invaded Poland in 1939. World War Ii Blank Map Of Europe Copy World W World War Ii Blank Map Of Europe Copy World War 2 Map Europe Scrapsofme Images Of World War Ii Blank Map Of Europe Copy World War 2 Map Europe Scrapsofme Lighting Design Proposal Sample Unique 15 Best Business Proposal Templates For New Client Projects Fresh Marriage Certificate Form Download.\nBlank World War 2 Coloring Map. Showing top 8 worksheets in the category – Blank World War 2 Coloring Map. Some of the worksheets displayed are Second world war, World war one information and activity work, Chapter 26 map activity world war ii in europe, World political, By the mcgraw hill companies all rights, Cold war map activity, World continents, World maps.\nWorld war 2 blank map. This is a complete three-week PowerPoint unit on World War II covering the *Virginia Standards of Learning (SOLs) for middle school history, which includes the following lessons/topics: 1.Rise of Dictators (Fascism) 2.War Map Activity (you have to supply the blank maps) 3.Timeline Activity 4.Steps This is an online quiz called World War 2 Map Quiz There is a printable worksheet available for download here so you can take the quiz with pen and paper. Search Help in Finding World War 2 Map Quiz – Online Quiz Version Blank Pacific Map | World Map HD Lesson 2: Turning the Tide in Europe, 1942–1944 | EDSITEment World War Ii Blank Map Of Europe Fresh World War Ii Maps Perry.\nwith more related things as follows printable blank world map countries, map world war 2 coloring pages and native american social studies worksheets. Our intention is that these World War II Map Worksheet pictures collection can be useful for you, give you more ideas and most important: make you have what you search. Oct 12, 2018 – Blank Map Of The World empty world map world war 2 blank map blank map of the world with 961 X 547 pixels. As the center ages Ww2 Map Of Europe https www google maps san francisco attractions map #88222 Outline Map of Asia, Printable Outline Map of Asia #88223 File:Ww2 allied axis 1942 jun.png – Wikimedia Commons #88224\nHow is this World War 2 colouring activity helpful for kids? This colouring activity features a map of Europe during the Second World War for kids to colour in according to which countries were Allies, Axis, Axis controlled or neutral. It is helpful for pupils learning about World War 2 and highlights which sides European countries were on. A World War II map that you can use with your students. The map comes pre-labeled so that all your student have to do is color in the Allied and Axis PowersMore Maps:WWI MapCold War MapMap Bundle – WWI, WWII, and Cold War – $5Be the first to know about new products (all new products are free first 2 Europe – World War 2 (November 1938, right after the First Vienna Award) more WWII maps will be added in time, depicting different time periods.. Step 2. Add the title you want for the map's legend and choose a label for each color. Change the color for all countries in a group by clicking on it.\nA collection of World War I maps for use by history teachers and students. World War I. 1914 – Outline map of Europe. 1914 – A satirical map of Europe. 1914 – The British Empire. 1914 – The German Empire. 1914 – The French Empire. 1914 – Warring powers in Europe. 1914 – The Schlieffen Plan. At Europe Map World War II pagepage, view political map of Europe, physical map, country maps, satellite images photos and where is Europe location in World map. Toggle navigation.. Europe Political Blank Map. Europe Population Map Countries 2006. European Union Countries GDP 2007. Languages Map of Europe. Languages of Europe. 10 Best Of World War 2 Printable Map – A map can be a representational depiction highlighting relationships involving parts of a distance, such as objects, regions, or motifs. Most maps are somewhat inactive, adjusted into paper or any other durable moderate, while others are somewhat dynamic or interactive.\nImage:BlankMap-World.png – World map, Robinson projection centered on the Greenwich Prime Meridian. Microstates and island nations are generally represented by single or few pixels approximate to the capital; all territories indicated in the UN listing of territories and regions are exhibited. Map of the World after World War One. Map of Europe 1936-1939: German aggressions prior WWII. Map of the Major Operations of WWII in Europe. Map of the Major Operations of WWII in Asia and the Pacific. Map of the Allied Operations in Europe and North Africa 1942-1945. Map of the Battle of Stalingrad July 17, 1942 – February 2, 1943. Map of. While we talk about World War I Map Worksheet, we already collected various variation of pictures to inform you more. world war 2 coloring pages, world war 1 map printable and world war 1914 europe map blank are three of main things we will present to you based on the gallery title.\nMap with the Participants in World War II: . Dark Green: Allies before the attack on Pearl Harbor, including colonies and occupied countries.; Light Green: Allied countries that entered the war after the Japanese attack on Pearl Harbor.; Blue: Axis Powers and their colonies; Grey: Neutral countries during WWII. Dark green dots represent countries that initially were neutral but during the war. World War II (WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. It involved the vast majority of the world's countries—including all the great powers—forming two opposing military alliances: the Allies and the Axis.In a state of total war, directly involving more than 100 million people from more than 30 countries, the major participants.""]"	['<urn:uuid:94f977e8-a36b-46b0-9d6c-8234cb75156d>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:06:54.527160	7	13	1026
79	when do cats stop teething and what signs show they are teething	Cats finish teething when they are around 8 months old. During teething, kittens show several signs including: slight bleeding from their gums, pawing at their face, loss of appetite, reduced grooming, increased drooling, bad breath, and increased chewing behavior. The teething process involves losing baby teeth between 4-6 months old, and these teeth may be found on the floor or furniture, though most are swallowed while eating without causing issues.	['This article is all about what to expect from a 5 month old kitten, including their physical development, what behavioral milestones they’ll be reaching, and how to give them the best possible care. We’ll answer all the most pressing questions asked by kitten parents at this time too, including ‘how much should a 5 month old kitten weigh? and ‘when do kittens calm down?!’\n- 5 month old kitten size\n- How much to feed a 5 month old kitten\n- Can a 5 month old kitten eat cat food?\n- 5 month old kitten care\n- 5 month old kitten behavior\n- When do kittens calm down?\n- Five month old kitten milestone checklist\nA 5 month old kitten has almost completely lost their baby features, and looks undeniably like a small adult cat. But they’re still hitting important milestones in terms of physical and behavioral development.\nLook how much Layo has changed by the time he’s 5 months old. His personality is shining through too. Let’s take a closer look at the development stages he’s going through.\nYour 5 Month Old Kitten\nMost five month old kittens have been with their forever homes for 3 months now. They’re likely to be feeling well established in their new families, and growing in confidence every day. You might be starting to think about allowing them access to the outdoors, in which case there are steps you can start taking now to prepare them for that.\n5 Month Old Kitten Size\nWhether or not their kitten is growing as they should is a major preoccupation of many pet parents. The appropriate 5 month old kitten weight for your cat depends mostly upon their breed. A Norwegian Forest Cat kitten or Maine Coon kitten might already weigh 6 or 7 pounds, whilst a Cornish Rex or dainty Singapura might only be 3 or 4 pounds. For domestic shorthair cats, and other kittens of unknown ancestry, a very crude rule of thumb is that a 5 month old kitten weighs just over half of their final adult weight. If you’re at all worried that your kitten isn’t growing as they should, then we recommend asking a veterinarian to examine them. Several common kitten ailments, including parasitic infections, can stop a kitten putting on weight.\nIf your kitten is slightly bigger or smaller than expected, but otherwise healthy, don’t worry. Factors such as sex, litter size, and individual variation all affect a kitten’s size too. So it’s normal to see kittens inside and outside the average weight range.\nHow Much To Feed A 5 Month Old Kitten\nHopefully your kitten has a hearty appetite to support how rapidly they are continuing to grow at the moment. As they get bigger and more confident, you might start to notice that they’re more curious about the food you eat too – or at least, bolder about expressing that curiosity. In fact it might seem like they are always hungry! This is all a natural part of exploring the world around them. But you should keep feeding them the portion sizes advised on the packaging, and restricting treat intake to no more than 10% of their daily calories, unless your veterinarian recommends otherwise.\nCan A 5 Month Old Kitten Eat Cat Food?\nIt’s likely your kitten is starting to look, and act, pretty grown up. But are they ready for adult cat food? Kitten food is specially formulated to meet the nutritional needs of growing cats. It contains exactly the right vitamins, minerals and calories to support bones and tissue formation at a safe and appropriate rate. They should keep eating it until they have reached their full adult weight, at about 12 months old.\n5 Month Old Kitten Care\nA five month kitten is gaining confidence and independence, but there’s still a lot they can only get from you!\nYou’ve probably noticed that their soft kitten coat is starting to be replaced by a thicker, smoother adult coat. This process is going to last about a month or so, and if their adult coat is long or wavy, it’s likely they’ll need help keeping it clean and tangle free. Hopefully you’ve already started introducing them to grooming, but if not, now is the time to start!\nBegin with just a single gentle touch with the brush in the first occasion and give them several treats, to make it a positive experience. Gradually build up to a single stroke at a time, then a few strokes, until you can brush their entire coat right down to the skin in a single sitting. Pay special attention to their ‘armpits’ and under their collar – the places where friction is most likely to cause knotting.\nKittens shed their deciduous (baby) teeth and grow their adult teeth between 4 and 6 month old. You might notice:\n- Lost baby teeth on the floor or furniture\n- Slight bleeding from their gums\n- Signs of discomfort including pawing at their face, and not wanting it to be touched by you\n- Loss of appetite\n- Reduced grooming (because their primary grooming tool – their mouth – is sore)\nThey might also be chewing on things more than usual, and it’s a good idea to have a stashing of appropriate teething toys to offer them, rather than sacrificing your own belongings!\nSpaying and Neutering\nNot a lot of people realise than kittens can reach sexual maturity and produce kittens of their own by 4 months old. Not only does this increase the homeless cat population though, it is very dangerous for a 5 month female kitten to get pregnant while she herself is still practically a baby.\nNeutering can also reduce the likelihood of unpleasant behaviors such as spraying urine in your home. For these reasons, and because all the evidence so far suggests it is safe to do so, veterinarians increasingly recommend that kittens be neutered as soon as possible after 4 months old, and before the onset of puberty. If you haven’t discussed this with your vet, do it sooner rather than later!\n5 Month Old Kitten Behavior\nIn terms of emotional maturity and behavior, your 5 month kitten has come a long way since they were born. And it’s likely you’ve got a pretty clear picture of their adult temperament by now. Research into kitten development suggests that many aspects of their personality are stable now, and will remain more or less the same throughout the rest of their lives. For example how sociable they are, how bold they are, how much interest they have in exploring.\nBut others will still continue to develop. For example territorial behaviors in males, such as urine marking, will start when they reach puberty in the next month or so. Predatory skills also develop in a non-uniform way between cats. So your five month old kitten might already be an ace hunter of fluffy kitten toys, or they might still seem bemused by them. Those who seem bemused now will usually catch up in the next few months.\nWhen Do Kittens Calm Down?\nA oft-asked question from our readers is ‘when do kittens calm down?!’ A 5 month old cat can be a real livewire. They have all the boisterousness and enthusiasm of a kitten, combined with the confidence and stamina of an adult cat. It’s a combination which is in turns endearing, entertaining and exhausting.\nA common behavioral problem at this age is unwanted and over the top play aggression. For example attacking your hands when you try to pet them, or pouncing at your ankles as you try and work in the kitchen. This is partly a natural phase which they will grow out of. But it can be made worse if\n- a kitten never learned to play properly, because they were separated from the mom and siblings too early,\n- they get overly aroused and can’t control own intensity,\n- or they get easily frustrated by game ending.\nWhether their behavior is within normal limits, or made worse by one of these factors, you can still help them mature out of it by redirecting them to suitable toys, stopping games before they get too excited to control their behavior, and using clear start and end cues for playtimes. And take heart that by the time they are an 8 month old cat, they will be much less hectic!\nLetting Your Kitten Outdoors\nFinally, let’s turn to an exciting event on the horizon. If you plan to let your cat outdoors, then they’re nearly ready for freedom. You can start introducing them to the area immediately outside your home as soon as their vaccination schedule is complete. But cat welfare experts recommend not letting them out of your sight until they are about 6 months old.\nTo bridge the gap, you can use a specially-designed kitten harness and leash to keep them close to you at first. It’s also a good idea to practice saying their name to them whilst feeding extra-tasty treats, so that you can call them in when it’s time to got to bed.\nFive Month Old Kitten Milestone Checklist\nTo sum up, here are some milestones to expect or prepare for as your kitten celebrates their 5-month birthday:\n- If your breeder provided an estimate of their adult weight, they should have reached at least half of that.\n- They will continue to loose baby teeth and grow adult teeth – contact your vet if an adult tooth comes through without the corresponding baby tooth falling out.\n- Their adult coat is coming through too – make sure you have a grooming routine established!\n- Schedule spay or neuter surgery now, if you haven’t already done so.\n- If they’ll be allowed outside as an adult, start introducing them to the area around your home now using a harness and leash. And teach them their name so you can call them back indoors!\n- And finally, tell us how they’re getting on, using the comments box down below!\nMore articles in this series\n- 3 Month Old Kitten – A Complete Guide From The Happy Cat Site\n- 4 Month Old Kitten – Growing And Changing Fast\n- 6 Month Kitten – Complete Guide To Caring For Your Cat\n- Denenberg. Preventing problem feline behaviour at different life stages. BSAVA. 2018.\n- Lowe & Bradshaw. Ontogeny of individuality in the domestic cat in the home environment. Animal Behavior. 2001.\n- Turner & Bateson. The Domestic Cat: The Biology of Its Behaviour. Cambridge University Press. 2000.\n- Welsh. Cat neutering: the earlier the better to tackle overpopulation. The Veterinary Record. 2018.', 'Ever looked at your cat and wondered how many teeth they have? You are not alone!\nAfter all, your cute and cuddly kitten is the pinnacle of hunting evolution – they are bound to have some serious chompers in there!\nAn adult cat will have 30 teeth unless something has gone wrong at some point!\nCats who have more than 30 teeth may run into problems so could benefit from a trip to the vet.\nCats who have fewer teeth may have had them accidentally or deliberately removed.\nCurious? Keep reading to discover everything you need to know about your cat’s dentition!\nHow Many Teeth Do Cats Have?\nLike humans, cats have two sets of teeth in their lives. They have a set when they are young that drops out when the second set pushes through the gums.\nThe first set of teeth, a bit like a human child’s milk teeth, are called deciduous teeth.\nA kitten will have 26 deciduous teeth that fall out when the permanent teeth, or adult teeth, develop. An adult cat should have 30 permanent teeth.\nWhen Do Cats Get Milk Teeth?\nLike a human baby, a kitten is born with no visible teeth. They are just gums!\nAt around 21 days old the deciduous milk teeth – the ones that drop out – erupt through the gums.\nThis teething process will carry on for around 5 weeks until your cat is roughly 2 months old.\nRead more about the teething process below.\nWhen Do Cats Get Permanent Teeth?\nFortunately for your kitten, teething in cats is a fairly swift window of their lives. The process is even quicker for permanent teeth.\nOnce teething has finished and all of their deciduous teeth have erupted, your cat should be around 2 months old.\nThese teeth will begin to fall out once your kitten is around 6 months old – this is just a 4-month window for them to have baby teeth!\nWithin 8 weeks, all of their deciduous teeth should have fallen out and been replaced by its permanent counterpart and a few extras.\nBy the time your cat is 8 months of age, they will have all 30 of their permanent teeth ready to start chomping!\nDo Kittens Go Through The Teething Process?\nYes, kittens go through teething.\nThe first phase is when their deciduous teeth are coming through. This can be an unpleasant experience for your kitten.\nAs teeth are central to a cat’s overall health and well-being, it is recommended that you get them used to tooth brushing as soon as possible.\nIt is generally recommended that you get your cat used to your fingers being in or near their mouths before this stage of teething begins.\nThis is because teething can be very uncomfortable, and you don’t want to create any negative connotations between the toothbrush and pain.\nThe second phase of teething takes place when the adult teeth push to erupt and deciduous teeth fall out.\nDuring this time, the tooth buds gradually get closer to the gum line – ‘tooth buds’ is the term for undeveloped permanent or adult teeth.\nThese buds move through the bone of the jaw and eventually out of the gums and into the mouth.\nIn an ideal world, this eruption of the adult tooth would force out all the deciduous teeth 100% of the time. However, this does not always happen.\nOccasionally you may notice that the adult tooth has grown alongside the tooth that should have fallen out. When this happens, the milk tooth is called a persistent deciduous tooth.\nYou should not be alarmed if you find deciduous teeth or tooth shells while your kitten is teething – it is perfectly natural!\nYou also shouldn’t worry if you do not find all of their deciduous teeth as most of them will fall out while your kitten is eating, and they are usually swallowed.\nThis rarely causes issues. Remember that your kitten is likely to be in discomfort during this time so may not want to eat.\nYou may also notice that your kitten is more irritable, more forlorn, producing more drool, or has bad breath.\nAgain, these symptoms are natural and are not necessarily something to be concerned about.\nIt may be necessary to seek veterinary advice if these symptoms are particularly severe or if they persist for longer than a few weeks.\nOne other key thing to remember is that your kitten will want to chew when they are first teething.\nThe pressure in their gums when they clamp down with their jaws can help alleviate the pain and encourage the new teeth to erupt.\nKeep in mind that you may need to direct your cat’s chewing to appropriate things!\nChewing toys? Yes! Chewing humans? No! You also need to make sure that they are not chewing very hard objects.\nThese will damage the teeth as they come out of the gum.\nWhat Is A Persistent Tooth?\nA persistent tooth is a deciduous tooth that does not fall out when it is supposed to and stays in place next to the permanent tooth.\nThis means that it is taking up room in your cat’s mouth that it shouldn’t, often causing the adult or permanent tooth to erupt at a bad angle or in a weird position.\nThis can have serious consequences.\nKeep a particular eye out for your kitten’s upper and lower canine, and incisor deciduous teeth. These are the ones that are most likely to be persistent.\nAre Persistent Teeth Bad?\nIn short, yes. Persistent teeth are not good for your cat.\nYour cat has evolved to have 30 permanent teeth as an adult – anymore is too many and will cause issues.\nOvercrowding of your cat’s teeth or a permanent tooth in a suboptimal position can cause an abnormal bite to develop.\nThis is called malocclusion and needs to be avoided as much as possible.\nThis is because your cat’s teeth are sharp so any tooth hitting tissue it isn’t supposed to will cause serious damage quickly.\nKeep in mind that any dental misalignment can have serious consequences and affect your cat’s comfort.\nIt is common that these types of issues prevent them from eating properly, causing more damage to their health down the line.\nProper orthodontic treatment will guarantee that your cat has a happy life!\nHow Do I Treat My Cat’s Persistent Tooth?\nKeep an eye on your kitten as their deciduous teeth fall out. As soon as you notice a persistent tooth, make an appointment with your vet.\nThey will likely need to extract the tooth to keep your cat healthy.\nRegularly checking your cat’s mouth is important as treating misalignment issues early gives your cat the best chances of normal and healthy development.\nShould I Brush My Cat’s Teeth?\nYes! Brushing your cat’s teeth is by far the easiest way to prevent dental plaque and to keep your cat’s oral health in top condition.\nYou need to slowly introduce your cat to having their teeth brushed, with the end goal of incorporating this into their daily routine.\nYou must implement a tooth cleaning regime slowly, no matter what age your cat is.\nKittens have 26 deciduous teeth – fall out – and then 30 permanent teeth as adult cats.\nIt is essential to keep an eye on your cat’s oral hygiene no matter their age but is particularly essential to regularly check your kitten’s mouth when their deciduous teeth are coming out to monitor any persistent teeth.\nAs soon as you notice two teeth in one spot, you need to make an appointment with your vet to have some treatment.']	['<urn:uuid:8c7fbfbc-5292-4d29-89dd-f333d3c6318b>', '<urn:uuid:a89aa88c-5492-41a5-bc0f-27343914df88>']	factoid	direct	long-search-query	distant-from-document	three-doc	novice	2025-05-13T05:06:54.527160	12	70	3048
80	What daily sodium limits are recommended for managing edema?	The recommended daily sodium intake should be limited to 1,500 to 2,000 milligrams per day. This can be managed by eating three meals each day with 500 milligrams or less of sodium, plus limiting snacks throughout the day to less than 500 milligrams of sodium. This restriction is important because sodium acts like a sponge to hold extra water in the body, which makes the heart work harder.	"[""Nutrition Tips for Congestive Heart Failure\nIf you have congestive heart failure, follow these nutrition guidelines:\n- Check food labels, and limit salt and sodium to 1,500 to 2,000 milligrams per day.\n- Replace salt and other high-sodium seasonings with alternatives that have no salt or are low in sodium (such as Mrs. Dash).\n- When eating out, think about hidden sources of salt and sodium, such as salad dressings and soups. Ask for options low in salt and sodium.\n- Choose meats and other foods that are low in saturated fat to help lower your cholesterol levels.\n- Avoid alcohol. If your heart failure is caused by alcohol, it's especially important that you don't drink any alcoholic beverages.\nSalt and Sodium\nSodium acts like a sponge to hold extra water in the body, which makes the heart work harder. Cutting down on sodium is one of the most important parts of your treatment plan. Sodium is found in large amounts in salt (sodium chloride) and is added to most prepared and processed foods.\nHere are some tips to lower the amount of sodium you eat:\nFollow this general guide: Eat three meals each day limited to 500 milligrams or less of sodium. Limit your snacking throughout the day to less than 500 milligrams of sodium.\nAvoid using salt at the table or in cooking. Remove the salt shaker you'll be less likely to use it.\nExperiment with new flavors. Use spices, herbs, and other seasonings instead of salt to flavor foods.\nEat fresh or frozen vegetables Fresh or frozen vegetables are low in sodium. Do not add salt or high-sodium seasonings (such as soy sauce). Balsamic vinegar and lemon juice enhance the flavors and can be used in place of salt.\nEat fresh vegetable salads and avoid bottled salad dressings. Make your own dressing and choose an oil and vinegar dressing while eating out. Potato or macaroni salads are often high in sodium. Ask if salt or pickles are used in these salads before ordering.\nEat fruit for dessert. Fresh, frozen, and canned fruit are lower in sodium than baked desserts.\nAvoid processed foods that come in cans or boxes. Canned and ramen noodle soups, macaroni and cheese, canned vegetables, tomato juice, baked or refried beans, packaged or bottled salad dressings and seasoning mixes, and instant potatoes are examples of processed foods high in sodium.\nLimit cheese. Most cheeses are high in sodium. If you love cheese, learn to read labels so you can find a low-sodium option to eat in small amounts.\nEat fresh meats, chicken, and fish. Processed and smoked foods, such as bologna, sausage, pepperoni, bacon, ham, hot dogs and battered chicken or fish, are all high in sodium.\nSnack on fresh fruits, vegetables, and unsalted nuts instead of salty snack foods such as chips or salted nuts. Healthy snacks are low in calories and good sources of vitamins, minerals, and fiber.\nLearn to read labels. Read food labels when you shop. The amount of sodium in the product is listed on the label. By reading labels, you can find low-sodium foods you like that can take the place of high-sodium foods you used to eat.\nFind a low-sodium cookbook or check the internet for low-sodium recipes and suggestions.\nBe patient. Changing food habits is a skill that takes time and practice. It takes taste buds three weeks to lose their taste for sodium. Get support from your family and friends and set realistic goals.\nSpice It Up\nHerbs and spices are a great way to make foods tasty without using salt. Some general guidelines for cooking with herbs and spices are:\n- To release more flavor and aroma, crumble dry leaf herbs — basil, bay leaf, oregano, savory, and others — between your fingers. Or finely chop fresh herbs just before using in recipes. Kitchen shears work great for this job.\n- In dishes that cook for a long time, such as soups and stews, add herbs and spices toward the end of the cooking time. That way the flavor won't cook out.\n- For chilled foods, such as salads and dips, add seasonings several hours ahead. That allows time for the flavors to blend.\n- When substituting fresh for dry herbs, 1 tablespoon of fresh herbs equals 1 teaspoon dried herb. Dry herbs are stronger than fresh; powdered herbs are stronger than crumbled herbs.\n- Some favorites used by many people are Mrs. Dash, curry powder, and cayenne or other hot pepper flavors.\n- Avoid using salt substitutes that are high in potassium. Using these products can lead to dangerously high levels of potassium that may cause problems with certain medicines used for heart failure.\nAvoid alcohol. If your congestive heart failure is alcohol-related, it's especially important for you to avoid alcoholic beverages.\nFat and Cholesterol\nA diet high in saturated fat and cholesterol can lead to more heart problems, such as clogged arteries. Instead of saturated fats such as butter, shortening, and stick margarine, choose small amounts of olive, canola, or peanut oil.\nFollow these tips to lower fat and cholesterol intake:\n- Avoid fatty cuts of meat, such as high-fat hamburger and prime cuts of meats.\n- Trim the visible fat off meat and remove the skin from poultry before cooking.\n- Eat more fish than red meat.\n- Bake, broil, grill, boil, or steam foods instead of frying.\n- Use fat-free milk and dairy products. Select cheeses low in sodium and fat.\n- Eat whole-grain cereals and breads. Remember to check sodium content on the label.\n- Use small amounts of canola oil or olive oil instead of solid fats when cooking.\n- Limit added fats such as salad dressing, mayonnaise, margarine, butter, or sour cream. When you do use these, choose low-sodium options.\nEating out is convenient, a nice break from cooking and a fun way to celebrate special occasions. Below are some tips for eating out without getting too much sodium.\n- Tell your server you are on a low-sodium diet and ask for suggestions that are low in salt and sodium.\n- Order grilled, baked, or broiled meat, chicken, or fish without added salt, sauces, or gravies. Use lemon and pepper to add flavor.\n- Select steamed rice, baked potato, or plain noodles instead of mashed potatoes or fried rice.\n- If the vegetables are not fresh or frozen, have a salad instead. Use oil and vinegar dressing or ask for the dressing on the side and use just a little.\n- Most menu items at fast food restaurants are high in sodium and fat. Ask for the printed nutrition information and choose low-sodium options.\n- Avoid condiments high in sodium, such as pickles, relish, and olives. Use just a small amount of ketchup, mustard, or mayonnaise.""]"	['<urn:uuid:641bca71-6e41-439b-972a-e4c981e00632>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	9	68	1129
81	What damage do marine munitions and fishing nets cause to sea life?	Marine munitions release toxic substances like TNT that cause liver tumors in fish and genetic damage in mussels. Meanwhile, fishing nets like gillnets trap and kill marine mammals, sea turtles, sharks and destroy coral reefs while tossing in currents.	"['Should they be monitored, removed or left alone? – researchers provide guidelines and decision-making support\nJoint press release from the Thünen Institute of Fisheries Ecology and the Alfred Wegener Institute\nThe bottom of the Baltic Sea is home to large quantities of sunken munitions, a legacy of the Second World War – and often very close to shore. Should we simply leave them where they are and accept the risk of their slowly releasing toxic substances, or should we instead remove them, and run the risk of their falling apart – or even exploding? Administrators and politicians face these questions when e.g. there are plans for building a new wind park, or laying an underwater cable. In the course of the international project DAIMON, researchers prepared essential decision-making aids, which were recently presented at the Thünen Institute in Bremerhaven.\nThe total amount of conventional munitions and chemical weapons in German waters is estimated at 300,000 metric tons. They were dumped in the water at the end of the war, with little thought given to the consequences for the environment. For example, just a stone’s throw from Kiel you’ll find the munitions dumping area Kolberger Heide – a restricted area near the beach in which ca. 35,000 metric tons of sea mines and torpedoes lie below the (max. 12-metre-deep) water. These munitions can still be dangerous even after decades on the seafloor, as an international team of researchers recently confirmed: the findings of the DAIMON (Decision Aid for Marine Munitions) project were presented at a conference jointly hosted by the Thünen Institute and Alfred Wegner Institute in Bremerhaven from 5 to 7 February 2019.\nThrough painstaking efforts, the contributing researchers collected samples and analysed the chemicals being released by the munitions. Traces of the munitions could be found in fish from dumping areas; these included metabolites of TNT and chemical weapons containing arsenic. Mussels that were exposed to the conditions in Kolberger Heide in small net cages accumulated TNT metabolites – clear proof that the bombs are still releasing toxic substances, which are absorbed by local biota. The researchers also confirmed that TNT is poisonous for mussels and causes genetic damage in fish, which can produce tumours. Specimens of the common dab, a sensitive flatfish species, caught in Kolberger Heide had more liver tumours than specimens from other waters, which indicates a causal relation between local TNT pollution and increased tumour rates. The metabolites of TNT are also mutagenic; as a result, marine organisms are still being affected by the munitions, even if the rapidly decomposing TNT itself is only a distant memory.\nThe results of this and other investigations were in turn used as the basis for practically and directly applicable recommendations on environmental monitoring and how to handle dumped munitions. As such, the chief outcomes of the DAIMON project are guidelines for monitoring and assessing risks: a collection of methods from the field of environmental monitoring that can be used to assess the acute risks to the ecosystem posed by munitions (DAIMON Toolbox), as well as a web-based system (Decision Support System) to help e.g. politicians and government offices decide whether munitions e.g. in the Baltic Sea should simply be monitored, or should be removed. During the conference, there was a live demonstration of the system, which was also available for interested participants to try.\nThe event was jointly organised by the Alfred Wegener Institute, Helmholtz Centre for Polar and Marine Research, and the Thünen Institute of Fisheries Ecology. More than 100 participants from the research, administrative, political and industrial communities attended.\nDr. Thomas Lang\nThünen Institute of Fisheries Ecology, Bremerhaven\nDr. Matthias Brenner\nAlfred Wegener Institute, Helmholtz Centre for Polar and Marine Research, Bremerhaven', ""Destructive Fishing Gear\nProtecting Belize’s ocean heritage\nTrawls and gillnets are some of the most destructive forms of fishing gear in the world.\nSince 2009, Oceana has been promoting the importance of protecting Belize’s natural resources, encouraging healthy fisheries and highlighting the impact of destructive fishing gear in Belize. Our long-term vision for the Belize Barrier Reef is a thriving ecosystem, capable of producing abundant, healthy marine resources that support a vibrant and sustainable local economy for Belizeans.\nIn December 2010, the Belizean government announced a ban on all forms of trawling in the country's waters. Oceana in Belize played a crucial role in the decision, collaborating with Belizean Prime Minister Dean Barrow’s administration to negotiate the buy-out of the two shrimp trawlers.\nTrawls are one of the most destructive forms of fishing gear in the world. Shrimp trawls are notorious for the amount of bycatch, or untargeted catch, they haul in. Thousands of sea turtles, marine mammals and untargeted fish are caught in shrimp trawlers around the world every year. Meanwhile, bottom trawlers’ weighted nets effectively clear-cut the ocean floor with every pass, destroying sensitive corals and anything else in their way.\nWith this ban, which went into effect December 31st, 2010, Belize became one of the first countries in the world to institute a complete and permanent ban on trawling in all its waters.\nOceana is also working to phase out gillnet use in Belize.\nGillnets are notorious for high levels of bycatch, often trapping and killing marine mammals, sea turtles, sharks and other ecologically and economically important fish. In Belize, there is evidence of the gear catching, entangling and killing protected species like bonefish, tarpon, permit, manatees and endangered species such as scalloped hammerheads. Gillnets are already credited with the regional extinction of sawfish.\nIn addition to removing an increasingly large number of fish from the ocean, gillnetting can be even more destructive when they become lost or forgotten in the water because they continue to catch animals, a phenomenon known as “ghost fishing”. Without anyone profiting from the catches, they are affecting already depleted commercial fish stocks. Caught fish die and in turn attract scavengers which will get caught in that same net, thus creating a vicious cycle of death.\nMoreover, when caught on a reef, nets not only catch fish, turtles, crustaceans, birds or marine mammals, they also destroy hard and soft corals, wiping out complete ecosystems while tossing around in the current. Aside from being bad for the environment, gill nets are bad for people.\nBased on economics, science, regional examples and stakeholder engagement, Oceana proposes a phase out of gillnets over two years in order to protect endangered species and livelihoods in tourism and fishing. During the phase out period, Oceana is committed to support Belize Fisheries Department in advancing the transition of the less than 200 licensed Belizean gillnet fishermen to alternative economic activities and fishing gears, and in reducing the impact of illegal fishing on Belizean fishermen.\nBy stopping the use of gillnets, Belize would take a great step to protect the traditions and livelihoods of thousands of fishers, and its source of affordable, nutritious fish for generations to come.""]"	['<urn:uuid:be88d2e8-ff0e-48c0-8c3c-91daed36cb10>', '<urn:uuid:75b08153-a73b-4943-bb88-e44c32d69a02>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T05:06:54.527160	12	39	1145
82	when earliest co2 levels were lower than today	Atmospheric CO2 concentrations in 2019 were higher than at any time in at least 2 million years	['Climate change is widespread, rapid, and intensifying, and some trends are now irreversible, at least during the present time frame, according to the latest much-anticipated Intergovernmental Panel on Climate Change (IPCC) report.\nCompiled by Tandin Wangchuk\nThe report confirms that the intensity and frequency of hot extremes, including heat waves, and heavy precipitation, as well as agricultural and ecological droughts, continue to increase as temperatures rise.\nFor South Asia including Bhutan and Nepal, the average precipitations, as well as heavy rainfall events, are projected to increase.\n“This will result in more flooding and landslide events. All this will have far-reaching consequences across the region with water-dependent energy sectors and water-intensive agricultural systems,” the report warns.\nIt further states the most vulnerable countries, including mountainous areas, are already at the forefront of fatal climate and weather disruptions. These countries with limited resources will continuously require international support to combat climate change, and at a scale well beyond current levels.\nFurther IPCC forewarns that the biggest polluters such as the G7 and G20 must take urgent action in response to the climate emergency, starting by adopting robust and bold 2030 climate commitments by the next UN climate summit – the COP26 – scheduled in November this year.\nMany of these changes are unprecedented, and some of the shifts are in motion now, while some – such as continued sea level rise – are already ‘irreversible’ for centuries to millennia, ahead, the report warns.\nThe UN Secretary-General António Guterres said the Working Group’s report was nothing less than “a code red for humanity. The alarm bells are deafening, and the evidence is irrefutable”.\nHe noted that the internationally-agreed threshold of 1.5 degrees above pre-industrial levels of global heating was “perilously close. We are at imminent risk of hitting 1.5 degrees in the near term. The only way to prevent exceeding this threshold, is by urgently stepping up our efforts, and persuing the most ambitious path.\n“We must act decisively now, to keep 1.5 alive.”\nThe UN chief in a detailed reaction to the report, said that solutions were clear. “Inclusive and green economies, prosperity, cleaner air and better health are possible for all, if we respond to this crisis with solidarity and courage”, he said.\nHe added that ahead of the crucial COP26 climate conference in Glasgow in November, all nations – especiall the advanced G20 economies – needed to join the net zero emissions coaltion, and reinforce their promises on slowing down and reversing global heating, “with credible, concrete, and enhanced Nationally Determined Contributions (NDCs)” that lay out detailed steps.\nThe report, prepared by 234 scientists from 66 countries, highlights that human influence has warmed the climate at a rate that is unprecedented in at least the last 2,000 years.\nIt is getting worse\nIn 2019, atmospheric CO2 concentrations were higher than at any time in at least 2 million years, and concentrations of methane and nitrous oxide were higher than at any time in the last 800,000 years.\nGlobal surface temperature has increased faster since 1970 than in any other 50-year period over a least the last 2,000 years. For example, temperatures during the most recent decade (2011–2020) exceed those of the most recent multi-century warm period, around 6,500 years ago, the report indicates.\nMeanwhile, global mean sea level has risen faster since 1900, than over any preceding century in at least the last 3,000 years.\nThe document shows that emissions of greenhouse gases from human activities are responsible for approximately 1.1°C of warming between 1850-1900, and finds that averaged over the next 20 years, global temperature is expected to reach or exceed 1.5°C of heating.\nThe IPCC scientists warn global warming of 2°C will be exceeded during the 21st century. Unless rapid and deep reductions in CO2 and other greenhouse gas emissions occur in the coming decades, achieving the goals of the 2015 Paris Agreement “will be beyond reach”.\nThe assessment is based on improved data on historical warming, as well as progress in scientific understanding of the response of the climate system to human-caused emissions.\n“It has been clear for decades that the Earth’s climate is changing, and the role of human influence on the climate system is undisputed,” said IPCC Working Group I Co-Chair, Valérie Masson-Delmotte. “Yet the new report also reflects major advances in the science of attribution – understanding the role of climate change in intensifying specific weather and climate events”.\nThe experts reveal that human activities affect all major climate system components, with some responding over decades and others over centuries.\nScientists also point out that evidence of observed changes in extremes such as heatwaves, heavy precipitation, droughts, and tropical cyclones, and their attribution to human influence, has strengthened.\nThey add that many changes in the climate system become larger in direct relation to increasing global warming.\nThis includes increases in the frequency and intensity of heat extremes, marine heatwaves, and heavy precipitation; agricultural and ecological droughts in some regions; the proportion of intense tropical cyclones; as well as reductions in Arctic sea ice, snow cover and permafrost.\nThe report makes clear that while natural drivers will modulate human-caused changes, especially at regional levels and in the near term, they will have little effect on long-term global warming.\nThe IPCC experts project that in the coming decades climate changes will increase in all regions. For 1.5°C of global warming, there will be increasing heat waves, longer warm seasons and shorter cold seasons.\nAt 2°C of global warming, heat extremes are more likely to reach critical tolerance thresholds for agriculture and health.\nBut it won’t be just about temperature. For example, climate change is intensifying the natural production of water – the water cycle. This brings more intense rainfall and associated flooding, as well as more intense drought in many regions.\nIt is also affecting rainfall patterns. In high latitudes, precipitation is likely to increase, while it is projected to decrease over large parts of the subtropics. Changes to monsoon rain patterns are expected, which will vary by region, the report warns.\nMoreover, coastal areas will see continued sea level rise throughout the 21st century, contributing to more frequent and severe coastal flooding in low-lying areas and coastal erosion.\nExtreme sea level events that previously occurred once in 100 years could happen every year by the end of this century.\nThe report also indicates that further warming will amplify permafrost thawing, and the loss of seasonal snow cover, melting of glaciers and ice sheets, and loss of summer Arctic sea ice.\nChanges to the ocean, including warming, more frequent marine heatwaves, ocean acidification, and reduced oxygen levels, affect both ocean ecosystems and the people that rely on them, and they will continue throughout at least the rest of this century.\nApart from the urgent need for climate mitigation, “it is essential to pay attention to climate adaptation”, said the WMO chief, Peteri Taalas, “since the negative trend in climate will continue for decades and in some cases for thousands of years.\n“One powerful way to adapt is to invest in early warning, climate and water services”, he said.”Only half of the 193 members of WMO have such services in place, which means more human and economic losses. We have also severe gaps in weather and hydrological observing networks in Africa, some parts of Latin America and in Pacific and Caribbean island states, which has a major negative impact on the accuracy of weather forecasts in those areas, but also worldwide.\n“The message of the IPCC report is crystal clear: we have to raise the ambition level of mitigation.”']	['<urn:uuid:2cec2d42-4fd9-4d5c-953e-cca9880bf358>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T05:06:54.527160	8	17	1262
83	bilingual education impacts academic content costs benefits	Bilingual education has both advantages and challenges in terms of academic content delivery. On the positive side, it provides English as Second Language (ESL) learners with opportunities to attain higher academic achievement compared to other educational programs by helping them gain communication skills necessary to learn and comprehend content. However, there are challenges as students may initially struggle with absorbing content delivered in two languages simultaneously. The focus on language acquisition can potentially shift attention away from academic content, as students need to first understand the language being used before engaging with the subject matter. Teachers need to dedicate significant effort to ensure learners understand the language of instruction, which can sometimes come at the expense of content delivery.	['Success Through Academic Interventions in Language & Literacy\nLanguage and Literacy\nWe have developed two research-based models, a structured English immersion model (ENG) and a transitional bilingual model (BE), that share a common goal—the acquisition of English skills and high levels of content knowledge in Spanish- speaking English language learners. In ENG, students receive all their instruction in English using a sheltered instruction approach. Our BE program promotes oral English language and literacy development in Kindergarten and Grade 1, and provides structured English literacy instruction in English in Grades 2 and 3, while maintaining some Spanish language and literacy instruction in these grades (2 and 3).\nTeachers in treatment classrooms use researcher-enhanced versions of language and literacy curriculum, while teachers in control classrooms continue with their regular methods of instruction. During the 90 minute reading block we provide enhanced phonics instruction, reading and skills practice, and oral language development through read-alouds (Lectura), in either Spanish or English depending on the program model. Outside of the 90 minutes reading block, for students in the bilingual program we provide English language development and English reading and skills practice. For students in the English program, we provide English writing and grammar practice.\nResults from the first grade assessments indicate the program is highly effective in developing word reading skills as well as vocabulary and listening comprehension. Students in the ‘enhanced’ classrooms significantly outperformed students in the regular classrooms in English oral vocabulary and English listening comprehension\non standardized assessments. Both groups of students performed very well on standardized assessments of word reading, with no differences between the two groups, even though students in the enhanced versions of instruction spent half\nas much time in reading skills instruction.\nWe use the basal reading materials as the basis for reading and skills practice as it is the curriculum selected by the district. For first and second grade students, the curriculum consists of six five-week modules.\nIn order to enhance the ease of using the materials each week of curriculum follows the same format. The week begins with a ‘Guided Reading of the Decodable Story’ that includes several components: students learn new vocabulary related to the story, practice reading high frequency words, listen to the story read orally by the teacher, respond to higher level questions, participate in echo reading, and read with their partner. Subsequent days have been organized for differentiated instruction so that teachers may use materials and instructional methods that best meet the needs of all the student sin the class. In addition, on one day of the week, students learn a reading and a study skill. Each skill mini-lesson includes four fundamental steps, introduction of the concept, group practice, individual practice, and review. The final day of the week the students have the opportunity to show what they have learned through the assessment.\nIn order to accommodate the needs of second-language learners we develop oral language proficiency in the context of teaching reading—we focus on developing students vocabulary and listening comprehension as well as their oral production skills and reading comprehension. For example, the SAILL Safari is designed to challenge and stimulate students in comprehension with unique activities and questions that get progressively more difficult and require higher order thinking skills. The My Phonics Word Book was developed so that students could create their own pictures and sentences to help them learn and practice new words. Glossaries of vocabulary and site words provide students with pictures of each word to help develop their vocabulary knowledge.\nWriting and grammar practice\nWe use the basal grammar materials as the basis for our grammar lessons. As with reading, each grammar and mechanics lesson is taught using a mini-lesson that includes four fundamental steps: introduction of the concept, group practice, individual practice, and review. We teach five genres of writing--- personal narrative, persuasive, explanatory, comparison, expository and narrative story and use a sequence of activities to teach writing —group practice researching the genre, group practice writing using the genre, individual practice writing with two rounds of editing based on teacher feedback, and writing presentations. Students use graphic organizers for both genre elements and language elements related to the genre (e.g. first, second, for …) and are provided with rubrics that help them assess their writing.\nLectura is a read-aloud program designed to promote the English vocabulary development and oral proficiency of Spanish-speaking English Language Learners in Grades 1 and 2. Through the systematic instruction of vocabulary and carefully designed scaffolding of reading materials, Lectura makes the content of rich children’s literature and science texts accessible to ELLs. In Grade 1 only narrative texts are used in the program, and in Grade 2 the Lectura techniques are applied both to narrative and expository texts.\nEach weekly lesson consists of a read-aloud book and vocabulary instruction using picture cards. Lectura teaches four types of words: key words (and key science words), basic words, and “other words and phrases.” These are described below.\nKey words : Key words are words that appear in the text and are taught before the text is read. We have selected these words for pre-teaching because:\nThey are important for understanding the meaning of the story.\nThey are words that are multidimensional, or have depth of meaning, as opposed to words whose referents are more concrete.\nKey words have been chosen in consultation with the Living Word Vocabulary, a reference that provides information about the grade levels at which a given word is known by most children. Following current research guidelines for vocabulary instruction of first and second graders (Biemiller & Slonin, 2001), we have selected key words that are known by most of the children who are in either fourth or sixth grade. In this way, we are confident that the difficulty of the key words corresponds to the conceptual level of development of children in Grades 1 and 2. Lectura key words are often new labels for known concepts: e.g., possess (have); leap (jump); joy (happiness), but they may also present important new concepts: e.g., honor, depend, unfortunate, hesitate, courage. A sample of key words picture cards.\nKey science words : Science words are taught in Grade 2 lessons that present expository texts. They are taught in the same manner as key words are taught, but unlike other key words, they do not necessarily appear in the science text. These words are useful in discussing the scientific processes described in the expository texts. They include terms such as: observe, locate, measure, identify, etc.\nBasic Words: The basic words from each week come from one category; the category is aligned with the theme of the read-aloud book, and with the ESOL Standards for the respective grade. Grouping words thematically in this way emphasizes the connections that exist among word meanings, and helps the students to develop semantic networks, or associations between a given word and other related words. To teach basic words, the teacher uses the text that accompanies images to talk about the meanings of the basic words. A sample of basic words picture cards.\nOther Words and Phrases: The “Other Words and Phrases” activity focuses on idioms and other expressions that may be difficult for second language learners.\nThe category “other words and phrases” comprises high frequency words. These\nare the core words of the English language, words that are speakers use and hear often, and which appear in print over and over again. Word types in this category include words that express grammatical relationships, such as conjunctions and prepositions (e.g., because, among), adverbs of time and place (e.g., often, across), and idioms (e.g., I don’t believe my eyes!). A sample of other words picture cards.\nIn teaching ‘other words and phrases,’ the target word or phrase is first defined in English. Second, a picture that demonstrates the word or phrase is presented. The word is then defined in Spanish. Finally, children are invited to explore the word through dialogues with the teacher that deepen the children’s understanding of the word.\nIn Grade 1, a total of 200 key words, 250 basic words and 100 other words and phrases are taught. In Grade 2, 120 key words, 120 basic words and 48 other words and phrases are taught.\nOther components of the read-alouds consist of interactive reading, glossary work, assessments, and review.\nInteractive Reading: Interactive reading employs paraphrasing and questioning techniques designed to clarify the meaning of the overall text and expand word knowledge. More specifically, Interactive Reading comprises:\nExplanations of key words and other words essential to understanding the text, either through brief explanations or reference to illustrations.\nBuilding background knowledge important in understanding the story.\nQuestioning techniques that allow the teacher to model comprehension strategies. Such strategies will be important tools for students in making meaning of text as independent readers. The strategies include visualizing, drawing inferences, making connections from text-to-text, text-to-self, and text-to- world, summarizing, and inferring word meaning.\nAdditional questions that alternate between questions for individuals and questions for partners.\nDiscussion questions that encourage students to infer, predict, evaluate or relate the book to their own experiences.\nGlossary Work: Each week, students are given a glossary of the key vocabulary (and scientific vocabulary for expository text) for homework. In this glossary, students complete a fill-in-the-blank exercise, using a picture of the word and its definition as presented in the key word activities. In addition, Glossary I presents two comprehension questions related to the text.\nAssessment: In Grade 2, only students in the bilingual classes are assessed on their vocabulary knowledge and comprehension of the story on Day 5. The assessment consists of:\nResponding to comprehension questions\nAn optional game that provides additional encounters with the vocabulary words\nReviews: Each week consists of four lessons and an assessment and review activities on Day 5. Every fifth week, there is a three-day review of material taught in the previous four lessons. In addition, during some lessons in the review week, students engage in an activity to build their conversational ability. The topics that are covered in this activity are aligned with grade level ESOL standards.', 'Integrating bilingualism in early education enhances the learners’ abstract thinking abilities, problem-solving skills and prepares them for the increasingly globalized and diversified world. Bilingual learning is a form of teaching in which literacy and content are delivered in two or more languages. In today’s world, learning institutions are providing their services to children from heterogeneous backgrounds. In this regard, using a second language has become a growing phenomenon as schools increasingly embrace and encourage multiple dialects. The trend is influenced by social processes and movements worldwide, such as immigration, immersing learners in a community where the new language is spoken. Generally, bilingual education is categorized into one and two-way programs. Although integrating bilingualism in early education can shift a learner’s focus and overshadow the absorption and cognition of specific content, it enhances the student’s adaptability and the ability to attain high levels of academic achievement.\ntailored to your instructions\nfor only $13.00 $11.05/page\nThe following paper explores bilingual programs in early childhood education, the definition, and the types of these strategies. It also enumerates and discusses the benefits of this learning system, including the ease of attaining fluency due to early exposure, cognitive advantages, and equipping students with high adaptability and flexibility. Additionally, the paper will highlight how bilingualism provides opportunities for English learners to attain higher academic achievements than using other types of educational programs. The discussion will provide insights on the counter arguments for implementing bilingual education in early learning and the associated implications.\nDefinition and Types of Bilingual Education\nBilingual education is a widely practiced model of learning in many countries across the globe. It refers to the system and structure of providing and delivering content to learners in two dialects, where one of them is invariably the child’s arterial language (Rodriguez-Tamayo & Tenjo-Macias, 2019). This teaching and instruction framework recognizes the indispensability of language as the foundation upon which all communication is anchored. From this dimension, bilingual education encompasses bicultural learning, acknowledging the inseparable and intertwined nature of culture and language (Krasniqi, 2019). The principal objective of bilingual education is to ensure that a learner who is proficient in their mother tongue achieves the same level of expertise and mastery in the other language. In this regard, students enrolled under dual language education programs simultaneously learn literacy and academic content delivered in English and their mother tongue, known as partner language. The bilingualism learning model is designed to support the integration of immigrants and other minorities into the dominant society and progressively adjust to the new country’s ways of life.\nTypes of Bilingual Education\nGenerally, dual language education is categorized into one- and two-way program models. The former targets learners predominantly from one expansive group and is further subdivided into three classes, including the one-way or world language immersion, which primarily serves English speakers. Under this model, learners develop academic skills in their native dialect while building and enhancing skills in a different language, thereby supporting such learners to become bi-literate, bilingual, and bicultural (August et al., 2015). Conversely, maintenance or developmental bilingual programs principally enroll learners who are the native speakers of the partner dialect (Téllez, 2018). This strategy targets English as a Second Language (ESL) and former ESL learners. The heritage or native language programs target learners who are dominant in English but have family or cultural links to the partner language.\nThe two-way dual language immersion model is anchored on the premise that two groups of learners, each with a distinct native language and one being English, can study together in a systematic and structured manner. This program combines English-speaking and partner-language-speaking students, preferably on a ratio of 1:1. According to Li et al. (2016), this language immersion program is a promising strategy through which educators and policymakers enhance students’ ability to become bilingual and bi-literate. Therefore, the parents, grandparents, or other ancestors spoke the partner language, although the pupils are dominant English speakers.\nBenefits of Implementing Bilingual Programs in Early Education\nIncorporating bilingual programs in early childhood education provides numerous benefits to the learners. According to Bialystok (2016), bilingualism for young children enhances their cognitive abilities, positively impacting their ease of gaining fluency in the new language. This perspective reflects the beneficial influence of bilingual education models regarding their effectiveness in reinforcing knowledge acquisition and conceptualization. Notably, bilingualism alters the human brain’s structure, making the cognitive mechanisms more flexible than in the conventional education systems (Wong et al., 2016). Bilingual learners bring rich, diverse, linguistic, and cultural experiences from their communities and homes and transition to the second language with ease (Zehr, 2007; New York State Association for Bilingual Education, 2017). For instance, native French speakers introduce their culture and customs in the bilingual classroom settings, triggering curiosity and exchanging experiences during interactions with their English–speaking counterparts. This multiplicity triggers unique and complex processes associated with developing strong cognitive skills, including attention, memory, and concept development.\nAdditionally, early exposure to a second language enhances the student’s ability to achieve fluency easily. Generally, gaining competency in another language becomes more difficult as one’s age advances. Hartshorne et al. (2018) contend that it is considerably challenging for students to reach native-level fluency if they delay learning the second language. This dimension incorporates bilingual programs in early childhood education, facilitating fluency and competency in the second language. In this regard, early childhood is the prime phase at which the acquisition of fluency and mastery of a second language is easily accomplished (Hartshorne et al., 2018). This view illustrates the decline in the learning abilities due to various occurrences, including changes in the brain’s plasticity, which significantly impairs the learners’ ability to achieve fluency at later ages.\nas little as 3 hours\nFurther, bilingual education provides children with a broad outlook and brings them close to diverse races and nationalities. Notably, exploring multiple cultural concepts in the classroom through language allows students to learn and grow alongside classmates from different backgrounds. Rodriguez-Tamayo and Tenjo-Macias (2019) assert that dual-language experiences help students to become comfortable engaging and interacting with their counterparts from various ethnographic groups. Similarly, the educational programs create cross-cultural school settings, which effectively ease the pressure and tensions between diverse language groups and communities. Consequently, these learners become highly adaptable and distinctively aware of the diversities of the world around them. Moreover, these students generally handle change easily since they are more accommodative to differences and are willing to adjust their behaviors and ideas. Also, learners who are enrolled in a bilingual program in early childhood education register remarkable brain development, enhancing their ability to switch easily and transition between tasks, environments, and circumstances.\nBilingual programs provide ESL learners with the opportunity to attain higher levels of academic achievement compared to other types of programs. Dual-language immersion strategies significantly minimize educational inequalities by helping non-native English speakers gain communication skills to learn and conceive the delivered content. Indeed, language constitutes one of the most prominent barriers which hinder the minority students from excelling and finishing their education (Li et al., 2016). Equipping learners with solid language and communication skills improves their comprehension of the issued instructions and the content, effectively stimulating their desire and willingness to study more. From this perspective, high-quality bilingual programs are a critical pedestal on which non-native English learners can effectively compete with their peers, study content, and engage in constructive discussions, contributing to higher educational attainment. Although other programs, such as No Learner Left Behind, eliminate these systemic disparities and structural inequalities, they do not enhance students’ specific abilities. In this regard, bilingualism in early childhood education provides opportunities for English learners to excel and reach high academic levels than other programs.\nCounterarguments for Implementing Bilingual Education in Early Childhood Settings\nAlthough bilingualism in early childhood education provides numerous advantages to learners, various counterarguments impede this model’s implementation. Among the prominent drawbacks of these programs is the progressive but subtle loss of cultural identity. Notably, culture and languages are intertwined and draw from one another. This implies that the aggressive levels of cross-cultural interactions through language lead to competing and conflicting cultures. Antonela and Sanja (2017) posit that dual-dialect educational programs place learners in a multicultural setting where contextual social pressures steadily erode their identity. For instance, students will feel compelled to adjust how they express their thoughts and emotions to fit in the new environment. Additionally, foreign languages’ influence may prevent learners from participating and involving themselves in their native cultural activities. This phenomenon exposes them to new lifestyles, customs, and habits and widens the culture gap experienced at the local level. Therefore, bilingualism in early childhood education significantly contributes to the loss of cultural identity.\nAdditionally, bilingual education programs can potentially shift the learner’s focus from the academic content to the new language. Notably, the effective delivery of the subject matter and content is anchored on the teachers’ and students’ ability to communicate successfully. This implies that disproportionate levels of effort should first be directed at ensuring that learners understand the language being used (Goldenberg & Wagner, 2015). As a result, the young students may find themselves fixated or pushed to obtain language literacy at the expense of the academic content. This scenario is worsened in situations where a student struggles with the knowledge acquisition of the new dialect.\nMoreover, bilingual education may be difficult at the commencement, which may exert pressure on learners. Notably, young schoolers may experience difficulties absorbing content delivered in the second language since two unfamiliar concepts are being introduced taught at the same time. In many schools, tutors teach one language during a specific part of the day then switch to the other dialect for the remainder of the session (Li et al., 2016). In this regard, students may require extra effort and additional time to learn the basics of the new language, which may impede their focus on content areas. This is closely associated with the potential increase in stress levels as the learners juggle between absorbing the academic matter and language literacy concepts. Additionally, the focus on language fluency may generate confusion due to mixing languages, which may impede or delay content cognition. In instances where learners may prefer one language over the other, this challenge becomes a major hindrance to acquiring fluency, comprehension, and mastery of the other dialect. Overall, students will generally excel in one language than two dialects.\nFurther, bilingual education programs are often associated with immigration, attracting some degree of stigma and ethnic prejudice. Some communities perceive this education model as a structured way of integrating immigrants and minorities into society instead of encouraging them to embrace their culture and languages. In this regard, bilingualism is viewed and considered a strategy for the local community to help the immigrants fit in the new environment, generating prejudicial thoughts (Goldenberg & Wagner, 2015). This creates stigmatization, particularly for the non-native English speakers who may be deemed inferior due to their inability to communicate effectively in the dominant language. This stigma has also contributed to the unavailability of trained and fully qualified teachers.\nBilingualism in early childhood entails the simultaneous delivery of academic content and language literacy. Educators using this model teach the learners in their native language in conjunction with a second dialect known as partner language. Generally, bilingual programs are categorized in one- and two-way dual-language models and are associated with numerous benefits, including high academic attainment, appreciation of diverse cultures, and learners’ adaptability. However, second languages are inconsistent and may not transcend beyond the lower education levels, are significantly challenging for students at the commencement, and limits learners’ participation in local cultures.\nBilingual education is increasingly gaining traction and is widely practiced across the world. Its primary objective is to ensure that children proficient in their mother tongue obtain similar expertise and mastery levels in other languages. Notably, most countries feel compelled to adopt and integrate bilingual education programs in early childhood settings due to the growth patterns of immigration and increasingly globalized and diversified society. Notably, bilingualism recognizes and respects the prevalent diversities in modern communities and creates a system which perpetuates multiculturalism. Additionally, it acknowledges the indispensability and the integral role of language in the process of knowledge acquisition. This scenario implies that, as communities become extensively globalized, there is a pressing need to embrace this diversity and create inclusive systems, including a non-discriminatory education.\nAdditionally, bilingual education promises better academic outcomes than other programs, which seek to eliminate the prevalent, educational inequalities and disparities. In this regard, it is imperative for government agencies and other relevant bodies to increase the funding and support for this kind of educational program. Moreover, adjustments and appropriate interventions in such areas as teacher training should be escalated to improve their effectiveness in delivering content areas in multilingual settings (Houston Independent School District, 2019). The support can be extended to other critical areas of bilingual programs in early childhood education, such as the most appropriate student ratio, content delivery methodologies, and mitigation of the adverse implications of this model of learning.\nOverall, although substantial evidence highlights the numerous benefits associated with implementing bilingual immersion programs, their success is primarily dependent on the fidelity with which they are executed. This implies that policymakers and other stakeholders should develop appropriate structures and strategies to enhance adherence in effecting the programs. Also, researchers should explore ways to capitalize and transfer the cognitive benefits associated with bilingualism to other areas.\nBilingual education is the use of instructions and delivery of academic content using two languages. This education model is premised on the indispensability of language as a tool of communication and an inseparable component of culture. Teachers using this approach promote the academic attainment of immigrants and other minorities, thereby eliminating educational inequalities and disparities. Bilingualism in early childhood enhances the learners’ cognitive abilities, promotes their adaptability, and stimulates the ease of attaining competency in a new language.\nAntonela, B., & Sanja, S. (2017). The relationship between bilingualism and identity in expressing emotions and thoughts. Íkala, Revista De Lenguaje Y Cultura, 22(2), 33–54. Web.\nAugust, D. & Boyle, A. (2015). Dual language education programs: Current state policies and practices. Center for English Language Learners at American Institutes for Research. Web.\nBialystok, E. (2016). Bilingual education for young children: Review of the effects and consequences. International Journal of Bilingual Education and Bilingualism, 21(6), 666–679. Web.\nGoldenberg, C., & Wagner, K. (2015). Bilingual education: Reviving an American tradition. American Educator, 39(3), 28-32. Retrieved April 6, 2021, from Web.\nyou can get a custom-written\naccording to your instructions\nHartshorne, J., Tenenbaum, J., & Pinker, S. (2018). A critical period for second language acquisition: Evidence from 2/3 million English speakers. Cognition, 177, 263–277. Web.\nHouston Independent School District. (2020). Dual language program evaluation report. Web.\nKrasniqi, K. (2019). The relation between language and culture (case study Albanian Language). Linguistics and Literature Studies, 7(2), 71–74. Web.\nLi, J., Steele, J., Slater, R., Bacon, M., & Miller, T. (2016). Implementing two-way dual-language immersion programs: Classroom insights from an urban district. International Multilingual Research Journal, 10(1), 31–43. Web.\nNew York State Association for Bilingual Education. (2017). Position statement on bilingual education in early childhood/preschool programs. Journal of Multilingual Education Research, 7(1), 15–18. Web.\nRodríguez-Tamayo, I., & Tenjo-Macías, L. (2019). Children’s cultural identity formation: Experiences in a dual language program. Gist Education and Learning Research Journal, 18, 86-108. Web.\nWong, B., Yin, B., & O’Brien, B. (2016). Neurolinguistics: Structure, function, and connectivity in the bilingual brain. Biomed Research International, 2016, 1–22. Web.\nZehr, M. A. (2007). N. J. Bucks tide on reading for English-learners: State cites studies finding advantage for bilingual approach. Education Week, 26(18), 1–12. Web.']	['<urn:uuid:195639f4-15b7-4f19-ac58-1db4e1eab4d4>', '<urn:uuid:7c02eb2d-a34e-41ed-938e-23beb0438ceb>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-13T05:06:54.527160	7	119	4272
84	What makes this new water filtering technology better than other existing methods for cleaning industrial water?	This new filter technology is superior because it doesn't foul (become clogged), making it last longer. It requires lower operating pressures, which means it needs a smaller pump that consumes less electricity. The filter's superhydrophilic surface treatment results in increased water flow through the membrane while blocking hydrophobic materials like oil. Traditional filters, in contrast, tend to clog easily, especially from hydrocarbons that form into globules, and multistage filters that could remove all contaminants are impractical due to cost and energy consumption.	['A superhydrophilic filter produced at Rice University can remove more than 90 percent of contaminants from water used in hydraulic fracturing operations. In this image, ‘produced’ water from a Marcellus shale fracturing site is at left, the retentate (carbon removed from the feed) is at center, and filtered ‘permeate’ water is at right. The hydrophilic treatment keeps the filter from fouling and restricting flow while rejecting contaminants.\nCredit: Barron Research Group/Rice University\nA new filter produced by Rice University scientists has proven able to remove more than 90 percent of hydrocarbons, bacteria and particulates from contaminated water produced by hydraulic fracturing (fracking) operations at shale oil and gas wells.\nThe researchers determined one pass through the membrane should clean contaminated water enough for reuse at a well, significantly cutting the amount that has to be stored or transported.\nThe work is reported in Nature’s open-access Scientific Reports.\nThe filters keep emulsified hydrocarbons from passing through the material’s ionically charged pores, which are about one-fifth of a micron wide, small enough that other contaminants cannot pass through. The charge attracts a thin layer of water that adheres to the entire surface of the filter to repel globules of oil and other hydrocarbons and keep it from clogging.\nA hydraulically fractured well uses more than 5 million gallons of water on average, of which only 10 to 15 percent is recovered during the flowback stage. “This makes it very important to be able to re-use this water,” Barron said.\nNot every type of filter reliably removes every type of contaminant, he said.\nSolubilized hydrocarbon molecules slip right through microfilters designed to remove bacteria. Natural organic matter, like sugars from guar gum used to make fracking fluids more viscous, require ultra- or nanofiltration, but those foul easily, especially from hydrocarbons that emulsify into globules. A multistage filter that could remove all the contaminants isn’t practical due to cost and the energy it would consume.\n“Frac water and produced waters represent a significant challenge on a technical level,” Barron said. “If you use a membrane with pores small enough to separate, they foul, and this renders the membrane useless.\n“In our case, the superhydrophilic treatment results in an increased flux (flow) of water through the membrane and inhibits any hydrophobic material — such as oil — from passing through. The difference in solubility of the contaminants thus works to allow for separation of molecules that should in theory pass through the membrane.”\nBarron and his colleagues used cysteic acid to modify the surface of an alumina-based ceramic membrane, making it superhydrophilic, or extremely attracted to water. The superhydrophilic surface has a contact angle of 5 degrees. (A contact angle of 0 degrees would be a puddle.)\nThe acid covered not only the surface but also the inside of the pores, and that kept particulates from sticking to them and fouling the filter.\nIn tests with fracking flowback or produced water that contained guar gum, the alumna membrane showed a slow initial decrease in flux — a measure of the flow of mass through a material — but it stabilized for the duration of lab tests. Untreated membranes showed a dramatic decrease within 18 hours.\nThe researchers theorized the initial decrease in flow through the ceramics was due to purging of air from the pores, after which the superhydrophilic pores trapped the thin layer of water that prevented fouling.\n“This membrane doesn’t foul, so it lasts,” Barron said. “It requires lower operating pressures, so you need a smaller pump that consumes less electricity. And that’s all better for the environment.”\nStory Source: Materials provided by University of Chicago Original written by Whitney Clavin.Note: Content may be edited for style and length.\nSamuel J. Maguire-Boyle, Joseph E. Huseman, Thomas J. Ainscough, Darren L. Oatley-Radcliffe, Abdullah A. Alabdulkarem, Sattam Fahad Al-Mojil, Andrew R. Barron. Superhydrophilic Functionalization of Microfiltration Ceramic Membranes Enables Separation of Hydrocarbons from Frac and Produced Water. Scientific Reports, 2017; 7 (1) DOI: 10.1038/s41598-017-12499-w']	['<urn:uuid:3c633cab-399e-4304-83e4-2c22b2ef3542>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T05:06:54.527160	16	82	659
85	What makes Blackberry keyboards special to use in projects?	Blackberry keyboards are known for their excellent tactile feel under the thumb. They are a great alternative to soldering numerous SMD tact switches or using complex snap domes. Specifically, the Blackberry Q10 keyboard can be found for around $3 from Chinese sources, making it a cost-effective option. These keyboards can be integrated into projects using a breakout board that exposes the key matrix and includes features like per-row backlight controls and a MEMS mic.	['In 2017 Spotify finally deprecated their public vanilla C SDK library, libspotify, and officially replaced it with dedicated SDKs for iOS and Android and this new-fangled web thing we’ve all heard so much about. This is probably great for their maintainability but makes writing a native application for a Linux or a hardware device significantly harder, at least without an application process and NDA. Or is it? Instead of using that boring slab of glass and metal in their pocket [Dani] wanted to build a handy “now playing” display and remote control interface but was constrained by the aforementioned SDK limitations. So they came up with a series of clever optimizations resulting in the clearly-named ESP8266 Spotify Remote Control.\nThe Spotify Remote Control has a color LCD with a touchscreen. Once attached to a Spotify account it will show the album art of the currently playing track (with a loading indicator!) and let you play/pause/skip tracks from its touch screen, all with impressively low latency. To get here [Dani] faced two major challenges: authorizing the ESP to interact with a user’s Spotify account, and low latency LCD drawing.\nIf you’re not on iOS or Android, the Spotify web API is the remaining non-NDA’d interface available. But it’s really designed to be used on relatively rich platforms such as fully featured web browsers, not an embedded device. To that end, gone are the days of asking a user to enter their username and password in a static login box, the newer (better) way is to negotiate for a per-user token (which is individually authorized per application), then to use that to authenticate your interaction. With this regime 3rd party applications (in this case an ESP8266) never see a user’s password. One codified and very common version of this process is called OAuth and the token dance is called a “workflow”. [Dani] has a pretty good writeup of the process in their post if you want more detail about the theory. After banging out the web requests and exception handling (user declines to authorize the device, etc) the final magic ended up being using mDNS to get the user’s browser to redirect itself to the ESP’s local web server without looking up an IP first. So the setup process is this: the ESP boots and displays a URL to go to, the user navigates there on a WiFi connected device and operates the authorization workflow, then tokens are exchanged and the Remote Control is authorized.\nThe second problem was smooth drawing. By the ESP’s standards the album art for a given track at full color depth is pretty storage-large, meaning slow transfers to the display and large memory requirements. [Dani] used a few tricks here. The first was to try 2 bit color depth which turned out atrociously (see image above). Eventually the solution became to decompress and draw the album art directly to the screen (instead of a frame buffer) only when the track changed, then redraw the transport controls quickly with 2 bit color. The final problem was that network transfers were also slow, requiring manual timesharing between the download code and the display drawing routing to ensure everything was redrawn frequently.\nCheck out [Dani]’s video after the break, and take a peek at the sources to try building a Spotify Remote Control yourself.\nHere at Hackaday we’re big fans of device-reuse, and what [arturo182] has done with the Blackberry Q10’s keyboard is a fantastic example. Sometimes you’re working on a portable device and think to yourself “what this could really use is a QWERTY keyboard”. What project doesn’t need a keyboard?\nTypically this descends into a cost benefit analysis of the horrors of soldering 60ish SMD tact switches to a board, which is no fun. With more resources you can use Snaptron snap domes like the [NextThingCo’s] PocketCHIP, but those are complex to source for a one off project and the key feel can be hard to really perfect. Instead of choosing one of those routes, [arturo182] reverse engineered the keyboard from a Blackberry Q10.\nWhen you think of good, small keyboards, there has always been one standout: Blackberry. For decades Blackberry has been known for absolutely nailing the sweet tactile feel of a tiny key under your thumb. The Q10 is one example, originally becoming avalible in 2013 as one of the launch devices for their then-new Blackberry OS 10. Like most of Blackberry’s business the OS and the phone are long out of date, but that doesn’t mean the keyboard has aged.\n[Arturo182] says he can find them from the usual Chinese sources for around $3 each, which is too cheap to not explore. Building on the work of [WooDWorkeR] (on Hackaday.io) and [JoeN] to reverse engineer the matrix and to find the correct connector, he integrated the keyboard into an easy to use breakout board that exposes the key matrix, per-row backlight controls, and even the MEMS mic! More excitingly, he has built a small portable device with all the trappings of the original Q10; a color LCD, joystick, function buttons, and more in a very small footprint.\nA serial backpack is really nothing more than a screen and some microcontroller glue to drive it. And a hammer is nothing more than a hardened weight on the end of a stick. But when you’re presented with a nail, or a device that outputs serial diagnostic data, there’s nothing like having the right tool on hand.\n[ogdento] built his own serial backpack using parts on hand and a port of some great old code. Cutting up a Nokia 1100 graphic display and pulling a PIC out of the parts drawer got him the hardware that he needed, and he found a good start for his code in [Peter Andersen]’s plain-old character LCD library, combined with a Nokia 1100 graphic LCD library by [spiralbrain]. [ogdento] added control for the backlight, mashed the two softwares together, and voilà!\nA simple screen with a serial port is a great device to have on hand, and it makes a great project. We’ve seen them around here before, of course. And while you could just order one online, why not make your own? Who knows what kind of crazy customizations you might dream up along the way.\nIf you have ever wondered what it took to make your own custom graphic LCD from scratch, this video from [Applied Science] is worth a watch. It’s concise and to the point, while still telling you what you need to know should you be interested in rolling your own. There is also a related video which goes into much more detail about experimenting with LCD technology.\n[Applied Science] used microscope slides and parts purchased online to make an LCD that displays a custom graphic when activated. The only step that home experimenters might have trouble following is coating the glass slides with a clear conductive layer, which in the video is done via a process called sputtering to deposit a thin film. You don’t need to do this yourself, though. Pre-coated glass is readily available online. (Search for Indium-Tin Oxide or ‘ITO’ coated glass.)\nThe LCD consists of a layer of liquid crystal suspended between two layers of conductive glass. An electrical field is used to change the orientation of crystals in the suspension, which modulate the light passing through them. Polarizing filters result in a sharp contrast and therefore a visible image. To show a particular shape, some of the conductive coating is removed from one of the layers in the shape of the desired image. The process [Applied Science] uses to do this is nearly identical to etching a custom PCB. Continue reading “How To Make A Custom LCD From Scratch”→\nA reflow oven is one of the most useful tools you will ever have, and if you haven’t built one yet, now is as good a time as any. [0xPIT’s] Arduino based reflow oven controller with a graphic LCD is one of the nicest reflow controllers we’ve seen.\nHaving a reflow oven opens up a world of possibilities. All of those impossible to solder surface mount devices are now easier than ever. Built around the Arduino Pro Micro and an Adafruit TFT color LCD, this project is very straight forward. You can either make your own controller PCB, or use [0xPIT’s] design. His design is built around two solid state relays, one for the heating elements and one for the convection fan. “The software uses PID control of the heater and fan output for improved temperature stability.” The project write-up is also on github, so be sure to scroll down and take a look at the README.\nAll you need to do is build any of the laser cutters and pick and place machines that we have featured over the years, and you too can have a complete surface mount assembly line!\n[Debraj] wrote in about his 2-wire serial backpack he developed for a Graphic LCD screen. It’s build on a hunk of protoboard and uses a pair of 595 shift registers to translate incoming serial data to the parallel interface which is used by the LCD screen. It takes more time to push commands this way, but the interface is still quite snappy as you can see in the clip after the jump.\nThe real trick here is how the hardware has been configured to get away without a third wire for latching the shift registers (if you need a primer on 595 chips check out this feature). The idea of using a latch is that all of the data can be shifted in over the serial pin before it appears on the output pins. Otherwise, the GLCD would see each bit as it shifts into the register, wreaking havoc on its communication protocol. [Debraj] gets around this by using a diode AND gate trick he learned from this other serial LCD project.\nOne good thing about this method is the 595 chips have a wide range of control voltage that will allow you to drive this with 3.3V or 5V microcontrollers. But you do need to implement the communication protocol and push those commands via serial. For nearly the same cost in chips something like an ATtiny2313 could be substituted to make an even simpler addressing scheme — or even switch to 1-wire protocol. But you’d then lose the wide input voltage tolerance.\nThe hardware setup is straight-forward. The screen has a 20-pin connector and operates at 5V. We don’t see it on his protoboard, but usually these displays also need a potentiometer which serves as a voltage divider for the screen contrast. The data and control pins eat up most of the available I/O on the ATmega328 chip he used, but the I2C and SPI pins are still open and he plans a future project to make this a wireless display for his PC using one of those protocols.\nAs for fonts and animation, [Tom] links to several tools which will come in handy. There’s a font program that will convert Windows system fonts into a C file for use on the Arduino. The animations start with a 1:1 ratio animated graphic drawn with his favorite image editing software. He then converts those to monochrome bmp files and used bmp2c to convert each frame to a C array. After the break there’s a seven second example that would work well as a boot screen for his project.']	['<urn:uuid:926118ab-e8d2-4873-9d88-534f53ba7d48>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T05:06:54.527160	9	74	1908
86	What are the build quality characteristics of the Laowa lens and greenhouse farming systems?	The Laowa 9mm features robust metal construction with large focusing and aperture rings, though it lacks weather sealing and has a somewhat inadequate lens hood. It accepts front filters and maintains a compact size when mounted. As for greenhouse farming systems, they utilize various robust setups like the Nutrient Film Technique with plastic pipes, water reservoirs and pumps, or the deep water culture system using non-transparent storage containers and air pumps. Commercial installations like Lufa Farms implement engineered greenhouse systems on 31,000-square-foot office buildings, capable of year-round production even in cold temperatures.	['If you attach the new Venus Optics Laowa 9mm f2.8 Compact Dreamer to a Sony a6500 camera, then you’ll see that while it’s a weird lens it’s also a lens that Sony really needs for their system. It’s billed as a lens with really low distortion–and it indeed doesn’t have a whole lot of it except around the corners. Designed for APS-C sensors, I also feel like this lens isn’t necessarily a big winner for Venus Optics. Many of their lenses have this beautiful character to them, but this one feels flat in many ways and that means you’re going to surely rely more on what the sensor is capable of doing. That’s fine, but I’ve never been so lukewarm about a lens from Venus Optics.\nEditor’s Note: Venus Optics hasn’t officially announced this lens yet. But due to their leak and my previous testing of the lens, they’re allowing me to publish my review.\nPros and Cons\n- Decent image quality\n- Keeps the distortion down a whole lot\n- With a lens like this you’re either focusing up close and personal or far away. There is no in between.\n- The colors are just adequate. There’s no spark!\n- Distortion around the corners is a bit annoying.\nThe Venus Optics Laowa 9mm f2.8 Compact Dreamer was tested with the Sony a6500.\nSiteLong Japan Co., Ltd. announces the new product “LAOWA 9mm F2.8 Zero-D” of LAOWA (Venus Optics).\n- Product name LAOWA 9mm F2.8 Zero-D\n- Suggested retail price Open price\n- Expected market price TBD\n- Scheduled release date around early April 2018\nMain Product Features\n- This product is the best lens for photographing landscape photographs, architectural photographs, interior photographs, star photographs and so on.\n- Use 3 special low dispersion lens (ED lens) and 2 glass aspheric lenses luxuriously. By adopting an optical design with extremely reduced distortion to zero, it is possible to capture landscape photographs and architectural photographs without the influence of distortion.\n- It is the widest angle lens (113 °) lens among the wide-angle lenses for APS-C cameras of the open F 2.8 which are on sale now.*\n- It adopts metal enclosure with excellent robustness and luxury. Effective flower type hood is attached.\n- This product is the brightest, the most compact and lightweight lens among the same class products. Because it is very convenient to carry, it is also ideal for daily snapshot.\n- Focal length: 9mm\n- Maximum F value: F2.8\n- Viewing angle: 113 degrees (corresponding format: APS-C)\n- Lens construction: 10 groups of 15\n- Number of diaphragm blades: 7\n- Shortest shooting distance: 12 cm\n- Maximum magnification: 0.13 times\n- Filter diameter: φ 49 mm\n- Weight: ~ 215g* Variable depending on mount\n- Corresponding mount: FUJIFILM X, Canon EF-M, Sony E\n*Product specifications are subject to change without notice due to improvements etc.\nThe Venus Optics Laowa 9mm f2.8 Compact Dreamer lens is an overall very simple optic. There’s a lens good and a front area. But unlike many other wide angle lenses, this one allows you to attach a filter on the front. Weird, but very convenient for sure.\nThe only controls on the Venus Optics Laowa 9mm f2.8 Compact Dreamer are the aperture ring towards the back and the front focusing ring. That’s all that there is to it.\nThe Venus Optics Laowa 9mm f2.8 Compact Dreamer has an all metal body, which is really nice. The focusing ring and the aperture ring are all large enough for most needs. The overall body is quite small and keeps the package pretty compact when attached to your camera. The lens lacks weather resistance though, so be careful with it. What Venus Optics could surely add to help would be a better lens hood. This one is okay, but not ideal I feel. With that said, some of the images in this review were shot in the rainfall and the camera and lens were just fine.\nEase of Use\nThis is a manual focus lens, so you’re basically just focusing back and forth manually and stopping the lens down accordingly. Set your camera to aperture priority and you’ll be all set. But even further, the lens focuses either really up close or really far away and there is no in between. So if you’re shooting subjects that are maybe a few feet away from you, expect everything to be in focus. It’s sort of odd but it works out.\nThe Venus Optics Laowa 9mm f2.8 Compact Dreamer is a manual focus lens. So you’ll be, wait for it, manually focusing. If you hate manual focus, good. You can essentially set the lens to infinity and forget about it after stopping down a bit.\nThe image quality from the new Venus Optics Laowa 9mm f2.8 Compact Dreamer lens is overall decently sharp enough. The bokeh is alright. The chromatic aberration is kept down quite enough. And the color is acceptable. But that’s part of what worries me here. Venus Optics lenses have always had character to them. This one has none. It’s sterile, it’s plain. I expect this from Canon or Nikon.\nThe Venus Optics Laowa 9mm f2.8 Compact Dreamer lens is a very wide angle lens. Don’t expect a whole lot of bokeh unless you’re focusing up close and personal and even then, don’t expect it to be the prettiest bokeh. Venus Optics has lenses with much nicer and smoother bokeh overall.\nIn my tests, I couldn’t really find any sort of major image quality problems with exception to distortion in the corners. But even then, it’s kept down quite a bit for a lens like this. For that, Venus Optics should be given quite a bit of praise.\nWhen you attach the Venus Optics Laowa 9mm f2.8 Compact Dreamer lens to a Sony camera, you’re best off really tweaking the colors. It’s as flat as flat can be and that’s because it’s probably aimed at landscape photographers. I’d have loved more vibrant colors, but alas I didn’t get them.\nSharpness with the Venus Optics Laowa 9mm f2.8 Compact Dreamer lens is very good overall and all around. At this point, I have come to expect this much with their lenses. The best spot for me is at f5.6 but f8 still yields nice results. While this lens is sharp, I still think that Venus Optics has sharper lenses still.\nExtra Image Samples\n- Low distortion\n- Lack of character, they engineered it out of their lenses\nThe Venus Optics Laowa 9mm f2.8 Compact Dreamer is a nice lens. But it’s missing that special mojo that I’ve come to love (and buy) about Venus Optics. I feel like this is a very clinically perfect lens and that’s perhaps what they were going for. But what I really want is for Venus Optics to bring their character back to their lenses. The low distortion is nice as is the sharpness. But that charm? It’s lacking. I mean, think about it like a sexy car without any sort of horsepower or in fully automatic mode. That’s sort of what this lens is like.\nThe Venus Optics Laowa 9mm f2.8 Compact Dreamer receives four out of five stars from us.', 'Fresh water may soon become a costly commodity. The fundamental social problem of feeding society is growing larger due to a rising scarcity of water and an ongoing depletion of agricultural land. Population growth, climate changes, pollution, and agricultural water waste contribute to growing fresh water shortages around the world. Depletion of soil nutrients through poor farming techniques, floods, poor irrigation, and winds have seriously damaged agricultural land.\nApproximately 40% of the world’s agricultural ground is unsuitable for farming. The role of agricultural business desperately needs to align with the evolving ethos of a rapidly growing society. Can hydroponic farming provide a sustainable solution to environmental problems caused by traditional farming methods? What practical applications does hydroponics have in densely populated urban areas? Farmers describe soil degradation as thinning and unproductive land that leads to low yielding crops.\nLand degradation includes nutrient depletion, loss of biodiversity, climate change, erosion by water, erosion by wind, reduced vegetative cover, pollution, drought, compaction by animals or machinery, sedimentation, increased soil temperatures, reduced organic matter, and salinization (Stockings, 2000 pg 5). According to the United Nation’s food and agriculture program, 854 million people do not have sufficient food for an active and healthy life (Sample, 2007). The population has increased by nearly 2 billion in the last 20 years and food production increased by 50%.\nIt is estimated that by 2050 the population will reach 9 billion. (Sample, 2007). State and federal officials have drafted accords in attempts to rectify water shortages. UN officials have gathered to create and execute a plan of action to improve conservation of soil and restoration of degraded land, but such plans are merely band-aids on a broken limb. The ultimate form of soil conservation is eliminating the use of soil in agriculture completely. Hydroponic agriculture offers a permanent solution to this rapidly growing problem of water shortages, pollution, and land degradation.\nDevelopment of hydroponic systems took place from 1925 through 1935. Experimentation with soil-less nutrient solutions and advancement in agricultural plastic by Professor Emery Myers Emmert at the University of Kentucky sparked an interest in hydroponic food production. Efforts were primarily aimed at large scale commercial food growth, but Hydroponic food systems were eventually abandoned due to high construction and operation costs (University of Arizona). Modern hydroponic systems are relatively inexpensive to build and offer several effective methods of food production.\nThe most popular commercial agriculture hydroponic system is the Nutrient Film Technique. NFT consist of a plastic pipe or gutter, a water reservoir, and a water pump. A thin film of nutrient infused water constantly flows through the tubing. Plant roots are suspended over the piping with only the roots touching the stream of nutrient water. The pipe or gutter is slightly elevated at the far end to allow water to drain back into the water reservoir. The NFT system delivers high levels of oxygen to the roots promoting vigorous plant growth.\nNFT systems are ideal for leafy greens such as lettuce, cabbage and basil but are effective for a multitude of fruits, herbs, and vegetables. A minimal operating cost makes this system ideal for commercial applications. [pic] [pic] A popular system for larger plants is the ebb and flow or flood and drain system. This table system consists of a plastic tray, water pump, timer, reservoir, and tubing. The plants lay on the top table separate from the nutrient reservoir. The pump is programmed to turn on in 15 minute increments with variations specific to the type of plant and stage of development.\nWhen activated, the pump will fill the table at top with the nutrient infused water from the reservoir below. Once it is turned off the water will drain back into the reservoir below. [pic] One of the simplest hydroponic methods is the deep water culture system. Often used in commercial applications, this system can be built with materials found at a local hardware store or discount retail market. The deep water culture system can be built with a non-transparent storage tote, an aquarium air pump, and an air stone commonly used in fish tanks.\nPlant roots are permanently submerged in nutrient rich water. This system does not require a water pump or timer. Urbanization and technological developments extended the distance produce traveled to reach consumers. Heilbroner explains the changed by describing that “industrial technology has literally refashioned the human environment, bringing with it all the gains-and all the terrible problems-of city life on a mass scale” (Heilbroner, Milberg pg 83 ). Hydroponics is the tool to address the basic social problem of feeding society by making produce readily accessible and grown in urbanized areas.\nAlthough traditional agriculture has served its purpose for many years, the ethos of rapidly growing demands sustainable and healthy forms of food production. Hydroponic farming in conjunction with greenhouses can help solve land degradation and water pollution problems through out the world. Lufa Farms, a Canadian farm based in Monteral, engineered a series of greenhouses on top of a 31,000-square foot office building. Greenhouse growing allows the Canadian farm to supply fresh produce all year round, even in 45 degree temperatures.\nIt is estimated that Lufa Farms can deliver more than 1,000 baskets of produce per week, three times more than land-based competitors. ( Business and the Environment). Greenhouse hydroponic farming not only solves the problem of land degradation and a water shortage due to traditional farming practices but it also alleviates the pressure of fossil fuel consumption and vehicle pollution. Because sustainable farming systems can be established in city rooftops and barges, transporting fruits and vegetables from remote farms is no longer necessary.\nProduce grown in the city have shorter distances to travel reducing transportation expenses and fuel consumption. Consumers can purchase a fresher product at their local market. According to Columbia University professor of public health and microbiology Dickson D. Despommier, a 30-story, one square block farm could produce the same amount of produce as a 2,400 outdoor soil based farm ( Business and the Environment ). The Science Barge, a greenhouse and hydroponic system on top of a barge based in Yonkers, New York, is a fully functioning urban farm prototype for a fully sustainable food production sytem.\nThe Science Barge produces tomatoes, cucumbers, and lettuce without carbon emissions, chemical pesticides, or waste water runoff (New York Sun Works, 2010-2011). The barge uses solar and wind power to sustain greenhouses on top of the barge. Purified river water and rain water is used to irrigate the crops. Science Barge grown produce use seven times less land and four times less water than traditionally grown crops. (New York Sun Works, 2010-2011). The economic advantage of hydroponic farming can be identified through a cost-benefit analysis.\nConsidering that in 2000, 41% of all freshwater used in the United States was for Agricultural purposes (Agricultural Resources and Environmental Indicators, 2006 Edition); reduction in water cost is one of the main economic advantages of hydroponic farming. BrightFarms is a company that has taken this emerging technology and developed a business model of “Better food, better prices, better environment. ” BrightFarms designs, finances, builds, and operates hydroponic greenhouse farms on supermarket rooftops.\nBrightFarms pledge is “to deliver produce at equal or higher service levels than the retailer currently requires of its other suppliers. There is no cost to the retailer to build the BrightFarm, only an obligation to purchase the output. ” The company intends to increase profits by reducing shrink due to longer produce shelf life. They have calculated that this practice will produce higher gross margins for the retailer. The company implements a long-term price fix contract to protect the retailer from unstable prices, rising cost costs and inconstant supply. BrightFarms 2012). BrightFarms feels that “with the elimination of shipping, and the drastic reduction of fuel consumption, carbon emissions and water use, BrightFarms enables grocery retailers to change their produce supply chain in a way that improves the planet and their bottom line. ” (BrightFarms 2012). According to Brian H. Kurbjeweit, professor of Contemporary Business at the University of Redlands, economic progress is an interconnected system comprised of science, economics, law, and ethics all influenced by the ethos of a society.\nThe ethos of modern society has expressed an urgent need for sustainable business practices. Science has engineered the tools to effectively create a new form of responsible farming. Materials to build hydroponic systems are readily available at a relatively low cost. Through social media, instructional videos and written instructions to build and operate hydroponic systems are readily accessible world wide through urban farming forums free of cost on the Internet. There is a basic economic need to feed and sustain life.\nThe basic economic problem creates a market for those willing to invest in large-scale urban farming businesses. Responsible agriculture is an ethical guideline that all farmers in global and local economies should follow to ensure the basic economic problem is addressed. Continued land degradation, water pollution, and pesticide smothered produce combined with a rapidly growing world population will lead to further human suffering. It is an ethical duty of world leaders to implement sustainable forms of agriculture to nurture its citizens. Law, the missing factor is yet to be addressed.\nIt may be the ethos of future generations that persuades governments to execute laws to speed up the implementation of sustainable farming techniques or it may be severe ethical violations by non-complying agricultural corporations that spark the creation of laws to effectively protect citizens from illness caused by pesticides and water pollution. It may simply be an epidemic interest in well being or as author Malcom Gladwell describes, a tipping point may be reached where local co-ops solve the basic economic problem, one small community at a time, using sustainable forms of agriculture.\nStockings, M., Niamh, M. (2000). Land Degradation. Guidelines for Field Assesment, 5, 59-67.\nHeilbriner, R., & Milberg W. (2008) The Making of Economic Society (12th Ed.). Saddle River, NJ: Pearson Education, Inc.\nSample, I (2007). Global food crisis looms as climate change and population growth strip fertile Land. The Guardian.\nRetrieved from http://www.guardian.co.uk/environment/2007/aug/31/climatechange.food. Buying Local Takes on New Meaning. (2011). Business & the Environment, 22(9), 1-4. University of Arizona, Growing Tomatoes Hydroponically Retrieved from http://ag.arizona.edu/hydroponictomatoes/overview New York Sun Works, Center For Sustainable Engineering. (2011). Retrieved from http://nysunworks.org/thesciencebarge\nBrightFarms. (2012) Better Food, Fresher Food. Retrieved from http://brightfarms.com/about/']	['<urn:uuid:bf7b7940-5fb0-4410-b772-612f6d2786f0>', '<urn:uuid:f48335fe-9324-48ac-9ffa-30fc843b9511>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T05:06:54.527160	14	92	2924
87	How do CT scans work differently from regular X-rays and what are the health implications?	CT scans take X-ray readings through the body at multiple angles (180 rotations with 160 points each) to create detailed cross-sectional images, making them about 100 times more sensitive than conventional X-rays for showing soft tissue differences. However, while this provides superior diagnostic capabilities, the radiation exposure carries health risks. Unlike single X-rays, CT scans deliver higher cumulative radiation doses. Research has shown that these exposures can increase cancer risk, particularly in vulnerable populations like pregnant women and young people, with effects that can be passed on to future generations through epigenetic changes.	"[""Computerized Transverse Axial Scanning (Tomography)\n|Computerized Transverse Axial Scanning (Tomography).|\n“The Nobel Prize in Physiology or Medicine was shared in 1979 by a physicist, Allan Cormack, and an engineer, Godfrey Hounsfield…. Hounsfield, working independently [of Cormack], built the first clinical [computed tomography] machine, which was installed in 1971. It was described in 1973 in the British Journal of Radiology. The Nobel Prize acceptance speeches (Cormack 1980; Hounsfield 1980) are interesting to read. A neurologist, William Oldendorf, had been working independently on the problem but did not share in the Nobel Prize…”\nOddly, Russ and I did not include Hounsfield’s 1973 paper in our list of references. I decided to dig it up and have a look. The reference and abstract is:\nHounsfield GN (1973) Computerized transverse axial scanning (tomography): Part I. Description of the system. Br J Radiol 46:1016-1022This article describes a technique in which X-ray transmission readings are taken through the head at a multitude of angles: from these data, absorption values of the material contained within the head are calculated on a computer and presented as a series of pictures of slices of the cranium. The system is approximately 100 times more sensitive than conventional X-ray systems to such an extent that variations in soft tissues of nearly similar density can be displayed.\nA dozen comments:\n- This is Hounsfield’s most highly cited paper, with 4667 citations according to Google Scholar. That’s a respectable number (ten times more than any of my papers have), yet seems curiously small for a Nobel Prize-winning advance.\n- Hounsfield’s paper is the first of a trilogy. Hounsfield is not a coauthor on the other two; they report clinical studies using the new technique.\n- Hounsfield lists his institution as “Central Research Laboratories of EMI Limited”. EMI is famous in the music industry; it is the recording label responsible for the early hits of the Beatles.\n- Hounsfield’s paper has only three references: two to his own preliminary reports and one to an article by Oldendorf. He didn’t cite Cormack’s papers.\n- Hounsfield sounds most impressed not by recreating three-dimensional images from two-dimensional projections (which to me is the big advance) but instead by the increased sensitivity of the technique to small differences in x-ray absorption coefficient.\n- Figure 3, illustrating the scanning device and sequence, is similar to Fig. 16.25 in IPMB.\nFig. 16.25 of IPMB.\n- Hounsfield measured 160 points in each translation and performed 180 rotations. Each two-dimensional image was represented by an 80 × 80 grid of pixels.\n- The reconstruction method was different from the two Russ and I analyze in Chapter 12 of IPMB: i) Fourier Transform Reconstruction and ii) Filtered Back-Projection. Instead, Hounsfield just fit his data using the least squares method (see Section 11.1 of IPMB). Hounsfield writes “Each beam path [in the CT scan], therefore, forms one of a series of 28,800 simultaneous equations, in which there are 6,400 variables and, providing that there are more equations than variables, the values of each [pixel] …. can be solved.”\n- The Hounsfield unit was introduced in Fig. 9, but he did not, of course, call it that. Interestingly, his definition is different than what is used today. Equation 16.25 in IPMB gives the Hounsfield unit as\nwhere μtissue and μwater are x-ray attenuation coefficients. In his paper, Hounsfield defines the unit the same way, except he replaces 1000 by 500.\n- The article describes preliminary experiments using an iodine-containing contrast agent and digital subtraction, analogous to Fig. 16.23 in IPMB.\n- The computer equipment pictured in Hounsfield’s paper look big and clunky today. I can only guess what paltry computer power he had available for these first reconstructions.\n- I love the British Journal of Radiology, known as BJR. [What journal did you think that Bradley John Roth would like?]\nI’ll conclude with Hounsfield’s final paragraph. To my ear, it sounds like classic British understatement.\nIt is possible that this technique may open up a new chapter in X-ray diagnosis. Previously, various tissues could only be distinguished from one another if they differed appreciably in density. In this procedure absolute values of the absorption coefficient of the tissues are obtained. The increased sensitivity of computerized X-ray section scanning thus enables tissues of similar density to be separated and a picture of the soft tissue structure within the cranium to be built up.\nAPeX - Far superior to colloidal silver in destroying viruses, bacteria and other pathogens.\nUltimate Curcumin - Natural pain relief, reduce inflammation and so much more.\nSupreme Fulvic - Nature's most important supplement! Vivid Dreams again! See Testimonials\nMitoCopper - First bioavailable copper destroys pathogens and gives you more energy.\nProdovite - The Secret To Healing is in the Blood! Complete absorption in 5 minutes."", 'This is an expanded version of an article I wrote a couple of days ago.\nWhen scientists speak of radiation, they speak not only of single doses but also of cumulative doses.\nSee for example, this research from the University of Iowa showing that “cumulative radon exposure is a significant risk factor for lung cancer in women”.\nAnd see these studies on the health effects cumulative doses of radioactive cesium. (As I noted on March 29th, the radioactive cesium fallout from Japan already rivals Chernobyl. And the amount of radioactive fuel at Fukushima dwarfs Chernobyl).\nAdmittedly, the damage from huge single doses may be greater than the same cumulative dose from many small exposures. But the smaller doses can still add up.\nRemember, the radiation from CT scans and x-rays are external emitters – the radiation emanates from outside the body. In contract, internal emitters keep emitting their radiation inside the body. Therefore, the cumulative effect of multiple small doses of radiation from internal emitters could be even more dramatic, depending on the half life, metabolic pathways and other properties of the particular radioactive particle.\nAs the European Committee on Radiation Risk notes:\nCumulative impacts of chronic irradiation in low doses are … important for the comprehension, assessment and prognosis of the late effects of irradiation on human beings …\nAnd see this.\nOne of the World’s Leading Experts on Radiation – Karl Morgan – Warned of Cumulative Low-Dose Exposures\nAmerican reporter Dahr Jamail reports today for Al Jazeera:\n“The U.S. Department of Energy has testified that there is no level of radiation that is so low that it is without health risks,” Jacqueline Cabasso, the Executive Director of the Western States Legal Foundation, told Al Jazeera.\nHer foundation monitors and analyzes U.S. nuclear weapons programs and policies and related high technology energy, with a focus on the national nuclear weapons laboratories.\nCabasso explained that natural background radiation exists, “But more than 2,000 nuclear tests have enhanced this background radiation level, so we are already living in an artificially radiated environment due to all the nuclear tests.”\n“Karl Morgan, who worked on the Manhattan project, later came out against the nuclear industry when he understood the danger of low levels of ionizing radiation-and he said there is no safe dose of radiation exposure,” Cabasso continued, “That means all this talk about what a worker or the public can withstand on a yearly basis is bogus. There is no safe level of radiation exposure. These so-called safe levels are coming from within the nuclear establishment.”\nKarl Morgan was an American physicist who was a founder of the field of radiation health physics. After a long career in the Manhattan Project and at the Oak Ridge National Laboratory, he became a critic of nuclear power and weapons. Morgan, who died in 1999, began to offer court testimony for people who said they had been harmed by the nuclear power industry.\n“Nobody is talking about the fact that there is no safe dose of radiation,” Cabasso added, “One of the reasons Morgan said this is because doses are cumulative in the body.”\nAs the Guardian wrote in Morgan’s obituary in 1999:\nKarl Morgan … was a pioneer of health physics – the science of the effect of exposure to radiation on health. He was a member of the research group which laid the foundations for the Manhattan project and produced the first atomic bomb. However, after 30 years in the inner cabinet of the nuclear establishment, Morgan changed sides and testified in key radiation cases on behalf of those who claimed they had been harmed by nuclear weapons and the nuclear power industry.\nThe first signs of his change of view came in 1968, when he became an influential campaigner in obtaining a US law that required the medical profession to control excessive doses of radiation during X-rays.\nAfter retirement in 1972 he became more active in drawing attention to the limitations of radiation protection measures.\nHe began his career as a physics professor but in 1943 was recruited to become a senior scientist in health physics to the top secret, atomic bomb project codenamed Manhattan Engineer District.\nThe following year, Morgan went to the newly-formed Oak Ridge national laboratory in Tennessee, where he became director of health physics from 1944 until his retirement. When told he would be in the health physics group, he was shocked and said it was a terrible mistake because he had never heard of health physics. The leaders of the research project said they had been in the same position. But they realised that since their attempts to build the first atomic pile, now known as a reactor, would create a source of intense radiation, they needed to understand how to protect people.\nHe wrote in his autobiography, The Angry Genie: One Man’s Walk Through The Nuclear Age, that he did not believe they had ever determined that it was safe. “We determined what we considered was acceptable.”\nThe Townsend Letter for Doctors & Patients wrote in 2002:\nOne of the original five ‘health physicists’ to set radiation safety standards was Karl Z. Morgan. Dr. Morgan served on the International Commission on Radiological Protection (ICRP), which set up most radiation standards. He also directed the Health Physics Division at Oak Ridge from 1944 until his retirement in 1972. In recent years, Dr. Morgan has publicly criticized the ICRP for failing to protect human health. In a 1994 article for the American Journal of Industrial Medicine, Dr. Morgan wrote: “The period of atmospheric testing of nuclear weapons by the United States, the United Kingdom, France and the USSR is a sad page in the history of civilized man. Without question, it was the cause of hundreds of thousands of cancer deaths. Yet there was complete silence on the part of the ICRP. During these years (1960-1965), most members of the ICRP either worked directly with the nuclear weapons industry or indirectly received most of their funding for their research from this industry.”\nThe ICRP’s alliance with the nuclear industry includes ties to the International Congress of Radiology. In his 1999 autobiography, The Angry Genie: One Man’s Walk Through the Nuclear Age (ISBN 0-8061-3122-5), Dr. Morgan related his concern about the ICRP’s refusal to address the danger of excessive X-ray exposure during diagnostic procedures and dentistry. Until the passage of the Radiation Control for Health and Safety Act of 1968, some X-ray equipment used in the 1950s and 1960s delivered 2 to 3 rem per X-ray. X-ray doses as low as 1.6 rem increase a woman’s chance of developing cancer, according to a 1974 study by Baruch Modan [Lancet (Feb. 23,1974), pp 277-279]. The Act did not address the cumulative effect of multiple, routine, and often unnecessary X-rays.\nOther Top Radiation Experts Agree\nBy any measure, Dr. John Gofman was one of the greatest scientists of the 20th century. Gofman earned his doctorate in nuclear and physical chemistry, and was also a medical doctor. He worked on the Manhattan Project, co-discovered uranium-232 and -233 and proved their fissionability, helped discover how to extract plutonium and led the team that discovered and characterized lipoproteins in the causation of heart disease.\nDr. Arthur R. Tamplin was a doctor of biophysics, who was tasked – as a group leader in the Biomedical Division at Lawrence Livermore National Laboratories – with predicting the ultimate distribution within the biosphere and in humans of each radionuclide produced in the explosion of a nuclear device.\nIn 1963 the Atomic Energy Commission asked Gofman and Tamplin to undertake a series of long range studies on potential dangers that might arise from the “peaceful uses of the atom.” They told the truth, and the AEC launched a campaign of harassment in response.\nWhat did they say, and why was the AEC so hostile?\nGofman and Tamplin documented that low levels of radiation can cause cancer and other diseases, and they argued that federal safety guidelines for low-level exposures should be reduced by 90 percent.\nErnest Sternglass (Emeritus Professor of Radiological Physics in the Department of Radiology, at the University of Pittsburgh School of Medicine), Dr. Alice Stewart (head of the Department of Preventive Medicine of Oxford University) and many other top scientists have also shown that low level radiation can cause cancer.\nMilitary Commanders Told that Low-Level Radiation Increases the Risk of Cancer\nA military briefing written by the U.S. Army for commanders in Iraq states:\nHazards from low level radiation are long-term, not acute effects… Every exposure increases risk of cancer.\n(Military briefings for commanders often contain less propaganda than literature aimed at civilians, as the commanders have to know the basic facts to be able to assess risk to their soldiers.)\nThe briefing states that doses are cumulative, citing the following military studies and reports:\n- ACE Directive 80-63, ACE Policy for Defensive Measures against Low Level Radiological Hazards during Military Operations, 2 AUG 96\n- AR 11-9, The Army Radiation Program, 28 MAY 99\n- FM 4-02.283, Treatment of Nuclear and Radiological Casualties, 20 DEC 01\n- JP 3-11, Joint Doctrine for Operations in NBC Environments, 11 JUL 00\n- NATO STANAG 2473, Command Guidance on Low Level Radiation Exposure in Military Operations, 3 MAY 00\n- USACHPPM TG 244, The NBC Battle Book, AUG 02\nInfants Are Most at Risk\nInfants are, apparently, most vulnerable to low level radiation exposure.\nAs Brian Moench, MD, notes:\nAdministration spokespeople continuously claim “no threat” from the radiation reaching the US from Japan, just as they did with oil hemorrhaging into the Gulf. Perhaps we should all whistle “Don’t worry, be happy” in unison. A thorough review of the science, however, begs a second opinion.\nThat the radiation is being released 5,000 miles away isn’t as comforting as it seems…. Every day, the jet stream carries pollution from Asian smoke stacks and dust from the Gobi Desert to our West Coast, contributing 10 to 60 percent of the total pollution breathed by Californians, depending on the time of year. Mercury is probably the second most toxic substance known after plutonium. Half the mercury in the atmosphere over the entire US originates in China. It, too, is 5,000 miles away. A week after a nuclear weapons test in China, iodine 131 could be detected in the thyroid glands of deer in Colorado, although it could not be detected in the air or in nearby vegetation.\nThe idea that a threshold exists or there is a safe level of radiation for human exposure began unraveling in the 1950s when research showed one pelvic x-ray in a pregnant woman could double the rate of childhood leukemia in an exposed baby. Furthermore, the risk was ten times higher if it occurred in the first three months of pregnancy than near the end. This became the stepping-stone to the understanding that the timing of exposure was even more critical than the dose. The earlier in embryonic development it occurred, the greater the risk.\nA new medical concept has emerged, increasingly supported by the latest research, called “fetal origins of disease,” that centers on the evidence that a multitude of chronic diseases, including cancer, often have their origins in the first few weeks after conception by environmental insults disturbing normal embryonic development. It is now established medical advice that pregnant women should avoid any exposure to x-rays, medicines or chemicals when not absolutely necessary, no matter how small the dose, especially in the first three months.\n“Epigenetics” is a term integral to fetal origins of disease, referring to chemical attachments to genes that turn them on or off inappropriately and have impacts functionally similar to broken genetic bonds. Epigenetic changes can be caused by unimaginably small doses – parts per trillion – be it chemicals, air pollution, cigarette smoke or radiation. Furthermore, these epigenetic changes can occur within minutes after exposure and may be passed on to subsequent generations.\nThe Endocrine Society, 14,000 researchers and medical specialists in more than 100 countries, warned that “even infinitesimally low levels of exposure to endocrine-disrupting chemicals, indeed, any level of exposure at all, may cause endocrine or reproductive abnormalities, particularly if exposure occurs during a critical developmental window. Surprisingly, low doses may even exert more potent effects than higher doses.” If hormone-mimicking chemicals at any level are not safe for a fetus, then the concept is likely to be equally true of the even more intensely toxic radioactive elements drifting over from Japan, some of which may also act as endocrine disruptors.\nMany epidemiologic studies show that extremely low doses of radiation increase the incidence of childhood cancers, low birth-weight babies, premature births, infant mortality, birth defects and even diminished intelligence. Just two abdominal x-rays delivered to a male can slightly increase the chance of his future children developing leukemia. By damaging proteins anywhere in a living cell, radiation can accelerate the aging process and diminish the function of any organ. Cells can repair themselves, but the rapidly growing cells in a fetus may divide before repair can occur, negating the body’s defense mechanism and replicating the damage.\nComforting statements about the safety of low radiation are not even accurate for adults. Small increases in risk per individual have immense consequences in the aggregate. When low risk is accepted for billions of people, there will still be millions of victims. New research on risks of x-rays illustrate the point.\nRadiation from CT coronary scans is considered low, but, statistically, it causes cancer in one of every 270 40-year-old women who receive the scan. Twenty year olds will have double that rate. Annually, 29,000 cancers are caused by the 70 million CT scans done in the US. Common, low-dose dental x-rays more than double the rate of thyroid cancer. Those exposed to repeated dental x-rays have an even higher risk of thyroid cancer.\nBeginning with Madam Curie, the story of nuclear power is one where key players have consistently miscalculated or misrepresented the risks of radiation. The victims include many of those who worked on the original Manhattan Project, the 200,000 soldiers who were assigned to eye witness our nuclear tests, the residents of the Western US who absorbed the lion’s share of fallout from our nuclear testing in Nevada, the thousands of forgotten victims of Three Mile Island or the likely hundreds of thousands of casualties of Chernobyl. This could be the latest chapter in that long and tragic story when, once again, we were told not to worry.']"	['<urn:uuid:db8ae22d-0dad-455b-8180-1535177f7258>', '<urn:uuid:032d2c2a-4c0f-4f8e-862d-fff6e55857f4>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T05:06:54.527160	15	93	3204
88	What are substitution rules and harmonic analysis methods?	For substitution, lowering any note in a diminished chord by one fret results in a seventh chord, creating four possible seventh chord shapes. For harmonic analysis, Roman numerals are used to denote chord-key relationships, with clear marking of key centers and chord types. Analysis must differentiate between major and minor keys, include chord extensions, and identify non-diatonic chords to determine new key centers.	['UD#36 Chameleon Chord: Diminished Chords Part II\n(The coolest thing you can do with dim7 chords)\nfrom Ukulele in the Dark w/ Guido Heistek\nThe coolest thing you can do with diminished chords…in my opinion…\nNOTE: For simplicity I will refer to diminished 7th chords as DIMINISHED CHORDS in this lesson.\nALSO NOTE: If you don’t feel like reading there is a video at the bottom of the page.\nTry this experiment\nTake a diminished chord like this one:\nLower one of the notes in the chord by one fret. Let’s lower the note on the G string. What does this mean? We take the note on the G string and lower it by one fret (move it one fret towards the headstock of the uke) which makes it into an open G note. Here’s what happens:\nYou’ll notice that you now have a seventh chord. A G7. So what? Read on…\nDo the same with the note on the C string. Lower it by one fret.\nYou may recognize the shape on the right as being a seventh chord. It is a C#7 chord. What’s this all about? Read on…\nLower the note on the E string one fret. Now you have an E7 chord! Take a look.\nLower the note on the A string by one fret. Now you have a B-flat seven chord.\nHey, I think we’re on to something!\nIf you lower any of the notes in a diminished chord by one fret (one semi-tone) you get a seventh chord.\nSo if we lower any note in this chord by one fret:\nWe get one of these 4 seventh chord shapes.\n(Notice that these are the 4 Seventh Chord shapes that we studied in our lesson on the G7 chord.)\nVery interesting how do we apply this new knowledge…\nSubstituting Diminished Chords for Seventh Chords:\nHere is a simple chord progression that’s very popular in jazz:\nJazzy Progression: OPEN VERSION\nPlay the progression through a few times with two beats on each chord.\nHere’s the same progression using inversions of the chords found up the neck of ukulele. A challenge for those who are interested.\nJazzy Progression: UP THE NECK.\nWe can create a very cool effect by replacing the seventh chords in this progression with diminished shapes which most closely resemble the shapes of the seventh chords. Here is the open version of the progression with the seventh chords swapped out for diminished chords:\nJazzy Progression: OPEN VERSION w/ Diminished Chords instead of 7th chords.\nCool Eh! This creates a very cool angular effect. Here is the up the neck version.\nJazzy Progression: UP THE NECK VERSION w/ Diminished Chords instead of 7th chords.\nThis diminished chord substitution is very useful for creating interest in chord melodies and solo ukulele arrangements.\nA WARNING: Be a little careful when you’re playing with other people. If everyone else is playing a seventh chord and you are playing a diminished chord substitution there will be some clashing because one of the notes in the chord won’t match. So in ensembles if you choose to replace one of the seventh chords with a diminished chord it has to be decided that everybody’s going to do that.\nI often use this type of diminished chord effect when I’m playing introductions on my own or if I’m doing a solo arrangement. Use your ears to tell you if the diminished substitution is a good idea or not. Experiment. Have fun.\nONE LAST THING: As you might remember from last lesson, diminished chords repeat every three frets. You can use this to “stitch together” progressions at different parts of the uke. Please watch the video to see how this is done. Here’s the video. Enjoy!\nHope you enjoyed this weeks newsletter. I’ll be sending out another newsletter in two weeks, the subject of which is yet to be determined. All the best in your playing.\np.s. I am regularly adding new lessons to Ukulele in the Dark. Please subscribe below to receive the lessons by e-mail. Be sure not to miss a thing…\nTags: diminished chords', 'Analysis of Jazz Harmony\nHarmonic analysis of tunes is extremely important to the understanding of jazz theory and hence to the ability to improvise. Roman numerals are used to denote the relationship between the chord and the key, hereafter referred to as RN analysis. The degree of the scale upon which each chord is built (root note of chord) is shown as a roman numeral (ex 1b).\nJazz tunes often modulate temporarily and it is necessary to show these modulations as key centres. Each key centre must be clearly shown before the sequence of chords. (ex 1e: key centres of G and F). In most cases a new key centre is made obvious by a V7-I cadence. (in this case the Gm7/C7-F – chords which are diatonic to the key of F, not G).\nChords are often chromatically altered in jazz so when using the roman numeral system to analysea chord sequence always remember to add the type and extension of the chord (e.g. m7, maj7, etc). NB: this is different to classical analysis\nex 9a RN Analysis of the 1st 8 bars of Laura (Mercer/Raskin)\nTips for RN analysis:\n- Differentiate clearly between major and minor keys. (e.g. C = C major, Cm = C minor. Do not use just lower case for minor keys, c is easily confused with C)\n- Always include the type of chord and extension.\n- Make a note of chords that are not diatonic to the starting key to identify new key centres.\n- Look for a tonic chord (maj7 or maj6 in a major key, min maj7, min6 or minor triad in a minor key) preceded by IIm7-V7 to define a key centre.\n- Minor 7 chords are very often II chords in a major key. (This is more likely than them being III or VI chords)\n- Minor 7 b5 (ø7) chords are often II chords in a minor key.\n- Mark the key centre clearly – circle it or use a different colour.\n- Bracket IIm7 – V7 together (to highlight the II-V relationship) and draw an arrow from the V chord to the target chord (if it resolves down a 5th) to denote resolution.\nIn addition to IIm7-V7-Is created through secondary dominants, many tunes are made up of IIm7-V7-Is with different key centres that may appear to be entirely random or may be related logically:\n- Misty: key centres in bridge move down a semitone then a major 3rd\n- Autumn Leaves: key centres Bb – Gm (major to relative minor)\n- Giant Steps: (key centres move up in major 3rds)\nOverlapping key centres\nAs well as the alternative key centres we mentioned earlier (see chapter 2 and ex 5d), it is possible for key centres to overlap. This happens where one or more consecutive chords could be in one of two key centres. In bar 5 of Autumn Leaves the Eb ma7 could either be chord IV of Bb major or chord (b)VI of the next key centre, G minor (based on the harmonic minor or the Aeolian mode). This gives us two possible ways to analyse the first eight bars (ex 9b and 9c below)\nex 9b: The Gm key centre is shown at bar 4.\nex 9c: The Gm key centre is shown on bar 5.\nThe Ebmaj7 is diatonic to both key centres. Although the analysis in ex 9b could be seen as correct, there are various reasons why 9c is better:\n- We have not yet established the G minor via a cadence (eg. D7 Gm), so our ears tell us that the Eb is still in the key centre of Bb.\n- Chord II(ø7) is an integral part of a IIm7- V7- I cadence.\n- The key centres make up two four bar phrases if the G minor key centre starts at bar 5.']	['<urn:uuid:bc332b98-e9aa-48df-9477-dea466fc4f59>', '<urn:uuid:b9f1f3d6-fdce-4134-980d-c7df969d149e>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:06:54.527160	8	63	1325
89	What's required for drinking fountain GFCI protection?	All electric drinking fountains must be GFCI-protected. If the receptacle is difficult to access, you can use a GFCI breaker for protection.	"['Q. Do I need a grounding conductor for a pole light, or is the ground rod good enough?\nA. You absolutely need an equipment grounding conductor. Metal parts of electrical raceways, cables, enclosures, or equipment must be bonded together (and to the supply system) in a manner that creates a low-impedance path for ground-fault current that facilitates the operation of the circuit overcurrent device [250.4(A)(5)].\nBecause the earth is not suitable to serve as the required effective ground-fault current path, an equipment grounding conductor is required to be run with all circuits.\nDanger: Because the contact resistance of an electrode to the earth is so high, very little fault current returns to the power supply if the earth is the only fault current return path. What’s the result? The circuit overcurrent device will not open, and all metal parts associated with the electrical installation, metal piping, and structural building steel will become and remain energized.\nQ. We are told that we must GFCI protect our drinking fountain, but the receptacle is buried behind the unit. Do we really have to do this?\nA. All electric drinking fountains must be GFCI-protected [422.52], but if you are really concerned about accessibility, you can always use a GFCI breaker for protection.\nQ. What are the NEC requirements when replacing 15A or 20A, 125V receptacles?\nA. See 406.3(D) as follows: Grounding-type receptacles. Where an equipment grounding conductor exists, grounding-type receptacles must replace nongrounding-type receptacles, and the receptacle\'s grounding terminal must be connected to an equipment grounding conductor in accordance with 406.3(C) or 250.130(C).\nNongrounding-type receptacles. Where no equipment grounding conductor exists in the outlet box for the receptacle, such as old 2-wire Type NM cable without an equipment grounding conductor, existing nongrounding-type receptacles can be replaced in accordance with one of the following:\n- Another nongrounding-type receptacle.\n- A GFCI-type receptacle marked ""No Equip¬ment Ground.""\n- A grounding-type receptacle, if GFCI protected and marked ""GFCI Protected"" and ""No Equipment Ground.""\nWhen existing receptacles are replaced in locations where GFCI protection is currently required, the replacement receptacles must be GFCI protected. This includes the replacement of receptacles in dwelling unit bathrooms, garages, outdoors, crawl spaces, unfinished basements, kitchen countertops, rooftops, or within 6 ft of laundry, utility, and wet bar sinks. See 210.8 for specific GFCI-protection requirements.\nAuthor\'s comment: GFCI protection functions properly on a 2-wire circuit without an equipment grounding conductor because the circuit equipment grounding conductor serves no role in the operation of the GFCI-protection device. See the definition of ""ground-fault circuit interrupter"" for more information.\nQ. A conduit contains six current-carrying 10 AWG THHN conductors on a roof for a photovoltaic system, where the ambient temperature 94ºF and the ambient temperature add is 60ºF in accordance with Table 310.15(B)(2)(c). How do I determine the conductor ampacity for this condition of use?\nA. When conductors are installed in an ambient temperature other than 78°F to 86°F, ampacities listed in Table 310.16 must be corrected in accordance with the multipliers listed in Table 310.16. Where the number of current-carrying conductors in a raceway or cable exceeds three, the allowable ampacity of each conductor, as listed in Table 310.16, must be adjusted in accordance with the adjustment factors contained in Table 310.15(B)(2)(a) and 310.15(B)(2)(c). When both of these conditions exist, you must do both calculations, but you can start with the 90°C ampacity for the calculations [110.14(C)(1)].\nTable 310.16 ampacity if 10 AWG THHN is 40A\nAmbient temperature correction [Table 310.16] = 0.58, based on 154ºF\nConductor bundle adjustment [310.15(B)(2)(a)] = 0.80, six current-carrying conductors\nAdjusted ampacity = 40A x .58 x 0.80\nAdjusted ampacity = 18.56 or 19A\nQ. We got written up for having our underground conduits too close together. I know you can\'t bundle cables together, but I didn\'t think this applied to raceways too. Does it?\nA. Actually, it does. In 310.15(B)(2)(b), it states that spacing between conduits, tubing, or raceways must be maintained, even though the Code doesn\'t say what the spacing is — or how to adjust when the spacing is not maintained. Hopefully, the next Code cycle will have better language for this rule.']"	['<urn:uuid:8d37ef0a-1fed-4445-8a3b-2dff894b7ef1>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T05:06:54.527160	7	22	686
90	tv shows music discovery replace radio gen z	According to a study by New York University Steinhart Music Business Program, terrestrial radio has failed to connect with Generation Z (people born after 1995). Instead of using radio for music discovery, consumers aged 13 and up now use digital services like YouTube and Spotify to find new artists. Television shows have become an important outlet for new artists to gain mass attention to their music, effectively taking over radio's former role.	['Last summer, a study made by New York University Steinhart Music Business Program reported that terrestrial radio has failed to connect with Generation Z — people born after 1995 — who have grown up in an on-demand digital environment. Consumers who are 13 and up no longer look to radio as a music discovery tool and instead take to digital services like YouTube and Spotify to find new artists. Digital streaming platforms have allowed new artists access to a wide range of consumers, but the downside is that it can be hard to distinguish their music from the thousands of songs released every day. This is where television comes in.\nThe beauty about shows like Atlanta and Insecure, outside of the compelling writing and storytelling, is the music featured on the show. The music isn’t just a placeholder but rather a conduit; a way to magnify a feeling or emotion tied to a situation the viewer is watching. Usually, the artists featured are burgeoning upstarts who aren’t well known. Because of this, a lot of the audience is listening to these artists for the first time. Taking over where radio left off, television has become an important outlet for new artists to gain attention to their music on a mass scale. The emphasis of new artists’ music is also encouraged because clearing songs for use from bigger or illustrious artists are extremely difficult.\n“I went from averaging like 5,000 views a day on YouTube to 15,000 a day or more.” — Rico Nasty\nLook at Rico Nasty, a 22-year-old rapper whose life changed after her single, “Poppin,” was featured on Insecure, season 2, episode 3 “Hella Open.” This episode concluded with “Poppin” playing while the credits came on with the viewers’ full focus on the record. It was an exclamation mark following the last scene of the episode where Issa Rae’s character triumphantly took control of her sex life. The exposure made Nasty’s song a viral hit.\n“I went from averaging like 5,000 views a day on YouTube to 15,000 a day or more,” Rico Nasty explains. “The video [for “Poppin”] went crazy. Television is kind of like their version of a music video and that’s why I think it’s more effective. Because when someone watches a show and it’s a song playing in the background, when it really hits you, that’s when you look shit up. Like, ‘Wow, I can really relate to this moment, to this song, to this scene’ and It really helps the person get a full idea of a song.”\nThe coverage also added to her fan base while building on the song’s message. Rico continued, “Insecure gave me a whole another aspect of my song. It actually put the perspective on males. Instead of you being a poppin’ ass bitch against women, [Issa Rae] placed it as ‘I’m a poppin’ ass bitch and I don’t need niggas.’ That really helps reach that other side of thinking and I really fuck with it. I hope I get way more shit on TV because TV is lit. So many fans come to my show like, ‘Oh my God I know you from Insecure.’”\nMusic in television has always been a component but it wasn’t a focal point of a show. One of the first series to emphasize music’s importance was NY Undercover in the mid to late 1990’s with James Mtume serving as music supervisor. In the series — which follows two cops trying to balance their personal life with their work — the biggest stars in urban music perform at Natalie’s, a fictional bar and restaurant owned by its namesake Natalie, who was played by Gladys Knight. The artists at the time, such as 112, SWV, Aaliyah, performed their latest singles, or exclusive covers.\n“The reason why Insecure and Atlanta have strong point of views in music is because the people who are writing the show are writing the show with the intentions of using music as a character.” — Scott Vener\n‘While NY Undercover was ahead of its time in regards to music integration, the foundation of today’s music curation in television was set by Entourage (2004 – 2011) and How to Make It In America (2010 -2011). What was played on the show developed the storyline, an aspect that Insecure and Atlanta thrives with now. Scott Vener, former Music Supervisor of Entourage and How to Make It In America, and current Music Supervisor of HBO’s Ballers points out that people behind the music of Insecure and Atlanta are genuine music lovers. Music for Insecure is handpicked by a team led by Issa Rae, music supervisor Kier Lehman, director Melina Matsoukas, and show composer Raphael Saadiq, among others with Solange also providing consulting. Atlanta’s music is picked by Donald Glover, Jen Malone and Fam Udeorji, who serve as music supervisor and consultant, and a few key figures.\n“The reason why Insecure and Atlanta have strong point of views in music is because the people who are writing the show are writing the show with the intentions of using music as a character,” Vener explains. “It’s like when a montage happens, and the visuals are telling a story without dialogue, and allowing the music to take you into a mood and help support the story. They let the music breath on both the shows.”\nWhen selecting music for shows, Vener explains why he focuses on new artists, outside of the Billboard charts. He elaborates that it wouldn’t make sense to play a song on the show for the sole reason that it’s popular. “For me personally, to go and recycle whatever is on the charts and to put it on my shows, doesn’t do anything for me personally as a brand,” he continues. “If I’m going to have this opportunity, I want to put new artists on and I like the idea of a being first responders to an artist’s music career then someone who just responding to what is already in pop culture or being talked about. I play those songs when it makes sense or if its funny or a cool idea.”\n“TV shows have a tendency to hit a new audience that you may not be reaching already, so it’s really important to think about out-of-the-box ways to get a new fan a day with whatever music you’re releasing.” — Kei Henderson\nFor new musicians, catching the attention of Music Supervisors like Vener can lead to major success. You’re a new artist who hasn’t been making music, ala Rico Nasty, for more than a few years. You’re generating local buzz and have sporadic posting of your projects and songs on major music websites. However, a glass ceiling still perches itself right above your trajectory. How do you break through? Television is a great alternative to the radio because it also hits a national audience from different demographics.\n“TV shows have a tendency to hit a new audience that you may not be reaching already, so it’s really important to think about out-of-the-box ways to get a new fan a day with whatever music you’re releasing,” explains Kei Henderson, CEO of Sincethe80s, an Atlanta-based Label, management and publishing company.\nThere isn’t a singular process to getting music on television. Half of the time it’s probably either luck or a coincidence. Music supervisor or showrunners still have to have some knowledge of the artist or hear the song. Managers and labels are bringing their music to shows like Insecure and Atlanta as a part of their rollout plan. Before SZA released her critically-acclaimed Ctrl album, she shared unreleased music to Insecure for the show. “Shows like Insecure are taking a chance on not just procedural music but different music to carry your piece,” Raphael Saadiq shares, who scores Insecure. “It’s about the feeling, the vibe that is sending over the screen into people’s home. It’s more of a nostalgic, feeling thing that’s new for television,” he adds.\n“The thing about television and a song being on TV or a movie that radio can’t do for you is, it can create an emotional memory for you.” — Scott Vener\nWhile radio’s influence in the music industry is far from fading, streaming, TV, and film, are opening new lanes for artists to become a household name overnight. TV allows music that wouldn’t work on the radio to become a hit record.\n“The thing about television and a song being on TV or a movie that radio can’t do for you is, it can create an emotional memory for you,” explains Vener. “Insecure is a great example of this. They used a lot of indie R&B songs that if you just put on the radio, if you don’t have your mind in the right place when you’re listening to that song, it may not connect with you the same way than if you’re watching the relationship that is happening on the show with the song. When you have that connection, it makes the song that much more meaningful and you can understand where the song is coming from because it provides you with a perspective and context to understand the song. That’s what TV and movies can do that radio can’t. But radio is more of a persistent beast that plays the song over and over again and TV can’t do that for artists.”\nIn an endless sea of music from an infinite, ever-growing bushel of artists, television has become a viable platform for new artists to have their music distinctly enter households on a grand scale. This is just a tip of the iceberg.']	['<urn:uuid:85040d8b-64d1-4f6b-a96f-4853e64dc4e6>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T05:06:54.527160	8	72	1598
91	how do public health researchers and university students study immigration differently	Public health researchers and university students approach immigration studies from different but complementary perspectives. Public health researchers focus on examining how policies impact immigrant health outcomes, analyzing issues like healthcare access, enforcement impacts, and discrimination effects. They work with advocacy groups to translate research into policy recommendations and push for inclusive reforms. University students, on the other hand, often study immigration through direct personal experiences and classroom engagement - at Berkeley, where 60% of undergraduates have at least one immigrant parent, students examine migration patterns, citizenship concepts, and integration challenges through courses that combine academic theory with their own lived experiences. Students also participate in research projects, like conducting interviews with immigrant families, allowing them to contribute to academic understanding while learning firsthand about immigration issues.	['How public health research can shape inclusive immigration policies\nPresident Joseph Biden and the Democrat-controlled Congress have started boldly with immigration. On Feb. 18, Democrats introduced the U.S. Citizenship Act of 2021, which would rollback many of Donald Trump’s policies and bring comprehensive immigration reform, including a pathway to citizenship for the estimated 10.5 million undocumented immigrants in the United States. Researchers and community advocates are recognizing a renewed opportunity to use public health research and advocacy lenses to inform the dialogue — and ultimately the policies — surrounding immigration reform.\nIn this Q&A, Steven Wallace, associate director at the UCLA Center for Health Policy Research and a professor of community health sciences at the Fielding School of Public Health, and his colleague Maria-Elena Young, a faculty associate at the center and assistant professor of public health at UC Merced, shared their recommendations to promote inclusive policies influenced by the public health mission to ensure the health and well-being of all people.\nWallace’s interest in immigration comes from listening to his grandfather’s stories about coming to the U.S. through Ellis Island, and his father’s memories of growing up in the immigrant neighborhood of Boyle Heights. An undergraduate summer internship in 1977 at a community health center where most of the patients were recent immigrants from Mexico sparked his interest in a more academic study of immigration, and since the mid-1980s he has published regularly on the subject.\nFor Young, immigrant inclusion has been central to her personal and professional experiences. Her mother’s family came to the U.S. from Nicaragua in 1950 and faced many challenges, but also economic and educational opportunity. As a youth in California, she observed the fear and division that resulted from the passage of Proposition 187. She became specifically focused on issues of immigration policy about 10 years ago when rules enacted under the Obama administration sharply increased deportations and she realized how immigration policy is a form of health policy.\nThey recently co-authored an editorial in the American Journal of Public Health, which outlines priorities for the new administration in taking an equity-focused approach to reform, as well as making the call to action for researchers and advocates to work together with the goal of improving immigrant health and policies.\nWhat is the relationship between immigration and health, and why is it crucial in the current talks about immigration reform?\nWallace: “Immigrant” is a political status given to those who move their primary residence from Vancouver to Seattle, or Tijuana to San Diego, but not to those who move from Boston to L.A. Depending on how people move their permanent residence they are subject to different sets of social and political pressures and resources that will impact their health. For example, according to an analysis of the California Health Interview Survey, an estimated 90% of undocumented immigrant adults ages 19-64 did not have comprehensive health insurance because they are in families with low-wage workers who are not provided insurance at work and they do not qualify for public programs. Even documented immigrants who live in families that include undocumented members — called mixed-status families — often report not obtaining benefits or seeking care they are eligible for because of their concerns about putting their undocumented family members at risk. And the chronic stress these families live under has negative health outcomes.\nYoung: The relationship between immigration and health is critical to current policy proposals because immigrants’ day-to-day experiences are shaped by immigration policies. Today, immigrants are less likely to have a place to receive health care — and the insurance to pay for that care — and more likely to work in low-wage, and often less safe jobs, than those born in the U.S. Across the U.S., they also report experiencing xenophobia and discrimination due to race, religion, citizenship and other factors.\nWe know that having access to health care, economic opportunities, educational advancement and living free of racism and discrimination are key determinants of good health. Federal, state and even some local policies have an enormous impact on whether immigrants are able to access those resources and protections and the extent to which they may be targeted by enforcement. Unfortunately, over time policymakers in the U.S. have largely created a system that excludes many noncitizens from critical resources and actively authorized their policing and surveillance. The public health evidence shows that this policy approach has been bad for the well-being of immigrants and people of color across the nation. There’s an opportunity to start to change that now.\nThe Biden administration has proposed changes to immigration policies which involve creating a task force to reunify families, addressing migration patterns, and restoring faith in the legal immigration system. What do inclusive immigration policies look like to public health researchers and advocates, and how can the current administration take inclusion into account?\nWallace: State and local policies on education, health care, employment, drivers’ licenses and law enforcement impact immigrants’ everyday lives, but federal policy sets the tone that can hinder, or help, these policies. For example, last year the Trump administration expanded the number of reasons an immigrant could be denied a green card, because they’d be considered a “public charge,” someone primarily dependent on government assistance. The additional reasons included services to provide health care, food and housing. Both the new policy and public discussions about this have deterred many immigrants from using public programs they’re legally qualified for.\nIn that way, a draconian federal policy to exclude people overshadowed many inclusive state and local policies, such as those to provide prenatal care to undocumented women whose permanent home is in the U.S., food assistance to lawful permanent residents, and other programs designed to help immigrants who are an integral part of our communities and economy.\nOur research can inform federal policy makers on the benefits of not only reversing some of the punitive actions of the previous administration, but by establishing new national initiatives to help immigrants and refugees become fully incorporated into the society. Our work has shown that inclusive policies at the state level improve immigrant outcomes, independent of exclusionary policies that may also exist. An example of this is a study we conducted in 2018, which found that inclusive state immigrant policies may reduce immigrant poverty by fostering economic advancement.\nYoung: As we describe in our current piece, key priorities that can advance equity are to decriminalize immigrants by dismantling and eliminating our enforcement infrastructure and creating a path to citizenship for all that is not contingent on increased enforcement against some groups. Within the public health field, we also should work to eliminate categories of “good,” or deserving, versus “bad,” or undeserving immigrants.\nBiden’s initial proposals are an encouraging start. But new policies shouldn’t simply be made in response to Trump’s anti-immigrant agenda. For example, Biden has ended the construction of a wall at the U.S.-Mexico border, but indicated there would be investment in a “virtual wall” which likely will increase surveillance of all people living at the border, whether citizens or not. Similarly, he is reviewing enforcement priorities, but even a shift in priorities does not address how enforcement arrests, detentions and deportations have led to stress, instability and discrimination in communities across the country. Public health researchers and advocates are ideally positioned to push the administration to make inclusion a central criterion for changes to the immigration system.\nHow can researchers and advocates work together to ensure that the pathway to immigration reform is fair and equitable?\nWallace: Where evidence is limited or dated, it invites ideological and harmful, or at best, only partially effective, responses to issues. Researchers need to work with affected communities to identify where solid, empirical data can help inform policies and efforts to improve policies. A few examples of where additional data might be useful include debunking the myths that health care programs are a “magnet” that attract immigrants and that some immigrants are more “deserving” of public services than others.\nYoung: It’s critical for researchers to work with advocacy groups, such as the California Immigrant Policy Center and the National Immigration Law Center, to support their current advocacy efforts. Public health researchers have established really critical evidence about the harms of restrictive immigration policies and the potential benefits of inclusive policies. That evidence should be translated into effective advocacy information, such as through better dissemination of the academic research to the advocates who are meeting directly with policymakers or through researchers’ direct involvement in talking to policymakers.', 'Faces of Immigration Studies at Cal\nSociologist Irene Bloemraad’s research on the contemporary immigrant experience has deeply personal roots. Born in Spain to Dutch parents, Bloemraad moved to Canada with her family as a young girl. She studied political science and sociology at McGill University in Montreal before earning her doctorate in sociology at Harvard.\nBut as a social scientist, her research on citizenship and national identity is a direct response to her daily work environment.\nSome 60 percent of Berkeley undergraduates have at least one immigrant parent, according to a 2006 survey. These Cal students offer remarkable evidence that education is one of the primary tools for socioeconomic mobility among recent immigrant generations, and a glimpse at the rich diversity of California’s future leaders.\nImmigration studies at Berkeley is growing rapidly. Courses examining the intersections of migration, race, citizenship, and politics have grown in recent years along with the changing demographics of the student population.\nA visit to one of the courses Bloemraad taught this fall adds faces and names to this evolution. Her sociology course “Contemporary Immigration in Global Perspective” leads students to examine why people migrate, how immigrants incorporate into their new homes, and debate future forms of membership and citizenship in a globalizing world.\n“This is the class that I have been preparing for over the last eight years, the class I’m most passionate about teaching,” Bloemraad says. “Most of the students who took this course are exactly the immigrants and second generation I am interested in.”\nDuring class, the group of 40 discusses how immigrants fare under different political systems and it breaks down which specific elements of a society seem to offer immigrants the best chance of success. One important factor is school. Educational institutions help children acquire language and academic skills. But they can build civic skills, too, for both children and parents, helping them to come out of the isolation of immigrant life.\nWhen this does not happen, and immigrants remain cut off from one another, there is a lost opportunity to develop meaningful civic engagement. Political economy major JungHee Jadon Won can attest to this. Won is the son of Korean immigrants. A self-described “1.5” immigrant — born outside of the U.S. but having largely grown up in this country — Won says he was struck as a youth by how little different ethnic groups mingled.\n“I have considered the U.S. a country of immigrants, but I did not see any attempt of assimilation, especially in La Crescenta, where I went to high school,” he says. “Korean students stayed within their circle, and Armenians only bonded together, and so forth. Even though they were living in next door, they failed to acknowledge the existence of their fellow citizens of other races. I wanted to learn why this might occur, and if I can do something about it.”\nBloemraad’s research found that immigrant students with access to clubs at school — an anti-discrimination club, La Raza club or even a French club —find support and resources.\n“It can be the seeds of activism,” she says. In 2006, she notes, a group of students at Richmond High School, including immigrant students, organized and sought support to file a lawsuit claiming that California’s high school exit exam discriminates against poor and minority students.\n“A lot of organizing was going on in the student clubs,” she says. “With the mentorship of an interested teacher, school clubs are often the first time students realize that if they get together, there are things they can do to address the issues they all face.”\nBloemraad recently completed a study involving interviews with some 200 immigrant parents and their 14-18 year old children throughout the Bay Area. She and colleague Christine Trost, with UC Berkeley’s Institute for the Study of Societal Issues, were looking for answers to several questions, including whether the children of immigrants teach their parents about the American political system and whether immigrant youth withdraw from or compensate for their parents’ exclusion from the civic and political mainstream.\nIn conducting the research, Bloemraad and Trost met with sixteen graduate and undergraduate student interviewers every two or three weeks to discuss emerging themes. The project became a training and teaching opportunity, beyond collecting new research data.\nWith a working title of Learning to Participate: Political Socialization in Mixed Status Immigrant Families, findings from the study are still being analyzed. It appears, however, based on interviews with Mexican, Vietnamese and Chinese high school students, that having immigrant parents does not mean that a young person will be end up disconnected from civic life.\n“Everything is stacked against these kids,” Bloemraad says, “yet even the children of undocumented immigrants become active in their schools and communities. In some cases, they are even more involved.”\nAt its core, this collaborative, student-assisted research poses questions about what it means to be a full member of the United States. Both parents and the youth interviewed expressed a sense that being “American” meant being white, affluent, living in safe neighborhoods and having rights. This, they felt, excluded them, but many felt that they could be, and were, good U.S. citizens. Importantly, the guarantee of birthright citizenship — which all the youth benefited from — blurred the boundaries between being American and being a citizen.\n“There is a sense of marginalization, but especially for the kids born here, the 14th Amendment gives them a sense that they are a part of these debates.”']	['<urn:uuid:5cab4d2d-c7f6-4886-a313-83ba431e6a18>', '<urn:uuid:eb911f49-90bd-4a55-bbe4-d983b2076989>']	open-ended	direct	long-search-query	similar-to-document	comparison	novice	2025-05-13T05:06:54.527160	11	126	2324
92	christmas island endemic species extinction threats causes	The endemic species on Christmas Island face multiple threats leading to extinction. The Forest Skink, Blue-tailed Skink, and Lister's Gecko are either extinct in the wild or nearly extinct. The causes remain unresolved but include predation by invasive Yellow Crazy Ants, which have formed supercolonies on the island and are known to secrete formic acid to subdue prey. Other potential causes include predation by Giant Centipedes or Wolf Snakes, competition with non-native reptile species, poisoning by insecticides used to control crazy ants, and disease.	['Gump has lived a cossetted life, nurtured in a cage on Christmas Island. Until last year, she had two acquaintances, but misadventure claimed them both.\nNow there is only Gump. She’s the last known individual Forest Skink on Earth; as close to extinction as a species can possibly get, and her relative longevity is staving off, very temporarily, the final obliteration of a species that has probably lived for tens, if not hundreds, of thousands of years.\nHers is a salutary tale. Over the course of our lifetime, Forest Skinks have gone from abundant to absent across the rainforests of the 135 km2 Christmas Island. Eminent herpetologist Hal Cogger remembers witnessing (as recently as 1998) more than 80 individual Forest Skinks basking and foraging around a single fallen tree; until recent decades they were common and widespread across the island.\nThen began a rapid and apparently inexorable decline. By 2003, they were confined to scattered pockets in remote parts of the island. By 2008, a targeted survey found them at only one remaining site. Now, recent repeated searches and trapping have failed: the species appears to have disappeared completely from its natural haunts.\nThe Emoia skinks to which the Forest Skink belongs are a large group (of more than 70 species), with marked radiation on islands in the Pacific. Most, including the Forest Skink, are moderately large, thickset, active during the day and ground-dwelling. The Forest Skink is relatively nondescript: largely unpatterned, chocolate-brown, about 20 cm long (of which about two-thirds is tail). A recent genetic analysis has concluded that the Forest Skink is the most ancestral of the genus.\nThe decline and imminent extinction of the Forest Skink has outpaced its recognition as a threatened species. Indeed, notwithstanding its looming extinction, it is not yet listed as threatened under the Environment Protection and Biodiversity Conservation Act, although it is likely to be so soon. This is as much an illustration of the shortcomings in the official listing process as it is of the speed of the skink’s decline.\nThe Forest Skink shares its plight and pattern of recent decline with most of the small set of native reptiles on Christmas Island. The Coastal Skink (Emoia atrocostata) has almost certainly declined to extinction on Christmas Island (but is still considered to persist elsewhere), the endemic Blue-tailed Skink (Cryptoblepharus egeriae) is probably now extinct in the wild (but more happily than for the Forest Skink, a functional captive breeding population has been established), the endemic Lister’s Gecko (Lepidodactylus listeri) is very nearly extinct in the wild (but has a captive breeding population), and the status of the Christmas Island Blind Snake is unknown (with only a single individual reported over the last two decades). Oddly, the other endemic species, the Christmas Island Giant Gecko (Cyrtodactylus sadleiri), has remained somewhat common, but has still suffered a significant - though unmeasured - decline.\nOne of the intriguing but frustrating issues in this case is that the cause or causes of the decline remain unresolved. This applies to Christmas Island’s other native reptiles, and perhaps also for the better known and remarkably parallel case of the Christmas Island Pipistrelle.\nThere are plausible suspects, most notably predation by the non-native Yellow Crazy Ant, Giant Centipede or Wolf Snake; competition with the five non-native reptile species; poisoning by insecticides used to try to control the crazy ants; and disease. There is no conclusive evidence for any of these factors, and it is now too late to decipher the clues and manage the threats for this species.\nBut, it is not yet entirely too late for the two other highly threatened Christmas Island lizard species that are held in captivity, and ongoing research with them may allow an informed post-mortem conclusion about the extinction of the Forest Skink.\nSadly, there is little future for the species. Its sole living representative will most likely die within the next couple of years. Over the last two years, there have been some dedicated searches for additional individuals, but these have proven fruitless. In the years immediately preceding, there were a few sightings and near misses of capture; tantalising but unrealised opportunities. In those failures, the survival of the species may have slipped through our fingers.\nJust possibly, we are too pessimistic. It may be that there remain a few secretive Forest Skinks that have not yet succumbed to whatever nemesis it is that haunts their lives. There may yet be time, albeit very limited time, for the optimistic to invest in a more comprehensive search that could yet find a mate for Gump, and buy more time to forestall the extinction.\nAn advisory group for Christmas Island’s reptile species has been established, and captive breeding (on Christmas Island and at Taronga Zoo) has provided hope for two of the other threatened reptile species.\nFailure is a good teacher. This is an extinction that ought not to have happened. There are clear lessons:\nDeclines can be very rapid and, without quick response, rapid declines are likely to lead to extinction.\nThe current process for listing threatened species is inadequate, though a listing process that fails to produce resources to identify and ameliorate the causes of the listing has limited value.\nEarly intervention, such as through captive breeding, may provide an irreplaceable opportunity for conservation.\nIsland biotas may be extremely susceptible to extinction and they need protection with adequate biosecurity measures.\nIt may be damnably difficult to identify critical factors that drive declines and extinctions.\nIndividually, Gump is an undistinguished and inconsequential lizard, but she has now become a talisman, marking the finality and proximity of extinction. Extinction is a chain, and the last link of the chain is the death of the last individual.\nThe Conversation is running a series on Australian endangered species. See it here.', 'From remote islands to our own backyards, invasive species threaten native plants and wildlife. These are 10 of the most unwanted and threatening invasive species throughout the world.\n1. Yellow Crazy Ants\nYellow Crazy Ants, believed to be native to West Africa, have been dispersed by human transportation systems to remote islands around the world, where their unique adaptations put native species at risk. The Yellow Crazy Ant does not bite or sting, but instead secretes formic acid to subdue its prey. On Christmas Island in the Indian Ocean, Yellow Crazy Ants have created supercolonies and decimated the native Red Land Crab population. Researchers are concerned that endangered species such as the Abbott’s Booby (Sula abbotti) could be at risk of extinction on the island if the Crazy Ant population persists.\n2. Brown Tree Snakes\nThe Brown Tree Snake was introduced to Guam shortly after World War II and since then has drastically altered the island ecosystem. Predation by this invasive snake has led to the local extinction of native lizards and six forest birds. Researchers are also concerned about cascading ecological effects leading to a decline in native plant species.\n3. Feral Cats\nThe domestication of cats has dispersed felines around the world. Those that do not become pets end up in ecosystems where they interact with species that did not evolve to defend against these agile hunters. Feral cats are known to hunt not only for food but also for fun, which poses a problem for island birds and other native species that have evolved to nest on the ground. Feral cats also do considerable damage in our own backyards. Keeping your pets inside is the only way to ensure they will not harm or kill native wildlife. For cats that are allowed outdoors, spaying and neutering are strongly encouraged to prevent proliferation of feral cats in the wild.\n4. House Mice\nSmall and seemingly inconsequential at first glance, house mice are one of the greatest threats to island species around the world. Midway Atoll, an island chain in the Southwest Pacific, is home to one of the largest breeding colonies of Albatross in the world, but house mice threaten these species. Predation on eggs, chicks, and even adults has led to drastic declines in Black-footed Albatross and Short-tailed Albatross. The House Mouse is quick to adapt to new surroundings and environments, which makes it particularly adept as an introduced species and often leads to invasion.\nRats are known for their ability to swim ashore to islands from ships, and when they’re not swimming onto shore, they’re simply walking onto the gangway and entering an ecosystem never designed to accommodate the voracious predator. The species is well known for its incredible adaptability and resourcefulness. On islands, invasive rats can and do lead to the catastrophic decline of native seabirds and other island wildlife.\nLionfish are native to the South Pacific and Indian Oceans, but with their introduction into the waters of the Southeastern United States in the mid-1980s they have become one of the most harmful aquatic invasive species. Popular in the pet trade, they are found in home aquariums throughout the world. Release into the wild has had disastrous consequences for native marine species. Lionfish are known to eat over 50 different species of fish, including economically and ecologically important species. Now conservation efforts are under way and people are encouraged to fish and eat the species to suppress the invasive population.\nMacaques are invasive on islands throughout Puerto Rico where wild, feral populations pose a threat to human health, livelihoods, and native species. Wild Macaques are known carriers of the Herpes B Virus and estimates suggest 70% of wild individuals carry the virus and have the potential for human transfer.\n8. Cane Toads\nOften intentionally introduced as a biological control agent, Cane Toads have become one of the most damaging invasive species in the world. Their toxic secretions make them unappealing to most predators and their rapid reproduction allows them to over-populate and out-compete native amphibians.\nGoats are quick to adapt to new environments and can devour vegetation at alarming rates. When they make their way to sensitive ecosystems, they can easily alter plant communities and create niche space for hardier invasive plants to proliferate in. On islands, feral goats pose a particular threat to specially adapted native vegetation as well as other native herbivores that rely on this vegetation for food. In the case of Redonda island, invasive feral goats depleted all vegetation and began to starve as a result.\nMongoose were often introduced on islands as a biological control to mitigate the effects of invasive predators such as rats and snakes. Ultimately, this tactic proved disastrous. Mongoose have wiped out several species on islands around the world. Mongoose are also known carriers of human and animal diseases such as rabies.\n- The Invasive Rose-ringed Parakeet – a Threat from Hawai’i to Spain - May 24, 2018\n- Birds and Insects Perking up on Antipodes Island after Restoration Project - May 24, 2018\n- Biodiversity: What is it and Why Does it Matter? - May 17, 2018\n- Palmyra Atoll – A Hope Spot for the World’s Oceans - April 27, 2018\n- Scientists Study Vocalizations of Thriving, Wild ‘Alalā - April 20, 2018\n- Conservation and the Future of Kaho’olawe - March 28, 2018\n- The Restoration of Kaho’olawe - March 23, 2018\n- Cultural Connections of Kaho’olawe - March 22, 2018\n- Sailor’s Hat – A Mark of Destruction on Kaho’olawe - March 16, 2018\n- The Danger of Kaho’olawe - March 13, 2018']	['<urn:uuid:c3fcb515-8654-48f3-af26-ea69dcd67c67>', '<urn:uuid:36f6cdac-221a-49ce-80c6-914304709d1e>']	open-ended	with-premise	short-search-query	distant-from-document	three-doc	expert	2025-05-13T05:06:54.527160	7	84	1897
93	What are the differences in microphone setups between Wand's live performances and the MegaReaper Drumkit's snare recordings?	The MegaReaper Drumkit features an extensive four-microphone setup for the snare, including Top, Bottom, and Left & Right overhead mics, with separate control over each mic channel. In contrast, when Wand performs live, Evan Burrows mentions they often play in settings where the drums aren't miked up at all, forcing him to rely on acoustic projection and specific tuning choices to ensure the drums don't get buried in the mix.	['by Adam Budofsky\nA number of bands today are successfully exploring the intersection of psychedelic, pop, and garage-rock music. Australia’s Tame Impala and King Gizzard and the Lizard Wizard, Britain’s Wytches and Syd Arthur, and America’s Ty Segall and Unknown Mortal Orchestra have bloomed boldly in the past year or two, each with its own unique approach to working the fertile musical ground planted in the ’60s by the Kinks, the 13th Floor Elevators, the Zombies, and Love. The Los Angeles–based group Wand, featuring drummer Evan Burrows, singer and guitarist Cory Hanson, and bassist Lee Landy, is among the most prolific of the current crop of galaxy-gazing garage-rockers, producing three full-lengths in just over a year. Burrows makes the most out of the group’s ambitious music-world view, muscularly navigating mini-epic, proggy workouts and delicately illustrating mellower moments with grace. We spoke to the drummer on the occasion of Wand’s brand-new release, 1000 Days.\nMD: The amount of thoughtfully put together music that Wand has released in a relatively short amount of time is impressive. And as a listener, you can hear a progression from one album to the next. Do you see yourselves as unusually efficient?\nEvan: Hey, thanks man. I don’t feel like we are particularly efficient—in fact, sometimes it feels like self-sabotage and discord are pretty central to our method. But I do think we are obsessive people, keen to be busy with all the activities and choices and struggles that creative practices involve, excited by what can happen when we all get together and play, when we all get together and try to stay together.\nThe waiting game that seems typical of most “album cycles” or whatever just doesn’t feel like an option for us—we can’t afford to wait around, and the waiting can be agonizing when it means allowing your enthusiasm and your work to be exhausted by all the misrecognition and confusion and cross-purposes that you will inevitably encounter if you decide to make your music public. So, when the masters for a new record fly away to the plant for several months, then let’s start the next record, the next band, the next plan or project. Or let’s work on more than one at a time.\nI prefer this rhythm of keeping our heads down, focused on practice and process and development—cherishing what you’re working on, getting carried away, staying critical, knowing that there are real stakes, but then trying not to feel too precious about anything that you’ve called “finished.”\nLuckily for me, I get to hang around with Cory and Lee and other psycho-genius friends who are always cooking up new ideas for us to slice up and hammer out and massage and ridicule and chew up and digest with our one stomach and three fore-stomachs.\nMD: It seems like Wand would be a lot of fun to play in; the songs vary in style and tempo and aggression. Does your playing in the group reflect most of your drumming interests?\nEvan: Yeah, Wand is a rewarding enterprise because I feel like just about anything that’s called for can be invited into our weird little zone. [laughs] And I’m optimistic that our range is only going to continue to expand. I came to drumming through punk music and I’m largely self-taught, so there are many strange limits to my technique. But I think that so far each time we’ve written a new record, on the level of performance we make efforts to write toward the outskirts of our collective and individual abilities, so that we all have to meet the new work beyond what we were capable of at the outset.\nI feel like my drumming, my thinking about percussion, my attention to the instrument have developed and deepened a lot in the last two years playing with these guys. Technique is not an end in itself for me, but if we are aspiring to make a music that does more liberating than encapsulating, does more to open than to close (is that right?), then I think the freer we can get here the better.\nLately we’ve been making more space in our live sets for more and less structured improvisation, and that has felt very good. Playing a lot of shows in a short period of time can really desensitize a body. It’s enlivening to have these opportunities in the set to be very attentive to timbre and tone and touch.\nI suspect that the next Wand record will be marked in some way by these modest adventures. I hope so. I hope that we can get and stay relatively wild—I’m interested in trying to make music that is increasingly inscrutable to the market and its attendant watchdog discourses, and yet increasingly affectionate toward other people. Maybe this isn’t so possible? Happily, music is pretty impossible stuff. Simple stuff.\nMD: “Paintings Are Dead” seems particularly fun, with the drums going from busy and heavy to light and double-time. Was there any particular challenge to recording that song?\nEvan: Recently I was talking to Cory about an imaginary record where the songs altogether feel like a handful of diamonds that are also seeds. I like 1000 Days because it feels sort of that way to me. And I think “Paintings Are Dead” is one of those jewel-seed songs. Teeming and sculpted and waiting to be dropped in the soil somewhere to become something else.\nMy recollection is that this was one of the easiest songs on the new record to write, and one of the trickiest to get a good drum take of—just wanting to make sure that there’s a common momentum animating the thing across these different changes and feels. And then that gradual tempo change at the very end was really difficult not to overthink– slowing down at a rate that felt solid, for a duration that felt solid. I remember fumbling a couple of strong takes right at that last moment.\nMD: The drums on “Clearer,” from Ganglion Reef, benefit from the arrangement very much highlighting your groove. You sound great on that. What are the demands in a section like that—just keeping it steady and avoiding the urge to mix it up too much?\nEvan: Thanks. The drums on that song are based on a demo that Cory had worked up on the computer using sample loops. I haven’t heard the demo in a long time, but I think the main drum motif on it was constructed by layering two different breakbeats on top of one another. So, being one body at one kit, I guess I just tried to distill the basic lopes and attach the right weights and anchors to that blimp of a riff. Wanting to achieve that ambling feeling while also leaving enough emptiness in the groove for the whole thing to feel airborne.\nI think I tend not to “mix it up” too much out of some measure of inability, but also because I like to think about composing in terms of rhyme or recurrence, especially when it comes to filling. I like to develop fills that are composed—like those stuttering little instants at the end of each verse phrase on “Clearer”—and then “deploy” them in a sequence, rearrange that sequence for the following four bars, repeat the sequence in the second verse, or invert it, switch one fill out, interrupt it, etc. I like how this creates a baseline of steadiness and expectation that then emphasizes any deviations or interruptions. It also overlays some wider and stranger rhythmic measures on top of the more obvious ones—like the two-beat pulse that is produced between the two identical fills that occur only once in each “Clearer” verse. Or the rhythm that is produced by a fill that might appear a few times on more than one track on a single record, or on multiple records, etc. Hopefully this does weird things to time.\nMD: The next song on that record, “Broken Candle,” has a great double-time feel, and I love the way you leave out the snare backbeats. Your bass drum foot on that is killer as well. Did you ever work specifically on foot technique?\nEvan: I’ve always struggled with discipline when it comes to practicing on my own, so I’ve never worked with much focus on particular skills. But I think writing and rehearsing for that record definitely helped me to develop my foot technique. There are a lot of snare-backbeat-driven parts on that record, so I wanted to try and devise different undergirding bass drum patterns that could provide variation.\nIn general, I think my body totally revolves around my bass drum foot when I’m sitting at the kit. I think I lead with it and rely on it, maybe to a fault. It’s been cool figuring out as many ways as possible to capitalize on that fact, and also trying to find new ways to upset the reflex and redistribute.\nMD: Where did you grow up, and what were your early musical experiences like?\nEvan: I grew up in Chicago. I started playing drums when I was eleven and started playing in bands as quickly as I could. I think I took a little less than a year of lessons around that time. I took a year of Beginning Band oboe and a year of Beginning Orchestra cello in high school. My formative experiences playing music were with friends, performing in basements and houses, the occasional club or school event, learning to record ourselves [while] making records in the attic at my folks’ house on a bootleg copy of Cool Edit Pro 2.0. The main attraction for me has always been the social dimension of it—all the beautiful and generous and malformed and sad ways people gather around music to start a band, have a practice, start a space, get to a show, throw a show, record a record, listen to a record, house and feed each other on tour, conduct an email interview, etc. I like to hang around and have these things to do and problems to solve and sounds to hear with friends.\nMD: There are several places on 1000 Days where we hear drum machines, loops, or trancey percussion-heavy sections. Are those ideas coming from you, your bandmates, or a combination?\nEvan: I only played the live percussion on the record. The drum machine programming was done by Cory and by our friend Caleb, who toured with us for the Ganglion Reef tours. When we play “1000 Days” and “Stolen Footsteps” live, I reinterpret those drum machine sections for the kit.\nMD: The end of “Lower Order” seems to be a couple of short loops strung together. It’s not really part of the song, but acts as a cool segue mechanism.\nEvan: The end of “Lower Order” is actually two snippets of “Dovetail” mixed differently and played back at different speeds and then spliced together. Recurrence! Hopefully this does weird things to time.\nMD: The way the drum-machine-like rhythm is introduced near the end of “1000 Days” is subtle but effective.\nEvan: Yeah, I like that moment too. Cory came up with that idea, and it’s been with the song since he first recorded a demo for it.\nMD: “Dovetail” is almost all rhythm. How did that track come about?\nEvan: That song came together after a really difficult evening of live tracking. We were all feeling dispirited, wiped out, and I think maybe we had just had an argument. So we called it a night and decided to start messing around. Cory wrote a drum machine loop we got excited about, so our dear friend and engineer Bob Marshall set up some minimal microphone drum coverage and I got to sit at the kit and improvise over that loop. Then we stopped, rewound the tape, and I did the same thing a couple more times, responding to each successive layer as I went. Then Lee did the same thing with two takes of synth. Cory ran a couple mixes and we had a song. By the time all was said and done at the studio and we were living with all the final mixes, I think it seemed pretty obvious to us that it had to be on the record. It’s the nerve center, or the boiler room, or maybe it wants to devour the rest of the record entirely—selfish little thing.\nMD: In “Sleepy Dog” the drum fill coming out of the spacey keyboard section and into the chorus makes me smile every time. Was there any specific thought behind your choice on that?\nEvan: I find myself attracted to these moments when the music might suddenly shift into the mechanical—as though the band were jammed up in some machine form, animated by other forces (absurd, nefarious), trapped in a loop. It’s kind of dumb, kind of funny, there’s oblivion and discipline at the same time, probably lots of other effects. There are several other moments throughout the record where I feel like the whole band kind of enters this mode together.\nI think originally during writing we rehearsed “Sleepy Dog” with that fill running only half as long, then during tracking I suggested we record it twice the length and fade it in or something. Then I guess we all grew accustomed to it and the whole eight repetitions made it through mixing intact. Happy accidents, triumphant machines.\nMD: What are your feelings about drum sounds—do you have general tones you like, or are you open to whatever the song dictates?\nEvan: I’m definitely open to whatever the song dictates. I like all the sounds. [laughs] On record, I like it when the production reveals all the subtle articulations of a drummer’s playing, I like when the production effaces the player, I like when it reconstructs something like the facts, and I like when it’s a total exaggeration. I guess you try and dress for the occasion. Or sometimes you wear your Donald Duck outfit to dinner—lil’ cap, sailor garb, bow tie, no pants—trying to prove some kind of point or something.\nLately, for live circumstances I’ve liked my bass drum tuned low, with the beater flipped to the plastic side to emphasize the attack. We’re pretty loud, so this helps keep that foundation from getting buried when we’re playing in settings where the drums aren’t miked up. I’ve been playing thinner snare heads, Ambassadors. I’m enjoying the relative brightness, but I also play through them really fast. I’ve been tuning the top head rather tight and the snare side head a little lower so it still has a bit of a bark to it. I tune my toms low and mute them slightly. For Wand, I prefer my cymbals to be articulate and light. For some other current projects, I’ve been getting excited about cheaper cymbals that are more unyielding and huge with lots of character.\nMD: What kind of gear do you play—drum and cymbal types and sizes, for instance? The live clips I’ve seen show you playing a pretty modest setup.\nEvan: Yeah, I like to keep it pretty simple. I also can’t afford to be too choosey or excessive due to matters of cash. I currently play a Gretsch New Classic kit that I’ve had for seven years or so, and the 14″ snare that came with it, which sounds really good to my ears. The toms are 12″ and 16″, I believe, and the bass drum is 22″ and pretty deep. I play 14″ Zildjian A Mastersound hi-hats, a 17″ Zildjian A Custom crash, and a 21″ Zildjian Sweet ride. DW 3000 kick pedal. 5B sticks!\nMD: What drummers did you emulate early in your drumming life, and as a listener, who do you gravitate toward today?\nEvan: Growing up and playing drums in Chicago during the time I was doing so, I feel like it was inevitable that my early style would boil down on some level to an ignoramus attempt to imitate Jimmy Chamberlain. Other big early influences for me were D.H. Peligro [Dead Kennedys], Jeff Nelson [Minor Threat], Brendan Canty [Fugazi], Phil Selway [Radiohead]. Lately I’ve been thrilled by so many drummers. Yikes…Charles Hayward [This Heat], Penny Rimbaud [Crass], Corey Rose Evans [Vexx], Jaki Liebezeit [Can], Dale Crover [Melvins], Adrian Tenney [Spokenest], Ryan Moutinho [Thee Oh Sees], Billy Ficca [Television], Moe Tucker [Velvet Underground], Tony Austin [Kamasi Washington Septet], Ronald Bruner Jr. [Kamasi Washington Septet], Palmolive [the Slits], Emil Bognar-Nasdor [Moil], Paul Erschen [Mayor Daley], Marian Li Pino [La Luz], Jensen Ward [Iron Lung], Denis Charles [Cecil Taylor Quartet], Colline Grosjean [Massicot], Ty Segall, Jody Stephens [Big Star], Bruce Smith [the Pop Group], Kyle Reynolds [Bad Drugs, Oozing Wound], Dan Swire [Gun Outfit], Anton Fier [the Feelies], Bill Ward [Black Sabbath], Ches Smith [Mary Halvorson, Xiu Xiu], Shannon Sigley [PC Worship], Al Daglis [Shade], Justin Sullivan [Kevin Morby Band], Sofia Arreguin [Personal Best], and on and on forever I can’t remember amen!\nMD: You recently released the single “Machine Man”/“M.E.” Neither song appears on 1000 Days, though. More evidence of an over-abundance of great material, or was there an artistic reason to separate those tracks out? I like the way you accent the riff on “Machine Man” as well, and you get to blow out some great fills toward the end.\nEvan: Yeah, those two earned their own little slab of wax. I think we actually wrote “Machine Man” first thing after finishing Ganglion Reef. So, that recording is from the sessions for Golem. Just didn’t fit with the emergent shape of that record, I guess.', 'This is a VERY deeply sampled version of this classic drum kit for Reaper (no add-ons required) (one of the deepest sampled kits in the world). And it is completely dry for you to sculpt as you like using Reaper’s superior processing.\nNote : This version has all un-normalized , unscaled samples, which, in my opinion, help with maintaining natural dynamics. However, many people better qualified than me do not agree. If you are one of them, you can use one of the may other options / versions on this website.\nAnd if you don’t have Reaper , you can use the included 1.9 GB of open 24 bit WAV samples in whatever sampler you choose. The samples are precisely named so mapping should be a breeze. But you should at least try the included Reaper kit even if you are not a Reaperite.\nA demo running the Lite Version (with zero processing barring eq for high passing) :\nWhat makes it so special ?\nGlad you asked. The instrument has :\nKick : 127 velocity layers x 2 round robins\nSnare No Ring : 127 velocity layers x 4 round robins\nSnare Studio Ring : 127 velocity layers x 4 round robins\nSnare Rimshots : 51 velocity layers x 2 round robins\nTom 1 : 49 velocity layers x 2 round robins\nTom 2 : 49 velocity layers x 2 round robins\nTom 3 : 49 velocity layers x 2 round robins\nHihat – Closed : 127 velocity layers x 2 round robins\nHihat – Foot Closed : 46 velocity layers x 2 round robins\nHihat – Foot Open : 46 velocity layers x 2 round robins\nHihat – Loose : 127 velocity layers x 4 round robins\nHihat – Open : 26 velocity layers x 2 round robins\nRide 17 – 49 velocity layers x 2 round robins\nRide 17 Bell – 49 velocity layers x 2 round robins\nRide 20 : 49 velocity layers x 2 round robins\nRide 20 Bell : 45 velocity layers x 2 round robins\nCrash 15 : 45 velocity layers x 2 round robins\nCrash 17 : 45 velocity layers x 2 round robins\nYes, you read that correct. The main snare (a classic 1965 Ludwig 5″x14″ Jazz Festival Snare drum) has 127 velocity layers and 4 round robins !! The kicks and hats also have 127 velocity layers and 2 round robins !! And I can confirm, it is a pleasure to play and program.\nPick up your jaw off the floor and let’s move on.\nThe mic channel layout is as follows :-\nThe Snare: four separate mics (Top, Bottom, Stereo combining Left & Right overheads mic).\nThe Kick Drum: one close mic as well as Stereo combining Left & Right overhead mics.\nThe Toms: one close mic as well as Stereo combining Left & Right overhead mics.\nThe Hats : One Stereo combining Left and Right mics.\nThe Cymbals : One Stereo combining Left and Right mics\nYes, that is correct. Each of those 127 x 4 snare samples have 4 mic channels (it’s a 4 channel / track WAV file). So effectively its 127 x 4 x 4 = 2032 samples just for just one snare articulation !!\nALL of the samples are multi-channel with the minimum being hats and cymbals (2 channel true stereo) and the main drums being 4 channels. ALL of the samples are unprocessed and un-normalized to preserve their natural dynamics. You can tweak them in a myriad of ways to your liking.\nThe Reaper instrument provides control over EACH mic channel. Example : for the snare you can mix in some more top mic channel , apply compression to the bottom mic channel and reverb to the overhead channels. Then you can further process these 4 channels combined in the snare bus.\nThe general flow is :\nMIDI In / Drum Sequencer\nSamples (multi channel wav files having upto 127 velocity layers and 4 round robins)\nSamplers (RS5K instances where the basic sample playback behavior /ADSR / MIDIMap can be tweaked)\nDrum Mic Channels (where you can set the mix amount and processing for each mic depending on what’s available)\nDrum Buses (where you can process the combined bus for each kit piece)\nThere are 2 versions available for you to choose from based on your requirement and system specs :\nSM MegaReaper Drumkit\nIt requires a minimum of 6 GB RAM and 2 GB HDD space.\nThis is the full kit with all the details as mentioned above.\nSM MegaReaper Drumkit Lite\nIt requires a minimum of 4 GB RAM and 1 GB HDD space.\nThis is the kit with no round robins (yet ALL of the velocity layers) and only 1 Crash, 1 Ride and no HH Loose articulation. It still sounds fantastic.\nThis is a very clear and helpful manual to get you started :\nYou should read it cover to cover (it’s a short 21 pages only) to understand the instrument (which has a lot of levels).\nNote : Make sure that the mixer (Ctrl + M) and the screensets (Ctrl + E) windows are visible and docked in Reaper at the bottom to maximize your experience.']	['<urn:uuid:1e05673d-37f4-43d4-8f28-49b521e8fddd>', '<urn:uuid:1b359a21-b066-492e-ba7b-7e80afe3c985>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T05:06:54.527160	17	70	3799
94	compare careers louis armstrong marian anderson racial barriers	Both Marian Anderson and Louis Armstrong were pioneering African American musicians who broke racial barriers, but in different ways. Anderson became the first African American to perform with the Metropolitan Opera in 1955 and faced significant racial discrimination, notably when she was refused permission to sing at Constitution Hall in 1939. She overcame these barriers through her talent and determination rather than political activism. Armstrong, on the other hand, became widely accepted earlier, emerging as an innovative jazz musician in the 1920s and eventually becoming regarded as perhaps the most important American musician of the 20th century, influencing popular music well beyond jazz.	"['Classical Music and Opera\nHere we highlight two of America\'s greatest musicians: composer Charles Edward Ives and singer Marian Anderson. These artists forged new musical paths and set high standards of excellence both in their work and in their personal lives.\nCharles Edward Ives (1874-1954)\nClara E. Sipprell (1885-1975)\nPhotograph, gelatin silver print, circa 1947, NPG.82.185\nNational Portrait Gallery,\nSmithsonian Institution, Washington, D.C.\nBequest of Phyllis Fenner\nHistorians agree that Charles Ives was one of America\'s most brilliant, if enigmatic, musical minds. Even those who question the artistic value of his work recognize his status as the first distinctly American composer of classical music. Up to the end of the nineteenth century, European forms and composers dominated the American concert hall. Ives was the first to combine classical forms with America\'s own cultural richness, beginning a tradition then carried on by composers such as Aaron Copland and George Gershwin. However, during much of Ives\'s own lifetime, his music was disparaged as an oddity by the few who knew of it. Truly, Ives\'s unconventional musical sensibilities were ahead of their time. More information about early musical instruments can be found by visiting the National Museum of American History.\nIves\'s father, a Connecticut gentleman and military bandmaster, taught him to find the inspiration for his compositions in everyday surroundings and occurrences. Also expressed in his music was a strong love of his country, a resistance to blind tradition, and a sometimes caustic wit. Ives showed considerable musical talent from a young age and was especially good at improvising on the keyboard. As a teenager, he composed pieces for his father\'s band and served as the organist for a local church. At Yale University, Ives put all his energy into music, nearly failing every other subject. Under the tutelage of composer Horatio Parker and others, Ives composed his first symphony and string quartet as well as numerous smaller pieces. After college Ives worked in the insurance business and composed music in his spare time.\nOne of the distinctive characteristics of Ives\'s work is the way that it often features, quotes, or borrows from other pieces of popular American music. Some of the better-known examples include variations on ""My Country \'Tis of Thee,"" written for the organ, and the final movement of his Second Symphony, which quotes ""Columbia, the Gem of the Ocean."" In addition, Ives was often inspired by American places or people, as in ""Three Places in New England"" and the ""Concord Sonata"" for piano. Several of his compositions anticipated practices that became common later in the twentieth century, such as polytonality, dissonance, and twelve-tone composition. Other pieces can only be described as joyfully anarchic, such as the second movement of ""Three Places in New England,"" which recreates the sensation of two bands converging on a village square from opposite directions, each blasting away in a different key and tempo.\nAfter 1918, ill health restricted Ives\'s ability to write music. Instead, he began to publish his compositions, the majority of which were still unknown to the public. By 1947, Ives had earned a Pulitzer Prize (for his Third Symphony) and the respect of musicians and critics alike. Today, Ives\'s popularity continues to grow as more of his music is discovered, published, performed, and recorded.\nBetsy Graves Reyneau (1888-1964)\nOil on canvas, 1955, T/NPG.67.76.03\nNational Portrait Gallery,\nSmithsonian Institution, Washington, D.C.\nGift of the Harmon Foundation\nToday, it would be difficult to imagine an American music scene bereft of African American performers. But although Marian Anderson was one of the most talented contralto singers ever, she was also the first African American musician to gain widespread popularity with mixed audiences in this country. Before Anderson embarked on her singing career, black musicians were not welcome on the American concert stage.\nAnderson began singing in the choirs of the Union Baptist Church in her native Philadelphia by the time she was seven years old, and practically from the outset, she was recognized as an unusually promising talent. Unfortunately, the pinched finances of her family, which worsened substantially after her father\'s death when she was about twelve, often made procurement of good vocal training difficult. Ultimately, however, she met voice teacher Mary Saunders Patterson, who provided her with free lessons. When Anderson was in her early twenties, the church raised funds that enabled her to study with Giuseppe Boghetti, a vocal instructor of high reputation in both Philadelphia and New York.\nAnderson\'s first break occurred in 1925, when she won a singing contest and a chance to perform solo with the New York Philharmonic. Following that triumph, however, racism generally confined her to performing in black communities and kept her from winning broad recognition in the American musical world. In 1931, in hope of breaking out of that constraint, she went to Europe, where she developed her signature concert mix of classical songs, arias, and African American spirituals. There, her powerful, rich voice, with its extraordinary three-octave range, drew unstinting praise and won her a large following. After witnessing one of her performances in Salzburg, Austria, conductor Arturo Toscanini told her,""A voice like yours is heard only once in a hundred years.""\nAfter 1935, in the wake of her European success, Anderson gained broader acceptance in the United States. The fact that she had not completely broken the race barrier, however, was dramatically proved in 1939, when the Daughters of the American Revolution refused to let her sing in its Constitution Hall in Washington, D.C. Protesting the snub was, among others, First Lady Eleanor Roosevelt, who resigned from the organization and convinced Secretary of the Interior Harold Ickes to allow Anderson to give an Easter Sunday concert on the steps of Washington\'s Lincoln Memorial instead. The concert was a huge success, with both live and radio audiences, and Anderson suddenly found herself at the center of the American music scene. When Anderson made her opera debut with New York\'s Metropolitan Opera in 1955, it was the first time an African American had performed with the company. In 1965, after twenty-five years of touring in the United States and abroad, Anderson gave her final performance at Carnegie Hall.\nAnderson\'s career did much to achieve equality for African Americans in the performing arts. However, hers was not a political crusade. Rather, her success was due to unflagging determination and the sheer weight of her talent. She recalled in a 1991 television documentary: ""I hadn\'t set out to change the world in any way. Whatever I am, it is a culmination of the goodwill of people who, regardless of anything else, saw me as I am, and not as somebody else.""\nCharles Edward Ives', '|The Jazz Vocalists\nThe Roots (The Blues and the Big Band Singers)\nBessie Smith (Blues) 1920s\nLouis Armstrong (trumpet, Voice) (1901-1971)\nComing to prominence in the 20s as an innovative cornet and trumpet virtuoso, Armstrong was a foundational influence on jazz, shifting the music\'s focus from collective improvisation to solo performers. With his distinctive gravelly voice, Armstrong was an influential singer, demonstrating great dexterity as an improviser, bending the lyrics and melody of a song for expressive purposes. He was also greatly skilled at scat singing, or wordless vocalizing.\nRenowned for his charismatic stage presence, Armstrong\'s influence extended well beyond jazz, and by the end of his career in the \'60s, he was widely regarded as a profound influence on popular music in general: critic Steve Leggett describes Armstrong as ""perhaps the most important American musician of the 20th century.""\nElla fitzgerald (Ella Jane Fitzgerald (April 25, 1917 – June 15, 1996, 78 years old))\nOne Note Samba\nGee Baby Ain’t I good to you (joe Pass and Ella)\nBillie Holiday (born Eleanora Fagan; April 7, 1915 – July 17, 1959) was an American jazz singer and songwriter.\nNicknamed Lady Day by her sometime collaborator Lester Young, Holiday was a seminal influence on jazz and pop singing. Her vocal style — strongly inspired by instrumentalists — pioneered a new way of manipulating wording and tempo, and also popularized a more personal and intimate approach to singing. Critic John Bush wrote that she ""changed the art of American pop vocals forever."" She co-wrote only a few songs, but several of them have become jazz standards, notably ""God Bless the Child"", ""Don\'t Explain"", and ""Lady Sings the Blues"".\nFine and Mellow\nWhat a little moonlight can do\nhttp://www.youtube.com/watch?v=bWtUzdI5hlE&feature=related the blues are Brewing\nBing Crosby Harry Lillis “Bing” Crosby (May 3, 1903 – October 14, 1977) was an Academy Award winning American popular singer and actor whose career lasted from 1926 until his death.\nOne of the first multimedia stars, from 1934 to 1954 Bing Crosby held a nearly unrivaled command of record sales, radio ratings, and motion picture grosses. He is cited among the most popular musical acts in history and is currently the most electronically recorded human voice in history.  Crosby is also credited as being the major inspiration for most of the male singers of the era that followed him, including Frank Sinatra, Perry Como, and Dean Martin. Yank magazine recognized Crosby as the person who had done the most for American G.I. morale during World War II and, during his peak years, around 1948, polls declared him the ""most admired man alive,"" ahead of Jackie Robinson and Pope Pius XII. Also during 1948, the Music Digest estimated that Crosby recordings filled more than half of the 80,000 weekly hours allocated to recorded radio music. Clarinetist Artie Shawdescribed Crosby as ""the first hip white person born in the United States.""\nPennies From Heaven\nPeggy Lee (May 26, 1920 – January 21, 2002)\nGreat Interpreters of the jazz standards and Latin\nAntonio Carlos Jobim\nTania Maria (Bresil)\nNancy Wilson (born February 20, 1937) is an American singer with seventy-plus albums, and three Grammy Awards so far in her career. She\'s been labeled a singer of blues, jazz, cabaret and pop; a ""consummate actress""; and ""the complete entertainer."" The title she prefers, however, is song stylist. She has received many nicknames--""Sweet Nancy, The Baby"" and the ""Fancy Miss Nancy"" are only two of them.\nSammy Davis Jr.\nHarry Connick Jr.\nCarmen McRae (piano/voice/composer/actress)(1922-1994) 60 albums during her career\nWhat a little moonlight can do to you\nhttp://www.youtube.com/watch?v=i7ktgyt3OTo&feature=related body and soul live\nBetty Carter (singer/Song writer) (May 16, 1929 – September 26, 1998) was an American jazz singer who was renowned for her improvisational technique and idiosyncratic vocal style. Carmen McRae once claimed that ""there\'s really only one jazz singer - only one: Betty Carter.""\nhttp://www.youtube.com/watch?v=tBgfKgDqzjs original song “Tight”\nhttp://www.youtube.com/watch?v=hXzDJF8Omc8 one of her last filmed performance\nCarter was born Lillie Mae Jones in Flint, Michigan and grew up in Detroit, where her father led a church choir. She studied piano at the Detroit Conservatory. She won a talent contest and became a regular on the local club circuit, singing and playing piano. When she was 16, she sang with Charlie Parker, and she later performed with Dizzy Gillespie and Miles Davis.\nhttp://www.youtube.com/watch?v=gvfNsZaDk-A&feature=related with Nat King Cole, and Mel Tome\nMel Torme (1925- 1999) the Velvet Fog\nSarah Lois Vaughan (nicknamed ""Sassy"" and ""The Divine One"") (March 27, 1924 – April 3, 1990) was an American jazz singer, described by Scott Yanow as having ""one of the most wondrous voices of the 20th century"". She had a contralto vocal range. \nSarah Vaughan was a Grammy Award winner. The National Endowment for the Arts bestowed upon her its highest honor in jazz, the NEA Jazz Masters Award in 1989.\nRound Midnight live with Dizzy Gillespie\nThe shadow of your smile\nThe sassy one\nDee Dee Bridgewater\nJoe Willians (Blues)\nBorn Anita Belle Colton in Chicago, Illinois on October 18, 1919. Anita died on Thanksgiving morning November 23, 2006. O’Day got her start as a teen. She eventually changed her name to O’Day and in the late 1930’s began singing in a jazz club called the Off- Beat, a popular hangout for musicians like band leader and drummer Gene Krupa. In 1941 she joined Krupa’s band, and a few weeks later Krupa hired trumpeter Roy Eldridge. O’Day and Eldridge had great chemistry on stage and their duet “Let Me Off Uptown” became a million-dollar-seller, boosting the popularity of the Krupa band. Also that year, “Down Beat” magazine named O’Day “New Star of the Year” and, in 1942, she was selected as one of the top five big band singers....(continued on her web site)\nTea for Two\nsweet Georgia Brown\nVocal Groups and Vocalize (writing words to Jazz Solos)\nLambert Hendrix and Ross (Annie Ross, John Hendrix, Dave Lambert(the vocal arranger)Group formed in the late 50s.\nKing Pleasure (His Hit Moody’s mood for love was in 1952)\nAmy winehouse (singing Moody’s Mood for Love) YouTube\nManhattan Transfer (Popular during the 80s and 90s)\nKurt Elling (born in 1967) 7 albums on Blue Note.\nJamie Cullum (28 years old from Britain)\nThe 28-year-old\'s Verve debut, Twentysomething, was a worldwide smash last year, selling over two million copies (including nearly 400,000 in the States) and garnering a Grammy nomination.\nNina simone (piano/voice)\nChet Baker was a primary exponent of the West Coast school of cool jazz in the early and mid-\'50s. As a trumpeter, he had a generally restrained, intimate playing style and he attracted attention beyond jazz for his photogenic looks and singing. But his career was marred by drug addiction.\nIt Could happen to you\nMy Funny Valentine.\nGeorges Benson (guitar/voice)\nEsperanza Spalding (Bass/voice)\nfor her biography go to her website.\nbody and soul live\nThere is a ton of videos on Utube check her out.\nNat King Cole (piano/voice)\nShirley Horne (Piano/voice)\nOnce I loved\nSomething happens to me\nDiane Shure (piano/voice)\nBlossom Dearie (Piano/voice)\nDiana Krall (Piano/voice)\nOfficial Web site\ntea for two live\n] Melody Gardot /ɡɑrˈdoʊ/ (born February 2, 1985) is a Grammy-nominated American singer, writer and musician in Philadelphia, Pennsylvania, though she considers herself a ""citizen of the world"". She has been influenced by such blues and jazz artists as Judy Garland, Janis Joplin, Miles Davis, Duke Ellington, Stan Getz and George Gershwin as well as Latin music artists such as Caetano Veloso. Her music has been compared to that of Nina Simone.\nGardot follows the teachings of Buddhism, is a macrobiotic cook and humanitarian who often speaks about the benefits of music therapy. She has visited various universities and hospitals to speak about its ability to help reconnect neural pathways in the brain, improve speech ability, and lift general spirits. In a recent interview she was rumored to be working closely in a university in the United States to help develop a program for music therapy and the management of pain, something she has spoken about establishing in the future on her own.[citation needed\nBaby I’m a Fool (live)\nWho will comfort me']"	['<urn:uuid:d0c5d15b-e867-4d7b-a4bc-23420046247d>', '<urn:uuid:167e116d-0e05-436d-b52e-ed5faf4ecb0d>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-13T05:06:54.527160	8	103	2458
95	aviation shipping road transport emissions reduction targets icao eu paris agreement	The EU set a target of 60% reduction in transport GHG emissions by 2050 compared to 1990. For aviation specifically, global emissions should be 41% lower by 2050 compared to 2005, while shipping emissions should be 63% lower. Additionally, the International Maritime Organisation (IMO) aims to reduce total shipping GHG emissions by 50% by 2050 compared to 2008. For aviation, CORSIA (Carbon Offsetting and Reduction Scheme for International Aviation) was established as the first global market-based scheme to achieve carbon neutral growth through emissions reductions, technological innovations, and sustainable aviation fuels.	"['Renewable fuels – Advancing European Market Uptake\nDiscovering the world, working with people across the globe, commuting to work every day or consuming exotic fruits imported to Europe – nearly every European is used to at least one of these habits producing a significant amount of carbon emissions. On the road to decarbonizing transport and reaching climate targets, the market uptake of renewable fuels is essential.\nThe European transport sector is the only major sector where greenhouse gas (GHG) emissions are continuously increasing. According to the European Environment Agency, in 2016 the transport sector contributed 27 percent of total EU greenhouse gas emissions and they were 26.1 percent higher compared to 1990. While road transport has contributed the most to the transport sector’s GHG emissions, the largest increase in final energy consumption has occurred in the aviation sector between 1990 and 2016, and this sector is expected to continue growing rapidly. International shipping activity is also expected to increase as it is driven by increasing globalization and trade.\nIn response to increasing emissions, the EU has set several targets to mitigate and limit GHG emissions from transport. An overall target of a 60 percent reduction in GHG emission within the transport sector by 2050 was set in 2011 (compared to 1990) – still too low to reach the 2015 Paris Agreement aspiring to limit global warming to less than 1.5°C. In 2015, the study “Emission reduction targets for international aviation and shipping” conducted for the European Parliament suggests that by 2050 global aviation emissions should be at least 41 percent lower than in 2005, and the global emissions of the shipping sector to be at least 63 percent lower.\nRenewable fuels are among the most viable options to reduce GHG emissions in the transport sector. While electrification becomes more significant, (mainly in road and rail transport), advanced biofuels are expected to maintain a significant role for the shipping and aviation sectors – especially in the short- and mid-term. A number of scenarios conducted for the European Commission and by the International Energy Agency (IEA) indicate the need for large quantities of renewable fuel demanded by 2050. With the aim of contributing to the Paris Agreement goals, the PRIMES scenarios foresee a major increase in current biofuel use. If demand in 2050 is to be met completely by advanced biofuels, this implies a more than 10-fold increase of their uptake in the time frame between 2017-2050. For the successful and sufficient market uptake of biofuels, feedstock has to be produced sustainably in sufficient quantities.\nEnsuring sustainable feedstock\nThere has been a great deal of skepticism followed by strong debates about the overall sustainability of biofuels especially their impact on land use patterns and food prices, and their carbon emissions across the production value chain. In response, a new generation of renewable transport fuels is emerging produced from non-food biomass called advanced biofuels.\nEurope has significant potential to produce advanced biofuels from lignocellulosic feedstock (such as waste and residues from agriculture and forestry). There are many initiatives that provide evidence to that. The EU H2020 projects FORBIO and SEEMLA demonstrated that biomass can be produced for bioenergy in a sustainable way on underused land, a process which is in line with the objectives of the EU Renewable Energy Directive (RED II), showing a high potential in promoting the production of advanced biofuels. The latest findings of the ADVANCEFUEL project show that many biomass sources are potential candidates that require sustainability efforts before they are readily available to produce advanced biofuels at a commercial scale (e.g. infrastructure, farmers experience, regulatory compliance and support, suitable for conversion).\nThe European transport sector is the only major sector where greenhouse gas (GHG) emissions are continuously increasing.\nWater scarcity, low fertility and marginality are major challenges concerning biomass productivity and profitability on underused land. The web geographic information system (GIS) developed by the BIOPLAT EU project helps identify suitable marginal, underutilized and contaminated (MUC) lands around Europe, assessing environmental, social and economical sustainability indicators of bioenergy value chains. Another option to tackle limited available fresh water and marginal land is to use seaweed as novel biomass for alternative transport fuels. The MacroFuels project developed several routes for the con- version of seaweed to biofuels which, once upscaled, will be economically viable. MacroFuels further achieved urgent technological breakthroughs towards large-scale seaweed cultivation such as deployment and cultivation at sea, automated harvesting or efficient storage and pre-treatment methods.\nAlthough RED II is an important step forward in recognizing the need to cover all bioenergy uses and supporting renewable fuels in the transport sector, lack of internationally recognized and harmonized criteria between different bioenergy sectors remains a major barrier to the commercialization of advanced biofuels that will increasingly be produced in multi-output biorefineries and supplied to international markets such as shipping and aviation. Harmonization of national and voluntary sustainability certification schemes at EU level is key for the market uptake of advanced biofuels. The ADVANCEFUEL project provides a set of sustainability criteria and indicators along the whole supply chain and recommendations to enhance the sustainability performance of biofuels.\nEnd use of advanced biofuels\nIn addition to incremental sustainability governance and availability of biomass for the production of advanced biofuels, a secure and stable policy framework and significant cost reductions are essential for a successful market uptake. According to the progress review by the European Commission conducted by Navigant, the EU is on track for reaching renewable energy and biofuel targets 2020. However, the IEA World Energy Outlook, flags that the biofuel production in EU member states is not on track to meet the IEA Sustainable Development Scenario (SDS) demand in 2030 as most biofuel consumption is at low percentage blend levels with fossil fuels. It forecasts annual production growth of 0.5 percent in Europe, falling short of the 8 percent of growth to meet the SDS. Higher biofuel blend rates or greater use of drop-in biofuels are essential to increase the consumption of biofuels. To tackle the high costs of advanced biofuel investment and production, the IEA recommends to introduce supportive policies to facilitate the technology learning and production scale-up necessary to reduce costs.\nEuropean renewable fuels market analysis by the ADVANCEFUEL project indicates that Europe started off well with the advanced fuels sub-obligation of 3.5 percent (including double counting) introduced in RED II. However, this may not be yet sufficient to meet the Paris Agreement targets. For road and rail transport, the EU introduced a mandatory renewable energy target. In contrary to road and rail, the aviation and shipping sectors are regulated at an international level. As of 2021, a global market-based measure, Carbon Offsetting and Reduction Scheme for International Aviation (CORSIA), will be operational addressing CO2 emissions in the aviation sector allowing airlines to buy emission reduction offsets from other sectors to compensate emissions or use eligible fuels with lower carbon use.\nProduction costs of advanced biofuels are more than twice the price of conventional fossil fuels.\nIn spring 2018, the International Maritime Organisation (IMO) adopted a strategy to reduce total GHG emissions from shipping by 50 percent in 2050, and to reduce the average carbon intensity by 40 percent in 2030 and 70 percent in 2050, compared to 2008 by focusing efforts on enhancing the energy efficiency performance of shipping, encouraging the development of national action plans to develop policies addressing GHG emissions from international shipping, and provide technical cooperation and capacity-building activities.\nScaling up advanced biofuels\nProduction costs of advanced biofuels from lignocellulosic feedstocks are typically more than twice the price of conventional fossil fuels and the current low fossil fuel prices have been a considerable obstacle to their development and deployment as shown by latest ADVANCEFUEL results. The shipping sector can use a large range of advanced biofuels. The future ability of advanced biofuels to compete in the market will depend on cost reductions such as technological learning, economies of scale, efficiency improvements, more affordable and more sustainable feedstocks as well as the evolution of fossil fuel prices. Making them competitive can be difficult, especially in the aviation sector due to their high production costs and energy intensity. In shipping, biofuels can be a viable option due to lower production costs and the large demand from the industry.\nTo reach climate goals and decarbonize road, maritime and air transport, upscaling biofuels is crucial and an indispensable way forward in the energy transition. In addition to using all possible renewable fuel options, maximizing energy efficiency and energy saving is necessary. There are still some roadblocks to remove in the strategy to ensure a smooth market uptake of renewable fuels. Concerted stakeholder action is essential to put transport biofuels on track with the Paris Agreement. Policy is therefore an important tool to steer future market uptake towards value chains that source sustainable feedstock and employ resource efficient conversion pathways generating rural and global economic development and a more decarbonized mobility around the world.', '(Click here for pdf version)\nCarbon Offsetting and Reduction Scheme for International Aviation (CORSIA) is the first global market-based scheme that applies to a sector. It complements other aviation in-sector emissions reductions efforts such as technological innovations, operational improvements and sustainable aviation fuels to meet the ICAO aspirational goal of carbon neutral growth.\n102 States Voluntarily Participate in CORSIA\nThe 40th ICAO Assembly strongly encouraged all States to voluntarily participate in the pilot phase and the first phase of CORSIA (Assembly Resolution\nA40-19, paragraph 9 c)).\nEighty-eight (88) States have volunteered to participate in offsetting CO2 emissions under CORSIA from its pilot phase that started on 1 January 2021. The list of these 88 States can be found\nThe Governments of Bahamas, Barbados, Belize, Cook Islands, Gambia, Grenada, Kiribati, Nauru, Saint Kitts and Nevis, South Sudan, Tonga, Trinidad and Tobago, Tuvalu, and Vanuatu notified the ICAO Secretariat of their intention to voluntarily participate in the CORSIA offsetting requirements from 1 January 2022, bringing the\ntotal number of participating States to 102.\nCORSIA in Numbers (as of 30 June 2021)\nUnder CORSIA, aeroplane operators with international flights are subject to\nmonitoring, reporting and verification (MRV) requirements. As of 1 January 2019, operators are required to monitor their annual CO2 emissions, have them verified through a third-party verification process, and submit them to the States to which they are attributed. States collect emissions data from all their operators and submit consolidated information to ICAO.\nIn addition to CO2 emissions, States are required to submit information on aeroplane operators attributed to them, and on verification bodies accredited in them. The\nlatest lists of aeroplane operators and verification bodies can be found in the fifth edition of ICAO document ""CORSIA Aeroplane Operator to State Attributions"" and in the eighth edition of ICAO document ""CORSIA Central Registry (CCR): Information and Data for Transparency"", respectively.\nReporting CO2 Emissions through the CORSIA Central Registry (CCR)\nIn accordance with the provisions of Annex 16, Volume IV, States have to submit CO2 emissions, through the CORSIA Central Registry (CCR), every year starting with the 2019 emissions that were due on 31 August 2020. To alleviate some of the negative impacts of COVID-19, the June 2020 ICAO Council agreed on some flexibility regarding the late submission of 2019 CO2 emissions data and requested the Secretariat to facilitate the reporting working closely with States that had problems meeting the original deadline.\nAs of 30 June 2021, 114 States had submitted their 2019 CO2 emissions through the CCR. Collectively, these States account for more than 95% of the total 2019 RTK. As per the SARPs in Annex 16, Volume IV, the Secretariat will proceed with completing the\nemissions gap for States that do not submit data through the CCR.\nFor 2020 CO2 emissions, the deadline for submission is 31 August 2021. With the experience of the 2019 CO2 emissions, States are now in a better position to comply with the CORSIA reporting deadlines.\nAs of 30 June 2021, 18 States have already started the reporting process for 2020 CO2 emissions data through the CCR.\nThe Secretariat will compile the CORSIA baseline CO2 emissions (taking into account the relevant decisions by the Council in June 2020) aggregated for all aeroplane operators on each State pair in the CORSIA document “CORSIA Central Registry (CCR): Information and Data for Transparency”. The document will be published no later than 30 November 2021 on the ICAO website following its approval by the ICAO Council.\n2021 CORSIA Implementation Deadlines\nIn accordance with Appendix 1 to\nAnnex 16, Volume IV, the following deadlines apply to 2021:\n1 June 2021 to 31 August 2021: States to conduct an order of magnitude check of the verified Emissions Reports for 2020, including any filling in of data gaps in case of non-reporting by aeroplane operators.\n1 August 2021: States to obtain and use the ICAO document entitled ""CORSIA States for Chapter 3 State Pairs"" applicable for the 2022 compliance year.\n31 August 2021: Using the CCR, States to submit to ICAO aggregate CO2 emissions per State pair for 2020.\n30 September 2021: States to calculate and inform aeroplane operators attributed to them of their average total CO2 emissions during 2019 and 2020.\n30 November 2021: Using the CCR, States to update their list of aeroplane operators that are attributed to them, and the list of verification bodies accredited in them.\n31 December 2021: States to obtain from the ICAO website and use the ICAO document entitled ""CORSIA Aeroplane Operator to State Attributions"" summarising a list of aeroplane operators and the State to which they have been attributed.\nCORSIA Capacity Building\nACT-CORSIA is the\nCapacity Building and\nTraining programme on\nCORSIA. It was established in June 2018 and encompasses the CORSIA Buddy Partnerships and all other ICAO outreach initiatives including\nsample model regulations for CORSIA, frequently asked questions (FAQs), brochure and leaflets,\nvideos as well as CORSIA\nseminars/webinars and online tutorials.\nCORSIA Frequently Asked Questions (FAQs): An updated version of the frequently asked questions (FAQs) on CORSIA, reflecting developments in 2020, is available\nACT-CORSIA Buddy Partnerships are the core of capacity building activities. They rely on individual training of CORSIA Focal Points and involves experts from 134 States. In May and June 2021, various activities took place involving\nMyanmar in partnership with\nArmenia, Azerbaijan, Belarus, Georgia, Kazakhstan, North Macedonia, Republic of Moldova\nand Serbia in partnership with\nGermany. More information can be found\nACT-CORSIA will now focus on CORSIA volunteer States providing training on offsetting compliance through the use of CORSIA Eligible Fuels and CORSIA Eligible Emissions Units.\nNavigating CORSIA - A guide to the scheme’s design & implementation\nSince the adoption of the CORSIA-related Standards and Recommended Practices (SARPs) in 2018, ICAO has developed the remaining elements needed for the CORSIA implementation.\nClick here to watch on the ICAO TV the Navigating CORSIA series of pre-recorded presentations that focus on the status of implementation of CORSIA and provide information on its key design elements.\nClick here for an infographic on the status of all implementation elements.\nCORSIA Eligible Emissions Units\nTAB will continue the assessment of emissions unit programmes, including the four material updates to previously-assessed programmes,\nAmerican Carbon Registry, Architecture for REDD+ Transactions, Clean Development Mechanism, and Verified Carbon Standard. TAB recommendations are expected to be considered by the ICAO Council in October/November 2021.\nThe 26th session of the Conference of the Parties (COP26) to the UNFCCC is scheduled to take place in November 2021. Among the issues to be discussed during COP26 is the\nimplementation of international carbon markets under Article 6 of the Paris Agreement. The UNFCCC pre-sessional period that took place in June 2021 included the discussions on three separate issues: a) guidance on\ncooperative approaches referred to in Article 6, paragraph 2, of the Paris Agreement; b) rules, modalities and procedures for the\nestablished by Article 6, paragraph 4, of the Paris Agreement; and c) work programme under the framework for\nnon-market approaches referred to in Article 6, paragraph 8, of the Paris Agreement. Summary notes are available\nKey issues on Article 6 that are of relevance for the work of ICAO on CORSIA include: the avoidance of double-counting; the achievement of overall mitigation of global emissions; levy on credits trading that can fund adaptation efforts; and decisions on whether credits generated under the Kyoto Protocol can continue to apply to emissions targets under the Paris Agreement. ICAO follows closely the developments under the UNFCCC and in particular, on any implications for the implementation of CORSIA and the CORSIA eligible emissions units.\n2022 CORSIA Periodic Review\nIn June 2021, the CAEP presented the results of its work to the 223rd session of the ICAO Council (C-WP/15209). The updated CAEP scenario-based analyses on potential impacts of COVID-19 on CORSIA and its executive summary are now available on the ICAO website.\n2021 CORSIA and Other Environment-related Events\nSave the Date:\nOther Environment Events:\nThe CORSIA Verification Training Week will provide you the opportunity to listen to a panel of experts and participate in stimulating discussion on this subject. Participant will also take part in a training course, available in English, Spanish and French, on how to verify CO2 Emissions Reports that have been prepared by aeroplane operators, in accordance with the provisions of the CORSIA Standards and Recommended Practices (SARPs).\nClick here for more information about the CORSIA Verification Training Week\nInternational Civil Aviation Organization999 Robert-Bourassa Boulevard, H3C 5H7, Montréal, Quebec, Canada\nFor more information, visit the ICAO website:']"	['<urn:uuid:09a8fc6a-0002-4743-ade4-8f119f121d9c>', '<urn:uuid:a7cc46d5-2b6f-4aee-bf5f-e2237c6a979d>']	factoid	direct	long-search-query	distant-from-document	three-doc	expert	2025-05-13T05:06:54.527160	11	91	2896
96	I'm interested in inter-departmental cooperation in government institutions. What regular meetings take place between different parliamentary departments to ensure they work together effectively?	Department Heads meet quarterly to identify collaboration opportunities and strengthen parliamentary administration. The Parliamentary Administration Advisory Group meets to oversee corporate initiatives across departments, and the Parliamentary ICT Advisory Board meets quarterly to oversee ICT service delivery. Additionally, Service Level Agreements are established between parliamentary departments to define service levels and performance.	['The Parliamentary Service\nThe Australian Parliament is established under the Constitution to make Commonwealth laws, supervise the spending of public money, scrutinise government activities and provide a forum for national debate. The Parliamentary Service provides professional support, advice and facilities to each House of the Parliament, to parliamentary committees and to Senators and Members of the House of Representatives. It consists of four parliamentary departments with distinct but overlapping roles.\nThe Department of the Senate provides the Senate, its committees, the President of the Senate and Senators with advisory and administrative support services to enable them to fulfil their representative and legislative duties.\nThe Department of the House of Representatives supports the House of Representatives in the role of a representative and legislative body by providing expert advice and services of a high standard.\nThe Department of Parliamentary Services provides a wide range of services and facilities to ensure that the Parliament functions effectively. These include audio visual and Hansard services, library and research services, art services, building grounds and design intent services, visitor services, security services and information and communication technology services.\nThe Parliamentary Budget Office informs the Parliament by providing independent and non-partisan analysis of the budget cycle, fiscal policy and the financial implications of proposals, and reports publicly on the budget impacts of the election commitments of the major parliamentary parties.\nThis plan adds a formal dimension to the engagement which has long characterised the work of the parliamentary service.\nWhile each department performs a unique role in supporting the functioning of the Australian Parliament, we share a common goal in serving and supporting the Parliament, and together provide services to ensure:\n- Parliament and its committees function effectively throughout the 45th Parliament\n- that senators and members are supported in the democratic process\n- the community can easily access and engage in the work of the Parliament and Parliamentary committees\n- Australian Parliament House (APH) is sustained as a workplace and national institution\n- our capacity as an independent, non-partisan parliamentary service is enhanced\n|Provide services and support to enable the Houses and their committees to function effectively\n- Continue to provide timely and accurate advice and support services related to the exercise of the legislative power of the Commonwealth\n- Deliver high quality secretariat support, and high quality research, analysis and advice to the Houses, their committees, parliamentarians and their staff\n- Effectively manage office accommodations including the supply and management of equipment, furniture, information systems, transport, publishing, printing and messenger services\n|Ensure parliamentarians are supported in their work today and we are responsive to the future\n- Develop cost effective and innovative ways to deliver services to parliamentarians\n- Continue to provide professional development opportunities for parliamentarians and their staff\n- Implement efficient and effective infrastructure, systems and services to respond to the changing needs of the Parliament and our parliamentarians\n- Explore and develop innovative technology and systems for the delivery of timely information and services to the Houses, their committees, parliamentarians and their staff\n|Enhance engagement in the work of the Parliament\n- Support the Parliament’s engagement with the community and initiatives to develop parliamentary democracy in our region\n- Improve and enhance electronic access to parliament information and proceedings for parliamentarians and the community\n- Improve accessibility and quality of services for visitors to APH\n- Continue to develop and promote the Parliament’s International program and capacity building work\n- Deliver high quality education and outreach programs that accurately reflect the Parliament and its work\n- Continue to promote significant parliamentary events to enable greater community engagement including through the use of social media and emerging technologies\n|Ensure Australian Parliament House operates as a safe and accessible workplace and national institution\n- Ensure a secure environment while maintaining public accessibility\n- Ensure adaptations of the uses of the building are strategic, appropriate and reference design integrity principles\n- Effectively manage all assets within APH, in keeping with its status as an icon of parliamentary democracy and building of national significance\n|Enhance our capability as an independent, non-partisan and professional parliamentary service\n- Continue to promote the Parliamentary Service Values through leadership and training\n- Proactive identification and application of collaborative approaches through shared learning and development across the parliamentary service\n- Continue to develop staff with the skills and capabilities to be responsive and solutions oriented in supporting parliamentarians and their staff\nacross the parliamentary service\nUnder the Parliamentary Service Act 1999, the heads of the parliamentary departments, reporting to the Presiding Officers provide leadership and strategic direction to the individual departments and manage the affairs of those departments in a way that is not inconsistent with the interests of the Parliamentary Service as a whole. They assist the Presiding Officers to fulfil their accountability obligations to the Parliament.\nThe four heads are supported in these endeavours by SES level officers, required under the Act to provide strategic leadership that contributes to an effective and cohesive Parliamentary Service; to promote cooperation within and between departments, including to deliver outcomes across department boundaries; and to promote the Parliamentary Service Values, the Parliamentary Service Employment Principles and compliance with the Code of Conduct.\nOur pride in parliamentary service is reflected in the quality of our work\nWe all share a common purpose of supporting Australia’s Parliament and parliamentarians. The parliamentary departments work together to support our common purpose through an established governance framework.\n- Meetings of Department Heads are conducted on a quarterly basis – these meetings are essential to identify opportunities for collaboration, consideration and decision making, strengthening the effectiveness of the parliamentary administration.\n- Parliamentary Administration Advisory Group meetings – this group supports the department head meetings by overseeing implementation of corporate initiatives of common interest across the parliamentary departments.\n- Parliamentary ICT Advisory Board meetings are conducted quarterly – these meetings represent the peak body within Parliament in overseeing and guiding all strategic element s of ICT service delivery within APH and across the electorate offices.\n- Service Level Agreements – are established between parliamentary departments to provide confidence and define the level of service and performance.\nIn addition, we are each accountable to the Parliament through a variety of parliamentary committees and advisory groups that oversee our operations.']	['<urn:uuid:1e82f1bd-5e06-457c-95fb-36283d98f349>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	23	52	1039
97	What are the key differences between how watersheds are managed in McKean County versus Florida's Water Management Districts in terms of their organizational structure and primary focus?	McKean County has a single Conservation District overseeing watersheds that primarily fall within the Allegheny River and Susquehanna River systems, while Florida divides watershed management among five Water Management Districts (South Florida, St. Johns River, Southwest Florida, Suwannee River, and Northwest Florida). Florida's districts maintain water quality and quantity, manage floodplains and flood protection, and establish minimum flow and water levels (MFLs) for rivers and streams.	"['Streambank Stabilization and Fish Habitat\nThe McKean County Conservation District has an ongoing Streambank Stabilization and Fish Habitat Project program. This work addresses streambank erosion problems which result in pollution to county streams. Soil loss and sediment from eroding banks has serious impacts on aquatic life, clogging the gills of fish, killing food sources for larger fish. Streambank stabilization structures hold soil in place and provide a growing location for plants and shrubs, which helps stabilizes the bank with root systems. Structures re-direct the flow of the stream back to the center. This creates deeper pools of water, which are needed by fish for feeding, rest, and breeding.\nDo you have an eroding streambank on your property?\nThe first step is to prevent additional erosion by allowing nature to regrow vegetation meant to hold the streambank. Native dogwoods, willow, and other shrubs develop root systems and hold the soil in place. Do not mow to the edge of the stream. Grass alone does not have deep roots to hold the soil, as shown in the photo to the right.\nEstablish a riparian buffer along the stream planted with native shrubs and trees. The buffer can be fenced to exclude livestock if necessary. This will also prevent soil loss and damage from erosion while protecting the stream with a living filter of plants.\nLandowners can contact the Conservation District to learn more about streambank protection on their property.\nLandowners wishing to improve their streambank on their own should contact the office for permitting needs. Permits are not required for plantings, but are needed for bank or in-stream work.\nPictured at left: Top - Root wad deflectors at the University of Pittsburgh at Bradford; a 2020 project\nBottom - Completed 2020 project at Three Miles Dairy with log-framed stone deflectors and riparian buffer planting; Phase 2 of two streambank stabilization projects on the property\nA watershed is an area of land over and through which water flows to the lowest point - a stream, river, wetland, lake, or ocean. Watersheds catch rain or snow melt and funnel water into flows across or under the landscape. They come in many different shapes and sizes. Watersheds can be hilly, mountainous, or nearly flat and can be comprised of many land uses including forests, farms, towns and cities. Most of McKean County is in the Allegheny River watershed, with a smaller portion in the southeast corner in the Susquehanna River watershed.\nProtecting our watersheds means clean water in our streams. Clean water in our streams means healthy drinking water in our homes, safe water activities, such as swimming and boating, in our backyards, and abundant wildlife and natural resources in and around our aquatic environments. Clean water is essential for life. Threats to our clean water do not follow political boundaries, but occur within watersheds.\nWhat is your watershed address?\nNo matter where we are, we are always in a watershed. Look at the following map of McKean County and determine your watershed address. A watershed address is the name of the watershed in which you live.\nWetlands are defined as areas where ground and/or surface water lingers for periods of time. In Pennsylvania, they are protected by both state and federal regulations and may also be protected by local regulations and ordinances. Wetlands are valuable because they function in ways that benefit the natural world, including the human community. Wetlands are important for wildlife, protection or water resources, and for water infiltration.\nAre wetlands present on my site?\nThis is a commonly asked question at the Conservation District. If a project or development is planned, a professional consultant MUST be hired for a wetlands delineation to determine if wetlands are present. The U.S. Fish & Wildlife Service has an online Wetlands Mapper for general use; but this is not definitive when delineation is required. For general information, not related to projects see the document on wetlands below.\nThere are two types of water resource pollution, Point Source and Nonpoint Source.\nPoint Source Pollution is a direct discharge into a river, stream, lake or pond. Examples of Point Source pollution include a sewage treatment plant or an industrial wastewater discharge.\nPoint Source Pollution is regulated by state and federal agencies to minimize the probability that water resources will be negatively impacted. One of the requirements for projects proposing to discharge wastewater is to get a National Pollution Discharge Elimination System (NPDES) permit. Development sites proposing earth disturbance must also have an approved Erosion and Sediment Control plan.\nNonpoint Source Pollution is pollution that enters our streams, lakes and ground water from indirect sources. An example would include stormwater runoff that can pick up sediments, nutrients, pesticides, herbicides, fertilizer, animal waste, petroleum, litter and all sorts of other things on its way to our waterways.\nWhat Can You Do?\nEveryone can help to minimize nonpoint source pollution. Just a few things you can do to protect water resources include the following:\nPONDS...Planning and Maintenance\nThe District Office can provide landowners a packet of helpful information regarding permit requirements, planning considerations, and maintenance for ponds. Call the office at 814-887-4001 and we can prepare a packet for you to pick up or be mailed to you. Resources have been compiled from partner agencies and have been distributed for the District pond workshop. Want to do your research online? Use these links for more information:\nPenn State Extension Water Resources\nPA Fish and Boat Commission', 'Florida Wetlands and Waters\nFree Permitting Checklist\nPractical Tips to Avoid Environmental Risk on all Your Projects\nDownload our environmental permitting checklist to get a step-by-step list of ways to protect your project from the 9 most common environmental risks.Download Your Checklist\nLearn everything you need to know about Florida waters and wetlands.\nWhy are Waters Defined Differently by State?\nIn the U.S., the Clean Water Act (CWA) protects waters. Jurisdictional Waters of the United States (WOTUS) are given extensive protection under this act. However, this act provides vague definitions for these waters, leading to the ambiguity of interpretations of these waters. States have added additional definitions to waters to gain clarity and protect waters that do not fall under the CWA jurisdiction as interpreted by different administrations.\nAdditionally, water use and needs vary by state—the different biogeographic regions in each state impact primary water sources and regulations are regionally specific. Protected waters serve as a wildlife habitat for many protected fish, plants, and other species that vary significantly by state. This variety of life and water use creates the need for state-specific regulations.\nFlorida Environmental Agency\nThe Florida Department of Environmental Protection protects the state\'s natural resources and enforces the state\'s environmental laws regarding water quality, air quality, conservation, and other state standards. This agency is divided into ecosystem restoration services, land and recreation, and regulatory programs. Additionally, the agency manages six district programs for local regulations throughout Florida. These statewide offices ensure developers uphold state and federal standards.\nFlorida Administrative Code\nThe Florida Administrative Code contains all the statewide rules and regulations. Within the code are all of Florida statutes and regulations around water.\nFlorida Surface Water Quality Standards\nThe Environmental Protection Agency (EPA), a federal agency, sets the Surface Water Quality Standards that the states uphold. These standards guide the regulations for surface waters, such as lakes, ponds, wetlands, and other waterways. Surface water quality is detrimental to the health of the environment and individuals. Therefore, extensive regulations and multi-level agencies work to protect these waters.\nWater Management Districts in Florida\nFlorida has five Water Management Districts. These districts maintain water quality and quantity, manage floodplains and flood protection, and manage natural resources. The state of Florida oversees these districts. The districts are the following:\nSouth Florida Water Management District\nSt. Johns River Water Management District\nSouthwest Florida Water Management District\nSuwannee River Water Management District\nNorthwest Florida Water Management District\nFlorida Rivers and Streams\nImportance of Florida Rivers and Streams\nThe streams, rivers, and waterways in Florida cover over 11,000 miles. These water bodies feed into other surface waters, impacting the environment\'s overall health. These waterways range from large rivers to temporary streams, requiring individual attention.\nProtection for Florida Rivers and Streams\nIn addition to federal and state water quality standards, the water districts mentioned above establish minimum flow and minimum water levels (MFLs) for rivers and streams. The state legislature shows a need for these levels.\nThese MFLs regulate how much water can be taken or withdrawn from the waterbody before impacting the wildlife and ecosystem. Though this is regulated, a concrete definition for ""significant harm"" is not provided when discussing the impacts. Each district has created regulatory criteria for establishing its MFLs. These criteria are based on scientific research of the water body and are peer-reviewed by independent scientists and agencies at the local, state, and occasionally federal levels. Developers should stay updated with these established levels and regulations when interacting with waters in their district.\nWetlands are a significant water storage source for Florida watersheds, with plants filtering pollutants from the waters. Additionally, these wetlands aid in flood protection for the state.\nWaterways that do not classify as federal wetlands may be under state wetland protection.\nFlorida Wetland Definition\nFlorida’s wetland definition is regions inundated or saturated by surface water or groundwater at a frequency and a period able to support, and under ordinary circumstances do support, a prevalence of vegetation adapted for life in saturated soils.\nThere is still debate over the state definition of ""wetland"" and ""delineation."" Rule 62-340 of the Florida Administrative Code provides a specific methodology for identifying Florida wetlands.\nFlorida Wetlands Protections\nThe Florida Environmental Resources Permit (ERP) Program is the standard for wetlands protection. This state regulation requires an ERP for developers in addition to those required by the federal government. When compared at the state level to the other Gulf Coast States, it is considered one of the most combative programs. The focus of this program is to ensure that negative impacts on water resources do not occur. These protections regulate water quantity and quality, and the ways wetland and surface water habitats serve fish and other wildlife.\nFlorida Wetlands Permits\nIn addition to obtaining a 404 permit, developers must also receive an ERP. This permit is regulated by the Florida Department of Environmental Protection (FLDEP) and five regional divisions, Water Management Districts (WMDs), which work jointly with the state. The United States Corps of Engineers, who independently process the permit, must approve some of these permits. The Corps cannot issue a 404 permit without a state permit.\nFree Permitting Checklist\nPractical Tips to Avoid Environmental Risk on all Your Projects\nOutstanding Florida Water\nThese waters hold special significance to the state, such as a vital source of drinking water, and have additional regulations. Outstanding Waters can be lakes, rivers, a spring, wildlife refuges, or national parks.\nThe Indian River Lagoon\nThe Indian River Lagoon is an estuary system home to over 2000 animal species. This area ranges across six counties on Florida\'s east coast. The health of these waters impacts the surrounding life and other waters, such as Lake Okeechobee and The Florida Everglades.\nIndian River Lagoon Act\nThis act protects the Indian River Lagoon System from pollution via wastewater discharge. This regulation protects the 156 miles of waters from pollutants harmful to the lagoon\'s biodiversity and surrounding and downstream waters.\nPermitting for the Indian River Lagoon\nDevelopers must obtain all permits required to comply with the federal Clean Water Act and the state water quality standards. Notably, developers must comply with the National Pollutant Discharge Elimination System (NPDES) stormwater permitting process. This federal permit is required to maintain the health of the Indian River Lagoon.\nFlorida Everglades Definition\nThe Florida everglades are a 2 million acre south Florida wetland water system that serves as a vast ecosystem and water resource for the state. The Everglades\' waters begin as overflow from Lake Okeechobee that travel across marsh-filled rivers, into the Everglades National Park, and eventually, deposit into the Florida Bay. Irrigation for most of the state and drinking water for about 33% of people come from the Everglades. These waters play a critical role to humans and wildlife alike. The Everglades are home to many species, including over 360 species of birds, crocodiles, and some of the 100 endangered Florida panthers left in the world. Human interaction and invasive species threaten the Everglades.\nEverglades Forever Act\nPassed in 1994, the Everglades Forever Act aims to ensure the water quality of the Everglades. This act regulates materials discharged into the Everglades and ensures the EPA and state water quality standards are upheld.\nComprehensive Everglades Restoration Plan\nThis plan is an effort by the federal and Florida state governments for Everglade restoration and protection. Congress authorized this restoration plan in 2000. Evaluations and changes to regulations for Everglades are to occur to meet restoration goals. The main focus of this plan is to regulate Lake Okeechobee\'s overflow. Rules will maintain water quality by managing the discharge of pollutants, such as excess phosphorus, into the waters.\nHow Do Protected Florida Waters Impact Land Developers?\nProtected waters fall under many federal, state, and local regulations. These waters can be challenging to identify, as in the case of temporary waters, and have consequences later in development. Delaying or incorrectly identifying waters can result in project delays and expensive recovery efforts.\nAdditionally, Florida water serves as a habitat for many protected species, such as fish and vegetation. Due to this, developments will be subject to other federal and state regulations, such as the Endangered Species Act. Additional rules may apply for individual wetlands, which can in project failure. Proper environmental due diligence aids in the identification of these waters and navigations of regulations.\nHow to Identify Protected Florida Waters\nEnvironmental consultants are experts trained to survey sites for potential environmental impacts. These consultants are experts in the federal and their respective region\'s regulations. The consultant can recommend necessary permits and steps to ensure environmental compliance if they find an environmental risk-such as protected water. This traditional process is completed manually by an environmental consultant over weeks or months.\nFlorida Department of Environmental Protection Open Data Administrator\nThis tool is a statewide mapping dataset composed of Geospatial Information Systems (GIS) Data regarding statewide protected waters available for public use. Though this data is frequently updated, it cannot provide site-specific information regarding protected waters and their required permits and next steps. This tool can be combined with other due diligence tools when selecting and developing a project site.\nNational Hydrography Dataset\nThis data depicts the nation\'s water drainage network. Software platforms use this data, which is available for download by the public. As the USFWS source, this tool does not outline permits needed, the jurisdiction of waters, and other regulations. Though these diligence tools are helpful, one of the easiest ways to identify wetlands is to use environmental due diligence software like Transect.\nTransect Protected Waters Mapping Tool\nTransect uses machine learning and integrated datasets, such as the National Hydrography Dataset, to automate the mapping of WOTUS. This software uses prior and current data about water locations to assess a specified region. The software provides the area and likelihood of regulated water appearing on a site in the jurisdiction of the water and includes a corresponding confidence level in its occurrence. Additionally, Transect will also provide a site-specific list of permits and next steps required to comply with federal, state, and local laws. This mapping tool can aid in selecting the right site for a project by generating this report in minutes.\nTransect Software helps land developers discover these waters on any given parcel of land.']"	['<urn:uuid:f10ecc5c-ec4f-40b7-aadd-bfaf4039be9a>', '<urn:uuid:249322b4-6f76-4107-bdbe-70b592442da3>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-13T05:06:54.527160	27	66	2610
98	What's the difference between basic and fancy goldfish varieties in terms of their physical features, and how does water cleanliness affect their health differently?	Basic single-tailed goldfish have torpedo-shaped bodies and one tail, closely resembling their wild carp ancestor, while fancy varieties have egg-shaped bodies, double tails, and various modified features like bubble eyes or head growths. Regarding water cleanliness, fancy goldfish are more sensitive and require pristine conditions due to their modified bodies, while single-tailed varieties are hardier. However, both types need water free from toxic chemicals, pesticides, and organic pollution, as these create conditions that encourage disease and parasites. High organic matter levels particularly affect fancy goldfish as they are slower swimmers and more susceptible to infections.	"['Looking to get into goldfish keeping? If you’ve never set up a goldfish tank before, the first thing that comes to mind is probably that typical torpedo-shaped orange fish you can win at the fair.\nLittle do many aquarists know that there is a whole world of goldfish (Carassius auratus) out there. Breeders have worked for over a thousand years to produce all sorts of selectively bred varieties. We’re not kidding here – this species was very popular in Ancient China.\nAs a result of all this, the amount of (fancy) goldfish varieties available today is extremely varied. It ranges from fish that are very similar to the ancestor of our modern day goldfish (the Prussian carp, Carassius gibelio) to varieties so far removed from it you can barely recognize them.\nInterested in finding out which types of goldfish you own or which variety would work best for aquarium? Keep reading for a description of 13 goldfish types, ranging from very popular to quite rare. Looking for general information on goldfish care instead? We included care tips for both fancy and common goldfish at the bottom of this article.\nSingle-tailed goldfish are so called because – you guessed it – they have a single tail. They are “non-fancy” goldfish, which means that physically they still resemble their Prussian carp ancestor pretty closely. Their bodies are slender without any extensive modifications aside from color and possibly a selectively bred long, flowy tail.\nThe most unassuming of all goldfish is the one every fishkeeper and non-fishkeeper has seen and likely even owned at one point. Common goldfish are your average orange, yellow or white childhood pet and also a common feeder fish.\nThis variety features an elongated body and a short, single tail. It can grow to a rather impressive adult size that many aquarists might not expect: the largest goldfish in the world was a whopping 19 inches. Although not all common goldfish grow this large, they still generally reach a size of at least around 10 inches if conditions are favorable.\nComet goldfish are another common single-tailed goldfish type. The difference between comets and common goldfish is in the tail. Commons have a short tail (also referred to as caudal fin), whereas comet goldfish feature a longer and flowier caudal. Quite a decorative sight when viewed from above in a pond!\nLike commons, comet goldfish can be found in orange, yellow or white. Additionally, you might also be able to find white comets with blotchy orange coloration. Comets, too, grow quite large and can reach sizes similar to those of common goldfish.\nShubunkin are a type of goldfish bred almost exclusively for their coloration. The “standard” version of this variety (known as the American Shubunkin) is almost identical to comet goldfish save for its calico pattern. Shubunkin’s spotted patterns appear in a range of colors including orange, blue, white and black. Pretty decorative, especially considering their mix of matte and shimmering scales.\nLooking for a Shubunkin that looks more similar to the classic common goldfish? You might like the short-finned London Shubunkin. For something a little more dramatic you can consider getting Bristol Shubunkin, which have long heart-shaped tails. Both of these sub-varieties will be more difficult to find, but you can try contacting other hobbyists to see if they’re selling any.\nThe Wakin goldfish is a bit of an odd duck. It almost seems half fancy and half non-fancy, with a double tail but an elongated body. It is suspected to be the ancestor of the fancy goldfish types we know today, but most sources categorize it as a common due to its body shape.\nLong-bodied goldfish like commons and Wakins grow larger than stockier fancies. Additionally, they are much more active swimmers. For this reason the Wakin is seen as a pond fish; in fact, in its country of origin you’ll likely see more Wakins in ponds than the commons we keep here.\n“Fancy goldfish” is a collective name for goldfish types that have been selectively bred to exhibit all sorts of physical traits that single-tails don’t. As a rule of thumb, all fancy goldfish have double tails. Additionally, they feature a different body shape than commons.\nRather than elongated and torpedo shaped, fancies have short and stocky bodies. They are generally slower swimmers and more sensitive. Most don’t respond well to very low temperatures and are best kept indoors. Don’t mix fancies and commons: the latter will outcompete the former for food, leaving the fancies hungry.\nFantail goldfish are probably the most common fancy goldfish type out there. They feature the standard fancy goldfish egg shape, an elongated back (dorsal) fin and most notably, a long and flowy tail. Fantails are considered the European version of the Ryukin goldfish (discussed below).\nIf you’re looking to get into fancy goldfish keeping, fantails might be a good choice. Unlike many of the more “extreme” fancy varieties that require extra care due to their body modifications, the fantail is quite hardy. Although it’s a little too slow to be kept with single-tails, it’s still quite agile and might actually outcompete slower fancies for food.\nRyukin goldfish are considered the Eastern version of the fantail. The difference is in the body shape: Ryukins are very tall goldfish with a significant shoulder hump right behind the head. In fact, this variety is often taller than it’s long! Unlike other fancies, Ryukins have a slightly pointed head. Their tail can be either short or long depending on the variety, with some Ryukins featuring a tail twice as long as the body.\nRyukin, like fantails, are known for being among the larger fancy goldfish types out there. If you’re interested in keeping them consider a larger aquarium than you might for other fancies. Or go for a pond; Ryukin are quite hardy and deal with lower temperatures relatively well.\nIf you’d like to keep Ryukin, keep in mind that they look very similar to fantails when young. As with many other fancy goldfish, their selectively bred traits (in this case the shoulder hump) aren’t very pronounced when the fish is still small. It isn’t until your Ryukin grows larger that it develops the typical tallness. Even then, the degree to which the shoulder hump grows depends on the quality of the fish and other factors like diet.\nWhen it comes to goldfish, the pearlscale is definitely one of the stranger types out there. This variety is oddly reminiscent of a golf ball, as it was selectively bred for an extremely round body and thick scaling.\nIf you’re looking into keeping pearlscale goldfish yourself, keep in mind that they’re not the best choice for those just starting out with fancies. Due to extensive selective breeding this variety isn’t always the hardiest, so you’ll have to provide pristine water quality at all times. Additionally, you might have to prepare yourself for issues with the fish’ swim bladder. Their body shape has been altered so extensively that in some cases the swim bladder fails to function correctly any more.\nBubble Eye Goldfish\nWe’re jumping straight to the most strange-eyed fancy goldfish out there. The bubble eye goldfish is quite appropriately named: this variety has been selectively bred to feature water-filled sacs under its eyes. These sacs grow as the fish does and can eventually reach a rather impressive size.\nIt goes without saying that bubble eye goldfish need specific care and aren’t a great beginner choice. You’ll have to bubble eye-proof your aquarium by removing any objects that might be sharp and placing a prefilter sponge around the filter intake. Their bubbles are very fragile and can actually pop when damaged. Although this doesn’t cause permanent eye damage and the bubble might regrow, there is always the risk of infection.\nAdditionally, be sure not to combine bubble eye goldfish with anything but other very slow fancy goldfish varieties. Faster ones, like Ryukin, will outcompete bubble eyes for food.\nTelescope Eye Goldfish\nOne of the most well-known fancy goldfish out there today is the telescope eye goldfish. As its name suggests, this variety is known for its round, protruding eyes. This variety is also commonly referred to as Demekin, its original Japanese name. Telescope eye goldfish come in several “subcategories”, including the popular black moor and sought-after panda telescope.\nAlthough telescope eyes are less fragile than bubble eyes you should still take great care to protect their eyes. Damage can result in complete loss of the eye and possibly infection. Always place a protector around your water change hose and use a prefilter sponge to avoid accidents.\nCelestial Eye Goldfish\nThe rather unusual celestial eye goldfish resembles a telescope eye and was selectively bred from a mutation of this variety. There is one clear difference that easily allows us to distinguish between the two, though. Celestial eye goldfish have eyes that point upwards rather than to the side, hence the name. Additionally, celestials lack a dorsal fin.\nThis variety has been around for a long time and its origin likely dates back to the 18th century. If you’d like to keep it in your aquarium, keep in mind that the celestial’s upturned eyes mean its vision isn’t exactly fantastic. Although these fish are quite active they should be protected from sharp objects and will not be able to compete for food with tankmates with better vision.\nOne of the more well-known fancy goldfish varieties is the Oranda. This goldfish was selectively bred for a fleshy mass on its head known as a wen, which some feel resembles a lion’s mane. Oranda fry start out their lives looking almost like a normal fantail goldfish, with the wen becoming more prominent as they age.\nOrandas are one of the hardier and faster fancy goldfish types out there, especially when their wen is still small. Do keep in mind that the headgrowth should be protected from damage to prevent infection. In some fish the wen might grow excessively, prompting goldfish enthusiasts to gently trim it with clean scissors. Obviously not something you should be attempting if you don’t know what you’re doing, but it is an option: the wen doesn’t contain any blood vessels.\nRanchu Goldfish & Lionhead Goldfish\nThese two fancy goldfish varieties are very easily confused. Both feature wens and lack a dorsal fin, but with some practice you should be able to see the difference. Lionheads will have a fuller headgrowth that appears to stick up from the head more and fuller cheeks. Additionally, their bodies are more elongated and the angle where the back joins the tail is less acute.\nAlthough lionheads and ranchus are two different varieties, their care is largely the same. Because they lack a dorsal fin these fish can be rather slow. Additionally, if the wen covers the eyes it can significantly impair their vision. All this means they won’t be able to compete for food with more “able-bodied” tankmates. In fact, they should ideally be kept in an aquarium containing only other ranchus and lionheads, although other slow varieties work as well.\nCommon Goldfish Housing\nIf you’re reading this article that probably means you’re interested in keeping (fancy) goldfish yourself – or you might already own your first one. We have to note that unfortunately, many goldfish end up in unsuitable homes. The myth that goldfish can be kept in bowls, vases and small tanks still seems to persist, which means many of these fish pass away prematurely.\nThe problem is greatest for common goldfish. Because a feeder goldfish sells for just a few cents and these fish even be won at your local fair, not many aspiring goldfish keepers actually look into what kind of care their new fish needs.\nChoosing the Right Tank Size\nAs we’ve discussed in the section on commons, these fish can reach quite a size. 10 inches is not uncommon for a well cared for goldfish. Most never grow to their full potential, however: a goldfish in a small bowl or tank will end up stunted in its growth. This is a mechanism developed by the wild Crucian carp in order to attempt to survive a little longer when the lake dries out or overcrowding occurs, so the fish can hopefully make it until conditions improve.\nThe fact that a goldfish in a small body of water stays much smaller is part of what spawned the myth that this species doesn’t need much room. In reality, a stunting is a sign of extremely unhealthy conditions. Not to mention the way ammonia and other harmful compounds build up in such a small amount of water!\nSo: we’ve established that a goldfish, whether common or fancy, will need a lot of room to thrive. If you were to want to keep common goldfish, what kind of environment would work, then? Much to the surprise of some, our answer is that you should only keep commons in a pond. These fish are so active, produce so much waste and grow so large that they need up to 100 gallons per fish to thrive and prevent stunting. A well-filtered pond of at least 200 gallons with at least two goldfish (they are social beings) is therefore your best choice. Temperature is not a problem, as commons, comets, shubunkin and wakin are winter-hardy.\nFancy Goldfish Housing\nAs we’ve discussed, fancy goldfish tend to stay smaller than commons. Additionally, they’re less active and most species aren’t winter-hardy. Does that mean they can be kept in bowls or a small tank? Still a no on that. Luckily, though, you can enjoy them indoors in an aquarium as long as you provide what they need.\nFancy goldfish keepers largely agree on a rule of thumb of 20 gallons of water per fancy goldfish, accompanied by what would be overfiltration for a normal fish tank. This means you’ll be looking at an aquarium of at least 40 gallons, given the fact that goldfish are group animals that look to their tankmates for safety in numbers and are clearly uncomfortable when kept alone.\nTop Notch Water Quality\nFor Ryukin and fantails it’s generally advised to go even larger than the 20 gallon rule. That being said, just try to provide as much room as possible. It’s hard to imagine how the small fancies being sold at pet stores can grow to the size of an orange, but they can! An adult fancy will make a 40 gallon aquarium look small. In fact, the author at one point provided 30 gallons per adult fancy goldfish and definitely felt the need to upgrade the aquarium as it still seemed very small.\nBecause goldfish produce so much waste and fancies are quite sensitive, you’ll have to watch the water quality like a hawk and stay on top of water changes. 50% water changes (or more!) twice a week are not uncommon for goldfish keepers!', 'SL10603 : SL10603 Water Management 1 Water quality and fish health : Water quality and fish health The single, most important factor affecting fish health and influencing disease in fish ponds and tanks is water quality.\nRaised levels of ammonia or nitrite, sub-optimum pH and water hardness levels or a high level of organic pollution will be stressful to fish; predisposing them to disease.\nIf we are to create healthy, optimum conditions and prevent disease, it is important to be clear what is actually meant by good water quality. 2 1. Low ammonia and nitrite : 1. Low ammonia and nitrite Fish are constantly polluting their own environment and producing ammonia.\nBoth ammonia and nitrite are highly dangerous, causing stress and physical damage to sensitive tissues. A major, major requirement of any fish keeping system is no detectable levels of either.\nThis particularly applies to new set-ups (new pond syndrome) and heavily stocked koi ponds. Biological filtration may be needed to maintain optimum levels. 3 2. Chemically clean water : 2. Chemically clean water The water should be chemically clean and free of chemicals such as pesticides, chlorine, heavy metals, organophosphates and chemicals used to treat fish diseases.\nThe presence of any toxic chemicals, even at fairly low levels, may be harmful. 4 3. Water hardness, pH and temperature : 3. Water hardness, pH and temperature Different species of fish have specific requirements for essential water parameters such as pH, water hardness, alkalinity and temperature.\nConditions outside of what are fairly narrow limits are liable to create stress.\nWater that fails to meet these criteria cannot for obvious reasons be considered good water quality 5 4. Low levels of organic pollution : 4. Low levels of organic pollution In addition to fish waste, the pond or tank is also being continuously polluted with uneaten food, algae and other detritus. As this organic matter decomposes it produces many organic and inorganic compounds.\nBiological filtration will take care of ammonia and nitrite, but there may be a build up of dissolved and particulate organic compounds.\nHigh levels of organics can create conditions that encourage disease, parasites and opportunistic bacteria. Water with high levels of organic matter cannot be considered good water quality. 6 5. Stability not fluctuation : 5. Stability not fluctuation Depending on the water chemistry, stocking levels and pond design, it is possible to have substantial fluctuations of pH, temperature and other parameters over a 24-hour period.\nConstant changes - even if they stay within the preferred range are liable to be extremely stressful, as the fish have to constantly adapt to changing conditions.\nAn example might be pH that varies between, 7 in the morning, rising to 9-10 in the evening on a hot sunny day. 7 Slide 8: Apart from stressing the fish, it will have other implications for other water chemistry aspects such as ammonia and many common disease treatments. Water that constantly fluctuates in quality and conditions cannot be said to be good water quality 8 Water Quality : Water Quality When a hatchery is on a reuse system, it is necessary to monitor many parameters to insure the water quality is optimum for fish survival 9 Slide 10: 10 Slide 11: Why Temperature Is Important\nHuman activities should not change water temperatures beyond natural seasonal fluctuations.\nTo do so could disrupt aquatic ecosystems. Good temperatures are dependent on the type of stream you are monitoring.\nLowland streams, known as ""warmwater"" streams, are different from mountian or spring fed streams that are normally cool. 11 Slide 12: In a warmwater stream temperatures should not exceed 89 degrees (Fahrenheit).\nCold water streams should not exceed 68 degrees (Fahrenheit).\nOften summer head can cause fish kills in ponds because high temperatures reduce available oxygen in the water. 12 Slide 13: Why pH Is Important\npH is a measure of the acidic or basic (alkaline) nature of a solution. The concentration of the hydrogen ion [H+] activity in a solution determines the pH. 13 Slide 14: Environmental Impact:\nA pH range of 6.0 to 9.0 appears to provide protection for the life of freshwater fish and bottom dwelling invertebrates\nRunoff from agricultural, domestic, and industrial areas may contain iron, aluminum, ammonia, mercury or other elements.\nThe pH of the water will determine the toxic effects, if any, of these substances.\nFor example, 4 mg/l of iron would not present a toxic effect at a pH of 4.8. However, as little as 0.9 mg/l of iron at a pH of 5.5 can cause fish to die. 14 Slide 15: Min. Max. Effects\n3.8 10.0 Fish eggs could be hatched, but deformed young are often produced\n4.0 10.1 Limits for the most resistant fish species\n4.1 9.5 Range tolerated by trout\n--- 4.3 Carp die in five days\n4.5 9.0 Trout eggs and larvae develop normally\n4.6 9.5 Limits for perch\n--- 5.0 Limits for stickleback fish\n5.0 9.0 Tolerable range for most fish\n--- 8.7 Upper limit for good fishing waters\n5.4 11.4 Fish avoid waters beyond these limits\n6.0 7.2 Optimum (best) range for fish eggs\n--- 1.0 Mosquito larvae are destroyed at this pH value\n3.3 4.7 Mosquito larvae live within this range\n7.5 8.4 Best range for the growth of algae 15 Slide 16: Why Chlorides Are Important\nChloride is a salt compound resulting from the combination of the gas chlorine and a metal. Some common chlorides include sodium chloride (NaCl) and magnesium chloride (MgCl2).\nChlorine alone as Cl2 is highly toxic, and it is often used as a disinfectant. In combination with a metal such as sodium it becomes essential for life. Small amounts of chlorides are required for normal cell functions in plant and animal life. 16 Slide 17: Environmental Impact:\nChlorides are not usually harmful to people; however, the sodium part of table salt has been linked to heart and kidney disease.\nSodium chloride may impart a salty taste at 250 mg/l; however, calcium or magnesium chloride are not usually detected by taste until levels of 1000 mg/l are reached.\nPublic drinking water standards require chloride levels not to exceed 250 mg/l. 17 Slide 18: Chlorides can corrode metals and affect the taste of food products. Therefore, water that is used in industry or processed for any use has a recommended maximum chloride level.\nChlorides can contaminate freshwater streams and lakes. Fish and aquatic communities cannot survive in high levels of chlorides 18 Slide 19: Why Dissolved Oxygen is Important\nDissolved oxygen analysis measures the amount of gaseous oxygen (O2) dissolved in an aqueous solution.\nOxygen gets into water by diffusion from the surrounding air, by aeration (rapid movement), and as a waste product of photosynthesis.\nWhen performing the dissolved oxygen test, only grab samples should be used, and the analysis should be performed immediately.\nTherefore, this is a field test that should be performed on site. 19 Slide 20: Environmental Impact:\nTotal dissolved gas concentrations in water should not exceed 110 percent.\nConcentrations above this level can be harmful to aquatic life.\nFish in waters containing excessive dissolved gases may suffer from ""gas bubble disease""; however, this is a very rare occurrence. 20 Slide 21: 21 Slide 22: The bubbles or emboli block the flow of blood through blood vessels causing death.\nExternal bubbles (emphysema) can also occur and be seen on fins, on skin and on other tissue.\nAquatic invertebrates are also affected by gas bubble disease but at levels higher than those lethal to fish. 22 Slide 23: Adequate dissolved oxygen is necessary for good water quality.\nOxygen is a necessary element to all forms of life.\nNatural stream purification processes require adequate oxygen levels in order to provide for aerobic life forms.\nAs dissolved oxygen levels in water drop below 5.0 mg/l, aquatic life is put under stress.\nThe lower the concentration, the greater the stress. Oxygen levels that remain below 1-2 mg/l for a few hours can result in large fish kills. 23 Slide 24: Why Total Iron Is Important\nIron is the fourth most abundant element, by weight, in the earth\'s crust.\nNatural waters contain variable amounts of iron despite its universal distribution and abundance.\nIron in groundwater is normally present in the ferrous or bivalent form [Fe++] which is a soluble state. 24 Slide 25: It is easily oxidized to ferric iron [Fe+++] or insoluble iron upon exposure to air.\nIron is a trace element required by both plants and animals. It is a vital oxygen transport mechanism in the blood of all vertebrate and some invertebrate animals. 25 Slide 26: Environmental Impact:\nIron in water may be present in varying quantities depending upon the geological area and other chemical components of the waterway.\nFerrous Fe++ and ferric Fe+++ ions are the primary forms of concern in the aquatic environment.\nThe ferrous form Fe++ can persist in water void of dissolved oxygen and usually originates from groundwater or mines that are pumped or drained. 26 Slide 27: Iron in domestic water supply systems stains laundry and porcelain.\nIt appears to be more of a nuisance than a potential health hazard.\nTaste thresholds of iron in water are 0.1 mg/l for ferrous iron and 0.2 mg/l ferric iron, giving a bitter or an astringent taste.\nBlack or brown swamp waters may contain iron concentrations of several mg/l in the presence or absence of dissolved oxygen, but this iron form has little effect on aquatic life. 27 Slide 28: Why Nitrate, Nitrite, and Nitrogen Are Important\nNitrogen is one of the most abundant elements. About 80 percent of the air we breath is nitrogen.\nIt is found in the cells of all living things and is a major component of proteins.\nInorganic nitrogen may exist in the free state as a gas N2, or as nitrate NO3-, nitrite NO2-, or ammonia NH3+.\nOrganic nitrogen is found in proteins and is continually recycled by plants and animals. 28 Slide 29: Slide 30: Environmental Impact:\nNitrogen-containing compounds act as nutrients in streams and rivers. Nitrate reactions [NO3-] in fresh water can cause oxygen depletion.\nThus, aquatic organisms depending on the supply of oxygen in the stream will die. 30 Slide 31: The major routes of entry of nitrogen into bodies of water are municipal and industrial wastewater, septic tanks, feed lot discharges, animal wastes (including birds and fish) and discharges from car exhausts. Bacteria in water quickly convert nitrites [NO2-] to nitrates [NO3-].\nNitrites can produce a serious condition in fish called ""brown blood disease.""\nNitrites also react directly with hemoglobin in human blood and other warm-blooded animals to produce methemoglobin. 31 Slide 32: Methemoglobin destroys the ability of red blood cells to transport oxygen.\nThis condition is especially serious in babies under three months of age.\nIt causes a condition known as methemoglobinemia or ""blue baby"" disease. Water with nitrite levels exceeding 1.0 mg/l should not be used for feeding babies.\nNitrite/nitrogen levels below 90 mg/l and nitrate levels below 0.5 mg/l seem to have no effect on warm water fish. 32 Slide 33: Why Phosphorus Is Important\nPhosphorus is one of the key elements necessary for growth of plants and animals.\nPhosphorus in elemental form is very toxic and is subject to bioaccumulation.\nPhosphates PO4--- are formed from this element. Phosphates exist in three forms: orthophosphate, metaphosphate (or polyphosphate) and organically bound phosphate.\nEach compound contains phosphorous in a different chemical formula. 33 Slide 34: Ortho forms are produced by natural processes and are found in sewage.\nPoly forms are used for treating boiler waters and in detergents.\nIn water, they change into the ortho form. Organic phosphates are important in nature.\nTheir occurrence may result from the breakdown of organic pesticides which contain phosphates.\nThey may exist in solution, as particles, loose fragments, or in the bodies of aquatic organisms. 34 Slide 35: Slide 36: Environmental Impact:\nRainfall can cause varying amounts of phosphates to wash from farm soils into nearby waterways.\nPhosphate will stimulate the growth of plankton and aquatic plants which provide food for fish.\nThis increased growth may cause an increase in the fish population and improve the overall water quality.\nHowever, if an excess of phosphate enters the waterway, algae and aquatic plants will grow wildly, choke up the waterway and use up large amounts of oxygen. 36 Slide 37: This condition is known as eutrophication or over-fertilization of receiving waters.\nThe rapid growth of aquatic vegetation can cause the death and decay of vegetation and aquatic life because of the decrease in dissolved oxygen levels.\nPhosphates are not toxic to people or animals unless they are present in very high levels.\nDigestive problems could occur from extremely high levels of phosphate. 37 Slide 38: 38 Slide 39: Why Fecal Coliform Testing Is Important\nTotal coliform bacteria are a collection of relatively harmless microorganisms that live in large numbers in the intestines of man and warm- and cold-blooded animals.\nThey aid in the digestion of food. A specific subgroup of this collection is the fecal coliform bacteria, the most common member being Escherichia coli.\nThese organisms may be separated from the total coliform group by their ability to grow at elevated temperatures and are associated only with the fecal material of warm-blooded animals. 39 Slide 40: Environmental Impact:\nThe presence of fecal coliform bacteria in aquatic environments indicates that the water has been contaminated with the fecal material of man or other animals.\nAt the time this occurred, the source water may have been contaminated by pathogens or disease producing bacteria or viruses which can also exist in fecal material.\nSome waterborne pathogenic diseases include typhoid fever, viral and bacterial gastroenteritis and hepatitis A. 40 Slide 41: The presence of fecal contamination is an indicator that a potential health risk exists for individuals exposed to this water.\nFecal coliform bacteria may occur in ambient water as a result of the overflow of domestic sewage or nonpoint sources of human and animal waste. 41 Slide 42: 42']"	['<urn:uuid:91c0d81f-f736-4172-9af7-f6e0a504be8d>', '<urn:uuid:9edc0dde-e144-4840-b6ce-a6deb2d0a2f6>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T05:06:54.527160	24	95	4797
99	cell therapy expert here how does hydroxyapatite scaffold architecture affect bone formation and cell migration properties	The architecture of hydroxyapatite scaffolds affects bone formation through concavities that promote spontaneous bone formation in direct contact with the hydroxyapatite substratum, while also enabling capillary invasion. Regarding cell migration, macroporous silk-functionalized hydroxyapatite scaffolds enhance initial cell adhesion and significantly improve cell migration into the scaffold compared to macroporous hydroxyapatite alone. The biomimetic features of hydroxyapatite combined with RGD-motifs in silk templates promote mesenchymal stem cell adhesion, proliferation, migration and differentiation.	"['such as ""Introduction"", ""Conclusion""..etc\nFig. 1 Tissue induction and morphogenesis by bone morphogenetic/osteogenic proteins (BMPs/OPs). (A) Islands of chondrogenesis with vascular invasion and osteoblastic differentiation and matrix synthesis as a recapitulation of embryonic development, 8 days after heterotopic implantation in a Long-Evans rat of 2.5µg of recombinant hOP-1 in conjunction with 25 mg of bovine insoluble collagenous bone matrix as carrier (original magnification x45). (B) Bone induction by naturally-derived BMPs/OPs purified from baboon bone matrix and implanted orthotopically in a calvarial defect of an adult baboon Papio ursinus: 30 days after implantation of 280µg of baboon osteogenic fractions after gel filtration chromatography there is induction of large osteoid seams in orange-red surfacing newly developed mineralized bone matrix in blue (undecalcified section, original magnification x25). (C) Low-power view of a corticalized ossicle induced after the implantation of 125µg of recombinant hTGF-β in the rectus abdominis muscle of an adult primate Papio ursinus showing vigorous osteogenesis with osteoid synthesis 30 days after heterotopic implantation (undecalcified section, original magnification x4.5).\nFig. 2 Morphology of calvarial regeneration by recombinant hTGF-β2 in conjunction with collagenous matrix as carrier. Low-power photomicrographs of calvarial defects treated by 100µg hTGF-β2 delivered by insoluble collagenous bone matrix and harvested on day 30 (A) and 90 (B) after implantation in non-healing calvarial defects of the primate Papio ursinus. Minimal bone formation at the edges of the defect on day 30 (A) and osteogenesis albeit limited is found in a specimen harvested 90 days after implantation with bone formation only pericranially. Note the trabeculae of newly formed bone facing scattered remnants of collagenous matrix particles, embedded in a loose but highly vascular connective tissue matrix (undecalcified sections, original magnification x3).\nFig. 3 Northern analyses of type IV collagen and Smad6 (A) and OP-1, BMP-3, TGF-β1 and type II and IV collagens mRNA expression (B) in tissues generated by rhTGF-β alone (A) and in ossicles generated by doses of porcine platelet-derived TGF-β1 alone and in combination with 25ìg hOP-1 implanted heterotopically with insoluble collagenous bone matrix as carrier in the rectus abdominis muscle of adult primates Papio ursinus and harvested on day 30. Minimal mRNA expression of Smad6 in ossicles generated by rhTGF-β when implanted heterotopically (A). (B) Upon implantation of 5 µg doses of recombinant hTGF-β1 there is expression of osteogenic markers of the TGF-β superfamily, namely OP-1 and BMP-3 gene products. Note the two to three fold increase of expression of collagen type IV mRNA, a marker of angiogenesis, in ossicles generated by TGF-β1 alone and in synergistic binary application with hOP-1 (B).\nFig. 4 Effect of the geometry of the substratum of biomimetic matrices on tissue morphogenesis and bone induction on day 30. (A) Capillary sprouting, invasion and elongation of the capillaries within a concavity of a biomimetic matrix of highly crystalline hidroxyapatite implanted heterotopically in the rectus abdominis muscle of Papio ursinus and harvested 30 days after implantation. Top right note the newly formed bone in direct contact with the hydroxyapatite substratum (decalcified section, original magnification x45). (B) Spontaneous initiation of bone formation within a concavity of the biomimetic matrix without the addition of exogenously applied BMPs/OPs. The newly formed bone in light blue in direct contact with the hydroxyapatite substratum is surfaced by contiguous osteoblasts (decalcified section, original magnification x60). (C) Detail of another specimen showing the spontaneous initiation of bone formation in direct contact to the crystalline hydroxyapatite together with capillary invasion within the fibrovascular tissue invading the concavity (decalcified section, original magnification x60).\nFig. 5 Tissue morphogenesis in concavities of the substratum 90 days after heterotopic implantation in the rectus abdominis muscle of the primate Papio ursinus. Low power view of a histological section of a monolithic disc of highly crystalline hydroxyapatite: bone has formed only within the concavities prepared on both planar outer surfaces (decalcified section, original magnification x12).\nSource: J. Cell. Mol. Med. Vol 8, No 2, 2004 pp. 169-180\nEnter the code exactly as it appears. All letters are case insensitive, there is no zero.', ""by Keyword: Hydroxyapatite scaffolds\nWidhe, M, Diez-Escudero, A, Liu, YL, Ringstrom, N, Ginebra, MP, Persson, C, Hedhammar, M, Mestres, G, (2022). Functionalized silk promotes cell migration into calcium phosphate cements by providing macropores and cell adhesion motifs Ceramics International 48, 31449-31460\nCalcium phosphate cements (CPCs) are attractive synthetic bone grafts as they possess osteoconductive and osteoinductive properties. Their biomimetic synthesis grants them an intrinsic nano-and microporosity that resembles natural bone and is paramount for biological processes such as protein adhesion, which can later enhance cell adhesion. However, a main limitation of CPCs is the lack of macroporosity, which is crucial to allow cell colonization throughout the scaffold. Moreover, CPCs lack specific motifs to guide cell interactions through their membrane proteins. In this study, we explore a strategy targeting simultaneously both macroporosity and cell binding motifs within CPCs by the use of recombinant silk. A silk protein functionalized with the cell binding motif RGD serves as foaming template of CPCs to achieve biomimetic hydroxyapatite (HA) scaffolds with multiscale porosity. The synergies of RGD-motifs in the silk macroporous template and the biomimetic features of HA are explored for their potential to enhance mesenchymal stem cell adhesion, proliferation, migration and differentiation. Macroporous Silk-HA scaffolds improve initial cell adhesion compared to a macroporous HA in the absence of silk, and importantly, the presence of silk greatly enhances cell migration into the scaffold. Additionally, cell proliferation and osteogenic differentiation are achieved in the scaffolds.\nJTD Keywords: Bioceramics, Bone, Bone regeneration, Composites, Degradation, Fabrication, Hydroxyapatite, Hydroxyapatite scaffolds, Injectability, Porosity, Recombinant spider silk, Rgd motifs, Silk, Stem-cells\nMochi F, Scatena E, Rodriguez D, Ginebra MP, Del Gaudio C, (2022). Scaffold-based bone tissue engineering in microgravity: potential, concerns and implications Npj Microgravity 8, 45\nOne of humanity's greatest challenges is space exploration, which requires an in-depth analysis of the data continuously collected as a necessary input to fill technological gaps and move forward in several research sectors. Focusing on space crew healthcare, a critical issue to be addressed is tissue regeneration in extreme conditions. In general, it represents one of the hottest and most compelling goals of the scientific community and the development of suitable therapeutic strategies for the space environment is an urgent need for the safe planning of future long-term manned space missions. Osteopenia is a commonly diagnosed disease in astronauts due to the physiological adaptation to altered gravity conditions. In order to find specific solutions to bone damage in a reduced gravity environment, bone tissue engineering is gaining a growing interest. With the aim to critically investigate this topic, the here presented review reports and discusses bone tissue engineering scenarios in microgravity, from scaffolding to bioreactors. The literature analysis allowed to underline several key points, such as the need for (i) biomimetic composite scaffolds to better mimic the natural microarchitecture of bone tissue, (ii) uniform simulated microgravity levels for standardized experimental protocols to expose biological materials to the same testing conditions, and (iii) improved access to real microgravity for scientific research projects, supported by the so-called democratization of space.© 2022. The Author(s).\nJTD Keywords: biomaterials, collagen/hydroxyapatite, composite scaffolds, in-vitro, mineralization, proliferation, regenerative medicine, stem-cells, vivo, Hydroxyapatite scaffolds\nBrennan M, Monahan DS, Brulin B, Gallinetti S, Humbert P, Tringides C, Canal C, Ginebra MP, Layrolle P, (2021). Biomimetic versus sintered macroporous calcium phosphate scaffolds enhanced bone regeneration and human mesenchymal stromal cell engraftment in calvarial defects Acta Biomaterialia 135, 689-704\nIn contrast to sintered calcium phosphates (CaPs) commonly employed as scaffolds to deliver mesenchymal stromal cells (MSCs) targeting bone repair, low temperature setting conditions of calcium deficient hydroxyapatite (CDHA) yield biomimetic topology with high specific surface area. In this study, the healing capacity of CDHA administering MSCs to bone defects is evaluated for the first time and compared with sintered beta-tricalcium phosphate (β-TCP) constructs sharing the same interconnected macroporosity. Xeno-free expanded human bone marrow MSCs attached to the surface of the hydrophobic β-TCP constructs, while infiltrating the pores of the hydrophilic CDHA. Implantation of MSCs on CaPs for 8 weeks in calvaria defects of nude mice exhibited complete healing, with bone formation aligned along the periphery of β-TCP, and conversely distributed within the pores of CDHA. Human monocyte-osteoclast differentiation was inhibited in vitro by direct culture on CDHA compared to β-TCP biomaterials and indirectly by administration of MSC-conditioned media generated on CDHA, while MSCs increased osteoclastogenesis in both CaPs in vivo. MSC engraftment was significantly higher in CDHA constructs, and also correlated positively with bone in-growth in scaffolds. These findings demonstrate that biomimetic CDHA are favorable carriers for MSC therapies and should be explored further towards clinical bone regeneration strategies. Statement of significance: Delivery of mesenchymal stromal cells (MSCs) on calcium phosphate (CaP) biomaterials enhances reconstruction of bone defects. Traditional CaPs are produced at high temperature, but calcium deficient hydroxyapatite (CDHA) prepared at room temperature yields a surface structure more similar to native bone mineral. The objective of this study was to compare the capacity of biomimetic CDHA scaffolds with sintered β-TCP scaffolds for bone repair mediated by MSCs for the first time. In vitro, greater cell infiltration occurred in CDHA scaffolds and following 8 weeks in vivo, MSC engraftment was higher in CDHA compared to β-TCP, as was bone in-growth. These findings demonstrate the impact of material features such as surface structure, and highlight that CDHA should be explored towards clinical bone regeneration strategies.\nJTD Keywords: beta-tricalcium phosphate, bone regeneration, calcium deficient hydroxyapatite, differentiation, engraftment, human bone marrow mesenchymal stromal cells, hydroxyapatite scaffolds, in-vitro, inhibition, osteogenesis, osteoinduction, stem-cells, surface-topography, tissue, Beta-tricalcium phosphate, Bone regeneration, Calcium deficient hydroxyapatite, Engraftment, Human bone marrow mesenchymal stromal cells""]"	['<urn:uuid:abb7759f-f779-48a5-b373-913c1f4f4407>', '<urn:uuid:36293126-a332-412c-9d83-93affa82e9a9>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T05:06:54.527160	16	71	1592
100	what causes sparkly color patterns on chalcopyrite	Chalcopyrite is a mineral that can exhibit iridescence, which is the production of a rainbow of colors caused by interference of light in thin films of different refractive indices and varying thickness, similar to an oil sheen on water.	"['|The Mineral Identification Key|\nColor is often a double-edged sword in mineral identification: There are many minerals which have distinctive colors; but there are also many which come in a variety of hues. And the same color can be seen in several different species. So one needs to use color as a criteria with care. That ""malachite-green"" mineral may not be malachite… That brass-yellow metallic mineral may not be pyrite… It is always a good idea to try and get a powdered streak from any colored mineral and compare it with descriptions of the streaks for the likely suspects you have in mind. It is also always wise to consider the habit of the mineral in conjunction with the color. A green prismatic crystal with a hexagonal cross-section is more likely to be elbaite than malachite. A brassy wedge-shaped crystal is more likely to be chalcopyrite than pyrite.\nColor, in general, should never be taken as diagnostic by itself. While it may be for certain species, more likely than not it isn’t. Else the job of mineral identification would be made easy.\nPlay of Color can be more helpful than the color itself. Characteristics such as opalescence, iridescence, chatoyancy and asterism are peculiar to a limited number of species, or varieties of species.\nOpalescence, as the word suggests, refers to an opal-like play of light, reflections off the mineral producing flashes of color that may appear somewhat like a patch-work of different ""grains"" of color that aren’t really there: Move the sample minutely and the color disappears from that spot. It is sort of like taking a pearly luster to the nth degree.\nIridescence Iridescence is the production of a rainbow of colors caused by interference of light in thin films of different refractive indices and varying thickness (the oil sheen on water is an example of this). Minerals with metallic luster, such as bornite and chalcopyrite are good examples of minerals which exhibit iridescence. A couple of cases where it does not involve metallic minerals are the way fracture surfaces in quartz may show it, and the way some fluorite crystals may exhibit it on the surfaces of their faces. Another exception is the type of iridescence known as labradoresence or schiller, found in labradorite and a very few other minerals.\nChatoyancy is the play of light off closely packed parallel fibers or parallel inclusions in cavities. The light reflects along lines – which may be straight or curved – giving the mineral a somewhat silky appearance. This characteristic is seen in such minerals as ""satin spar"" gypsum, ""tiger’s eye"" (fibrous crocidolite replaced by quartz), and chrysoberyl.\nAsterism is a type of chatoyancy in which the fibers or inclusions reflecting the light are arranged in a pattern radiating outwards from a point producing a star-like pattern. This is most often seen in rubies and sapphires, and sometimes in phlogopite mica that has rutile inclusions.\nLuminescence is the emission of light by a mineral other than the reflected light of the sun or a lamp – the mineral ""glows"" due to some other reason. The usual reason is reaction to ultraviolet light, though X-rays and electrons may produce it as well. The types of luminescence seen in minerals are fluorescence and phosphorescence – two closely related phenomena. Fluorescence results from electrons orbiting the mineral’s atoms being excited by ultraviolet light; the electrons ""absorb"" the energy and jump to higher orbits, then fall back to their original orbits – giving off light in the visible spectrum as they do. Phosphorescence is basically the same thing, but continues for a time after the source of excitation is removed, giving off energy as visible light more slowly. The fact is that most fluorescent minerals exhibit phosphorescence to some extent, though it usually can only be seen under careful lab conditions. Only a very few minerals phosphoresce well enough to see in a simple darkened room, and the phenomenon is usually rather short lived.\nFluorescence is a useful field identification tool for collectors who have UV lights (and a thick blanket when in the field) Many localities have at least a couple of fluorescent minerals, and some – like Franklin/Ogdensburg, New Jersey, USA – have a wealth of them. Where they are present, the UV light can be put to use in identifying them. Some animals such as scorpions also will fluoresce so be careful when picking up a fluorescent object in the field.\n[ Table of Contents ] [ Introduction ] [ Identification Kit ] [ Mineral Properties ] [ Environments & Associations ] [ In Conclusion ] [ The Mineral ID Key ]']"	['<urn:uuid:65397d6c-8c20-4475-9f64-453401e9501b>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T05:06:54.527160	7	39	771
