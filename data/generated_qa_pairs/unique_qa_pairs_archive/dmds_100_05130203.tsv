qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	I'm worried about my teen drinking. How can family dinners help?	Regular family dinners significantly reduce the chances of teens engaging in risky behaviors. Research shows that teens who share meals with parents at least five days per week are one-third less likely to drink alcohol. Additionally, these teens are half as likely to smoke cigarettes and marijuana, and 40% more likely to say they will never use drugs in the future. In contrast, kids who have fewer than three family meals per week are more than twice as likely to get drunk at least once per month.	"['Why TV and Dinner Should Be Forever Separate\nParents are always searching for ""cures"" for our kids, aren’t we? A cure for bad grades, bad friends, or perhaps even more serious issues like involvement in drugs.\nWhat if I told you there was a cure—a simple, easy and cheap way of dramatically reducing the chances of your kids ending up in serious trouble with the law and maybe even winding up on their school honor roll instead?\nThe solution? Eating dinner together. Granted, you can’t do this just once and have everything get better, but solid research headed up by Columbia University for 11 years is showing families who eat dinner together, at least five days per week, enjoy many benefits.\nFor example, teens who share mealtimes with parents on a frequent basis are…\nOne-third less likely to drink alcohol.\nHalf as likely to smoke cigarettes and marijuana\nForty percent likelier to say, ""Future drug use will never happen.""\nAlthough not quite as dramatic, a statistically significant higher number of studied teens who frequently eat dinner with their parents have either all A’s or A’s and B’s in school as opposed to those who do not regularly eat with their parents.\nTurned the other way around, kids in families who have fewer than three meals together per week are…\nTwice as likely to say they smoke at least one cigarette per day.\nMore than twice as likely to say they get drunk at least once per month.\nLikewise, parents of children who have fewer than three dinners together per week are…\nFive times more likely to say they have a ""fair"" or ""poor"" relationship with their teen.\nOne and a half times more likely to not know their teen’s friends or only know them ""not very well.""\nTwice as likely to say they deserve ""not very much blame"" or ""no blame"" at all when their teens use illegal drugs.\nFor many Californians, this is not news, with Governor Schwarzenegger’s wife Maria Shriver declaring September 25 Family Day as ""A Day to Eat Dinner With Your Children."" For the rest of the country, there is a valuable lesson to learn from California’s good example and Columbia University’s research.\nHowever, as easy as it sounds to simply enjoy a dinner together, researchers have also discovered the main reasons why it’s not happening in so many homes—and interestingly the top reasons given by teens and parents contradict each other.\nTeens say they aren’t eating with parents because one or both parents work late. Parents place that reason as number three on their list, and instead say it is due to conflicting schedules with their teens that prevent them from sharing dinners together. (Teens listed ""conflicting schedules"" as the number three reason.) Both groups agreed on the second reason—simply being ""too busy.""\nFurther down the list, but still coming in at sixth place on the teens’ reasons for separate eating is ""watching TV."" The distracting nature of television has long been a concern for sociologists who research family communications, and once again it is proven to be a contributing factor in preventing families from sharing time together. (I also recently reported on another study that indicates how TV can alter dietary behavior in children.)\nWhile Columbia University appears to be the leader in this research area, other studies have noted similar benefits to having families eat dinner together. A few of these include:\nThe more you eat meals together, the more likely adolescents are eating fruits, vegetables, grains and calcium-rich foods and the less likely they are consuming soft drinks. (Neumark-Sztainer)\nA similar correlation was noted at Brigham Young University, which also discovered college students were more likely to prepare healthy meals if they were exposed to meal preparation in their homes.\nAn Iowa State University study revealed that significant bonding takes place at a dinner table and ""when families eat dinner together, parents can teach children table manners, social skills, family values, a sense of community and cooking skills."" A study in Spain confirms the reverse: Children from families that don’t eat together frequently shared less activities, practiced less family rituals, and showed a lower level of satisfaction with family functions.\nFortunately, there is good news. Columbia University’s research indicates slightly more families are eating together now than in years past. In 1998, only 47 percent of teens reported having frequent family meals (five dinners per week or more) versus 58 percent today.\nIf you want to improve your frequency of eating dinner together, or want to make meals even more effective, here are some further tips published by Ohio State University:\nSet a pattern of family dinners when the children are young so it becomes a habit.\nCheck your schedules often and make family meals a priority. These meals may be breakfast, lunch or dinner; eaten at home or in a restaurant. The key is to spend the time together.\nTurn off the television, radio, cell phones and other distractions and allow time for tuning into each other.\nAllow every member of the family to contribute to the conversation and keep the conversations polite. The dinner table is not the place for conflict or discipline.\nListen to other family members’ thoughts and views. Get to know what they think and feel about a variety of topics, not just what happened to them during the day. An occasional conversation starter may be needed.\nGet every family member involved in family meals whether it’s planning the menus, shopping for groceries, setting the table, chopping the vegetables, or doing the dishes.\nRemember, parents serve as role models for healthy eating.\nIn our family, we occasionally have had to move our dinner mealtime to accommodate school schedules and working times for our 18-year-old son. The inconvenience has been more than made up by the benefits we have enjoyed. Although none of the Gustafson’s has a problem talking and stating their opinion (we compare our conversations with merging onto a busy LA freeway), I’ve often wondered if that would have been the case had we not made eating a meal together each day a priority.\nFor more information on this subject:']"	['<urn:uuid:b2b508c3-f912-408f-a5b2-4b36f99715fd>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T02:03:54.148118	11	87	1024
2	I'm interested in comparing breeding times of birds. Can you tell me how the breeding seasons of the Lark Sparrow and Comoro Fody compare in terms of their timing?	The Lark Sparrow and Comoro Fody have different breeding seasons. The Lark Sparrow's breeding activities occur in late spring and early summer, with copulation from May 17-27, nest building from May 14-31, eggs from May 19 to July 1, nestlings from May 31 to June 29, and fledglings from June 30 to July 12. The Comoro Fody, on the other hand, breeds from October to March on Mayotte.	"['Chondestes grammacus grammacus, C. g. strigatus\nStatus: Common regular breeder and spring and fall migrant statewide.\nDocumentation: Specimen: strigatus, UNSM ZM7348, 16 Jun 1901 Badlands, Sioux Co; grammacus, UNSM7353, 16 May 1914 Cass Co.\nTaxonomy: Two subspecies are recognized (Pyle 1997): strigatus, breeding from British Columbia to Manitoba south to California and west Texas, wintering California to Florida, and grammacus (including quillini), breeding from Minnesota and east Texas east to Pennsylvania and Georgia, wintering from southeast Texas to Florida.\nAccording to Bruner et al (1904), grammacus breeds in eastern Nebraska west to the 98th meridian [just east of Grand Island], while birds breeding elsewhere in the state are strigatus. Swenk (Notes Before 1925) noted that specimens taken in the Kearney, Buffalo Co area were mostly strigatus. This suggests that grammacus is the breeding race of the original tallgrass prairie area, while the rest of the state is occupied by strigatus. It is unknown whether recent increasing numbers in the east are grammacus or strigatus, or intergrades.\nNebraska breeders within the range of strigatus have been separated as quillini, but differences are slight, and this taxon is not currently recognized; these birds appear to be intergrades with grammacus (Pyle 1997). Indeed, it is likely that most birds in central Nebraska are intergrades.\nSpring: Apr 11, 12, 12 <<<>>> summer\nEarlier dates are 23 Mar 2009 Hall Co, 1 Apr 2016 Richardson Co, 5 Apr 2016 Rock Co, 8 Apr 2013 Douglas Co, and 8 Apr 2013 Saunders Co.\nMigrants usually arrive in mid-Apr.\n- High counts: 900 in Chase Co around 1 May 1999, 155 at Calamus Reservoir, Loup Co 7 May 2005, 130 in East Ash Canyon, Dawes Co 17 May 2017, and 120 at Fort Robinson SHP, Dawes Co 20 May 2015.\nSummer: Lark Sparrow breeds statewide and is “our most common sparrow” in the summer breeding season (Mollhoff 2016). They can be abundant in the Sandhills, particularly in pastures where low shrubs such as yucca are found. Numbers have increased in the east since 2001 (Mollhoff 2001, 2016), where it is now as common as in most of the rest of the state. Jorgensen (2012) stated that there were no breeding records in the eastern Rainwater Basin; there were no confirmed breeding records in that area 2006-2011 (Mollhoff 2016).\n- Breeding phenology:\n- Copulation: 17-27 May\nNest building: 14-31 May\nEggs: 19 May-1 Jul\nNestlings: 31 May-29 Jun\nFledglings: 30 Jun-12 Jul\n- High counts: 130 in the Lake McConaughy, Keith Co area 22 Jun 2002 and 118 at Crescent Lake NWR, Garden Co 26 Jul 1995.\nFall: summer <<<>>> Oct 18, 20, 21\nLater dates are 24 Oct 2019 Gage Co, 25 Oct 2011 Otoe Co, 30 Oct 2020 Lancaster Co, 31 Oct 2020 Lancaster Co, and 4 Nov 2008 Otoe Co. Very late was one photographed in Saunders Co 14-15 Dec 2020 (eBird.org).\nSummering birds departed James Ranch, Sioux Co, 21 Aug 1995; adults may depart breeding grounds as early as mid-Jul or Aug (Rosenberg et al 1991, Martin and Parrish 2020). Migration ends by mid-Oct.\nThere are two CBC reports without documentation, likely mis-identifications. There is only one documented Dec or winter record, although two were reported in South Dakota at the Gavin’s Point Dam area 12 Feb 2011.\n- High counts: 89 in southwest Kimball Co 2 Sep 1999, 50 in Dawes Co 8 Aug 2006, 50 in East Ash Creek Canyon, Dawes Co 5 Sep 2015, and 50 at Crescent Lake NWR 15 Aug 2015.\nCBC: Christmas Bird Count\nNWR: National Wildlife Refuge\nSHP: State Historical Park\nUNSM: University of Nebraska State Museum\nPhotograph (top) of a Lark Sparrow at LaPlatte Bottoms, Sarpy Co 4 Jun 2007 by Phil Swanson.\nBruner, L., R.H. Wolcott, and M.H. Swenk. 1904. A preliminary review of the birds of Nebraska, with synopses. Klopp and Bartlett, Omaha, Nebraska, USA.\nJorgensen, J.G. 2012. Birds of the Rainwater Basin, Nebraska. Nebraska Game and Parks Commission, Lincoln, Nebraska, USA.\nMartin, J.W. and J.R. Parrish. 2020. Lark Sparrow (Chondestes grammacus), version 1.0. In Birds of the World (A. F. Poole and F. B. Gill, Editors). Cornell Lab of Ornithology, Ithaca, NY, USA. https://doi.org/10.2173/bow.larspa.01.\nMollhoff, W.J. 2001. The Nebraska Breeding Bird Atlas 1984-1989. Nebraska Ornithologists’ Union Occasional Papers No. 7. Nebraska Game and Parks Commission, Lincoln, Nebraska, USA.\nMollhoff, W.J. 2016. The Second Nebraska Breeding Bird Atlas. Bull. Univ. Nebraska State Museum Vol 29. University of Nebraska State Museum, Lincoln, Nebraska, USA.\nPyle, P. 1997. Identification Guide to North American Birds. Part I, Columbidae to Ploceidae. Slate Creek Press, Bolinas, California, USA.\nRosenberg, K.V., R.D. Ohmart, W.C. Hunter, and B.W. Anderson. 1991. Birds of the lower Colorado River Valley. University of Arizona Press, Tuscon, Arizona, USA.\nSwenk, M.H. Notes before 1925. Bird notes from A.M. Brooking of Hastings, C.A. Black of Kearney, and B.J. Olson of Kearney, based chiefly on their collections, up to January 1, 1925. Typed manuscript in the Nebraska Ornithologists’ Union Archives, University of Nebraska State Museum, Lincoln, Nebraska, USA.\nSilcock, W.R., and J.G. Jorgensen. 2021. Lark Sparrow (Chondestes grammacus). In Birds of Nebraska — Online. www.BirdsofNebraska.org\nBirds of Nebraska – Online\nUpdated 11 Jun 2021', 'Weaver speciesChoose different species from drop-down list and press \'Go\' button. See Full species list.\nComoro Fody Foudia eminentissimaIUCN: Least concern Discovery: 057\nIntroductionThe Comoro Fody was formally described by Charles Lucien Bonaparte, a French biologist and ornithologist. Around 1849 Bonaparte began work on preparing a classification of all the birds in the world, visiting museums across Europe to study the collections. In 1850, he published the first volume of his Conspectus Generum Avium which included 3 weavers. Bonaparte studied the type specimen of the Comoro Fody in the Paris Museum.\nThe Comoro Fody was collected by Loius Rosseau. Rousseau travelled the western Indian Ocean on the French naval corvettes Prevoyante and Dordogne in 1839-1841. The Prevoyante visited Mayotte in 1840 and 1841 and the Dordogne visited Mayotte and Zanzibar at least in 1840 (Cheke 2011). The Comoro Fody may have been introduced to Zanzibar, and then collected there by Rousseau, or Rousseau mis-labelled his specimen.\nThe Comoro Fody differs slightly in measurements and plumage on the different islands of the Comores, and Benson (1960) showed that the type specimen matches the subspecies on Moheli Island.\nThe first illustration of a Comoro Fody is by Verreaux (1867) of the nominate subspecies. The next illustration was published the following year by Pollen (1868), but of subspecies algondae. Reichenbach (1863) mentioned the species, and provided an English name, but did not illustrate it.\nScientific citationFoudia eminentissimus Bonaparte 1850 Consp. Gen. Av., 1, p.446 ""Zanzibar"", error for Mohe\'li Island, fide Benson (1960, Ibis, 103b, p.101).\nMeaning of nameseminentissima - Latin. eminentissimus, very conspicuous, outstanding (super. of eminens, prominent).\nFirst English nameThe Zanzibar Foudi (Reichenbach 1863).\nAlternate namesGrand Comoro Fody, Johanna Fody, Mayotte Fody, Red Forest Fody, Red-headed Fody, Red-headed Forest Fody.\nDate collected1840 - 1841.\nLocality collectedZanzibar = Moheli Island, Comores.\nType specimensThe type specimen is in the Paris Museum.\nThe above is based on Weaver Wednesday 2, a weekly series about the discovery of each weaver species.\nThis species text first appeared as Weaver Wednesday  - Discovery : Comoro Fody on 2015-10-14\n1. Basic biology\nThe Comoro Fody, or Red-headed Forest Fody, Foudia eminentissima breeding male has a red head, breast and rump. It is distinguished from the introduced Madagascar Fody by having larger and longer bills, and clearer wing bars.\nThe Comoro Fody is found on the Comoro Islands (see map below, based on Birds of the Malagasy region). There are 4 subspecies, found on different islands:\nHabitat. The Comoro Fody inhabits well wooded habitats and forested areas. On Mayotte it is not found in intact stands of evergreen forest. On Grand Comoro, Mohéli and Mayotte it is found close to sea-level.\nFood. The diet of the Comoro Fody is mainly insects, including beetles, grasshoppers, and ant pupae. The Comoro Fody also feeds on fruit and spiders.\nBreeding. The Comoro Fody is probably monogamous. It is territorial, and males sing from tree tops in their territories. The nest is globular with a side entrance near the top. There is usually a porch above the entrance. Nests may have a ceiling of moss.\nThe eggs (clutch of 3) are pale blue, with a few fine spots. Possible nest predators include the exotic black rat, lemurs, and raptors.\nThe above is based on Weaver Wednesday, a weekly series about weaver species.\nThis species text first appeared as Weaver Wednesday : Comoro Fody on 2014-08-20\n2. Breeding facts\nOct-Mar on Mayotte\nsuspended rarely 1 m above ground, most often higher than 10 m\nMale starts nest and, once female has accepted it, both sexes build, and female may complete the structure with little male participation\npale blue-green, occasionally few black specks\nincubation by female only, period 13-16 days\nChicks and nestling period\nchicks fed by both parents, male usually began feeding only after 3 days, nestling period 15-18 days\nBreeding information based on Handbook of the Birds of the World, Vol. 15.\n3. Photos of Weaver NestsNo records yet - be the first to submit a PHOWN record!\nSee PHOWN summary page for this species here.\n4. Breeding distribution\nGoogle map showing distribution (For species with small ranges you need to zoom in at the correct area to see the range):\nyellow blob - range of weaver species; read more about this here.\n- PHOWN records with photos\n- PHOWN records with no photos (Nest Record Cards, other records)\n- Birdpix records\n- comments on out of range records, or interesting records\n- type locality\nCLICK on the marker on the map to see individual record details.\n5. Range changesStill coming\nThe above is based on Weaver Wednesday 3, a weekly series about range changes in South African weaver species.\nThis species text first appeared as Still coming']"	['<urn:uuid:afb0582e-2504-4e39-b864-7caab826804e>', '<urn:uuid:8dbebdfe-0a87-425b-a3b6-5ee51f2443d3>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T02:03:54.148118	29	68	1640
3	compare speech therapy vs aba therapy autism treatment	Speech therapy and ABA therapy serve different but complementary purposes in autism treatment. Speech therapy specifically focuses on speech, language, communication, and feeding skills, with specialized training in areas like augmentative communication and oral-motor difficulties. ABA therapy, on the other hand, is a broader approach that focuses on changing behaviors through positive reinforcement and can be applied to many different skills including communication, self-care, academic skills, and social behaviors. While speech therapy is provided by speech-language pathologists, ABA therapy is provided by board-certified behavior analysts (BCBAs) with master's degrees or PhDs in psychology or behavior analysis.	"['The Autism Center of The Studer Family Children\'s Hospital at Sacred Heart offers a range of services for children on the autism spectrum.\nThe Sacred Heart Autism Center at the Studer Family Children\'s Hospital is the region\'s leading expert on diagnosis and treatment for children on the autism spectrum and their families.\nOur mission is to provide early diagnosis and early intervention service to children under the age of 12 on the autism spectrum and to train their caregivers to implement behavior intervention in the home, in the school and in their community settings.\nAt the Sacred Heart Autism Center, our multi-disciplinary team provides a range of services for children on the autism spectrum, including:\n- Diagnosis and medical management\n- Applied Behavior Analysis (ABA)\n- Occupational therapy\n- Speech therapy\n- Feeding therapy\n- Parent instruction and hands-on training\n- Social skills groups for various ages\n- Group training for families and professional\nOur New Location\nThe Autism Center has recently moved to a new location with expanded capabilities! Our newly refurbished 5,500 square-foot building -- more than quadruple the size of the previous 1,200 square-foot center -- is at 5154 Bayou Blvd. on the Sacred Heart Hospital Pensacola campus and includes:\n- Six private treatment rooms\n- A large open space to be used for treatment, as well as caregiver-education classes\n- ""Calm,"" low-sensory room equipped with a Vecta Mobile Sensory Station, which is designed to calm autistic children using a stereo, interactive bubble column, projector and colorful fiber-optic ""tails"" for children to braid or comb with their fingers.\nFor more information, please call the Autism Center at 850-416-4933.\nFocusing on Family-Centered Care\nAt The Children\'s Hospital, we understand that each family has unique needs, and we strongly encourage you to be an active participant in your child\'s therapy sessions, since family involvement makes an enormous impact on the effectiveness of a child\'s treatment. You are a valued member of your child\'s care team, and your input regarding your child\'s goals and treatment is a valued and necessary component of our services.\nWe also understand families need help and support in order to improve their child\'s behavior, communication and social skills. To help your child succeed and to support you along this journey, we offer parent-training classes, individual consultation and hands-on instruction to maximize progress and achieve more significant gains.\nParent Education Classes\nOur team offers the following classes to help you along your child\'s journey with autism. Space is limited, and pre-registration is required for all classes. Classes are open to all families facing autism in the community, no matter where you are receiving treatment. For more information and to see a complete list of class dates, please click on the links below:\n- What to Do While You Wait - If your child has recently been diagnosed with autism – or if you are currently awaiting an evaluation – the future may seem overwhelming! This free class will help connect you with community resources and provide tips for addressing your child\'s behaviors. Learn more and register here.\n- ABCs of ABA - This free class is designed to help parents of children with autism learn more about Applied Behavior Analysis (ABA), how this therapy can benefit your child and what to look for when choosing an ABA-based autism center for your child\'s care. Our instructors will provide tips for parenting an autistic child, such as how to handle tantrums and manage mealtime meltdowns. Learn more and register. Learn more and register here.\n- Parent Empowerment Program- This 12-hour multi-session class is a hands-on learning opportunity to explore the basics of applied behavior analysis (ABA) and how to apply these techniques in working with children who struggle with problem behaviors, language deficits, and/or social impairments. PEP is taught by a board-certified behavior analyst and costs $120/person or $180/couple. Onsite child care is available during this class series by contacting Autism Pensacola at 850-434-7171. Learn more and register here.\nPartnering with Local Resources\nCollaboration is important to us. We coordinate with your child\'s teachers, physicians, therapists and additional family members in order to help your child achieve maximum progress. We also partner with the University of West Florida, CARD, Autism Pensacola, Inc., Early Steps and the local school districts.\nAs an official practicum site for interns in the University of West Florida\'s Master\'s-Level ABA program, we are academically focused and leading our field using the latest research and best practices for our patients. Students conduct research, connect the latest research to practice and gain hands-on feedback from our experienced board-certified Behavior Analysts. Individualized data is monitored regularly by our leaders, and changes and adjustments are made according to specific protocols and feedback.\nSetting the Standard for Autism Intervention\nOur mission is to be recognized as a center of excellence, so our team sets standards far above those required by both insurance companies and the Behavior Analyst Certification Board.\nOur strategies are based on the principles of Applied Behavior Analysis (ABA). ABA uses scientifically validated principles, such as motivation, reinforcement and prompting to improve important behaviors. That means increasing skills we want to see while eliminating or decreasing those that interfere with learning and the ability to develop new skills. Every member of our staff receives extensive training on these techniques. We have expertise in Verbal Behavior, a specialized type of ABA that focuses on the use of language and communication skills.<\nApplied Behavior Analysis (ABA) services\nEarly Start Denver Model (ESDM)\nAges 18 months to 5 years\nThe Early Start Denver Model (ESDM) is a research-based assessment, curriculum and intervention program for children ages 18 months to 5 years who have been diagnosed with autism or who exhibit characteristics of autism. In this model, a parent and child meet with a board-certified behavior analyst or registered behavior technician one to two times per week for a duration of three to six months. During a coaching session, the parent works directly with the child and the therapist provides coaching and support. The therapist and parent work together to develop a plan for implementing strategies throughout the child\'s day in a variety of activities. The parent is then responsible for putting the plan into action and keeping notes on progress.\nCOMPASS Family Navigation\nAges 5 years to 12 years\nCOMPASS is an ABA-based family training program designed to help caregivers become an agent of change for their child. The program consists of two, 1.5- to 2-hour sessions per week over a period of 6 months.\nBrief Behavioral Interventions\nAges 5 years to 12 years\nBrief Behavioral Interventions is our step-up program for children who qualify for the COMPASS program but may need more frequent sessions to achieve their goals. The program consists of three, 1.5- to 2-hour sessions per week over a period of 6 months. After completing the BBI program, children may continue into the COMPASS program if additional services are needed.\nAges 18 months to 12 years\nFor children on our wait list, we offer 1- to 2-hour behavior consultation services with a board-certified behavior analyst (BCBA) to address one specific behavior goal. The consults are available in-person or via telemedicine (online) and are not covered by insurance. Fees are $55 per hour and are due at the time of service.\nSpeech and feeding therapy\nOur pediatric speech-language pathologists (SLPs) are expertly trained to assess, diagnose and treat disorders or delays related to speech, language, cognitive-communication, voice, feeding/swallowing, articulation, fluency and social skills in children of all ages. Services are collaborative with Occupational Therapy (OT) and Applied Behavior Analysis (ABA) therapies as needed.\nOur specialized training includes:\n- Augmentative and Alternative Communications (AAC/voice output devices)\n- Picture Exchange Communication Systems (PECS)\n- Sign Language\n- Feeding therapy for food aversions and oral-motor difficulties\n- Home programming/parent education\n- Neurodevelopmental treatment for oral-motor\n- movement disorders (ex: Apraxia)\n- Auditory processing therapy\n- Language processing therapy\n- Social skills therapy\nPediatric occupational therapists (OTs) help children with everyday life skills and tasks. Our occupational therapists will evaluate and provide treatment to maximize independence in the child\'s daily ""jobs,"" which are age dependent and may include:\n- Cutting with scissors\n- Hygiene skills (tooth brushing, toileting, etc.)\n- Independence in self-feeding\n- Playing with others\n- Playing with toys\n- Sensory Integration\nSocial skills therapy\nOur social skills therapy program offers individualized therapy plans for children of various ages. The treatment plans are created based on the child\'s needs and administered during direct intervention and/or in social groups with similar age peers. The children in social groups meet weekly to practice skills such as problem-solving during disagreements, conversation skills, positive peer interactions, emotional regulation, reading and using non-verbal language appropriately, and cooperative play through games and therapeutic activities that help reinforce appropriate social skills. Certain social groups meet monthly at locations in the community in order to practice social skills in other environments (ex: bowling, restaurants, mini-golf).\nFor more information on services offered at the Autism Center of The Studer Family Children\'s Hospital at Sacred Heart or to schedule an appointment, please call our office at 850-416-4933.\nAll services at the Sacred Heart Autism Center – except for behavioral consultations – require a physician referral and prior authorization from your insurance company. Please contact your child\'s primary care physician for a referral, and ask that they fax a referral form and script to 850-416-7776.', 'What is Applied Behavior Analysis (ABA)?\nApplied Behavior Analysis (ABA) is the practice of applying the psychological principles of learning theory in a systematic way to alter behavior in humans or animals. The practice is used extensively in education, healthcare, animal training, and business management. It is particularly prominent in the treatment of Autism Spectrum Disorder (ASD), for which it is one of the only scientifically valid therapies available.\nHow It Works\nABA takes a research approach to therapy based on proven theories of learning and behavior. Therapists who use ABA understand how human behaviors are learned and how they can be changed over time. The therapist evaluates a client’s behavior and develops treatment plans to help improve the communication and behavior skills necessary for success in their personal and professional lives. ABA therapists can also provide training to parents and teachers. For the greatest results, ABA requires heavy monitoring and continuous evaluation. Therapists and other health professionals work within settings such as schools, homes, and community centers to evaluate and modify treatment as it progresses.\nWhat to Expect\nWhen working with an ABA therapist, you will:\nDetermine which behaviors require change\nSet goals and expected outcomes\nEstablish ways to measure changes and improvements\nEvaluate where you are now\nLearn new skills and/or learn how to avoid negative behaviors\nRegularly review your progress\nDecide whether or not further behavior modification is necessary\nHow does ABA therapy work?\nApplied Behavior Analysis involves many techniques for understanding and changing behavior. ABA is a flexible treatment:\nCan be adapted to meet the needs of each unique person\nProvided in many different locations – at home, at school, and in the community\nTeaches skills that are useful in everyday life\nCan involve one-to-one teaching or group instruction\nPositive reinforcement is one of the main strategies\nused in ABA.\nWhen a behavior is followed by something that is valued (a reward), a person is more likely to repeat that behavior. Over time, this encourages positive behavior change.\nFirst, the therapist identifies a goal behavior. Each time the person uses the behavior or skill successfully, they get a reward. The reward is meaningful to the individual –examples include praise, a toy or book, watching a video, access to playground or other location, and more. Positive rewards encourage the person to continue using the skill. Over time this leads to meaningful behavior change.\nAntecedent, Behavior, Consequence\nUnderstanding antecedents (what happens before a behavior occurs) and consequences (what happens after the behavior) is another important part of any ABA program.\nThe following three steps – the “A-B-Cs” – help us teach and understand behavior:\ntthis is what occurs right before the target behavior. It can be verbal, such as a command or request. It can also be physical, such a toy or object, or a light, sound, or something else in the environment. An antecedent may come from the environment, from another person, or be internal (such as a thought or feeling).\nA resulting behavior:\nthis is the person’s response or lack of response to the antecedent. It can be an action, a verbal response, or something else.\nthis is what comes directly after the behavior. It can include positive reinforcement of the desired behavior, or no reaction for incorrect/ inappropriate responses.\nLooking at A-B-Cs helps us understand:\nWhy a behavior may be happening\nHow different consequences could affect whether the behavior is likely to happen again\nThe teacher says “It’s time to clean up your toys” at the end of the day.\nThe student yells “no!”\nThe teacher removes the toys and says “Okay, toys are all done.”\nHow could ABA help the student learn a more appropriate behavior in this situation?\nThe teacher says “time to clean up” at the end of the day.\nThe student is reminded to ask, “Can I have 5 more minutes?”\nThe teacher says, “Of course you can have 5 more minutes!”\nWhat Does an ABA Program Involve?\nGood ABA programs for autism are not “one size fits all.” ABA should not be viewed as a canned set of drills. Rather, each program is written to meet the needs of the individual learner.\nThe goal of any ABA program is to help each person work on skills that will help them become more independent and successful in the short term as well as in the future.\nPlanning and Ongoing Assessment\nA qualified and trained behavior analyst (BCBA) designs and directly oversees the program. They customize the ABA program to each learner\'s skills, needs, interests, preferences and family situation.\nThe BCBA will start by doing a detailed assessment of each person’s skills and preferences. They will use this to write specific treatment goals. Family goals and preferences may be included, too.\nTreatment goals are written based on the age and ability level of the person with ASD. Goals can include many different skill areas, such as:\nCommunication and language\nSelf-care (such as showering and toileting)\nPlay and leisure\nPlay and leisure\nLearning and academic skills\nABA Techniques and Philosophy\nThe instructor uses a variety of ABA procedures. Some are directed by the instructor and others are directed by the person with autism.Parents, family members and caregivers receive training so they can support learning and skill practice throughout the day.\nThe person with autism will have many opportunities to learn and practice skills each day. This can happen in both planned and naturally occurring situations. For instance, someone learning to greet others by saying ""hello"" may get the chance to practice this skill in the classroom with their teacher (planned) and on the playground at recess (naturally occurring).\nThe learner receives an abundance of positive reinforcement for demonstrating useful skills and socially appropriate behaviors. The emphasis is on positive social interactions and enjoyable learning.\nThe learner receives no reinforcement for behaviors that pose harm or prevent learning.\nABA is effective for people of all ages. It can be used from early childhood through adulthood!\nWhat Can Children Learn Through ABA?\nMost of the time, ABA is intended to ""extinguish"" undesirable behaviors and teach desired behaviors and skills. For example, ABA may be used to reduce outbursts and tantrums or to teach a child to sit quietly, use words to make requests, or wait their turn in the playground. ABA can also be used to teach simple and complex skills. For example, ABA can be used to reward a child for brushing his teeth correctly, or for sharing a toy with a friend.\nWhile classic ABA can be used in a ""natural"" setting (a playground, for example), it is not intended to build emotional or social skills. So, for example, while ABA might teach a child to shake hands or greet another person with a handshake, it won\'t help that child to feel an emotional connection with another person. It takes an extraordinary therapist to use ABA to teach academic content, imaginative or symbolic thinking, or empathy; as a result, those skills are usually taught in other ways.\nWho provides ABA services?\nA board-certified behavior analyst (BCBA) provides ABA therapy services. To become a BCBA, the following is needed:\nEarn a master’s degree or PhD in psychology or behavior analysis\nPass a national certification exam\nSeek a state license to practice (in some states)\nWhat is the evidence that ABA works?\nABA is considered an evidence-based best practice treatment by the US Surgeon General and by the American Psychological Association. “Evidence based” means that ABA has passed scientific tests of its usefulness, quality, and effectiveness. ABA therapy includes many different techniques. All of these techniques focus on antecedents (what happens before a behavior occurs) and on consequences (what happens after the behavior).\nMore than 20 studies have established that intensive and long-term therapy using ABA principles improves outcomes for many but not all children with autism. “Intensive” and “long term” refer to .programs that provide 25 to 40 hours a week of therapy for 1 to 3 years. These studies show gains in intellectual functioning, language development, daily living skills and social functioning. Studies with adults, though fewer in number, show similar benefits.\nStudies that have shown positive progress with ABA\nChance a child will move into general curriculum classes after 2–3 years of intensive ABA therapy*\nDecades of proven results from ABA therapy\nIs ABA covered by insurance?\nSometimes. Many types of private health insurance are required to cover ABA services. This depends on what kind of insurance you have,and what state you live in.\nAll Medicaid plans must cover treatments that are medically necessary for children under the age of 21. If a doctor prescribes ABA and says it is medically necessary for your child, Medicaid must cover the cost.\nPlease see our insurance resources for more information about insurance and coverage for autism services.\nYou can also contact the Autism Response Team If you have difficulty obtaining coverage, or need additional help.\nIs it right for my child?\nOnly you can decide which therapy is right for your kiddo, and we recommend discussing the options with the pediatrician. ABA’s scope is broad, so to arm you with the proper knowledge and direction for that conversation, here are some areas where your child may benefit from it.\nABA Therapy is provided in the child’s own environment. It facilitates active family participation\nCommunity-based ABA Therapy\nABA Therapy is provided in the child’s daycare. It enables generalization of skills and peer interaction.\nABA in a Group Setting\nGives your child tools to improve social skills, emotional intelligence and self-confidence..\nCenter-based ABA Therapy\nABA Therapy is provided in a structured environment. It facilitates planned socialization with peers.']"	['<urn:uuid:d6f3718d-2c01-484b-b6a1-323954b592b7>', '<urn:uuid:df306e36-ebd9-4193-bb1b-1eb680987de7>']	factoid	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-13T02:03:54.148118	8	96	3166
4	I'm interested in old papers and documents - what material were they using back in medieval times that helped their documents survive for so many centuries?	These medieval documents were written on vellum, which is described as a strong and utilitarian material that aided in their survival through 700 years.	['September 5, 2015\nValue of a 14th Century Medieval Manuscript\nI was recently selling a NYC coop, feeling overwhelmed at the amount of paperwork and due diligence necessary to transfer ownership of shares, when I was offered a medieval deed. It is a relatively small and simple document, when a man could transfer his earthly possessions- or in this case half his landholdings- on a mere document only 7 x 3″ inches in size. It opens simply in Latin: To all [men] present and future…\nSo, what is the value of a document like this from the 14th century? Surprisingly, they can be collected for rather modest sums. Here are a couple records for other similar documents pulled from the ABPC manuscript database – a subscriber database that is an essential tool for examining past auction records of comparable books and documents and substantiating a fair market value.\nEngland – _ KENT. – Document. Deed of Gift. [23 Feb 1411]. No size or length given. William George conveys a house in the village of Shynglewelle [sic]. Parchment. In Latin. Stained. – Winter, Apr 12, 2006, lot 294, £140 ($248)\nEngland – _ KENT. – Document. Deed of Gift. [12 Mar 1398]. No size or length given. John Spernor de Cobham & John Topleche convey to Simon Lepy a plot of land in Shyngled Well [sic] .Parchment. In Latin. Stained. – Illus in cat – Winter, Apr 12, 2006, lot 293, £240 ($424)\nTrue, this one is perhaps a bit earlier than some of the examples listed above (and dates to the early part of the 14th century). However, at auction it would at most reasonably fall into the $400-500 range and perhaps a bit less as it is missing its original hanging seal.\nTo me that is rather remarkable: this small and ephemeral document has escaped the ravages of time for 700 years and is only worth approx. $500? Thankfully, it is written on vellum, a strong and utilitarian material which aided its survival. Still, the manuscript provides insight into paleography (the study of ancient and historical handwriting), English medieval history, early legal history, and when framed is a rather remarkable and impactful object for the pleasure of both the eye and mind. Perhaps old Latin documents are bit too erudite for most and that has kept the prices low, but for the keen collector they are a bargain that will not last in the years ahead. And certainly, I should send one to my real estate lawyer to show him how simple a document could be 😉\n[ENGLISH MEDIEVAL DEED] Early 14th century. [Incipit] Sciant p[re]sentes & fut[ur]i q[uo]d ego Robert de edui? dedi concessi & hac p[re]senti carta mea confirmaui Simoni filio… A fine medieval example of a deed of gift bequeathing half of his lands to his son Simon.. 7 x 3 inches on vellum, evidence of attached seal at lower center. With scarce 14th-15th century English explanatory text to verso. Small holes but generally very good.']	['<urn:uuid:1a677257-b027-4411-94f2-19e212207f19>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-13T02:03:54.148118	26	24	500
5	school counseling and autism therapy help kids	Both counseling services and autism therapy help children develop essential skills. School counselors help students develop social, organizational, and problem-solving skills while working on their language-based learning disabilities. For autism, early intervention services are critical and include Applied Behavioral Analysis (ABA), speech therapy, occupational therapy, and physical therapy. These treatments focus on developing communication, social, and cognitive skills, with evidence showing that early intervention before school can lead to significant improvements.	"['The Counseling Department consists of licensed, masters-level clinicians who see students for weekly counseling and provide check-ins and case management. There’s ongoing communication and consultation with staff, parents and outside professionals. Counseling reports, written each term, document progress toward social emotional goals in each student’s Individual Educational Program. Counselors work closely with academic staff to develop individualized, expected targeted behavior goals reported on daily by each classroom teacher. This feedback acts as a daily communication tool for counseling sessions as well as for parents and guardians. In addition, counselors teach Health and Student Issues Classes (HSI classes) which follow the modified Massachusetts Common Core State Standards. The counselors carry a prime responsibility for teaching the basic principles of Social Thinking™ in counseling sessions as well as HSI classes. Thinking Maps® are used across the curriculum as well as in HSI classes and individual counseling sessions.\nCounseling services focus on helping students further develop their social, organizational and problem-solving skills, as well as other issues related to coping with their language-based learning disability. Counselors work on developing and strengthening:\nOccupational Therapy (OT) services are an integral part of the programming at LPS.\nElementary School Occupational Therapy Services\nOccupational Therapy skills are addressed, including:\nSkills addressed in OT sessions vary according to each student’s needs. OT sessions use a wide variety of techniques ranging from arts and crafts projects, pencil and paper work, technology and board games, to strengthening and coordination exercises.\nActivities in the Sensorimotor Room use a sensory integrative and/or perceptual motor approach and may include obstacle courses, activity stations, exercises and therapeutic games. Various therapeutic equipment are used, such as scooter boards, a trampoline, therapy balls and several types of swings.\nThinking Maps® are incorporated into treatment sessions to assist with organizing and completing presented activities.\nMiddle School Occupational Therapy\nMiddle School Work Center/OT is an extension of Occupational Therapy services for older children at LPS. Skills that are addressed include:\nThese skill areas are addressed within the context of practical learning experiences. Students engage in various activities involving life skills and age appropriate “jobs” around the school. Students may be involved in the following activities in Work Center/OT:\nWork related vocabulary and basic work behaviors are emphasized throughout the curriculum. Various tools are used to address skill development.\nRating sheets are completed at the end of each class to give students feedback on their performance in individual skills, such as initiation, organization, problem solving, sequencing, collaborating and time management.\nThe ACT Program (Activate/Calm/Think)\nMany students at LPS have mild to significant issues with sensory processing and modulating their level of arousal. These difficulties can have a significant impact on the student’s readiness to learn, ability to focus and overall performance in the classroom. The Occupational Therapy Department developed a simple strategic program to assist faculty in the classroom and therapy sessions.\nLPS faculty is trained on the ACT Program, including the specific movement activities and ways in which to incorporate the process into their classes. ACT posters demonstrating the program activities are in all of the classrooms. These activities, which only take a few minutes to perform, may be done at the start of each class as part of the students’ routine. Activities can also be performed during the midpoint of the period or during class transitions if the teacher wants to re-focus students. These activities also provide much needed and legitimate “movement breaks” in a structured manner.\nThe program consists of five simple movement activities that can be completed in a variety of ways. Each activity is designed to provide specific sensory input, which have a “grounding” effect as well as provide other sensory stimulation.\nOver time, our goal is for the students to learn how to independently use these strategies in school, as well as other areas of their lives.\nSpeech and Language\nThe LPS Speech and Language Department is made up of licensed, board-certified speech and language pathologists. Our focus on communication disorders allows us to address a variety of deficits that affect oral, written and social language, as well as disorders in oral motor production, articulation, voice and central auditory processing disorders. Treatment for students in Elementary and Middle Schools involve direct weekly pull-out services so that individual goals can be addressed in a clinical, therapeutic setting.\nStudents who attend High School receive intervention either through small group services or through push-in services, where clinicians co-teach with classroom ELA teachers and reinforce strategies previously addressed in Middle School.\nAssistive Technology Devices and Services\nMany children with disabilities have difficulties with reading, writing, and math. Some children have problems with vision, hearing, listening and/or communicating. Others have physical, mobility, and motor problems. Assistive technology helps children use their strengths to compensate or ""work around"" weaknesses caused by the disability. Assistive technology includes ""devices"" and ""services.""\nIDEA 2004 requires IEP teams to consider the assistive technology needs of all children with disabilities. (20 U.S.C. 1414(d)(3)(B)(v))\nThe IEP team makes decisions about assistive technology devices and services based on your child’s unique needs so that he can be more confident and independent. The law requires schools to use assistive technology devices and services ""to maximize accessibility for children with disabilities."" (20 U.S.C. 1400(c)(5)(H))\nIf the IEP team determines that your child needs assistive technology devices and services, the school district is responsible for providing these and cannot use lack of availability or cost as an excuse.\nNote: Assistive technology is not a substitute for teaching your child to read and write.\nIDEA defines an \'assistive technology device\' as...\nany item, piece of equipment, or product system, whether acquired commercially off the shelf, modified, or customized, that is used to increase, maintain, or improve functional capabilities of a child with a disability. (20 U.S.C. 1401(1))\nIDEA defines an \'assistive technology service\' as...\nany service that directly assists a child with a disability in the selection, acquisition, or use of an assistive technology device. Such term includes -\n(A) the evaluation...\n(B) purchasing, leasing, or otherwise providing for the acquisition of assistive technology devices...\n(C) selecting, designing, fitting, customizing, adapting, applying, maintaining, repairing, or replacing...\n(D) coordinating and using other therapies, interventions, or services with assistive technology devices...\n(D) training or technical assistance for such child, or ...the family of such child...(F) training or technical assistance for professionals... (20 U.S.C. 1401(2))', '- About ASF\n- What is Autism?\n- How Common is Autism?\n- Early Signs of Autism\n- Autism Diagnosis\n- Following a Diagnosis\n- Treatment Options\n- Beware of Non-Evidence-Based Treatments\n- Autism and Vaccines\n- Autism Science\n- Quick Facts About Autism\n- What We Fund\n- Baby Siblings Research Consortium\n- Resources for Grantees\n- Funding Calendar\n- ASF Funded Research\n- ASF Supported Findings\n- Apply for a Fellowship\n- Apply for a Research Accelerator Grant\n- Apply for an Undergraduate Summer Research Grant\n- Apply for IMFAR Travel Grant\n- Get Involved\n- Participate in Research\n- Student Clubs\n- Live Chat with Scientists\n- Jobs & Internships\n- Apply for a Grant\n- Day of Learning\n- Contact Us\nScientists agree that the earlier in life a child receives early intervention services the better the child’s prognosis. All children with autism can benefit from early intervention, and some may gain enough skills to be able to attend mainstream school. Research tells us that early intervention in an appropriate educational setting for at least two years prior to the start of school can result in significant improvements for many young children with Autism Spectrum Disorders. As soon as autism is diagnosed, early intervention instruction should begin. Effective programs focus on developing communication, social, and cognitive skills.\nEarly diagnosis of ASD, coupled with swift and effective intervention, is paramount to achieving the best possible prognosis for the child. Even at ages as young as six months, diagnosis of ASD is possible. Regular screenings by pediatric psychiatrists are recommended by the Centers for Disease Control (CDC). Even if your child is not diagnosed with an ASD before the age of 3, under the Individuals with Disabilities Education Act (IDEA), your child may be eligible for services provided by your state. In addition, many insurance companies will provide additional assistance for the coverage of proven therapies. More information on Insurance and Autism can be found here.\nThe most effective treatments available today are applied behavioral analysis (ABA), occupational therapy, speech therapy, physical therapy, and pharmacological therapy. Treatment works to minimize the impact of the core features and associated deficits of ASD and to maximize functional independence and quality of life. In 2012, the Missouri Guidelines Initiative summarized the findings from 6 reviews on behavioral and pharmacological interventions in autism. The consensus paper includes current evidence of what interventions have been studied and shown effective, why or why not, and can be found here.\nApplied Behavioral Analysis (ABA) works to systematically change behavior based on principles of learning derived from behavioral psychology. ABA encourages positive behaviors and discourages negative behaviors. In addition, ABA teaches new skills and applies those skills to new situations\nEarly Intensive Behavioral Intervention (EIBI) is a type of ABA for very young children with an ASD, usually younger than five, often younger than three.\nPivotal Response Training is a variation of ABA that works to increase a child’s motivation to learn, monitor his own behavior, and initiate communication with others by focusing on behaviors that are seen as key to learning other skills, such as language, play, and social skills. This training works to generalize skills across many settings with different people.\nDiscrete trial teaching is a common form of ABA, in which what is being taught is broken down into smaller steps, and taught using prompts and rewards for each step. Prompts and rewards are phased out over time.\nThe Lovaas Model consists of 20-40 hours of highly structured, discrete trial training, integrating ABA techniques into an early intervention program. The intervention typically begins when the child is between the ages of 2-8 years old, and no later than 12 years old. The technique utilizes child-specific reinforcers to motivate and reward success. Additionally, the use of language and imitation are crucial for the teaching model. Click here to learn more about the Lovaas Model.\nThe Early Start Denver Model is an early intervention program designed for infants, toddlers, and pre-schoolers ages 12-48 months with autism. Developed by Geraldine Dawson, Ph.D., and Sally Rogers, Ph.D., it is the only experimentally verified early-intervention program designed for children with autism as young as 18 months old. ESDM applies the principles of ABA to an early-intervention program. Similar to Pivotal Response Training, interventions are delivered within play-based, relationship-focused routines. Studies testing the efficacy of the treatment have found the intervention “resulted in significant improvements in IQ, language, adaptive behavior, and autism diagnosis”. To learn more about the Early Start Denver Model, read more here.\nSince people with ASDs have deficits in social communication, speech therapy is an important treatment option. Speech therapy with a licensed speech-language pathologist helps to improve a person’s communication skills, allowing him to better express his needs or wants. For individuals with ASD, speech therapy is often most effective when speech-language pathologists work with teachers, support personnel, families, and the child’s peers to promote functional communication in natural settings.\nSome individuals with ASD are nonverbal and unable to develop verbal communication skills, and the use of gestures, sign language, and picture communication programs are often useful tools to improve their abilities to communicate.\nOccupational Therapy (OT)\nOccupational therapy is often used as a treatment for the sensory integration issues associated with ASDs. It is also used to help teach life skills that involve fine-motor movements, such as dressing, using utensils, cutting with scissors, and writing. OT works to improve the individual’s quality of life and ability to participate fully in daily activities. Each occupational therapy program is based on individual evaluations and goals. Occupational therapy for young children with ASD often focuses on improving sensory integration and sensorimotor issues. In older children, OT often focuses on improving social behavior and increasing independence.\nPhysical Therapy (PT)\nPhysical therapy is used to improve gross motor skills and handle sensory integration issues, particularly those involving the individual’s ability to feel and be aware of his body in space. Similar to OT, physical therapy is used to improve the individual’s ability to participate in everyday activities. PT works to teach and improve skills such as walking, sitting, coordination, and balance. Physical therapy is most effective when integrated in an early intervention program.\nPharmaceutical treatments can help ameliorate some of the behavioral symptoms of ASD, including irritability, aggression, and self-injurious behavior. Additionally, by medically reducing interfering or disruptive behaviors, other treatments, including ABA, may be more effective. Medications should be prescribed and monitored by a qualified physician.\nRisperidone is the first FDA-approved medication for the treatment of symptoms associated with of ASD in children and adolescents, including aggressive behavior, deliberate self-injury, and temper tantrums. Read the studies below.\nAripriprazole is also FDA-approved for the treatment of irritability in children and adolescents with ASD. A 2009 study published in Pediatrics found that in a group of 98 children, by week 8, 52% of those taking aripriprazole, in the form of Abilify, experienced a 25% or greater reduction in autism-related irritability symptoms compared with 14% of those who took the placebo. Read the studies below.\nScahill, L. et al. Effects of risperidone and parent training on adaptive functioning in children with pervasive developmental disorders and serious behavioral problems. J Am Acad Child Adolesc Psychiatry 51, 136-146, doi:10.1016/j.jaac.2011.11.010 (2012).\nTroost, P. W. et al. Long-term effects of risperidone in children with autism spectrum disorders: a placebo discontinuation study. J Am Acad Child Adolesc Psychiatry 44, 1137-1144, doi:10.1097/01.chi.0000177055.11229.76 (2005).\nShea, S. et al. Risperidone in the Treatment of Disruptive Behavioral Symptoms in Children With Autistic and Other Pervasive Developmental Disorders. Pediatrics 114, e634-e641, doi:10.1542/peds.2003-0264-F (2004).\nArnold, L. E. et al. Parent-defined target symptoms respond to risperidone in RUPP autism study: customer approach to clinical trials. J Am Acad Child Adolesc Psychiatry 42, 1443-1450, doi:10.1097/00004583-200312000-00011 (2003).\nMarcus, R. N. et al. Safety and tolerability of aripiprazole for irritability in pediatric patients with autistic disorder: a 52-week, open-label, multicenter study. The Journal of clinical psychiatry 72, 1270-1276, doi:10.4088/JCP.09m05933 (2011).\nAman, M. G. et al. Line-item analysis of the Aberrant Behavior Checklist: results from two studies of aripiprazole in the treatment of irritability associated with autistic disorder. J Child Adolesc Psychopharmacol 20, 415-422, doi:10.1089/cap.2009.0120 (2010).\nMarcus, R. N. et al. A placebo-controlled, fixed-dose study of aripiprazole in children and adolescents with irritability associated with autistic disorder. J Am Acad Child Adolesc Psychiatry 48, 1110-1119, doi:10.1097/CHI.0b013e3181b76658 (2009).\nStigler, K. A. et al. Aripiprazole in pervasive developmental disorder not otherwise specified and Asperger’s disorder: a 14-week, prospective, open-label study. J Child Adolesc Psychopharmacol 19, 265-274, doi:10.1089/cap.2008.093 (2009).']"	['<urn:uuid:c10fd365-2ac6-4a1a-8154-03ac2b05560e>', '<urn:uuid:20d0b5c6-594d-4d21-99c9-8aa7dfe5b0c3>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T02:03:54.148118	7	71	2471
6	julius caesar augustus comparison death who killed them where died how	Julius Caesar was killed by 40 plotters led by Brutus with 23 knife wounds in the halls of the Theater of Pompeo in 44 BC, and his body was cremated in the Roman Forum. In contrast, Augustus died in 14 AD at age 75, possibly from natural causes, though there were unconfirmed rumors his wife Livia poisoned him.	"['Facts about Julius Caesar: between History and Myth\nBy the time of Julius Caesar in 50 B.C. the Roman Republic was living a time of deep crisis. These were years marked by revolts and political clashes. Murders and violence were the order of the day, and the republican government seemed to have been closer to anarchy than ever before.\n|Credits: image by @efrye|\nCaesar’s personal ambitions\nJulius Caesar belonged to one of the most noble families in Rome, the gens Iulia. However, they had lost most of their fortune and were burdened by debts. Caesar’s dream was to give back to his family the lost honor and the power that once belonged to the gens Iulia.\nIn 65 B.C, Caesar was no longer a boy. He was 32 years old and had already shown his value as a soldier. During these years he was sent to Rome’s provinces in Spain to stop an insurrection. On this occasion, Caesar proved his strength, his value and impressive strategic skills. The military campaign in Spain also had political implications. For Julius Caesar, building his own reputation among the ranks of the army in an empire found on military supremacy was just the first step to acquire a political influence.\nHis final goal was indeed to become consul, the most prestigious office in Rome. This way, not only he could have achieved great powers, but also the governorate of one province. Caesar’s object of desire became the Gaul, one of the richest provinces of Rome. He knew that he could have made a lot of money out of a victorious campaign in Gaul, and he was eager to escape from his creditors in Rome.\nThe Run for Office\nWilling to do everything it took to reach his goals, Caesar started showing another interesting trait of his personality. Not only he was a great soldier, but he was a leader too, able to reach, involve and persuade the audience. His populist campaign inspired many. Among them there was Brutus, a young man who quickly become Caesar’s favorite.\nHowever, by achieving his goal to conquer the favor of the masses, Caesar soon lost the support of the Roman aristocracy and the of senators, who saw his innovative ideas as a threat to the status quo.\nCaesar knew his weaknesses. He also knew that, notwithstanding the electoral results, the Senate would have still had the power to refuse him the governorship of Gaul. So, how do you win an electoral campaign and the governorship of the richest province of the time without the favor of the nobility and of the senate?\nFor Julius Caesar the answer was pretty obvious! He needed allies and not just any ally. He turned to two of the most important men in the Roman Republic, Crassus and Pompey Magnus. The first one, Caesar’s longtime friend, was also the richest men in Rome, while Pompey was one of the most popular general at the time.\nThanks to his power of persuasion, Caesar convinced both of them to create an alliance, the first Triumvirate, and to seal the contract he gave his daughter Julia in marriage to Pompey.\nCaesar won the consulate and became governor of two Gaul provinces for 5 years. All the three of them had become very powerful. Once he had reached his goals, Caesar left Rome and headed North looking for gold and fame.\nMilitary success in Gaul\nDuring his time in Gaul, Caesar and his army fought against barbarians threatening Rome. In 58 B.C the Romans faced the Helvetians. The enemies were annihilated. By that time, Caesar’s plans have changed. Now he was aiming at expanding the Roman empire and conquering new lands.\nMeantime in Rome, Caesar’s expansionist ambitions severely concerned the senators, who tried to convinced Pompey to break the alliance. Although the antagonism between the two of them had grown strongly over the past years, Pompey was more concerned with his personal life. He had married Giulia and was very in love with her. Together they traveled a lot across Italy, and Pompeo started neglecting politics to spend time with his beloved wife.\nCaesar’s military successes continued in Gaul. He defeated the barbarians led by Ariovistus and brought the Roman troops up to Britain. He was the first Roman to ever enter those lands. However, the dispatches he sent to Rome only increased the senators rivalry. Also Brutus himself started questioning the real intentions of his mentor and doubting the legitimacy of Caesar’s growing power. By that time Caesar could rely on a trusty army and he was collecting great wealth.\nThe time had come for the senators to stop Caesar, who in their minds had become the major threat to the status quo and the aristocracy. A series of fortuitous coincidence allowed the Senate to succeed. Crassus, longstanding supporter of Caesar, died during a military campaign. At the same time, Julia, Pompey’s wife, died in childbirth, along with their baby-son. Pompeo was devastated and probably felt like he had nothing to lose. He broke the alliance with Caesar and became the champion of the Roman aristocracy and conservatorism.\nIn the meantime Caesar’s military successes continued. The Romans had conquered the North of Europe. His last desire was to go back to Rome and see his successes recognized. In 49 B.C. the senators asked Caesar to go back to the capital as a private citizen, which meant he had to give up his army. Caesar knew that by doing so he would have faced a sure death. He was ready to do anything it took, also to invade Rome, if necessary.\nLeading his troops, he moved south from Gaul and reached the Rubicone river, which basically marked the border between the provinces and Rome itself. Caesar knew that crossing the border with his troops meant a declaration of war, but he had no choice and did it anyway.\nChaos spread all over Rome. Pompey and the aristocracy knew that Caesar was backed by loyal soldiers, a unified army forged by 10 years of military campaigns. To take time, Pompeo went to Rodi, Greece, to put together a new, more powerful army.\nThe battle of Pharsalus in 48 B.C. marked the defeat of Pompey’s troops. He fled to Egypt seeking refuge, but he was stopped and betrayed by the king, who beheaded him and sent his head to Caesar.\nThe Ides of March\nIn 47 B.C. Caesar had reached an undisputed and absolute supremacy over Rome. He started the reconstruction of the city and funded military campaigns to conquer new colonies. In the middle of the Republican age, Caesar was the first one to foresee the destiny of Rome as a great, real Empire. However, the dictatorship was causing unrest in Rome. Brutus was persuaded by the Senators to stop him and became the champion of the conspiracy that caused Caesar’s assassination.\nOn 44 B.C, the Ides of March, 40 plotter led by Brutus killed Caesar with 23 knife wounds in the halls of the Theater of Pompeo. His body was then carried to the area known today as the Roman Forum and there it was cremated.', 'Text #9686""Augustus"", in .\nAugustus (Latin: Imperātor Caesar Dīvī Fīlius Augustus; 23 September 63 BC – 19 August 14 AD) was the founder of the Roman Empire and its first Emperor, ruling from 27 BC until his death in AD 14.\nHe was born Gaius Octavius into an old and wealthy equestrian branch of the plebeian Octavii family. His maternal great-uncle Julius Caesar was assassinated in 44 BC, and Octavius was named in Caesar’s will as his adopted son and heir. He, Mark Antony, and Marcus Lepidus formed the Second Triumvirate to defeat the assassins of Caesar. Following their victory at Philippi, the Triumvirate divided the Roman Republic among themselves and ruled as military dictators.[note 4] The Triumvirate was eventually torn apart under the competing ambitions of its members. Lepidus was driven into exile and stripped of his position, and Antony committed suicide following his defeat at the Battle of Actium by Octavian in 31 BC.\nAfter the demise of the Second Triumvirate, Augustus restored the outward facade of the free Republic, with governmental power vested in the Roman Senate, the executive magistrates, and the legislative assemblies. In reality, however, he retained his autocratic power over the Republic as a military dictator. By law, Augustus held a collection of powers granted to him for life by the Senate, including supreme military command, and those of tribune and censor. It took several years for Augustus to develop the framework within which a formally republican state could be led under his sole rule. He rejected monarchical titles, and instead called himself Princeps Civitatis (“First Citizen of the State”). The resulting constitutional framework became known as the Principate, the first phase of the Roman Empire.\nThe reign of Augustus initiated an era of relative peace known as the Pax Romana (The Roman Peace). The Roman world was largely free from large-scale conflict for more than two centuries, despite continuous wars of imperial expansion on the Empire’s frontiers and one year-long civil war over the imperial succession. Augustus dramatically enlarged the Empire, annexing Egypt, Dalmatia, Pannonia, Noricum, and Raetia; expanding possessions in Africa; expanding into Germania; and completing the conquest of Hispania.\nBeyond the frontiers, he secured the Empire with a buffer region of client states and made peace with the Parthian Empire through diplomacy. He reformed the Roman system of taxation, developed networks of roads with an official courier system, established a standing army, established the Praetorian Guard, created official police and fire-fighting services for Rome, and rebuilt much of the city during his reign.\nAugustus died in AD 14 at the age of 75. He may have died from natural causes, although there were unconfirmed rumors that his wife Livia poisoned him. He was succeeded as Emperor by his adopted son (also stepson and former son-in-law) Tiberius.\nAugustus was known by many names throughout his life:\nAt birth, he was named Gaius Octavius after his biological father. Historians typically refer to him simply as Octavius (or Octavian) between his birth in 63 until his adoption by Julius Caesar in 44 BC (after Julius Caesar’s death).\nUpon his adoption, he took Caesar’s name and became Gaius Julius Caesar Octavianus in accordance with Roman adoption naming standards. He quickly dropped “Octavianus” from his name, and his contemporaries typically referred to him as “Caesar” during this period; historians, however, refer to him as Octavian between 44 BC and 27 BC.\nIn 42 BC, Octavian began the Temple of Divus Iulius or Temple of the Comet Star and added Divi Filius (Son of the Divine) to his name in order to strengthen his political ties to Caesar’s former soldiers by following the deification of Caesar, becoming Gaius Julius Caesar Divi Filius.\nIn 38 BC, Octavian replaced his praenomen “Gaius” and nomen “Julius” with Imperator, the title by which troops hailed their leader after military success, officially becoming Imperator Caesar Divi Filius.\nIn 27 BC, following his defeat of Mark Antony and Cleopatra, the Roman Senate voted new titles for him, officially becoming Imperator Caesar Divi Filius Augustus. It is the events of 27 BC from which he obtained his traditional name of Augustus, which historians use in reference to him from 27 BC until his death in AD 14.\nWhile his paternal family was from the town of Velletri, approximately 40 kilometres (25 mi) from Rome, Augustus was born in the city of Rome on 23 September 63 BC. He was born at Ox Head, a small property on the Palatine Hill, very close to the Roman Forum. He was given the name Gaius Octavius Thurinus, his cognomen possibly commemorating his father’s victory at Thurii over a rebellious band of slaves.\nDue to the crowded nature of Rome at the time, Octavius was taken to his father’s home village at Velletri to be raised. Octavius only mentions his father’s equestrian family briefly in his memoirs. His paternal great-grandfather Gaius Octavius was a military tribune in Sicily during the Second Punic War. His grandfather had served in several local political offices. His father, also named Gaius Octavius, had been governor of Macedonia. His mother, Atia, was the niece of Julius Caesar.\nIn 59 BC, when he was four years old, his father died. His mother married a former governor of Syria, Lucius Marcius Philippus. Philippus claimed descent from Alexander the Great, and was elected consul in 56 BC. Philippus never had much of an interest in young Octavius. Because of this, Octavius was raised by his grandmother (and Julius Caesar’s sister), Julia Caesaris.\nIn 52 or 51 BC, Julia Caesaris died. Octavius delivered the funeral oration for his grandmother. From this point, his mother and stepfather took a more active role in raising him. He donned the toga virilis four years later, and was elected to the College of Pontiffs in 47 BC. The following year he was put in charge of the Greek games that were staged in honor of the Temple of Venus Genetrix, built by Julius Caesar. According to Nicolaus of Damascus, Octavius wished to join Caesar’s staff for his campaign in Africa, but gave way when his mother protested. In 46 BC, she consented for him to join Caesar in Hispania, where he planned to fight the forces of Pompey, Caesar’s late enemy, but Octavius fell ill and was unable to travel.\nWhen he had recovered, he sailed to the front, but was shipwrecked; after coming ashore with a handful of companions, he crossed hostile territory to Caesar’s camp, which impressed his great-uncle considerably. Velleius Paterculus reports that after that time, Caesar allowed the young man to share his carriage. When back in Rome, Caesar deposited a new will with the Vestal Virgins, naming Octavius as the prime beneficiary.\nOctavius was studying and undergoing military training in Apollonia, Illyria, when Julius Caesar was killed on the Ides of March (15 March) 44 BC. He rejected the advice of some army officers to take refuge with the troops in Macedonia and sailed to Italy to ascertain whether he had any potential political fortunes or security. Caesar had no living legitimate children under Roman law, and so had adopted Octavius, his grand-nephew, making him his primary heir. Mark Antony later charged that Octavian had earned his adoption by Caesar through sexual favours, though Suetonius describes Antony’s accusation as political slander. After landing at Lupiae near Brundisium, Octavius learned the contents of Caesar’s will, and only then did he decide to become Caesar’s political heir as well as heir to two-thirds of his estate.\nUpon his adoption, Octavius assumed his great-uncle’s name Gaius Julius Caesar. Roman citizens adopted into a new family usually retained their old nomen in cognomen form (e.g., Octavianus for one who had been an Octavius, Aemilianus for one who had been an Aemilius, etc.). However, though some of his contemporaries did, there is no evidence that Octavius ever himself officially used the name Octavianus, as it would have made his modest origins too obvious. Historians usually refer to the new Caesar as Octavian during the time between his adoption and his assumption of the name Augustus in 27 BC in order to avoid confusing the dead dictator with his heir.\nCassius Dio’s Roman History: Books 45–56, English translation\nLife of Augustus by Nicolaus of Damascus, English translation\nSuetonius’ biography of Augustus, Latin text with English translation\nThe Res Gestae Divi Augusti (The Deeds of Augustus, his own account: complete Latin and Greek texts with facing English translation)\nAllen, William Sidney (1978) . Vox Latina—a Guide to the Pronunciation of Classical Latin (2nd ed.). Cambridge University Press. ISBN 0-521-37936-9.\nAndo, Clifford, Imperial ideology and provincial loyalty in the Roman Empire, University of California Press, 2000.\nBivar, A. D. H. (1983). “The Political History of Iran Under the Arsacids”, in The Cambridge History of Iran (Vol 3:1), 21–99. Edited by Ehsan Yarshater. London, New York, New Rochelle, Melbourne, and Sydney: Cambridge University Press. ISBN 978-0-521-20092-9.\nBlackburn, Bonnie and Holford-Strevens, Leofranc. (1999). The Oxford Companion to the Year. Oxford University Press. Reprinted with corrections 2003.\nBourne, Ella. “Augustus as a Letter-Writer”, Transactions and Proceedings of the American Philological Association (Volume 49, 1918): 53–66.\nBowersock, G. W. (1990). “The Pontificate of Augustus”. In Kurt A. Raaflaub and Mark Toher (eds.). Between Republic and Empire: Interpretations of Augustus and his Principate. Berkeley: University of California Press. pp. 380–394. ISBN 978-0-520-08447-6.\nBrosius, Maria. (2006). The Persians: An Introduction. London & New York: Routledge. ISBN 978-0-415-32089-4 (hbk).\nBunson, Matthew. (1994). Encyclopedia of the Roman Empire. New York: Facts on File Inc. ISBN 978-0-8160-3182-5\nChisholm, Kitty and John Ferguson. (1981). Rome: The Augustan Age; A Source Book. Oxford: Oxford University Press, in association with the Open University Press. ISBN 978-0-19-872108-6\nDio, Cassius. (1987) The Roman History: The Reign of Augustus. Translated by Ian Scott-Kilvert. London: Penguin Books. ISBN 978-0-14-044448-3.\nDavies, Mark; Swain, Hilary; Davies, Mark Everson, Aspects of Roman history, 82 BC-AD 14: a source-based approach, Taylor & Francis e-Library, 2010.\nEck, Werner; translated by Deborah Lucas Schneider; new material by Sarolta A. Takács. (2003) The Age of Augustus. Oxford: Blackwell Publishing (hardcover, ISBN 978-0-631-22957-5; paperback, ISBN 978-0-631-22958-2).\nEder, Walter. (2005). “Augustus and the Power of Tradition”, in The Cambridge Companion to the Age of Augustus (Cambridge Companions to the Ancient World), ed. Karl Galinsky, 13–32. Cambridge, MA; New York: Cambridge University Press (hardcover, ISBN 978-0-521-80796-8; paperback, ISBN 978-0-521-00393-3).\nEveritt, Anthony (2006) Augustus: The Life of Rome’s First Emperor. Random House Books. ISBN 1-4000-6128-8.\nGreen, Peter (1990). Alexander to Actium: The Historical Evolution of the Hellenistic Age. Hellenistic Culture and Society. Berkeley, CA; Los Angeles; London: University of California Press. ISBN 0-520-05611-6.\nGruen, Erich S. (2005). “Augustus and the Making of the Principate”, in The Cambridge Companion to the Age of Augustus (Cambridge Companions to the Ancient World), ed. Karl Galinsky, 33–51. Cambridge, MA; New York: Cambridge University Press (hardcover, ISBN 978-0-521-80796-8; paperback, ISBN 978-0-521-00393-3).\nHolland, Richard, Augustus, Godfather of Europe, Sutton Publishing, 2005.\nKelsall, Malcolm. “Augustus and Pope”, The Huntington Library Quarterly (Volume 39, Number 2, 1976): 117–131.\nMackay, Christopher S. (2004). Ancient Rome: A Military and Political History. Cambridge University Press. ISBN 978-0-521-80918-4.\nRaaflaub, Kurt A.; Toher, Mark, Between republic and empire: interpretations of Augustus and his principate, University of California Press, 1993.\nRowell, Henry Thompson. (1962). The Centers of Civilization Series: Volume 5; Rome in the Augustan Age. Norman: University of Oklahoma Press. ISBN 978-0-8061-0956-5\nScott, Kenneth. “The Political Propaganda of 44–30 B.C.” Memoirs of the American Academy in Rome, Vol. 11, (1933), pp. 7–49.\nScullard, H. H. (1982) . From the Gracchi to Nero: A History of Rome from 133 B.C. to A.D. 68 (5th ed.). London; New York: Routledge. ISBN 978-0-415-02527-0.\nSuetonius, Gaius Tranquillus (2013) . Thayer, Bill, ed. The Lives of the Twelve Caesars. J. C. Rolfe, trans. University of Chicago. Original publisher Loeb Classical Library.\nSuetonius, Gaius Tranquillus (1931). Lives of the Twelve Caesars. New York: Modern Library.\nShaw-Smith, R. “A Letter from Augustus to Tiberius”, Greece & Rome (Volume 18, Number 2, 1971): 213–214.\nShotter, D. C. A. “Tiberius and the Spirit of Augustus”, Greece & Rome (Volume 13, Number 2, 1966): 207–212.\nSmith, R. R. R., “The Public Image of Licinius I: Portrait Sculpture and Imperial Ideology in the Early Fourth Century”, The Journal of Roman Studies, Vol. 87, (1997), pp. 170–202, JSTOR\nSouthern, Pat. (1998). Augustus. London: Routledge. ISBN 978-0-415-16631-7.\nStarr, Chester G., Jr. “The Perfect Democracy of the Roman Empire”, The American Historical Review (Volume 58, Number 1, 1952): 1–16.\nSyme, Ronald (1939). The Roman Revolution. Oxford: Oxford University Press. ISBN 0-19-280320-4.\nWalker, Susan, and Burnett, Andrew, The Image of Augustus, 1981, British Museum Publications, ISBN 0-7141-1270-4\nWells, Colin Michael, The Roman Empire, Harvard University Press, 2004.']"	['<urn:uuid:f2f530af-3dc4-43e7-a37d-2658b1924cce>', '<urn:uuid:b0fa9fc8-6634-4ff0-9b7e-ec92cc460daf>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-13T02:03:54.148118	11	58	3297
7	monkfish egg veil dimensions maximum length width thickness	A monkfish egg veil can be up to 40 feet long and 5 feet wide, with eggs arranged in a single layer that is just over one-tenth of an inch thick.	"[""Click image to launch slide show\nNEFSC researcher Larry Alade holds a tagged goosefish, or monkfish, prior to release during a cooperative monkfish migration study with commercial fishermen in 2009 and 2010. Photo Credit: Pasha Ivanov\nClick image to enlargeA large female goosefish is measured after being caught during a bottom trawl survey. Photo credit: NOAA Fisheries/NEFSC\nClose-up of part of an egg veil. It may look a bit like bubble wrap, but the light tan colored spots, or bubbles, are eggs. The image was taken during a Gulf of Maine shrimp survey on the R/V Gloria Michelle. Credit: NOAA Fisheries/Heidi Marotta, NEFSC\nApril 19, 2017\nContact: Shelley Dawicki\nMonkfish mysteries: Study finds they are serial spawners\nResearchers have found that individual goosefish, commonly called monkfish, spawn more than once during a spawning season that can last six months. Their size has also declined over time, leading to a population dominated by young small females with reduced spawning capacity compared to the older, larger females more common in the past.\nThe findings, published online in the Journal of Fish Biology, compared findings from 1982-1985 and 2009-2012, two very different periods for monkfish. Although often caught by fishermen, monkfish was not a viable commercial fishery until the 1980s, when a market for them developed. Today, monkfish (Lophius americanus) is one of the most valuable finfish fisheries in the Northeast.\nMonkfish are known for spawning their egg in masses, called egg veils since they look like bolts of delicate lace billowing in the ocean's current. Each veil can be up to 40 feet long and 5 feet wide, with eggs arranged in a single layer just over one-tenth of an inch thick. The largest females can produce more than two million eggs per veil. The veil is spawned by the female and fertilized by nearby males. Eggs are protected in chambers within the veil and develop over a period of weeks until the veil breaks up and the larvae are released into the water column.\n“We have evidence that female monkfish spawn more than once in a season, and perhaps as many as three times,” said Richard McBride, a fishery biologist at the Northeast Fisheries Science Center and lead author of the study. “A female at the New England Aquarium has been observed to produce more than one veil per year, but there were no males in the tank, so it was not clear if this was normal. Now we have observed monkfish in the ocean preparing for serial spawning, which is a common bet-hedging strategy in fishes. Environmental and feeding conditions vary over time, so by spawning several times a year in different seasons, more eggs will likely survive and grow into adult fish, thereby contributing to a more resilient population and fishery.”\nMcBride and NOAA Fisheries colleagues examined 54 spawning-capable female monkfish, the largest number examined to date. These fish were collected between 2009 and 2012 from the southern management area on the Mid-Atlantic Bight shelf region. Samples were collected by commercial gillnets during February to July, trawl catches during the Northeast Fisheries Science Center’s 2009 Cooperative Monkfish Survey, and from the NEFSC’s spring 2012 bottom trawl survey. Results were compared to 17 samples collected from 1982 to 1985.\n“Our comparisons suggest that the females are not producing as many eggs as they used to, partly because they are smaller now, but perhaps because feeding conditions now are different than they were in the 1980s when the last similar study was done,” McBride said. “We looked at the southern management area, which includes the Mid-Atlantic seaboard and southern New England, because we had a previous study for comparison.”\nThe recent study focused on three areas of reproductive biology: fecundity, spawning frequency, and the seasonality of female spawning. Fecundity means the number of eggs produced in a single spawning. One of the smallest monkfish produced about 230,000 eggs per veil, and one of the largest monkfish produced 2.2 million eggs per veil. The average monkfish producing 3 egg veils per year spawns 5-6 billion eggs in her lifetime, as calculated for the recent period in the southern management area.\nSpawning activity by the population was prolonged, occurring nearly year-round, but was most common from March to October. It progresses northward during the year: collections of monkfish larvae peak in May-June offshore of Chesapeake Bay but July-August in the Gulf of Maine, which is in the northern management area.\nThese results were used to estimate changes since the 1980s in the reproductive potential of the population. The model assumed that spawning frequency increased with age during both time periods. The results showed that the current population reproductive potential is less than 50 percent of what it was in the 1980s study because fecundity per individual is lower and there are fewer large females in the population now.\n“We know much more than we used to about monkfish, but there are still some major gaps,” said Anne Richards, a fishery biologist who leads monkfish assessments at the NEFSC and is a co-author of the study. “The gaps include stock structure, migration patterns, and growth rates. We are making progress filling those gaps, but it takes time.”\nRichards says there has been some encouraging news. “In 2015 there was a very large year class of monkfish produced in the southern management area, and in 2016 a large recruitment event happened in the northern management area. These year classes are helping to resolve some long-standing questions about monkfish growth and are expected to boost catch rates as these recruits grow into exploitable size. This is good news for both scientists and fishermen.”\n# # #\nNOAA Fisheries Service is dedicated to protecting and preserving our nation's living marine resources and their habitat through scientific research, management and enforcement. NOAA Fisheries Service provides effective stewardship of these resources for the benefit of the nation, supporting coastal communities that depend upon them, and helping to provide safe and healthy seafood to consumers and recreational opportunities for the American public. Join NOAA Fisheries on Facebook, Twitter and our other social media channels.\nNOAA's mission is to understand and predict changes in the Earth's environment, from the depths of the ocean to the surface of the sun, and to conserve and manage our coastal and marine resources. Join us on Twitter, Facebook, Instagram, and our other social media channels.""]"	['<urn:uuid:033db962-bfff-4d6f-9fe2-49c82736ac1b>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T02:03:54.148118	8	31	1060
8	How do weather forecasters calculate rain probability?	Weather forecasters calculate rain probability by simulating thousands of weather outcomes and counting how many show rain. For example, if 75% of their model results showed rain in an area, they report a 75% chance of rain.	['In Part 1 of this series we discussed how probabilistic forecasting retains each estimate’s uncertainty throughout the forecast. We looked at how weather forecaster’s present uncertainty in their predictions and how people seem comfortable that the future cannot be predicted perfectly and life still continues. We need this realization in IT forecasts!\nIn Part 2 we look at the approach taken in the field of probabilistic forecasting, continuing our weather prediction analogy.\nWe can observe the present with certainty. Meteorologists have been recording various input measure for years, and evidence suggests ancient cultures have understood the seasons to the extent they knew what food items to plant and when. These observations and how they played out over time form the basis for tomorrow’s weather forecast. Modern forecasters combine today’s actual weather conditions with historical observations and trends, using computer models.\nCombining uncertain inputs (creating joint probability distributions) is the trick to responsible forecasting. Mainstream weather forecasters model weather by simulating many possible outcomes using historical data and computing the likelihood of any possible outcome.\nFor example, if there are two corners with unsynchronized traffic lights on your drive to work, there are four possible combinations of impact to your commute. Both sets are green, you hit one of the lights but not the other (two combinations), or you hit both sets of lights. Your chance of hitting 2 green lights is 1 in 4.\nWhen the weather forecaster says 75% chance of rain, they most often^ mean that 75% of their model results showed rain in your area. They have simulated thousands (or hundreds of thousands) of weather outcomes and counted the number that forecast rain versus the total: this is the probability of rain. Each simulation uses different input starting values chosen randomly from historic observations, and then simulates how these inputs interact and compound in the complex weather system.\n^ Sometimes they mean 100% chance of rain in 75% of the area, but this is almost never the case.\nGood forecasts require computation time. If there are many input variables, the number of simulations required to get a good result explodes, which explains why forecasters use large computers to forecast climate change and major storms. Also, most good meteorologists run multiple models. Some are good at picking temperature range, and some are good at picking precipitation.\nForecasting is the art of predicting to the best of your ability, seeing if it comes to fruition, and learning from that. How do forecasters know which model to trust? They find out tomorrow which one is right, and favor that model in the future! This feedback loop improves the models over time, which explains why forecasts are rarely completely wrong unless there is severe weather operating at the boundary of modeling behavior.\nThe process we just described is called “Monte Carlo simulation”. It is the “addition operator for uncertainty”. It is the main tool used for understanding risk and outcomes in fields like mineral exploration, insurance, and finance (nobody said the models are perfect yet!).\nIT project forecasting carries a lot of uncertainty. When we estimate or forecast a project or required team size, we should acknowledge and incorporate our uncertainties into our forecasts. Commonly available tools fail to do this. We look stunned every time these estimates turn out wrong, but given the naive approaches we use, it should be no surprise.\nIn the next part of this series we will examine what to do when historical data isn’t available. Future parts of this series will examine specifically at how IT forecasting uncertainty can be predicted and improved.']	['<urn:uuid:32cf9f1c-c271-4bbb-93bd-45912c666ee0>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-13T02:03:54.148118	7	37	598
9	What have been the major advancements in improving both efficiency and stability in perovskite solar cell technology over the past decade, from both materials and manufacturing perspectives?	Perovskite solar cells have seen remarkable progress in the past decade. The solar conversion efficiency jumped from 3% to more than 25% in just 10 years, compared to 60 years for traditional semiconductors to reach similar efficiency levels. Recent breakthroughs include a 2-D perovskite solar cell that improves by 18% when exposed to sunlight, due to sunlight causing atomic layer contraction that enhances electron flow. For stability, researchers like Grätzel achieved major improvements using CuSCN as hole extraction layer, with devices retaining over 95% efficiency after 1000 hours at 60°C. Manufacturing advances include new deposition methods like the solvent and vacuum-free route developed by Han's team, achieving 12.1% efficiency for large 36.1 square centimeter modules. Interface engineering and iodide management techniques have pushed efficiencies above 22% while maintaining stability.	['Army, Navy Lend Muscle To New 2-D Perovskite Solar Cell That’s 18% More Efficient\nA new perovskite solar cell promises high efficiency, low cost, and a long life cycle thanks in part to support from the US Army and US Navy.\nPublished9 hours ago\nThe US Department of Defense has been pivoting into a climate action posture, and that includes support for new clean tech. In the latest development, both the Army and the Navy have lent their financial firepower to a new 2-D perovskite solar cell research project. If all goes according to plan, the result will be a light weight, inexpensive, highly efficient solar cell that can deliver clean kilowatts over a 20-25 year lifespan.\nPerovskite Solar Cell To The Rescue\nResearch on perovskite solar cells only began in earnest about 10 years ago, with the US Department of Energy among those in hot pursuit. The results have been impressive. The solar conversion efficiency of a perovskite solar cell leaped from a lowly 3% to more than 25% over that 10-year time span. In contrast, the solar cell semiconductors currently in use took about 60 years to reach the 25% benchmark.\nTo gild the clean tech lily, synthetic crystalline materials that mimic perovskite are relatively easy and inexpensive to grow.\nArmy, Navy Join The 2-D Perovskite Solar Cell Team\nThere being no such thing as a free lunch, “raw” perovskite tends to fall apart when exposed to humidity and sunlight. That complicates matters for the R&D phase, but it is not an insurmountable problem. Researchers have already figured out a suite of workarounds that can bring perovskite solar cells to market.\nThe remaining conundrum is how to build durability into a perovskite solar cell without sacrificing solar conversion efficiency.\nThat brings us to Army-Navy mashup, which has helped bring about the Mother of All Workarounds: a two-dimensional perovskite solar cell formula that gains durability and efficiency when exposed to sunlight, instead of losing it.\nThe Army Research Office and the Office of Naval Research provided financial support for the new perovskite research, along with the National Science Foundation and the Energy Department’s Office of Science.\nThe heavy lifting was done by a team of engineers working out of the lab of Aditya Mohite, at the George R. Brown School of Engineering at Rice University. Here, let’s have Rice explain:\n“The lab of Aditya Mohite of Rice’s George R. Brown School of Engineering discovered that sunlight itself contracts the space between atomic layers in 2D perovskites enough to improve the material’s photovoltaic efficiency by up to 18%, an astounding leap in a field where progress is often measured in fractions of a percent.”\nFor the record, the project was a collaborative one that also involved Purdue and Northwestern universities, too. The Energy Department also chipped in with a hand from its Los Alamos, Argonne and Brookhaven laboratories. An assist from the Institute of Electronics and Digital Technologies in France made it an international affair, with additional support from the Academic Institute of France.\nThe 2-D Perovskite PV Solution: How It Works\nThe Rice team built their new 2-D perovskite solar cell on previous research indicating a pathway for balancing durability with efficiency.\n“We’ve been working for many years and continue to work with bulk perovskites that are very efficient but not as stable. In contrast, 2D perovskites have tremendous stability but are not efficient enough to put on a roof,” explains Mohite.\nRecent iterations of perovskite solar technology have added 2-D structures to enhance stability. The Rice team decided to do away with the 3-D “bulk” structure and go straight for the 2-D. That’s when the magic happened.\n“We find that as you light the material, you kind of squeeze it like a sponge and bring the layers together to enhance the charge transport in that direction,” said Mohite.\n“This effect has given us the opportunity to understand and tailor these fundamental light-matter interactions without creating complex heterostructures like stacked 2D transition metal dichalcogenides,” he added.\nThe actual amount of squeezing is minuscule, but it makes a big difference. After exposure to a solar simulator, the team’s perovskite lattice contracted about 0.4% lengthwise and 1.0% vertically.\n“It doesn’t sound like a lot, but this 1% contraction in the lattice spacing induces a large enhancement of electron flow,” said co-lead author Wenbin Li, who is a Rice Applied Physics graduate student. “Our research shows a threefold increase in the electron conduction of the material.”\nThe lattice also quickly resumed its normal shape after the light was removed, indicating improved durability compared to 3-D versions.\nGroup Hug For US Taxpayers\nCircling back around to that thing about a 10-year timeline to 25% conversion efficiency compared to 60 years, that kind of accelerated clean tech R&D will sure come in handy now that the people of the Earth only have about a 10-year window for preventing catastrophic climate change.\nIn the perovskite field, part of the credit does go to the material itself. A quantum leap in materials science in the past 30 years has also factored in, including the introduction of new, more powerful diagnostic tools funded by the taxpaying public, so group hug for us.\nFor example, Argonne National Laboratory physicist Joe Strzalka, who is a co-author on the new solar cell study, credits the “ultrabright” X-rays of the lab’s Advanced Photon Source with enabling the team to observe nanoscale changes in the structure of their perovskites in real time.\n“For changes like this, it’s important to do operando studies,” Strzalka adds. “The same way your mechanic wants to run your engine to see what’s happening inside it, we want to essentially take a video of this transformation instead of a single snapshot.”\n“The sensitive instruments at beamline 8-ID-E of the APS allow for ‘operando’ studies, meaning those conducted while the device is undergoing controlled changes in temperature or environment under normal operating conditions,” Rice University adds.\nAPS was built in the 1990s, and apparently you ain’t seen nothing yet.\n“A comprehensive upgrade to the facility’s electron storage ring, scheduled to begin in April 2023, will increase the brightness of those X-rays by up to 500 times and will enable new discoveries and innovations we cannot yet imagine,” Argonne enthuses. “New beamlines and enhancements will further expand the capabilities of the APS, and keep it at the forefront of global X-ray science.”\nAccelerating Climate Action In The US\nThe Rice team anticipates that the new, improved APS will help propel them into the next step up the R&D ladder.\n“We’re on a path to get greater than 20% efficiency by engineering the cations and interfaces,” Sidhik explains. “It would change everything in the field of perovskites, because then people would begin to use 2D perovskites for 2D perovskite/silicon and 2D/3D perovskite tandems, which could enable efficiencies approaching 30%.”\nIt’s unfortunate that President Joe Biden’s Build Back Better climate action bill is being held in limbo, but the new perovskite research is just one among many initiatives that is baking climate action into national policy, through the deep pockets of the Department of Defense.', '10 most influential scientists in Perovskite Solar Cells\n● Tsutomu Miyasaka, Toin University of Yokohama\nMiyasaka’s long term experience and research has been focused on the design of low-temperature solution-printing process for fabrication of dye-sensitized solar cells and solid-state hybrid photovoltaic cells. Also, he developed photovoltaic cells using organo-lead halide compounds as light-absorbing materials---perovskite solar cells. Miyasaka has contributed to the pioneering works and discovered perovskite’s potential as low-cost, large-area high-efficiency thin film solar cells. His first paper on perovskite solar cells published in 2009 has been cited more than 4400 times. Now Miyasaka group has been endeavoring in practical applications of perovskite solar cells and dye-sensitized solar cells, plus these technologies to flexible devices and photodetectors.\nPark, Sungkyunkwan University\nNam-Gyu’s research has been focused on high efficiency dye-sensitized solar cells. He is specialist in design and synthesis of inorganic nanostructured materials as well as perovskite solar cells.\nNam-Gyu had worked with Michael Grätzel (École Polytechnique Fédérale de Lausanne) and reported on solid-state mesoscopic heterojunction solar cells employing nanoparticles (NPs) of methyl ammonium lead iodide (CH3NH3)PbI3 as light harvesters in 2012. The perovskite NPs were produced by reaction of methylammonium iodide with PbI2 and deposited onto a submicron-thick mesoscopic TiO2 film, whose pores were infiltrated with the hole-conductor spiro-MeOTAD. Illumination with standard AM-1.5 sunlight generated large photocurrents (JSC) exceeding 17 mA/cm2, an open circuit photovoltage (VOC) of 0.888 V and a fill factor (FF) of 0.62 yielding a power conversion efficiency (PCE) of 9.7%, the highest reported to date for such cells.\n● Henry J. Snaith, University of Oxford\nSnaith has made several significant advances for solution-processed solar cells, including the first demonstration of gyroid structured titania for dye solar cells. In 2012, Snaith has discovered the remarkable PV properties of metal halide perovskite which has emerged as a new field in PV research.\nSnaith teamed up with Miyasaka, used spiro-OMeTAD as the hole-conducting layer, and deposited it on a mixed halide perovskite, CH3NH3Pbl2Cl. When the team made solar cells with those compounds and TiO2, they observed conversion efficiencies near 7.6 %, a value that just a year earlier would have turned heads. But when they replaced TiO2 with alumina (Al2O3), an insulator that cannot conduct electrons to the electrode---that is, when they made a solar cell that was sure to fail---it surprisingly delivered near 11 % conversion efficiency. The team proposed that alumina serves only as a high-surface-area scaffold but that it mediates formation of a layer of high-quality perovskite crystals. Snaith group suggested that the quality of the film is likely the reason the crystals can collect and transport electrons so efficiently. (Science 2012, DOI: 10.1126/science.1228604)\n● Michael Grätzel, École Polytechnique Fédérale de Lausanne, EPFL\n- 2013, Grätzel published a research paper ”Sequential deposition as a route to high-performance perovskite-sensitized solar cells.” (Nature volume499, pages316–319 (18 July 2013))\nFollowing pioneering work, solution-processable organic–inorganic hybrid perovskites—such as CH3NH3PbX3 (X = Cl, Br, I)—have attracted attention as light-harvesting materials for mesoscopic solar cells. So far, the perovskite pigment has been deposited in a single step onto mesoporous metal oxide films using a mixture of PbX2 and CH3NH3X in a common solvent. However, the uncontrolled precipitation of the perovskite produces large morphological variations, resulting in a wide spread of photovoltaic performance in the resulting devices, which hampers the prospects for practical applications. Here we describe a sequential deposition method for the formation of the perovskite pigment within the porous metal oxide film. PbI2 is first introduced from solution into a nanoporous titanium dioxide film and subsequently transformed into the perovskite by exposing it to a solution of CH3NH3I. We find that the conversion occurs within the nanoporous host as soon as the two components come into contact, permitting much better control over the perovskite morphology than is possible with the previously employed route. Using this technique for the fabrication of solid-state mesoscopic solar cells greatly increases the reproducibility of their performance and allows us to achieve a power conversion efficiency of approximately 15 % (measured under standard AM1.5G test conditions on solar zenith angle, solar light intensity and cell temperature). This two-step method should provide new opportunities for the fabrication of solution-processed photovoltaic cells with unprecedented power conversion efficiencies and high stability equal to or even greater than those of today’s best thin-film photovoltaic devices.\n- 2017, Grätzel published a research paper “ Perovskite solar cells with CuSCN hole extraction layers yield stabilized efficiencies greater than 20%”. (Science 28 Sep 2017:eaam5655; DOI: 10.1126/science.aam5655)\nPerovskite solar cells (PSC) with efficiencies >20% have only been realized with highly expensive organic hole-transporting materials. We demonstrate PSCs achieving stabilized efficiencies exceeding 20% with CuSCN as hole extraction layer using fast solvent removal method to create compact, highly conformal CuSCN layers that facilitate fast carrier extraction and collection. The PSCs showed high thermal stability under long term heating, however, their operational stability was poor. This instability originates from potential induced degradation of the CuSCN/Au contact. The addition of a conductive reduced graphene oxide spacer layer between CuSCN and gold allowed PSCs to retain >95% of their initial efficiency after aging at a maximum power point for 1000 hours at 60 Celsius. Importantly, under both continuous full-sun illumination and thermal stress, CuSCN based devices surpassed the stability of spiro-OMeTAD based PSCs.\n● Yang Yang, University of California, Los Angeles, UCLA\n- 2014, Yang Yang published a research paper “Interface engineering of highly efficient perovskite solar cells.” (Science 01 Aug 2014:Vol. 345, Issue 6196, pp. 542-546; DOI: 10.1126/science.1254050)\nAdvancing perovskite solar cell technologies toward their theoretical power conversion efficiency (PCE) requires delicate control over the carrier dynamics throughout the entire device. By controlling the formation of the perovskite layer and careful choices of other materials, we suppressed carrier recombination in the absorber, facilitated carrier injection into the carrier transport layers, and maintained good carrier extraction at the electrodes. When measured via reverse bias scan, cell PCE is typically boosted to 16.6% on average, with the highest efficiency of ~19.3% in a planar geometry without antireflective coating. The fabrication of our perovskite solar cells was conducted in air and from solution at low temperatures, which should simplify manufacturing of large-area perovskite devices that are inexpensive and perform at high levels.\nAnd Yang Yang Lab utilizes Enli Tech QE-R Solar Cell Quantum Efficiency Measurement System to measure quantum efficiency for solar cells.\n● Sang Il Seok, Ulsan National Institute of Science and Technology\n- 2017, Sang ll Seok has focused on inorganic-organic hybrid solar cells and perovskite solar cells. He published a research paper” Iodide management in formamidinium-lead-halide–based perovskite lay ers for efficient solar cells. ” (Published 2017 in Science; DOI:10.1126/science.aan2301)\nIn this study, the research team demonstrated the introduction of additional iodide ions into the organic cation solution, which are used to form the perovskite layers through an intramolecular exchanging process, decreases the concentration of deep-level defects. The formation of a dense and uniform thin layer on the substrates is crucial for the fabrication of high-performance perovskite solar cells (PSCs) containing formamidinium with multiple cations and mixed halide anions. The concentration of defect states, which reduce a cell’s performance by decreasing the open-circuit voltage and short-circuit current density, needs to be as low as possible. We show that the introduction of additional iodide ions into the organic cation solution, which are used to form the perovskite layers through an intramolecular exchanging process, decreases the concentration of deep-level defects. The defect-engineered thin perovskite layers enable the fabrication of PSCs with a certified power conversion efficiency of 22.1% in small cells and 19.7% in 1-square-centimeter cells.\n● Liyuan Han, Shanghai Jiao Tong University\n- 2017, Liyuan Han published a research paper” A solvent- and vacuum-free route to large-area perovskite films for efficient solar modules.” (Nature volume 550, pages 92–95 (05 October 2017))\nThe method produces homogenous films with relatively few defects, which leads to a record efficiency of 12.1% for a solar module made from a methylammonium lead halide film that is just over 36 cm2 in size.\nRecent advances in the use of organic–inorganic hybrid perovskites for optoelectronics have been rapid, with reported power conversion efficiencies of up to 22 % for perovskite solar cells. Improvements in stability have also enabled testing over a timescale of thousands of hours. However, large-scale deployment of such cells will also require the ability to produce large-area, uniformly high-quality perovskite films. A key challenge is to overcome the substantial reduction in power conversion efficiency when a small device is scaled up: a reduction from over 20 % to about 10 per cent is found when a common aperture area of about 0.1 square centimetres is increased to more than 25 square centimetres. Here we report a new deposition route for methyl ammonium lead halide perovskite films that does not rely on use of a common solvent or vacuum: rather, it relies on the rapid conversion of amine complex precursors to perovskite films, followed by a pressure application step. The deposited perovskite films were free of pin-holes and highly uniform. Importantly, the new deposition approach can be performed in air at low temperatures, facilitating fabrication of large-area perovskite devices. We reached a certified power conversion efficiency of 12.1 % with an aperture area of 36.1 square centimetres for a mesoporous TiO2-based perovskite solar module architecture.\n● Yi-Bing Cheng, Wuhan University of Technology\n- 2018, Yi-Bing Cheng published a research paper” Influence of Hot Spot Heating on Stability of Large Size Perovskite Solar Module with a Power Conversion Efficiency of ~14%.” (ACS Appl. Energy Mater., 2018, 1 (8), pp 3565–3570; DOI: 10.1021/acsaem.8b00803; Publication Date (Web): July 16, 2018)\nMaking perovskite solar cell technology commercially viable is facing a challenge that scaling-up of a small device always experiences a substantial reduction in power conversion efficiency (PCE). In this research, we adopted a volume expansion routine to scale-up from a 0.16 cm2laboratory-scale device to large size perovskite solar module (PSM) with a PCE loss of 8% and reached a certified PCE of 13.98%. The PCE of the PSM with Au electrode dropped about 40% of the initial efficiency after 16 days’ storage, while the efficiency of PSM with Cu as counter electrode retained 90% of the initial after 30 days’ storage. We also introduced hot spots heating (HSH) characterization method to investigate the stability of PSM. HSH reveals that Cu electrode greatly reduces numbers of hot spots and extent of temperature increase in a PSM compared with Au electrode. Therefore, counter electrode also plays an important role in the stability improvement of PSM.\n● Michael D. McGehee, Stanford University\n- 2017, Michael D. McGehee published a research paper” 23.6%-efficient monolithic perovskite/silicon tandem solar cells with improved stability.” (Nature Energy volume 2, Article number: 17009 (2017))\nAs the record single-junction efficiencies of perovskite solar cells now rival those of copper indium gallium selenide, cadmium telluride and multicrystalline silicon, they are becoming increasingly attractive for use in tandem solar cells due to their wide, tunable bandgap and solution processability. Previously, perovskite/silicon tandems were limited by significant parasitic absorption and poor environmental stability. Here, we improve the efficiency of monolithic, two-terminal, 1-cm2 perovskite/silicon tandems to 23.6% by combining an infrared-tuned silicon heterojunction bottom cell with the recently developed caesium formamidinium lead halide perovskite. This more-stable perovskite tolerates deposition of a tin oxide buffer layer via atomic layer deposition that prevents shunts, has negligible parasitic absorption, and allows for the sputter deposition of a transparent top electrode. Furthermore, the window layer doubles as a diffusion barrier, increasing the thermal and environmental stability to enable perovskite devices that withstand a 1,000-hour damp heat test at 85 ℃ and 85% relative humidity.\n● Jingbi You, Institute of Semiconductors, Chinese Academy of Sciences\n- 2018, Jingbi You developed the latest perovskite solar cells, the highest power conversion efficiency up to 23.3 % in 2018. During the same year, he published a research paper” Efficient green light-emitting diodes based on quasi-two-dimensional composition and phase engineered perovskite with surface passivation.” (Nature Communications volume 9, Article number: 570 (2018)) And Jingbi You group utilizes Enli Tech LED Photo-Luminescence Quantum Yield Measurement System to measure external quantum efficiency up to 10 %.\n- In perovskite solar cell field, Jingbi You published a research paper” A polymer tandem solar cell with 10.6% power conversion efficiency.” (Nature Communications volume 4, Article number: 1446 (2013))\nAn effective way to improve polymer solar cell efficiency is to use a tandem structure, as a broader part of the spectrum of solar radiation is used and the thermalization loss of photon energy is minimized. In the past, the lack of high-performance low-bandgap polymers was the major limiting factor for achieving high-performance tandem solar cell. Here we report the development of a high-performance low bandgap polymer (bandgap <1.4 eV), poly[2,7-(5,5-bis-(3,7-dimethyloctyl)-5H-dithieno[3,2-b:2′,3′-d]pyran)-alt-4,7-(5,6-difluoro-2,1,3-benzothia diazole)] with a bandgap of 1.38 eV, high mobility, deep highest occupied molecular orbital. As a result, a single-junction device shows high external quantum efficiency of >60% and spectral response that extends to 900 nm, with a power conversion efficiency of 7.9%. The polymer enables a solution processed tandem solar cell with certified 10.6% power conversion efficiency under standard reporting conditions (25 °C, 1,000 Wm-2, IEC 60904-3 global), which is the first certified polymer solar cell efficiency over 10%.\n- In 2014, he published a research paper” Low-Temperature Solution-Processed Perovskite Solar Cells with High Efficiency and Flexibility.” (ACS Nano, 2014, 8 (2), pp 1674–1680; DOI: 10.1021/nn406020d; Publication Date (Web): January 5, 2014)\nPerovskite compounds have attracted recently great attention in photovoltaic research. The devices are typically fabricated using condensed or mesoporous TiO2 as the electron transport layer and 2,2′7,7′-tetrakis-(N,N-dip-methoxyphenylamine)9,9′-spirobifluorene as the hole transport layer. However, the high-temperature processing (450 °C) requirement of the TiO2 layer could hinder the widespread adoption of the technology. In this report, we adopted a low-temperature processing technique to attain high-efficiency devices in both rigid and flexible substrates, using device structure substrate/ITO/PEDOT:PSS/CH3NH3PbI3–xClx/PCBM/Al, where PEDOT:PSS and PCBM are used as hole and electron transport layers, respectively. Mixed halide perovskite, CH3NH3PbI3–xClx, was used due to its long carrier lifetime and good electrical properties. All of these layers are solution-processed under 120 °C. Based on the proposed device structure, power conversion efficiency (PCE) of 11.5% is obtained in rigid substrates (glass/ITO), and a 9.2% PCE is achieved for a polyethylene terephthalate/ITO flexible substrate.\nTsutimu (Tom) Miyasaka\nNam-Gyu Park, (2012) Lead Iodide Perovskite Sensitized All-Solid-State Submicron Thin Film Mesoscopic Solar Cell with Efficiency Exceeding 9%.\nMitch Jacoby, Chemical & Engineering News (2014) Tapping Solar Power with Perovskites\nMichael Grätzel, (2014). Sequential deposition as a route to high-performance perovskite-sensitized solar cells\nMichael Grätzel, (2017). Perovskite solar cells with CuSCN hole extraction layers yield stabilized efficiencies greater than 20%\nSang Il Seok, (2017). Iodide management in formamidinium-lead-halide–based perovskite layers for efficient solar cells\nThe OSA DIRECT NEWSLETTER, (2018). Large-area perovskite films go solvent- and vacuum-free\nMichael D. McGehee, (2017). 23.6%-efficient monolithic perovskite/silicon tandem solar cells with improved stability\nYibing Cheng, (2018). Influence of Hot Spot Heating on Stability of Large Size Perovskite Solar Module with a Power Conversion Efficiency of ~14%']	['<urn:uuid:b09aca58-1b06-48c2-8dee-3d86c6b72fa2>', '<urn:uuid:51559b54-4430-48d0-8cdc-c85601614dbc>']	open-ended	direct	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-13T02:03:54.148118	27	129	3676
10	Are seat bookings required on Trans-Siberian and Dutch trains?	There's a difference in seat booking requirements. On Trans-Siberian trains, beds are allocated in advance with place numbers printed on tickets. In contrast, on Dutch Regional and Intercity trains (including Sprinter), seat reservations are not mandatory.	"['If you’re planning a Trans-Siberian, Trans-Manchurian or Trans-Mongolian journey the first thing I’d recommend is to read The Man in Seat 61’s excellent article on the topic. His website is a must for any rail, bus or ferry travel research but this particularly in-depth piece will help you decide which route to take and how to go about doing it; from organising it all yourself to finding an agent to arrange it for you, and anywhere in between.\nThe following advice and general info is based on our experience and covers things you might want to know before getting on that first train. (Unless you want it all to be a complete surprise, which is okay too – stop reading now if that’s the case! :)\nPersonally I think second class (four bed compartment) is the best way to go. First class (two beds) is significantly more expensive and you’ll miss out on much of the social interaction which is half the point of taking this journey in the first place.\nThird class (an open carriage with numerous bunks) is available on some trains, but while it’s cheap and sometimes recommended for single female travellers (safety in numbers) the complete lack of privacy is a bit much for me.\nSpeaking of safety, we found the whole experience to be very safe. Indeed I’ve read that the Trans-Siberian is sometimes considered safer than any Russian city. If you do have any problems report them immediately to the conductors on your carriage. There’s a decent chance of stolen items being recovered as the thief can’t escape the train while it’s moving, and if need be the conductors will involve the police at the next station.\nBeds are allocated in advance and your place number is printed on your ticket. You can find your allocated bed by referring to the number either on the door of your compartment or on the wall near the window.\nIf you get a choice when booking your tickets I’d recommend a lower bed for solo travellers, or for couples/pairs to take one upper and one lower bed. The lower beds have the best storage (see below) but they’re also seats for all four compartment members during the day: if you take both of the lower beds you could be kicked out of bed if those on the top wake up early and want to sit down. (It’s unlikely because they’d probably be too polite to do that, but I wouldn’t want to feel like I was keeping them trapped up there if I wanted to sleep in.) If you share the cabin down the middle by taking one upper and one lower bed the storage space is evenly distributed and it’s easier to run on your own schedule.\nAs far as storage goes, most of the space in the compartment is underneath the two lower beds. Sometimes this area is even enclosed in a metal box that can’t be accessed unless the bed is lifted up. The only other storage space is above the compartment door but sometimes it’s only enough for one large travel backpack or a couple of small backpacks.\nAs a result of this, backpacks or small (carry on size) suitcases are the easiest to deal with. There isn’t any room in the four bed compartments for large suitcases (I’m not sure about first class) but it may be possible to store one in the conductor’s cabin (possibly for a fee).\nYou need to show your train ticket (and sometimes passport) before you board the train, then your ticket again shortly after the train departs. Sometimes the conductors will keep your ticket during the journey but they’ll give it back just before you disembark.\nThere are Russian, Mongolian and Chinese trains. The staff are from the same country as the train they work on.\nThe trains generally have a similar layout but there are slight differences. For example, the Rossiya (Moscow-Vladivostock) is newer and features a powerpoint in each compartment. Other trains may only have a couple of powerpoints in the corridor, if at all.\nYou could be mixed in with the locals or separated from them in a kind of “tourist only” carriage. We took four sleeper trains on our Trans-Mongolian route: two of them were mixed and two were segregated. You never know what you’ll get but both are great experiences in different ways!\nEastbound Trans-Mongolian trains run on Moscow time until they switch to Ulaanbaatar time in Mongolia, even though they pass through four other timezone changes before then.\nThe restaurant car is changed in each country to match, ie it will be a Russian restaurant car in Russia, Mongolian in Mongolia and Chinese in China.\nThen there are some legs where there’s no restaurant car at all. For example, between leaving the Russian one behind and collecting the Mongolian one after crossing the border.\nThe restaurant car food is okay but there are cheaper ways to feed yourself (see below). Also, the restaurant does sometimes run out of food (or particular dishes, at least).\nA samovar (boiling water dispenser) is available at the end of each carriage and it’s free to use. (I did see a price list which included plain boiling water on a Mongolian train but no one ever had to pay – we think maybe that was the fee if you asked the attendant to bring it to you.) It’s best to bring your own tea/coffee making supplies but if you don’t you can buy teabags from the conductors and they can also provide a glass.\nThere should be a timetable posted on the wall of each carriage that lists all the stops the train will make and how long they’ll last. (Note however that it might only be available in cyrillic.) Some stops are only a couple of minutes but others can be up to half an hour, enough time to hop off and stretch your legs on the platform.\nThere are opportunities to buy food at these extended stops: either from convenience booths on the platform or from women who sell homemade snacks such as cooked or dried fish, piroshki and dumplings for around 50-100 руб each.\nSome conductors will herd you back onto the train before it moves on but others won’t so you’d better keep an eye on the time or you may find yourself watching your train (with all your luggage) chugging off into the distance without you!\nThere are two western style toilets in each carriage and they are cleaned during the journey, but they can get a little smelly as toilet paper can’t be disposed of in the toilet bowl, it has to go in the bin – with everyone else’s used toilet paper. (But you’ll probably be used to this if you’ve spent any time in Russia, Mongolia or China.)\nThere are no showers in second class carriages, only the washbasins with the toilets. If you aren’t breaking up your journey with any overnight stays in towns along the way and are desperate for a shower there’s a lengthy (3 hour) stop at Наушки (Naushki, the last Russian stop before the Mongolian border where the first round of passport checks are done). Public showers are available on the platform for 90-100 руб per person.\nPeople may come down the corridor offering goods for sale such as knitted shawls, fur hats or gold necklaces but you don’t have to buy anything. Near the borders you’ll also get people offering money exchange but the rate is very poor. (Either way, use up or get rid of your tugriks before you leave Mongolia as no one will touch them outside the country.)\nI can’t speak for the Trans-Manchurian or the straight Trans-Siberian routes but as far as the Trans-Mongolian goes, the best views are between Irkutsk, Ulaanbaatar and Beijing. The view on the longest leg (Moscow-Irkutsk) is unfortunately flat and repetitive.\nIf you’re alighting from the train early in the morning it would be wise to set an alarm, but the conductors will most likely to wake you one hour before your stop (whether you think you need a whole hour or not).\nWhat to bring:\n- Enough cash for the journey, ideally any rubles and yuan in small denominations (it doesn’t matter so much for tugriks). You won’t encounter any ATMs at the short stops along the way.\n- Some food. As mentioned above there are opportunities to buy food along the way but you should bring breakfast supplies and it’s worth having a back up stash of things such as cup noodles, easy open tinned tuna, crispbread, spreads that don’t require refrigeration, nuts, fruit.\n- Camping utensils or a good spork.\n- Light and sturdy cups and tea/coffee supplies.\n- Bottled water, although you can buy more at the longer stops.\n- Toilet paper. Sometimes it’s provided, sometimes it’s not – better to be safe than sorry!\n- Cleaning wipes.\n- Eye mask.\n- Russian phrase book. You might not need it but we only had the bare essentials in a combined Eastern Europe phrase book and we wish we’d had something more detailed for the 80 hours we spent sharing a compartment with a chatty Russian babushka.\n- Lockable luggage cable to secure bags to handles and fixtures so no one will be able to walk off with them. (We did this and it actually felt a bit unnecessary, but again, better safe than sorry.)\n- A book to read – but don’t expect the journey to be extremely productive. Even though you’ll be spending days on a train after a while you do enter a kind of twilight zone where all you may be doing is chatting, eating and napping, but it somehow seems to consume a lot of time.\nRegarding foodstuffs: you don’t have to bring these with you from your home country – if anything it’s more fun to stock up at a Russian supermarket!\nRegarding alcohol: apparently you’re not supposed to take any (I’ve heard you can even get thrown off the train if you’re caught) but we were advised (by a Russian) to take one litre of vodka with us, “just in case”! In the end we took half a litre, but none of the Russians we met were interested in sharing it with us. (Plenty of travellers were though!) Note that the Russian way to drink vodka is to eat a bite of pickled gherkin after each sip, so if you do decided to take some vodka bring pickles too (or olives) for authenticity.\n- Pack essentials into a smaller, easier to access bag – it can be quite difficult/annoying to get into your main pack once it’s stowed away.\n- Keep your food supplies together in one bag so it’s easy to access.\n- Have flipflops or slippers handy for walking around the train and on the platforms.', ""Dutch trains: Regional and Intercity\nRegional and Intercity trains serve all major cities and smaller towns in the Netherlands.\n- Book up to 120 days in advance\n- Regional trains\nDutch trains: Regional and Intercity\nIntercity are domestic trains operated by NS, the Dutch national railway, in the Netherlands. The trains, also known as IC, make stops between major cities. Intercity trains exist as single decker and double decker, both equipped with first and second class carriages. Many IC offer free Wi-Fi Internet on board. Seat reservations are not mandatory.\nOther regional trains include the Sprinter. They are mostly used to connect major cities to smaller towns. Sprinter trains are mostly modern trains equipped with first class and second class carriages. A double decker version of the Sprinter train is also operated by NS. As in the Intercity trains, seat reservations are not mandatory.\nNote: A supplement is needed when travelling on the Amsterdam Schiphol-Rotterdam route with the Intercity direct.\n|Main routes||Travel time|\nReceive your ticket directly to your address. Just take them with you and you are ready to travel. Shipping fees apply.\n- Dutch trains: Regional and Intercity tickets are open for booking 120 days ahead.\n- Get the lowest prices by booking early and don’t wait until the last minute as cheaper seats sell the fastest.\n- Opt for off-peak trains when you have to travel short notice. They are more affordable than morning and evening trains along with those running on holiday eves, Friday and Sunday afternoon.\nA class apart\nClasses of service\n|First class||Second class|\n|Comfortable seats Spacious seats with a headrest and generous legroom. small>|\n|Ergonomic seats Cosy with more legroom, ergonomic seats are ideal to enjoy the trip. small>|\nFrom our travel community\nReviews & ratings Dutch trains: Regional and Intercity\nCommon Questions, Simple Answers\nQ. Can I choose a specific seat on a train when making my reservation?\nA. At this time, it’s not possible to choose a specific seat on our website when booking a reservation.\nQ. What's the difference between a rail pass and a train ticket?\nA. A rail pass and a train ticket are two completely different types of travel documents.\nA train ticket is valid for a single journey between two cities. Many tickets also include a reservation, which mean that they are valid only for a specific train, on a specific date and at a given time. To learn more about tickets, read our article: tickets explained.\nA rail pass, on the other hand, is good for travel in an entire geographical area (1 country, 2 countries or more). With it you can take as many train trips as you want within that geographical area, for as many travel days as you purchased, within a set period of time (the pass validity period).\nQ. Is it possible to have travellers of different genders in the same sleeping compartment?\nA. For gender specific compartments, men and women must be in separate sleeping compartments. However, if you reserve all beds within a compartment, you may mix genders.\nGender-specific compartments include private, double, triple, deluxe double, deluxe triple, and quad sleeping compartments. If your traveling party doesn’t fill the compartment, you will be divided by gender.\nQ. How much luggage can I take on the train?\nA. Technically, you’re not restricted in terms of luggage. However, keep in mind that you’ll need to carry your bags through the train station and onto the train. Once on board you’ll need to find a place to put your bag(s). Small and medium sized bags typically fit on racks located above the seats. Larger suitcases and such can be stored in luggage racks found near the train door entrance.\nBe aware that you are responsible for your luggage. The railways assume no responsibility in case of loss or theft of baggage carried on board.\nWhen traveling on a Eurostar, Thalys, or TGV train, you are limited to two large items per traveller (one large item per child on Eurostar trains), maximum 85cm (33 inches) in any one dimension, plus one small item of hand luggage.\nYour bag(s) must be properly labeled with your first and last name. In general, it’s a good idea to keep luggage tags on your baggage for easy identification.\nQ. How do I use my Italo print at home e-ticket for travel in Italy?\nA. While Italo train tickets are considered print at home e-tickets, you don’t actually need to print anything or retrieve a voucher or boarding pass at the station.\nAll that is required is the confirmation (PNR) code printed on your Rail Europe booking confirmation email. Just present your e-ticket confirmation code and a valid photo id to the conductor on the train - it’s that easy!\nQ. What is the Rail Protection Plan™?\nA. We offer an exclusive Rail Protection Plan™ that gives you the flexibility to change your train ticket, rail pass, reservation, and select activities with no questions asked. You’ll also be protected in the event of a railroad strike for train tickets, and in the case of loss or theft of paper tickets, rail passes, city passes or tours.\nThe Rail Protection Plan™ may be added to your booking on the delivery page after adding the products to your itinerary or any time prior to final payment being applied to your booking. (Select city passes and tours are excluded from coverage). The Rail Protection Plan™ fee is non-refundable and non-exchangeable. View the detailed terms and conditions of the Rail Protection Plan™ coverage.\nQ. What is a Rail Pass?\nA. A rail pass is a travel document that entitles you to travel on European trains in a specific geographic area (country/countries), for a specific number of days.\nBe mindful that having a rail pass doesn’t necessarily mean that you can hop on any train at will, within the geographical area covered by your rail pass. Depending on the country, the type of train and the route, you may need to purchase seat reservations at an additional cost.\nLastly, rail passes are packed with additional benefits, also known as rail pass bonuses which can range from discounted shopping in retail stores, to preferential rates on other modes of transportation like ferry or bus, to rebates on hotel nights, reduced admission at museums attractions, and more.\nQ. Can I ensure that my traveling party will share a compartment on an overnight train?\nA. Our booking system always attempts to book all travellers in the same compartment on overnight trains, as long as you submit your request for your entire traveling party at once.\nOur ability to book all travellers in the same compartment ultimately depends on compartment availabilities on the train you’re booking. The earlier you make your reservations, the better the chance that everyone in your travel party will be in the same compartment.\nIf a compartment can’t be fully filled by the traveling party alone, some trains may need to separate you into gender-specific compartments. Therefore, if you are a traveling party of mixed gender – the way to ensure that you will be traveling together is to make sure that you book a compartment that you fill entirely.\nQ. What is the difference between 1st and 2nd class on the Trenitalia (Italy) train?\nA. First class on the Italian day train offers large, comfortable reclining seats, and Bar-buffet car offering a complete selection of food and beverages for purchase. (on major routes)\nSecond class on the Italian day train offers generous legroom and Bar-buffet car offering a complete selection of food and beverages for purchase. (on major routes)\nFrecciarossa high-speed trains offer four levels of service, accommodating all travel budgets. Choose from Executive, Business, Premium, and Standard class.\nIn Executive class, you’ll indulge in luxury and comfort including lounge access at the station, dedicated car with just 8 luxurious reclining leather seats, convenient meeting area to conduct business, gourmet meal served at your seat, open bar, and more.\nBusiness class also provides an upgraded level of service with plush leather reclining seats, welcome drink & snack, and platform check-in.\nIn Premium class you’ll experience the services of 1st class in a 2nd class seat, including a welcome drink & snack, sleek leather seating, and more.\nLastly, Economy class is a budget-friendly 2nd class seat, offering a comfortable reclining cloth seat.\nQ. How far in advance can I buy my rail pass?\nA. A rail pass can be purchased anywhere from 6 to 11 months prior to your first travel day. We advise you to check the specific conditions of use for the pass you are interested in.""]"	['<urn:uuid:9fe90044-c8e3-4b7b-94ed-bb7256b14f70>', '<urn:uuid:34f6cd42-7fd8-46bb-af34-e8245eec2098>']	factoid	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T02:03:54.148118	9	36	3252
11	As someone deeply interested in metalworking techniques, I'd like to know what specialized equipment and workspace setup is required for jewelry making, and what are the current industry standards for material safety in professional jewelry?	The workspace and equipment requirements can be adaptable to different situations. According to 'Simple Soldering,' metalworking doesn't necessarily require a large studio space or expensive equipment - you can set up a jewelry workspace that fits your budget and living space. Essential tools include those for soldering, hammering, casting, and surface decoration as mentioned in 'The Complete Jewelry Making Course.' For industry standards regarding materials, there are specific guidelines, particularly the ASTM F specification for adult jewelry, which covers toxic metals and mechanical dangers. While children's jewelry has strict lead content regulations (CPSIA guidelines), adult jewelry has fewer restrictions, though there are growing concerns about cadmium content in adult jewelry, with testing showing some items containing over 90% cadmium.	"['- A Selection of the Best Jewelry Books Available\n- New Encyclopedia of Jewelry Making Techniques\n- The Complete Book of Jewelry Making: A Full-color Introduction to the Jeweler’s Art\n- Jewelry Making and Beading For Dummies\n- The Complete Jewelry Making Course: Principles, Practice and Techniques: A Beginner’s Course for Aspiring Jewelry Makers\n- Simple Soldering: A Beginner’s Guide to Jewelry Making\nA Selection of the Best Jewelry Books Available\nNew Encyclopedia of Jewelry Making Techniques\nSince its first publication in 1995, the Encyclopedia of Jewelry Making Techniques has been devoured by students borrowing a well-thumbed copy from their college library, established jewelers wishing to find out more about new techniques, as well as all those who just enjoy making jewelry as a hobby, who have kept it beside their workbench at home as a third hand to help them through a tricky process. Since then many books have been written about jewelry making. However, this volume still enjoys a huge following. To update it, a finished example now illustrates every technique to give a context to the method described. There are also new articles on harvesting scrap metal, designing jewelry, comparing metals’ properties, and a wonderful demonstration of precious metal clay – a material that didn’t even exist when this book was first published. Doyenne of metal clay, Sue Heaser, gives a masterclass in this technique. Jewelry making is a rewarding and fascinating craft. Although you may find that you enjoy some aspects of it more than others, the techniques described here will give you a thorough understanding of all the basic information you need to create unique and professional looking pieces.\nThe Complete Book of Jewelry Making: A Full-color Introduction to the Jeweler’s Art\nThis amazing comprehensive guide to the jeweller’s art is a goldmine of information – big, beautiful, and full of hundreds of colour photographs. It’s simply the most comprehensive sourcebook available, with its enlightening discussion of basic metallurgy (alloying, smelting, annealing and pickling), and professional-quality techniques for cutting, piercing, soldering and making closures for rings. Learn the tricks of the trade for enhancing surfaces; etching, combining metals, creating textures, adding colour, engraving, enamelling and gem setting. Seven magnificently presented projects include a silver repousse pendant, an articulated gold bracelet with clap, a multi loop-in-loop chain, and a square hollow-construction ring to die for.\nJewelry Making and Beading For Dummies\nThis title features step-by-step projects and full-color examples. Create unique jewelry to wear with everything from jeans to a ball gown! Want to make your own jewelry? This easy-to-follow guide gives you lots of hands-on instruction in making simple, fabulous jewelry and working with beads. From wire wrapping and knotting to weaving and polishing, you’ll get all the skills you need to make necklaces, bracelets, earrings, pins, key chains, wine charms and items for the home. This guide helps to discover how to: create funky and elegant designs; work with leather, silver, and stones; string, knot, and weave beads; avoid common mistakes; and, host a trendy jewelry party.\nThe Complete Jewelry Making Course: Principles, Practice and Techniques: A Beginner’s Course for Aspiring Jewelry Makers\nThis comprehensive and heavily illustrated manual teaches the craft of jewelry making to students looking to create professional quality items. The author covers every step of the process, from creating original design concepts to fashioning professionally finished pieces of jewelry. She lists all required tools and equipment, explains their uses, advises on safe working practices, and then guides her readers through every stage of the jewelry making process in a series of carefully structured tutorials. Students will learn–\n- How to start with an original idea, translate it into a workable design, and then use the design as a pattern to make beautiful wearable jewelry\n- How to work with precious and semiprecious stones, metals, glass, plastic, resin, and wood\n- How to present and sell one’s creations to dealers and wholesalersClear, full-color, step-by-step photos demonstrate the methods of fashioning metals by heating, hammering, casting, soldering, riveting, polishing, finishing, and adding surface decoration. At-a-glance panels explain how to apply the right techniques when working in specific metals, as well as ideas for experimenting with inexpensive substitutes before advancing to precious materials. Here’s the perfect book for transforming hobbyists into true professionals. It will also serve as a useful textbook for those conducting organized courses in jewelry design. Color photos on every page.\nSimple Soldering: A Beginner’s Guide to Jewelry Making\nMetalworking is generally regarded as a skill that takes years of dedication, requires a large studio space, and costs a lot of money. Fortunately, Simple Soldering proves that does not need to be the case. This handy how-to guide is complete in its exploration of the craft of creating soldered metal jewelry, including tools, techniques, and 20 beautiful projects that beginners and enthusiasts can make at home.\nAuthor and teacher Kate Richbourg demystifies basic soldering for any home crafter, showing how to create sophisticated, polished, and professional-looking jewelry pieces through simple soldering techniques. First, she instructs how to set up a jewelry workspace that fits the confines of your budget and living space. Detailed step-by-step instructions walk you through the basic tools and materials you need, plus how to use them. A sample chapter gives a host of introductory exercises that teach solid skills, allowing you to test these techniques on a small scale. Finally, you’ll discover 20 finished projects that include earrings, pendants, rings, bracelets, and clasps that may also include bead or wire embellishment.\nKate also demonstrates how to combine and layer techniques to gorgeous effect. She also examines common mistakes, shows how to correct or adapt them, and gives advice on when it’s time to start over. Most of all, having taught thousands of classes on soldering, Kate has a “you can do it!” attitude that shines through to help even the most reluctant jewelry maker fire up the torch with ease.\nPaired with an instructional DVD, Kate’s expert teaching skills will help projects come alive, right in your own studio.\nWith Simple Soldering, the art of metal working one-of-a-kind jewelry is now at your fingertips.', 'Jewellery British English or jewelry American English  consists of small decorative items worn for personal adornmentsuch as broochesringsnecklacesearringspendantsbraceletsand cufflinks. Jewellery may be attached to the body or the clothes. From a western perspective, the term is restricted to durable ornaments, excluding flowers for example.\nAccording to the Consumer Product Safety Improvement Act CPSIA""any children\'s product that contains more than parts per million ppm of lead in any part that is accessible will be treated as a banned hazardous substance. Lead-content limits: Starting on February 10,consumer products intended for children 12 and under were not allowed to contain more than parts per million [PPM] of lead in any accessible part. On August 14,the allowable lead content level dropped to parts per million [PPM].\nConsumer advocates were hopeful cadmium had disappeared from the U. No laws address cadmium in adult jewelry, however, and last year the center decided to check those products. Lab testing found 31 adult jewelry items purchased from retail stores were at least 40 percent cadmium, and most were more than 90 percent, according to results shared exclusively with the AP.\nMost of the time, adulting can be a real drag. But there are some perks, like hosting Pinterest-worthy wine tastingsupgrading your apartmentand finally being able to invest in some quality jewelry. The idea of grown-up accessories may seem dull, and we totally get it — nobody wants to give up fun, playful baubles in the name of being grown. But adding just a few timeless pieces into your mix of colorful earrings, rings, necklaces, and the like will go a long way toward taking your look from dorm room to board room.\nWe opted to directly incorporate the CPSIA-mandated federal lead limit into our developing standard, which was intended particularly to address cadmium content. We also wished for companies to be certain that products manufactured in accord with the completed standard would be in accord with all known assessments of risk. What came out of the hard work of that F\nProducts in our online store are labeled according to the classification chart below. Information on this page covers the basics you need to know to comply with California law. Even if you don\'t live in California, you might need to follow these guidelines.\nExpand your metalsmithing vocabulary and learn how to make convex and concave shapes. Learn embossing, working with dapping tools, hammers, sinusoidal stakes, and more. Students will create shallow formed earrings, two rings, and a concave bracelet.\nThis specification only applies to adult jewelry, which refers to jewelry intended to be worn primarily by people over 12 years of age. Jewelry intended to be worn by people under 12 years of age is covered by the ASTM F specification for children\'s jewelry. The ASTM F specification covers two main types of hazards: the presence of toxic metals and mechanical dangers.\nThough you might think you already know what you need to do to care for your jewelry, chances are there are some tips and tricks — and important factors — that you\'ve never considered before. If you\'ve never had nice jewelry before, caring for the nicer pieces you might acquire as you get older, like engagement rings, fancier necklaces or bracelets, or silver pieces jewelry or not you may inherit from family members, can be kind of a mystery. Maybe you\'ve never really needed to go out of your way to take care of your jewelry before, because it was mostly cheaper, trendier jewelry.']"	['<urn:uuid:0023713b-13d6-4b30-bad5-6e0de39eb351>', '<urn:uuid:ef878cdc-1ca4-4089-bf56-d1c419fbcff0>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T02:03:54.148118	35	119	1597
12	my jaw hurts after dental operation three days ago but was fine before is this normal	If after three to four days of comfort you begin to have increased pain, throbbing, or earache in the area of surgery, you should contact your dental office. Most surgical discomfort should begin to subside three to four days after surgery, so increased pain after a period of comfort could be a concern that needs medical attention.	['After Wisdom Tooth Removal\nThe removal of impacted teeth is a serious surgical procedure. Post-operative care is very important. Unnecessary pain and the complications of infection and swelling can be minimized if the instructions are followed carefully.\nImmediately Following Surgery\n- The gauze pad placed over the surgical area should be kept in place for a half hour. After this time, the gauze pad should be removed and discarded.\n- Vigorous mouth rinsing or touching the wound area following surgery should be avoided. This may initiate bleeding by causing the blood clot that has formed to become dislodged.\n- Take the prescribed pain medications as soon as you eat something, before the numbness wears off.\n- Restrict your activities the day of surgery and resume normal activity when you feel comfortable.\n- Place ice packs to the sides of your face where surgery was performed. Refer to the section on swelling for explanation.\n- Bite on gauze (do not chew) for 30 minutes. Then remove the gauze and check for bleeding.\n- Repeat process with fresh gauze as needed until bleeding has diminished to a slow oozing or stops altogether.\n- The gauze must be placed directly over the surgical sites to be effective.\n- For continued bleeding, a tea bag dipped in cold water may be very helpful in place of the gauze.\n- Do not rinse or spit for the first 24 hours. This will disturb the blood clot and prolong bleeding. It is better to just swallow normally at this time.\n- Do not use a straw for the first three days. Smoking is also very harmful to the blood clot and should be kept to a minimum during this time.\n- Some bleeding for the first day or two is normal. If bleeding persists, or is excessive, please notify us.\n- Fill your prescriptions and take your medications as directed.\n- Take the first dose of pain medication after you get home before the numbness wears off. We recommend you NOT take the medicine on an empty stomach, so take it with a large glass of liquid or light food (milkshake, frosty, soup, etc.)\n- Remember that certain antibiotics may render birth control pills ineffective. Therefore, all female patients who are currently taking birth control pills should use another form of contraception, during the entire cycle, when antibiotics have been prescribed.\n- Do not operate machinery or drive a car for twenty-four (24) hours following your surgery if IV sedation has been administered or while taking the narcotic.\n- Apply ice packs immediately to all areas of the face where surgery was performed.\n- Place the ice packs on for 30 minutes and off for 30 minutes.\n- Ice is helpful for the first 24 to 48 hours.\nDiet- Your nutrition is extremely important.\n- Do not eat any foods that require chewing while you are numb, however, resume a normal diet as soon as possible after surgery.\n- Eating solid foods will help limber up the muscle stiffness in your jaws that normally accompanies surgery. Gentle exercise and chewing gum may also be helpful.\nSwelling and/or Bruising\n- Swelling following any surgical procedure is normal. This is especially true with removal of difficult or impacted teeth. The swelling and/or bruising may be minimal or extensive, and may persist for several days.\n- Be sure to use the ice packs as recommended.\n- It is normal to experience a slight earache or sore throat after surgery. There may also be a slight elevation of temperature for 24 to 48 hours. If the temperature continues after this period of time, please notify us.\nHygiene- Cleanliness is a must for a rapid and uncomplicated recovery.\n- You should brush your teeth gently.\n- The day after surgery, brush as you normally would and begin rinsing with a warm salt water solution (1/2 teaspoon of salt in a glass of warm water) four to five times per day for one week.\n- We do not recommend the use of commercial mouthwashes as they contain alcohol that can irritate the wound.\n- If you were given a syringe to use to help keep the areas clean, begin using it five to six days after surgery with warm water at least twice a day until the holes are closed (usually within three to four weeks).\n- Most surgical discomfort should begin to subside three to four days after surgery.\n- If after three to four days of comfort you begin to have increased pain, throbbing, or earache in the area of surgery, please call our office.\n- Often operations are performed very close to nerves. This can sometimes cause lingering numbness and altered sensations, especially of the lower lip or tongue. This condition is usually temporary and will generally disappear when swelling recedes and the nerve heals. Should numbness persist beyond the first week, please inform us.\n- Sutures are placed in surgery sites to minimize post-operative bleeding and to aid in the healing process. The sutures will begin to dissolve and become dislodged a few days after surgery. Just remove the suture form your mouth and discard it. The extraction sites heal from the inside out, so it is normal for the gum tissue to be open still. These areas will continue to fill in over the next few weeks.']	['<urn:uuid:9342ea38-8312-4dfe-9c7c-97aa3eec532f>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T02:03:54.148118	16	57	890
13	What happens when YouTubers appeal removed videos?	Between July and September, YouTube removed over 5.6 million videos for violating Community Guidelines and received approximately 271,000 removal appeals from creators. After reviewing these appeals, YouTube reinstated about 29,000 videos. Any Creator on YouTube has the option to submit an appeal when their video is removed, and this appeals process helps YouTube understand the accuracy of their enforcement systems.	"[""On policy development at YouTube\nDec 01, 2022 – minute read\nSince the earliest days of YouTube, we've had Community Guidelines to establish what's allowed on our platform. These rules of the road gave rise to creative expression while prioritizing the protection of the entire YouTube community from harmful content. This balance is critical for allowing new voices to participate and to promote the sharing of ideas. It’s also necessary to ensure YouTube’s long-term success as a business, as our advertising partners fundamentally do not want to be associated with harmful content.\nA few questions have regularly cropped up around how we decide where to draw these lines or why it can take so long to develop and launch new policies (for a broader overview of how we think about our responsibility efforts, see here). So in this blog, we’re shedding more light on how we develop our policies and the processes that go into enforcing them.\nThe world moves quickly and our policies need to keep up. That’s why we regularly review our policies to make sure that — similar to the laws that govern civil society — they reflect the changes that occur both on and off our platform. To be clear: the vast majority of content on YouTube does not violate our guidelines. But we still check for gaps that may have opened up or hunt for emerging risks that test our policies in new ways.\nAs we work to keep our policies evolving with the current landscape, our guiding focus is around one major goal: preventing egregious real-world harm. This doesn’t mean that we remove all offensive content from YouTube, and we generally believe that open debate and free expression leads to better societal outcomes. But we’re careful to draw the line around content that may cause egregious harm to our users or to the platform.\nThis can include physical harm. For example, when claims that linked 5G technology to the spread of COVID-19 resulted in damage to cell towers across the United Kingdom, we moved quickly to make them violative. Or it could mean significant harm to democratic institutions, which is why we don’t allow claims that aim to mislead people about voting — including by promoting false information about the voting times, places or eligibility requirements.\nWe also work closely with NGOs, academics, and relevant experts from all sides and different countries to inform this policy review. They help flag new concerns, or bring a deep understanding to complex topics that are prone to consistent change. For example, we established our COVID-19 misinformation policy at the start of the pandemic alongside health authorities like the Center for Disease Control and World Health Organization. Later, as their guidance shifted to ease mask and social distancing restrictions, we updated our policies around content that questioned the efficacy of masks and social distancing.\nOnce we’ve identified an area where a policy update is needed, that’s where our Trust & Safety team comes in to develop a tailored solution. We start by assessing a few things. How commonly found is this specific type of harmful content on YouTube (and what’s its potential to grow)? And how is it managed under our current Community Guidelines?\nThen we watch dozens or even hundreds of videos to understand the implications of drawing different policy lines. Drawing a policy line is never about a single video; it’s about thinking through the impact on all videos, which would be removed and which could stay up under the new guideline. Following this comprehensive review, the team shares various options for policy lines, making sure to detail examples of videos that would be removed or approved for each (as well as different enforcement actions, like removal vs. age-restriction).\nA top choice is selected from those draft options and then goes through further rounds of assessment. At this stage, we’re looking to understand whether the proposal can meaningfully achieve a few key goals:\nIf we’re satisfied that we’re hitting these targets, an executive group made up of leads across the company reviews the proposal. Final sign-off comes from the highest levels of leadership, including YouTube’s Chief Product Officer and CEO. If at any point there is consistent disagreement between teams about where we’ve drawn the line, the policy is sent back to the drawing board.\nThroughout the policy development process, we partner closely with a range of established third-party experts on topics like hate speech or harassment. We also work with various government authorities on other important issues like violent extremism and child safety.\nPartnerships are especially critical to support policy enforcement for regional issues, where language or cultural expertise is often needed to properly contextualize content.\nExperts help us forecast how global events could cause harmful content to spread across our platform, including uncovering gaps in our systems that might be exploited by bad actors, or providing recommendations for new updates. And like with COVID-19, they provide input that helps us adapt policies in situations where guidance can change quickly.\nThese partnerships are also especially critical to support policy enforcement for regional issues, where language or cultural expertise is often needed to properly contextualize content. For example, we worked closely with experts in 2021 during the coup d'état in Myanmar to identify cases where individuals were using speech to incite hatred and violence along ethno-religious lines. This allowed us to quickly remove the violative content from our platform.\nPeople often think about content moderation as reactive in nature — that we only take content down when it’s flagged by our systems or people. In reality, the bulk of our work focuses on the future. There’s a long process that’s designed to give our teams visibility into emerging issues before they reach, or become widespread on, our platform.\nThat valuable visibility is driven by our Intelligence Desk, a team within YouTube’s Trust & Safety organization. These specialized analysts identify potentially violative trends — whether new vectors of misinformation or dangerous internet challenges — and the risks they pose. They’re also regularly monitoring ongoing threats like extremist conspiracy theories, both tracking their prevalence across media and evaluating how they morph over time.\nThese insights then feed into thinking through how current or future policies would manage these new threats. For example, based on evidence gathered by the Intelligence Desk, we updated our hate and harassment policies to better combat harmful conspiracy theories on our platform.\nThe implementation of a new policy is a joint effort between people and machine learning technology. In practice, that means in order for a policy to be successfully launched and enforced, people and machines need to work together to achieve consistently high levels of accuracy when reviewing content.\nWe start by giving our most experienced team of content moderators enforcement guidelines (detailed explanation of what makes content violative), and ask them to differentiate between violative and non-violative material. If the new guidelines allow them to achieve a very high level of accuracy, we expand the testing group to include hundreds of moderators across different backgrounds, languages and experience levels.\nAt this point, we begin revising the guidelines so that they can be accurately interpreted across the larger, more diverse set of moderators. This process can take a few months, and is only complete once the group reaches a similarly high degree of accuracy. These findings then help train our machine learning technology to detect potentially violative content at scale. As we do with our content moderators, we test models to understand whether we’ve provided enough context for them to make accurate assessments about what to surface for people to review.\nAfter this testing period, the new policy can finally launch. But the refinement continues in the months that follow. Every week, our Trust & Safety leadership meet with quality assurance leads from across the globe (those responsible for overseeing content moderation teams) to discuss particularly thorny decisions and review the quality of our enforcement. If needed, guideline tweaks are then drafted to address gaps or to provide clarity for edge cases.\nOnce models are trained to identify potentially violative content, the role of content moderators remains essential throughout the enforcement process. Machine learning identifies potentially violative content at scale and nominates for review content that may be against our Community Guidelines. Content moderators then help confirm or deny whether the content should be removed.\nThis collaborative approach helps improve the accuracy of our models over time, as models continuously learn and adapt based on content moderator feedback. And it also means our enforcement systems can manage the sheer scale of content that’s uploaded to YouTube (over 500 hours of content every minute), while still digging into the nuances that determine whether a piece of content is violative.\nFor example, a speech by Hitler at the Nuremberg rallies with no additional context may violate our hate speech policy. But if the same speech was included in a documentary that decried the actions of the Nazis, it would likely be allowed under our EDSA guidelines. EDSA takes into account content where enough context is included for otherwise violative material, like an educational video or historical documentary.\nThis distinction may be more difficult for a model to recognize, while a content moderator can more easily spot the added context. This is one reason why enforcement is a fundamentally shared responsibility — and it underscores why human judgment will always be an important part of our process. For most categories of potentially violative content on YouTube, a model simply flags content to a content moderator for review before any action may be taken.\nWe’re driven in all of our work to live up to our Community Guidelines and further our mission to allow new voices and communities to find a home on YouTube. Success on this front is hard to pin down to a single metric, but we’re always listening to feedback from stakeholders and members of our community about ways we can improve — and we continuously look to provide more transparency into our systems and processes (including efforts like this blog).\nFrom July through September of this year ... for every 10,000 views, between 10 and 11 were of content that violated our Community Guidelines.\nTo measure the effectiveness of our enforcement, we release a metric called our violative view rate, which looks at how many views on YouTube come from violative material. From July through September of this year, that number was 0.10% – 0.11%, which means that for every 10,000 views, between 10 and 11 were of content that violated our Community Guidelines.\nWe also track the number of appeals submitted by creators in response to videos that are removed (an option available to any Creator on YouTube), as this helps us gain a clearer understanding about the accuracy of our systems. For example, during the same time period mentioned above, we removed more than 5.6 million videos for violating our Community Guidelines and received roughly 271,000 removal appeals. Upon review, we reinstated about 29,000 appeals.\nAnd while metrics like appeals, reinstatements, and our violative view rate don’t offer a perfect solution to understand consistency or accuracy, they’re still pivotal in benchmarking success on an ongoing basis.\nCommunity Guidelines are concerned with language and expression — two things that, by their very nature, evolve over time. With that shifting landscape, we’ll continue to regularly review our policy lines to make sure they’re drawn in the right place. And to keep our community informed, we’ll be sharing further how we’re adapting in the months ahead.""]"	['<urn:uuid:ed34dcd6-2fb1-48c7-a1da-307baf891647>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T02:03:54.148118	7	60	1922
14	Working in climate adaptation planning, I need to understand how Canada's warming trends compare to global averages, and what specific land management measures are recommended for relocating at-risk infrastructure?	Canada is experiencing warming at more than twice the global rate, with the Canadian Arctic warming at about three times the global rate. This accelerated warming particularly affects infrastructure through permafrost thaw and coastal erosion. Regarding land management measures for relocating infrastructure, buildings must be moved to locations that are landward of projected erosion areas for 75 years or the building's lifetime. This requires creating site plans showing original building footprints and relocation sites, ensuring compliance with legal setback requirements, and considering property size constraints. For rapidly eroding sites, a 70 ft (20 m) setback is recommended, while low to moderately eroding sites require 35 ft (10 m) setback.	"[""Climate change adaptation in Canada\nCanada’s climate is already changing. Higher temperatures, shifting rainfall patterns, extreme weather events and rising sea levels are just some of the changes already affecting many aspects of our lives.\nChanges in climate will persist and, in many cases, will intensify over the coming decades. That will have significant impacts on Canadian communities through our economy, social well-being (health, culture, etc.) and environment. We must understand these impacts and the options available to us if we want to build resilience through adaptation, reduce the associated risks and costs of climate change’s impacts and support sound decision-making.\nWhat is climate change adaptation?\nClimate change adaptation is any activity that reduces the negative impacts of climate change or helps people cope with them, or one that takes advantage of new opportunities that result from climate change.\nMitigation activities reduce the rate and magnitude of climate change, while adaptation addresses current and future impacts. Successful adaptation doesn’t mean that negative impacts won’t occur, only that they will be less severe than without adaptation.\nAdaptation involves making adjustments in our decisions, activities and thinking because of changes in our climate. As part of our focus on adaptation, we’re working to understand what climate change means for those living in Canada, which approaches to climate change impacts and adaptation are most effective and where gaps in knowledge and action remain.\nSince 1998, the Climate Change Impacts and Adaptation Division of Natural Resources Canada has conducted work on climate change impacts and adaptation. It has developed considerable expertise together with a vast network of external experts and partners. The division leads the Canada in a Changing Climate National Assessment Process, delivers Canada’s Climate Change Adaptation Platform and uses its network and expertise to lead a program that co-funds projects to address key knowledge and capacity barriers to adaptation, including for communities and natural resource sectors.\nAreas affected by climate change in Canada\nBecause of its northerly location, Canada experiences climate change at twice the rate of the world’s average. That makes adaptation particularly important in certain Canadian sectors, if we are to make them more resilient to — and able to take advantage of — the effects of climate change. Explore the research and work that’s happening at Natural Resources Canada to learn more about climate change and potential adaptation strategies in Canada’s North, for coastlines and for Canadian forests.\nCanada is warming faster than the world as a whole — at more than twice the global rate — and the Canadian Arctic is warming at about three times the global rate. Due to this rapid warming, sea-ice deterioration and changes in permafrost are expected to put communities and infrastructure in the North at risk. Understanding current permafrost and sea-ice conditions and how they may evolve in response to a changing climate is essential for the assessment of climate change impacts and the development of adaptation strategies in northern Canada. The Geological Survey of Canada (GSC) conducts geoscience research to inform land-use planners, local governments and community leaders, industry and regulators who need to adapt to changing environments in Canada’s North. They use this information to create more resilient communities and infrastructure, and to respond to opportunities to develop natural resources.\nCanada is surrounded by oceans on three sides: Pacific, Arctic and Atlantic. Climate change will impact a number of ocean properties, such as temperature, sea ice, sea level, acidity and dissolved oxygen. Sea-level rise and more extreme high-water events will increase the risk of coastal flooding in some coastal communities. These changes will result in greater impacts on cities in the future, unless appropriate adaptation and risk management are implemented. The GSC has produced relative sea-level projections for Canada to support planning and adaptation tools like Fisheries and Oceans Canada’s Canadian Extreme Water Level Adaptation Tool.\nCanada’s forests cover a greater land area and store more carbon than do the forests of almost any other nation. How Canada manages its forests is therefore a global concern. That’s why the Canadian Forest Service (CFS) is working to identify options so Canada’s forest sector can adapt to climate change. New knowledge is helping forest managers reduce the risks of climate change negatively affecting ecosystems and the forest sector. It’s also helping managers optimize what benefits may come from climate change. The CFS is also working with provinces, territories, universities and industry to develop decision support tools for managers and policy makers.\nMoving forward on climate change adaptation\nSuccessful adaptation practices continue to emerge, but large gaps remain in Canada’s preparedness for climate change. We’re seeing the increased frequency and intensity of costly natural disasters, such as floods and wildfires, as well as growing risks to infrastructure, supply chains and communities as a result of sustained changes to our environment, like permafrost thaw and coastal erosion.\nIt’s critical to Canada’s economic and social well-being that we take rapid action on adaptation. That includes working here at home as well as looking at the climate change impacts and adaptation action occurring elsewhere in the world, which can strongly affect food availability, trade and immigration in Canada.\nCanada’s National Adaptation Strategy\nThe Government of Canada is developing Canada’s first National Adaptation Strategy as part of the strengthened climate plan, A Healthy Environment and a Healthy Economy. The Strategy presents a blueprint for whole-of-society action on climate change adaptation. It outlines a shared path to a more climate-resilient Canada and establishes a shared vision of what we want our future to look like across five key systems that are at the heart of our lives and communities:\n- Health and well-being\n- Nature and biodiversity\n- Economy and workers\n- Disaster resilience\nNRCan leads the Economy and Workers System of the National Adaptation Strategy. The advisory table for this system considered the impacts climate change will have across the economy, from finance, investment and insurance to labour and skills. It also considered the impacts on seven sectors identified as being particularly vulnerable to climate change in Canada's National Issues Report: forestry, fisheries, agriculture, mining, energy, transportation and tourism. Engagement with key partners and stakeholders throughout the development of the Strategy has shaped goals and objectives that strengthen the resilience of the economy, and prepare and protect workers.\nNRCan leads in areas of science necessary to improve the understanding of how climate changes impact terrestrial systems, including groundwater, erosion and deposition, permafrost changes, coastal dynamics, forest ecosystems and timber supply. The department has also been leading adaptation programs for over two decades to help businesses, communities and practitioners understand, assess and develop solutions to the climate change impacts they face. The programs develop the knowledge, tools, guidance and skills needed to help economic sectors — including wildfire resilience in the forestry sector, resilient housing, geoscience and geospatial data — understand and monitor impacts. As the lead of the Economy and workers system, NRCan will continue to convene and collaborate with experts, partners and stakeholders and to play a key role in advancing adaptation solutions for industries, businesses and communities.\nTo complement the Strategy, the Government of Canada Adaptation Action Plan sets out the federal role in preparing Canadians for climate hazards and outlines specific investments, programs and initiatives that are making Canada more resilient to climate impacts. The Action Plan includes $1.6 billion in new federal spending to support climate change adaptation. New funding is being provided to three federal programs that will be delivered by Natural Resources Canada for Canadians across the country, specifically:\n- Boosting the Flood Hazard Identification and Mapping Program to advance nation-wide flood mapping coverage and share all accessible flood hazard information to help keep Canadians and communities safe.\n- A Wildfire Resilient Futures Initiative to enhance the FireSmart Canada program and build wildland fire knowledge through research and pilot projects on fire risk reduction measures. A Centre of Excellence for Wildland Fire Innovation and Resilience will also be created to help transform wildland fire management in Canada and internationally through innovation, knowledge exchange and supports for Indigenous fire stewardship.\n- A Climate Resilient Coastal and Northern Communities program to support systems-based approaches to adaptation actions in coastal and northern regions, building on the Climate Change Adaptation Program. Pilot projects will work with coastal partners, Indigenous rights-holders and stakeholders to address key knowledge gaps on sea level rise, coastal erosion, permafrost thaw and glaciers, and to develop an integrated approach to planning and implementing regional-scale climate resilience.\nThe Government continues to work with provinces, territories, municipalities, Indigenous organizations and communities, and other stakeholders to increase Canada’s resilience to a changing climate. The National Adaptation Strategy and Government of Canada Adaptation Action Plan will be updated regularly, and a measurement framework will be developed to ensure Canada is on the right track to increase resilience to our changing climate.\n- Date modified:"", 'Credit 1.6: Managed Retreat\nThis credit applies when existing buildings can be moved inland to help prevent shoreline erosion and adapt to ongoing or predicted shoreline recession.\nMoving buildings inland will avoid the need for shore protection and other negative impacts associated with managing erosion in the future.\nWhere this credit applies\nThis credit applies to any site where buildings are at risk from shoreline erosion. It applies to marine and freshwater shores, and particularly marine shores that are subject to long term sea level rise and shoreline recession.\nRetreat should only be considered or recommended if the building in question can be moved to a location that is above the area affected by shoreline erosion/recession predicted to occur within 75 years or the life of the building, whichever is greater. Also, only consider retreat if the property is large enough to allow this move without compromising other legal setback requirements, or if the building can be relocated off the property.\nThis credit offers up to 10 base points* plus 3 bonus points.\n|Move an existing major building** to a location that is above the area affected by shoreline erosion and recession predicted to occur within 75 years or the life of the building, whichever is greater.||\n|Move an existing minor building*** to a location that is above the area affected by shoreline erosion and recession predicted to occur within 75 years or the life of the building, whichever is greater||\n|Bonus (available once 1 or more base conditions have been met)||\n|Add the effects of sea level rise into determining the relocation of a major or minor building.||\n*10 points can be achieved for addressing both a major building and a minor building.\n**Major building refers to a permanent house or dwelling.\n***Minor building includes garage, boathouse, shed, etc.\nHow to proceed\nTo meet this credit requires:\nA site plan that shows the original building footprint and the site to which the building has been or will be relocated. The relocation site must be landward of the area affected by projected erosion for 75 years or the life of the building, whichever is greater. General rules of thumb for an adequate distance are 35 ft (10 m) from the predicted receded OHWM/ NB or receded bluff/bank edge for low to moderately eroding sites, and 70 ft (20 m) for rapidly eroding sites;\nTo include climate change-induced sea level rise, determine what the projected SLR is for your area. For example, SLR of 0.5 m by 2050, 1.0 m by 2100 and 2.0 m by 2200 is predicted for most of the BC coast (Ausenco Sandwell, 2011). Determine how this will change the location of the OHWM over 75 years or life of the building (whichever is greater), and determine the relocation site accordingly.\nMeasurements of planned/existing building setbacks and the usable space available for moving the buildings back would have to be assessed and quantified from project drawings and also in the field. This involves simple linear measurements and does not require any special qualifications other than judging where suitable land is available.\nMarine: An example of a house moved landward to adapt to shoreline conditions. (From MSDG 6-8).\nFor more information\nArlington Group Planning and Architecture Inc. et al. 2013. Sea Level Rise Primer: A Toolkit to build adaptive capacity on Canada’s south coasts. Go to http://www2.gov.bc.ca/ and enter “sea level rise adaptation primer” in the search box.\nAusenco Sandwell. January 2011. Climate Change Adaptation Guidelines for Sea Dikes and Coastal Flood Hazard Land Use. For BC Ministry of Environment. Go to http://www2.gov.bc.ca/ and enter “coastal flood hazard” in the search box.\nBornhold, B. 2008. Projected Sea Level Changes for British Columbia in the 21st Century. 12 pg. Go to http://www2.gov.bc.ca/ and enter “Bornhold sea level changes” in the search box.\nNew Zealand Ministry for the Environment, July 2008. Coastal Hazards and Climate Change: A Guidance Manual for Local Government in New Zealand http://www.mfe.govt.nz/publications/climate/coastal-hazards-climate-change-guidance-manual/\nNickel Brothers House Moving – http://www.nickelbros.com/\nUniversity of Washington Climate Impacts Group and Washington Department of Ecology. 2008. Sea level Rise in the Coastal Waters of Washington State. 11 pg. http://www.cses.washington.edu/db/pdf/moteetalslr579.pdf']"	['<urn:uuid:25071a38-4df2-4298-ae60-6e501570a091>', '<urn:uuid:3b502651-a79b-4f08-a273-e3c3c32722c1>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-13T02:03:54.148118	29	109	2156
15	im chemist looking for alternatives to perc whats safest solvent	Instead of using perchloroethylene (PERC), which is classified by the EPA as a potential human carcinogen and can cause neurological effects, behavioral changes, coordination impairment, dizziness, and loss of consciousness, you can use natural alternatives like water, vinegar, lemon juice, salt, baking soda, and hydrogen peroxide. These are safer options for cleaning that don't pose health risks through inhalation or skin contact.	['You love keeping your place clean and healthy. But if you’ve ever looked at the ingredient list of conventional household cleaners, you’ll know that, even though they’ve got the “cleaning” part down, the same can’t be said for the “healthy” part.\nWhen we examined what’s in conventional household cleaners, we were shocked by what we found. (Spoiler alert: they’re really toxic).\nAnd today we’re sharing these findings with you.\nWe’ve put together a list of the most dangerous chemicals in household cleaners. Use this post to sort through your household cleaning products and stop exposing yourself (and your family) to harmful substances.\nWe’ve also researched some natural alternatives to household cleaning products, all vetted by the Environmental Working Group (EWG) to equip you with some safe home cleaning product options that you can switch to.\nIf natural DIY cleaning recipes are more your style, we’ll also tell you about ingredients that have even amazed researchers when they tested their cleaning power.\nWithout further ado, here are the most toxic ingredients in your household cleaners.\nHarmful chemicals in household cleaners\nPhthalates are nasty because they disrupt your endocrine system – meaning your hormones, which are important for the proper functioning of several body systems, especially the reproductive system.\nThey accumulate in your body over time. Your body acquires them both through contact with your skin and inhalation (or ingestion).\nPhthalates commonly lurk in soaps of all kinds (including shampoos, hand soaps, body washes, and dish soap), hair sprays, anything containing artificial fragrance (including household cleaners and baby wipes), and even toilet paper.\nCheck the ingredient list of your household cleaning products to make sure they do not contain phthalates. According to the National Academies Press, these are the most commonly found phthalates:\n- DMP – Dimethyl phthalate\n- DEP – Diethyl phthalate\n- DBP – Dibutyl phthalate\n- DIBP – Diisobutyl Phthalate\n- BBP – Benzylbutylphthalate\n- DEHP – Diethylhexyl phthalate or DOP – dioctyl phthalate\n- DINP – Diisononyl phthalate\nUnfortunately, there are far more than can be listed here. Suffice it to say: if there’s an ingredient that ends with the word “phthalate”, or that is listed as an acronym that ends in “P”, steer clear of it.\n2. Perchloroethylene (PERC) or Tetrachloroethylene\nThe U.S. Environmental Protection Agency (EPA) has classified perchloroethylene as a potential human carcinogen.\nIt also wreaks havoc on your nervous system. Symptoms of exposure include neurological effects, behavioral changes, impairment of coordination, sleepiness, dizziness, and loss of consciousness. (Yikes!)\nIt can also irritate your respiratory tract and cause eye and kidney problems. This is mega-bad stuff, folks.\nPerchloroethylene is most commonly found in dry-cleaning products. That’s why it’s super important to only go to a dry-cleaner that refrains from using perchloroethylene.\nShould you encounter household cleaning products that include perchloroethylene on the ingredient list, it’s important to avoid inhalation and contact with the skin.\n3. Quarternary ammonium compounds, or “Quats”\nThese chemicals can be found in products like floor cleaners, fabric softeners, oven cleaners, hard-water-stain removers, toilet cleaners, stove top cleaners, all-purpose household cleaners, and in chemically-based antibacterial products.\nLike the recently-banned triclosan, Quats promote the growth of antibiotic-resistant bacteria. According to the EWG’s Healthy Guide to Cleaning, Quats should not be inhaled or come into contact with skin.\nThe best way to avoid these ingredients is to avoid using chemical disinfectants and store-bought fabric softener. (Vinegar is a great alternative on both counts).\nIf that’s not your style, we have created a list of common Quats so you can check the ingredients of your products:\n- Babassuamidopropalkonium chloride\n- Behentrimonium chloride\n- Behentrimoniu methosulfate\n- Benzalkonium chloride\n- Cetalkonium chloride\n- Grapefruit seed extract (…I know! But this stuff is actually\nheavily processed with intense chemicals, hence the Quats.)\nGuar hydroxypropyltrimonium chloride\n- Methylbenzethonium chloride\n- Stearalkonium chloride\n- Vegetable oil quaternary\nThis is what gives the household cleaner products in your cabinet—multipurpose cleaners, kitchen cleaners, bathroom cleaners, and window cleaners—their sweet smell.\nAccording to the EPA, exposure to 2-butoxythanol enters your body through skin contact, inhalation, and/or ingestion. Once inside, it can damage your red blood cells and liver, depress your central nervous system, and cause cancer.\nProducts that contain 2-butoxythanol can be dangerous to your health, and should be avoided.\nAlthough many people believe they will be fine as long as they use the product in a well-ventilated room, our two cents is: why be so cavalier with your health? After all, if it’s a scented product and you can smell it, then you’re breathing it in.\nWe recommend using safer alternatives like water, vinegar, lemon juice, salt, baking soda, and hydrogen peroxide (available at any drugstore).\n5. Ammonium hydroxide\nAmmonium hydroxide is found in glass cleaners and polishing agents for sinks and bathroom fixtures, as well as oven cleaners, drain openers, toilet cleaners, stove top cleaners, and all-purpose household cleaners.\nThis substance can be dangerous if inhaled, leading to asthma and chronic bronchitis.\nAccording to the New York State Department of Health, exposure to this chemical even at low concentrations leads to skin or eye irritation, and even blindness. Higher concentrations cause severe injury and burns.\nAmmonia that comes into contact with products containing chlorine (including bleach) gives off fatal fumes.\nThis is why we recommend staying away from products that include ammonia.\n6. Chlorine (sodium hypochlorite)\nIf you use bleach, toilet bowl cleaners, laundry whiteners, scouring powders, oven cleaners, drain openers, hard water stain removers, stove top cleaners, household cleaners, or mildew removers, you may be exposing yourself to chlorine.\nAccording to The New York State Department of Health, chlorine can lead to airway irritation, wheezing, difficulty breathing, sore throat, cough, chest tightness, eye irritation, and skin irritation (learn more about things you should never put on your skin).\nAs we saw, chlorine produces a poisonous (potentially fatal) gas when it comes into contact with ammonia. This combination can happen even if you are not mixing bleach with other cleaning products. For example, urine naturally contains ammonia. Therefore, using chlorine to clean a diaper pail, kitty litter box, or urine mess can inadvertently release poison gas.\nChlorine also gives off poisonous gas when it comes into contact with anything acidic–which includes vinegar, lemon juice, window cleaners, drain cleaners, and other types of cleaning products.\nChlorine bleaches are harmful in other ways besides creating poison gas. They kill germs in a heavy-handed way that helps to create drug-resistant “superbugs”.\nChlorine is also highly corrosive, and can damage the surfaces and materials in your home. Chlorine bleach should never be used on wood, paper, stone (including stone countertops), tile, grout, or fabric, because it corrodes these materials. It also shouldn’t be used on anything metal.\n7. Coal tar dyes (aniline)\nThe National Toxicology Program and the International Agency for Research on Cancer (IARC) have found coal tar to be carcinogenic. Also, the Campaign for Safe Cosmetics warns of exposing your skin to coal tar.\nRemarkably, many household cleaning products include coal tar dyes even though they serve no purpose in the cleaning function!\nTo spot coal tar dyes, look out for these ingredients on cleaning product labels:\n- Coal tar solution\n- Coal tar solution\n- Coal tar solution USP\n- Crude coal tar\n- KC 261\n- Picis carbonis\n- High solvent naphtha\n- Naphtha distillate\n- Benzin B70\n- Petroleum benzine\n8. Nonylphenol ethoxylates (NPEs)\nA study published in the Oxford Journal of Toxicological Sciences has shown that nonylphenol ethoxylates affect your endocrine (hormone) system and harm male reproductive capabilities.\nNonylphenol ethoxylates are found in numerous cleaning products, including stain removers, toilet bowl cleaners, all-purpose cleaners, air fresheners, degreasers, liquid laundry detergents, and car wash products.\nOur suggestion: avoid the NPEs. Instead, use genuinely natural cleaning products made from good ingredients.\nHere’s a list of Nonylphenol ethoxylates (NPEs) to look out for on your household cleaning products:\n- 2-(p-Nonylphenoxy) ethanol\n- 2-(2-(p-Nonylphenoxy)ethoxy) ethanol\n- p-Nonylphenol polyethylene glycol ether\n- Nonylphenol hepta(oxyethylene)ethanol\n- Nonylphenol nona(oxyethylene)ethanol\n- Onylphenoxy ethanol\n- Oxirane, methyl-, polymer with oxirane, mono(nonylphenyl) ether\n- 2-(2-(2-(2-(p-Nonylphenoxy)ethoxy) ethoxy)ethoxy) ethanol\n- Nonylphenol polyethylene glycol ether\n- Ethanol, 2-[2-(nonylphenoxy)ethoxy]-\n- Nonylphenol ethoxylate\n- Poly(oxy-1,2-ethanediyl), alpha-(nonylphenyl)-omega- hydroxy-, phosphate\n- Nonylphenol ethoxylate\n- Ammonium salt of sulphated nonylphenol ethoxylate\n- Poly(oxy-1,2-ethanediyl), alpha(isononylphenyl) omega-hydroxy\n9. Sodium dichloroisocyanurate dihydrate\nThis ingredient has been found to seriously irritate the respiratory system and eyes. Inhaling its fumes or dust causes respiratory damage.\nHands should be washed upon contact, as sodium dichloroisocyanurate dihydrate can also severely irritate the skin.\nThis ingredient is commonly found in deodorizers, surface cleaners, toilet bowl cleaners, and disinfectants.\nNow that you know some of the worst common ingredients in cleaning products, and no longer want to use them, how do you get rid of them?\nHow to safely dispose of products containing these ingredients\nHere are a few tips to keep in mind in order to do this the right way.\nWhat NOT to do:\n- Do NOT pour these products down the drain (as this will contaminate the environment and your local water supply)\n- Do NOT toss them in the garbage (as the container will eventually leak, causing the toxins inside to leach into the ground and groundwater).\nWhat to do:\n- Drop them off at your local collection point for toxic household products. Most local governments offer at least one “hazardous waste” collection day per year; if you live in a big city, your local collection point might be open several days a week. A quick web search will reveal what’s available in your area.\n- Talk to members of your household about the dangers of these products in order to prevent them from reappearing in your cupboards.\n- Choose safer alternatives. Water, club soda, hydrogen peroxide, vinegar, lemon juice, salt, baking soda, non-chemically-based dish soap, and good old-fashioned elbow grease are wonderfully versatile cleaning agents. When you are armed with knowledge about the clever uses of each, no household grime will stand a chance.\nReady to learn more?\nDIY natural household cleaners for cleaning your home\nIf you want to stop using conventional household cleaners, the good news is that there are many natural alternatives available to you that work well, especially for removing hard water stains.\nVinegar is one of them.\nA study published in Environmental Health showed that vinegar can be used as a household cleaner. Tests have shown that it kills 98.6% of common bacteria found in households. Vinegar’s acidity also helps it to dissolve dirt and grime.\nIt’s a simple, all-natural substitute to conventional cleaners and well-suited for cleaning windows, blinds, kitchen appliances, bathtubs, shower floors, sinks, stove tops, and many other areas in your home. (Just don’t use vinegar on marble or granite countertops, because the acidity can etch the stone). Lemon juice substitutes well for vinegar.\nAnother ingredient that makes for a great DIY household cleaner is baking soda. Sprinkling some baking soda onto a moist cloth is a great for cleaning surfaces like kitchen counters, sinks, tiles and other areas.\nWhat if you don’t like the smell of vinegar, and are looking for a stronger cleaner than baking soda? Then you can choose from a number of carefully-formulated natural cleaning products.\nThere’s one important caveat though. You need to ensure that each product you buy is truly safe for your health, and isn’t just a marketing hoax.\nIn the next section we’re going to walk you through what to look out for when choosing a truly natural household cleaner.\nNatural alternatives to conventional cleaning products\nThere are some amazing natural cleaners available that facilitate your move away from toxic household cleaners to healthy, natural alternatives.\nBut don’t take a product’s packaging at face value. As consumers have grown more health-conscious, manufacturers have gotten increasingly clever at leading us to believe that their products are “green,” even when they’re not.\nAn independent study by The Sins of Greenwashing found that only 4.5% of products that claim to be healthy and environmentally friendly actually live up to their promise.\nThe most common sins that manufacturers commit include:\n- Cherry-picking certain positive attributes without mentioning a product’s weightier harms (e.g., organic cigarettes; sugary cereal fortified with iron; a cleaning product that contains essential oils alongside really harmful ingredients.)\n- Making vague promises, or not providing any proof for claims (e.g., “MSM is known as the ‘beauty mineral’.”)\n- Designing product labels in “earthy” tones, or using retro / nostalgic / childlike graphic design to imply that a product is wholesome.\n- Making meaningless “green” claims (e.g., advertising a product as “CFC-free” even though CFC is a universally-banned substance.)\n- Implying that a harmful ingredient is safe by offering irrelevant information about it (e.g. “laureth-7 (plant-derived cleaning ingredient)”).\n- Making outright false statements about a product’s environmental impact.\nThe surest way to protect yourself from harmful ingredients –and from marketers’ greenwashing attempts– is to read a product’s full ingredient list.\nThe list of dangerous chemicals in household cleaners that we presented in this post can help you to a certain extent to spot dangerous ingredients and identify greenwashers.\nAt the same time, chances are you will quickly run into ingredients on product labels that you do not recognize. When that happens, ignore anything the manufacturer may say on the label or website (e.g. “plant-derived”) to attempt to explain the ingredient away. Instead, look up the ingredient or product in the EWG database.\nThe EWG database is a powerful tool that helps you to see through a company’s marketing communications and spot ingredients that can harm your health.\nBut in case you don’t have time for that, we created a list of common cleaning products that got excellent scores for health and sustainability from the EWG:\nTheir natural laundry soap gets your clothes clean, makes them smell great, and all of this without any (and we mean any) harsh or toxic substances.\nHere’s their ingredient list:\nWater, Decyl glucoside, Sodium oleate, Glycerin, Caprylyl glucoside, Lauryl glucoside, Sodium chloride, Sodium gluconate, Carboxymethyl cellulose, Alpha-amylase, Protease, Lipase, Citrus Limon (Jade lemon) peel oil, Citrus aurantium bergamia (Bergamot) peel oil (Furocoumarin-free), Syzygium aromaticum (Clove) bud oil, Citrus limon (Lemon) peel oil, Cinnamomum zeylanicum (Cinnamon) bark oil, Eucalyptus radiata oil, Rosmarinus officinalis (Rosemary) leaf oil.\nWe checked each ingredient individually to ensure the formula is safe, and we were very pleased with the result. This is a truly natural product that we’re happy to recommend. You can check for yourself in the EWG database to see if you agree and then read what other customer had to say about this amazing product.\nThis next cleaner easily removes food residue on your countertop and dinner table. It works great in your bathroom, even electronic gadgets, hardwood floors, granite, glass, metal, painted surfaces, plastic, porcelain, stainless steel, and any other solid surface.\nAnd all of this without using any harsh and toxic ingredients.\nThe ingredients of this all-purpose cleaner are: lavender oil, grapefruit oil, coco-glucoside, lauryl glucoside, ethanol, and water.\nThe EWG gave this product an “A” rating, and you can check out how the individual ingredients were rated in the EWG Guide to Healthy Cleaning database.\nHaving a natural all-purpose cleaner in your household can be very handy, especially since there’s no natural alternative for some types of special cleaning products.\nThis is one example where a natural all-purpose cleaner comes in handy and if you happen to have a glass or ceramic stovetop, and want a simple, non-toxic solution, then a stove-top scraper can serve you in addition.\nDue to the especially harsh toxins in conventional toilet bowl cleaners, you should consider using a natural alternative to protect yourself and your family.\nWe found a toilet bowl cleaner that was formulated without ingredients that compromise your health. You can read the reviews here:\nNot all Seventh Generation products are of equally high quality. However, we are happy to recommend Seventh Generation’s Toilet Bowl Natural Cleaner, Emerald Cypress & Fir because the formulation is so non-toxic that the EWG gave it an “A” rating.\nDish soap is another household cleaner product that contains ingredients that are more harmful than most people would expect.\nConventional dish soaps often contain ingredients like:\n- Benzisothiazolinone (leads to skin irritation or damaged skin)\n- Dipropylene glycol (causes cancer, damages DNA, irritates or damages skin, damages vision, affects respiratory, digestive, nervous, endocrine, and reproductive systems.\n- Fragrance (unspecified ingredient used to hide a large number of toxic chemicals)\n- Hydroxypropyl methylcellulose (affects your digestive system)\n- Methylchloroisothiazolinone and methylisothiazolinone (irritates and sensitizes skin, causes allergic contact dermatitis)\n- Polysorbate-20 (damages DNA, causes cancer, developmental, endocrine, and reproductive problems, affects respiratory, nervous, and digestive systems, irritates or damages skin, and damages vision)\n- Sodium laureth sulfate and sodium lauryl sulfate (damages DNA, affects many body systems including respiratory, endocrine, reproductive, digestive, and nervous systems, and causes cancer)\nYour dishes are what you eat from. Your (moist) food spends time on the dishes, and picks up any chemical residue lingering on it– which is why the chemicals in your dish soap matter more than any other household cleaning product in your house.\nHere’s an all-natural dish soap we’re excited to recommend. It’s made from just six easy-to-understand ingredients:\n- Distilled water\n- Vegetable glycerin\n- Coconut oil\n- Castor oil\n- Lavender essential oil\nNow that you’ve learned which ingredients in household cleaners to avoid, find out how to detox your body naturally in 8 sure-fire steps.\nAnd if you wanna read more about how to keep your home sparkling clean, here’s a list of the top 100 house cleaning and housekeeping blogs.\nHey there reader! Welcome to the Sunshine Organics blog where we share proven beauty tips that are so natural even your great-grandmother would understand them. If you have any questions or if you want to write for us or work with us please drop us a line at howdy [at] sunshine-organics.com or leave a quick comment below. We always love to hear from you!']	['<urn:uuid:c452bc92-a827-46c9-b36e-97d7fb72ea81>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-13T02:03:54.148118	10	62	2987
16	What are the basic functions of kidneys and how do they get damaged?	Kidneys have multiple vital functions including regulating blood volume and pressure, stabilizing blood pH, eliminating organic wastes, and conserving nutrients. They can become damaged through various mechanisms including viral infections, toxins, and high pressure in glomerular capillaries, which can lead to scarring (glomerulosclerosis) and eventual kidney dysfunction. This damage typically starts in deeper juxtamedullary glomeruli and extends to superficial nephrons, ultimately resulting in proteinuria and potential kidney failure.	"[""functions of the urinary system\n1. regulating plasma concentrations of ions;\n2. regulating blood volume and pressure by adjusting the volume of water los and releasing erythropoietin and renin;\n3. helping stabilize blood ph;\n4. conserving nutrients;\n5. eliminating organic wastes;\n6. synthesizing calcitriol\ncontents of the urinary system\nkidneys, ureters, urinary bladder, urethra\nhow is the position of the kidneys maintained in the abdominal cavity?\n1. overlying peritoneum; (but is retroperitoneal organ)\n2. contact with adjacent visceral organs;\n3. supporting connective tissues\nthree concentric layers of connective tissue in the kidneys?\nrenal capsule, adipose capsule, renal fascia (most outer)\ncovers outer portion of kidney\nhelps keep shape\nwhat surrounds the renal capsule?\nwhat does the renal fascia do?\nanchors the kidney to ab wall\nwhat is attached to the hilus of the kidney?\nthe ureter and renal blood vessels\ndivisions of the kidney\nouter renal cortex, central renal medulla,\ncortex + pyramids\nwhat are renal columns composed of?\nwhat do renal columns do?\nseparate adjacent pyramids\nwhat does the renal lobe contain?\n1. renal pyramid; 2. overlying area of renal cortex; 3. adjacent tissues of the renal columns\nwhat is the renal sinus continuous with?\nwhat does the vasculature of the kidneys include?\nrenal, segmental, interlobar, arcuate, and interlobular arteries to the afferent arteries that supply the nephrons\nhow does the blood flow in the kidneys?\nfrom the nephron, into the interlobular, arcuate, interlobar, and renal veins\nwhat innervates the kidneys and the ureters?\nwhat does sympathetic activation do to kidneys?\nregulates glomerular blood flow and pressure, stimulates renin release, and accelerates sodium ion and water reabsorption\nwhat is the basic functional unit of the kidney?\nwhat does the nephron consist of?\n1. renal corpuscle - plasma filtered via glomerulus and Bowman's capsule\n2. renal tubule - prox convoluted tubule, loop of Henle, distal convoluted tubule\nwhere does tubular fluid travel from the renal corpuscle?\nthrough the proximal convoluted tubule (PCT), the loop of Henle (nephron loop) and the distal convoluted tubule (DCT) then through the connecting tubule, collecting duct, and papillary duct to reach the renal minor calyx\nshort loops of Henle that dives into r. medulla\nlong loops of Henle extending deep into medlla\nallow kidney to excrete more diluted or concentrated urine\nwhat do the efferent arteriole provide blood to?\nthe peritubular capillaries\nwhat do peritubular capillaries surround?\nwhat are nephrons responsible for?\n1. Glomerular filtration (filtrate to cappillaries in g. capsule to renel tubule\n2. Tubular reabsorption of organic nutrients, water, ions\n3. Tubular Secretion\nwhat does the renal corpuscle contain?\ncapillary knot of the glomerulus and Bowman's capsule\nwhat is another name for Bowman's capsule?\nwhat happens at the glomerulus?\npodocytes of visceral epithelium wrap their feet around the capillaries\nwhat are pedicels of the podocytes separated by?\nnarrow filtration slits\nwhat does the capsular space separate?\nthe parietal and visceral epithelia\nnotable characteristic of lamina densa of basal lamina?\nit is unusually thick\nhow does blood arrive at the vascular pole of the renal corpuscle?\nvia the afferent arteriole\nhow does blood depart from the vascular pole of the renal corpuscle?\nin the efferent arteriole\nwhere does blood go from the efferent arteriole?\nenters the peritubular capillaries and the vesa recta that follow the loops of Henle in the medulla\nwhich convoluted tubule contains the most microvilli\nproximal convoluted tubule (PCT)\nfunction of PCT\nactively reabsorbs nutrients, ions, plasma proteins, and electrolytes from the tubular fluid\nwhat do the limbs of loop of Henle contain?\nthick segment and thin segment\nwhat does the loop of Henle's descending limb do?\ndelivers fluid to the distal convoluted tubule (DCT)\nwhat is passively transported out of the ascending limb of the loop of Henle?\nwhat does reabsorption in the PCT and loop of Henle reclaim?\nall of the organic nutrients, 85% of the water, and more than 90% of the Na and Cl ions\ndistal convoluted tubule is an important site for what?\n1. active secretion of ions, acids, and other materials;\n2. reabsorption of sodium ions;\n3. adjusting osmotic concentration and balance\nwhat is juxtaglomerular appartaus composed of?\nmacula densa, juxtaglomerular cells, extraglomerular mesangial cells\nwhat does juxtaglomerular apparatus secrete?\nthe hormones renin and erythropoietin\nwhat does the DCT open into?\nthe collecting system\nwhat does the collecting system consist of?\nconnecting tubules, collecting ducts, papillary ducts\nfunctions of collecting system?\n1. transports fluid from the nephron to the renal pelvis; 2. adjusts the osmotic concentrations and volume"", 'Causes proteinuria or nephrotic syndrome with or without renal insufficiency.\nProteinuria occurs due to the dysfunction of the glomerular filtration barrier.\nThe glomerular filtration barrier is tripartite with a fenestrated endothelium, glomerular basement membrane, and podocytes, the latter are epithelial cells with interdigitated foot processes interconnected by multi-protein slit diaphragms (Manchuca E et al).\nHistologic pattern of injury that characterizes broad spectrum of diseases with different underlying pathophysiologies.\nAccounts for approximately 20% of cases of nephrotic syndrome in children and 40% of such cases in adults.\nMajority of cases are secondary, not primary.\nAnd estimated incidence of 7 per million (Kititakara C et al).\nIncidence increasing in adults for past few decades.\nThe most common problem of glomerular disorder resulting in end-stage renal disease in the US.\nPrevalence of 4%.\nDiagnosis confirmed by histopathologic testing.\nCardinal abnormality glomerular scarring.\nFocal refers to involvement of a subgroup of glomeruli and segmental refers to involvement of a portion of the glomerular tuft.\nPrimary injury is to the podocytes, with proliferation of mesangial, endothelial and epithelial cells in early stages.\nPrimary focal segmental glomerulosclerosis presumed to be a circulating permeability factor diffusely toxic to podocytes.\nMutations in podocyte genes are associated with genetic focal segmental glomerulosclerosis.\nGenetic forms are usually resistant to steroid therapy and often progress to end-stage renal disease.\nFamilial forms are associated with mutations in genes that encode podocyte proteins.\nMYO1E mutations are associated with childhood onset, corticosteroid resistant focal segmental glomerulosclerosis (Mele C et al).\nEarly disease is focal with involvement of a minority of glomeruli, and segmental with involvement affecting a portion of the glomerular globe.\nAs the disease progresses more widespread and globular glomerulosclerosis develops.\nIn later stages there is shrinkage and or collapse of the glomerular capillaries and eventually sclerosis.\nProposed mechanisms for the process include: viral, toxins and high intraglomerular capillary pressure.\nUsually starts in the deeper juxtamedullary glomeruli and extends to the superficial nephrons.\nCharacteristic lesion: segmental solidification of the glomerular tuft in the perihilar region and sometimes in the peripheral areas.\nEventually global glomerulosclerosis develops.\nDiffuse foot process fusion occurs mainly in the sclerotic segments. In\nDefining feature is proteinuria, which is typically associated with hypoalbuminemia, hypercholesterolemia and peripheral edema.\nNephrotic syndrome in children is defined as proteinuria of greater than 1 g of urine protein per square meter of body surface area per day, albumin level of less than 2.5 g, cholesterol total of greater than 200 mg/dL and edema.\nNephrotic syndrome in adults defined is a urine protein level of more than 3.5 g per day, albumin level less than 3.5 g/dL.\nApproximately 75-90% of children and 50-60% of adults have associated nephrotic syndrome at presentation of focal segmental glomerulosclerosis.\nThere are many subsets of disease including a cellular variant, a collapsing variant, and tip lesions.\nClassification includes primary (idiopathic ) and secondary forms.\nMay be a primary process, or a secondary process associated with drugs, viral infections, and diseases that affect the renal mass and glomerular hemodynamics (Barisoni L et al).\nPrimary forms include: focal segmental glomerulosclerosis, progression from minimal change disease, progression from immunoglobulin M nephropathy, progression from mesangial proliferate glomerulonephritis, superimposed on other primary glomerulonephritis conditions and variant of primary focal segmental glomerulosclerosis.\nSecondary forms include: familial, virus associated, drug-induced and adaptive.\nSecondary FSGS is a response to reduction in the number of functioning nephrons from or am abnormal stress on initially normal nephrons.\nApproximately 80% of cases are primary and idiopathic.\nFocal segmental glomerulosclerosis and the related disorder minimal change disease are podocyte diseases.\nPodocyte foot process injuries correlate with nephrotic proteinuria.\nAlterations in podocyte shape causes rearrangement of the actin cytoskeleton, a process which is a reversible with glucocorticoids therapy in minimal change disease but it is lrreversible and progressive in focal segmental glomerulonephritis.\nPrimary FSGS may respond to immunosuppressive treatment.\nSecondary FSGS treatment is centered around unloading the pressure on glomeruli using renin-angiotensin-aldosterone inhibition.']"	['<urn:uuid:8951f05b-fe2e-4c7d-94b1-ae89f1b7ce36>', '<urn:uuid:10937637-1ac9-486e-b544-e6d5b8063c9b>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T02:03:54.148118	13	68	1392
17	what happened iranian small boat attacks us ships	Iranian small boat harassment has dwindled since a 2016 spike. Instead of small boat harassment, Tehran has increased its use of unmanned aerial vehicles in the region.	['THE PENTAGON – Amphibious warship USS Iwo Jima (LHD-7) crossed into the Persian Gulf this week, returning a U.S. Navy capital ship to the region for the first time since late March, USNI News has learned.\nThe two-and-a-half-month gap between the arrival of Iwo Jima and the March departure of aircraft carrier USS Theodore Roosevelt (CVN-71) is the longest gap the Navy has had without a capital ship — a carrier or a big-deck amphibious assault ship — in the Gulf in recent memory.\nWhen USS Nimitz (CVN-68) departed the Gulf in October, amphib USS America (LHA-6) passed through the Strait of Hormuz into the Gulf a few days after. In 2015, a two and a half month gap between the departure of the Roosevelt CSG and the arrival of the Harry S. Truman CSG was covered by initial strike sorties from Marine Harriers on USS Kearsarge (LHD-3) and the French aircraft carrier Charles de Gaulle.\nWhen contacted by USNI News, service representatives declined to elaborate on the move of Iwo Jima.\n“The Navy remains committed to supporting U.S. Central Command to ensure it has the capabilities it needs for maritime security in the U.S. Naval Forces Central Command area of operations,” reads a statement from U.S. 5th Fleet to USNI News on Thursday. “We are not going to discuss the timing of operational movements of ships into, around, and out of the U.S. Central Command area of responsibility.”\nFor the last several weeks, U.S. naval power has been focused in the Mediterranean and Baltic seas in U.S. 6th Fleet rather than in the Middle East and 5th Fleet. The Harry S. Truman Carrier Strike Group that left the U.S. in mid-April with a larger-than-average number of escort ships still remains in 6th Fleet. The Truman CSG has been launching strikes against Islamic State targets from the Eastern Mediterranean, compared to the Theodore Roosevelt CSG doing so from the Persian Gulf during its deployment. Additionally, the two other ships in the Iwo Jima Amphibious Ready Group – USS Oak Hill (LSD-51) and USS New York (LPD-21) – are operating in the Baltic and Mediterranean seas respectively, along with the embarked 26th Marine Expeditionary Unit spread among the three ships.\nThe recent emphasis of forces in and around Europe and Northern Africa are indicative of how the Navy is deploying forces differently since the rollout of new strategic guidance from the Pentagon in January, two Navy officials told USNI News on Thursday.\nActing under Secretary of Defense James Mattis’ mandate to be “strategically predictable and operationally unpredictable,” the Navy has been experimenting with the idea of “dynamic force employment.” Unlike past deployments, where East and West Coast carriers moved to the Gulf, operated for three months and then returned to their respective homeports, the new dynamic force employment model would attempt to take the predictability out of the deployment schedule.\n“If you think about all the ways you could increase the power of the Navy, it’s not necessarily going forward, disaggregating the strike group and doing those sorts of operations,” Chief of Naval Operations Adm. John Richardson told USNI News last month.\nPart of the new model is to emphasize preparing for higher-end warfare against great power adversaries like Russia and China, which is arguably easier the more expansive water of the Mediterranean versus the much more narrow Persian Gulf.\n“It’s an aspect of the security environment that it’s getting harder to do things without being observed, no matter where you are. So we’re going to have to be clever about that,” Richardson told USNI News last month, who suggested that conducting high-end training in a less-predictable manner while overseas or making more forces available to train back at home would be required.\nThe presence of a U.S. capital ship in the Gulf – usually a carrier — has been a near constant since the terror attacks of Sept. 11, 2001.\nSorties from carriers in the Persian Gulf at one time accounted for a third of the strike missions over Afghanistan during Operation Enduring Freedom. When Secretary of Defense James Mattis was the head of U.S. Central Command, he pushed to have two carriers strike groups in the Gulf in order to support the OEF mission and act as deterrence to Iran, whose fleet of small boats was actively harassing naval and merchant vessels at the time.\nFollowing the implementation of the Budget Control Act, in 2013 the Navy pared down its carrier presence in the Persian Gulf. Around the same time, the air war in Afghanistan began to draw down, and naval aviation instead focused on strikes against Islamic State targets in Iraq and Syria. In addition to a shift in air missions for the U.S., Iranian small boat harassment has dwindled since a 2016 spike which could, in turn, reduce the need for a larger U.S. Navy presence in the region. Instead, Tehran has increased its use of unmanned aerial vehicles in the region.']	['<urn:uuid:5e701623-8c16-4977-903d-76b19161b4a6>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T02:03:54.148118	8	27	825
18	interested in early film editing history how did they cut movies before and how does france preserve old films today	In the earliest days, film editing was literally called 'cutting' because editors physically cut film strips with scissors and cemented pieces together. 'Cutters' would hold strips up to light to determine where to make their cuts. This evolved into more sophisticated editing that allowed manipulation of time and space in storytelling. Regarding film preservation, France has played a crucial role through establishing the film archives movement in Paris in 1936. Today, this has evolved into the International Federation of Film Archives, with institutions in over 77 countries dedicated to preserving moving images. France continues this commitment through initiatives like the Cinema Museum, which works to sustain the theatrical experience and maintain film history through curated programs and screenings.	"['Film Editing: Activity 1\nThe Origins of Movie Magic\nThe very first films in the late 1800s, made by the Lumiere Bros. and Thomas Edison among others, were single-shot actualities: a train pulling into a station, people leaving a factory, ladies walking down the street. The camera was locked in place. It recorded, in its entirety, the ""event"" taking place. It was the magic of capturing movement that captivated audiences. Editing was originally called ""cutting,"" as it actually was the cutting together of two pieces of film. ""Cutters"" held the strips of film up to the light and cut them with scissors, cementing the two pieces together at the desired point.\nIt was no coincidence that several early filmmakers performed as magicians. The jump cut, a deliberate mismatching of two scenes, evolved into the first ""special effect"" of movies and was probably discovered by accident. Within the same scene, an actor could be made to ""disappear"" by stopping the camera, removing the actor, and resuming the scene without moving the camera. George Méliès, a Parisian magician, produced dozens of elaborate ""trick"" films using this effect as one of his primary marvels.\nStage-bound presentations, which had actors performing in the proscenium-like frame of the film without moving the camera, soon gave way to bold close-ups, medium shots, and tracking shots under the direction of film pioneers Alice Guy Blache of France and Edwin S. Porter and D.W. Griffith of the U.S., among others. The storytelling concepts used in magic-lantern slide shows (and later comic books) were used to create a language of film. Cutting from a long shot of an actor standing by a tree to a similar shot of just his face near the tree created a sense of continuous action, even though the shots may have been filmed on different days. Cutting evolved into ""editing,"" the manipulation of time and space. The ability to manipulate time and space also allows the filmmaker to change our emotional and intellectual responses to what we see on the screen.\nReview the editing terms listed on the activity master. You might complete the viewing activity as a class, using one of the films suggested, or students might make their own selections and complete the activity at home in preparation for a class discussion. Films with sequences that have no edits at all include the opening sequence of Orson Welles\' Touch of Evil, the shot on top of the train or the shot walking through the camp in Bound for Glory, and the shot from the dressing room to the ring in Raging Bull. Good examples of rapid cutting can be found in the film-within-a-film sequence of Buster Keaton\'s Sherlock Jr., the Odessa Steps sequence in The Battleship Potemkin, the ambush scene in Bonnie and Clyde, the shower scene in Psycho and the phone booth attack scene in The Birds.\nAsk your students to take a short scene from a film and discuss the effect that re-arranging the placement of the sounds or dialogue would have. Ask students to consider what happens when we hear one thing and see another instead of just seeing the sound and picture from one source.', 'For the next six months as part of its ongoing fundraising effort The Cinema Museum is hosting a season of French films that will, hopefully, provide a snapshot of French cinema over the last 80 years. The programme has been curated by Jon Davies and will screen work from the Lumiere brothers up to the present day. The films will be shown in chronological order each with an introduction contextualising the films and a post screening question and answer session.\nThis is part of the cinema museums continuing commitment to sustaining the theatrical experience of cinema going by keeping alive the experience of watching films on a screen as part of an audience.\nIt is curated programmes such as this that play an essential role in ensuring that certain directors, specific historical periods and national cinemas are considered part of cinemas rich heritage and which have a crucial role in maintaining film history. In these days of downloadable films, streaming and cheap DVDs bought over the Internet it is more important than ever to watch films in ways that encompass the whole of the movie going experience.\nFilm should be discussed, debated, and argued about.\nThe French film industry is the most successful in Europe but in Britain we see very few French films. French cinema is generally confined to the art house circuit or late night screening on BBC2 or Channel 4. French films fare worse in the British market than they do in other countries in Europe.\nThe problem is predominantly one of venue – art house cinemas and subtitles define French films as challenging to audiences. The danger of constructing French films as ‘art’ or ‘high culture’ is their effective ghettoisation as films coded for consumption by the educated middle classes. It is the distribution and exhibition practices that contribute to French films being viewed as difficult that this season at The Cinema Museum hopes to challenge.\nHaving pointed this out it is as well to remind ourselves that French cinema is often more thematically and aesthetically complex than the average Hollywood blockbuster and demands a more critically engaged spectatorship.\nFrench film has always been at the cutting edge of innovation. Just a (very) brief glance at some of the milestones of French cinematic history reveals the contribution to cinematic art made by the French film industry.\nIt was the Lumiere brothers who revolutionised our sense of the visual when they screened ten short films in the Salon Indien Café in Paris in December 1895 on their new invention – the cinematographe. Although there were other people working on the projection of the cinematic image at the same time as the Lumiere Brothers this screening was the first truly cinematic experience, the first public film show.\nGeorges Melies introduced the fantastic into cinema with his 1902 short film Le Voyage dans la Lune generally considered to be the first example of a science fiction film.\nThe film archives movement originated in Paris in 1936. Today the International Federation of Film Archives has institutions in over 77 countries dedicated to the preservation of the moving image.\nThe first female director, not really remembered now, was Alice Guy-Blanch who made such films as L’Enfant de la Barricade.\nThe young generation of film writers who worked with Andre Bazin at Cahiers du Cinema, founded in 1951, transformed the way in which film criticism functioned and its politique des auteurs resulted in Hollywood film being studied seriously and paved the way for an iconoclastic style of filmmaking that became known as the Nouvelle Vague.\nThe 1990s saw a left turn in French cinema that dealt with the increasing encroachment of neoliberalism into all areas of our lives making some of the most politically astute films of that period such as the Widow of Saint Pierre.\nPerhaps because of this history it is French cinema, more than other national cinemas, that has put up the most consistent and sustained defence against the homogeneity of the dominant Hollywood product. In France cinema is not just a cultural product it is also a political issue; during the GATT negotiations in the early 1990s it was predominantly the French who fought the USA who were demanding the inclusion of cultural products (cinema) in the liberalisation of the trade in goods. This would have meant the cinema being treated as a purely commercial product and would have resulted in the quota system in France, where 20 percent of films on French screens have to be made in France, becoming illegal. It would also have prevented the payment of the considerable subsidies contributed by the French government to support the production of French cinema.\nThe criteria for choosing these films, according to Jon Davies, is that all the films are culturally significant not just to French culture in particular but to film culture as a whole.\nInevitably there will be some disappointments – no Truffaut or Godard – none of the films that were part of the left turn of the 1990s – but this season is in the nature of an experiment and if it is successful there will be other seasons.\nTo check out the programme for the French Sundaes please visit The Cinema Museum website.\nDeirdre O’Neill is PhD student at the University of Ulster. Her 12-year-old son volunteers as an usher at The Cinema Museum.']"	['<urn:uuid:ed5c7531-60e2-45a6-a9e8-b91b2112f197>', '<urn:uuid:f784ad39-af37-4167-ae49-ff3d9fb276ac>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T02:03:54.148118	20	118	1419
19	compare characters stories created by ma wing shing stan sakai martial arts theme	Ma Wing-shing and Stan Sakai both created martial arts-themed comics with distinct characters. Ma created The Storm Riders, which features martial arts heroes in an underworld setting, with characters like Bo Ging-wan and Nip Fung engaging in fantasy martial arts stories. Stan Sakai, in contrast, created Usagi Yojimbo, which follows Miyamoto Usagi, a samurai rabbit character living in late-sixteenth and early-seventeenth-century Japan. Both creators developed long-running series with martial arts themes, but while Ma's work focused on human characters in a contemporary fantasy setting, Sakai's work featured anthropomorphic characters in a historical Japanese setting.	"['Action hero not riding into the sunset\nMa Wing-shing, creator of The Storm Riders, wants to put life back into the local comics industry\nAt the crowded opening of The Birth of a Hero exhibition, Ma Wing-shing, the local comic artist featured in the show, was mobbed by hundreds of guests - from government officials to industry veterans and press.\nThey were applauding his success and admiring his creative journey that is on show at Comix Home Base, the heritage building that\'s been turned into a creative space between Mallory Street and Burrows Street in Wan Chai.\nIt was in stark contrast with the old times.\n""When I first started, Hong Kong comics were deemed violent and vulgar,"" says Ma, reclining in a single-seat sofa in his spacious office in Quarry Bay, pondering the contrast between then and now.\n""It has taken 30 years to change society\'s perception.""\nMa\'s unique brand of illustrations telling stories of the martial arts underworld from The Chinese Hero to The Storm Riders have mesmerised a couple of generations of Hongkongers.\nOver that time, he says, the comics have slowly changed people\'s perceptions.\n""Those who grew up with [our works] are now in their 40s or 50s. Many of our readers are now professionals or even school headmasters. They understand that Hong Kong comics are not toxic, and so they won\'t discourage their children from reading them,"" says the 52-year-old.\n""The government\'s attitude has changed too, recognising Hong Kong comics as part of local culture, a kind of simple cultural and creative industry that can be initiated by individual artists as opposed to film and animation productions, which are costly and labour-intensive,"" he says.\nIronically, despite improving public perception, the market is shrinking.\n""Now it\'s not about developing a new market. There isn\'t even a market now,"" Ma says.\nIt is estimated that the number of weekly comics fell from the peak of 50 in the 1990s to about 20 today.\nHong Kong comics were a lucrative business when Ma achieved his fame. Studies show that their annual retail sales in 1990 reached about HK$17.9 million. Jademan Comics, founded by Tony Wong Yuk-long, was listed on the stock exchange in 1986.\nDespite the frequent display of coarse language and depiction of sex and violence, scholars agreed that these comics were important cultural products.\nIn recent years the government has shown support for comics through its industry agency CreateHK. Last year, the Avenue of Comic Stars was opened in Kowloon to woo fans.\nIt features 24 figures that are up to three metres tall of classic Hong Kong comic characters and has sponsorship of HK$1.5 million to HK$2 million of taxpayers\' money.\nIn July this year, the grade II historic Green House was revitalised as the HK$200 million Comix Home Base, dedicated to the culture of comics and operated by the Hong Kong Arts Centre.\nThe city has a long history of comics, evolving from political illustrations from early in the last century to the second world war period, then to the manhua serials in the 1950s to 1960s which were popular with the post-war baby boomers who were in need of reading material for children.\nOld Master Q by Alfonso Wong Kar-hei, who published the title under pen name Wong Chak (his eldest son\'s name), and 13-Dot Cartoons by Lee Wai-chun were among the best-selling works in the 1960s.\nThe trend was then overtaken by a new martial arts genre in the 1970s under the influence of kung fu films and the popularity of Bruce Lee. Wong\'s series such as Siulauman in 1970 fitted the bill at the time but was criticised for its violence and vulgarity.\nMa, a disciple of Wong, took things further with his realist style of drawings in The Chinese Hero that was often compared to the work of Japanese manga artist Ryoichi Ikegami, creator of crime thriller Crying Freeman.\nFirst published in 1982 as a supplement to Wong\'s Drunken Fist, The Chinese Hero was recognised as a breakthrough in Hong Kong comics. Its popularity encouraged Ma to turn it into an independent series the following year. It sold 40,000 copies a week.\n""I only wanted to show my creativity through my works,"" Ma recalls.\nHe started drawing when he was a teenager, earning just HK$150 a month. ""A box of bean curd stick and fatty pork rice cost only HK$2 to $3 back then. I spent every penny at the end of the month, and I worked overnight in the office all the time. But I was very happy back then.""\nMa left Wong in 1988 and in 1989 founded Tin Ha Publishing, publishing Tin Ha Pictorials, featuring The Storm Rider, which was adapted into a movie starring Aaron Kwok Fu-shing, martial arts hero Bo Ging-wan and Ekin Cheng as his comrade Nip Fung. Its overwhelming popularity has kept the series going, probably until next year.\nBut the end of the long-running series is in sight.\n""If I keep drawing, I will disappoint my fans,"" Ma says.\n""I\'ve been in this business for almost 40 years. Long series always have a market, but do I have to do it my whole life? Maybe there\'s another story in my life. It\'s time to let go.""\nHe began planning to conclude the fantasy series about the underworld of martial arts four years ago.\n"" The Storm Riders is the most profitable [title], and giving it up is very difficult for the company,"" he says.\nThe first thing he had to do, says Ma, was to stop expanding so that the ending of The Storm Riders would not be a severe blow to the company, which now has about 30 people.\nThe series will be put out monthly before it all comes to an end in the next 10 to 12 months.\nBut Ma is not going to cut his ties with the comics world. He wants to become a freelancer and begin writing stories without the need to worry about the market.\nHe is also venturing into animation. Together with partners from Hong Kong, he is betting on an animation and merchandising project on the mainland. They have pooled 30 million yuan (HK$37.8 million) and hope to help revive Hong Kong\'s comics industry, which has reached a dead end in the market.\nBefore the series - of which Ma was reluctant to disclose details - goes to air in Beijing and Shanghai in mid-September, a range of toys and props will be manufactured based on market studies and will be ready for sale when the series hits the screen.\n""This is a whole chain of production. Hong Kong is difficult because the comics industry is part of a broken chain,"" Ma says.\nMa, who is often reported in the media as a wise property investor, says that although his project is costly, chasing after a dream isn\'t.\n""Creative people can live a simple life happily because they have hope. But today\'s young people don\'t have any hope except for buying a property,"" he says. ""We need to give them more upward mobility.""', 'Sakai at San Diego Comic Con 2006\n|Born||Stan Masahiko Sakai\nMay 25, 1953\n|Area(s)||Artist and writer|\nStan Sakai (スタン 坂井?, born May 25, 1953 in Kyoto, Japan) is a third-generation Japanese American Cartoonist comic book creator. He is best known as the creator of the comic series Usagi Yojimbo.\nHe began his career by lettering comic books (notably Groo the Wanderer by Sergio Aragonés and Mark Evanier) and wrote and illustrated The Adventures of Nilson Groundthumper and Hermy; a comic series with a medieval setting, influenced by Sergio Aragones\'s Groo the Wanderer. The characters first appeared in Albedo #1 in 1984, and were subsequently featured in issues of Critters, GrimJack, Amazing Heroes and Furrlough.\nSakai became famous with the creation of Usagi Yojimbo, the epic saga of Miyamoto Usagi, a samurai rabbit living in late-sixteenth and early-seventeenth-century Japan. First published in 1984, the comic continues to this day, with Sakai as the lone author and nearly sole artist (Tom Luth serves as the main colorist on the series, and Sergio Aragonés has made two small contributions to the series: the story ""Broken Ritual"" is based on an idea by Aragonés, and he served as a guest inker for the black-and-white version of the story ""Return to Adachi Plain"" that is featured in the Volume 11 trade paperback edition of Usagi Yojimbo). He also created a futuristic spinoff series Space Usagi. His favorite movie is Satomi Hakkenden (1959). The Japanese American National Museum in Los Angeles\'s Little Tokyo presented an exhibit entitled ""Year of the Rabbit: Stan Sakai\'s Usagi Yojimbo"" from July 9 through October 30, 2011.\nSakai was the artist for Riblet, the back-up feature in the trade paperback of Stupid, Stupid Rat Tails.\nIn 2013, Sakai illustrated the limited comic book series 47 Ronin, an adaptation of the famed story of the 47 Ronin written by Dark Horse Comics Publisher Mike Richardson and with Lone Wolf and Cub writer Kazuo Koike as an editorial consultant.\nHe resides in Pasadena, California.\n||This section of a biography of a living person does not include any references or sources. (April 2012)|\n- 1990 Parents\' Choice Award for ""Skillful weaving of facts and legends into his work""\n- 1991 Inkpot Award from Comic-Con International: San Diego for ""Lifetime Achievement in the Field of Cartooning""\n- 1996 Eisner Award for ""Best Letterer"" (Groo and Usagi Yojimbo)\n- 1996 Eisner Award for ""Talent Deserving of Wider Recognition"" (Usagi Yojimbo)\n- 1999 Eisner Award for ""Best Serialized Story"" (Usagi Yojimbo ""Grasscutter"")\n- 1999 Haxtur Award for ""Best Short Story [in Spain]"" (Usagi Yojimbo ""Noodles"" [Spanish Edition])\n- 2000 Haxtur Award for ""Best Script [in Spain]"" (Usagi Yojimbo\'s ""Grasscutter"" [Spanish Edition])\n- 2001 Ursa Major Award for ""Best Anthropomorphic Comic Book or Strip""\n- 2002 National Cartoonists Society Comic Book Division Award (Usagi yojimbo)\n- 2002 Ursa Major Award for ""Best Anthropomorphic Comic Book or Strip""\n- 2003 Ursa Major Award for ""Best Anthropomorphic Comic Book""\n- 2003 La Plumilla de Plata (Silver Inkpen Award) in Mexico for his lifetime achievements and contributions to comic books.\n- 2004 Ursa Major Award for ""Best Anthropomorphic Other Literary Work"" (The Art of Usagi Yojimbo) and ""Best Anthropomorphic Comic Book"" (Usagi Yojimbo)\n- 2005 Ursa Major Award for ""Best Anthropomorphic Comic Book"" (Usagi Yojimbo)\n- 2007 Harvey Award for ""Best Letterer""\n- 2011 Cultural Ambassador Award\n- 2012 Eisner Award for ""Best Lettering"" (Usagi Yojimbo)\n- 2014 Inkwell Award for The All-in-One Award (47 Ronin)\nFrom 1993 through 2005, Stan Sakai has received twenty-one Eisner Award nominations. He has also been nominated for the Comics Buyer\'s Guide Award for Favorite Writer in 1999 and 2000.\n- ""The Art of \'Usagi Yojimbo\' at the Cartoon Art Museum"". Animation Insider. Retrieved 2010-11-25.\n- ""Usagi Yojimbo creator comes back to where it all began"". Seattle Post-Intelligencer. Retrieved 2010-11-25.[dead link]\n- ""USAGI YOJIMBO: Stan Sakai and Diana Schutz"". Mania.com. Retrieved 2010-09-02.\n- ""Interview: Stan Sakai: Down the Rabbit Hole with Usagi Yojimbo"". The Trades. Retrieved 2010-11-25.\n- Solomon, Charles (March 8, 1993). ""Take one part Toshiro Mifune. Then add adventure and humor to get artist Stan Sakai\'s \'Usagi Yojimbo.\'"". The Los Angeles Times. Retrieved 2010-08-30.\n- Solomon, Charles (December 18, 2005). ""Don\'t get between the rabbit and his sword"". The Los Angeles Times. Retrieved 2010-08-30.\n- ""25 YEARS OF ""USAGI YOJIMBO"""". Comic Book Resources. Retrieved 2010-08-30.\n- ""25 Years of the Rabbit Ronin: Stan Sakai on Usagi"". Newsarama. Retrieved 2010-08-30.\n- ""BCC: SPOTLIGHT ON STAN SAKAI"". Comic Book Resources. Retrieved 2010-08-30.\n- ""Stan Sakai Talks Usagi Yojimbo"". UGO.com Comics. Retrieved 2010-08-29.\n|Wikimedia Commons has media related to Stan Sakai.|\n- Stan Sakai\'s biography at Usagi Yojimbo.com\n- Sakai\'s LiveJournal page\n- Comic Book Awards Almanac\n- Stan Sakai – Video interview by the Japanese American National Museum, Discover Nikkei project (September 28, 2010)']"	['<urn:uuid:11f500f4-6d98-4573-be74-e6f43bee4af1>', '<urn:uuid:a1da3146-a84c-425b-a810-c708e29c028d>']	open-ended	direct	long-search-query	distant-from-document	comparison	novice	2025-05-13T02:03:54.148118	13	94	1979
20	How did Muhammad Ali's communication style evolve over time, and what insights does this offer about modern screen-based communication between families?	Muhammad Ali was known as the 'Louisville Lip' in his youth for being extremely loud, brash, and boastful, but by age 50 he had evolved into a respected worldwide goodwill ambassador. This contrasts with modern communication patterns where, instead of direct interaction, families often resort to screen time, with parents and teens turning to screens for stress relief (40% of parents and 58% of teens), and family members pulling out mobile devices at restaurants rather than talking to each other.	"['“The man who views the world at 50 the same as he did at 20 has wasted 30 years of his life.”\nThis was the answer the newly crowned Heavyweight Champion, Muhammad Ali gave when asked about his philosophy of life. At 50, Muhammad Ali was one of the most revered men throughout the entire world. As an athlete, that reverence was on display four years later in 1996 when the former Olympic Gold Medalist was given the honor of carrying the torch to light the Olympic flame during the Summer Olympics in Atlanta, Georgia. Muhammad Ali, in his fifties, was a worldwide goodwill ambassador and was known as such for the rest of his life. A life that ended suddenly on June 3rd of 2016.\nMuhammad Ali not only viewed the world differently at 50 than he did at 20, but the world viewed him differently at 50 than it viewed him at 20. By the age of 20, Ali, who was still known as Cassius Clay, was an up and coming heavyweight contender with a flashy style and extremely loud mouth. At age 2o, with his boxing moniker, the “Louisville Lip” was brash, boastful, went out of his way to be heard and seen and brutally taunted opponents into matches against him and most annoyingly WHILE engaged in a match against him. This was the strategy he used to get a match against Sonny Liston, whom everyone assumed would knockout Clay and do it rather quickly. That didn’t happen. Clay beat him and beat him easy. Clay’s skill was his speed, which he used to hit Liston whenever he wanted and avoid every punch thrown at him. At 22, Cassius Clay became the youngest fighter to defeat a reigning champion in boxing history. And history is what happened.\n“I know where I’m going and I know the truth, and I don’t have to be what you want me to be. I’m free to be what I want.”\nBack in the early 1960s, Black athletes were not as prevalent as they were today. Yes the likes of Jim Brown, Willie Mays and Bill Russell were becoming stars in their own right, they weren’t anything like we see black athletes today. They certainly had not the sort of prestige and privilege they do today. Muhammad Ali, willfully went against that grain. But you have to put into perspective what he was.\nFact #1: He was Black. The Civil Rights Movement had only begun to get off the ground. Black men and black people in general were still effectively second-class citizens in many ways, if not by law, certainly by expectation. They “had their place”. Some were burgeoning stars, but we’re respectful and we’re consider safe and likeable, much like a Jackie Robinson. Not Ali.\nFact #2: He was arrogant. Muhammad Ali was really good at what he did and he knew it and was not afraid to tell you about it, quickly displacing the given moniker, the “Louisville Lip” with a chosen moniker, “The Greatest”. If mainstream America had their opinions on the place and status of black people, it certainly was made worse with one as proudly boastful as Ali was.\nFact #3: He was Muslim. Not just any typical Muslim, which then had not the same connotation as it does today. But Muhammad Ali, shortly after winning his title had began using the name Muhammad Ali when he joined the Nation of Islam, which was notable for its extremism and militancy. Think what Black Lives Matter is today with the reputation of today’s Klu Klux Klan. That was how the majority felt about the Nation of Islam. Now imagine one of the more popular athletes in American sports joins such a group. How would we react today if Tom Brady or Bryce Harper announced very publicly that they are joining the Aryan Brotherhood. How would America and the World treat these figures in today’s world? That was the position Muhammad Ali was in. The biggest difference is that Bryce Harper and Tom Brady are already accepted and welcomed figures. Muhammad Ali was Black and arrogant and then had the audacity to join Black Separatist, who the nation feared and rejected, to make it even worse.\nThe aversion to what had become to Muhammad Ali wasn’t just with mainstream White America, but it was also held by many within Black America. Former champions like Joe Louis were shunned by him. Jackie Robinson, the first Black player in the Major Leagues, who was rather universally accepted by most Americans, he would not bring himself to call the Muhammad Ali by his new name. This action was repeated by other fighters like Ernie Terrell and former champion, Floyd Patterson. Patterson, who before facing Ali, said he wanted to beat Ali to “bring the title back to America”. This was Muhammad Ali in 20s. Reviled by White America. Disrespected by Black America. This would be bad enough, but as a militant, arrogant black athlete in the 1960s, he then made it worse.\nFact #4: Muhammad Ali refused to be drafted into the Vietnam War. Across America young men were being drafted to go to war and Ali was expected to do the same. Yet, he refused. He did what thousands of others could not do and would not do. The United States of America was in the mist of stopping the spread of Communism and fighting for the world’s freedom and America’s most notable champion refused that fight. What he did, many across America considered unpatriotic and shameful. But because of his very public personal and religious beliefs, he would not accept being drafted. And he did this at the expense of his career. Not only was Muhammad Ali a champion at the top of his career, in the prime of his career, by his actions and his suspension and facing imprisonment, Muhammad Ali was willing to give up his career for an undetermined length of time. It should have been a matter of months, but it ended up being a matter of years and could have easily been a matter of decades. There was no way to tell but he still refused.\nAnd that’s when things started to change. We saw the toll the war begin to take on our population and specifically, black men who mostly fought it. Not to mention, if there is one thing all Americans can recognize, no matter the guise, is a man that is self-made and a man who gains his own achievements on his own merits. They also recognize when something is unjust and done without order. The American people recognized Muhammad Ali’s skill and his championship he earned and they saw it taken away from him and not because of something he could not do. He did not lose his title. It was taken from him. And just like he did, freely telling crowds about his greatness, he took that same message about his own career. And surprisingly enough, the people started to listen.\nEventually, Ali was able to make it back to boxing. He suffered a setback in his attempt to regain his championship in 1971 against Joe Frazier in the most watched bout in the world. But his star was not diminished. Frazier would receive a savage beating from another Olympic Medalist and Champion, George Foreman. Foreman who was universally feared by the public in general, let alone all other heavyweight boxers, he was challenged by Ali in Zaire, Africa. The Rumble in the Jungle it was called. When Ali arrived in Africa he was celebrated in a way that he wasn’t in America. Crowds flocked to him by the thousands shouting his name, wanting to be with him. Ali would go on to win. Something no one assumed he would do. His next great test came against his greatest rival, Joe Frazier, in yet another part of the world, the Phillipines. Muhammad Ali would not only face insurmountable challenge after insurmountable challenge but he made sure the world was able to see him over come each of these challenges. And as much as he brought his displays of talent to the world at large, he also brought something even more invaluable; himself. An ambassador of Black America and Islam, to a world that is unfamiliar with each.\nI talk a lot on politics and society and the world as we see it. Yes, I tend to keep conversations friendly, but one way to get me to shut up about it is to talk boxing. I’m a huge fan of the sport and there’s almost no conversation that does not feature Muhammad Ali. But it’s almost impossible to talk about Muhammad Ali and not talk about the man he had become later in his career and after. Given the circumstances today, where certain presidential candidates have made remarks and criticisms of Islam and Muslims worldwide, recognition of Muhammad Ali and the life he’s lived is critical in bridging the gap of misunderstanding and misconceptions of what Islam actually is. He was probably the world’s most recognizable Muslim and the most visible example of what being a Muslim looks like. Even towards the end of his life when he was no longer able to speak, his actions spoke much louder. In the ring he was the greatest. But outside the ring, in his life, he was much more. For as much as he is known for his skill in the ring, his greatest fights were outside the ring. When he was between ropes with a pair of gloves on he was unquestionably a champion, known as ‘the Greatest”. But to the rest of the world, he wasn’t just “the Greatest”, he was the entire World’s Greatest.\n“Live everyday as if it will be your last because some day you’re going to be right.”', ""Hansa Bhargava MD FAAP\nStaff Physician, Children's Healthcare of Atlanta\nMedical Editor, WebMD\nPediatricians are seeing more and more teens suffering from stress. Whether they are complaining of it or having somatic symptoms such as headaches and stomach aches, it seems that stress and anxiety are on the rise. We know that over scheduling, homework, and the pressures of getting into college can contribute to this. But can media also affect it? Is screen time and media a stressor or a remedy for stress?\nIn a recent WebMD survey published in their Teens and Stress report, 54% of teens were stressed according to parents. Interestingly, 40% of parents turned to the screen for family stress relief while 58% of teens did. Social media and texting was used as stress relief by almost half the teens. This is on the heels of the Common Sense Media survey reporting that US teens were using media for 9 hours a day. Other recent reports have shown that 94% of teens with mobile devices are online daily with many online constantly.\nSo it seems that stress is on the rise and media use is on the rise. Although there may not be a direct relationship, some real issues impact stress and anxiety. Consider this: 23 % of teens report cyberbullying, especially girls. There have been reports of “Facebook depression” and loneliness, as kids who aren’t in social media conversations may feel left out. Other negative consequences can also have an impact: many teens are in front of a screen late at night or ‘sleep text’, both of which can contribute to lack of sleep, which in turn can decrease focus and potentially cause irritability and depression. And last but certainly not least, what about the time media consumes?\nTime spent on media is time often not spent communicating with family. Lately, when I’ve gone into a restaurant, I’ve observed that as soon as a family sits down, everyone pulls out a mobile device. No one is really talking. So even the short amount of time not doing homework, playing soccer, or at school is being compromised. Psychologists, community leaders and experts have long reported that family time can contribute to less depression, less anxiety, better academic performance and generally happier kids. But what if that family time is on media??\nAs the AAP reviews our screen time recommendations, I feel that we, as pediatricians should continue to advise parents about basic principles.\nParents need to lay down some parameters about when and how media is used. Media is a centerpiece of teens’ lives and is not going away, but just as we don’t give our kids a set of keys to our car and say “just drive”, we need to enforce appropriate media use. And good modeling is also critical: parents need to put down their mobile devices and simply communicate with their kids. Old fashioned parenting and just talking to your kids can build the foundation to a less stressful childhood and hopefully a happier life.""]"	['<urn:uuid:45b7d019-3778-440d-8196-889139110046>', '<urn:uuid:9ee5a44c-ac0e-4471-bc2a-f3b31e423c46>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T02:03:54.148118	21	80	2144
21	bathroom surface cleaning methods vs phone sanitation techniques compare bacteria prevention	For bathrooms, pre-cleaning to remove debris and bodily soils is essential, followed by using EPA-registered products with specific kill claims for pathogens like MRSA and norovirus. Special attention must be paid to frequently touched surfaces, and products must be chosen that can break down uric acid crystals for odor elimination. For phones, cleaning options include UV-C light devices that kill up to 99% of germs without damaging electronics, specialized solutions like Whoosh! that leave protective coatings, or a DIY mixture of water and alcohol applied carefully with a microfiber cloth. Both environments require specific cleaning protocols due to their high potential for germ transmission, but phones require more delicate handling to prevent damage to sensitive electronics.	['What the Tech? How to Sanitize Your Smartphone\nEveryone’s talking about washing hands to stop the spread of the flu and the coronavirus, but what about the filthiest thing we touch every day?\nThe thing that can transmit flu-carrying germs straight to clean hands? You know, smartphones and computers.\nCleaning these expensive devices isn’t simple and may not be all that effective at killing germs for a simple reason: cleaning solutions and simple soap and water can damage screens and even cause sudden death of the devices.\nFortunately, there are a few products that help keep them clean.\nSeveral companies are selling products that kill up to 99% of germs and bacteria using ultra-violet light. Health officials say UV-C light kills germs without affecting the electronics inside the phone.\nUV Angel and Phone Soap have released new products that use UV-C light to clean smartphones. You simply place the device in a tray or case and turn on the light. These are in the $100 range.\nZeiss makes portable wipes that come in small packets to clean the screen and keyboards. These use a proprietary solution to wipe away smudges and will eliminate dust, grime, and some germs.\nWhoosh! is a popular product made especially for smartphones. The solution comes in several styles. There are single-use wipes and spray bottles with an anti-microbial cloth. Whoosh! claims the solution leaves an invisible coating that prevents germs, dust, and grime from attaching to the phone.\nClorox also makes cleaning wipes for computers, laptops, and phone screens that kill germs. The products made for screens do not drip.\nAs for using the common Clorox or Lysol wipes, these can drip and should only be used on smartphone cases and laptop cases and computer mice. It’s crucially important not to let drops of water or any liquid get into smartphone ports or underneath a laptop keyboard.\nFor computers you can use a bit of compressed air on the keyboard to remove any dust or food crumbs that may have fallen underneath. Be careful though, too much pressure and you might blow the keys right off the keyboard.\nFor DIY’ers you can make your own cleaning solution of water and white vinegar or alcohol. Put just a few drops of a 50-50 mixture onto a microfiber cloth and wipe down the devices. Be careful that the cloth is not dripping with the solution. You won’t need much of it.\nWipe down the device and dry it off with another cloth. I recommend using the alcohol mixture because isopropyl or rubbing alcohol will evaporate quickly.\nOne of the best ways to keep your phone clean is by keeping your hands cleaner. Washing them frequently for 20 seconds or more will kill germs that cause the flu or other illnesses.\nAnd think about this the next time you take your phone to the bathroom. If you use it, then wash your hands, the germs on your phone will be transferred to your freshly washed hands almost immediately.\nHealth officials say viruses can live on hard glass surfaces for up to 96 hours.', 'This article appears in the Sept 1 issue of Claning and Maintenance management Magazine.By Rosie D Lyles\nThe Nano-Shield Antimicrobial System is 99.99% effective against all of the microbes discussed in this article. We offer the answer to both health and aesthetic issues related to public restroom maintenance.\nCleaning for Health in Public Restrooms\nIn any number of public and commercial settings—from office buildings, hotels, restaurants, and schools to health care facilities and other businesses—restrooms are consistently cited as one of the toughest areas for cleaning and maintenance professionals to maintain—and the No. 1 source of customer complaints. This is due in large part to the dual imperatives of public restroom maintenance: cleaning for aesthetics and cleaning for health.\nEliminating odors and maintaining a visibly clean restroom is extremely important, as research has shown consumer perceptions of facilities’ restrooms can impact bottom lines. However, restroom cleanliness is also very important to public health.\nThe following is an overview of the critical role restroom cleaning and disinfection plays in protecting public health, and steps that in-house custodial professionals and building service contractors (BSCs) can take to help control and prevent the spread of germs in their facilities’ restrooms.\nPublic Restrooms and Public Health\nSome people mistakenly assume that illness-causing germs and multi-drug resistant organisms (MDROs), or superbugs, are a health care-specific problem and don’t pose much of a threat in other public settings. However, there is a growing body of research that shows that many of these microbes are commonly found in public restrooms and are easily transmitted between individuals through contact with contaminated surfaces.\nIn a 2011 study published in the journal PLoS ONE, researchers took samples from 10 restroom surfaces on the University of Colorado at Boulder campus, including door handles into and out of the restroom, handles into and out of a restroom stall, faucet handles, the soap dispenser, toilet seat, toilet flush handle, floor around the toilet, and floor around the sink in restrooms. The study, “Microbial Biogeography of Public Restroom Surfaces,” found that human-associated microbes were commonly found on restroom surfaces.\nFrom a public health perspective, the findings were significant because the high number of skin and gut-associated bacteria found throughout the restrooms suggested that “pathogens commonly found on skin (e.g. Staphylococcus aureus) could readily be transmitted between individuals by the touching of restroom surfaces.”\nIn another study of university restrooms at San Diego State University in California, researchers found within one hour of cleaning and disinfection, bathrooms were completely recontaminated with microbes, and fecal bacteria was found on a variety of surfaces from toilet seats to soap dispensers.\nIn a 2013 study, “Could Public Restrooms Be an Environment for Bacterial Resistomes?” published in PLoS ONE, researchers from the Queen Mary University of London took a closer look at Staphylococcaceae bacteria—which are commonly found in restrooms and are a major cause of infections both in hospitals and the larger community—to determine if non-health care restrooms could be a source of antibiotic-resistant bacteria. To test this theory, the research team collected samples from public restrooms in public buildings, testing samples from various sites in each restroom. The researchers identified 19 different types of Staphylococcaceae from the restroom samples, and more than one-third (37.8 percent) were antibiotic-resistant.\nThere’s no question that restroom cleaning is one of the toughest and most important jobs in the industry, but there is good news: Health-focused restroom maintenance can actually help ensure that restrooms are both aesthetically and hygienically clean.\nOftentimes, according to studies published in the Journal of Applied Microbiology and the Journal of Applied and Environmental Microbiology, seemingly aesthetic issues, such as unpleasant odors and visibly stained or soiled surfaces, can signal the presence of harmful microorganisms, such as Shigella, Salmonella, Hepatitis A, E. coli, and norovirus, so health-focused cleaning and disinfecting protocols must also address aesthetics.\nHere are some cleaning and disinfecting tips that any facility can follow to help prevent the spread of germs and improve aesthetics in public restrooms:\n• Pre-clean surfaces. Remove debris and bodily soils, such as urine, feces, and vomit, and then use a U.S. Environmental Protection Agency (EPA)-registered product with kill claims for contagious and hard-to-kill pathogens, such as methicillin-resistant Staphylococcus aureus (MRSA), influenza, and norovirus, to disinfect surfaces.\n• Vet your products. Pay particular attention to disinfecting toilet surfaces and other surfaces frequently touched by hands by using products designed for those surfaces. Remember to check the product label and follow manufacturer’s instructions to ensure proper use and contact time. Different products have different contact times for killing certain pathogens .\n• Don’t mask odors. Eliminate restroom odors by breaking them down at their source. Not all products can actually break down and eliminate the root cause of urine odor—uric acid crystals. This is why it is important to choose products that include stable active ingredients, such as ready-to-use hydrogen peroxide-based solutions, which fight urine odors and stains and require no additional training for cleaning staff to use.\n• Clean the floors. Remember that floor care is important, too. Remove grime and scuff marks on restroom floors, and also disinfect them. Restroom floors are often germ hot spots with about 230 bacterial species, compared to 150 species in other restroom locations, as reported in the survey, “Microbial Biogeography of Public Restroom Surfaces.”\n• Clean mirrors and glass. Keep them shining by removing water marks, soils, and streaks with a general glass and surface cleaner. Scrub away soap scum on sinks, countertops, and other surfaces with products specifically formulated to break it down.\n• Encourage hand hygiene. Remember, handwashing is an important part of preventing the spread of infections. Cleaning staff should wash their hands regularly with soap and warm water, especially after emptying waste baskets, touching used tissues, or using the bathroom.\nThe potential for germ transmission in public restrooms will always be high. However, by implementing thorough cleaning and disinfecting protocols that focus on tell-tale signs of contamination, such as unpleasant odors and visible stains or soils, and addressing key surfaces where germs are most common, cleaning professionals can help provide a healthy environment for building occupants and visitors.']	['<urn:uuid:5ca7739d-c728-4aa0-a468-1c33931fdb98>', '<urn:uuid:3c00645a-f27a-48bc-9cde-2abd6df58a14>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-13T02:03:54.148118	11	116	1532
22	who wrote poem in flanders fields	Lieutenant-Colonel John McCrae wrote 'In Flanders Fields' in 1915 after seeing red poppies growing in bomb-ravaged fields littered with bodies.	"[""World War I, the war that was originally expected to be “over by Christmas,” dragged on for four years with grim brutality brought on by the dawn of trench warfare and advanced weapons, including chemical weapons. The horrors of that conflict altered the world for decades—and writers reflected that shifted outlook in their work.\nAs Virginia Woolf would later write, “Then suddenly, like a chasm in a smooth road, the war came.”\nEarly Works Glorified the War\nAmong the first to document the “chasm” of the war were soldiers themselves. At first, idealism persisted as leaders glorified young soldiers marching off for the good of the country.\nEnglish poet Rupert Brooke, after enlisting in Britain’s Royal Navy, wrote a series of patriotic sonnets, including “The Soldier,” which read:\nIf I should die, think only this of me:\nThat there’s some corner of a foreign field\nThat is for ever England.\nBrooke, after being deployed in the Allied invasion of Gallipoli, would die of blood poisoning in 1915.\nThe same year, Canadian doctor Lieutenant-Colonel John McCrae, upon seeing how red poppies grew in the fields that had been ravaged by bombs and littered with bodies, wrote “In Flanders Fields.” The poem, memorializing the death of his friend and fellow soldier, would later be used by Allied militaries to recruit soldiers and raise money in selling war bonds:\nIn Flanders fields the poppies blow\nBetween the crosses, row on row,\nThat mark our place, and in the sky,\nThe larks, still bravely singing, fly,\nScarce heard amid the guns below.\nLiterary Tone Shifts After Grueling WWI Combat\nWhile both Brooke’s and McCrae’s works lent patriotic tones to the sacrifices of war early in the conflict, as time wore on, the war’s relentless horrors spawned darker reflections. Some, like English poet Wilfred Owen, saw it their duty to reflect the grim reality of the war in their work.\nAs Owen would write, “All a poet today can do is warn. That is why the true poet must be truthful.” In “Anthem for the Doomed Youth,” Owen describes soldiers who “die as cattle” and the “monstrous anger of the guns.”\nOwen’s fellow army officer, Siegfried Sassoon, writes of corpses “face downward, in the sucking mud, wallowed like trodden sand-bags loosely filled” in his 1918 poem, “Counter-Attack.”\nFrom the opposite side of the firing lines, German writer Erich Maria Remarque also experienced the grim day-to-day life of a soldier. Later, in 1929, he published an unflinching account in his novel, All Quiet on the Western Front.\nAmong other prominent works reflecting the horrific realities of war was the four-part tome, Parade’s End, by English novelist Ford Madox Ford, and from the Eastern Front, Dr. Zhivago by Soviet Russian writer Boris Pasternak, in which the main character describes grotesque injuries inflicted on the war’s battlefields.\nErnest Hemingway Pens 'A Farewell to Arms'\nIn one of the most famous works set during the “Great War,” American writer Ernest Hemingway offers a gripping love story between a soldier and a nurse set against the chaotic, stark backdrop of World War I.\nA Farewell to Arms is among the writer’s most autobiographical: Hemingway himself served as an ambulance driver during the war, was severely wounded on the Austro-Italian front and had been sent to a hospital in Milan, where he fell in love with a nurse.\nVirginia Woolf Writes on War's Impact on Society\nThe literary response to World War I was not only to portray its horrors at the front but also the reverberations of the war throughout society.\nVirginia Woolf, who had been a close friend of the fallen poet Rupert Brooke, wove profound references to the war’s effects throughout her works.\nIn the setting of her acclaimed novel Mrs. Dalloway, the war has ended, but everyone remains deeply affected by it, including one of the novel’s main characters, a veteran with severe shell shock (now known as PTSD, or post-traumatic stress disorder).\nModernism Emerges in Works by Joseph Conrad, James Joyce, T.S. Eliot\nThe disillusionment that grew out of the war contributed to the emergence of modernism, a genre that broke with traditional ways of writing, discarded romantic views of nature and focused on the interior world of characters.\nWoolf’s novels reflected this emerging tone, as did the works of Joseph Conrad (Heart of Darkness) and James Joyce (Ulysses). T.S. Eliot’s “The Waste Land,” considered to be one of the most significant poems of the 20th century, presents a haunting vision of postwar society, with the opening lines:\nApril is the cruellest month, breeding\nLilacs out of the dead land, mixing\nMemory and desire, stirring\nDull roots with spring rain.\nAldous Huxley’s dystopian novel Brave New World questions once-accepted social and moral notions in presenting a nightmarish vision of the future.\nWorld War I devastated continents, leaving some 10 million soldiers and 7 million civilians dead. But writers responded with profound and groundbreaking work as they and the rest of the world grappled with the war’s upheaval.\nAs Remarque wrote in All Quiet on the Western Front: “All these things that now, while we are still in the war, sink down in us like a stone, after the war shall waken again, and then shall begin the disentanglement of life and death.”""]"	['<urn:uuid:abe39343-610d-4691-a6c2-415088257f94>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T02:03:54.148118	6	20	876
23	explain meaning model coefficients negative positive regression numbers	In a linear regression model, the coefficient values indicate both the magnitude and direction of how predictor variables affect the response. A positive coefficient means that when the predictor variable increases by one unit, the response variable increases by that coefficient's value (when all other variables are held constant). A negative coefficient indicates that when the predictor variable increases by one unit, the response variable decreases by that coefficient's value. For example, in a model E(y) = 1.8 – 2.35X1 + X2, the coefficient -2.35 shows that for each one-unit increase in X1, there is a 2.35 unit decrease in the response, assuming X2 remains constant.	['Regression models describe the relationship between a dependent variable, y, and independent variable or variables, X. The dependent variable is also called the response variable. Independent variables are also called explanatory or predictor variables. Continuous predictor variables might be called covariates, whereas categorical predictor variables might be also referred to as factors. The matrix, X, of observations on predictor variables is usually called the design matrix.\nA multiple linear regression model is\nyi is the ith response.\nβk is the kth coefficient, where β0 is the constant term in the model. Sometimes, design matrices might include information about the constant term. However, fitlm or stepwiselm by default includes a constant term in the model, so you must not enter a column of 1s into your design matrix X.\nXij is the ith observation on the jth predictor variable, j = 1, ..., p.\nεi is the ith noise term, that is, random error.\nIn general, a linear regression model can be a model of the form\nwhere f (.) is a scalar-valued function of the independent variables, Xijs. The functions, f (X), might be in any form including nonlinear functions or polynomials. The linearity, in the linear regression models, refers to the linearity of the coefficients βk. That is, the response variable, y, is a linear function of the coefficients, βk.\nSome examples of linear models are:\nThe following, however, are not linear models since they are not linear in the unknown coefficients, βk.\nThe usual assumptions for linear regression models are:\nThe noise terms, εi, are uncorrelated.\nThe noise terms, εi, have independent and identical normal distributions with mean zero and constant variance, σ2. Thus\nSo the variance of yi is the same for all levels of Xij.\nThe responses yi are uncorrelated.\nThe fitted linear function is\nwhere is the estimated response and bks are the fitted coefficients. The coefficients are estimated so as to minimize the mean squared difference between the prediction vector bf(X) and the true response vector y, that is . This method is called the method of least squares. Under the assumptions on the noise terms, these coefficients also maximize the likelihood of the prediction vector.\nIn a linear regression model of the form y = β1X1 + β2X2 + ... + βpXp, the coefficient βk expresses the impact of a one-unit change in predictor variable, Xj, on the mean of the response, E(y) provided that all other variables are held constant. The sign of the coefficient gives the direction of the effect. For example, if the linear model is E(y) = 1.8 – 2.35X1 + X2, then –2.35 indicates a 2.35 unit decrease in the mean response with a one-unit increase in X1, given X2 is held constant. If the model is E(y) = 1.1 + 1.5X12 + X2, the coefficient of X12 indicates a 1.5 unit increase in the mean of Y with a one-unit increase in X12 given all else held constant. However, in the case of E(y) = 1.1 + 2.1X1 + 1.5X12, it is difficult to interpret the coefficients similarly, since it is not possible to hold X1 constant when X12 changes or vice versa.\n Neter, J., M. H. Kutner, C. J. Nachtsheim, and W. Wasserman. Applied Linear Statistical Models. IRWIN, The McGraw-Hill Companies, Inc., 1996.\n Seber, G. A. F. Linear Regression Analysis. Wiley Series in Probability and Mathematical Statistics. John Wiley and Sons, Inc., 1977.']	['<urn:uuid:b674c9ff-51f2-466e-8897-adb723be9459>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T02:03:54.148118	8	106	567
24	concerned relative need to know main parts body system affected by non hodgkin disease	The lymphatic system is affected by NHL and includes lymph (infection-fighting fluid), lymph nodes (small bean-shaped organs throughout the body), bone marrow (soft tissue in bones), thymus gland (under the breastbone), and spleen (under the ribs on the left side).	['Hemic and Lymphatic\nNon-Hodgkin Lymphoma (NHL)\nLymph nodes, bone marrow, the spleen and the thymus gland are all parts of the lymphatic system. This system works to help the body fight infection.\nNHL is cancer of the lymphatic system. The lymphatic system is part of the immune system, which helps the body fight infection. The lymphatic system includes:\n- Lymph: Infection-fighting fluid made mostly of a certain type of white blood cell called lymphocytes.\n- Lymph nodes: Small bean-shaped organs that filter lymph and store white blood cells. Lymph nodes are grouped together throughout the body. Some areas where they are found include the neck, armpit, and groin.\n- Bone marrow: Soft tissue found in the center of bones. Blood cells are made in the bone marrow.\n- Thymus gland: The thymus gland helps build the immune system in children. It produces a type of white blood cell called T-lymphocytes, which fight infection. The thymus gland is located underneath the breastbone.\n- Spleen: Organ that stores certain lymphocytes and filters the blood. It’s located under the ribs on the left side of the body.\nWith NHL, cancer cells form in the lymphatic system. When the cancer cells group together, they form a tumor. The tumor can spread (metastasize) to another part of the body, such as the lungs. The presence of cancer cells makes it hard for the body to fight infection and can cause other health problems.\nCall 504-842-3910 for more information or to make an appointment.\nWho Gets NHL?\nChildren at any age can get NHL, but kids ages seven to 11 are affected most often. NHL is not contagious, meaning your child can’t pass it to another person.\nWhat Causes NHL?\nNHL occurs when white blood cells grow abnormally (mutate). What causes this to happen is not fully known. If the cells crowd lymph nodes or other areas of the body, they can cause tumors to form. Mutations in certain genes may affect the way your child’s cells grow. This gene mutation is random and couldn’t have been prevented. In rare cases, other factors, such as exposure to certain viruses, chemicals or radiation, play a role. But most often, the cause of cancer in children is unknown.\nWhat Are the Symptoms of NHL?\nSome common symptoms of non-Hodgkin lymphoma include fever, night sweats and swollen lymph nodes in the neck, chest, armpits or groin. Your child may have experienced some of these symptoms, or other symptoms.\nHow Is NHL Diagnosed?\nYour child’s healthcare provider examines your child. You will be asked about your child’s health history. Your child may also have one or more of the following:\n- Blood tests to take a sample of the blood and test it\n- Imaging tests to take detailed images of areas inside the body. These may include a chest x-ray, MRI, CT scan, and PET scan.\n- Bone marrow aspirations and biopsies to take samples of bone marrow from the hipbones\n- Lumbar puncture, also called spinal tap, to take a sample of the fluid that surrounds the spinal cord from the child’s lower back\nStaging and Grading of NHL\nStaging is the process that determines the size of the cancer and how much it has spread. Most cancers have their own staging system. Grading is used to describe how abnormal the cancer cells look when seen through a microscope. The more abnormal the cells are, the faster they grow. Staging and grading help the healthcare team plan treatment for your child. They also help determine the likelihood of cure (prognosis). The process used for NHL takes into consideration the following:\nThe presence of symptoms related to NHL\n- Location of the primary tumor\n- Tumor size and number of tumors\n- If the cancer has spread to other areas of the body\n- How abnormal the cancer cells look under a microscope\nThe cancer is also broken down into stages one through four (often written as I through IV). The different stage numbers refer to the tumor’s size and if it has spread. For instance, stage I is a very early stage of cancer. Stage IV means the cancer is widespread. NHL is also broken down into further classifications. Your child’s healthcare provider can tell you more if needed. Talk to the healthcare provider if you have any questions about the stage of your child’s cancer.\nHow Is NHL Treated?\nThere are many different types of NHL. Your child’s healthcare provider will talk to you about the type your child has. The treatment your child receives depends on the type of NHL he or she has. Treatments may be combined. Your child may require one or more of the following treatments:\n- Surgery to remove all or part of a tumor\n- Chemotherapy (“chemo”) to destroy cancer cells with powerful cancer-fighting medications. Multiple chemo medications may be used. They are given through a tube (IV) that’s usually put into a vein in the arm or chest. Or, they may be given by mouth or injection.\n- Radiation therapy to destroy cancer cells and shrink a tumor using high-energy x-rays. Radiation may be used before or after other treatments.\nThe goal of supportive treatments is to protect the child from infection, prevent discomfort and bring the body’s blood counts to a healthy range. During your child’s treatment, he or she may be given antibiotics. These are medications that help prevent and fight infection. Anti-nausea and other medications may also be given. These help ease side effects caused by treatment. Your child may receive a blood transfusion to restore the blood cells destroyed by treatment. Blood is taken from a donor and stored until the child is ready to receive it.\nWhat Are the Long-Term Concerns?\nWith early treatment, NHL is often curable. But chemotherapy and radiation may cause some problems, such as damage to certain organs. So your child’s health will need to be monitored for life. This may include clinic visits, blood tests, imaging tests and ultrasounds of the heart.\nReceiving a cancer diagnosis for your child is scary and confusing. It’s important to remember that you are not alone. Your child’s healthcare team will work with you and your child throughout your child’s illness and care. You may also wish to seek information and support for yourself. Doing so can help you cope with the changes cancer brings. Learning about and talking with others who also have a child with cancer may help you and your family cope.']	['<urn:uuid:9bb5149d-0cbe-4ced-bf08-eac70eb4992e>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-13T02:03:54.148118	14	40	1081
25	What colors was Depression Glass commonly available in?	Depression glass came in several colors including clear, pink, pale blue, green and amber. There were also some less common colors like red and black.	"['Well, maybe there is one good thing that came out of The 1930s Depression: Glass. To be more specific, Depression Glass, which is highly collectible and showing signs of increasing in value right now - especially the Green color. It could be that Green Depression Glass reminds people, in a soothing way, of the two current crises we face now: the dreadful economy (similar to the 1930s) and global warming. If people living during the 1930s turmoil could get through it, so can we. Favoring Green just reinforces the idea we\'ve promised to keep cutting down on our energy use. Green is THE color in everything this year - including Depression glass.\nIn case you\'re not familiar, Depression glass is the clear or colored glass that was given away as a premium or ""enticement"" to people during the 1930s. If they bought a box of cereal, a movie ticket or even a gallon of gas, they\'d also get a free piece of glassware. Along with your Quaker Oats, you might find a pink glass saucer inside the box. Giving away Depression glass was a great marketing idea for a time when people weren\'t in the mood (or couldn\'t afford) to go shopping. Depression glass give-aways grew so popular, during the heyday at least 20 U.S. factories were churning out the 100 glass patterns to meet demand. Place settings came in a wide variety of colors - clear, pink, pale blue, green and amber. There were also some less common colors (like red and black,but in all my years of collecting, I\'ve never seen any). People could find their patterns at the 5 & 10-cent store (Woolworths) when they wanted to fill in their settings or replace a piece. We\'re not talking Steuben or Orrefors; Depression glass was inexpensively produced, so it was generally heavier and sometimes a bit flawed often (glass bubble, for instance), unlike the high-end glass from France or Sweden.\nI\'ve been picking up Depression Glass pieces here and there, as I\'ve gone about collecting my other passions. I\'ve always focused on the more Modernistic Depression Glass pieces, which are generally square, not round. I love to pour a cup of coffee in my dark green Modernistic Depression glass cup (no saucers required). I\'ve also managed to acquire Depression Glass cake plates, sherbets, small bowls and various serving dishes in a variety of patterns. For me, it\'s all about the color. One trend in interior design these days, seems to be to mix unmatched glassware with unmatched antique or vintage glass or china (generally in floral patterns in the colors of the Depression Glass). As I mentioned, there are about 100 patterns of Depression Glass; it\'s pretty much your choice whether you want to stick to one pattern (say Old Cafe) or go for colors (blue from Hazel Atlas Co. goes for the big bucks). Of course, just like every other popular collectible known to man and woman, the copiers have been out and about - mainly since the 1960s, when Depression Glass first became so popular to collect. I pretty much know a reproduction when I see one -- first, the color, clarity and weight of any newer fake Depression dish or cup is unlike the original. The copiers, unfortunately, grow better by the day, but if you pick up a clunky piece of so-called Depression Glass and it shows no wear of any kind - beware or be wary.\nI like to find authentic Depression Glass at estate sales where I can see what other types of items are being offered (like those from the 1930-40s) or flea markets. Yesterday I visited an antique mall and saw numerous examples of Depression Glass (mainly pink) in various patterns, all selling for inflated prices. It would be - and is- nice to pick up a piece of Depression Glass at a charity thrift store - generally $2.99 for a plate or $.69-$.99 for a cup - depending on where you live. If you get into this, in a big way, you\'ll want to go to the library or the 2nd hand store and get a Guide to Depression Glass, so you can tell the patterns and their values. Since I\'ve never collected Depression Glass for resale, I don\'t have a Guide. But in the case of someone who wants to collect or re-sell for profit, a Guide is necessary, given the numbers of patterns. The Guides also offer hints on how to tell a reproduction from an original.\nWhat prompted me to think about Green Depression glass was a newsletter mention by Mrs. Kovel (of the famed Kovel Antique Guides), who recently wrote she\'d gone to a swap meet and three different sellers reported selling sets of green drinking glasses that very day. It wasn\'t a far leap to go check out the sales on eBay to discover some Depression Glass selling in the 3 figures (some selling for much less, of course). What caught my eye was a Modernistic green Depression Glass fish bowl (the round vintage variety) that sold for $175. Don\'t suppose there are too many of those out there, but you just never know.\nThe other thought about Depression glass was how few ""enticements"" are given out these days. I\'m not talking Fast Food toys (although there\'s a good market for the very oldest giveaways), but free, practical items that would come in cereal boxes or soap boxes or with gas-station fill-ups (like the 1930s). Okay, I dream. Of course, just when I decided that practical giveaways would be a good idea, I saw a commercial for McD\'s and their current promotional glass giveaway. (At a fast food outlet near you!). If contemporary glass is not your thing, check out Depression Glass. A goblet, found in a thrift store, may cost you less than a big value meal at McDs. And you won\'t feel guilty for indulging in a little fast (fats) food.\nHere\'s a list of some of the more expensive and popular patterns to be looking for: Adam, American Sweetheart, Block Optic,Cherry Blossom, Dogwood,Lace Edge, Manhattan, Miss America, Moderntone, Moondrops, Tea Room, Windsor. Of course, pristine condition and certain sought-after colors will determine the varying values.\nCollecting Depression Glass is not as easy as it sounds or looks, but it\'s certainly fun to send yourself on a treasure hunt. I expect Depression Glass (certain patterns)to go up and up in the near future. Just two days after I predicted a boom in tobacco-product collecting, the U.S. government decided to regulate cigarettes and their advertising. Voila. Instant price increase for some cigarette collectibles. For some of us, it\'s really hard making green backs these days. Scouting around for a little green Depression Glass to buy and sell for profit couldn\'t hurt. And if you can\'t sell it, you can always drink your morning coffee in it.\nWith thanks to Kovels, eBay, internet resources and my library of antique & collectibles reference books.']"	['<urn:uuid:2991ede0-3544-4a88-b13c-b01ff71bb995>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T02:03:54.148118	8	25	1159
26	I'm a college student looking to transfer to a new university. What kinds of support and services do universities typically provide to make campus life accessible for students with different needs and abilities?	Universities typically provide comprehensive support through multiple approaches. They offer dedicated disability services offices for special assistance, but also take a holistic approach to create an inclusive environment. This includes physical accommodations like ramps, automatic doors, and accessible dorm rooms, as well as accessible online resources and websites. Universities train faculty and staff to work with students of different abilities, provide necessary educational equipment, and ensure confidentiality of disability-related information. They also focus on universal design principles in classroom instruction, offering multiple teaching methods to reach all students. Additionally, student services like admissions, registration, tutoring centers, career services, housing, and food services are made accessible to all students. Universities also regularly review their policies and practices to remove barriers and promote inclusivity.	['A document for Supporting Students with Special Needs\nAl-Majmaah University Document to support students with special needs: Believing in its duty of equally serving all the community members from all levels, the University of al-Majmaah strives to give equal opportunities to students with special needs, like their healthy peers, to be able to join its educational and social programs. Hence, providing the necessary qualitative equipments to support students with special needs is regarded as a fundamental pillar in the University planning and decision-making processes. The university also regularly revises its study plans in various colleges to make sure that they are convenient for people with special needs while retaining the quality of the content of the academic and scientific programs. The university potentials and its staff are all devoted to ensure a suitable individual and collective setting for serving students with special needs from the first moment they join the university. Al-Majmaah University Policy to Support students with special needs: The university is seeking to create a suitable and attractive Academic environment for students, especially those with special needs throughout: 1. The collaboration between Faculty members and employees to establish a convenient Academic setting with all its components in order for students to be able to fully perform their roles in the university. 2. Evaluating students on the basis of their capabilities. 3. Boosting the sense of personal responsibility among students with special needs. 4. Providing the appropriate environment for students with special needs in all the university colleges to give them the opportunity to attend the university educational and social programs. 5. Reaching a proper sense of Disability awareness for a comprehensive university community. 6. Special training for the university staff especially those who have direct contact with disabled students. 7. Providing the necessary educational equipment for each case of disability to help the student continue his/her higher education. 8. Training disabled students on how to use the appropriate educational equipments for their academic achievement. 9. Encouraging and motivating colleges, deanships, and departments to support students with special needs through providing the appropriate special services. 10. Ensuring confidentiality of the information obtained about their disabilities, without disclosing them only when required and according to the regulations and instructions approved by the relative authorities. 11. Ensuring that the occupational Safety will be inclusive of the real needs of disabled people. The University Rector, Dr. Khaled bin Saad al-Migrin, relies on the university staff’s dedication to support Disabled people: All the university Staff, from faculty members to employees, are committed to equally treat students with special needs and their colleagues in all the common aspects, to perform their duties without discriminating between the students, to adopt a constructive management system that tolerates the disability resulting behavior and which would affect other students in classrooms; and to create an appropriate setting for disabled students to get closer to them and discuss with them the issues that affect their university studies. The faculty members are also committed to apply a flexible curriculum that satisfy the needs of this group of students while maintaining the regulating academic standards of educational materials entrusted to them, after coordinating with the relative authorities, and to evaluate them on the basis of their educational level and not on the degree of their disability. The vice dean for Educational Affairs and the Vice rectorfor Graduate Studies and Scientific Research. The responsibilities of students with special needs: Collaborating with the disabled students, sharing with them the responsibility to provide their accommodating means, and their keenness to discuss their specific needs (and provide help if necessary) is considered as a fundamental pillar in the provision of individual and collective services at the right time and in the right place; and this means that expressing their opinion about the education materials and assessing the extent to which theses materials satisfy their special needs would be more useful in order to reach an appropriate and comprehensive university environment. Students with special needs can participate in creating such environment through: 1. Adhering to the regulations and laws set by the university. 2. Showing respect and appreciation towards the university staff. 3. Providing the necessary information that will help supply them with the appropriate services at the right time. 4. Providing documents and reports that help determine the disability type from the certified authority. 5. Communicating regularly with their college deanship in order to cooperate in identifying the obstacles and accordingly the appropriate solutions for the difficulties that hinder their education. 6. Ensuring the correct and proper use of the equipment and services allocated to them.\nSpecial Needs Students Agreement', 'Self-Examination: How Inclusive Is Your Campus?\nCollege students are learning to take responsibility for their own lives. They’re making choices that will determine their success in life and careers. For those of us fortunate enough to be part of their education, there are responsibilities as well. We need to be prepared to work with students with very different backgrounds, experiences, first languages, abilities, and disabilities.\nTypically, campuses provide accommodations for students with disabilities through a disability services office where students go to get special assistance. This model tries to fit individuals with disabilities into a system that may not be well‑designed for them. While some accommodations may always be necessary, others are needed simply because a course or resource was created in an inaccessible format.\nA truly inclusive campus takes a more holistic approach. As reported by Beatrice Awoniyi, director and assistant dean for the Student Disability Resource Center at Florida State University, “We all have responsibilities for providing access and a welcoming environment for anyone on our campus. It is not an office responsibility; it is not just the disability services office or the disability services provider or the director or the coordinator that is responsible to making sure that access is provided for students with disabilities, or that their needs are being met. It is a university-wide responsibility.”\nIt takes an entire campus, from administration to architecture to academics, to create a welcoming and inclusive environment. Accessibility issues should be considered at all levels. Explained this way by John Pedraza, specialist at the Resource Center for Persons with Disabilities at Michigan State University, “It’s important that you have a president and a provost and deans that are supportive of accommodating persons with disabilities. And fortunately, I work at an institution that is like that.”\nA proactive approach to designing environments and programs that are accessible to, usable by, and inclusive of everyone is called universal design (UD).\nStudent Services and Physical Spaces\nFrom the very first contact, potential students with and without disabilities need access to the massive amount of information a college or university makes available online. Having a campus web accessibility policy is critical to ensuring consistent, well-organized, and universally accessible electronic resources. Alice Anderson, a technology access program coordinator at the University of Wisconsin—Madison, notes, “Having a campus policy will do many things. It can give you a sense of how to focus your training. You can bring different groups together to discuss the issues and the solutions. A policy can also set a measuring stick for where you are and where you want to be. A policy is especially beneficial when you’re looking at how accessible you want your e-resources to be.”\nOnce on campus, students with disabilities need physical access to classrooms, libraries, dining areas—basically, everywhere a student might go. Plans for access should be included in building and landscape architecture. At North Georgia College and State University, as reported by Rodney Pennamon, “[We have] an ADA committee, an Americans with Disabilities Act committee, that looks at access issues around the campus. We recently have added students to that group of individuals, and so [feedback on access barriers] could come from a student, or oftentimes will come from a faculty member who may have some suggestions or some input…then my office will look at those…access issues.”\nWhen addressing physical access, consider including ramps and doors with automatic openers, adjustable countertops or countertops at various heights in student service areas, materials in accessible formats, and dorm rooms with accessible bathrooms.\nIt is also important to consider accessibility issues when planning events such as career fairs, two‑year and four-year college transfer events, and campus tours. Hold them in accessible locations and announce the availability of accommodations in brochures and on websites.\nCampuses should ensure that facilities, police, public safety, and all other relevant groups work together to ensure the accessibility of all student services, including admissions, registration, tutoring centers, career services, housing and food services, computer labs, and student organizations.\nFor guidelines on making student services accessible to all students, consult the video and publication Equal Access: Universal Design of Student Services. Publications tailored to specific services can be found on our Applications of Universal Design page.\nInstruction is the core of any college or university. By using UD and universal design for learning (UDL), instructors can create accessible spaces and course content, faculty can minimize the need for additional accommodations. As suggested by Rodney Pennemon, “When you’re designing your class and putting it together, you really need to think globally in terms of who’s going to be [your] audience, who do [you] need to reach? [There may be] younger students, non-traditional students, students from different backgrounds and different cultures…and so if you go in with the approach that you’re going to try to reach as many people as possible, it’s similar to giving a lecture: you’re going to have an audience of people that come from a lot of different backgrounds, and so you want to be able to appeal to as many different levels as possible.”\nInstructors are encouraged to use a variety of methods for teaching content and offer alternative ways for participants to participate in class activities. A statement on the course syllabus encouraging students to meet with the instructor to discuss accommodations (in addition to standard wording for requesting specific accommodations through a centralized unit) helps create a supportive atmosphere. As summarized by Meryl Berstein, director at the Center for Academic Support at Johnson and Wales University, “Universal design is really making the classroom accessible for all students, not just for students with disabilities, and it really comes down to just good teaching. By teaching students through all the modalities—by hearing, by seeing, by touching—the information is accessible to all students in different ways.”\nAt the campus center for faculty development, UD and UDL could be integrated into all offerings. For more information on universal design of instruction (which combines UD and UDL principles), consult the video and publication entitled Equal Access: Universal Design of Instruction.\nCampus Accessibility Indicators\nThe goal of systemic change for inclusion of people with disabilities on campus is to ensure that all people experience the campus fully and equally through the same venues. This can be done through ongoing development of an inclusive campus that embraces and celebrates diversity. The following indicators are starting points for conversation about how to create an inclusive campus. Suggestions for improving the list should be sent to firstname.lastname@example.org.\n- Institution-level mission, vision, and values statements are inclusive of all people, including those with disabilities.\n- Disability is included in campus discussions of and training on diversity and special populations.\n- Policies, procedures, and practices are regularly reviewed for barrier removal and inclusivity of people with a diverse range of characteristics, including disability.\n- Administrators, staff, faculty, and student leaders are trained and empowered to take action around universal design and accommodation issues.\n- People with disabilities are visible (even if their disabilities are not) on campus including in positions of power and authority (administrators, faculty, student leaders, etc.).\nInfusion in all aspects of campus:\n- Budgeting reflects the reality of the cost of employing UD and UDL principles and providing accommodations for current and prospective employees, students, and visitors with disabilities.\n- Measures of student success (retention, course completion, graduation, etc.) are the same for all student populations, including students with disabilities, and institutional research includes this data.\n- Campus marketing, publications, and public relations include images and content related to disabilities.\n- Campus publications and websites, including web-based courses, meet established accessibility standards.\n- Disability issues are regularly included as a component of the curriculum.\n- All campus facilities are physically accessible.\nUD is a work in progress. We at DO‑IT have collaborated with a national team of educators to create professional development materials and train faculty and administrators throughout the United States. We offer suggestions for the design of inclusive courses, technology, campus services, and physical spaces. Each UD application is accessible to, usable by, and inclusive of everyone, including those with disabilities.\nVideo to Complement this Publication\nThe content of this publication can be found in the video entitled Self-Examination: How Accessible Is Your Campus? This title, along with many other DO-IT videos, may be freely viewed at www.washington.edu/doit/videos/index.php. DVDs can be purchased from DO-IT at www.washington.edu/doit/do-it-videos-books-and-training-materials-order-form\n- The Board Room\n- The Center for Universal Design in Education\n- The Employment Office\n- The Faculty Room\n- The Student Lounge\n- The Student Services Conference Room\n- The Veterans Center\nWithin these websites are guidelines and information that lead to more accessible courses and programs and that help students with disabilities prepare for success in college.\nFor more information about applications of universal design consult www.washington.edu/doit/resources/popular-resource-collections/applications-universal-design or The Center for Universal Design in Education at www.washington.edu/doit/programs/center-universal-design-education/overview. The book Universal Design in Higher Education: From Principles to Practice published by Harvard Education Press shares perspectives of UD leaders nationwide.\nDO-IT (Disabilities, Opportunities, Internetworking, and Technology) serves to increase the successful participation of individuals with disabilities in challenging academic programs such as those in science, engineering, mathematics, and technology. Primary funding for DO-IT is provided by the National Science Foundation, the State of Washington, and the U.S. Department of Education. DO-IT is a collaboration of UW Information Technology and the Colleges of Engineering and Education at the University of Washington.\nGrants and gifts fund DO-IT publications, videos, and programs to support the academic and career success of people with disabilities. Contribute today by sending a check to DO-IT, Box 354842, University of Washington, Seattle, WA 98195-4842.\nYour gift is tax deductible as specified in IRS regulations. Pursuant to RCW 19.09, the University of Washington is registered as a charitable organization with the Secretary of State, state of Washington. For more information call the Office of the Secretary of State, 1-800-322-4483.\nTo order free publications or newsletters use the DO-IT Publications Order Form; to order videos and training materials use the Videos, Books and Comprehensive Training Materials Order Form.\nFor further information, to be placed on the DO-IT mailing list, request materials in an alternate format, or to make comments or suggestions about DO-IT publications or web pages contact:\nUniversity of Washington\nSeattle, WA 98195-4842\n206-685-DOIT (3648) (voice/TTY)\n888-972-DOIT (3648) (voice/TTY)\n509-328-9331 (voice/TTY) Spokane\nFounder and Director: Sheryl Burgstahler, Ph.D.\nThis publication was developed under a grant from the U.S. Department of Education, OPE #P333A050064. However, the contents do not necessarily represent the policy of the U.S. Department of Education, and you should not assume endorsement by the federal government.\n© 2019, 2015, 2012, 2009, 2007 University of Washington. Permission is granted to copy these materials for educational, noncommercial purposes provided the source is acknowledged.']	['<urn:uuid:a4fc3c6e-db2c-4820-bf89-2adb898ac362>', '<urn:uuid:0e6b5421-5b57-4bdc-a871-fc9146c34bc8>']	open-ended	with-premise	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-13T02:03:54.148118	33	122	2552
27	I'm studying how companies get natural resources - what drives them to keep extracting them, and what damage does this cause to the environment?	Companies and states continue aggressive resource extraction to deliver prosperity to citizens and shareholders, despite growing public concern about resource depletion. This often involves multinational companies encroaching on indigenous and rural lands. The environmental damage from these activities is severe, including erosion, sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals. Additional forest logging often occurs near extraction sites for storing debris. In some cases, like in Africa, when companies don't fulfill their responsibilities, mining areas can become lifeless wastelands, threatening plant species and wildlife.	"['Demand for natural resources such as oil, water, and land remain undiminished at the start of the 21st century despite growing public anxiety about their depletion. As key resources become scarce, new resources come into existence. Across the globe, states and corporations have redoubled efforts to extract conventional and unconventional resources in an attempt to deliver ongoing prosperity to citizens and shareholders. The contradictions and violence of these endeavors are most apparent in state-sanctioned encroachment of multinational companies on indigenous and other rural lands.\nThe resurgence of anthropological research on natural resources, a field with a long and continuous trajectory, stems from the recognition of these dilemmas and their growing impact on the peoples and places anthropologists study. Until the past decade and a half, anthropological studies tended to focus—with some noteable exceptions—on agriculture, hunting, fishing, foraging, and similar activities involving the exploitation of so-called renewable resources. However, more and more anthropologists have turned their attention to the study of natural resources per se. They have produced studies of water, sapphires, gold, oil, coltan, forests, and biodiversity (Acheson 2006, Behrends et al. 2011, Mantz 2008, Orlove and Caton 2010, Whiteford and Whiteford 2005), of specific extractive regions such as Australia and Papua New Guinea (Rumsey and Weiner 2004), of modes of engagement with resources such as [End Page 5] extraction and conservation (Ballard and Banks 2003, Carrier and West 2009), and of conceptualizations of specific resource processes such as adaptation and commoditization. The common refrain of these studies is that while natural resource exploitation continues to play a critical part in shaping the human condition, it does not do so in a uniform or environmentally deterministic manner. However, with the exception of Ferry and Limbert’s (2008a) stimulating edited volume, little effort has been made to examine resources as a theoretical and comparative problem in a way that would conceive of their “resourceness” as going beyond their status as particular kinds of commodities.\nThis special collection explores questions in the anthropology of natural resources that have thus far remained implicit, including questions about resources’ specific characteristics and capacities, the processes through which they come into being, and how such processes of resource making can be studied ethnographically. We suggest that placing these kinds of questions—questions of an ontological bent—at the center of inquiry can enhance the possibilities for a comparative ethnographic analysis. They also help interrogate the logics that perpetuate natural resource exploitation and specify an anthropological intervention in cross-disciplinary debates. Our questions arise out of broader intellectual trends in the social sciences and philosophy to which anthropologists have contributed that probe the legacies of modernist divisions between human and nonhuman, the social and the material, and what is active and what is acted upon in the environment. Terms such as “socionature” (Swyngedouw 1999), “natureculture” (Haraway 1997, Latour 1993), “nature regimes” (Escobar 1999), or “second nature” (Biersack 2006) have been developed to convey the sense that nature “is humanly produced (through conceptualization as well as activity) and that [it] therefore partakes, but without being entirely, of the human” (2006:14).\nNatural resource exploitation—as a sustained project of abstracting substances identified as useful, valuable, and natural in origin from their environment—has long played a central role in that continuing human effort to become “modern.” It is a process of boundary making par excellence—of distinguishing subject from object, nature from culture, and science from politics (Latour 1993). The intellectual agenda of scholars mentioned above, sometimes dubbed “posthumanist” or “new materialist,” has been partly driven by ethical concerns about climate change and ecological disasters, and the exploitation of resource environments. [End Page 6] One of anthropology’s key contributions to these discussions stems from research about the differences in how people relate to their surroundings and about worlds premised on principles other than modernist ones (Descola and Pálsson 1996, Ingold 2000, Strathern 1980). Curiously, geographers have been quicker than anthropologists to import concepts of nonhuman or material agency, which have also emerged from anthropological work, back into the study of resources as such (Bakker and Bridge 2006, Bridge 2009, Kaup 2008). In this special collection, we draw...', ""Mining has several bad effects. It leaves behind a huge hole after mining is done. Secondly it damages natural beauty. A beautiful landscape which once existed is now a huge piece of dug up earth.\nEnvironmental Effects. Environmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil.\nThe effects of mining in Africa have left large-scale devastation when companies do not honour their responsibility. Because mining areas are left in an unsustainable condition, plant species and wildlife are threatened and these areas are at risk of becoming lifeless wastelands.\nThe Impact and Effect of Illegal Mining (galamsey) towards the Socio-economic Development of Mining Communities: A Case Study of Kenyasi in the Brong Ahafo Region Adjei Samuel1, N.K.Oladejo1, I.A. Adetunde2, * 1University for Development Studies, Department of Mathematics, Navrongo. Ghana.\nSome of the major effects of mining on the environment are as follows: Minerals are the natural resources which play an important role in the economic development of the country. But the extraction and mining of these natural resources leads to some adverse effect on our environment as well.\nMar 09, 2017· The mining industry has the potential to disrupt ecosystems and wipe out wildlife populations in several different ways. Here's how mining affects the environment and wildlife. Habitat Loss; Mining can lead to the destruction of habitats in surrounding areas. The …\nModern mining is an industry that involves the exploration for and removal of minerals from the earth, economically and with minimum damage to the environment. Mining is important because minerals are major sources of energy as well as materials such as fertilizers and steel.\nApr 25, 2017· Mining is the extraction of minerals and other geological materials of economic value from deposits on the earth. Mining has the potential to have severely adverse effects on the environment including loss of biodiversity, erosion, contamination of surface water, ground water, and soil.\nSome gold can be found by panning in rivers; heavy gold will remain in the pan, whereas lighter rocks and minerals float out. This small-scale form of gold mining has little effect on the body of water, but the large-scale practice of mining gold from ore can have tremendous negative effects on water quality.\nMining can effect the earth because first, deforestation, and because mining requires large portions of land to be removed before they can start mining, lots of trees and plants are removed.\n1.1 PHASES OF A MINING PROJECT There are different phases of a mining project, beginning with mineral ore exploration and ending with the post-closure period. What follows are the typical phases of a proposed mining project. Each phase of mining is associated with different sets of environmental impacts. 1.1.1 Exploration\nFeb 07, 2018· The effects in such cases can be devastating for the environment. Be it due to ignorance of the regulations or just a freak accident, incidents like the Guyana spill of 1995 may occur again. This highlights the fact that issues like mining's effect on the environment are worth some serious deliberation.\nAug 26, 2010· Dust, radon and mercury impact miners' health. Dust, radon and mercury impact miners' health. ... Miners Face Health Risks, Even on Good Days ... mining …\nThe effects of mining coal on the environment. There are 2 ways to mine coal – Strip Mining and Underground Mining – both ways have their own impact to the environment and health. We know it but coal is such a cheap energy source that we don't want to let go of it. The negative effects of coal mining cannot be disputed:\nApr 21, 2019· The human health effects due to cyanide leach gold mining are not well documented, and this is no exception in Montana. The State of Montana has done no formal studies to specifically study mine-related health effects. Pegasus, the last mining company at Zortman-Landusky, started to fund a health study with the $1.7 million supplemental money from the 1996 settlement, but because …\nADVERTISEMENTS: Some of the major environmental effects of mining and processing of mineral resources are as follows: 1. Pollution 2. Destruction of Land 3. Subsidence 4. Noise 5. Energy 6. Impact on the Biological Environment 7. Long-term Supplies of Mineral Resources. Mining and processing of mineral resources normally have a considerable impact on land, water, […]\npositive and negative effects of mining on the environment. Mankind has been mining for precious metals since 42000 years ago and that's a staggeringly long time ago and that's exactly how long our species has been digging into the ground, to harvest its precious metals.\nDownload Coal Mining sounds ... 76 stock sound clips starting at $2. Download and buy high quality Coal Mining sound effects. BROWSE NOW >>>\nMining affects the environment by exposing radioactive elements, removing topsoil, increasing the risk of contamination of nearby ground and surface water sources, and acidification of …\nApr 20, 2015· Effects of Mining. Coal mining, the first step in the dirty lifecycle of coal, causes deforestation and releases toxic amounts of minerals and heavy metals into the soil and water. The effects of mining coal persists for years after coal is removed.\nJul 25, 2018· Environmental impacts from fossil fuel pollution are rapidly increasing in regions that have the highest concentrations of fuels. There are multiple effects of mining fossil fuels. Drilling and mining practices take a substantial toll on local water sources, biologic life and natural resources.\nPublished by the American Geosciences Institute Environmental Awareness Series. ... How can metal mining impact the environment? PDF version. Material adapted from: Hudson, T.L, Fox, F.D., and Plumlee, G.S. 1999. Metal Mining and the Environment, p. 7,20-27,31-35,38-39. Published by the American Geosciences Institute Environmental Awareness Series.\nMining operations usually create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact. Work safety has long been a concern as well, and …\nEffects of mining on aquatic resources are both physical and chemical in nature. Most of earthmoving activities of mining occurred well before the enactment of laws designed to protect aquatic resources - particularly the 1977 Federal Water Pollution Control Act.\nThe former is known as underground mining, the latter as strip mining or mountaintop removal. Either process contributes a high level of damage to the environment: #12 Noise pollution. One of the most obvious (albeit perhaps least harmful) environmental effects of coal mining is noise pollution.\nMining has an adverse effect on soil quality. Soil degradation is the prime impact. Another impact is deforestation and loss of fauna and flora.\nThe impact of mining on the environment and the effects of mining techniques need to be more advanced with the utilization of modern equipment to be unintrusive to the environment. Economic growth is high on the agenda of leading countries, sustaining …\nMining is an inherently invasive process that can cause damage to a landscape in an area much larger than the mining site itself. The effects of this damage can continue years after a mine has shut down, including the addition to greenhouse gasses, death of flora and fauna, and erosion of land and habitat.\nNov 14, 2016· After mining is over, the land is left as barren land. The effects of mining sometimes vary depending on what is mined out, but these are some of the general effects you will see in all mine-areas. I'm not an expert when it comes to health impact on miners, but here are some of the things I know will affect them-\nJul 08, 2017· In coal mining, the extraction, crushing, and transport of coal can generate significant amounts of airborne respirable (extremely fine) coal dust. Dust less than 10 microns in size (cannot be seen with the eye). In non-coal mining, stone, and san...\nEnvironmental impacts of mining can occur at local, regional, and global scales through direct and indirect mining practices. Impacts can result in erosion, sinkholes, loss of biodiversity, or the contamination of soil, groundwater, and surface water by the chemicals emitted from mining processes. These processes also have an impact on the atmosphere from the emissions of carbon which have ...\nApr 04, 2017· The Dangerous Effects of Illegal Mining. April 4, 2017 Environmental Issues Written by Greentumble. Illegal mining has been ravaging our planet for. decades. Not only is illegal mining riskier from a safety perspective for those who choose to participate, but it encourages reckless behavior and leads to outcomes that have negative long-term ...""]"	['<urn:uuid:339b376b-01fb-42ed-a8c0-652a3b23a417>', '<urn:uuid:11ce18f5-51f7-4dfa-a26b-c4a5796e1622>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T02:03:54.148118	24	90	2137
28	scamp scoring system minimum score maximum score range points allowed	The SCAMP scoring system ranges from 5 to 20 points in total, with scores being calculated by adding up numbers from each section. Half points (like 3.5) are allowed, but no other decimals are permitted.	"['SCAMP stands for ""Sanity Checking Any Maintainer Proposal"". It\'s a simple algorithm for generating a somewhat reproducible score for any proposal, based on Kahneman\'s idea that algorithms are more noise-free than people\'s subjective judgements, and often perform better in low-validity situations.\nSCAMP is not the final word on the proposal. It need not even be important - after scoring a proposal, you\'re free to discard the score and then vote whichever way you want. Its most important goal is to make you think harder about each proposal.\nThis score represents how easy it is for anyone to agree after-the-fact whether someone succeeded in their proposal, or failed.\n- Impossible to measure: there\'s no way to say if the proposal succeeded.\n- Difficult to measure: it might be possible to measure this, but < 50% people agree on how, or it\'ll take > 2 days\n- Possible to measure: > 50% people can agree whether the proposal succeeded or failed if they spend < 2 days trying\n- Easy to measure: > 75% people can agree whether the proposal succeeded or failed if they spend < 1 hour trying\nThis score represents how carefully the proposal considers possibilities for failure, and how to mitigate them.\n- Blind to failure: almost no discussion about why the proposal could fail (or has failed in the past)\n- Admits possibility of failure: mentions some failure modes (or past failures), but says almost nothing to address them\n- Seriously considers failure: devotes < 25% of the proposal to giving a clear picture of how the proposal could fail (or has failed in the past)\n- Nate Silver: devotes >25% of the proposal to explaining how the proposal has failed in the past, and admits a significant (> 25%) possibility of failure this time, despite best efforts.\nAssuming this proposal succeeded, this score represents how much you agree with the objectives of the proposal, and the reasoning behind it.\n- Actively harmful: if the proposal worked, it would have been better to do nothing at all\n- Unaligned: if the proposal worked, it would make so little positive impact that just spending that time doing the current highest impact activity (eg. writing wiki articles, contributing to metakgp github projects) is better\n- Impactful: if the proposal worked, it would have enough impact that it\'s worth doing this instead of the current highest impact activity.\n- Awesome: if the proposal worked, it would have at least as much impact as the current highest impact project (eg. wiki, mftp, mfqp, naarad, or whatever you think is highest impact).\nThis represents how well defined and well reasoned is the proposal.\n- All air: The goal is very vague, the steps are unclear and there is no time line. Basically the proposal doesn\'t tell what you\'ll do, how will you do it and when will you do it.\n- Largely vague: The goal is clear but step to take and time line are vague.\n- Sketchy: The goal is clear, the steps to take and time line is broadly defined.\n- Rock solid: Everything, the goal, the step and the time line is very well defined.\nThis score represents what you personally, subjectively, feel about the proposal, irrespective of any objective criterion (including the ones above).\n- Don\'t like it\n- It\'s okay\n- Like it\n- Love it\nHow to score[edit source]\nPick a single number from each of the sections. Half points (eg. 3.5) are okay, but no other decimals allowed. Add the score from each of the sections. The final score can be from 5 to 20.']"	['<urn:uuid:49bd89c9-1762-4e06-9265-0f8e2674874f>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-13T02:03:54.148118	10	35	600
29	f4 phantom speed record internal blowing effect	The F-4 Phantom II set a world speed record of 1,604 mph in 1961 on a 15-mile circuit and used internal blown flaps. Internal blown flaps improved the aircraft's low-speed characteristics by increasing lift 2-3 times compared to conventional flaps, but did not affect its maximum speed capability which came from its two powerful 17,900-pound-thrust turbojets.	"['Blown flaps, or jet flaps, are powered aerodynamic high-lift devices used on the wings of certain aircraft to improve their low-speed flight characteristics. They use air blown through nozzles to shape the airflow over the rear edge of the wing, directing the flow downward to increase the lift coefficient. There are a variety of methods to achieve this airflow, most of which use jet exhaust or high-pressure air bled off of a jet engine\'s compressor and then redirected to follow the line of trailing-edge flaps.\nBlown flaps may refer specifically to those systems that use internal ductwork within the wing to direct the airflow, or more broadly to systems like upper surface blowing or nozzle systems on conventional underwing engine that direct air through the flaps. Blown flaps are one solution among a broader category known as powered lift, which also includes various boundary layer control systems, systems using directed prop wash, and circulation control wings.\nInternal blown flaps were used on some land and carrier-based fast jets in the 1960s, including the Lockheed F-104, Blackburn Buccaneer and certain versions of the Mikoyan-Gurevich MiG-21. They generally fell from favour because they imposed a significant maintenance overhead in keeping the ductwork clean and various valve systems working properly, along with the disadvantage that an engine failure reduced lift in precisely the situation where it is most desired. The concept reappeared in the form of upper and lower blowing in several transport aircraft, both turboprop and turbofan.\nIn a conventional blown flap, a small amount of the compressed air produced by the jet engine is ""bled"" off at the compressor stage and piped to channels running along the rear of the wing. There, it is forced through slots in the wing flaps of the aircraft when the flaps reach certain angles. Injecting high energy air into the boundary layer produces an increase in the stalling angle of attack and maximum lift coefficient by delaying boundary layer separation from the airfoil. Boundary layer control by mass injecting (blowing) prevents boundary layer separation by supplying additional energy to the particles of fluid which are being retarded in the boundary layer. Therefore, injecting a high velocity air mass into the air stream essentially tangent to the wall surface of the airfoil reverses the boundary layer friction deceleration thus the boundary layer separation is delayed.\nThe lift of a wing can be greatly increased with blowing flow control. With mechanical slots the natural boundary layer limits the boundary layer control pressure to the freestream total head. Blowing with a small proportion of engine airflow (internal blown flap) increases the lift. Using much higher quantities of gas from the engine exhaust, which increases the effective chord of the flap (the jet flap), produces supercirculation, or forced circulation up to the theoretical potential flow maximum. Surpassing this limit requires the addition of direct thrust.\nDevelopment of the general concept continued at NASA in the 1950s and 60s, leading to simplified systems with similar performance. The externally blown flap arranges the engine to blow across the flaps at the rear of the wing. Some of the jet exhaust is deflected downward directly by the flap, while additional air travels through the slots in the flap and follows the outer edge due to the Coandă effect. The similar upper-surface blowing system arranges the engines over the wing and relies completely on the Coandă effect to redirect the airflow. Although not as effective as direct blowing, these ""powered lift"" systems are nevertheless quite powerful and much simpler to build and maintain.\nA more recent and promising blow-type flow control concept is the counter-flow fluid injection which is able to exert high-authority control to global flows using low energy modifications to key flow regions. In this case the air blow slit is located at the pressure side near the leading edge stagnation point location and the control air-flow is directed tangentially to the surface but with a forward direction. During the operation of such a flow control system two different effects are present. One effect, boundary layer enhancement, is caused by the increased turbulence levels away from the wall region thus transporting higher-energy outer flow into the wall region. In addition to that another effect, the virtual shaping effect, is utilized to aerodynamically thicken the airfoil at high angles of attack. Both these effects help to delay or eliminate flow separation.\nIn general, blown flaps can improve the lift of a wing by two to three times. Whereas a complex triple-slotted flap system on a Boeing 747 produces a coefficient of lift of about 2.45, external blowing (upper surface blowing on a YC-14) improves this to about 7, and internal blowing (jet flap on Hunting H.126) to 9.\nWilliams states some flap blowing tests were done at the R.A.E. before the Second World War and that extensive tests were done during the war in Germany including flight tests with Arado 232, Do-24 and Bf 109 aircraft. Lachmann states the Arado and Dornier aircraft used an ejector-driven single flow of air which was sucked over part of the trailing edge span and blown over the remainder. The ejector was chemically powered using high pressure vapour. The Bf 109 used engine-driven blowers for flap blowing.\nRebuffet and Poisson-Quinton describe tests in France at O.N.E.R.A. after the war with combined sucking at le of first flap section and blowing at second flap section using a jet engine compressor bleed ejector to give both sucking and blowing. Flight testing was done on a Breguet Vultur aircraft.\nTests were also done at Westland Aircraft by W.H. Paine after the war with reports dated 1950 and 1951.\nIn the United States a Grumman F9F Panther was modified with flap blowing based on work done by John Attinello in 1951. Engine compressor bleed was used. The system was known as ""Supercirculation Boundary Layer Control"" or BLC for short.\nBetween 1951 and 1955 Cessna did flap blowing tests on Cessna 309 and 319 aircraft using the Arado system.\nDuring the 1950s and 60s, fighter aircraft generally evolved towards smaller wings in order to reduce drag at high speeds. Compared to the fighters of a generation earlier, they had wing loadings about four times as high; for instance the Supermarine Spitfire had a wing loading of 24 lb/ft2 (117 kg/m2) and the Messerschmitt Bf 109 had the ""very high"" loading of 30 lb/ft2 (146 kg/m2), whereas the 1950s-era F-104 Starfighter had 111 lb/ft2 (542 kg/m2).\nOne serious downside to these higher wing loadings is at low speed, when there isn\'t enough wing left to provide lift to keep the plane flying. Even huge flaps could not offset this to any large degree, and as a result many aircraft landed at fairly high speeds, and were noted for accidents as a result.\nThe major reason flaps were not effective is that the airflow over the wing could only be ""bent so much"" before it stopped following the wing profile, a condition known as flow separation. There is a limit to how much air the flaps can deflect overall. There are ways to improve this, through better flap design; modern airliners use complex multi-part flaps for instance. However, large flaps tend to add considerable complexity, and take up room on the outside of the wing, which makes them unsuitable for use on a fighter.\nThe principle of the jet flap, a type of internally blown flap, was proposed and patented in 1952 by the British National Gas Turbine Establishment (NGTE) and thereafter investigated by the NGTE and the Royal Aircraft Establishment. The concept was first tested at full-scale on the experimental Hunting H.126. It reduced the stall speed to only 32 mph (51 km/h), a number most light aircraft cannot match. The jet flap used a large percentage of the engine exhaust, rather than compressor bleed air, for blowing.\nOne of the first production aircraft with blown flaps was the Lockheed F-104 Starfighter which entered service in January 1958. After prolonged development problems, the BLCS proved to be enormously useful in compensating for the Starfighter\'s tiny wing surface. The Lockheed T2V SeaStar, with blown flaps, had entered service in May 1957 but was to have persistent maintenance problems with the BLCS which led to its early retirement. In June 1958 the Supermarine Scimitar with blown flaps entered service. Blown flaps were used on the North American Aviation A-5 Vigilante, the Vought F-8 Crusader variants E(FN) and J, the McDonnell Douglas F-4 Phantom II and the Blackburn Buccaneer. The Mikoyan-Gurevich MiG-21 and Mikoyan-Gurevich MiG-23 had blown flaps. Petrov states long-term operation of these aircraft showed high reliability of the BLC systems. The TSR-2, which was cancelled before it entered service, had full-span blown flaps.\nStarting in the 1970s the lessons of air combat over Vietnam changed thinking considerably. Instead of aircraft designed for outright speed, general maneuverability and load capacity became more important in most designs. The result is an evolution back to larger planforms to provide more lift. For instance the F-16 has a wing loading of 78.5 lb/ft2 (383 kg/m2), and uses leading edge extensions to provide considerably more lift at higher angles of attack, including approach and landing. Some later combat aircraft achieved the required low-speed characteristics using swing-wings. Internal flap blowing is still used to supplement externally blown flaps on the Shin Meiwa US-1A.\nSome aircraft currently (2015) in service that require a STOL performance use external flap blowing and, in some cases, also use internal flap blowing on flaps as well as on control surfaces such as the rudder to ensure adequate control and stability at low speeds. External blowing concepts are known as the ""externally blown flap"" (used on the C-17 Globemaster ), ""upper surface blowing"" (used on the An-72 and An-74) and ""vectored slipstream"", or ""over the wing blowing"", used on the An-70 and the Shin Meiwa US-1A and ShinMaywa US-2.\nPowered high-lift systems, such as externally blown flaps, are not used for civil transport aircraft for reasons given by Reckzeh, which include complexity, weight, cost, sufficient existing runway lengths and certification rules.\n- Aerodynamics for Engineering Students, E.L. Houghton & P.W. Carpenter, Elsevier\n- o. Smith, A. M. (1975). ""High-Lift Aerodynamics"". Journal of Aircraft. 12 (6): 508. doi:10.2514/3.59830.\n- http://naca.central.cranfield.ac.uk/reports/arc/rm/3304.pdf p.1\n- http://cafefoundation.org/v2/pdf_tech/Drag.Reduction/NASA.Synergistic.Airframe.1998.pdf p.22\n- Control of High-Reynolds-Number Turbulent Boundary Layer Separation Using Counter-Flow Fluid Injection, B.E. Wake, G. Tillman, S.S. Ochs, J.S. Kearney, 3rd AIAA Flow Control Conference, 2006\n- ""Aerodynamic issues in the Design of High-Lift Systems for Transport Aircraft"" Figure 1. Trends in Boeing Transport High Lift System Development, Agard CP-365\n- http://cafefoundation.org/v2/pdf_tech/Drag.Reduction/NASA.Synergistic.Airframe.1998.pdf p.18\n- ""1954 | 3066 | Flight Archive"".\n- Rebuffet, Pierre; Poisson-Quinton, P. H. (April 1952). ""Investigations of the boundary-layer control on a full scale swept wing with air bled off from the turbojet"" (PDF). Cite journal requires\n- Schmitt, H. (July 1985). ""Discussion of the paper, Some Aspects of Propulsion for the Augmenter-Wing Concept, by D. C. Whittley"" (PDF). Cite journal requires\n- ""U.S. Naval Air Superiority Development of Shipborne Jet Fighters 1943-1962"" Tommy H. Thomason, Midland Publishing, Hincklet 2007, ISBN 978-1-58007-110-9, page 81\n- ""Cessna Wings for the World, the Single-Engine Development Story"" by William D. Thompson, 1991\n- Flight International 1963 p454\n- ""United States Army and Air Force Fighter 1916-1961"" produced by D.A. Russell, Harleyford Publications Limited, Letchworth 1961, Library of Congress Card No.61-16739(United States) page 132\n- American Military Training Aircraft\' E.R. Johnson and Lloyd S. Jones, McFarland & Co. Inc. Publishers, Jefferson, North Carolina\n- ""British Naval Aircraft Since 1912"" Owen Thetford, Putnam & Co. Ltd.London, 1962, p.318\n- ICAS Archive\n- ""TSR2 with Hindsight"" edited by Air Vice-Marshal A F C Hunter CBE AFC DL, Royal Air Force Historical Society 1998, ISBN 0-9519824 8 6, page 181\n- ""Aerodynamiic Design of Airbus High-Lift Wings in a Multidisciplinary Environment"" Daniel Reckzeh, European Congress on Computational Methods in Applied Sciences and Engineering ECCOMAS 2004\n- ""Blow, blow thou BLC wind"", Flight International, 1971', ""Price excl. VAT\nAvailable as instant download\n- Article number: AS13614\n- Publisher: Milviz\n- Language: English\n- Current version: A1.170721\nThe McDonnell two-place, twinjet, all-weather F-4 Phantom II, with top speeds more than twice that of sound, was one of the most versatile fighters ever built. The F-4 established 16 speed, altitude and time-to-climb records.\nIn 1959, its prototype set the world altitude record at 98,556 feet. In 1961, an F-4 set the world speed record at 1,604 mph on a 15-mile circuit. F-4s saw combat in both the Vietnam War and Operation Desert Storm and served with the air forces of 11 countries in addition to the United States. Both U.S. military flight demonstration teams, the Navy Blue Angels and the Air Force Thunderbirds, flew the Phantom II from 1969 to 1973.\nMax Speed: Mach 2.23 (1,472 mph, 2,370 km/h)\nService Ceiling: 60,000 ft (18,300 m)\nRate of Climb: 41,300 ft/min (210 m/s)\nPower Plant: Two 17,900-pound-thrust General Electric J79-GE-17A turbojets\nArmament: 15,983 pounds of weapons\nA Definitive Simulation\nUtilizing every resource available to us and spending countless hours in design, testing, revising, and more testing again, we're confident that our model represents an extremely true-to-life rendition of the real world aircraft. We've taken great care in presenting an authentic environment for the virtual pilot; closely reproducing the original in looks, systems, sounds and flight modelling.\nAttention To Detail\nThe cockpit of an F-4 represents a busy, complex workplace for the pilot. However, we've strived to perfect the feel of actually sitting in the pilot’s seat, being able to scan your vital instrumentation with a glance, all replicated with high resolution textures and smooth 3D animation. Being able to run through checklists, flipping all of the needed switches and toggles, monitoring systems as they spring to life. Being able to light the engines and feel the rumble of the massive turbojets behind you. Every bit of it, in your control and command!\nChoose Your Loadout\nWe've included a very large selection of authentic weapons and pods to outfit your F-4 with. Our configuration menu lets you setup and configure your payload any way you wish, allowing you to configure for almost any mission. And of course, adding or removing ordinance or fuel will have an appropriate effect on flight dynamics and handling characteristics.\nVariety And Possibility\nSeeing as the F-4E was the most numerous Phantom variant, with some 1,370 built, as well as the most widely exported Phantom, having seen service with 9 countries outside of the US, there is no shortage of interesting liveries to consider. Our package ships with numerous high resolution liveries, and we also have made available a detailed paint kit for users to create their own!\n- Realistic systems and avionics\n- Realistic startup and shutdowns\n- Fully functional ACM (loadout menu)\n- Large selection of weapons and pods\n- Flight dynamics created by Bernt Stolle\n- Realistic DX10/DX11 night lighting, landing lights and custom effects\n- Includes numerous highly detailed liveries\n- High quality sound set recorded from a real F-4\n- High quality external model using normal and specular maps\n- High quality internal model complete with custom 3D gauges\n- Includes external F-4J/S model (not carrier capable, no ILS)\n- High resolution paint kit available\n- Highly detailed product manuals included\n- Flap and Gear indicators\n- Slimelight on wing for J version\n- Radar scope reflection rotated 90* CCW\nSystem Requirements (Minimum):\nMicrosoft Flight Simulator X (Acceleration), FSX: Steam Edition or Lockheed Martin - Prepar3D V3/V4\nWindows Vista, Windows 7 (use in all other versions of Windows is unsupported)\nProcessor: 2.6 Ghz or higher\nMemory: 4 GB RAM\nGraphics: DirectX®9 compliant video card or greater, 1024 MB video RAM or higher\nHard Drive: 2 GB available space\n.pdf Manual (English): >> View here <<\nFree Livery Set : >> Click here <<""]"	['<urn:uuid:2a540bf1-6474-4fc1-827a-cc84892dfa3d>', '<urn:uuid:9b5e173d-a48b-4a92-af91-45043dc0c8f9>']	factoid	direct	short-search-query	similar-to-document	comparison	novice	2025-05-13T02:03:54.148118	7	56	2631
30	Which illness kills faster, cholera or Salmonella?	Cholera can kill infected people within hours if not treated promptly, while Salmonella infections typically last 4-7 days and most people recover without antibiotics. Cholera causes severe dehydration through violent diarrhea that can quickly lead to death if untreated, whereas Salmonella usually causes milder symptoms of diarrhea, fever, and stomach cramps that resolve on their own.	"['Cholera is an infection that is caused by a bacterium known as Vibrio cholerae, which attacks the small intestines. This disease is extremely contagious, especially in areas where there is poor sanitation or during disasters such as floods. It is capable of killing people who are infected within hours in severe cases if it is not treated promptly.\nAccording to research statistics, an approximate of a million to four million cases of cholera infections are reported yearly. The annual number of deaths due to cholera can range from twenty thousand to an eighth of a million.\nCholera affects adults and children, as well as men and women alike. Therefore, cholera is a disease that can affect anyone. However, there are people who are more prone to getting it than others. There are places where cholera is endemic. In other areas, it breaks out as an epidemic.\n""Endemic"" means that a particular disease is native to the place. There have been several cholera outbreaks in the past that have caused deaths in thousands or even in millions. Cholera epidemics usually arise in areas with poor sanitation and scarce resources like water and toilets.\nThe areas that have been affected by wars, natural disasters, poverty, or any other factors that force people to overcrowd are the most prone to massive outbreaks of cholera. The reason is that such places will most likely have poor sanitation and means of waste disposal more so feces. The bacteria that cause cholera will thrive under such conditions.\nCholera is mainly transmitted through contaminated food and water. Flies are the main carriers of the bacteria to the food. When they land on the food, they contaminate it, and whoever eats the food, also gets infected. Eating an undercooked food will also make people get infected if the bacteria were present in the meal.\nContaminated water also helps in spreading the disease. Basically, human feces are the carriers of Vibrio cholerae. When clean water gets into contact with the bacteria, it becomes contaminated. Anyone who drinks such water or uses it to cook food gets infected.\nWho is more prone to the infection?\nCholera is a highly communicable disease. Aside from living in areas where there are epidemics or factors that favor its spread, the following people are more likely to be infected:\nKids like playing a lot. They can carry out their games near sewers, pits, around latrines, and at times, as they play with water, their games can be held in ponds. All these are the places that are prone to have the bacteria that cause cholera.\nTherefore, children get exposed to factors that aid in the spread of the disease more than adults. When they come into contact with the contaminated water, the likelihood that they will get cholera is very high. The reason is that some will suck their fingers afterward or eat food without washing their hands. Through unhygienic practices, they will introduce the bacteria into their bodies.\n2. Careless Doctors and Nurses\nCareless doctors and nurses who handle cholera patients are also likely to get the disease. The reason is that some of the healthcare professionals do not observe maximum caution when dealing with their patients, e.g., not wearing gloves. Eating food without washing their hands after handling the patients also heightens their risk of getting infected.\n3. Family Members and Friends of Victims\nWhen people get sick, the closest around them are the ones who take care of them. Such people may include friends or family, which makes them prone to getting infected, too.\nParts of the World that Are Most Affected by Cholera\nThe parts of the world that are most affected by cholera are:\n- South-East Asia\n- Some parts of Africa\n- Central America\n- Central Mexico\nThese areas are heavily populated and the levels of sanitation are extremely low. These areas are able to provide a suitable place for the bacteria to exist and multiply.\nHow does cholera cause death?\nWhen the bacteria get inside your body, they cause diarrhea that can be very violent in some cases. Severe diarrhea results to dehydration of the body. In severe cases, the blood pressure drops and infected people go into a state of shock. If they do not receive medical attention quickly, death occurs within hours.\nWhat are the symptoms of cholera?\nPeople with cholera show symptoms differently. Some may experience symptoms within a few hours after infection, while in others, symptoms may begin to show after several days. The symptoms of cholera may include:\n- very watery diarrhea (main symptom)\n- muscle cramps can also occur at times\n- diarrhea that usually leads to an excessive loss of water from the body (dehydration)\nDehydration can cause the following conditions:\n- development of wrinkles on the hands and feet\n- the skin becoming less elastic\n- sunken eyes\nPeople who experience severe dehydration will require an immediate correction of the fluid imbalance in their body. Therefore, they have to be hydrated in order to prevent any further damage or complications.\nTips on How to Minimize the Risks of Getting Cholera\nOne of the ways used to control cholera is the use of a vaccine. However, vaccination is not a guarantee, as the vaccine does not usually last for long. That is why it becomes necessary to avoid getting infected with cholera. There are a number of things that you can do to lower your chances of being infected by the disease. Listed below are tips on how to prevent a cholera infection:\n- Parents should always be aware of their kids’ activities. They should keep track of where they spend their time and make sure that it is not in areas where they are likely to pick up the bacteria. The levels of hygiene should also be very high. They should make sure that the kids wash their hands before they eat and after they use the toilet.\n- Implementing high standards of sanitation and hygiene also make you less likely to get infected.\n- Doctors, nurses, or any other medical practitioners who handle patients with cholera should also exercise caution by using protective gears such as gloves. They should also observe high levels of sanitation.\n- Any friends or relatives around a person with cholera should also be careful in how they carry out themselves, especially on hygiene issues.\nThe Bottom Line\nCholera is a water-borne disease. Since this bacteria will more likely be transmitted through water, it is very important to be careful when using water in areas that are prone to the disease. Drinking water should be well-boiled and food should also be cooked properly. These precautions will help ensure that the bacteria are killed by the heat.\nThe main issue is for hygiene to be exercised at the highest of levels. If it is possible, avoid areas that prone to the disease. If you cannot avoid these places, then following the above-given tips should allow you a better chance at avoiding the disease.\nAnyone can get cholera and children are the ones that are most likely to get infected. Thus, keep an eye on your kids and should they show any symptoms, visit a medical facility as quickly as possible as they can get worse quickly and even die.\n- Who Gets Cholera?\n- Cholera Risk Factors\n- Prevention of Cholera', 'Questions and Answers\nWhat are Salmonella?\nSalmonella are bacteria that make people sick. They were first discovered by an American scientist named Dr. Daniel E. Salmon in 1885.\nWhat illness do people get from Salmonella infection?\nMost types of Salmonella cause an illness called salmonellosis, which is the focus of this website. Some other types of Salmonella cause typhoid fever or paratyphoid fever.\nWhat are the symptoms of infection?\nMost people with Salmonella infection have diarrhea, fever, and stomach cramps.\nSymptoms usually begin six hours to six days after infection and last four to seven days. However, some people do not develop symptoms for several weeks after infection and others experience symptoms for several weeks.\nSalmonella strains sometimes cause infection in urine, blood, bones, joints, or the nervous system (spinal fluid and brain), and can cause severe disease.\nScientists classify Salmonella into serotypes (types) by identifying structures on the bacteria’s surfaces. Although more than 2,500 serotypes have been described, fewer than 100 are known to cause human infections.\nHow is Salmonella infection diagnosed?\nSalmonella infection is diagnosed when a laboratory test detects Salmonella bacteria in a person’s stool (poop), body tissue, or fluids.\nHow is infection treated?\nMost people recover from Salmonella infection within four to seven days without antibiotics. People who are sick with a Salmonella infection should drink extra fluids as long as diarrhea lasts.\nAntibiotic treatment is recommended for:\n- People with severe illness\n- People with a weakened immune system, such as from HIV infection or chemotherapy treatment\n- Adults older than 50 who have medical problems, such as heart disease\n- Infants (children younger than 12 months).\n- Adults age 65 or older\nCan infection cause long-term health problems?\nMost people with diarrhea caused by Salmonella recover completely, although some people’s bowel habits (frequency and consistency of poop) may not return to normal for a few months.\nSome people with Salmonella infection develop pain in their joints, called reactive arthritis, after the infection has ended. Reactive arthritis can last for months or years and can be difficult to treat. Some people with reactive arthritis develop irritation of the eyes and pain when urinating.\nHow do people get infected?\nSalmonella live in the intestines of people and animals. People can get Salmonella infection from a variety of sources, including\n- Eating contaminated food or drinking contaminated water\n- Touching infected animals, their feces, or their environment\nWho is more likely to get an infection and severe illness?\n- Children under 5 years old are the most likely to get a Salmonella infection.\n- Infants (children younger than 12 months) who are not breast fed are more likely to get a Salmonella infection.\n- Infants, adults aged 65 and older, and people with a weakened immune system are the most likely to have severe infections.\n- People taking certain medicines (for example, stomach acid reducers) are at increased risk of infection.\nWhat should I know about antibiotic resistance and Salmonella?\nResistance to essential antibiotics is increasing in Salmonella, which can limit treatment options for people with severe infections. One way to slow down the development of antibiotic resistance is by appropriate use of antibiotics.\nWhat can be done to prevent antibiotic resistance and resistant bacteria?\nAppropriate use of antibiotics in people and animals (use only when needed and exactly as prescribed) can help prevent antibiotic resistance and the spread of resistant bacteria.\nHow common is Salmonella infection?\nCDC estimates Salmonella cause about 1.35 million illnesses, 26,500 hospitalizations, and 420 deaths in the United States every year.\n- CDC. Antibiotic Resistance Threats in the United States, 2019. Atlanta, GA: U.S. Department of Health and Human Services, CDC; 2019.\n- Scallan E, Hoekstra RM, Angulo FJ, Tauxe RV, Widdowson MA, Roy SL, Jones JL, Griffin PM. Foodborne illness acquired in the United States–major pathogens pdf icon[PDF – 9 pages]. Emerging Infectious Diseases. 2011;17(1):7-15.\n- CDC. Foodborne Diseases Active Surveillance Network (FoodNet): FoodNet Surveillance Report for 2012 (Final Report) pdf icon[PDF 9 – pages]. Atlanta, Georgia: U.S. Department of Health and Human Services, CDC. 2014.\n- CDC. Suspecting Foodborne Illnesses in Special Populations: Quick Facts for Providersexternal icon. Atlanta, Georgia: U.S. Department of Health and Human Services, CDC. 2012.\n- Carter JD, Hudson AP. Reactive arthritis: clinical aspects and medical managementexternal icon. Rheum Dis Clin North Am. 2009 Feb; 35(1): 21-44.']"	['<urn:uuid:46c4c8dd-72b2-464a-8d37-ff6bdee772f1>', '<urn:uuid:228cba84-af4e-4635-bc49-f860292b6969>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-13T02:03:54.148118	7	56	1947
31	density scanning hip spine forearm locations	For bone density measurements, both Bone Mineral Density (BMD) scans and DEXA scans examine the hip and spine as standard locations. This is because the BMD at these sites has been shown to be the best way of predicting fracture risk. In some cases, such as when the hip or spine is unavailable due to surgery or implants, the forearm might also be scanned. Since conditions affecting bone density tend to affect the whole skeleton, examining these few sites is sufficient to establish the overall bone density.	['What to expect\nWhat to expect when going for a mammogram?\nDuring a mammogram, the radiographer will position you and image the breast.\nThe breast is first placed on a special cassette and compressed with a paddle (made of clear Plexiglas or other soft plastic).\nBreast compression is necessary in order to:\n• Even out the breast thickness so that all of the tissue can be visualized\n• Spread out the tissue so that small abnormalities aren’t obscured by overlying breast tissue\n• Allow the use of a lower x-ray dose since a thinner amount of breast tissue is being imaged\n• Hold the breast still in order to eliminate blurring of the image\n• Reduce x-ray scatter which also leads to poor image quality\nThe radiographer will make the first x-ray exposure.\nYou will be asked to change positions slightly between images, and then the process will be repeated for the other breast.\nGenerally, this is a painless procedure.\nYou will feel pressure on the breast as it is squeezed by the compressor.\nSome women with sensitive breasts may experience discomfort. Be sure to inform the radiographer if pain occurs as compression is increased.\nThe examination process may take up to half an hour.\nSometimes additional images need to be taken from different angles or with differing degrees of compression to show internal breast structures more clearly.\nIt may also be necessary to perform an ultrasound or MRI examination to add more information, particularly if breast tissue is dense, or if a lump can be felt or seen.\nWhat to expect when going for a Breast MRI?\nPlease visit our MRI service page for a comprehensive review of what to expect and how to prepare for your breast MRI.\nMost MRI exams are painless, however, you might find it uncomfortable to remain still during the imaging process.\nFor an MRI of the breast, you will lie face down on a platform specially designed for the procedure. The platform has openings to accommodate your breasts and allow them to be imaged without compression.\nThe electronics needed to capture the MRI image are actually built into the platform. It is important to remain very still throughout the exam.\nBe sure to let the radiographer know if something is uncomfortable since discomfort increases the chance that you will feel the need to move during the exam.\nWhat to expect when going for a Bone Mineral Density (BMD) scan?\nOur Bone Mineral Density scan is a quick and painless procedure.\nYou will be asked to lie down on a padded table.\nAs a standard, your spine and one/both hips will be examined.\nIn some cases, the forearm might also be scanned, if either the hip or spine is unavailable (usually due to surgery or implants).\nAs any condition affecting bone density tends to affect the whole skeleton, a snapshot of a few sites is sufficient to establish the overall bone density.\nThe BMD at the hip and spine has been shown to be the best way of predicting the risk of fracture.\nThe BMD scan is a painless procedure.', 'DEXA (DXA) Scan: Bone Density Test\nWhat is a bone density test?\nA bone density test, DEXA, measures the mineral content of the bones in certain areas of the skeleton. It’s a way to measure bone loss as you age. Healthcare providers sometimes call these tests bone densitometry tests, or DXA.\nWhat is a DEXA scan?\nA DEXA scan is a type of medical imaging test. It uses very low levels of x-rays to measure how dense your bones are. DEXA stands for “dual-energy X-ray absorptiometry.”\nMedical experts consider DEXA scans to be the most useful, easy, and inexpensive test for helping to diagnose osteoporosis. The test is quick and painless.\nWhat is osteoporosis?\nOsteoporosis is a term used to describe brittle bones and also the risk for having a broken bone. As you age, your bones can lose thickness and strength. Osteoporosis literally means “porous bone.” DEXA tests help your healthcare provider track your bone density and risk for having a broken bone over time. Providers often use DEXA tests to help diagnose osteoporosis.\nOsteoporosis results when you to lose bone faster than your body can create new bone tissue. This is most common in postmenopausal women. Over time, bones get weaker. Brittle bones break more easily.\nProviders sometimes call osteoporosis a “silent” disease because it doesn’t hurt. Many people first realize they have osteoporosis after they break a bone from a minor fall.\nWhat is Osteopenia?\nOsteopenia is a term used to describe “low bone mass.” Many individuals, including those who are slender and very active, have lower bone mass all of their lives. But this does not necessarily mean that they will develop osteoporosis. Many world-class athletes have low bone mass but their bones are healthy and very strong. Osteopenia is not “pre-osteoporosis,” but sometimes, if a person has other risk factors for fracture, an osteoporosis medication will be recommended to help prevent future fractures.\nHow does a DEXA scan work?\nDEXA scans measure the mineral content in certain bones, such as the hip, spine and/or wrist. It works this way:\n- You will be asked to lie on a special DEXA x-ray table. The technologist will help position your correctly and use positioning devices such as foam blocks to help hold the desired position.\n- As the arm of the DEXA machine passes over the body, IT uses two different x-ray beams. The beams use very little radiation to keep the test safer, and help to distinguish bone from other tissues.\n- The scanner translates the bone density measurement data into pictures and graphs. Bone is most easily seen in white, while the, fat and muscle tissue look like shadows in the background on the technologist’s computer monitor.\n- These results are then reviewed and interpreted by a radiologist or other physician trained in DEXA interpretation\n- Your healthcare provider is sent a copy of the written report to discuss with you and consider what treatment is most appropriate.\nWho gets a DEXA scan?\nHealthcare providers consider many factors when deciding who may benefit from a DEXA scan and how often. Healthcare providers often recommend a DEXA scan to assess your bone health for osteoporosis and fracture risk if you are older than 50, have had a broken bone, or other illnesses that put your bone health at risk.\nResearch shows women start losing bone mass earlier and faster than men. So healthcare providers usually recommend women get a DEXA scan to screen for osteoporosis at younger ages compared to men.\nYour provider may recommend a DEXA scan if you have one or more risk factors for osteoporosis or fractures:\n- Increased age: Many individuals lose bone mass as they get older. The National Osteoporosis Foundation recommends people at average risk get a DEXA scan starting at 65 (women) and 70 (men).\n- Family history: If one or more family members have had osteoporosis or more than one fracture, you could be at a higher risk for bone loss.\n- Previous fracture injuries: Breaking a bone, especially after age 50, may be a sign that you’re at greater risk. Porous (less dense) bones break more easily.\n- Medications: Some medications, such as the steroid prednisone, cancer drugs, and drugs used after an organ transplant can weaken your bones.\n- Your overall health: Many chronic medical disorders can make your bones more likely to break. Risky conditions include rheumatoid arthritis, lupus, diabetes, liver disease and kidney disease.\nWhat else do healthcare providers use DEXA scans for?\nHealthcare providers may also order a DEXA scan to:\n- Track bone health changes over time.\n- Monitor your response to treatment, such as an osteoporosis medication.\n- Evaluate body composition, such as how much fat and muscle mass your body has (and where).\nHow often should a DEXA scan be done?\nMedicare allows a DEXA scan to be done once every two years, and this is the current recommended timeframe. There are exceptions to this rule if you have certain diseases.\nYour healthcare provider will consider several factors, such as your age, level of fracture risk, previous DEXA scan and current medications. Your healthcare provider will then make a personalized plan for how to assess and protect your bone health.\nHow should I prepare for a DEXA scan?\nMost people don’t need to change their daily routine before a DEXA scan. Eat, drink and take any medications as you normally would, unless your provider tells you otherwise. You will be asked to fill out a questionnaire that asks about your current health, your family history of broken bones, smoking history, and current medications.\nBefore your test, please do the following:\n- Stop taking calcium supplements 24 hours before your test: This includes multivitamins as well as antacids such as TUMS® (commonly used to treat heartburn).\n- Wear loose-fitting clothing with no metal: Wear comfortable clothes. Try to choose items that don’t have metal (zippers, buttons or buckles). Sweatpants and a casual top may be good choices.\n- Tell your doctor if you might be pregnant: DEXA scans use low levels of radiation. Medical experts recommend avoiding all radiation exposure during pregnancy to protect an unborn baby.\nHow is a bone density test done?\nDEXA bone density tests are outpatient procedures. You may be able to wear your regular clothes during the test. Or you may be asked to change into a hospital gown. There are no needles or injections in this test. Getting a DEXA test is similar to having a standard X-ray.\nHow long does a DEXA scan take?\nA DEXA scan usually takes no more than 25 minutes. Many people are in and out of the room in less than 30 minutes.\nIs a DEXA scan painful?\nNot at all. You won’t feel a thing as the X-ray beams pass through your body.\nYou may feel slightly uncomfortable lying on the table, depending on how you’re positioned. But you’ll only need to stay in that position for a few minutes.\nHow accurate are DEXA scans?\nDEXA scans offer a high degree of precision and accuracy. Medical experts consider DEXA scans to be an accurate test for diagnosing osteoporosis.\nUnlike x-ray machines, DEXA machines are checked daily for their ability to measure bone mineral accurately, and no two DXA machines are exactly alike. That is why your healthcare provider will insist that you have all of your DEXA tests done on the same machine.\nResults and Follow-Up\nWhat should I expect after a DEXA scan?\nDEXA scans are quick and painless. You should be able to resume your usual activities immediately after the test.\nProfessionals certified to interpret DEXA images will review your results and write a report that will be sent to your healthcare provider.\nYour healthcare provider will explain your test results and help you understand what they mean for your health. Your healthcare provider can help you make decisions about how to keep your bones strong. They can also recommend diet and lifestyle changes that may help to lower your risk of a fracture.\nWhat should I ask my healthcare provider?\nIf your healthcare provider has recommended a DEXA scan, you may want to ask:\n- Why do you recommend this test for me?\n- Where do you recommend I have this test?\n- What should I do to prepare for my test?\n- When should I expect to get test results?\n- Will I need other tests?\n- What can I do at home to keep my bones healthy?\nMedical experts consider DEXA scans the “gold standard” for diagnosing osteoporosis and fracture risk. Many individuals lose bone density as they age. But that doesn’t mean you have to accept fragile bones simply as a reality of getting older. It is never too early to learn how to take care of your bones! Ask your provider if a DEXA scan may help assess your bone health today. Then, talk about steps you can take to slow bone loss or protect your bones for years to come.']	['<urn:uuid:0a35df2a-6f0c-4443-a139-8365362e9fe0>', '<urn:uuid:2376dcb0-6391-43ef-8834-19a861086ca7>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-13T02:03:54.148118	6	87	2016
32	What's the easiest way to tell if gas is leaking?	The easiest way to detect a gas leak is by the distinctive rotten egg or sulphur smell. This is due to a compound called mercaptan that suppliers add to natural gas, which is naturally odourless, to make it easier to detect.	['Natural gas burning appliances are great for heating the home and heating your water supply while often carrying lower running costs than their electrical counterparts. However, it’s wise to know about natural gas safety in case something goes wrong. That something in particular being a gas leak.\nWhether it’s caused by physical damage or improper installation, natural gas leaks can harm your health. It’s unlikely that a leak will lead to a mammoth explosion like in the movies, but gas leaks are dangerous. That’s why we’ve put together this guide to their warning signs, what to do in an emergency and the possible consequences of not acting accordingly. Here’s what you need to know about gas leaks.\n5 Warning Signs of a Gas Leak\nThe below warning signs are common for a small leak, a large one and everything in between. But that doesn’t guarantee ever yone will be present in your home should you experience a leak. Check appliances and gas lines regularly, keeping a lookout for one or more of these 5 warning signs:\nRotten Egg Smell\nNatural gas is odourless, so the suppliers add a unique compound named mercaptan to make the presence of gas easier to detect. This additive is easily recognisable as it smells similar to sulphur or rotten eggs. If you can smell rotten eggs within your home or on the property, then you’re likely experiencing a leak.\nNatural gas arrives compressed and pressurised. So, as it escapes from a line or tank, the gas will produce a hissing or whistling sound. The noise can be faint, so it’s always a good idea to check pipes, connections and gas appliances regularly, listening closely for sounds.\nExposure to natural gas can affect the health of those in your household. Low-level exposure can cause headaches and breathing difficulties, while high levels of exposure can cause nausea, loss of consciousness, and suffocation. Take unexplainable headaches and breathing issues as a sign that it’s time to check for gas leaks.\nDying Plant Life\nDead, dying, or wilted plants can be a sign of a gas leak. The presence of natural gas prevents plants from absorbing oxygen, leading to their demise. If you can’t explain why household plants are dying, the cause could be a gas leak.\nHigher Gas Bills\nAn unexpected increase in your gas bill can indicate a natural gas leak. Seasonal increases are to be expected as a natural gas heater could be in use during winter. However, increases during periods of minimal use are worth investigating.\nSo, check your home for gas leaks, and if you detect any of these warning signs, it’s time to take further steps, ensuring your safety and your family members.\nSymptoms of Natural Gas Leaks\nThe most concerning factor of a gas leak is the physical symptoms. These pose significant health risks if the exposure continues or the gas leak worsens. According to Healthline, a gas leak reduces the amount of oxygen in your home or in a confined space. A slight oxygen reduction is even enough to cause serious symptoms.\nHere are the symptoms you should look for:\n- Breathing problems\n- Eye and throat irritation\n- Ringing in ears\nComing into direct contact with natural gas can cause blisters on the skin, and these symptoms can also affect pets.\nNatural Gas Leak vs. Carbon Monoxide Poisoning\nGas leaks are commonly confused with exposure to carbon monoxide, as the two can cause similar symptoms. Carbon monoxide poisoning occurs after the gas is burned and the fumes are not properly removed from a home. This is why all gas-burning appliances must be fitted with a flue, a slimline chimney or an exhaust for removing carbon monoxide from home. Carbon monoxide has no smell or taste, so call an expert immediately If you suspect CO is leaking into your home.\nWhat to do When You Discover a Gas Leak\nIf you’ve discovered a gas leak, act fast and follow these simple steps to reduce the likelihood of it impacting your health.\nOpen Doors and Windows\nOpening as many doors and windows as possible will help vent the natural gas away from the home. Do not take too long, though, as remaining inside increases the risks of long term exposure.\nShutoff Gas Supply\nIf possible, shutting off your gas supply will eliminate any further gas from leaking into your home. The natural gas supply valve will be located at the meter box or at the gas pipes leading into your home. Simply turn the valve 90 degrees to shut off the gas supply. If you can’t locate the valve, speak with the landlord or utility company for clarification.\nEvacuate the Premises\nEveryone indoors should evacuate as quickly as possible and regroup in a safe location. A preferable place is away from the property. Do not return until an expert says it’s safe to do so.\nCall a Professional\nUse a cell phone to contact a certified inspector, gas fitter, utility company or fire department if dealing with a serious leak. They can eliminate the leak and possibly provide the necessary repairs at the same time. If you’re unsure about specific numbers, dial 911.\nStay Safe by Installing a Gas Leak Detector\nA gas leak detector operates just like a wall or ceiling mounted smoke detector but can alert occupants to gas leaks. These detectors are hard-wired into the home and provide audio and visual cues when gas is detected.\nJust note that there are differences between gas leak detectors and carbon monoxide detectors. Both natural gas and carbon monoxide have different compositions and require specific tools to detect their presence. Some units are dual function, while others are not.\nYou can purchase a gas leak detector from Walmart, Home Depot and all good hardware and electronic stores.\nAnd with this knowledge, you should be better equipped to detect gas leaks and take the necessary steps to ensure the health and wellbeing of your family. Contact your local gas supplier or manufacturer of the gas appliances in use for additional health and safety information.']	['<urn:uuid:dc70ec5b-1b02-4f03-be06-d7971b1f6867>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T02:03:54.148118	10	41	1011
33	board game sales us canada hobby games 2016	Sales of hobby board games in the U.S. and Canada reached $305 million in 2016.	['In a development that would have been hard to imagine a generation ago, when video games were poised to take over living rooms, board games are thriving. Overall, the latest available data shows that U.S. sales grew by 28 percent between the spring of 2016 and the spring of 2017. Revenues are expected to rise at a similar rate into the early 2020s—largely, says one analyst, because the target audience “has changed from children to adults,” particularly younger ones.\nMuch of this success is traceable to the rise of games that, well, get those adults acting somewhat more like children. Clever, low-overhead card games such as Cards Against Humanity, Secret Hitler, and Exploding Kittens (“A card game for people who are into kittens and explosions”) have sold exceptionally well. Games like these have proliferated on Kickstarter, where anyone with a great idea and a contact at an industrial printing company can circumvent the usual toy-and-retail gatekeepers who green-light new concepts. (The largest project category on Kickstarter is “Games,” and board games make up about three-quarters of those projects.)\nGrowth has also been particularly swift in the category of “hobby” board games, which comprises more sophisticated titles that are oriented toward older players—think Settlers of Catan. These games, compared to ones like Monopoly and Cards Against Humanity, represent a niche segment, but that segment is becoming something more than a niche: According to ICv2, a trade publication that covers board games, comic books, and other hobbyist products, sales of hobby board games in the U.S. and Canada increased from an estimated $75 million to $305 million between 2013 and 2016, the latest year for which data is available.\nHobby-game fanaticism is still very much a subculture, to be sure, but it is a growing one. At the 2017 iteration of Gen Con—North America’s largest hobby-gaming convention, in Indianapolis—turnstile attendance topped 200,000. For the first time in the event’s history, all the attendee badges were purchased before the event began. Whether they knew it or not, the many thousands of people carpeting the field level of Lucas Oil Stadium wouldn’t be there if it weren’t for a small group of obsessives on the other side of the Atlantic.\nThe rise of hobbyist games is legible in the career arc of one of the genre’s most famous present-day designers, Phil Eklund. He was born and raised in the United States. But tellingly, he didn’t really hit his stride until moving to Germany. Eklund took to game design early in life. As a teenager growing up in Tucson in the 1970s, he became frustrated with the narrow, child-oriented fare on offer at his local toy shops—roll-and-move games like Sorry! and Monopoly. So he started creating his own games, making photocopied print runs of a few hundred or so and mailing them out to customers.\nWithin America’s then-tiny board-game subculture, Eklund was making a name for himself. But he felt like part of the lowest caste of nerds. “I’d go to a gaming convention, and everyone would be crowded around the computers,” he tells me. “My board-game setup would be off in the corner. The only people who’d wander over were the folks looking for a garbage can so they could throw out their gum.”\nThat’s in the past. Eklund now lives in Germany, where he’s attained the status of cult celebrity. He has no plans to move back to the United States. “One of the reasons I came to this country is because I knew it was the place where people take board games really seriously,” he told me. “The designers have status. They put their name on the box, and people will buy based on their reputation.”\nNow a board-game star in Germany, Eklund’s friends include such masterminds as Friedemann Friese, the creator of the game Power Grid, and the legendary Uwe Rosenberg, who designed award-winning classics such as Agricola, Le Havre, and Patchwork. At Germany’s world-leading Internationale Spieltage (“International Game Day”) fair in Essen—which now attracts an audience from all over the world numbering almost 200,000—bookish introverts are mobbed by groupies looking for selfies. “It’s not like I destroy hotel rooms or go out with movie stars,” Eklund tells me. “But it’s sufficiently intense that when I get back home, it takes a week just to recover.”\nHobbyists around the world started paying serious attention to German-style board games (or “Eurogames,” as they’re now more commonly known) following the creation of Settlers of Catan in 1995. While it took more than a decade for that game to gain a cultural foothold, there seems to be no going back: Much in the way that Cold War–era American beer connoisseurs gravitated to the higher quality and vastly larger variety offered by European imports in the era before stateside microbrews took off, players who’d become bored with the likes of Monopoly and Scrabble started to note the inventive new titles coming out of Germany.\nCatan, as Klaus Teuber’s hyper-profitable franchise is usually called, has many of the signature features associated with Eurogames: randomized board layouts, flexible scoring systems, an aesthetic that tends toward rustic themes and wooden pieces. But as Eklund and other Eurogame pioneers explained to me, these games’ philosophy of play is rooted in trends dating to the Second World War.\nIn North America, the complex board games created during the latter half of the 20th century typically took the form of simulated warfare. In Risk, Axis & Allies, Star Fleet Battles, and Victory in the Pacific, players take on the role of generals moving their units around tabletop maps. But for obvious reasons, this wasn’t a model that resonated positively with the generation of Germans who grew up in the shadow of the Third Reich. Which helps explain why all of the most popular Eurogames are based around building things—communities (Catan), civilizations (Terra Mystica), farms (Agricola)—rather than annihilating opponents. The result is a vastly more pacifist style of a game that can appeal to women as much as men, and to older adults as much as high-testosterone adolescents.\n“When I was young, one of my first creations was a Star Trek–type game with humans fighting other races in space,” Eklund says. “I now realize it was more or less a racist concept. It’s been done many times. It’s just not that interesting.” In Germany, by contrast, he’s created games such as Pax Renaissance, in which players take on the role of bankers navigating the vicissitudes of war and religious upheaval in 15th- and 16th-century Europe.\nBut the gulf between the traditional American games of yore—“Ameritrash,” as the genre is dismissively referred to by the board-game cognoscenti—goes beyond the divide between militarism and pacifism. In Monopoly, that great bonfire of friendships, the conflict between players is direct, brutal, and zero-sum: You bankrupt me or I bankrupt you. Which is why so many rounds of Monopoly finish on a note of bitterness. The one game of Monopoly I ever played with my wife ended with her staring me down icily and declaring, without any hint of warmth or irony, “I have never seen this side of your personality.”\nIn Eurogames, by contrast, such naked metaphors for capitalism and predation are outré. The Spanish-themed El Grande, for instance, does not permit players to attack their opponents directly. Rather, players maneuver their caballeros around a map of medieval Spain in a bid to win the favor of local courtiers. Players don’t beat their opponents so much as thwart them. The same is invariably true in rail-themed Eurogames such as Ticket To Ride, in which players rush to claim choice routes. The action is always passive-aggressive—never just aggressive.\nThis mode of play is pleasant on multiple levels. There is an enormous amount of fussy micromanagerial satisfaction that comes from amassing A so you can invest in B, so you can trade for C, so you can build a D, which in turn pumps out more A. To outsiders, this churn of wood, brick, sheep, ore, and wheat always makes Eurogames seem overly complicated. (In Friedemann Friese’s masterpiece Power Grid, there is even a step called the “bureaucracy” phase.) But in practice, all the busywork keeps players immersed in their own projects, and less spiteful in regard to others’ success. Which makes for gentler competition, fewer arguments, and (in my experience) less in the way of intra-spousal recrimination.\nSince the Eurogame genre came into being roughly four decades ago (the inception of Germany’s Spiel des Jahres award, celebrating the “game of the year,” would indicate 1978 as a rough date of momentum-gathering), the earliest creators understood something fundamental about the psychology of gaming: While people can tolerate losing, they despise the feeling of being eliminated from a game in progress. And so most Eurogames are designed such that scoring comes at the end of the game, after some defined milestone or turn limit, so that every player can enjoy the experience of being a contender until the final moments. If this sounds somewhat Euro-socialistic, that’s because it is. But such mechanisms acknowledge that no one wants to block off three hours for gaming, only to get knocked out early and bide their time by watching TV as everyone else finishes up.\nPerhaps no game encompasses this egalitarian ethos more fully than the aforementioned Power Grid (or Funkenschlag, as it’s known in Germany), in which players take on the role of CEOs in a highly regulated, centrally administered energy market. While the first player who builds houses and hotels in Monopoly can easily leverage their initial advantage to build yet more houses and hotels and crush the competition, the exact opposite dynamic takes place in Power Grid: The more players expand their energy network, the lower their priority in acquiring the coal, oil, uranium, and recyclables they need to actually fuel their power plants. The feature acts as a natural damping mechanism on runaway leaders, so that players tend toward parity as the action progresses, and almost every game is fairly close until the last turn.\nThis way of playing caters to what most people actually want out of game nights: to unwind, to avoid boredom and humiliation, and to end the night as friends. One of my current favorites, for instance, is a game called Biblios, in which each player takes on the role of an abbot seeking to amass the greatest possible library of sacred books. Buying up Boardwalk and Park Place, seizing Asia, sinking an opponent’s battleship: These are all fine for children. But for adults, none of it compares to the white-hot joy of creating a well-functioning library.\nWe want to hear what you think about this article. Submit a letter to the editor or write to email@example.com.']	['<urn:uuid:19b82da8-8e2e-491f-a72c-ac0187ab89ab>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-13T02:03:54.148118	8	15	1783
34	pacific warm water storm effects measurements	El Niño events are characterized by warm Pacific Ocean waters, which are measured through a network of ocean buoys and satellites that track water temperature and currents. When sea surface temperatures rise 0.5°C above historical average for at least three months, it's officially declared an El Niño. These warm waters affect storm patterns - during the 1997-1998 El Niño, the strongest on record, this led to unrelenting rains and mudslides in California, causing hundreds of millions in damages. The position of the jet stream during El Niño typically leads to wetter conditions in southern states and warmer temperatures in northern areas.	"['It was the winter of 1997-1998 when the granddaddy of El Ninos - the one by which all other El Ninos are judged - vaulted the climate term to household name status. It had such a noticeable impact on U.S. weather that it appeared everywhere from news coverage of mudslides in Southern California to Chris Farley\'s legendary sketch on ""Saturday Night Live."" Basically, it was the ""polar vortex"" of the late ‘90s.\nSo it\'s no wonder that it is the touchstone event that people think of when they hear that name. And naturally, as the current El Nino event has gained steam, the comparisons to 1997 have been increasingly bandied about.\n16 U.S. Weather Photos That Will Amaze You\nThe most recent came this week in the form of an image from the National Oceanic and Atmospheric Administration that compares satellite shots of warm Pacific Ocean waters - a hallmark of El Nino - from this June to November 1997, when that El Nino hit its peak.\nOn the one hand, the two are comparable given that 1997 was the strongest El Nino on record and, at the moment, the best science indicates that the current event could match or rival that one - at least in terms of ocean temperatures. But on the other hand, each El Nino event is its own beast, the product of conditions in the ocean and atmosphere, of climate and weather that are unique in that particular place and time.\nIn the, albeit very short, modern record of El Ninos, ""we cannot find a single El Nino event that tracked like another El Nino event,"" Michelle L\'Heureux, a forecaster with NOAA\'s Climate Prediction Center, said.\nEl Nino Messes With Earth In Weird Ways\nForecasters like L\'Heureux cringe at comparisons because there\'s no guarantee the impacts of one El Nino will be just like that of a previous one, even if they look broadly similar. And it\'s those impacts - like potential rains in drought-stricken California - that most really care about.\nStormy Weather El Nino is not, as Farley\'s sketch had it, an individual storm, like a hurricane. Rather it is a shift in the background state of the climate brought about by the sloshing of warm ocean water from its normal home in the western tropical Pacific over to the east. That redistribution affects how and where ocean heat is emitted into the atmosphere, which can alter the normal patterns of winds and stormy weather in the region.\nThose more local shifts can telegraph through the atmosphere and, in the case of the United States, can alter the position of the jet stream over the country during the winter months, typically leading to wetter-than-normal conditions over the southern tier of states and warmer temperatures over the north.]\nThose are the effects of El Nino very broadly speaking, though. Such teleconnections, as they are called, tend to be more reliable when the El Nino is a strong one.\nNEWS: El Niño Makes A Late Appearance\nSuch was the case with both the strong events of 1997-1998 and 1982-1983. January and February 1998 were the wettest and warmest first two months to a year for the contiguous United States in the 104-year record at that time, according to NOAA. The position of the jet stream meant that some northern states saw temperatures up to 15 degrees above normal and both the Southeast and Southern California were awash in a series of storms.\nIn California, the rains were so unrelenting that they led to mudslides that caused houses to crumble off disintegrating cliffs and racked up hundreds of millions of dollars in damages.\nWith California now five years into a debilitating drought that has led to the first statewide water restrictions in its history, some El Nino-fueled rains (if not the more damaging aspects) may be quite welcome right now.\nBut here\'s the thing: Those two strong El Ninos that saw heavy winter rains in California are only that, a sample of two. In science, that\'s too small a pool to make any firm conclusions, L\'Heureux said.\nPHOTOS: Weirdest Weather of 2014\n‘Not the Only Ball Game\' There are other factors, from the inherent chaos of the atmosphere, to other large-scale climate signals, that can potentially override any push provided by El Nino.\nThis is exactly what happened with the El Nino of 2009-2010, which while it wasn\'t as strong as 1997, was still significant. But other climate signals helped blunt its effects in the United States, particularly in terms of temperatures, L\'Heureux said. Events like that make forecasters cautious about comparing the current El Nino to 1997. (NOAA acknowledged as much by changing out the original image it used and noting that it did so to avoid confusion).\nVIDEO: It\'s An El Nino Year, So ... What\'s An El Nino?\n""We think that the strength of (El Nino) is important,"" L\'Heureux said, but the exact strength it achieves is no guarantee of impacts similar to 1997, ""and that\'s simply because there\'s other stuff going on,"" she said. ""El Nino is not the only ball game in town.""\nSo where does that leave us in terms of looking ahead to what El Nino might bring this winter? We have an event that is looking more and more robust (when comparing June 2015 to June 1997, the broad ocean temperature patterns are very similar) and forecasting models are in pretty good agreement that that event will strengthen as we head towards winter and El Nino\'s typical peak. But exactly when it will peak and what its final strength will be is still uncertain. Even more uncertain is what those other influences on U.S. weather will be.\nSo what forecasters can say for now is that the likelihood of those typical El Nino impacts, including rains in Southern California, are higher, but exactly where those rains might fall isn\'t yet known.\nOne factor that may influence that is the remarkable pool of very warm waters that has been parked off the West Coast for a couple years now, a feature that was not present back in 1997. That feature could impact the typical changes El Nino brings to the jet stream, Daniel Swain, a PhD student in climate science at Stanford University, said in an email. It is possible that if the El Nino builds up enough strength, it could overcome that influence, though, he added.\n""If El Nino really does make into record territory during the coming winter, it\'s hard to envision California not experiencing a wetter-than-average winter, at least to some degree,"" he said.\nThe only real guarantee that forecasters can make, though, is that this El Nino event ""will evolve in its own way,"" L\'Heureux said. ""It may be similar to certain past events,"" but it won\'t be exactly the same.\nMore From Climate Central:\nEl Niño Helps Boost Pacific Storm Season Why Do We Care So Much About El Niño?\nEl Niño in 90 Seconds This article originally appeared on Climate Central, all rights reserved.', 'What is El Niño Southern Oscillation (ENSO)?\nWhat is El Niño?\nThe El Niño Southern Oscillation (ENSO) is a large-scale climatic phenomenon that originates in the tropical Pacific but affects global climate patterns. The warm phase is known as El Niño, and the cold phase is La Niña. El Niño occurs irregularly every two to seven years and peaks in winter.\nThe term El Niño, Spanish for ‘child’ or ‘the Christ child’, was first used by fishermen along the coasts of Ecuador and Peru to refer to a warm ocean current that typically appears around Christmastime and lasts for several months.\nWhat causes an El Niño event?\nIn a normal year, when ENSO is inactive, the equatorial Pacific trade winds blow from east to west. The winds push the warmer water towards the west, and colder water rises up from deeper in the ocean to replace it. This creates an east-west difference in sea surface temperature and hence an east-west difference in sea level pressure that maintains the trade winds and so drives a positive feedback loop.\nDuring an El Niño year, the east-west SST difference weakens, the pressure difference weakens, and the trade winds and their effects on the ocean weaken, so the eastern Pacific warms further. However, during a La Niña year, the opposite happens, the east-west temperature difference strengthens, the pressure difference strengthens, and the trade winds and their effects on the ocean strengthen, so the east Pacific cools further.\nSuch changes in sea surface temperatures affect the atmosphere over vast areas, with local and global repercussions. Locally, the associated atmospheric circulation changes drive increased atmospheric convection and precipitation over the central and eastern Pacific, while rainfall is reduced over the western Pacific. In addition, these changes have remote impacts throughout the tropics and at higher latitudes via the atmosphere, especially when ENSO is at its strongest in winter.\nHow do we measure El Niño?\nA network of ocean buoys measures water temperature, currents and the wind. Satellites also provide surface temperature and current data, providing the ability to monitor ENSO in real-time. We can also predict El Niño months in advance as we can measure the heat content of the upper ocean, one of the precursors for the onset of El Niño and use our climate models to predict its evolution. Although various criteria exist, we usually say an El Niño event is underway when sea surface temperatures in the equatorial Pacific (officially called Niño region 3.4) rise 0.5°C above the historical average for at least three months in a row.\nImpacts of El Niño and La Niña\nThe most substantial impacts are experienced by those countries near the tropical Pacific origin of ENSO. Changes in surface temperatures, winds and moisture affect rainfall intensity and patterns, leading to extreme events such as flooding and drought. During an El Niño event, Peru, Ecuador and the south-eastern parts of South America receive heavy rainfall. In northern Brazil, drier conditions or even drought results. Indonesia, South Asia and parts of Australia are also more likely to experience drought during El Niño. The change in weather patterns associated with El Niño can significantly impact the economy, particularly agriculture, water resources, fisheries and public health. ENSO also strongly influences the occurrence and intensity of tropical cyclones, and Atlantic hurricane activity weakens during El Niño but strengthens during La Niña. In the UK, we experience the socio-economic impacts of an El Niño event, partly through increased food prices. There are effects on the jet stream and European weather, especially in late winter when El Niño increases the chances of cold snaps. La Niña increases the chances of wet and stormy conditions.\nHow does El Niño affect global temperature?\nEl Niño releases heat into the atmosphere and increases subsequent global temperatures. For example, global average temperatures for 2016 were around 1.1°C above preindustrial values and the strong El Niño episode of 2015/2016 partly contributed. However, researchers have concluded that the warming from El Niño is only accountable for about 0.2°C of this overall figure.\nEl Niño and climate change\nBecause of the large event-to-event variations of El Niño, we don’t have enough past years of observations to show a clear impact of climate change on its properties. However, there is now some evidence that the effects of El Niño on rainfall may increase in the future and that we may even see more extreme ENSO events, but these remain active research questions.']"	['<urn:uuid:3a780ace-3206-46ab-975e-e5e92c8b2b50>', '<urn:uuid:5c500c11-408a-4bc2-b27d-e1b63643da23>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T02:03:54.148118	6	101	1910
35	How does cloud tech help businesses and track money?	Cloud technology helps businesses by enabling real-time performance insights, automation, and flexible content delivery through multi-CDN systems. For financial tracking, cloud solutions like SaaS have specific accounting guidelines from FASB where fees can be treated either as capitalizable software licenses or as service contracts charged to operating expenses, impacting how costs are recorded and depreciated.	['Multi-CDN delivery has been growing in popularity for a variety of reasons, ranging from security to performance to simply updating legacy architecture.\nFor companies that deliver content via live streaming though, there are unique reasons to consider multi-CDN. Real-time performance insights, automation, on-the-fly control and flexibility, and — most of all, quality of experience — are crucial to success. Let’s explore why you should evaluate multi-CDN and what key factors to consider in doing so for your quality of experience (QoE) goals.\nQoE benefits of multi-CDN delivery\nBeyond high availability and network redundancy, multi-CDN delivery also enables providers to adapt to changing network conditions with multiple delivery paths to the viewer:\nGeo-coverage and optimizations: Delivery networks differ in characteristics and coverage geographically. Understanding these differences and where one network might be more performant provides a key lever in QoE.\nEyeball network optimization and coverage: Closely tied to geo-coverage, edge delivery networks may not reach eyeball networks the same way. These differences can allow a broadcaster to route around network congestion to avoid jitter, latency, or other adverse QoE conditions.\nPeak traffic capacity, on-demand: Multi-CDN delivery allows a broadcaster to leverage all the peak capacity available to each edge provider simultaneously. Some providers will also provide reserved capacity for peak events.\nA clear vision of your fan experience goals should drive how you leverage these benefits. For example, the best delivery path is not necessarily measured just by latency. A 4K stream requires significantly more throughput than a 1080p stream. A low-latency stream experience is much less tolerant of network jitter, which can cause rebuffering. Multi-CDN delivery allows you to select the optimal path for the performance profile based on your specific QoE goals.\nKey considerations for multi-CDN delivery\nThe QoE goals you focus on should also dictate your multi-CDN vendors — including understanding what vendor-specific features you might have to give up or normalize across all of them. Here are a couple QoE considerations to keep in mind when choosing CDN vendors:\nTraffic management: The fundamentals of multi-CDN delivery require a traffic management layer. Your QoE goals will influence what criterion drives a CDN switch and whether you select DNS, the player, or server-side manifest manipulation to route traffic to each edge provider.\nBut it is equally important to consider how each CDN and the aggregate of all of the providers in your architecture will impact your origin infrastructure. Consider that AWS Elemental MediaStore, a high-performance cloud storage system designed for low latency live stream, can handle 1,000 requests per second. Some CDN providers have thousands of mid-tier servers that provide caching hierarchies to reduce the impact of massive requests for new objects. In live video delivery, every segment is a new object. And the manifest itself is a newly refreshed object that must be accessed by the player constantly. In a multi-CDN delivery deployment, this could mean 10s of thousands of requests going back to your MediaStore every second--several times more than what it can handle.\nProtecting your origin from this load is critical to maintaining availability to the stream. Products like Fastly’s Media Shield provide this critical origin offload, further collapsing the requests of many mid-tier CDN servers to just a few per second going back to origin infrastructure.\nObservability: In order to determine success, you need to be able to see what’s going on. This is not just having monitoring or analytics. It is having a complete picture of what’s going right and wrong, where that’s happening and why. Commonly referred to as observability, this methodology in DevOps practices translates to streaming with significant benefits.\nOf critical importance, real-time stream switching requires observability on every downstream delivery component, from each CDN to the player to the ad ecosystem (if that’s a factor). Note that some CDNs do not provide real-time logging critical to this workflow. Fastly’s Media Shield provides real-time logging in multi-CDN deployments for downstream CDNs.\nWhere to go from here?\nThe most important thing to remember as you’re architecting multi-CDN delivery is to understand your primary motivation for choosing this in the first place and ensuring that your application, infrastructure, and engineering teams are in a position to make your implementation successful.\nReady for more info? Check out this multi-CDN delivery survey we did on barriers, drivers, and strategies, to help drive your thinking. Also, if you’re interested in CDN performance, read our VP of Technology’s posts on performance testing and how to better measure cache hit ratios.', 'Keeping it simple when you’re discussing the Return On Investment (ROI) of a process, tool, or product with your senior leadership allows you to quickly overview benefits—but discussing the bottom line benefits of tool usually speaks to a deeply complex equation to get there. When it comes to ERP, there’s no denying some complexity in defining the investment side of the ROI equation.\nHonestly, the danger in writing about a clear assessment of ROI is that it will appear to be so complex as to discourage anyone who might be considering a new ERP implementation. Fear not! It’s important for technical leadership to face needed change instead of fearing it or putting it off.\nERP veterans do groan at the idea of new ERP implementation—for good historical reason—because ERP projects generally have a less-than-stellar track record with regard to time and cost.\nWhen it comes to time, overruns can often be traced to a failure of project management processes or team dynamics. Cost overruns, on the other hand, are usually a result of not having a clear understanding of all cost components, and/or underestimating the value for some parts of the project.\nSo, to be ahead of the game, avoid time/cost overruns and provide a true estimate of the cost of an ERP software investment, the first step is to define each element, the number of items that comprise each element, and the cost per item.\nBelow are 5 key ROI factors that can impact the investment and costs you’ll be considering. Some of these factors may even change the accounting treatment of the implementation project.\nThere are very likely more than just five factors, and we’d love to hear from you in the comments if you think of other elements worth considering for an ERP implementation. We hope this sheds some light on the decisions you’ll be making at this stage, and on how they will impact the Return on Investment of any ERP software solution you choose to implement:\nKey ROI Factor #1: Hardware Acquisition and/or Upgrade\nTraditional ERP software would have been installed on existing IT infrastructure, the only option. However, the availability of Cloud-based ERP solutions, SaaS configurations, and Cloud Service Providers now allows for many ERP software acquisition, deployment, and usage options.\nYou and your IT team will have to choose which solution is best for your business. If it is on-premise, you may have to upgrade or acquire computer hardware, additional servers and/or telecommunication equipment.\nThe ROI calculation needs to identify and include these equipment. Typically, computer hardware is purchased, recorded on the balance sheet as an asset, and depreciation is calculated in over its useful life. The positive impact of the additional depreciation in terms of reduced taxes, or improved cash flow should be captured as a benefit (Return) in the ROI calculation.\nKey ROI Factor #2: Software Acquisition\nIn the past, software was treated the same way as hardware purchase, in that the purchase of a software license was capitalized and depreciated over its useful life. However, Software-as-a-Service (SaaS) subscription configurations have dramatically changed the manner in which ERP software is acquired and used. In fact, it has been a bit of a hassle within the Accounting industry to provide guidance and definition to the proper accounting treatment for fees paid in a SaaS arrangement.\nRecently proposed guidelines by the Financial Accounting Standards Board (FASB) provide the definition that if a fee paid under a SaaS arrangement includes a software license element, that cost element needs to be identified and treated as any other software license; that is, capitalized and depreciated over its useful life. If the SaaS arrangement does not identify a software license element, then the SaaS fee is treated as Service Contract and charged to operating expenses.\nYou can see here how the difference in treatment of the software acquisition cost in an on-premise arrangement or a SaaS arrangement can have a significant impact on the calculation of ROI. This decision needs to be made prior to defining ROI on an ERP implementation.\nKey ROI Factor#3: Software Customization Cost\nEven when an ERP product has a good “fit” to an organization’s needs, some customization of that software may be necessary. The good news is that some of the customization costs of ERP implementations can also be capitalized and depreciated, adding to the financial benefit of the ROI calculation.\nThe bad news is that perhaps no single ERP implementation cost component is more underestimated than the cost of software customization. No other cost component can scuttle an entire ERP project or so drastically impact ROI calculations. Moreover, software customizations can grow almost exponentially, adding to implementation time and delaying the realization of the benefits sought from the ERP investment. It will save you many headaches and explanations later on to exercise great care and accuracy when estimating the full scope, time, and cost of ERP customization efforts.\nKey ROI Factor #4: Project Management and Implementation Cost\nWhile the all-up costs of the implementation efforts cannot be capitalized as part of the acquisition cost from an accounting point of view, many of these costs should be identified and included as part of the investment piece of the ROI calculation.\nThe cost of external consulting or project management services should be included, as well as the cost of training and the cost of any temp services to fill personnel gaps during the implementation. Initial training and data conversion costs should also be identified, estimated, and included as part of an ERP implementation cost.\nKey ROI Factor #5: Ongoing Costs\nEven after an ERP system is implemented, it will need to be maintained. Software maintenance fees are usually defined when an ERP system is purchased, whether in an on-premise or SaaS configuration. Such costs should be estimated for the useful life of the software and included in the ongoing expense as incurred. The cost of training new staff, or re-training existing staff when software upgrades are installed, should also be estimated and included as an ongoing expense of maintaining (and maximizing) your ERP investment.\nAgain, the danger in writing an article like this is that it could present ERP implementation as a bit of a decision minefield! Don’t get me wrong—ERP software can be a game-changer for your business (check out our customer stories page). But being aware of all the scope and cost elements involved, as well as which processes and decisions will impact those elements, can only better set up your company for a fast and accurate ROI calculation to produce and prove a successful implementation.\nWith due diligence, a complete and accurate ROI assessment will ensure that your company asks the right questions and makes the right decisions up front, acquires the best ERP system for your needs, and is empowered to fully realize the benefits of a new solution.\nConsidering an ERP implementation? We can help you assess your needs and understand the decisions you’ll want to make early the process. To get started on your research, you can download our free ebook, “Clearing the ERP Clouds” with the link below.']	['<urn:uuid:0867617c-a28f-405c-a3a5-8e2fb23bac79>', '<urn:uuid:28ba6d2b-1a44-41c2-9819-772ccb442bd0>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T02:03:54.148118	9	55	1925
36	What are the characteristics of Highland and Island region whiskies, and how are Scottish whisky companies contributing to local marine ecosystem restoration?	Highland and Island region whiskies include notable distilleries like Highland Park, which produces award-winning balanced single malts, and Tobermory, located in a picturesque fishing village on the Island of Mull. These regional distilleries are part of Scotland's whisky industry, which is now actively participating in marine conservation. Through initiatives like the Dornoch Environmental Enhancement Project, whisky companies are helping restore historically widespread native oyster populations, which provide essential ecosystem services like water filtration and biodiversity enhancement. This approach has become a model for marine habitat restoration across Europe and has inspired multiple community-based restoration projects.	"['Volume: 750 ML Distillery: Auchentoshan Region: Lowland Age: 15 Year Old ABV: 40.0%\nFounded in 1823, Auchentoshan is one of the only Distilleries in Scotland to use a triple distillation process. This triple distillation and unpeated malt gives the whisky a more delicate and sweet flavor than most Scotch whiskies. Alexander Murray & Co brings you this rare bottling of 15 year old Auchentoshan, that has been matured in...\nVolume: 750 ML Distillery: Tobermory Region: Islands Age: 19 Year Old ABV: 40.0%\nTobermory is one of the most picturesque distilleries in Scotland. It is found in the sleepy fishing village of Tobermory, on the Island of Mull, on the West coast of Scotland. The distillery is situated in an area called Ledaig, which means ""Safe haven"" in Gaelic tongue.Add your own customized personal label for only $10 more. See...\nVolume: 750 ML Distillery: Longmorn Region: Speyside Age: 21 Year Old ABV: 40.0%\nThe Longmorn Distillery Company was founded in 1893 by John Duff, Charles Shirres and George Thomson. The distillery is situated between the Rothes Glen and Elgin, right next door to Benriach Distillery. The name Longmorn derives from the Gaelic \'Lanmarnoch\' which translates as the church of St. Marnoch. This fine Alexander Murray & Co Bottling...\nVolume: 750 ML Distillery: N.S. Region: Islay Age: 23 Year Old ABV: 40.0%\nSituated on the North end of the Island, this Distillery was established in 1881. Unusual for an Islay Single Malt, being lightly peated in contrast to its fiery Southern neighbors. Alexander Murray brings you this rare Bottling of Islay Single Malt Distilled in 1989, matured for 23 years in Oak Casks.Add your own customized personal label...\nVolume: 750 ML Distillery: N.S. Region: Speyside Age: 40 Year Old ABV: 40.0%\nNamed for its location bordering the River Spey, Speyside is the true heart of Scotch Whisky production. Each whisky region has its own defining characteristics and Speyside Single Malts tend to be slightly sweet and a little bit peaty with floral notes. Alexander Murray\'s 40 year Speyside Single Malt is quite characteristic of the region. It...\nVolume: 750 ML Distillery: Balvenie Region: Speyside Age: 40 Year Old ABV: 48.5%\nMalt Master David Stewart has put to use his five decades of experience to create this rare expression. Balvenie 40 Year Old is crafted by marrying together beautifully aged Balvenie matured in traditional whisky oak casks and sherry oak casks. The Balvenie Distillery lies at the heart of Speyside, in the Scottish Highlands. Nowhere else will you...\nVolume: 750 ML Distillery: Macallan Region: Speyside Age: 30 Year Old ABV: 43.0%\nThe Macallan Fine Oak is Triple Cask Matured in a unique and complex combination of exceptional oak casks. The casks are filled with ""New Make"" spirit and gently rested over 30 years.\nVolume: 750 ML Distillery: Bowmore Region: Islay Age: 40 Year Old ABV: 42.0%\nBowmore 1955 40 Year Old is a truly rare and unique spirit. Bottled in a beautiful crystal decanter, with crystal stopper. It comes complete with padlocked case and certificate of authenticity. Bottle No. 267.\nVolume: 750 ML Distillery: Glenglassaugh Region: Highland Age: 40 Year Old ABV: 41.5%\nA Highland Single Malt, distilled at the Glenglassaugh Distillery. Matured in a Refill Sherry Hogshead. Bottled in 2014. Aged for over 40 Years. One of only 298 bottles. Non-chill filtered and bottled at natural cask strength.\nVolume: 750 ML Distillery: Highland Park Region: Islands Age: 18 Year Old ABV: 40.0%\nFirst released in 1997, Highland Park immediately found favour with whisky writers and enthusiasts all over the world. It is a perfectly balanced single malt with a toffee sweetness and a mouthwateringly smokey finish. It was awarded the ultimate accolade in 2005 and again in 2009, when US spirits writer F. Paul Pacult named it ""Best Spirit in the...', 'Description of impactHeriot-Watt University research into critically important biogenic habitats and the ecosystem services they provide continues to shape policy and management objectives in international maritime spatial planning initiatives. This research has led to significant economic and policy impacts by: providing exemplars of marine habitat restoration inspiring international proliferation of similar initiatives; working with nature in a widely publicized novel corporate partnership; shaping government policies and plans to promote sustainable practices through marine management and stimulated the growth of new oyster restoration supply chains for the NE Atlantic from the economically fragile Scottish Highlands.\nNarrativeIn REF 2014 HWU research was shown to be instrumental in global marine spatial planning initiatives, particularly those focused on biogenic habitats, including the designation and management of Marine Protected Areas (MPAs) in Panama, Colombia, Galápagos and Europe. HWU research has since expanded our expertise providing underpinning evidence to support conservation policy implementation and management of protected biogenic reef habitats, and their Ecosystem Services (ES), across North East Atlantic Europe.\nThe Dornoch Environmental Enhancement Project (DEEP: Sanderson, Mair, Harries, Henry and Porter) is a collaboration between HWU, The Glenmorangie Company and the Marine Conservation Society. The business interest is in the mitigation of organic discharges into the adjacent marine environment by exploiting the remediative biofiltration (ES) of oyster reefs which concomitantly increases biodiversity through the provision of oyster reef structure and function. Good environmental stewardship has added brand value. DEEP has pioneered, demonstrated and enhanced restoration of the historically widespread native oyster; demonstrating that restoration can be accomplished across the European Atlantic MPA network. Press coverage measured as ‘Opportunity to See’ has reached 1,000,000 to 865,000,000 people in years 2016 to 2020.\nA research has contributed directly to the development of UK Environment Agency and UK Department of the Environment Fisheries & Rural Affairs agenda 2025, restoring UK coastal marine habitats. The UK Environment Agency stated that they used the approach taken by the HWU partnership in Scotland as inspiration for a new Environment Agency initiative; Restoring Meadow, Marsh and Reef (ReMeMaRe) and in 2019 organised for the first time in the UK a successful international conference on estuarine and coastal habitat restoration: Restoring Estuarine and Coastal Habitats (REACH). This was widely attended including Ministers and Chairs of government bodies. The initiative is now sponsored by the Defra Group which includes JNCC, the Marine Management Organisation, Natural England and Cefas as well as the Crown Estate and Inshore Fisheries Conservation Authorities.\nIt has led to the development of the first UK and Ireland Oyster Restoration manual (now a model for seagrass, saltmarsh and beneficial use of dredged material manuals), the first oyster biosecurity workshop hosted by the Native Oyster Restoration Alliance at HW in January 2019, the development of oyster restoration potential maps for England, and; has culminated in a multi-million bid to Defra and HMT under the 4 year strategic review for additional funds for restoration of not only oysters, but seagrass and saltmarsh too.\nSenior Estuary and Coast Planning Manager at the UK Environment Agency said it ""….proved to be a defining moment for the future of marine environmental protection and recovery in the UK …"", and has, “demonstrated … how practical intervention could actually help to restore habitats that have been lost”, and, “all of this inspired by Heriot-Watt and Dr Sanderson’s work in the Dornoch Firth”.\nFurthermore, the work is cited by an increasing number of community-based restoration projects and SMEs as inspiration and proof of concept:\n•“As a result, we have secured funding from the National Lottery GBP225,000 to establish Scotland’s first community-led native oyster restoration project at Craignish...”, Project Coordinator, Seawilding.\n•“Involved in DEEP…[we]...have created a spin-out company called The Native Oyster & Shellfish Company Ltd.”, NOSCO Ltd.\nThe unique DEEP business collaboration with one of the world’s leading whisky distillers has revolutionised the approach taken to marine conservation and restoration and recognised in multiple awards 1,2,3,4,5, including the Guardian Award for University Business Collaboration. It also provides a marine exemplar of ‘working with nature’ to remediate human impacts. Together, DEEP partners have developed a restoration pathway with global applicability in temperate systems that has helped inspire and inform other marine restoration projects across Europe, not least through the hosting of the Native Oyster Restoration Alliance conference in 2019 (155 participants; 15 countries). The DEEP business collaboration is already bringing about a range of benefits including:\n•The development and increase in oyster reef habitat in the Dornoch Firth\n•Biofiltration by the oyster reef with associated water quality benefits\n•Benefiting the rural economy and the native oyster supply chain, evaluated by an independent Scottish government report as having the potential to create up to 50 FTE jobs and GBP3,000,000 GVA in fragile rural communities\nSanderson has presented the research in partnership with Glenmorangie’s Directors of CSR and Operations, in diverse settings from the local communities and at international events. The science underlying the project has-shaped the whisky industry’s approach to CSR; reaching tens of thousands of members of the public on whisky tours, festivals and open days and generating media coverage worth an advertising equivalent of >GBP15,000,000. The project has inspired stakeholders across many countries, elevating the reputation of partners and forging a new frontier in restoration at the start of the 2021 UN Decade on Ecosystem Restoration.\nSanderson, Mair, Harries, and Porter’s ecosystem services (ES) research has demonstrated the habitat provisioning value of biogenic shellfish habitats for commercial species and has become a key driver of enhanced protection by Scottish Government such that policy and management advice for Scottish Priority Marine Features now delivers enhanced protection from fishing impacts. Furthermore, there is mounting evidence of significant ES value in the carbon stores generated through biogenic habitats (Sanderson, Mair, Harries). As part of the Scottish Government’s objective to lead the world in adopting evidence-based policies to mitigate climate change, Porter and Want’s audit of blue carbon resources for Orkney is the most comprehensive regional audit of blue carbon anywhere in the world. Porter, Woolf, Bell, Johnson, McWhinnie, Kerr and Fernandes subsequently undertook a review of the Orkney State of Environment Assessment where blue carbon, pressures and sensitivity data have been incorporated as a basis for the statutory Orkney Marine Spatial Plan (a regional extension of the Scottish State of Environment Plan).\n1. VIBES Awards 2017 (winner of Hydro Nation Award)\n2. Walpole British Luxury Awards 2017 (winner of ‘Luxury With a Heart’)\n3. RSPB Nature of Scotland Awards 2017 (winner of Corporate Award)\n5. Brave New World Foundation Lighthouse Award 2020\n|1 Jan 2015 → 31 Dec 2020\n|Category of impact']"	['<urn:uuid:c7a47c3d-1260-415f-b07f-d4a6531ed99d>', '<urn:uuid:8fb490fc-bece-4b37-b108-8871d3ff4843>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T02:03:54.148118	22	95	1719
37	agile customer feedback team size tradeoffs	Extreme Programming relies on continuous customer feedback through short 1-2 week releases and user stories, which helps teams adjust to business environment changes. However, according to the Size/Time/Cost Complexity Profile, as team size grows from 3-4 members to over 10 members, projects become increasingly complex. For highly complex projects with multiple diverse teams, continuous customer communication becomes more challenging, requiring more rigorous processes for tracking progress and controlling output.	"['Agile is considered as just a framework or guideline for “light weight” development methodologies. There are many methodologies that fall under the agile framework. [i] To determine if a particular project is suited to a specific agile methodology, it is important review the characteristics of each methodology. Extreme Programing is one of the earlier “lightweight” programming methodologies considered when developing the agile framework.[ii] Extreme Programming relies on continuously releasing iterations of working software to the customer. It also focuses on small teams to perform the development. Furthermore, it relies on continuous feedback from the customer. All of these characteristics match the framework described in the Agile Manifesto.[iii], [iv] Extreme Programming has a unique composite of processes and tools that are used in the development of software.\nExtreme Programming meets the Agile methodology framework. One Agile characteristic it utilizes is its iterative releases. Extreme Programming relies on short one to two week releases of functioning software. These short releases allow the customer to provide continuous feedback as the software is released.[v] [vi] With the short iterations and continuous feedback, developers are better able adjust to any changes in the business environment. With retrieval of continuous feedback, Extreme Programming, like the other Agile programs, require close constant communication with the customer and continuous customer participation in the process.\nImage retrieved from http://www.extremeprogramming.org/map/project.html\nConstant customer participation and communication is a very important aspect of the agile framework that Extreme Programming fully adheres to. The utilization of the customer’s participation is utilized throughout the phases of Extreme Programming.[vii] During the “planning phase,” one way Extreme Programming teams implement customer participation and communication is by acquiring user stories from the customers. User stories used in Extreme Programming replaced the bulkier requirements documentation obtained in the traditional methods.[viii] Having the customer explain the features they want at the beginning of each iteration, in as few as three sentences, keeps the customers involved and keeps them aware of the status of the project. Furthermore, along with the user stories, negotiation of what features will actually be included in each iteration is important to the planning phase of Extreme Programming. The negotiation activity is important because it helps establish better understanding and more realistic expectations between the development team and client. When realistic expectations are established, they are more likely to be met, which in turn leads to high customer satisfaction. Another way Extreme Programming utilizes communication with the customer is through obtaining continuous feedback during testing. Acceptance testing is one of the major activities where the customer is heavily involved. It provides the customer opportunity to provide the development team with real world scenarios in which to test the product against and verify that the functionalities in selected user story for a particular iteration was properly met.[ix] While continuous feedback from the customer is crucial to the success of Extreme Programming, another aspect that is important is team dynamics.\nTeam dynamics is a very important aspect of the agile framework and is implemented well with in the Extreme Programming methodology. Job rotation is a key aspect of Extreme Programming. It allows team members to learn different skill sets and prevents one team member from becoming so crucial to the project that it would result in failure if they were to leave. One way Extreme Programming teams have implemented job rations is through the paired programming setup. Paired programming has a team size of two, and are each assigned a functionality or a piece of code to work on for an iteration. On a daily or weekly basis, many teams have one member of each pair rotate to another pairing. One person is kept stationary on particular functionality to keep continuity, while the rotating member is able to acquire some knowledge on each piece of code being worked on.[x]\nThe iterative scheduling, close customer interaction, team dynamics and other characteristics all contribute to the makeup of the values of Extreme Programming which have been categorized as, “simplicity, communication, feedback and courage.”[xi] Simplicity relates to the small iterations and continuous improvement to the code. Nagy Ramadan Darwish said in his paper, Improving the Quality of Applying eXtreme Programming (XP) Approach, “XP encourages starting with the simplest solution” because “A simple design always takes less time to finish than a complex one.” Refactoring is an important part of Extreme Programming’s simplicity value. It strives to continuously simplify code, so that it is easier to maintain and reuse code.[xii] Feedback and communication relates back to both the continuous communication with the customer and the team dynamics. For communication with customers, it is important to get their perspective through use of user stories and continuous feedback. In Šimičić Martina master’s thesis, Managing Agile Software Development Projects: Best Practices and Project Management Tools, Martina says, “The main goal is to give to developers the same view as customers have.” [xiii] If a customer sees a change in the business environment, through constant communication with the customer, the developers will also be able to see the change. Within teams, communication can be seen in the use of paired programming because paired programming is designed to share knowledge of code pieces with multiple members. Also Extreme Programming encourages shared ownership and open work spaces to facilitate the free flow of ideas. A unique aspect of communication in Extreme Programming is the standup meetings. They allow communication to the entire team in a very efficient manner, without the risk of entering into long drawn out discussions that are not relevant to the entire team because no one wants to be standing up for a long period of time. Courage on the other hand relates to the ability of the developers to make tough decisions and have the confidence to change code as they see necessary.[xiv] This relates to decisions related to refactoring and the willingness to remove code already created in the pursuit of making a better product.[xv] Examining the characteristics and values behind Extreme Programming; which characteristics are most important to contributing to success?\nResearchers have found that team dynamics and customer contribution in planning were the most important aspects to the performance of an Extreme Programming team. Research by Stephen Wood, George Michaelides, and Chris Thomson concluded in their research paper, Successful Extreme Programming: Fidelity to the Methodology or Good Team Working?, that “It has shown that two key elements of the XP methodology– customer planning and its specific protocols for teamwork –are important to its success.” In their study, they developed 9 hypothesis and used a method called “method of multiple hypothesis.” They conducted these test on graduate and undergraduate software engineering teams that were assigned to develop software for either the university or commercial interests. The dependent variable, performance, was measured through feedback from the instructors, the client and the team members themselves. Each evaluator filled out a questionnaire on each team/individual, designed on the evaluator’s perspective of the project. Through the study, researchers found that there was a positive correlation between “XP-specific - team factors (continuous integration, coding standards, and collective code ownership)” and performance. Another factor that had a positive correlation with performance was “customer planning (release planning/planning game, customer access, short releases, and stand-up meeting).” While team factors and customer planning had a positive correlation with performance, foundation related activities such as testing, paired programming and refactoring, had a negative correlation with performance. Therefore, it was concluded based on the variables of the study, that foundation activities may have been creating “barriers to producing high quality products.” Even with the results found in this one study, the results may not necessarily apply to your team. It is important to experiment and use the tools and processes that work best with your team.[xvi]\nExtreme programming utilizes many unique processes and tools to implement the agile methodology. Agile has been a successfully implemented at many companies. When implementing Extreme Programming, it is important to focus on have short iterations, constant customer communication and small teams that are designed to allow for job rotation. Even though processes like refactoring, testing and paired programming were shown to have a negative effect in one research conducted, it important to implement the tools, processes and methodologies that work best for your team. Many teams choose to use combination of multiple agile methodologies.\n[i] Ambily O. A., Dr. T. Judeth Malliga. Agile Software Development an Approach to Light Weight from Heavy Weight. International Journal of Engineering Science and Technology, 399-405\n[iii] Wooda, S., George Michaelides, G. , Thomson, C. Successful extreme programming: Fidelity to the methodology or good teamworking? Information and Software Technology vol. 55, 2013, 660-672\n[iv] Darwish, Nagy Ramadan. Improving the Quality of Applying eXtreme Programming (XP) Approach. International Journal of Computer Science and Information Security,vol. 9, No.11, 2011, 16-22\n[v] Wells,Don. Extreme Programming.org. (1999) Retrieved from http://www.extremeprogramming.org/rules/releaseoften.html\n[vi] Beck, K., ""Embracing change with extreme programming,"" Computer , vol.32, no.10, pp.70,77, Oct 1999\n[vii] Darwish, Nagy Ramadan. Improving the Quality of Applying eXtreme Programming (XP) Approach. International Journal of Computer Science and Information Security,vol. 9, No.11, 2011, 16-22\n[viii] Copeland, Lee. Extreme Programming. Computer World Inc. 2001. Retrieved from http://www.computerworld.com/s/article/66192/Extreme_Programming?taxonomyId=11&pageNumber=1\n[ix] Wells, Don. Extreme Programming.org. (1999) Retrieved from http://www.extremeprogramming.org/rules/functionaltests.html\n[x] Wells, Don. Extreme Programming.org. (1999) Retrieved from http://www.extremeprogramming.org/rules/movepeople.html\n[xii] Usha, K., poonguzhali, A Quantitative Model for Improving the Effectiveness of the Software Development Process using Refactoring. International Journal of Recent Trends in Engineering, Vol 2, No. 2, (November 2009.) 222-224\n[xiii] Martina, Šimičić. Managing Agile Software Development Projects: Best Practices and Project Management Tools. (2013) Retrieved from http://agile-only.com/master-thesis/software-dm/agile-s-dm/c-of-am-\n[xiv] Darwish, Nagy Ramadan. Improving the Quality of Applying eXtreme Programming (XP) Approach. International Journal of Computer Science and Information Security,vol. 9, No.11, 2011, 16-22\n[xvi] Wooda, S., George Michaelides, G. , Thomson, C. Successful Extreme Programming: Fidelity to the Methodology or Good Teamworking? Information and Software Technology vol. 55, 2013, 660-672', 'This article considers the unique complexities of large, long-duration, high-cost projects that pose challenges to project success, and offers both old and new management strategies to handle the complexities. Refer to Table 1: Size/Time/Cost Complexity Profile to examine the nature of these project characteristics as the size/time/cost dimensions increase.\n|Complexity Dimensions||Project Profile|\n|Independent Project||Moderately Complex Project||Highly Complex Project||Highly Complex Program\n|Size: 3–4 members\nTime: < 3 months\nCost: < $250K\n|Size: 5–10 members\nTime: 3–6 months\n|Size: > 10 members\nTime: 6 – 12 months\nCost: > $1M\n|Size: Multiple diverse teams\nCost: Multiple Millions\nTable 1: Size/Time/Cost Complexity Profile\nWhat Makes Large, Long, High-Cost Projects Complex?\nOf the various elements that combine to make long-duration projects complex, the most significant is the inevitable changes that will occur in the business environment, which will necessitate adjustments to virtually all elements of the project. Knowing this, the successful project leadership team evolves, practicing situational project leadership, adapting and modifying their approach to accommodate the inevitable changes. In addition to adapting to change, the sheer size of the work involved for large projects weighs heavy on the project team. Research has demonstrated that the smaller the project team and the fewer deliverables, the greater the likelihood of project success. Therefore, the project leadership teams need to reduce the size of work packages to ""seem like"" many small projects, as opposed to one very large endeavor. As a final point, team fatigue and burnout lead to complex human interactions and unavoidable staff turnover, both of which are difficult to predict and manage.\nManaging the Complexities of Large, Long, High-Cost Projects\nThe complexities of large projects require that particular attention be directed to planning the project, developing and delivering the solution, selecting team members, and sustaining a high-performing team over the long haul.\nPlanning the Project\nSix important strategies for planning and structuring large, long, high-cost projects are offered, both conventional and adaptive in nature:\n- Adaptive management approaches complement traditional practices\n- Progressive elaboration allows the project to evolve\n- A systematic, reliable approach to estimating increases confidence and accuracy\n- Rigorous time and cost management increases reliability\n- Stage-gate management enables continuous improvement\n- Rigorous risk management pre-empts challenges and seizes new opportunities\n1. Adaptive Management Approaches Complement Traditional Practices\nFor large projects, the ability to adapt is the difference between success and failure. The leadership team should analyze the situation, correctly answering questions like: Is this really a program? Is it a series of modestly scoped, small projects? Must the project or program deliver a product line, a system of systems? Can the solution be delivered in components? Only after this analysis should management decisions be made. In particular for long-duration projects, success depends on selecting the management approach best suited to deal with the changes that will inevitably occur. The team strives to recognize the nature of the problem and solution, and to understand whether the conventional, reductionist systems/software engineering and project management approaches will work effectively. Only then can we make the right choice of management approaches (e.g., conventional vs. adaptive techniques, appropriate project cycles, the best project team structure). It is also prudent to build continuous customer and end-user evaluation and feedback into the approach to ensure that the project delivers what is needed-which often is not what was originally proposed for large, long-duration projects.[i]\n2. Progressive elaboration allows the project to evolve and the solution to emerge\nContinuously improve and add detail to the project approach as more information becomes available. Allow more accurate and complete plans to emerge from the successive iterations of the planning process. Instead of trying to plan the entire project, start by scheduling only the activities that define firm basic requirements.[ii] Then, begin to plan activities to develop a conceptual design of the solution at a high level, resisting design decisions that will impose constraints.\n3. A systematic, reliable approach to estimating increases confidence and accuracy\nEstimating is hard, very hard. One precondition to being assigned as manager of a complex project should be a track record of developing reliable estimates. To increase reliability, use multiple estimating techniques. Educate your project sponsor and other key stakeholders about the fallibility of estimates in general and discuss the reliability they can expect from your specific estimates at key points in the project. Without a doubt, early estimates will be highly unreliable, exhibiting a wide range of variability. Numerous uncertainties are involved when building something unique with a team that has not worked together in the past. However, once the project has executed through a few iterations (if using incremental techniques) or through a few project phases (if using linear techniques), you can begin to gauge the speed of progress and adjust your original estimates accordingly.\n4. Rigorous time and cost management increases reliability\nDelivering on schedule is one of the main challenges for a long-duration project, simply because of the enormous amount of work to be accomplished. Implement a rigorous process for tracking progress and controlling output. Track progress to the next milestone or release scrupulously. Manage the schedule and budget by establishing a project support team to update and maintain the schedule and budget baselines; emphasize to team members that they should bring any issues that put the next milestone/release in jeopardy to your attention immediately.\n5. Stage-gate management enables continuous improvement\nStage-gate management can be used to create opportunities to gather feedback from your customers and your team members on a frequent basis. After completing each phase, iteration, or release, conduct informal team-based quality reviews of deliverables. As part of these reviews, determine what worked well and identify opportunities for improvement to the solution development process and team operations. Subsequently, conduct a formal external quality assurance review of major deliverables and incorporate actions to correct defects found in the deliverables that must be resolved before work can proceed. Update the project cost, schedule, and scope baselines for the remaining near-term project phases/iterations, incorporating lessons learned into the plans. As part of the review process, examine the business case to validate that business benefits will be achieved and the investment is still sound. Conduct a formal project review with the project sponsor and other key stakeholders to secure approval to formally launch and expend funds for the next phase/iteration.\n6. Rigorous risk management pre-empts challenges and seizes new opportunities\nFew projects perform adequate risk management. For large, long-duration projects, it is essential to identify risks after each iteration/phase and re-examine risk responses to:\n- Ensure the risk response plans are managing known risks\n- Identify new risks and develop risk response plans\n- Identify new project dependencies and interrelationships and develop dependency management plans\n- Identify previously unknown opportunities to increase the business value of the solution\nDeveloping and Delivering the Solution\nFive important strategies, both conventional and adaptive, to deliver the solution on large, long, high-cost projects are presented:\n- Iteration is the best defense against uncertainty\n- Scope minimization is the key to success\n- Last responsible moment decision making keeps your options open\n- Rapid application development reduces time to market\n- Lean development techniques increase efficiencies\n1. Iteration is the best defense against uncertainty\n""Projects should always be managed by rapid learning cycles because what we are doing is so complex that nobody knows the answer to begin with.""\n-T. Gilb, software engineer and author\nResearch has repeatedly demonstrated that short-duration projects are more likely to be successful than prolonged endeavors.[iii] Oftentimes business transformation projects involve a mix of complex development efforts, such as business process reengineering, legacy IT system replacement, and the creation of new, innovative business practices that rely heavily on technology. To increase the probability of project success, structure your project into multiple deployments of small solution components rather than taking the ""big bang"" implementation approach. As you develop and deliver the solution in increments, incorporate lessons learned from each increment into the next iteration and constantly test for alignment with business objectives.\nThe Standish Group Recipe for Project Success (Table 2) asserts that ""success is practically in the oven"" when a project follows this recipe. Standish reports that it is prudent to reduce the amount of resources to no more than four people, for no longer than four months, at a cost of less than $500,000. For large, long-duration projects, the only way to get the resources down to this level is to structure the effort into a program comprising multiple projects and to use incremental/iterative solution development.\n|Ingredients||Clear business objective; minimized scope (microprojects with rigorous configuration management); constant communication and collaboration; proven, standard, stable software infrastructure (vs. custom code); firm basic requirements; formal methodology; reliable estimates|\n|Mix with||Full-time, co-located core team members (experienced business analyst, project manager, business visionary, architects, and developers) coached by an involved executive project sponsor, involved stakeholders, an iterative development process, and effective decision-making tools (requirements tools, project management tools, design/analysis tools, and modeling tools)|\n|Bake||No longer than six months; no more than six people; at no more than $750,000 (1999)\nNo longer than four months; no more than four people; at no more than $500,000 (2001)\nTable 2: Standish Group Recipe for Success, 2001\n2. Scope minimization is the key to success\nThe motto of 21st century projects is: ""Barely sufficient is enough to move on."" The more features and functions, the larger the project; as we have discovered, less is more. Initially, deliver a minimum viable subset of the full solution to start adding value for the organization as early as possible. Then, continue to deliver components of the system in short-interval deployments. Limit the dependencies between solution components to reduce the cost of changes. Design the solution to be flexible and agile to allow the customer to respond to changes in the business need, technology, or market conditions. End the project when the return on investment in additional increments is marginalized.\n3. Last responsible moment decision making keeps your options open\nFlexibility comes from delaying design decisions and the start of major activities for key project drivers (information flows, technical decisions, and business decisions) until the last responsible moment; that is, the latest moment possible without compromising cost or schedule. This ""keep your options open"" approach allows for maximum flexibility.[iv]\n4. Rapid application development reduces time to market\nIf requirements are understood and scope is contained, rapid application development (RAD) allows for a greatly abbreviated timeline. RAD is a method of fielding multiple design/build/test/deliver teams to work concurrently. This component-based approach permits incremental testing and defect repair, significantly reducing risk compared to single, comprehensive delivery. Caution: RAD can be costly if (1) requirements aren\'t well-defined, causing a high risk of requirements defects, or (2) the design is not sound, with a minimal number of well-understood dependencies between increments, which can create a high risk of integration and maintenance issues.\n5. Lean development techniques increase efficiencies\nEven though the project is long and complex, do not be tempted to apply more rigor than necessary. Produce documents and conduct meetings only if they add value to the project. Continually verify that the project is building the minimum viable solution. Keep in mind the motto: ""Barely sufficient is enough to move on.""\nSelecting Team Members and Maintaining Team Health\nFor complex, long-duration projects, we offer three suggestions for maintaining team health:\n- Select team members for the long haul\n- Attention to team health pays dividends\n- Share resources to give team members a break in the action\n1. Select team members for the long haul\nWhen selecting team members for a long-duration project, keep in mind the special personality traits and coping skills that are needed. Prolonged forced interaction is simply not for everyone. For key positions, select team member who are resilient against social burnout and psychological stress.\n2. Attention to team health pays dividends\nLonger projects require that attention be directed to the physical and emotional stresses on the project team members. Focusing on the health of the team, making strategic personnel changes at critical junctures to infuse new blood, and providing appropriate team leadership will go a long way in sustaining the team.\n3. Share resources to give team members a break in the action\nOn long-duration projects, critical resources may not always be fully engaged. When this is the case, ""lend"" them out to a short-duration effort to give the team members a break, allow them to feel the gratification of completing a task or meeting an objective, and then bring them back to your project refreshed and ready to dive back in.\n|Managing large, long-duration projects|\nTable 3: Approaches for Managing Large, Long, High-Cost Projects\n[i] Linda Vandergriff, Complex Venture Acquisition, 2006. Complexity Conference White Paper.\n[ii] The Standish Group International, Inc. Extreme Chaos, 2001.\n[iv] Robert Lane, Vincent C. Lepardo, Graham Woodman, How to Deal with Dynamic Complexity on Large, Long Projects. Online at http://www.pbworld.com/library/technical_papers/pdf/32_HowtoDealwithDynamicComplexity.pdf, accessed January 2008), p. 5.\nThis article was adapted with permission from Managing Complex Projects, A New Model, by Kathleen B. Hass. ©2009 by Management Concepts, Inc. All rights reserved. www.managementconcepts.com/pubs.\nDon\'t forget to leave your comments below\nKathleen Hass is the president of Kathleen Hass and Associates, Inc., a consulting practice specializing in the business analysis, project management, and strategy execution. Ms. Hass is a prominent presenter at industry conferences, author and lecturer. Her expertise includes IT strategic planning, implementing and managing PMOs and BACOEs, facilitating portfolio management, leading technology and software-intensive projects, executive coaching, building and leading strategic project teams, and managing large complex programs. Ms. Hass has over 25 years experience providing professional services to Federal agencies, the intelligence community, and various Fortune 500 companies. Certification include: SEI CMMI appraiser, Baldrige National Quality Program examiner, Zenger-Miller facilitator, and Project Management Institute Project Management Professional. Ms. Hass serves as Director at Large International Institute of Business Analysis. She has authored numerous white papers and articles on leading edge PM/BA practices, the renowned series entitled, Business Analysis Essential Library, a compilation of six titles on critical BA practices. Her book, Complex Project Management, A New Model, was selected to receive the 2009 PMI David I. Cleland Project Management Literature Award to honor the best project management literature published in the last calendar year. Kathleen Hass, PMP, Senior Practice Consultant, can be reached at303.663.8655 Email: email@example.com Website: www.kathleenhass.com.']"	['<urn:uuid:36c1c950-bee8-4db7-80e9-20efa48b990c>', '<urn:uuid:9362dec2-7cac-4a02-b498-7872e098e675>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T02:03:54.148118	6	69	4016
38	parental access rights hipaa ferpa health education records compare differences	Under FERPA, parents have explicit rights to inspect and review their children's education records at K-12 schools and can seek to amend information they believe is inaccurate or misleading. In contrast, under HIPAA, parents don't automatically have access rights to their children's health information - access to ePHI requires written consent from the individual, except in specific permitted circumstances like treatment and payment operations. FERPA's parental rights extend to educational records including grades and schedules, while HIPAA governs access to physical/mental health information and medical records.	['Given the Health Insurance Portability and Accountability Act’s (HIPAA) extensive protections and restrictions regarding electronic protected health information (ePHI), cell phones present a challenging grey area to navigate. However, implementing a HIPAA-compliant cell phone policy and appropriate security controls will help your healthcare organization properly adhere to regulations.\nHIPAA-Compliant Cell Phone Policies and Usage\nAchieving and maintaining HIPAA compliance can easily be threatened by healthcare personnel’s cell phone usage. At first consideration, cell phones and their various security risks would seem opposed to HIPAA compliance but the right implementations can help any healthcare entity with their efforts.\nEnsuring HIPAA-compliant cell phone usage requires:\n- Understanding ePHI as it relates to HIPAA compliance and potential breaches\n- Knowing what telecommunication methods to monitor\n- The beneficial policies and security measures healthcare entities should implement\nA HIPAA compliance and cybersecurity expert can advise your compliance program. Further, a managed security services provider (MSSP) will provide many of the cybersecurity measures and training programs healthcare personnel should implement.\nUnderstanding ePHI and HIPAA Compliance\nImplementing HIPAA policies and security measures with respect to cell phones first requires understanding the ePHI that must be safeguarded. The Department of Health and Human Services (HHS) refers to and summarizes the HIPAA Privacy Rule’s demarcation of ePHI as “individually identifiable health information” covering:\n- Physical or mental health or conditions (whether past, present, or future)\n- Provisioned healthcare and any payments thereof\n- Demographic data (when not individually identifiable)\n“De-identified health information” doesn’t count as ePHI. To be considered de-identified, the data must neither identify an individual nor provide a reasonable basis to do so via one of two methods:\n- A statistician’s formal determination\n- All specified identifiers related to the individual and their relatives, household members, and employers must be removed to the extent that a given healthcare entity “has no actual knowledge that the remaining information could be used to identify the individual.”\nHIPAA-Permissible ePHI Uses and Disclosures\nAn individual’s ePHI may only be used or disclosed (i.e., made known or accessed by an unauthorized party) with their written consent or in the following circumstances without it:\n- To the individual\n- For conducting the individual’s treatment, payment, and healthcare operations\n- Following a clear opportunity provided to the individual to agree, acquiesce, or object\n- As a result of or incidental to permissible uses and disclosures—so long as reasonable safeguards have been adopted and the shared information was kept as minimal as possible\n- For public interest and benefit purposes, although rigid circumstances and procedures must be met to remain compliant\n- As part of limited data sets, if it is de-identified information that also meets additional criteria:\n- Used and disclosed for research, healthcare operations, or public health purposes\n- The individual has given consent in a data use agreement\n- Specified safeguards have been implemented to protect the identifiable portions of the information\nUnderstanding HIPAA’s Definition of ePHI Breaches\nEnsuring HIPAA-compliant cell phone usage also requires understanding what the regulations define as constituting a “breach.” Your organization cannot prevent security issues it doesn’t understand.\nHHS and the Breach Enforcement Rule define a breach as “an impermissible use or disclosure under the Privacy Rule that compromises the security or privacy of the protected health information.” Thus, any ePHI accessed, acquired, or interacted with by unauthorized personnel, without receiving an individual’s written consent, or outside the six circumstances described above constitutes a HIPAA breach.\nThis definition of breach does allow for specific exceptions:\n- If the covered entity or business associate responsible demonstrates that the probability that the ePHI has been compromised is low—based upon conducting a risk assessment that includes the following factors:\n- The nature and extent of the used or disclosed ePHI, notably, the types of identifiers and chance of an individual’s identification\n- The unauthorized party that used the ePHI in question or to whom the disclosure occurred\n- If the ePHI was actually used or viewed\n- If the use and disclosure risks have been mitigated appropriately\n- If the acquisition, access, or use of ePHI by a covered entity’s employee or a business associate acting under their authority was unintentional, made in good faith, and within their authoritative scope\n- If an authorized person made the disclosure to another person that is generally authorized to access individuals’ ePHI within the scope of their responsibilities due to:\n- Being employed by the same healthcare entity\n- Being employed by the healthcare entity’s business associate\n- Operating as part of an organized healthcare arrangement in which the healthcare entity participates\n- If the party to whom ePHI is disclosed is unable to retain the information in any capacity\nAre Cell Phones HIPAA-Compliant?\nGenerally, HIPAA regulations specify:\n- The security standards that must be upheld to safeguard ePHI\n- Entities’ responsibility to regularly conduct risk analysis for determining the necessarily implemented administrative, technical, and physical measures\nWhile unprotected cell phone access to or transmission of ePHI would significantly risk a HIPAA breach, the regulations do not explicitly prohibit cell phone or other specific technology usages outright. However, inadvertent use or disclosure and data or device theft that do constitute HIPAA breaches exponentially increase if ePHI is accessed on a cell phone or discussed over unsecure lines of communication.\nSo long as the appropriate security controls and processes are in place, cell phone use does not constitute a HIPAA breach.\nAre Cell Phone Conversations HIPAA-Compliant?\nBoth phone call conversations and faxed documents do not factor as ePHI, per the HIPAA Privacy Rules’ definitions under 45 CFR § 160.103. However, a phone conversation may constitute the disclosure of PHI if any discussion of identifiable health information falls outside of the HIPAA permissible circumstances listed above.\nWith the use of a second cell phone line app, HIPAA-compliant telecommunications may be more easily achieved. These services provide a secondary line for phone calls, texting, and voicemail personnel to interact with inside a segmented window on an existing device.\nAs an organizational policy, your entity’s representative on a phone or web-conferencing call should always identify themselves and confirm the other person’s identity. This practice helps ensure that the entity’s representative confirms their authorization to discuss PHI and that they are speaking to the individual whose information is being discussed (or someone acting in an official and recognized capacity as the individual’s representative).\nSecurity Controls and Policies for HIPAA-Compliant Cell Phone Usage\nHIPAA requires healthcare entities and their business associates to implement and maintain technical, administrative, and physical safeguards. The first two categories directly apply to cell phone usage. While there are ultimately no realistic physical safeguards that may be adopted for cell phones, certain technical measures (e.g., passcodes and authentication, encryption) will virtually eliminate physical security and compliance risks.\nHealthcare entities should consider technical safeguards ranging from activating (or deactivating) native device capabilities to implementing additional security measures. In addition, some technical safeguards may also be provided as native to cell phone applications and services.\nAdministrative safeguards consist of the mobile device policies that healthcare entities should enact and enforce. These policies should establish behavior expectations that oversee personnels’ cell phones usage.\nDevice-Native Technical Safeguards for HIPAA-Compliant Cell Phone Usage\nMany cell phones come equipped with native capabilities that healthcare professionals and business associates should activate or deactivate as part of organization-wide HIPAA compliance.\nNative security capabilities to enable include:\n- Passcodes – All cell phones and other portable devices that may be locked with pass- or pin codes should have the security feature enabled. Locking devices will certainly not deter all potential attempted and unauthorized access instances but provides an immediate barrier to protecting any stored ePHI.\n- Device Encryption – Android and iOS cell phones natively provide device encryption capabilities. Device-level encryption methods render firmware, software, and stored data unreadable without the associated cryptographic key. Device-level encryption is critical for HIPAA-compliant cell phone use, as it provides entities with a security measure that enforces two of the specified exceptions to incidents that constitute data breaches:\n- The ability to demonstrate that the probability ePHI was compromised is low.\n- The party to whom ePHI was disclosed (accidental or otherwise) will not be able to retain the data in any capacity (e.g., reading it on the screen, transferring it to another device) without the cryptographic key.\nDeactivation as a Technical Safeguard\nWhile most technical safeguards and security measures native to devices—or to the IT resources and storage they access—will need to be enabled, automatic backup and file sharing capabilities should be deactivated.\nThese capabilities do provide benefits in personal device usage. However, this functionality constitutes a HIPAA violation if ePHI is automatically backed up to any personal and unsecure storage locations (e.g., Google Drive, a cell phone carrier’s cloud storage) or shared.\nAs with many HIPAA violations, automatic backups would most likely lead to inadvertent noncompliance. Unfortunately, HIPAA penalty enforcement does not consider whether an incident that qualifies as a violation was intentionally or inadvertently committed.\nMultifactor Authentication (MFA) and Stored Login Credentials\nMultifactor authentication (MFA) requires users accessing a given IT resource (e.g., system, application, cloud service) to provide at least one additional method of identity verification as part of the login process. Generally, personnel provide a standard username and password combination before receiving a prompt for additional verification. This capability should be activated for any IT resource capable of interacting with or storing ePHI that provides MFA.\nMFA methods for IT resources accessed via mobile device include:\n- One-time passwords (OTPS) – OTPS may enforce MFA via various methods, including:\n- “Authenticator” applications – These applications display a randomly generated pin code that remains a valid credential for identity verification for a set duration (e.g., 30 seconds). After the duration concludes, a new random pin code is displayed.\n- SMS or Email – This MFA method will send the user the OTP via SMS or email for manual input.\n- Biometric identification – These methods include:\n- Fingerprint scanning\n- Facial recognition\nIn addition to enforcing MFA, any cell phone that stores or interacts with ePHI should not also store login credentials for any IT resource. If a hacker obtains access to the cell phone, stored credentials allow them to immediately access apps, services, cloud storage, and more without enforcing any additional security measures.\nImplemented Technical Safeguards for HIPAA-Compliant Cell Phone Usage\nSome technical safeguards that healthcare entities should implement will not be native to cellphones. The safeguards include:\n- Virtual private networks (VPNs) or virtual private clouds (VPCs) – VPNs and VPCs provide secure network and cloud connections to protect any ePHI data transmitted to or from a cell phone.\n- Remote wiping or disabling – Should a cell phone be lost or stolen, these capabilities allow an entity’s security team to delete ePHI (and other data) or prevent any access to it.\n- Firewalls and antivirus – Mobile device-specific implementations of standard security measures should be enforced on cell phones to prevent viruses, malware, phishing, and other common intrusion methods.\nAdministrative Safeguards for HIPAA-Compliant Cell Phone Usage\nAs important as technical safeguards are for protecting ePHI that cell phones interact with or store, your organization must also construct, promulgate, and enforce official policies to help ensure HIPAA compliance. A HIPAA cell phone policy should include specifications for:\n- Secure network and internet connections – Healthcare personnel’s cell phones should never connect to any entity IT environments or interact with ePHI over public or unsecure networks.\n- Email restrictions – ePHI should never be transmitted via email.\n- Cell phone cameras – While cell phone cameras provide an efficient and easy means for uploading images or providing a reference to colleagues, personnel should never take photos of anything that constitutes PHI.\n- Contacts – Storing patient contact information on a phone constitutes a HIPAA compliance violation waiting to happen. Similarly, any patient that stores healthcare provider contact information on their cell phone may cause a HIPAA violation. Since entities cannot control their patients’ cell phone contacts, they should consider adding a disclaimer (to be verified as binding by appropriate legal counsel) amongst signed forms.\n- Updates and Other Configurations – Cell phones should be configured to automatically install all available updates upon their release (e.g., operating systems, applications, threat signatures). Further, policies should prohibit any changes to HIPAA-specific and general cybersecurity configurations that the entity deems necessary.\n- Security awareness training – Healthcare entities should regularly provide their personnel with regular training to educate them on security practices and IT-related HIPAA compliance efforts. Advanced security awareness training may include elements such as phishing simulations to help personnel better recognize indicators of malicious activity.\nEnsure HIPAA-Compliant Cell Phone Usage\nAs with other HIPAA compliance efforts, ensuring that healthcare personnel’s cell phone usage adheres to regulations requires extensive technical and administrative safeguards to protect ePHI. While cell phones are not inherently HIPAA-compliant or noncompliant, interacting with or storing ePHI on mobile devices presents a far greater likelihood for violations to occur.\nWithout conscientious effort, healthcare personnel may inadvertently violate HIPAA-compliant cell phone practices.\nTo establish, assess, or remediate your organization’s cell phone policies and security implementations for HIPAA compliance, contact RSI Security today. As a HIPAA (and HITRUST) compliance and cybersecurity expert, we can help your organization maintain regulatory adherence.', 'The Family Educational Rights and Privacy Act (FERPA) (20 U.S.C. § 1232g; 34 CFR Part 99) is a Federal law that protects the privacy of student education records. … Parents or eligible students have the right to inspect and review the student’s education records maintained by the school.\nWhat rights do FERPA students have?\nThe eligible student has the right to have access to his or her education records, the right to seek to have the records amended, the right to have control over the disclosure of personally identifiable information from the records (except in certain circumstances specified in the FERPA regulations, some of which are …\nWhat information is covered by FERPA?\nThe Family Educational Rights and Privacy Act (FERPA) is a federal privacy law that gives parents certain protections with regard to their children’s education records, such as report cards, transcripts, disciplinary records, contact and family information, and class schedules.\nHow does FERPA apply to college students?\nFERPA protects college students’ education records so they remain confidential between the college and student. How involved you are in their academic life is now at your student’s discretion—even if you’re paying the bills.\nCan a parent observe a classroom under FERPA?\nParents are told that “the law” requires schools to deny parent requests to observe their child’s class because this would violate the rights of other students. The only law that deals with confidentiality is Family Education Rights and Privacy Act (FERPA). FERPA regulates education records, nothing more.\nWhat are parents rights under FERPA?\nAt the K-12 school level, FERPA provides parents with the right to inspect and review their children’s education records, the right to seek to amend information in the records they believe to be inaccurate, misleading, or an invasion of privacy, and the right to consent to the disclosure of personally identifiable …\nHow long does FERPA last?\nA FERPA Certificate of Completion must be renewed every two years.\nHow often are FERPA rights provided?\nFERPA regulations require that local education agencies give annual notification to parents and eligible students of their rights under FERPA (34 CFR § 99.7).\nWhat is considered an educational record under FERPA?\nThe term “education records” is defined as those records that are: (1) directly related to a student; and (2) maintained by an educational agency or institution, or by a party acting for the agency or institution.\nWhat is a FERPA release form?\nBy signing the consent form, students give the College authority to share information contained in the student’s educational record to their parent or guardian. FERPA does not allow for information to be released on the assumption that if the student is in the room that they have given their consent.\nWhat must be included in a FERPA compliant consent form?\nFERPA requires that a consent for disclosure of education records be signed and dated, specify the records that may be disclosed, state the purpose of the disclosure, and identify the party or class of parties to whom the disclosure may be made.']	['<urn:uuid:73324921-150a-4a27-b2b6-a9782e42207a>', '<urn:uuid:8fedef3e-c52b-49fc-aebd-13e8c8006d46>']	open-ended	direct	long-search-query	similar-to-document	comparison	expert	2025-05-13T02:03:54.148118	10	86	2710
39	What damages can sun cause to roofs and what eco-friendly solutions exist?	Sunlight can cause severe damage to roofs through both UV and infrared radiation, leading to problems like bumps, cracks, split seams, and compromised fasteners. Dark-colored roofs particularly retain heat, continuing to damage the roof even after sunset. These issues affect various roofing materials and can allow water to penetrate the building. As solutions, there are two main eco-friendly options: cool roofs and green roofs. Cool roofs use light, reflective materials that minimize heat transfer and can reduce energy costs by up to 10%. Green roofs cover the building with vegetation and growing medium, providing insulation, storm water mitigation, and can extend the roof waterproofing membrane's life by two to three times.	['Sunlight Damage on Roof\nSimilar to the effects of the Sun on your skin, it is important to note that commercial roofing systems are also very sensitive to the Sun. Particularly, in certain areas of the United States like the Western parts. The geographical locations are heavily impacted by the amount of Sun they get. For instance, if you reside in the Los Angeles area, the amount of Sun that you will get is about 44% of the time throughout the year. Unfortunately, even though some people may enjoy this warm weather, this amount can wreak havoc on roofing systems, especially because the Sun is composed of both UV radiation and infrared radiation.\nSunlight Damage on Roof\nNegative Domino Effects of Sunlight on Your Roofing\nThere are a number of different negative domino effects that you will most likely see on your roof today. Some of the more commonly known include bumps, cracks as well as other kinds of damages. In fact, as the beams of the Sun break down the roofing material, it causes the chemicals in the seam sealants to split and open prematurely over time. In addition to the seams of the roofing opening up, the damages from the Sun can also adversely affect the fasteners on the roof because of the pulls, tears and the twist that negatively compromises the fasteners. Further damages may be caused by the integrity of the roof, even after the Sun has set in the afternoon. This is because dark coloring on your roofing system will inherently hold onto the heat and will not allow it to cool down. Thereby, causing the heat that remains to continue to compromise the integrity of your roof.\nHidden Damages Exposed When Ultraviolet Radiation Compromises the Roofing\nEven more insidious, you will also discover that the ultraviolet radiation, another part of the sunlight, is not easily detected but imposes a lot of extra damage to the roofing’s overall structure. Therefore, it does not matter what kinds of materials used in your roofing system, the degradation continues in pretty much any type that is installed (i.e. single-ply-membrane, bitumen and others). Also, because this is hidden damage that is being done behind the scene in a more subtle fashion, no one will notice that these problems have been occurring until the rain clouds move in as the Sun goes away. Hence, when you inspect the roofing system on your building, the after effects usually show split seams, cracking, and blistering that will ultimately allow water flows to work their way down inside the building causing all kinds of interior damage.\nDealing with the damages that occur to your roofing system can be a huge issue for you when you are impacted by its effects. Due to the severe damages that the Sun can cause on the roofing materials and the seams in the design, the damages can completely destroy the tops of your building. To avoid these issues and damages, it is important to note that many property owners are turning to a newer innovative solution that is called cool roof systems.', 'THE COOL ROOF AND THE GREEN ROOF\nThere are two main types of environmentally friendly roof treatments i.e the cool roof and the green roof.\nCool roofing is an easy and cost-efficient, environmentally friendly finishing technique that can be considered by all prospective home and building owners in order to minimise energy use. It calls for either the cladding of rooftop surfaces in high solar reflective materials or the painting of this in light, bright colours, such as white which means of cutting energy consumption.\nBENEFITS OF COOL ROOFING\nThe concept is as simple as wearing a white shirt on a hot day. Your roof can get up to 50 degrees hotter than external temperatures. Light, or reflective surfaces, absorb less heat and therefore minimise heat transfer to the underlying slab of the roof and subsequently into the building’s interior.\nAchieving this means:\n- decreased necessity for artificial cooling which can save up to 10 percent on energy costs\n- increased indoor comfort through creating a cooler environment\n- decreased roof maintenance costs due to longer roof lifespan\n- inexpensive material and installation costs\nENVIRONMENTAL BENEFITS OF COOL ROOFING\nAside from user-specific benefits, cool roofing can also play a role in improving overall environmental conditions in more urban areas. Dark pavements and roof surfaces in more developed areas can contribute to air pollution and greenhouse gas emissions through creating an increased platform for heat absorption and subsequently energy consumption, leading to greenhouse gas emissions from power plants. This phenomenon is referred to as the ’heat island effect’, which causes these areas to release accumulated heat back into the atmosphere and cause up to a 5-degree temperature rise in comparison to less urban areas.\nExtensive use of cool roofing city/island wide could manifest environmental benefits such as:\n- improved air quality\n- a cooler urban environment\n- reduced greenhouse gas emissions\nGREEN ROOFS IN TROPICAL CLIMATES\nA green roof or living roof involves completely covering the top exterior level of a building with vegetation and growing medium. Traditionally, these are known for their ability to better insulate homes in temperate climates, providing a warmer indoor environment. However, these can also be utilised to serve various purposes in tropical climates.\nThe average underlying system consists of:\n- roofing waterproofing membrane\n- root barrier\n- air barrier\n- water retention mat\n- drainage/water storage/ aeration\n- filter fabric\nGREEN ROOF BENEFITS\nGreen roofs help minimise heat transfer into building interiors through the shading provided by its plants and associated growing medium which helps block sunlight from reaching underlying roof membranes. This occurs alongside a cooling effect achieved through moisture build up in the soil. On hot summer days, this makes a green roof cooler than the surrounding air temperature. Local landscape architect Andre Kelshall believes green roofs reap a multitude of benefits if properly designed and installed; stating that this roofing type could be particularly efficient for developers by ensuring a cut in energy consumption by tenants, thus ultimately lowering utility bills. He also concurs that the build-up of layers involved in the system could put homeowners in the Caribbean at an advantage when it comes to flooding and water mitigation. Installation costs can run between $90.00 – $150.00 BBDS per square foot. If wanting to opt for a less pricey, lower maintenance system, he recommends the use of turf instead of grass, as this can be engineered to grow at a slower rate.\nPerformance advantages of green roofs in the Caribbean include:\n- cooler building interiors\n- stormwater mitigation\n- lengthening the life of the roof waterproofing membrane by two to three times\n- increase indoor comfort by diminishing stress associated with heat waves\n- noise reduction\n- create an aesthetically pleasing or recreational roof landscape\n- increased property value\n- can prevent the overheating of solar panels if installed\n- long-term cost efficiency\nENVIRONMENTAL BENEFITS OF GREEN ROOFS\nGreen roofs are capable of reducing surrounding temperatures by a process which releases moisture into the air known as evapotranspiration. Vegetation naturally removes air pollutants and greenhouse gas emissions through carbon sequestration and storage (the absorption and storage of carbon with immediate potential to become a CO2 gas) as well as through the reduction of energy demand and production.\nSuch factors aid the environment by:\n- minimising heat island effect\n- diminishing air pollutants and greenhouse gas emissions\n- supporting habitat provisioning and biodiversity\n- reclaiming land excavated from buildings footprint\nAndre Kelshall, Design, [www.dla.design.com]\nUnited States Environmental Protection Agency, ‘Using Green Roofs to Reduce Heat Islands’, epa.gov, [https://www.epa.gov/heat-islands/using-green-roofs-reduce-heat-islands]\nUnited States Environmental Protection Agency, ‘Reducing Urban Heat Islands: Compendium of Strategies ’, epa.gov, [https://www.epa.gov/sites/production/files/2017-05/documents/reducing_urban_heat_islands_ch_3.pdf]\nCool Roof Rating Council, ‘Why Cool Roofs are Way Cool’, coolroofs.org, [http://coolroofs.org/documents/IndirectBenefitsofCoolRoofs-WhyCRareWayCool_000.pdf]\nEnergy Saver, ‘Cool Roofs’, energy.gov, [https://energy.gov/energysaver/energy-efficient-home-design/cool-roofs]']	['<urn:uuid:291efccb-36a3-4d9e-8aa6-d189339caf38>', '<urn:uuid:67435366-3a60-4769-8b1b-17eac0d76b71>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T02:03:54.148118	12	111	1304
40	when was carrowmore megalithic central tomb built	The central monument of Listoghil at Carrowmore Megalithic Cemetery dates to 3600 B.C.	['Steeped in local history, Strandhill is a haven for the culturally curious. With many ancient sites, beautiful beaches and migrant birds, this seaside village should be on everyone’s bucket list.\nHeritage & History\nStrandhill: Where history meets nature\nThe first thing you may want to do after arriving in Strandhill is visit Knocknarea. The majestic mountain features a prehistoric cairn dating back 5,000 years – making it older than the Egyptian pyramids – and according to local legend it’s the burial ground of ancient goddess Queen Maeve.\nFond of a good story, locals will tell you all about Connaught’s warrior queen. The large chambered cairn on top of Knocknarea is known as Queen Maeve’s Grave and word on the street is that Maeve is buried here ready for battle – upon her horse facing her enemies in Ulster.\nStanding at nearly two hundred feet long and forty feet high it’s the largest unopened cairn in Ireland and it’s thought to conceal a Neolithic passage tomb. The best way to reach this legendary site is along the Queen Maeve trail which takes you up the northern slopes of Knocknarea, past megalithic and Bronze Age remains, as well as derelict stone cottages. It takes about 90 minutes to complete the trail, which has 500 wooden steps making for an easy climb to the summit.\nIf this climbing adventure has you itching to discover more about megalithic tombs then the Carrowmore Megalithic Cemetery is the next stop for you.\nThis is one of the largest Stone Age cemeteries in Europe and with the central monument of Listoghil dated to 3600 B.C this is one place you do not want to miss. Carrowmore also has a visitor centre where you can learn more about the prehistoric sites and a guided tour can be arranged, or you can simply stroll around.\nFor ruins of a different kind there’s Killaspugbrone, an ancient church and adjoining cemetery behind the now defunct Strandhill airport. Killaspugbrone church was founded by Saint Patrick in the fifth century, but the present ruins date to the period between 1150 and 1220. The older ruins are long buried under the sand, but the church and cemetery have been preserved thanks to the planting of marram grass.\nAs well as being a place of historical importance, the Killaspugbrone Loop is also the perfect area to experience the beauty of a Strandhill sunset. There are many intriguing ruins dotted along the walkway, so we recommend allowing extra time to take in the beauty and history before the sun goes down.\nFor those looking to meet some local and visiting Creatures, Culleenamore is a diverse animal habitat. Embrace the life of a local with a morning stroll along Cullenamore beach where you’ll see birds flocking overhead and if you’re lucky you might even spot some seals in the distance. Another unique feature are the oyster beds, which are one of the oldest in the county with huge mounds of oyster shells buried under the sand.\nThis walking route has two options – a 90 minute walk along the sandy curve of the shore from Culleenamore to Strandhill, or a 45 minute stroll across the sand dunes – both routes lead to Strandhill Village where you can feast on local cuisine.\nBut it’s not time to go home yet – no visit to Strandhill is complete without traveling back in time to Coney Island. On this quaint island you can observe the village life of yesteryear, while admiring the beauty of unspoilt nature. The wetlands here welcome Brent Geese from as far as the Canadian Arctic Circle during winter months, so it’s also a great place to spot some migrant birds.\nJourneying to the island is just as magical as the destination itself. You can drive, cycle or walk across the seabed when the tide is out. There are fourteen pillars to guide you along the safest route – a one and a half kilometre stretch – to the island. But the tide comes in fast and heavy, so it’s important to check the tide times before venturing out. If you’re planning a visit simply text ‘Coney’ to 51155 on the day of your trip and you’ll receive a full forecast of safe times to visit the island. There is only one small pub on the island – business hours vary – so if you’re planning to stay for a few hours it’s best to be prepared with some snacks and refreshments.']	['<urn:uuid:6f7aecbf-0289-41b8-a2f5-4c404c35bf93>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T02:03:54.148118	7	13	745
41	I'm wondering about lawn cleanup timing - what's the main difference between raking leaves in spring versus autumn, and how should I handle the debris differently in each season?	In spring, raking is focused on cleaning up leftover winter debris to ensure the lawn gets enough sunlight to green up quickly. You need to be gentle during spring raking to avoid damaging new turf growth, using specifically a leaf rake rather than a hard garden rake. In autumn, raking is primarily focused on managing falling leaves from deciduous trees before they become wet and sticky. The autumn leaves can be more actively utilized - you can either add them to your compost bin or mulch them up with your mower and use them directly on garden beds as mulch. This mulch application helps retain soil moisture and prevent erosion while improving overall soil quality.	"['Post By RelatedRelated Post\nAs your lawn wakes up from its winter slumber, there are things you can start doing almost immediately to clean it up and get a proactive start on maintenance.\nA Note on Timing\nWhile it may seem beneficial to get out as soon as the weather begins to warm up, a little patience can go a long way to maximize the results of your work. Wait until your lawn has mostly greened up to begin mowing, aeration or other maintenance. Too much traffic on the lawn before it’s green and actively growing increases the chances of killing off new shoots before they mature, or compacting the soil.\nTurf Type Matters\nUnderstanding whether you’ve got cool-season grasses (such as bluegrass, fescue, and rye) or warm-season grasses (such as Bermuda, St. Augustine and zoysia) is important to knowing which maintenance tasks you’ll need to perform, and when it’s best to perform them.\nCool-season grasses have two primary growth periods: one in the spring and another in the fall. They can struggle and go dormant in the heat of the summer, so spring maintenance should focus on strengthening the lawn to better deal with the summer heat.\nWarm-season turfgrasses begin growing shortly after the final spring frost and hit their full growth stride as the midsummer heat cranks up.\nHere are some maintenance items you can perform to help your lawn be more beautiful and weed-free this season, with notes on timing for warm- and cool-season grasses:\nYard Raking and Cleanup\nMost lawns require some debris cleanup first thing in the spring. Raking up leaves and other organic material helps ensure your lawn is getting the sunlight it needs to green up quickly. As you rake, make sure you are being gentle as not to damage the turf, and use a leaf rake and not a hard garden rake. Make note of any areas of the lawn that may be compacted (and need to be aerated) or have excessive thatch (and need to be dethatched).\nYou won’t necessarily need to fertilize your lawn in the spring, particularly if you applied fertilizer last fall. But depending on what type of fertilizer program you’re on, spring can be a good time to have your soil tested to check available nutrient levels. Cool-season grasses can benefit from a light fertilizer application in spring, once the grass is actively growing. Note that some pre-emergent weed control also includes a fertilizer, so you do not want to over fertilize. For warm-season grasses, it’s best to fertilize in late spring as soon as the lawn greens up and is actively growing.\nRegardless of whether you have warm- or cool-season grasses, a spring application of pre-emergent herbicide can be very beneficial for your lawn. For control of annual weeds, such as crabgrass and foxtail, you want to apply the weed control as soon as the soil temperature consistently reaches 55 degrees. Often, your local extension office can help you understand the soil temperatures for your area. If you plan to perform any seeding to your lawn, you will want to choose your herbicide carefully, as most will prevent the germination of grass seed. According to the University of Nebraska Extension in Lancaster County, the only pre-emergent herbicide that can be used with new seeding is siduron, which is commonly sold as Tupersan.\nCool-season grasses can be planted anytime the ground is not frozen and the soil is not too wet. Then, as the ground temperature rises above 50 degrees, the seed will begin to germinate. The goal is to plant as soon as conditions allow to give the grass as much time to establish before the stress of summer heat bears down on it. Fall is a better time to plant cool-season grasses, so your focus for spring planting should ideally be on filling in bare spots. Warm-season grasses should be planted when soil temperatures are consistently in the 60s and all threat of frost has passed.\nYou may find that if your lawn went into the winter in a well-maintained condition, it may not need all of these steps to be ready for spring. Lawn maintenance you performed in the fall, such as aeration or dethatching, may or may not need an additional spring application. It all depends on the condition of your lawn coming out of winter. And if you maintain your mower in the fall, you won’t need to do another comprehensive service in the spring.\nSpring Maintenance Techniques for a Healthy Lawn— University of Nebraska Extension in Lancaster County\nSpring Lawn Care Tips— Lawncare.org\n5 Spring Lawn Care Tips— HowStuffWorks.com\nSpring Lawn Care Guide— TodaysHomeowner.com\nSpring Lawn Care Tips – The Red Light-Green Light Version— Michigan State University Extension', ""Goodbye stinking hot Summer, and hello lovely placid Autumn! You’ve no doubt noticed the cooler mornings and nights and chances are you’re enjoying a rest after the peak growing season in your garden.\nAutumn is great time for gardeners, as it lets us get in and tackle some of those jobs that were just too hard to justify during the peak of summer. If you’re not sure what you should be doing in your garden, then read on for our Autumn To Do List.\nAutumn Gardening Tips\nGrab some hand tools and do some cutting!\nAutumn is a top time for you to prune your shrubs and trees before the Winter time lull. This is the last real growth period for most plants for the next 4 months or so, so it’s really the time to get things looking the way you want them to until Spring.\nCutting back hedges and shrubs moderately, and removing dead wood is good activity, as it will get you the shapes you want, whilst leaving and promoting enough growth to sustain the plants over winter.\nIt’s important to note, to keep an eye out for flower buds when you’re pruning at this time of year. Lots of trees and shrubs set their Spring and Winter buds this early, and cutting them off will mean no flowers, and no one wants that!\nStart working on your lawn\nI like to do 3 things to my lawn in Autumn. Cut, feed and treat. In Autumn the warm days and cool nights mean your grass has begun to slow down, but is still actively growing in the root system, meaning a little bit of tough love is quickly repaired and the risk of damage from a heat spell is less likely.\nThe first is bringing the height right down in the early stages of Autumn. By gradually mowing your lawns lower and lower at a high frequency (about the same as you were doing in Summer) you can seriously drop the length of your grass to a more manageable height if it got away from you during Summer.\nGetting lower will also help knock a bit of thatch out of your turf which will make it less ‘spongey’ and make your Spring time turf reno a heap easier.\nSecond is feeding. Spring is the best time for fertilising lawns, but a quick feed in Autumn will help kick a bit of last minute growth into the root systems and help repair any damage the hotter days caused, as well as give the lawn the best chance to make it through Winter looking good.\nLastly, Autumn is a prime time to test your pH and treat your soils. Grab a soil pH testing kit and moisture meter from any hardware store and test a few different spots in your lawn.\nDepending on the results, you might need to add some stuff to your lawn to level out the pH to around neutral (7). If your pH is low, add some ground limestone.\nIf it’s high, add some Ferrous sulfate, which has the added bonus of boosting the iron levels in your lawn.\nGet rid of those weeds\nAutumn is the perfect weather to get outside and pull some weeds. The cooler weather means that you wont fry in the sun, but also means that most weeds should be slowing down as well, and be very close to setting seed.\nIf you’ve kept on top of the weeds during the Summer, now is the time to get out and really dig them out. Pull any about to seed, as well as any little seedlings that are sprouting up.\nFeel free to cultivate your soil a bit too, because you’re about to have a plentiful supply of mulch…\nTime to do some raking\nAutumn is all about deciduous trees. They’ll be starting to turn and drop by now, and you know it’s only going to get worse.\nSee our review on the best garden rakes in Australia.\nGetting a jump on fallen leaves means the mess becomes more manageable, and you wont lose patches of grass or garden that have been smothered.\nThrow your leaves into your compost bin, or mulch them up with your mower and send them straight onto your garden beds.\nNote from Jim: It’s that time of year again where the days get shorter, and colder. This week I’ve pulled together some general tips and ideas for how to keep your garden in the best possible shape at this time of year, heading towards winter.\nHope you find this helpful, let me know in the comments below. It’s the end of summer. As the leaves change color and the grass grows slower, you can feel nature preparing for the long winter ahead.\nAutumn is a time of preparation. Each plant and each blade of grass on your property is storing nutrients and energy so it can survive the winter and blossom in all its majesty come spring.\nAdd in some mulch\nMulching every autumn is important for your plants. Mulching helps retain soil moisture and prevent erosion while improving overall soil quality. If you want to make your own mulch, fall is the best time to do it.\nRake up your fallen leaves before they get wet and sticky and chop them up into smaller bits. You’ll need about three inches deep of mulch to sprinkle around your shrubs and flower beds.\nMow Your Lawn\nYou’ll need to continually mow your lawn, even in autumn, to keep it vibrant and healthy. The recommended autumn height for lawns is three inches.\nThis height helps lock moisture inside every blade of grass. Leave a third of your clippings on the lawn to act as a natural, organic fertiliser.\nWait! If you don't want to do neither, install a robot lawn mower instead.\nFertilise Your Lawn\nDon’t forget to feed your grass and other plants every autumn because this is in fact the most important yearly feeding they will get for a couple of reasons.\nFirst of all, your lawn needs to recover from any damage it may have sustained from the summer sun.\nSecondly, it needs to prepare for the winter by storing up nutrients, just like a bear going into hibernation. Feeding your lawn in Autumn will take care of both.\nBecause plants – and that includes weeds – are busy absorbing energy and nutrients in the fall, apply an organic weed killer that the weeds will also absorb and you’ll guarantee they won’t return next spring.\nYou have many natural weed killers on the market to choose from or you can make one yourself. If you buy an organic weed killer, the main ingredients to look for are citrus or corn gluten.\nBecause these types of weed killers cost a bit more than traditional chemical herbicides, you may want to make your own natural weed killer.\nTo mix your own organic weed killer, dilute vinegar in water if your lawn is sensitive; but for tougher, sturdier lawns, you can use the vinegar by itself.\nApply the solution directly to the weeds. You may have to use more than one application to completely kill the weeds.\nFor bigger lawn owners, be sure to check our review on the best garden sprayers in Australia.\nBe sure you are planting native, Australian perennials that provide shelter and food sources for native wildlife, such as bees and birds. In the autumn, leave these perennials alone and wait until spring to cut them back and promote new growth.\nThis technique will help the critters who help our lawns, our gardens, and the world’s food supply to flourish.\nIt’s important to remember that although things seem to slow down in autumn, our lawns and gardens are as busy as ever and they need your continued, loving care to stay healthy and vibrant.\nGot any other tips for Autumn gardening? Let us know below!""]"	['<urn:uuid:06023421-f903-4893-853d-80fa8b112121>', '<urn:uuid:94c844eb-80f6-4ad7-bed6-2bf5001e13b4>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T02:03:54.148118	29	115	2108
42	I smoke cigarettes and grow herbs in my garden. Why do herbs like cilantro and parsley seem unaffected by tobacco mosaic virus while other plants get infected, and how is this related to the virus's core structure?	Herbs like cilantro, parsley, and oregano show no effects of tobacco mosaic virus because their different amino acids and starch compounds offer little for the virus to feed on. This relates to the virus's core structure, which consists of viral nucleic acid (DNA or RNA) surrounded by a protein shell (capsid) - this structure needs specific cellular conditions and materials to successfully replicate, which these herbs don't provide.	['General Properties of Viruses\nWhat is virus?\nThe smallest infectious and acellular microbe.\nConsisting only one kind of nucleic acid (DNA or RNA), and which obligately replicate inside host cells.\nThe complete mature viral particles.\n(The intact infectious virus particles.)\n- The simplest: acellular microbes contain either DNA or RNA\n- The smallest: Pas through 0.2μm filters\n- Obligatory intracellular parasites.\nI. Size, shape and structure\nThe unit of measurement nm\nComparative sizes of virions and bacteria\n1. Staphylococcus aureus\n5. Bacteriophage of E. coli\n6. Influenza virus\n8. Encephalitis B virus\nTobacco mosaic virus: rod-shaped\nVSV (Vesicular stomatitis virus): bullet-shaped\nBacteriophage T4: tadpole-shaped\nEbola Virus: filamentous shape\nCore: Viral nucleic acid (DNA or RNA)\nCapsid: Protein shell\ncapsomers (morphological subunit)\npolypeptide molecules (chemical subunit)\nCore + Capsid → nucleocapsid\nSize, shape and structure\nOthers: enzymes, etc.\ne.g. Retrovirus has reverse transcriptase\nSymmetry of viral nucleocapsids: Is decided by arrangement of capsomeres\n(e.g., tobacco mosaic virus)\n(e.g., poxviruses )\nViral nucleic acid: ssDNA, dsDNA, ssRNA, dsRNA\nprotection, mediate the attachment of virue to specific receptors on host cell surface\ndetermine species and organ specificity\nimportant antigens, superantigen\nHuman Hepatitis D\nNOTE: a single circular RNA molecule without a protein coat which mainly cause plant diseases.\nProteinaceous infectious particle\nBovine spongiform encephalopathy (BSE)\nNOTE: infectious agents composed of a single glycoprotein with MW 27-30 kDa.\nIn host cell, virus replicates its nucleic acid and synthesizes\nits proteins, then assembles them to form progeny viral\nparticles that are released by budding or cell lysis.\n- Adsorption /Attachment\ni. Adsorption / Attachment\nSpecific binding of a viral attachment protein (VAP) with a receptor on the surface of host cell;\nVAP (on virion ) --- viral surface protein\nSpike – enveloped virus\nCapsid protein – naked virus\nViral receptor (on host cell)\nGlycoprotein, carbohydrate or glycolipid\ne.g., CD4 (HIV), CD46 (measles virus), Sialic acid (influenza virus)\nSome enveloped viruses\nMost naked virus\nB. Direct fusion of cell membrane with viral envelope:\nOnly enveloped viruses\nC. Nucleic acid translocation:\nSome bacteriophages and naked virus\nThe process of removing capsid and releasing viral nucleic acid into the cytoplasm;\nAcidification of the content of the endosome\nProteases are needed;\n- Viral genome replication\n- Viral protein synthesis\ndsDNA; ssDNA; dsRNA; +ssRNA; -ssRNA; retrovirus\n+ssRNA with DNA intermediate in life cycle (HIV);\ndsDNA with RNA intermediate (HBV);\n+ssRNA virus (Poliovirus, HAV)\nViral genomic RNA serve as mRNA;\nEnzymes for replication are made after infection, not carried in virion;\n(Extracted) Viral genomic RNA is infectious\n-ssRNA virus e.g., influenza virus\nVirion carries RDRP;\nFirst step: Transcription of viral genome;\nExtracted -ssRNA not infectious;\nNaked virus: capsid + viral genome → nucleocapsid (virion, complete structure)\nEnveloped virus: capsid + viral genome → nucleocapsid (incomplete structure)\na. DNA viruses (except poxvirus): cell nucleus;\nb. RNA viruses and poxvirus: cell cytoplasm;\na. assemble as empty shell (procapsid), then viral genome fill in.\nb. Viral capsomeres array around the viral genome to form helical\nThe process of progeny viruses getting out of host cell.\nNaked viruses:released by cell lysis.\nEnveloped viruses:usually released by budding.\nDuring budding enveloped viruses acquire their envelope.\nDefective measles virus: release from cell to cell via cell bridges.\nenveloped virus replication （1）\nenveloped virus replication （2a）\nenveloped virus replication （2b）\nenveloped virus replication （3）\nenveloped virus replication （4）\nTwo aspect factors:\nnon-permissive cells → Abortive infection\nare genetically deficient and incapable of producing infectious progeny virions.\ncan supplement the genetic deficiency and make defective viruses replicate progeny virions when they simultaneously infect host cell with defective viruses.\ne.g., HDV & HBV\n- Defective viruses lack gene(s) necessary for a complete infectious cycle;\n- helper viruses provide missing functions;\n- 100:1 (defective to infectious particles)\n- DIP (defective interfering particle) : When the defective viruses can not replicate, but can interfere other congeneric mature virion entering the cells, we call them defective interfering particles (DIP).\nVirus infection which does not produce infectious progeny because the host cell cannot provide the enzyme, energy or materials required for the viral replication.\nThe host cells that cannot provide the conditions for viral replication.\nThe host cells that can provide the conditions for viral replication.\nIII. Viral interference:\nWhen two viruses infect simultaneously one host cell, One type of virus\nmay inhibit replication of another type of virus.\nRange of interference occurrence\n- between the different species of viruses;\n- between the same species of viruses;\n- between the inactivated viruses and live viruses.\nMain mechanisms of viral interference:\na. One type of virus inhibit or prevent subsequent adsorption and penetration\nof another virus by blocking or destroying receptors on host cell.\nb. The competition of two viruses for replication materials, e.g., receptor\npolymerase, translation initiation factors, etc.\nc. One type of virus may induce the infected cell to produce interferon that\ncan prevent viral replication.\nThe mechanism of IFN function\nSignificance of viral interference:\na. Stop viral replication and lead to patient recovery.\nb. Inactivated virus or live attenuated virus can be used as vaccine to\ninterfere with the infection of the virulent virus.\nMay decrease the function of vaccine when bivalent/trivalent vaccine is used.\nJust for your practice see the answers at the end.\nFill in the blank\n1-The surrounding protein coat of a virus is called the _______ and it is composed of protein subunits called _________.\n2-Viruses that are only covered with a protein coat outside viral genome are called ______ viruses, while those that have an additional lipid-containing membrane covering are called ________ viruses.\n3. The general steps of the viral replication cycle include ( in the order of their occurrence) ___________________, ___________, ____________, __________, _________________.\n3-Attachment, Penetration, Uncoating, Biosynthesis, Assembly and release.', 'How to Detect Tobacco Mosaic Virus in Your Plants\nAt one time or another, many growers who use tobacco products end up introducing tobacco mosaic virus into their gardens—sometimes without even knowing it! So is it really something to worry about?\nYou take the time to plant the seeds, provide them with enough light and water, and watch proudly as they germinate and become stronger, branch out and fill in the spaces between each stem. Then one day you notice a brown spot, then another and another, until your entire crop is covered with them. The leaf tips start to curve to the left, then downward, turning even browner until they dry to a crisp and crumble to the touch. You watch as even the fruits and flowers fall victim to a mysterious plant ailment\nYou start to wonder, “Why is this happening?” as you try different ways to solve the problem. You notice nothing seems to stop this destructive menace from covering your entire garden. Frustrated, you give up and remove the dead plant leaves, while cigarette smoke lingers nearby.\nIf this scenario sounds at all familiar, you have most likely introduced tobacco mosaic virus into your garden and it has now destroyed your plants. This virus cannot be treated, cannot be removed and cannot be stopped once discovered.\nAbout Tobacco Mosaic Virus\nMany growers who use tobacco end up introducing tobacco mosaic virus into their gardens at one time or another. The virus attaches itself to everything that it comes in contact with, including the smoke from the burned tobacco, and nothing is able to destroy it.\nTobacco mosaic virus comes from the tobacco plant and has been a problem of tobacco growers for decades. The virus attaches itself to the inner cell walls of the plants’ tissue, first forming on the outer layers of the leaves, and then slowly taking over the entire plant. Brown start to form on the plant leaves as they lose moisture and dry. The leaf tips turn towards the left and curl either under or upwards before drying out. This is followed by curling of the fruits or flowers themselves.\nThe vines, stalks and even the roots themselves shrivel away to nothing, leaving only the now dormant virus cells that are waiting to be transplanted into another garden. Most growers chalk these symptoms up to other causes, like deficiencies in the soil, water or nutrients, which means infections go largely undetected.\nThere are 11 known types of the mosaic viruses, all of which received their names from the mosaic patterns they leave as they destroy your garden. Tobacco, tomato, squash and zucchini mosaics are the four most common, with tobacco mosaic being the worse. Infected tomatoes, squash and zucchini plants are still safe for consumption if they are not too far gone (decayed).\nEven the toughest tomatoes show the long thick scars that form along the outer skin, making them look like a burn or wound that is mostly healed. These tomatoes are still safe for consumption and are even found in stores.\nSquash can also withstand most of the mosaic virus abuse, both inside and out, and are safe for consumption if they are not infected too much. You have to look out for the thick, open scars that will appear on the outer area near the stem. Infected zucchinis, on the other hand, start to dry up right at the flower stems while the skin erupts and splits open. These zucchinis end up being worthless.\nTobacco mosaic virus can infect any plant life with various results, yet perennials don’t appear to succumb to its effects. Different amino acids and starch compounds offer little for the virus to feed on, therefore making some plant species appear to be immune to its effects. Herbs such as cilantro, lemongrass, parsley and oregano show no effects of the virus, but are generally cultivated long before you see anything wrong. Plants that produce bulbs, such as onions, potatoes, garlic and the like, must also be harvested before the effects are noticed. However, since the virus attacks the root mass last, this is normally not an issue when it comes to consumption.\nPreventing your garden from contracting tobacco mosaic virus is simple: don’t touch, smoke or use any sort of tobacco product. The virus can live in any soil or soilless medium. Tools used to till the soil, pH probes, hoses, buckets and containers—even indoor lighting fixtures—can carry the virus and spread it to the next crop. If your indoor or outdoor garden becomes infected with tobacco mosaic virus, you basically need to start over, making sure you’ve removed it from all areas where it may have lived.\nYour equipment may be spreading the virus as well. If an infection returns after you’ve removed the affected plants and cleaned your tools, your equipment may need to be replaced.\nFor example, containers will show a thick, brown scale-like substance that won’t come off no matter how hard you scrub. Hydroponic and aquaponic systems show this scale buildup in the lines. Replacement of outdoor soil helps, but doesn’t always eliminate the virus completely.\nSo why all the fuss if tobacco mosaic virus has always been going on, yet causes no ill effects to consumers? Easy—it kills your garden, reducing the amount of crops that could have been used or sold, and it gets stronger and stronger every time it surfaces. So take charge and do what it takes to prevent this virus from infecting your garden.']	['<urn:uuid:a046236c-8ab8-43ea-9b7d-2c32355cb959>', '<urn:uuid:a03d05fe-d8ef-4fd6-a61a-be44c23816d0>']	factoid	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-13T02:03:54.148118	37	68	1862
43	As a legal writer, what's the purpose of typography in briefs and how are briefs formatted?	Typography is the visual component of written work that enhances arguments without distracting from them - similar to how lawyers consider their appearance in court. For legal briefs, specific formatting is required by court rules, including font face and size, line spacing, page size, margins, section dividers, tables of contents, and binding methods. If a brief doesn't follow proper formatting, it will be rejected by the court. Briefs must be well-organized under headings covering facts, procedural history, judgment, analysis, and dicta.	['By Tyler D. Helmond, Voyles Zahn & Paul\nAttorney and Author\nHe is a graduate of Harvard University and the University of California Los Angeles School of Law. He is an attorney. He is the typeface designer behind Equity, a font for lawyers. And he is the author of “Typography for Lawyers.” He is Matthew Butterick, and he has been served with interrogatories.\nQ: For lawyers who have not read your book, what is typography and why is typography important in legal writing?\nA: Typography is the visual component of the written word. And since lawyers depend on the written word, good typography is part of good lawyering. Obviously, nothing is more important than the content of the writing. But it’s like an oral argument in court. Judges and juries aren’t supposed to decide a case based on the lawyer’s clothes or speaking style. But lawyers still give thought to those issues, because we want the presentation to enhance the argument, not distract from it. It’s the same on the printed page.\nQ: What are some of the most common mistakes lawyers make with typography?\nA: First, the belief that there’s some special canon of typography rules that apply to legal documents. I consider lawyers to be professional writers who have access to professional-quality typesetting equipment (i.e., modern word processors and laser printers). Therefore, the rules of professional typesetting should apply. Much of what we think of as traditional legal typesetting is an accumulation of bad habits handed down from high-school typing class. But the typewriters are gone. So the typewriter habits should go too.\nBeyond that, the overuse of ALL-CAPS would be my biggest complaint with legal typography. It’s an example of what I call self-defeating typography. The reason you’d put something in caps is to emphasize it. And for less than one line of text, it works. But when you have a whole paragraph of caps, it’s harder to read. So what do readers do? Do they pay more attention? No. They skip it. Which is the opposite of what you want. Lawyers sometimes defend this habit by saying “There’s a law that says you have to use caps.” To which I say, “OK, show me.” And no one ever can.\nQ: One of the misconceptions you dispel in your book deals with court rules. Many lawyers assume court rules prevent good typography when they often do not. But sometimes they do. If you were in charge of making a model rule on typography, what would it look like?\nA: It’s surprising how many lawyers haven’t read their court rules about document formatting. They just imitate what they see other lawyers doing, on the assumption that those other lawyers have read the rules. But no, they haven’t either.\nTrue, there are some restrictive court rules out there. But there’s always latitude to make the typography better. And it’s not about being different for the sake of being different. Court rules, on their own, don’t produce good typography. They just set minimum standards. The rest is up to you.\nAs for a model rule, in general I’d want it to be lenient rather than restrictive. A big improvement would be to move from page limits to word limits. Most court rules about type size and line spacing were set up to support page limits by ensuring a consistent number of words per page. This made sense in the typewriter era. It makes no sense in the computer era.\nA: Lawyers working in page-limit jurisdictions told me that while they wanted to switch away from tired old Times New Roman, it was the most efficient font in terms of fitting words per page, so they were reluctant to give it up. And it’s true — Times New Roman was originally designed for newspaper typesetting, so it’s a bit narrower than most traditional typefaces that you might see in a magazine or book. So my goal with Equity was to make a font family that was as efficient as Times New Roman on the page, but better looking.\nConcourse was designed as a sans serif companion to Equity, with the goal of being equally practical. With sans serifs, what you often see is that they look good in all-caps, or good in body text, but not both. So I tried to find a middle ground.\nQ: How ubiquitous have they become?\nA: I wouldn’t call them ubiquitous, but they’re off to a very good start. Thousands of lawyers are using them. But I’m not in a rush. A good typeface has a lifetime measured in decades.\nQ: “Typography for Lawyers” has been hugely successful, with one of best endorsements imaginable coming from Bryan Garner – “If Matthew Butterick didn’t exist, it would be necessary to invent him.” What are you planning for the follow up?\nA: Bryan has been a true friend to the project, which I’m tremendously grateful for. Reading his “Modern American Usage” start to finish made me want to write a usage guide for typography.\nAs for the follow-up, it probably wouldn’t surprise you to hear that many of my readers are nonlawyers. So I’m working on a new version of the book aimed at them.\nQ: Your career has taken more interesting turns than most: from typeface designer, to website developer, to lawyer and author. If you had to give a commencement speech, what would you suggest to the graduates as the key to success?\nA: Well, I think of my career as less peripatetic and more cumulative than it may appear to others. I’m interested in this broad theme of how we create the written word. It leads to new investigations, which I try to combine with what I’ve already learned. Sometimes with unexpected results. For instance, I wouldn’t have expected that writing a book about typography would improve my type-design skills. But it did.\nThe best advice I got before I went to law school was to find out which professors had the best evaluations, and then take their classes, regardless of the topic. But I think that’s good advice out in the world, too. Kind and smart people will make any topic interesting. But they are rare. So if you meet one, you should make the most of what they have to share with you.\nQ: In your experience, is there any overlap between excellent design and excellent lawyering? You have said “solving problems is the lowest form of design.” Is there a lesson there for lawyers?\nA: Sure. I think design and lawyering are similar in that they both have this huge layer of mechanics and production interposed in this essentially human interchange. The challenge is keep bringing the human aspects forward, and not let them get lost in the noise. We could say that filing briefs is the lowest form of lawyering because a lawyer’s job is not to file briefs. It’s to represent clients. The brief is just the vehicle.\nSometimes lawyers say to me, “I see, typography is important because it makes things pretty.” Yes and no. True, we’re not trying to make things uglier. But the reason to “make it pretty” is to make the most of the limited attention we get from our readers. We need that attention to best serve our clients. Once a reader has lost interest, we’ve also lost our chance to persuade.\nQ: What are the last three albums you added to your iPod?\nA: I’m a big streaming-music user these days, so based on the phrasing of your question, I don’t need to confess to all the guilty pleasures I keep over there. The last three albums added to the iPod were “Silver Age” by Bob Mould, a six-disc compilation of 70s soul, and “Cargo” by Men at Work.', 'In the legal system, a brief is a written document advising the court of the legal reasons for the lawsuit or other legal action. The legal grounds for the action must be spelled out according to the party’s reasoning, the facts of the case, and the laws and regulations that apply. In essence, a legal brief makes an argument as to why the party submitting the brief should prevail in the action. To explore this concept, consider the following definition of brief.\nDefinition of Brief\n- A concise statement of points of fact or law used in a legal action\n- A written argument submitted to a court of law\n- A written outline of all the information and arguments on one side of a legal controversy\n1250-1300 Middle English bref\nWhat is the Purpose of a Brief\nIn a legal matter, a brief is a written statement of facts and the legal issues which form the basis of the lawsuit or other action. In the brief, the party, or attorney representing that party, submitting the document, attempts to convince the court to rule in its favor. Legal briefs, sometimes referred to as “points and authorities,” are generally required to be submitted in a certain format, as defined in the court’s rules, which are available from the clerk of the court in which the matter is filed.\nTypes of Legal Brief\nLegal briefs are used in a variety of legal situations, and while many of them are written documents actually submitted to the court, some briefs are written to organize the information for an attorney or other legal professional working on the case. Commonly used legal briefs include:\n- Trial brief – a formal written statement presented to the court, outlining the party’s position in the case, whether a civil lawsuit, or criminal matter. A trial brief may also provide needed information to the judge, such as specialized terminology used, or procedures specific to the issue in a technical case.\n- Legal brief – statements presented to the court as part of a pre-trial motion, such as a motion questioning the admissibility of evidence or testimony.\n- Merit brief – also referred to as a “brief on the merits,” this brief is submitted to an appeals court, and explains the wrongs, or rights, of the prior decision on the case. A merit brief may also be submitted to the Supreme Court when appealing a decision of an appellate court. Such a brief explains why the decision of the prior court of appeals is flawed.\n- Amicus brief – amicus curiae, which translates literally to “friend of the court,” refers to a brief submitted to a Supreme Court by a party not directly involved in the case, but which has a strong interest in its outcome. An amicus curiae brief, commonly referred to simply as an amicus brief, advises the court of additional information or arguments that are very relevant to the case, and which the court might want to consider in making its decision. A party not involved in the case directly must request permission from the court to file an amicus brief.\nABC school district in California files a civil lawsuit against the federal government for enacting a system of fining districts whose students’ mandatory test scores fall below a certain level. These fines come in the form of decreased funding for the following school year. ABC school district is responsible for educating a large number of children of financially troubled families, many of which tend to have difficulty learning at the same pace as other children.\nThe case has been appealed to the U.S. Supreme Court, where ABC school district argues that the government’s system of punishing school districts in such a situation actually serves to deny the children the quality education they deserve.\nOther school districts throughout the state of California, and many in other states, have monitored this situation as the lawsuit moved through the court system, as the policy has a broad effect considered to be harmful by many districts. Once the matter has been accepted by the Supreme Court, XYZ school district asks for, and is granted, permission to submit an amicus brief.\nWhile XYZ school district is not directly involved in the lawsuit, it has gathered information and statistics from other schools near and far, which may be helpful for the court in reaching its decision on the matter. An amicus brief may be written in an attempt to sway the Court to either side of the issue, depending on the party’s point of view.\n- Appellate brief – a brief submitted to the court at the appeals level. The appellate brief advises the court of the basic circumstances of the case, and the legal basis on which the party is appealing the trial court’s decision, or why the court should disregard the party who is appealing that decision.\nWhat is an IRAC Brief\nAn IRAC (Issue, Rule, Application, and Conclusion) brief is usually an internal document prepared by a paralegal or junior attorney, and ultimately used by the primary attorney on a case to guide him in preparing and arguing the case. The acronym IRAC provides an organized method of researching and organizing information on a case. An IRAC brief is commonly a short, one-page review of the case, clearly outlining the answers to the questions:\n- What is the issue or legal question at hand in the case?\n- What rule of law applies to the issue?\n- How are the rules stated in number two above applied to the specific facts of the issue?\n- What is the conclusion to the legal issue when the rules are applied to the facts of the case? The conclusion of an IRAC may not introduce any new rules or applications.\nWhen preparing an IRAC brief, the preparer must have access to all of the facts of the case, as these facts are fundamental to each step of the IRAC process. The issues of the case can only be divined from the facts of the case. Likewise, the facts bring to light the most pertinent rules of law that apply to the issues, and analyzing the case requires a competent interpretation of the facts and the rules of law.\nFormatting of a Legal Brief\nAlthough the term brief was likely intended to refer to a brief summary or statement of the case, one which would be shorter than an oral argument, modern times see briefs that are quite lengthy. This is because legal briefs of varying types are used to specifically outline the party’s position, the legal arguments, and to summarize case precedent. The correct formatting of a legal brief depends entirely on the rules of the court in which the brief is to be filed.\nBecause the courts receive mountains of legal documents and legal briefs, requiring them to be organized and formatted in a specific manner helps the judge quickly identify pertinent information at any point in the process. Most briefs organize the voluminous information under various headings, such as:\n- Facts of the case – an outline of the facts, and reference to where those facts are more specifically detailed.\n- Procedural history – a statement of the events that have already occurred within the court system, which have ultimately brought the case before the present court.\n- Judgment – a statement of how the previous court, usually the trial court, decided the case.\n- Analysis – a review of the rationale given by the court for its decision, or by the parties for their positions on appeal.\n- Dicta – the opinions or reasoning given by the previous court, which are to be considered authoritative, but not binding.\nA brief filed with the appellate or Supreme Court should be well-organized and include a table of contents, as well as an index if necessary to aid those reading it to find pertinent information easily. It is commonplace for legal briefs, especially lengthy briefs, to include footnotes and other annotations.\nThe rules of the court in which the brief is to be filed spell out the actual formatting of all briefs to be submitted. These rules include such formatting issues as font face and size, line spacing, page size, and margins. Correct formatting must also observe the rules about the type and color of section dividers, the inclusion of tables of contents and indices, how many holes should be punched in each brief, and where they should be located, as well as whether the brief should be stapled or bound by a spiral device.\nBriefs having to do with different areas of law or legal issues are often required to have a cover of a specific color, for the purpose of aiding the court staff to keep the high volume of briefs organized. Finally, the court’s rules specify how many copies of the brief, all formatted and bound as outlined, must be submitted to the court. The number of copies depends on how many judges sit on the panel that will review the case. Each judge or justice must receive a copy of every brief, and the court will not make additional copies.\nIn most courts, if a brief is submitted without the proper formatting, proper cover color and binding, or without enough copies, it will be rejected. The party will then need to correct the errors and make another attempt to file. There are strict time limitations on filing such documents, and this can become a problem if the proper format is not discovered until filing time.\nTips for Writing a Successful Legal Brief\nWhile any case can be eloquently outlined with every conceivable point of law and precedent, providing the court with all of the information that could possibly aid in its decision, there is a lot to be said for submitting a brief brief. In law, as in many areas of life, less may be more in these situations. Professional educators in the legal field have offered tips to writing a successful legal brief, or one which is more likely to be read from cover to cover.\n- Choose a Format – this refers to the format in which the information inside the brief is organized. While there are a number of ways the information can be presented, it is important to choose a format that best illustrates the merits of, or problems with, the specific case.\n- Choose an Appropriate Caption – every brief should begin with the case name, the court in which it was tried, the year it was decided, and the page on which it is listed in the casebook. The court may have other requirements for brief captions.\n- Identify the Facts of the Case – identify each fact of the case in an outline format for easier reading. In an appeals brief, discuss only the facts that are relevant legally to the appeal. Relevant facts are those that directly impacted the outcome of the case.\n- Outline the Procedural History – this section should identify all of the legal events that have taken place so far in the case. These include all court actions, including motions, hearings, and trial, that have occurred leading up to the case being brought to the court in which the brief is being filed.\n- Specify the Issues in Question – describe in detail the issues and legal questions for which the case is being brought before the current court. In this section, it is a good idea to break down these issues into their most basic parts, discussing how the law or rules apply to each.\n- Answer each Issue Concisely – provide a brief answer to each issue brought to the current court. This is best done with a yes or no answer, followed by a couple of sentences stating the legal principle relied on by the court to reach its decision on that issue.\n- State the Prior Court’s Rationale – for each decision, or “holding,” made by the lower court, discuss the rationale given by the court. To do this coherently, describe each step of the court’s reasoning for each issue, one at a time.\n- Describe the Final Disposition of the Case – Specify what the court decided, and any orders that came of the decision. If the prior court was an appellate court, state whether the court affirmed or overturned the trial court’s decision, and what other instructions the court has issued.\n- List any Concurring or Dissenting Opinions – While it is necessary to discuss the prior court’s final decision, as well as its reasoning for that decision, a brief submitted to an appellate or Supreme Court may make use of other opinions given in the case. These include both concurring and dissenting opinions of members of the panel of judges that reviewed the case. These opinions often help the higher court, as well as the parties, see the court’s decision from a different angle.\nRelated Legal Terms and Issues\n- Appellate Court – A court having jurisdiction to review decisions of a trial-level or other lower court.\n- Civil Lawsuit – A lawsuit brought about in court when one person claims to have suffered a loss due to the actions of another person.\n- Concurring Opinion – A written opinion by a judge which agrees with the overall decision made by a panel of judges, but which has different, or additional reasons for his decision.\n- Dissenting Opinion – A written opinion by a judge who disagrees with the majority opinion of the panel of judges, giving his reasoning for dissent.']	['<urn:uuid:f83d37a9-8dec-4015-9331-1f08ff2e913c>', '<urn:uuid:0dca4637-ce70-4083-b65a-fa80e489cb0f>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T02:03:54.148118	16	81	3567
44	I run a small business selling imported products and I'm concerned about compliance. What kind of product information needs to be on labels, and how does UL monitor companies to ensure they follow the rules?	Product labels must include various elements depending on the product type: warnings for hazardous materials and allergens, flammability warnings, ingredient lists for food/drugs/cosmetics, tracking information like production date and batch numbers for children's products, storage instructions, and care/use instructions for textiles. For UL compliance specifically, UL conducts quarterly unannounced visits to inspect materials, processing records, and work in progress. Non-compliance can be found if unauthorized materials are used, processing conditions fall outside approved ranges, or if there's inadequate tracking of materials and conditions. Both labeling violations and UL non-compliance can result in significant fines and business disruption.	"[""What You Don't Know CAN Hurt You – and Could Destroy Your Business\nWhat do the Federal Trade Commission (FTC), Consumer Product Safety Commission (CPSC), Federal Drug Administration (FDA), Department of Agriculture (USDA), and Environmental Protection Agency (EPA) have in common? Each is, of course, a U.S. government agency with myriad regulations, but each also can impose millions of dollars in fines and civil penalties for violations and completely disrupt business and customer relations. These product regulations apply to both U.S.-made and products that are imported to the U.S., and include requirements for labeling, advertising, product registration and certification, and reporting. Many obligations, particularly the mandatory reporting of product defects, apply to manufacturers, importers, distributors, and retailers.\nAmong the many statutes and regulations, and accompanying implementing regulations, guides, and standards, are:\n- Federal Hazardous Substances Act\n- Federal Insecticide, Fungicide and Rodenticide Act\n- Fair Packaging and Labeling Act\n- Organic Foods Production Act\n- Textile, Fur, and Wool Products Labeling Acts\n- Hazard Communication Standard\n- Toxic Substance Control Act\n- Resource Conservation and Recovery Act\nIf you don’t know what these laws address, you’re in good company. Unfortunately, many businesses learn the hard way, as illustrated by these examples:\nIn 2013, Amazon, Macy’s, Sears, and Leon Max, Inc., agreed to pay $1.26 million in civil penalties for violating the Textiles Products Identification Act by claiming that textile products were made of bamboo when, in fact, they were made of rayon. The FTC found such claims deceptive even though bamboo was the primary or sole source of the cellulose that was processed into rayon.\nThere are many such seeming anomalies in product regulations. For example, “made in USA” claims, whether explicit or implied, may only be made when “all or virtually all” of the product’s components are from the U.S., and “all or virtually all” of the manufacturing, processing, packaging, or assembly took place in the U.S. too. The site of final assembly or processing must be in the U.S., and the cost of the foreign parts or processing must not be a majority of the total manufacturing cost. Claims that products are organic or environmentally-friendly face similar requirements as well.\nProduct-specific labeling requirements also include warnings (hazardous materials, allergens, flammability), ingredient lists (food, drugs, pesticides, cosmetics), tracking information, such as the date and place of production or batch number (children’s products), storage and first aid instructions, preparation for use (medical devices), fiber content, and care and use instructions (textiles).\nIn December 2015, LifeLock, an identity theft protection company, agreed to pay $100 million for violating an FTC order that prohibited the company from engaging in deceptive advertising regarding the security of consumer data, among other things. The FTC claimed that LifeLock falsely advertised that it would send alerts “as soon as” it received a report of identity theft and that it used the same high-level safeguards used by financial institutions to protect consumer data. Both of these claims were found to be deceptive.\nOften, whether a product claim goes too far (“as soon as”) is not always intuitive, and companies need to be aware of the guidance and precedent of the agencies with authority over marketing specific products. The FTC, the FDA, and even the Federal Communications Commission (FCC) have authority to propose and enforce regulations covering product and services advertising. The FTC most frequently investigates whether advertising is truthful and not deceptive, i.e. it must not contain a statement or omit information that is material and likely to mislead a reasonable consumer. Advertising claims must also be supported by evidence, such as testing or scientific or consumer studies. The FCC oversees advertising on radio and television network broadcasting, and enforces regulations requiring disclosures of paid product placements, subliminal advertising, and obscene or indecent material. Special regulations apply to advertising aimed at children, as well as advertising of tobacco and alcoholic beverages.\nCertification and Reporting\nIn March 2016, a Chinese company agreed to pay a record $15.45 million fine to CPSC for knowingly failing to report to the Commission (within 24 hours) a defect in its dehumidifiers that posed an “unreasonable risk of serious injury” due to the dehumidifiers overheating and catching fire. And the EPA recently shut down an Oregon company’s sales of a product called “Mold Manager” that prevents the growth of mold, mildew, algae, and moss. Not only was “Mold Manager” not labeled properly, but the product, which qualifies as a pesticide, had not been registered with the EPA under the Federal Insecticide, Fungicide, and Rodenticide Act.\nThe costly consequences in each of these examples could have been avoided with timely consultation about the applicable rules. Product defects and consumer injuries generally must be disclosed right away to the relevant government agency. In addition, several categories of products, such as pesticides, require advance registration or certification. Safety products, such as fire extinguishers, automatic sprinkler systems, scaffolding, and hoisting machines, must receive a Nationally Recognized Testing Laboratory certification if used in a workplace. A wide range of children’s products, from infant carriers to sleepwear, require third-party safety testing at accredited laboratories. Other products require instead a General Certificate of Conformity certifying compliance with various FTC Safety Standards, including adult bicycle helmets, bunk beds, lawn darts, certain apparel, and rugs and carpets.\nAlthough these enforcement examples may seem extreme, there are thousands of similar stories of lesser fines and near-misses. Those cases nevertheless generate significant costs to the company in time spent responding to these enforcement actions, and the wasted spending on non-compliant labeling and advertising, which might have been avoided with regulatory guidance prior to product launch."", 'Most electrical consumer products in the US have a UL (Underwriters Laboratory) listing, which means that the product meets certain safety standards. In some cases it may be a requirement for a product to be UL listed, in other cases it becomes a sales and marketing decision. UL requirements are driven from the final product down to the component level. The requirement for flexible circuit recognition is driven by the product manufacturer as flexible circuits themselves are not sold directly to the consumer. The following are some common questions and answers relating to UL recognition.\nWhat is UL?\nUL stands for Underwriters Laboratory and is a global organization that provides safety standards for consumer products.\nWhat is the difference between UL listed, UL classification and UL recognized? (source)\nUL Listing means that UL has tested representative samples of a product and determined that it meets UL’s requirements. These requirements are often based on UL’s published and nationally recognized Standards for Safety. A UL listing can only be applied to the final product as sold to the consumer.\nUL Classification typically means that UL has tested and evaluated representative samples with respect to certain properties of the product. Product that passes flammability testing would be granted a Flammability classification mark.\nUL Recognition means that UL has evaluated components or materials intended for use in a complete product or system. These components are intended only for end-use products that may be eligible for UL certification. Since most flexible circuits are components, it would be technically incorrect to state that a flexible circuit is UL listed, it can only be UL recognized.\nHow do you get a Product UL Recognized?\nThere are two basic approaches:\n- Submit the generic product construction for UL listing. This requires extensive planning, product/sample build and testing. The process can take over 6 months, but once completed, the supplier has limited latitude in material and process changes. To receive qualification for a generic flexible circuit product listing, the fabricator chooses a number of possible variations that would be common (i.e. thickness, number of layers and trace configuration) and submits ranges to cover each of the common potential variations.\n- The second option is to submit a specific part number for testing. This is the fastest way to get a part listed as the qualification process can be completed in as little as a week. The main drawback with this method is there is little room for any design or process variation. The marking only applies to the exact construction and layout of the submitted part. If a customer wanted to change the copper thickness or polyimide thickness, those variations would need to be submitted for testing.\nWhat are the flammability ratings?\nUL 94 covers the flammability ratings. The ratings are as follows: (source)\n- HB: slow burning on a horizontal specimen; burning rate < 76 mm/min for thickness < 3 mm or burning stops before 100 mm\n- V-2: burning stops within 30 seconds on a vertical specimen; drips of flaming particles are allowed.\n- V-1: burning stops within 30 seconds on a vertical specimen; drips of particles allowed as long as they are not inflamed.\n- V-0: burning stops within 10 seconds on a vertical specimen; drips of particles allowed as long as they are not inflamed.\n- 5VB: burning stops within 60 seconds on a vertical specimen; no drips allowed; plaque specimens may develop a hole.\n- 5VA: burning stops within 60 seconds on a vertical specimen; no drips allowed; plaque specimens may not develop a hole.\nDoes UL Recognition also relate to product performance?\nUL only tests for safety issues, a UL listing in general does not apply to product performance, unless it relates to safety.\nWhat All Flex Products are UL Recognized?\nAll Flex has a number of constructions for flexible circuits and heaters that are UL listed. The following are current files:\nThe applicable UL spec for flexible circuits is UL796\nAre Process Parameters Included in the Listing?\nAny processing parameter that could affect aging or degradation of the material must be part of the submission. The UL listing would be restricted to those processing conditions. Temperature, pressure, chemicals and humidity are process parameters that are considered to potentially age or degrade materials.\nHow does UL monitor for Compliance?\nUL will perform a quarterly (unannounced) visit. During this visit the auditor will inspect the applicable materials, processing records, and work in process to assure compliance.\nWhat would be an example of non-compliance?\nAny material used in a product build that was not specified in the listing would be an example of non-compliance. Any recorded processing condition falling outside of the submitted range would be considered an incident of non-compliance. Failure to adequately track or record materials and conditions would also be incidents of non-compliance.\nWhat happens if UL finds non-compliance?\nWhen an incident of non-compliance is found, UL allows the company to submit a written response. Non-compliance can result in significant fines and restrictions on applying UL markings on products.']"	['<urn:uuid:7f7b813f-1816-41d6-ada4-4cecdb5f5e7d>', '<urn:uuid:1e66ce80-bae9-4fd6-92ad-c6c7ee4a4782>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T02:03:54.148118	35	97	1768
45	norse religious term asatru origins and uses vivat history middle ages courtly traditions	The term Asatru was first found in a song by Norwegian composer Edvard Grieg around 1873, though it was coined earlier in the 1820s during the Scandinavian Romantic movement. It became official when recognized by the Icelandic government in 1972. Meanwhile, 'Vivat' comes from medieval Latin and was used in courtly traditions - it's the subjunctive form of 'to live' meaning 'long live.' It was used in medieval university anthems and is still used in British coronation ceremonies, where Queen Elizabeth II was hailed with 'Vivat Regina' in 1953.	"[""The History of the Original Principality Awards of Northshield\n(by Gwyneth Felton, first Princess of the Principality of Northshield)\nIn order to become a Principality, Northshield had to submit the basis for at least two awards with the package to the Middle Kingdom and the Board. The two awards were the Saltire, named in honor of Sir Angus Ulrich, who had passed away during the Principality process, and what became the Constable, though it was actually submitted as a non-specific fighting award.\nWe worked along with the Stallari to come up with other awards. One of the things we were trying to do was to recognize things that, at least at the time, were NOT recognized within the award structure of the Middle Kingdom. Some of the more interesting ones were:\nNorthshield already had a strong culture of bardic and performing arts, and so, in the beginning, we chose to recognize and encourage that tradition with the Crwth, as opposed to a generic A&S award. The Crwth is a welsh stringed instrument, and can also mean crowd.\nOriginally named the Heathstone, this was given to groups who consistently (or specifically) excel at showing hospitality and hosting events. We felt that none of the existing awards really recognized the good will and hospitality that groups could show to other groups and/or people within the Principality, so we created this one. The imagery was that of the comfort of a warm hearth on a cold night.\nGiven for putting forth great effort at researching, creating and maintaining a period persona and atmosphere. At the time, there was nothing else that recognized people who made a greater attempt at researching and presenting a more accurate persona, representing the ideal of the SCA. The Pyxis name comes from the latin, where it means a mariner’s compass, indicating the direction that people should be following.\nThe Sheriffs and Constables were intended to encourage martial activities, leadership, participation on behalf of the Principality, and to show that all forms of martial activity – rattan, rapier, archery, equestrian, etc. – were valued equally in our eyes, because the awards could be given equally for any of them. We looked at several other kingdom’s awards structures, and tried to incorporate some of the requirements and meaning of those other awards.\n(Heraldic Note: As the orders are closed, the arms are still registered to the Kingdom; but since the orders' names were never registered, the badges are registered to the KINGDOM, not any particular order. By custom, but not by law, they remain reserved for companions of those former orders.)\nScroll of Honor:\nThis was an idea I completely stole from another Kingdom – maybe Ealdormere? I thought it was a good way to be able to recognize people who did something spiffy that didn’t fit into any other category\nThe History of non-armigerous awards in the Kingdom of Northshield\nThe Great Bear:\n(by Fina ingen A'eda, second queen of the Kingdom of Northshield, and co-creator of the award)\nX-1165 There shall be in Northshield an Order into which the Crown may enter those who embody the spirit of the Kingdom. No more than one person may be admitted into this Order during any reign. Members of this Order may style themselves “Bears of Northshield.”\nThis Order is different from all other honors that Northshield confers in two ways. First it has no name, and second it can only be given to one person per reign. The reason that it has no name is that Tarrach and Fina were frustrated at the slow pace of approvals for award names, and by the fact that names were changing more than once before approval was granted, making it hard for people to remember which awards were which. The award was intended to remain without an official name and badge. However, should a future reign wish to name the honor, the hope is that the name would be submitted and approved before it is used or written into Northshield law.\nThis Order is meant to honor those who are such an integral part of the Kingdom that their participation helps to define the character and spirit of the Realm.\nMembers may style themselves ‘Bears of Northshield’ as a reminder of the Barony of the Great Bear, which was the first Barony in Minnesota – see The History of Northshield for more information.\nInformation coming soon….\nBonus history tidbit:\nWhy do we cheer Vivat? by Gwyneth Felton (first Princess of the Principality of Northshield)\nIn the early days of the Principality, we thought it was important to help guide and establish a culture for Northshield that, while not completely separate from the Middle Kingdom, was still unique and distinctive to Northshield. One of the things that came up was the cheer to be used in Court. The Middle Kingdom uses “hoobah”, which is, as far as anyone was able to tell me, a made-up word. We decided we wanted to use something that was appropriate to the medieval period, and so after some research decided on Vivat! (singular) and Vivant! (Plural).\nWhy Vivat? Here is a description (from http://en.wikipedia.org/wiki/Vive,_Viva) that explains it well.\n“Vive, Viva and Vivat are interjections used in the Romance languages. Vive in French, Viva in Italian, Portuguese and Spanish, and Vivat in Latin and Romanian are subjunctive forms of the verb “to live”. They literally mean, “live!” (imperative form), and are usually translated to English as “long live”. . . . . Additionally, in monarchical times the king of France would be wished “Vive le Roi!” and the king of Italy “Viva il Re!”, both meaning “Long live the King!”.\n“The acclamation “Vivat” is still used in the British coronation ceremony, and when Queen Elizabeth II was crowned in Westminster Abbey in 1953 she was hailed with the words “Vivat Regina”.”\n“ The medieval university Latin anthem De Brevitate Vitae has verses like: Vivat academia! Vivant professores! Vivat academia! Vivant professores! Vivat membrum quodlibet Vivant membra quaelibet Semper sint in flore.”\nThe tradition also began to use the cheer of “Skol” during feasts, when toasting Royalty or other Noble personages, as it is traditionally a Norse/Scandinavian drinking toast."", 'The earliest use of the term Asatru that anybody has so far able to find was its use in a song written by Norwegian composer Edvard Grieg. It was written around 1873 as part of the third scene of Grieg\'s opera Olav Trygvason, of which only the first act - the Heathen one - was ever written. But the word was coined sometime before that, earlier around the 1820s, as part of the Scandinavian Romantic movement.\nThe term Asatru became official when the religion was recognized by the Icelandic government in 1972 at the urging of the poet, Sveinbjorn Beinteinsson. Today the term refers to the re-birth of the ancient ethnic pre-Christian religion of northern Europe in its modern manifestation.\nIn the 1930\'s, Alexander Rudd Mills coined the term Odinist and founded the ""The First Anglecyn Church of Odin"" for his countrymen of northern European ethnic heritage. His activities resulted in his arrest and political detention by the Australian government during World War 2.\nAsatru is a living religion, practiced by a rapidly\ngrowing number of people throughout the world. Asatru is a polytheistic,\nnature-oriented faith grounded in an honouring of the High Gods of the Aesir and\nthe Vanir, the reverence for ancestors and our moral code is implicit in the\nobservance of the Nine Noble Virtues\nThose who practice Asatru are called Asatruar, Asafolk, Tru Folk, Odinists, Germanic Heathens or just worshippers of the Germanic religion.\nAsatru\'s beginnings are lost in prehistory, but as an organized system, it is older by far than Christianity. Strictly speaking, since Asatru is the religion which springs from the specific spiritual beliefs of the Northern Europeans, it is as old as this particular branch of the human race.\nAfter having few, if any, practitioners for many centuries, this religion was revived as Asatru in the 19th century by the Geatish Society. It received a special impetus in the late 1960s and early 1970s when Sveinbjorn Beinteinsson was instrumental in getting Asatru recognized by the Icelandic government in 1973 and a several organizations sprung up in England, Germany, and North America.\nThere are two groups of Gods in the Germanic pantheon; the Aesir, who are the predominant group, and the Vanir, some of whom have been adopted into the Aesir. The gods are, of course, mysterious and unknowable in some sense, but in another, they are very knowable. One may know them from the stories of their various actions, and one may further know them by spiritual experience. They seldom announce who they are when they appear in a dream or in some other way, but one generally knows.\nThese Deities of Northern Europe are best known to most people as the Gods and Goddesses of the Old Norse legends, although these same Deities were once worshiped by most of the peoples of pre-Christian Germanic Europe. But because the Old Norse legends provide the best knowledge of them, we usually refer to them by their Norse Germanic names.\nMany people are primarily committed to one god or goddess. Males tend to follow male gods, and women to follow female ones, but not always. It is largely a matter of compatibility, for the divinities have strong personalities, and even the Gods and Goddesses we hear little about can make their presence strongly felt.\nThe basic religion of Asatru involves keeping the major festivals of the year, which generally fall on the solstices, equinoxes, and cross-quarter days with one or two exceptions. These festivals can be kept personally, with the family or one or two friends, or with an organized Asatru group.\nThe two main styles of ritual celebrations which are done to honor the Gods and Goddesses and to keep ones troth are called the blot and the sumbel. In addition to these there are also numerous social and cultural activities.\nBlot is an Icelandic word meaning blessing, and involves pouring out mead, ale, or other drinks onto the Earth as a sacrifice to the Gods & Goddesses, as well as sharing that blessing with the participants by passing around a drinking horn with that mead or ale, or by sprinkling some of the mead or ale on the participants.\nAn Asatru religious ceremony is called a Blot, Pronounced ""bloat"".\nEight major Blots are celebrated by Asatruars each year. These are listed below,\nwith the modern English name of each given first, followed by its Old Norse name\nin parenthesis, and its approximate date -- although the usual practice is to\nhold the Blot on the nearest weekend.\nDisfest (Disablot) 31 January\nOstara (Ostara) 21 March\nMay Eve (Valpurgis) 30 April\nMidsummer (Midsumarsblot) 21 June\nFreysfest (Freysblot) 1 August\nHarvestfest (Haustblot) 21 September\nWinter Night (Vetrnaetr) 31 October\nYule (Jol) 20th December - January 1st\nThere is also the Alfablot to honour the Alfar. Other Blots are also held for special life events such as weddings, funerals, coming of age rites, etc.\nBesides the Blot, there is another kind of Asatru ceremony called the Sumbel, which is a kind of formalized religious toasting. Sumbels are held whenever people want to hold them.\nThe sumbel is a ritualized toasting in which a horn with mead or ale is passed around and each participant makes a toast, a boast, an oath, or says something significant over the drink. It is said that the words spoken at a sumbel echo throughout all the Nine Worlds so words spoken there should be thoughtful and meaningful.\nPRACTICES - ARTS:\nSome who practice Asatru only adhere to the religion, keeping the festivals with blots and sumbels, but many are also involved in the esoteric arts of Germanic magic, such as the use of Runes and the shamanic practice known as Seidhr.\nOther popular activities include the making of traditional crafts and honing of ancient skills, including: archery and other ancient sports, mead and ale brewing, rune carving, cooking, sewing, writing poetry and linguistic studies, camping and more. Many Asatruar are avid scholars and will read a great deal, including pursuing obscure texts, in order to gain more knowledge.\nThe Asatru way of life esteems: courage, honour, hospitality, independence (and liberty), individuality (with self-reliance, family and self-responsibility), industriousness (and perseverance), justice (including an innate sense of fairness and respect for others), loyalty (to family, friends and folk), truthfulness, and a willingness to stand up for what is right.\nNATURE of ASATRU:\nThe Germanic Heathen religion of Asatru is an indigenous, ethnic, and ancestral faith much like many African, Oriental, Native American and Native Australian Heathen religions. It represents the indigenous pre-christian spiritual beliefs of the Germanic peoples and a return to our traditional values. This spiritual practice is the veneration of the ancestral Gods and Goddesses of the Pan-Germanic peoples (Northern Europeans), which includes the peoples, and their descendants, of present-day Scandinavia, England, Germany, Holland, Austria, Iceland, and Belgium among others.\nAs the ancestral religious ""common law"" of the Northern European peoples, Asatru can survive by tradition much like the Anglo-Saxon common law, and does not require a lot of theology and dogma, just like the British parliament evolved without an equivalent of the U.S. Constitution. Important source material include ancient accounts of our ancestors written by their contemporaries, the prose and poetic Eddas written in Iceland during its golden age of saga literature, but other guidance can be found by studying the folklore, history, and antiquities of the Germanic peoples as well as the religions of their ethno-religious cousins (Druidism/Celtic & Greco-Roman religion, and early Hinduism).\nThe Asatru approach to religion is very similar to the motivating factors behind the Protestant Reformation in which most of the Nordic peoples in different countries around Europe, ranging from northern France and Germany to the Baltic states, Scandinavia, and Scotland rejected Vatican authority. They sought the right to run their own local church government and the right to find religious truth through personal learning, analysis, and self-examination rather than through coercion by a centralized source, dogma, unquestioned ""divine"" revelation. Thus, as might be expected, rites and practices of the faith in modern times vary from person to person and group to group, differing branches of a tree whose roots are found in the common sagas and traditions of the pre-Christian Norse Germanic cultures.\nOur ancestors constantly watch over us and are with us in everything we do. We honour them and their past endeavors and keep them constantly in our thoughts. They are our guiding light to the future and we learn from their mistakes and their victories alike. To separate an Asatruar from their ancestors and relatives is to separate them from the vital aspects of existence that knit together our universe and keep the great World Tree of Yggdrasil strong against the forces of destruction.\nWhile Asatru is the ETHNIC and ANCESTRAL religion of the\nGermanic peoples, it should be emphasized that Asatru is not a political or\nracial supremacy movement. There have been some in the past who have tried to\nuse Asatru as a cover for a political agenda but the vast majority of Asatru organizations\nare all religious organizations which are not interested in being used as a\nplatform for any racist or political propaganda. Those who wish to do so would\nnot be welcome in any of the major Asatru organizations. Asatruar span the\npolitical spectrum from conservative to liberal, A lot of us are involved in the\necological movement, and many have libertarian leanings, and in any case many\nusually keep their religion separate from their politics.\nThis ethnocentric orientation of Asatru does not mean that we are indifferent to the goals and aspirations of others. Indeed the followers of the Heathen Way believe very strongly in the expression of individuality and thrive both intellectually and creatively in an atmosphere of diversified pursuits. Our ancestors were great traders and adventurers and were accustomed to meeting with and doing business with the peoples of various faiths and cultures long before Christianity entered the scene. It is well known today that Swedish Heathens opened the trade routes to the East through Russia and had contact with Arab and Oriental populations many centuries ago. And in fact, Heathen Germanic explorers discovered North America, which they called ""Vinland"", hundreds of years before Christopher Columbus was even born.\nAsatru is fundamentally a folk-oriented belief system and well-suited to followers with cultural and genetic ties to the history and beliefs of the pre-Christian peoples of Northern Europe. The reasons for this are to be found in the nature of the faith itself. The High God Odin, known as our All-Father, is described as the creator of our people and the worship of Heathen ancestors is very closely allied to a belief in the halls of the dead and the protection and guidance offered to the living by those who have gone before us.\nAsatru is a polytheistic, nature-oriented belief system in which the Earth, called Midgard (personified by our Goddess Nerthus, or Jord) and the preservation of her splendor, fertility and wholeness is a constant concern. In this respect we share much with our other Heathen brothers and sisters who continuously and tirelessly work in the fields of conservation and ecology. We believe that the Gods gave this planet called Midgard to us as a mother and a womb for future generations and her health and well being are paramount - or there will be no future generations. Consequently, we do not focus on a life hereafter and its rewards but on the life we are given as people of our folk on Earth, here to live, work in harmony with all life and with all of Earth\'s people.\nAsatru places a high value on self-reliance, individuality, strength of mind body and soul, trust, staying true to ones word, and camaraderie; individualism and community are not seen as mutually exclusive, but as complimentary.\nIf you feel that you would help revitalize the native religion of your ancestors, you must do so after careful consideration and research. Asatruar do not acknowledge the holding of simultaneous religious beliefs, thus Asatru has one imperative, once you have made up your mind, and that is that you will take an oath of loyalty to the High Gods and that you will swear to uphold the Nine Noble Virtues and honour your folk and family with all of your being. No one will ever stop you from leaving the faith if you so wish. You are your own individual and your path is chosen only by your own integrity. Do not give your word lightly for your word is your bond in Asatru.\nUpon death, the valiant and noble of Asatru are chosen by the Valkyries and taken to the various Great Halls of the Gods or Goddesses, there they are honoured until they are chosen to return to the Earthly plane as guides and leaders. This Germanic spiritual tradition of re-incarnation is unique in that most of our people are eventually reborn back into our tribe along Family lines.\nOrganized Asatru groups are known as a kindred, hearth, garth, or by other names.\nAsatru organizations generally favor democratic and republican forms of spiritual government, as inspired by the parliamentary Althings of the Viking era and subsequent parliamentary systems of Britain and the Scandinavian countries, and promote individual rights and freedom of speech reminiscent of Norsemen of the saga era and their more modern descendants.\nIn the world today, Asatruar are largely misunderstood, distrusted and at times openly ridiculed and condemned. We stand strong in our desire to be free and we support all those who likewise face discrimination. Victory is never easy when the path is overgrown. We strive therefore to clear the way and to bring the light of the Gods to every follower of Asatru who hears the call of the ancient way.\nIn conclusion, the path of Asatru is not an easy one. Rewards are not material and struggles are difficult to win. There is no Heaven, there is no Jesus and there is no Satan. Sometimes there is no road and the way must be cut with the luminescent sword of knowledge. There is only steadfastness and integrity and a supreme and abiding love and respect for the Gods and Goddesses of our folk to bring us through the darkness of Ragnarok and fear of the unknown.']"	['<urn:uuid:83c1fe15-681f-46fb-8972-a3f9cbbb5088>', '<urn:uuid:22b5955e-a48e-4221-ad78-a630cb069547>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T02:03:54.148118	13	89	3414
46	How do review processes ensure both quality and diversity goals?	Review processes for quality and diversity goals involve multiple monitoring mechanisms. Quality control includes peer reviews with documented outcomes, quality checklists, project schedule analysis, and regular audits to identify issues and improvements. Similarly, diversity progress is monitored through quarterly and annual DEI reviews, with executive-level accountability metrics tied to performance assessments. At Dentsu, this has resulted in measurable improvements, such as increased women's representation at executive levels (up 6%) and BIPOC representation (up to 20.1% in executive ranks). Both areas require systematic documentation, clear performance standards, and regular assessment to ensure targets are met.	['Dentsu Americas released a comprehensive diversity, equity and inclusion report today measuring progress and initiatives over the past year.\nThe company reported “slight increases” in the diversity of its hiring throughout 2020 and into 2021, with women accounting for 60% of Americas hires and multicultural representing 33% of US hires.\nThe company said it is also working to have a more diverse candidate pool from all underrepresented backgrounds, including LGBTQIA+, differing ability levels, military and veterans.\nWomen represent 58% of the Americas region workforce, for identified data, up 4% from last summer.\nAmong professionals, women represent 60.5% (entry-level through early director levels) and remain above 50% at management levels (Director through VP/SVP). At the executive level, women represent 36.1%, an increase of 6% from the prior year. The goal: at least 50% women in executive levels by 2025.\nU.S. multicultural or BIPOC representation (Black, Indigenous, Asian, Latinx and Hispanic, Two or More Races and other) is 27% of the firm’s identified workforce data, up 2.6% from June 2020.\nThe company credits the improvements to ongoing and new DE&I initiatives including “building a more diverse [hiring] pipeline.” The goal: At least 30% of the U.S. workforce being BIPOC by 2025.\nThe report was authored by Christena Pyle who was named Dentsu International’s first chief Equity Officer last August.\nPyle states in the report that transparency and accountability, representation and sponsorship, education and continuous learning and client and community impact are the “pillars” guiding the company’s DEI efforts. “While there is more work to be done, we are proud of the progress that we’ve made so far in collaboration with our people, clients and partners.”\nWomen of color account for 15.9% of the U.S. workforce with men of color at 11.1%. Among the executive ranks BIPOC representation is up to 20.1% from 16.9% in June 2020, with increases in Black and Asian, which the company said exceeds its 2021 goal. By 2025 the company’s goal is to have at least 25% BIPOC in executive roles with a more equal distribution among men and women, who are currently less represented.\nBIPOC employees represent 29.6% within professional ranks, up 1% over the past year. Management ranks are 19.7% BIPOC, also up 1%.\nAs part of its DE&I effort Dentsu has imposed greater accountability metrics at the executive level which are tied to broader performance assessments. Starting last July, there are now both quarterly and annual DEI reviews.\nThe report also addresses a number of partnerships—new and existing—that are helping to drive the company’s DEI goals including alliances with 4A’s MAIP, AAF MPMS program, adfellows, COOP and One Club’s ONE School. Pyle herself is an alumna of MAIP, which she noted during a press briefing was testament to the power of mentor and internship programs when implemented effectively.\nOne example of a new partnership: Merkle has launched a first-of-its-kind alliance with Howard University whose School of Business faculty will work with Merkle’s professional experts to develop leading-edge database technology and consumer marketing curricula for four courses in the marketing and information systems and supply chain management departments. Its designed to give students a fresh perspective on current marketing industry standards. Merkle will also support the School of Business’s programmatic strategies through various activities, including a summer boot camp.\nThe company has also focused on the suppliers its works with and has set a goal 15% of spending to be with diverse suppliers including businesses owned by women, BIPOC, LGBTQIA+, persons with a disability and Veterans.\nDentsu International has also joined The World Economic Forum’s Racial Justice in Business initiative and says that to date it is the only holding company (or agency) to do so among the dozens of members representing about 6.5 million employees.\nSee more from the report here.', 'Development Cooperation Handbook/Designing and Executing Projects/Project Execution and Control/Implement Quality Assurance and Quality Control\nin Project Execution and control , quality control involves monitoring the project and its progress to determine if the quality assurance activities defined during Project Planning are being implemented and whether the results meet the quality standards defined during Project Initiation and Planning. The entire organization has responsibilities relating to quality, but the primary responsibility for ensuring that the project follows its defined quality procedures ultimately belongs to the Project Manager. The following figure highlights the potential results of executing a project with poor quality compared to a project executed with high quality:\n|Poor Quality||High Quality|\n|Increased costs||Lower costs|\n|Low morale||Happy motivated project team|\n|Low beneficiary satisfaction||Beneficiary satisfaction|\n|Increased risks||Lower risks|\nQuality control should be performed throughout the course of the project. Some of the activities and processes that can be used to monitor the quality of deliverables, determine if project results comply with quality standards, and identify ways to improve unsatisfactory performance, are described below. The Project Manager and Project Sponsor should decide which are best to implement in their specific project environment.\n- Conduct Peer Reviews – the goal of a peer review is to identify and remove quality issues from a deliverable as early and as efficiently as possible. A peer review is a thorough review of a specific deliverable, conducted by members of the Project Team who are the day-to-day peers of the individuals who produced the work. The peer review process adds time to the overall Project Schedule, but in many project situations the benefits of conducting a review far outweigh the time considerations. The Project Manager must evaluate the needs of his/her project, determine and document which, if any, deliverables should follow this process, and build the required time and resources into the Project Schedule. Prior to conducting a peer review, a Project Team member should be identified as the facilitator or person responsible for keeping the review on track. The facilitator should distribute all relevant information pertaining to the deliverable to all participants in advance of the meeting to prepare them to participate effectively. During the meeting, the facilitator should record information including: Peer review date Names and roles of participants The name of the deliverable being reviewed Number of quality issues found Description of each quality issue found Actions to follow to correct the quality issues prior to presenting the deliverable to the approver Names of the individuals responsible for correcting the quality issues The date by which quality issues must be corrected This information should be distributed to the Project Manager, all meeting participants, and those individuals not involved in the meeting who will be responsible for correcting any problems discovered or for producing similar deliverables. The facilitator should also solicit input from the meeting participants to determine if another peer review is necessary. Once the quality issues have been corrected and the Project Manager is confident the deliverable meets expectations, it may be presented to the approver.\n- Use Quality Checklists – – both the Project Manager and Project Team members can create and make use of various checklists to be sure items are not overlooked while a product is being developed. Checklists may be simple hardcopy lists of “things to do,” or may be generated using more formal, electronic-based tools. In either case, a checklist should be comprehensive and detailed enough to ensure that the resulting product or deliverable has been built to the level required to meet quality standards.\n- Maintain and Analyze the Project Schedule – this activity should never be taken lightly, regardless of the size of the project. Updating the Project Schedule on a regular basis while keeping a close watch on the timeline and budget is the primary mechanism to measure quality of the schedule. If the project timeline or budget are not on track, the Project Manager can determine why and take immediate action to remedy the problem\n- Conduct Project Audits – the goal of a project audit is to ensure that the Quality Assurance activities defined in Project Planning are being implemented and to determine whether quality standards are being met. It is a process to note what is being done well, to identify real or potential issues, and to suggest ways for improvement. Audits should be performed on a regular basis, depending upon the size and length of the project. At a minimum, it is recommended that an audit be performed at the end of each phase, at least once during Project Execution and Control, and at the end of the project.\n- the project triangle\n- programme quality plan\n- Quality standards\n- The 9 topic areas of project management knowledge: Integration, Scope, Time , Cost, Quality, Human Resources, Communication , Risk, Procurement.\n- How to Show Values Through Action\n- Employee performance review']	['<urn:uuid:b53d21c1-87d3-4d76-8dca-7db259ac5ac1>', '<urn:uuid:37d4eebd-a54c-4a95-b07c-7e481a038b96>']	open-ended	with-premise	concise-and-natural	distant-from-document	three-doc	expert	2025-05-13T02:03:54.148118	10	94	1435
47	How common are mental health problems in Scotland?	In Scotland, one in four people will experience a diagnosable mental health problem each year, and half of all diagnosable mental health problems start before a child reaches the age of 14.	['An organisation based in Falkirk, Scotland, that works to promote mental health recovery for adults aged 16 and over.\nIn Scotland, one in four people will experience a diagnosable mental health problem each year. Falkirk and District Association of Mental Health (FDAMH) provides a range of services including counselling, befriending, family and carer support groups, a mental wellbeing drop-in, educational and social activities and training. It also works in GP surgeries, with patients referred to FDAMH as an alternative to medication.\nWhat the judges said:\nIn Scotland as a whole it’s reckoned to be about one in four individuals will at some time in their life experience some level and degree of mental ill-health.\nSome of our areas in this part of the country suffer from greater levels of social depravation than average.\nWe also have problems with recruiting and managing GP practices. We have seen the number and the demand for the services grow.\nI’ve had quite a hard upbringing. I think basically I always felt unwanted and unloved as a child upwards. Just I was afraid of everything, of being around people, not being around people, being in the house, being out the house, it was a scary scary time. And then I came in here, and wow it’s been a big change coming in here and meeting people.\nFalkirk’s Mental Health Association is known in the community as FDAMH and our organisation goes back to the early 80s. So this is an organisation who are prepared to challenge, be bold but also be very innovative. In FDAMH the recovery based approach is important in encouraging people to participate in other activities and groups.\nWe have the immediate health service that is for anybody that’s in crisis, can walk in off the street.\nThe service is really helpful because people can’t necessarily get appointments with their doctors when they really need it and it also helps people to engage with the drop-in and other services as well.\nA number of our trainers go into schools and explain to children and young people what they thought ill-health is, what to look out for, what they can do to support themselves.\nWe have the befriending department to help to build up their social skills and linking back into the community, getting them used to being out and about again, not being so isolated. We’ve got two areas within that, the standard befriending which is for people with a diagnosis of mental health. And then we have the Third Age befriending which is predominantly for people aged 55 years and over and they’re at risk of developing mental health conditions.\nThe befriending can be vital because it’s not a medical path, but it’s the sports and joy of films, normally just trying to give them a normal conversation.\nWe also have a little social contact group and then we have the telephone befriending which is for people that cannot go out the house either due to severe anxiety or problem of physical limitations.\nWe’ve coined the phrase ‘have a blether together’. We found that really beneficial for people.\nSo social security service was set up a couple of years ago support for the patients with reduced mental wellbeing. We have two practitioners based at two GP surgeries in the Forth Valley area.\nSocial prescribing is about providing non-medial sources of support for individuals without FDAMH the social prescribing initiative would not have taken off in the Forth Valley.\nOne of the skill’s that’s absolutely essential to being a GP is knowing how to support your patients to find ways they can support themselves.\nI think what FDAMH has done is respond to that need and offer services that are responsive in the right way.\nFDAMH been very much involved in the community from day one, for building resilience project, it gives people skills to manage and cope themselves. In the last year over 50 organisations have come to see what we’re doing and our approach to take it back to develop policy elsewhere and that cascading is just marvellous.\nFDAMH have been able to punch a way beyond their weight here in the Falkirk area, they play an important part across the whole of Scotland as well.\nI realised that I wasn’t alone, that there is other people that’ve got the same things as me or they’ve got something going on and that I’m safe here, it is a safe place to be, I didn’t have to be afraid.', 'In Scotland half of all diagnosable mental health problems start before a child reaches the age of 14\nCounselling in Secondary schools\nTransition through secondary school can present many challenges for young people which can bring on stress, anxiety, depression or relationship problems with those around them.\nThe Spark’s school based counselling and therapeutic services offer support to young people faced with difficult life experiences.\n- Why do we need school based counselling?\n- How can school based counselling help young people?\n- The value of counselling in Secondary schools\n- Get in touch\nWhy do we need school based counselling?\nThe Spark’s youth counsellors help individuals identify underlying issues leading to enhanced mental and emotional wellbeing.\nIn Scotland half of all diagnosable mental health problems start before a child reaches the age of 141.\nEarly diagnosis and intervention work is therefore vital in reducing the levels of poor mental health amongst adults2.\nWithout early intervention and support, mental health problems amongst teens and adults result in significant personal, relational, societal and economic costs.\n“The Spark’s youth counselling has proven to be invaluable to our school. It’s wonderful to see (pupils) engaging positively with their counsellor.\nThe mental and emotional wellbeing of all our young people is of paramount importance to us and The Spark is a superb support and resource to the young people of Hillpark.”\nEileen Edgar, Depute Head Teacher, Hillpark Secondary School\nContact The Spark’s Children and Young People Team to find out more about our school based counselling services.\nHow can school based counselling help young people?\nSchool based counselling can help young people deal with a range of personal problems.\nCommon issues addressed by The Spark’s youth counsellors in Secondary schools include3:\n- Family breakdown\n- Body image / self-confidence / sexual identity\n- Relationships and sex\n- Violence, anger & aggression\n- Exam stress\n- Depression / anxiety / self-harm\n- Drug and alcohol abuse\n- Eating disorders\n- Young carer responsibility\nGet in touch with our Children and Young People Team to discuss your counselling and support needs.\nThe value of counselling in Secondary schools\nThe Spark’s therapeutic service aims to help the high number of young people in Scotland affected by poor mental health by:\n- providing support to build resilience\n- developing new copy skills and strategies\n- making better sense of relationships\n- helping to manage change and transitions\n- providing a space to explore various life experiences and challenges.\nThis approach is in line with the Scottish Government’s strategy ‘Getting it Right for Every Child’ recognising that each child is individual and that some require alternative support to access education fully and realise their potential3.\nContact The Spark’s Children and Young People Team to find out more about the value and importance of school based counselling.\nCounselling in schools\nCounselling offers a safe, confidential place for young people to talk about experiences that may be confusing, painful or uncomfortable.\nThese experiences may exist within their own home, community or at school. Typically they will be impacting upon the young person and their relationships.\nWithout support young people often cope by avoiding certain situations/ withdrawl, bullying, self-harm and aggression amongst many others.\nThe Spark’s Children and Young People Team (CYP) utilise a combination of therapeutic play, art therapy and talking therapy.\nCounsellors establish a therapeutic relationship with the young person through acceptance, trust and empathy. Thus providing young people with an opportunity to express and process feelings at their own level and pace.\nThis positive relationship can then be internalised by the young person and help instil a sense of competence, improved ability to form healthy relationships and re-align negative patterns of behaviour or thinking.\nAll The Spark counsellors are members of the British Association for Counselling & Psychotherapy (BACP) or The British Association for Art Therapists (BAA) .\nContact The Spark’s Children and Young People Team for more information about school based counselling and support services.\nMany young people do not need counselling and are able to resolve emotional difficulties with the help of their family and friends. For some who have challenging difficulties, counselling can offer an important support for the young adult’s health and wellbeing.\nFor more information on youth counselling in Secondary schools please contact the CYP Team on 0141 222 3910 or complete an enquiry form.\n1. Scottish Children’s Services Coalition (2016). ‘Vulnerable children and young people with mental health problems need care and support.’ (visited 30/10/2016).\n2. Scottish Government (2008). ‘Early Years and Early Intervention: A joint Scottish Government and COSLA policy statement.’ (visited 30/10/16).\n3. Young Minds (2016). ‘What’s the problem?’ (visited 30/10/2016).']	['<urn:uuid:9a6449cf-459c-45cd-b878-23ca57b92038>', '<urn:uuid:2555fca7-9f85-4c31-bb99-b312e1a6d246>']	factoid	direct	concise-and-natural	distant-from-document	three-doc	novice	2025-05-13T02:03:54.148118	8	32	1518
48	waves water damage history ancient greece modern island problems both natural disaster tsunami climate	The study of waves dates back to ancient Greece, where philosophers studied sound waves and vibrations, particularly in music. Today, waves continue to pose serious threats - in 1998, tsunami waves up to 15 meters high struck Papua New Guinea, killing over 2,200 people. Currently, Small Island Developing States face severe risks from another type of wave threat - sea level rise due to climate change. This threatens the very existence of many low-elevation islands through coastal erosion and flooding, with projections showing dangerous impacts if global warming exceeds 1.5°C above pre-industrial levels.	"['On July 17, 1998, three huge waves – ""tsunamis"" – up to 15 meters high struck the north coast of Papua New Guinea, killing at least 2,200 people. A major earthquake, itself consisting of waves traveling through the Earth, triggered an underwater landslide that created the tsunamis. Radio stations reported the disaster by transmitting electromagnetic radio waves to listeners around the world. Listeners were able to hear the news transported by sound waves created by their radios.\nWaves of one form or another can be found in an amazingly diverse range of physical applications, from the oceans to the science of sound. Put simply, a wave is a traveling disturbance. Ocean waves travel for thousands of kilometers through the water. Earthquake waves travel through the Earth, sometimes bouncing off the core of the Earth and making it all the way back to the surface. Sound waves travel through the air to our ears, where we process the disturbances and interpret them.\nAncient wave theories\nMuch of our current understanding of wave motion has come from the study of acoustics. Ancient Greek philosophers, many of whom were interested in music, hypothesized that there was a connection between waves and sound, and that vibrations, or disturbances, must be responsible for sounds. Pythagoras observed in 550 BCE that vibrating strings produced sound, and worked to determine the mathematical relationships between the lengths of strings that made harmonious tones.\nScientific theories of wave propagation became more prominent in the 17th Century CE, when Galileo Galilei (1564-1642) published a clear statement of the connection between vibrating bodies and the sounds they produce. Robert Boyle, in a classic experiment from 1660, proved that sound cannot travel through a vacuum. Isaac Newton published a mathematical description of how sound travels in his work Principia (1686). In the 18th Century, French mathematician and scientist Jean Le Rond d\'Alembert derived the wave equation, a thorough and general mathematical description of waves, which laid the foundation for generations of scientists to study and describe wave phenomena.\nOur understanding of wave motion began with the study of\nWaves can take many forms, but there are two fundamental types of waves: ""longitudinal"" and ""transverse"" (see Figures 1 and 2). Both of these wave types are traveling disturbances, but they are different because of the way that they travel. As a wave travels through a medium, the particles that make up the medium are disturbed from their resting, or ""equilibrium"" positions. In a longitudinal wave, the particles are disturbed in a direction parallel to the direction that the wave propagates. A longitudinal wave consists of ""compressions"" and ""rarefactions"" where particles are bunched together and spread out, respectively (see Figure 1). For another view of this type of wave, take a look at the longitudinal wave video clip below. In a transverse wave, the particles are disturbed in a direction perpendicular to the direction that the wave propagates. The transverse wave video clip below provides a dynamic visualization of this type of wave. After either type of wave passes through a medium, the particles return to their equilibrium positions. Thus, waves travel through a medium with no net displacement of the particles in the medium.\nSound waves are examples of longitudinal waves: the individual particles (air molecules) vibrate back and forth in the direction that the sound is traveling. An example of a transverse wave is the classic sports arena phenomenon known as ""The Wave."" As the wave travels around the stadium, each spectator stands up and sits down. Thus, the displacement of the ""particles"" is perpendicular to the direction the wave travels. Many other waves, such as ocean waves or Rayleigh Surface Waves are combinations of longitudinal and transverse wave motion.\nParticles bunch together and spread out in _____ waves.\nThe waves we described above are all examples of \'periodic waves,\' in that they involve a cyclical pattern of motion. Waves travel through space and time, and can be described in terms of their characteristics in both of these dimensions. Imagine a Slinky®, a toy that consists solely of a long, loosely coiled piece of metal or plastic. By shaking one end of the Slinky® up and down in a periodic fashion, it is possible to produce a transverse wave, as shown in the figures below.\nAmplitude and wavelength\nFigure 3 represents a snapshot of a Slinky®, such as the one in the transverse wave video clip, as it is vibrating. The vertical axis represents the vertical position of the Slinky®, and the horizontal axis represents its horizontal position. As indicated in the figure, the amplitude (A) of the wave is the maximum displacement of a particle from its equilibrium position – or the height of the wave. The length of the wave is the wavelength (λ), and is simply the length of one cycle of the wave. In the figure, the wavelength is shown as the distance between two successive wave crests. The wavelength can also be measured between successive troughs, or between any two equivalent points on the wave. Both the amplitude and the wavelength of a wave are commonly measured in meters.\nThe height of a wave is know as its\nFigure 4 is a graph of the displacement of one point on the Slinky® as a function of time. The amplitude of the wave is still the same measurement as before – the maximum displacement of the point from its equilibrium position. The wave period (T) is the time (measured in seconds) required for the point to complete one full cycle of its motion, from its highest point to its lowest and back again.\nThe frequency of a wave (f) (not indicated in the figure) is a measure of how frequently the point completes one cycle of its motion. In other words, the frequency is the number of wave cycles completed by one point along the wave in a given time period. The frequency of a wave is related to the period of a wave by the following equation:\nWhere f is the frequency and T is the wave period. The frequency is measured in cycles per second, or hertz (Hz). If the wave period is 10 seconds (that is, it takes 10 seconds for the wave to complete one cycle), then the frequency is 0.1 Hz. In other words, the wave completes 0.1 cycles every second.\nA frequency of 0.1 Hz means that a wave completes _____ cycles every second.\nRemember that a wave is a traveling disturbance. Wave speed is a description of how fast a wave travels. The speed of a wave (v) is related to the frequency, wave period, and wavelength by the following simple equations:\nwhere v is the wave speed, λ is the wavelength, T is the wave period, and f is the frequency. Wave speed is commonly measured in units of meters per second (m/s). For example, the musical note ""A"" is a sound wave with a frequency of 440 Hz. The wavelength of the wave is 78.4 cm. What is the speed of the sound wave?\nThis value (345 m/s) is the approximate value of the speed of sound in air. Interestingly, the speed of sound in air depends on temperature and pressure. A musician who plays a wind instrument, such as a trumpet, could tune her trumpet at the base of a mountain, hike up the mountain to where the air pressure is lower, and find that her trumpet is no longer in tune. Similarly, a change in air temperature could also change the tuning of the instrument.\nAs the example above illustrates, waves are all around us in everyday life. The Ancient Greeks began their study of waves by thinking about music, but now almost every branch of physics involves waves in one way or another.\nWaves have been of interest to philosophers and scientists alike for thousands of years. This module introduces the history of wave theory and offers basic explanations of longitudinal and transverse waves. Wave periods are described in terms of amplitude and length. Wave motion and the concepts of wave speed and frequency are also explored.\nThe study of waves dates back to the ancient Greeks who observed how the vibrating strings of musical instruments would generate sound.\nWhile there are two fundamental types of waves - longitudinal and transverse - waves can take many forms (e.g., light, sound, and physical waves).\nWaves can be described by their exhibited properties: frequency, speed, amplitude, and wavelength.', 'Introduction to special issue\nSmall Island Developing States (SIDS) are positioned as particularly vulnerable to the effects of climate change due to their small size, geographical features and concentration of infrastructure, economic activities and population in coastal zones (Mertz et al., 2009; MacPherson & Akpinar-Elci, 2013, Nurse et al. 2014).\nThe extreme exposure and vulnerability of SIDS to sea level rise, increased intensity of extreme events, changes in temperature and precipitation and resultant impacts on economic and social structures (Gamble et al., 2010; Scott, 2012, Hernández-Delgado, 2015), stands in stark contrast with the negligible contributions of SIDS to global greenhouse gas emissions, the drivers of climatic change. Sea level rise and coastal erosion are particularly disturbing impacts of climate change as they threaten the very existence of many small, low elevation islands (Albert et al., 2016; Storlazzi, Elias & Berkowitz, 2015).\nIn addition, observed and projected impacts on coastal ecosystems, especially on coral reefs, severely threaten livelihoods in island regions and are projected to cause high economic damages (Chen, Chen, Chu & McCarl, 2015). Small islands also have especially sensitive fresh water supply systems and water stress is likely to pose a serious threat (Karnauskas, Donnelly & Anchukaitis, 2016; Terry & Chui, 2012).\nThese dire projected effects of climate change on SIDS has led the Alliance of Small Island States (AOSIS) to advocate strongly for a cap of 1.5⁰C on global warming above pre-industrial levels (AOSIS, 2015; Benjamin and Thomas, 2016).\nCurrent levels of warming at nearly 1⁰C have already had observable impacts on a global scale including declines in marine fisheries and food production and increases in sea level rise and flooding (UNFCCC, 2015). While warming of 1.5⁰C will result in significant additional impacts, it is likely that there will still be adaptation options for SIDS as sea level rise is projected to remain below 1 meter and terrestrial and marine species important for SIDS have higher likelihoods of survival (UNFCCC, 2015). With a warming of 2⁰C, impacts are thought to exceed the limits of adaptation for SIDS and result in irreversible and dangerous levels of climate change (UNFCCC, 2015).\nWhile there is a significant body of work focused on climate change and SIDS, there is a lack of literature that focuses specifically on the 1.5⁰C temperature limit and its implications for SIDS. The upcoming IPCC special report on 1.5°C represents an unique opportunity to address this important literature gap and this special issue aims to facilitate a timely and comprehensive collection of new contributions to this matter that will feed into the IPCC 1.5°C report.\nFor this special issue, we welcome submissions from variety of disciplines across both social and natural sciences that address the issue of 1.5⁰C and SIDS. Original research articles and commentaries are welcomed for submission. For commentaries, we explicitly invite contributions that revisit already existing analysis to specifically address the 1.5°C question.\nSubmissions that focus on a particular geographic region, i.e. Pacific, Caribbean, etc. or on a particular country or community within SIDS are also welcome. Submissions that address any of the below questions or any question related to 1.5⁰C and SIDS will be well received.\n- What are the projected biophysical and/or socioeconomic impacts of 1.5⁰C for SIDS?\n- What are the avoided impacts and reduced risks at 1.5⁰C compared to higher levels of warming?\n- How is vulnerability and adaptation of SIDS affected at 1.5⁰C of warming?\n- What are existing methods of adaptation and are these methods applicable at 1.5⁰C?\n- What are limits to adaption for SIDS and will they be reached at 1.5⁰C or above?\n, University of The Bahamas\n, Climate Analytics\nPolicy and Technical Expert, Climate Change,\nDeadlines for submissions\nFull papers submission: September 2017\nProposed special issue published date: March 2018\nAlbert, S. et al. (2016). Interactions between sea-level rise and wave exposure on reef island dynamics in the Solomon Islands. Environmental Research Letters, 11 (5), 54011.\nAOSIS. (2015) ‘Submission by AOSIS on the Outcome of the Structured Expert Dialogue and the 2013-2015 Review.\nBenjamin, L. and Thomas, A. (2016). ‘1.5⁰C to stay alive? AOSIS and the long term temperature goal in the Paris Agreement. IUCNAEL E-Journal\nChen, P.-Y., Chen, C.-C., Chu, L. & McCarl, B. (2015). Evaluating the economic damage of climate change on global coral reefs. Global Environmental Change, 30, s. 12–20.\nGamble, D.W., Campbell, D., Allen, T.L., Barker, D., Curtis, S., McGregor, D. and Popke, J. (2010) ‘Climate Change, Drought, and Jamaican Agriculture: Local Knowledge and the Climate Record’, Annals of the Association of American Geographers, 100 (4), 880–893.\nHernández-Delgado, E. A. (2015). The emerging threats of climate change on tropical coastal ecosystem services, public health, local economies and livelihood sustainability of small islands: Cumulative impacts and synergies. Marine Pollution Bulletin, 101 (1), s. 5–28.\nKarnauskas, K. B., Donnelly, J. P. & Anchukaitis, K. J. (2016). Future freshwater stress for island populations. Nature Climate Change, (April), s. 1–7. doi:10.1038/nclimate2987\nMertz, O., Halsnaes, K., Olesen, J.E. and Rasmussen, K. (2009) ‘Adaptation to climate change in developing countries’, Environmental Management, 43, 743–752.\nMacpherson, C. and Akpinar-Elci, M. (2013) ‘Impacts of climate change on Caribbean life’, American Journal of Public Health, 103 (1), e6.\nNurse, L. A. et al. in Clim. Chang. 2014 Impacts, Adapt. Vulnerability. Part B Reg. Asp. Contrib. Work. Gr. II to Fifth Assess. Rep. Intergov. Panel Clim. Chang. (Barros, V. R. et al.) 1613–1654 (Cambridge University Press).\nScott, D. (2012) ‘The vulnerability of Caribbean coastal tourism to scenarios of climate change related sea level rise’, Journal of sustainable tourism, 20 (6), 883–898.\nStorlazzi, C. D., Elias, E. P. L. & Berkowitz, P. (2015). Many atolls may be uninhabitable within decades due to climate change. Nature Scientific Reports, 5:14546, s. 1–9. doi:10.1038/srep14546\nTerry, J. P. & Chui, T. F. M. (2012). Evaluating the fate of freshwater lenses on atoll islands after eustatic sea-level rise and cyclone-driven inundation: A modelling approach. Global and Planetary Change.\nUNFCCC (2015) ‘Report on the structured expert dialogue on the 2013-2015 review’']"	['<urn:uuid:3d8529e2-5613-4a1a-8bee-0d53c1cd9014>', '<urn:uuid:49ca088c-46c8-467f-9c81-e807acba1b4a>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T02:03:54.148118	14	93	2403
49	whats difference normal vs special radar	Traditional radar operates like a single flashlight beam, sending out one radar signal at a time and waiting for it to bounce back, which is slower and less flexible. In contrast, AESA (Active Electronically Scanned Array) radar uses multiple small radar modules that can individually send and receive signals. These modules can work together to scan multiple areas simultaneously, making AESA radar faster, more efficient, and capable of tracking multiple targets at once. This makes it much more useful in modern military aircraft where quick and accurate information is crucial.	"['Explained: Behind India\'s move to build Super Su-30MKIs fighters\nThe upgrade program, conducted by Hindustan Aeronautics Ltd (HAL), will focus on advanced technology integration, including AESA radar, electronic warfare systems, and hypersonic missile capabilities. Defence Analyst Girish Linganna explains\nIndia is taking steps to upgrade and modernize its Su-30MKI fighter aircraft, driven by concerns about the strengthening air capabilities of neighbouring nations such as Pakistan and China. These upgrades will incorporate various new technologies developed within the country\'s domestic defence sector. As part of its continuous endeavours to boost the capabilities of its main fighter aircraft, India is working towards transforming it into a version known as the \'Super Sukhoi\'.\nThe upgrade program for India\'s Su-30MKI fighter jets will be conducted by Hindustan Aeronautics Ltd (HAL) at an anticipated cost of approximately 56,250 crore Indian Rupees (INR), with all work taking place domestically. Nevertheless, collaboration with Russia\'s United Aircraft Corporation (UAC) remains essential.\nThe Indian Air Force holds the distinction of being the largest operator of Su-30MKI aircraft outside of Russia, with 272 locally produced under license by Hindustan Aeronautics Limited (HAL). The effort to enhance the capabilities of India\'s primary fighter aircraft is slated to begin in 2026, with the objective of aligning the Su-30MKI with the advancing demands of modern warfare technology and capabilities.\nNew Delhi may encounter notable hurdles if it fails to elevate its Su-30MKI fighter jets to the ""Super Sukhoi"" standard, given that its counterparts like Pakistan and China currently possess more advanced fighter aircraft equipped with Active Electronically Scanned Array (AESA) radar technology.\nIndia\'s apprehensions regarding the effectiveness of its Su-30MKI aircraft became evident when the Pakistan Air Force introduced the JF-17 Block 3 fighter aircraft, reportedly influenced by China\'s fifth-generation J-20 \'Mighty Dragon\' aircraft.\nAlongside the JF-17 Block 3, Pakistan has also procured China\'s J-10C fighter jets, notable for their larger AESA radars featuring both transmit and receive modules. During times of increased tension caused by ongoing border disputes between the two neighbouring countries, Beijing has reportedly stationed its advanced J-20 fighter aircraft at airbases in northern India as well.\nIndia and China have also been involved in armed clashes on multiple occasions. Local media reports suggest that approximately 80 to 100 Su-30MKI fighter jets will undergo upgrades in the program\'s initial phase, with plans to eventually extend the modernization to the entire Indian Air Force fleet of these aircraft.\nNevertheless, the upgrade initiatives will exclude any modifications to the airframe and engines of India\'s Su-30MKI aircraft.\nCurrently, India\'s Su-30MKI fighter jets are equipped with AL-31 F Engines manufactured by a Russian company called NPO Saturn. As per sources, the upgrade initiatives will encompass outfitting the fighter aircraft with Beyond Visual Range (BVR) capabilities, modern electronic warfare systems, and AESA radar.\nThe AESA radar set to be installed on the Su-30MKI as part of the upgrade program is a domestically-developed version named \'Uttam\', originally being developed for the Tejas Mk1A aircraft.\nIndian defence media reports indicate that the AESA radar destined for the Su-30MKI will feature larger Transmit and Receive (TR) modules, aiming to enhance its tracking and targeting capabilities.\nApart from integrating AESA radar, the Su-30MKI upgrade program in India will also involve enhancing the fighter\'s flight control systems to support the carriage of hypersonic guided missiles and various other guided munitions.\nWithin the framework of this program, the Su-30MKI aircraft will also be furnished with a new Mission Computer, an enlarged touchscreen cockpit display, and an updated weapon systems package.\nThe program\'s objective is to annually upgrade around 25 Su-30MKI aircraft, with the goal of completing upgrades for all the aircraft by the year 2034.\nWhat is AESA Radar?\nAESA radar stands for \'Active Electronically Scanned Array\' radar. Radar is crucial for detecting and tracking enemy aircraft, ships, and missiles. It provides early warning and situational awareness, helping military pilots make informed decisions during combat.\nImagine traditional radar like a flashlight that you have to manually move around to see things in the dark. AESA radar is like having a bunch of tiny flashlights that can each point in different directions simultaneously.\nA traditional radar sends out one radar signal at a time and then waits for the signal to bounce back (like a single flashlight beam). This can be slower and less flexible. The AESA Radar, on the other hand, uses lots of small radar modules that can individually send out and receive signals. These modules can work together to scan multiple areas at once, providing faster and more precise information (like having many flashlights that can point wherever you want). So, AESA radar is faster, more efficient, and can track multiple targets at the same time, making it much more useful in modern military aircraft and other applications where quick and accurate information is crucial.']"	['<urn:uuid:f8c9778d-9272-4f9a-9b07-5cb9da7891f0>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-13T02:03:54.148118	6	90	796
50	empathy benefits risks negative effects	While empathy is beneficial as a bonding mechanism found in various species including humans and helps in understanding others' feelings, it also carries risks. The benefits include improved collaboration and better relationships. However, excessive empathy can lead to 'emotional contagion' where people absorb others' negative emotions, potentially causing burnout, stress, anxiety, and even physical illness. This is particularly concerning for caregiving professionals who often experience 'secondary traumatic stress' or 'vicarious traumatisation', leading to reduced ability to feel empathy and poorer care quality.	"[""Sharing others Feelings\nOne reason sometimes given for not being interested in attending a support group or even in talking with others who have cancer is that their worries and feelings will be contagious. That is, people sometimes worry that they will leave a group feeling worse than they did before they came. In a well-run group, that won't happen as the facilitator will be attentive and careful.\nThanks to Allison for sending on this article that confirms that this is real worry and gives some excellent tips to avoid that outcome:\nHow sharing other people’s feelings can make you sick\nEveryone says we need more empathy – but too much of it can burn you out. Mind training lessons from monks and psychopaths could help us care without crumbling\nBy Emma Young\nTANIA SINGER wasn’t the first person to put a Buddhist monk in an fMRI machine. But the neuroscientists who had scanned supposedly caring, sharing brains before did it to find out where empathy comes from. Singer was looking for ways to avoid it.\nFew people would argue that the world is cursed with an excess of empathy. But we are starting to discover that our capacity to share other’s emotions and take their perspective comes with a sting in its tail. Overdosing on the misfortunes of others is not just a problem for those in high-exposure professions such as nursing. All of us are vulnerable to catching the pain of others, making us angrier, unhappier, and possibly even sicker.\nFortunately, work on locating the root of empathy in the brain has also led to the discovery that with the right training, we might be able to tune how much we let others’ emotions affect us. This could allow us the best of both worlds – to care, without letting it consume us.\nEmpathy is undeniably a good thing. Understanding how others are feeling is a bonding mechanism that we are finding in an increasing number of animals, including dolphins and rats. In humans, primatologist Frans de Waal of Emory University in Atlanta, Georgia, has suggested that being affected by another’s emotional state was the earliest step in our evolution as a collaborative species.\nBut the pitfalls will be apparent to anyone who has been in a room full of babies. If one starts crying, pretty soon, they’re all at it. Babies don’t understand the difference between their own emotions and those being felt by others, and so what one feels, they all feel. Negative and positive emotions alike spread like a virus. As our sense of self develops, we learn to distinguish other people’s emotions from our own, although a variety of experiments, most recently studying our behaviour in online social networks, indicate we are not entirely free of the risk of emotional contagion (see “Socially contagious“).\nThat’s because the distinction between what we and others feel isn’t terribly clear to our brains. Singer, then at University College London (UCL), and her colleagues demonstrated this in 2004 when they put 16 romantic couples into an MRI scanner. When they gave the volunteers a painful electrical shock, this elicited activity in brain regions known to respond to physical pain and also in regions tuned to emotional pain. But when volunteers saw their loved one get a shock, no activity registered in their physical pain centres – while the emotion regions lit up like fireworks. Notable among these was the anterior insula, where a lot of the coordination between brain and body takes place.\nSince then, many other studies have confirmed that this “empathy for pain” network exists, and that it doesn’t distinguish whether the pain you’re observing is physical or psychological. “The basic principle is the same,” says Singer, who is now at the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig, Germany.\nWhat’s more, over the past few years it has become apparent that we don’t just catch pain from those we are intimate with. The first hints came from people in care-giving professions who often see the stress and pain of others, such as hospice staff, nurses, psychotherapists and paediatricians. Since the early 1990s, a kind of empathy burnout has increasingly been documented – given names including “secondary traumatic stress” and “vicarious traumatisation”. Symptoms include lowered ability to feel empathy and sympathy, increased anger and anxiety, and more absenteeism (see “The hurt locker”). Various studies link these symptoms with an indifferent attitude to patients, depersonalisation and poorer care.\nIt’s perhaps unsurprising that empathy burnout can affect people frequently surrounded by other people’s pain. But a recent spate of experiments suggests that the dark side of empathy spells trouble for everyone. You can “catch” stress any time you understand someone else’s pain and share in it, activating your empathy for pain network.\nRead more: https://www.newscientist.com/article/mg23030732-900-how-sharing-other-peoples-feelings-can-make-you-sick/"", '3 Types of Empathy - Goleman and Oprah\nBeing cool in crisis seems essential for our being able to think clearly. But what if keeping cool makes you too cold to care? In other words, must we sacrifice empathy to stay calm? That’s the dilemma facing those who are preparing top teams to handle the next Katrina-like catastrophe we might face. Which gets me to Paul Ekman, a world expert on emotions and our ability to read and respond to them in others. Paul and I had a long conversation recently, in which he described three very different ways to sense another person’s feelings.\nThe first is “cognitive empathy,” simply knowing how the other person feels and what they might be thinking. Sometimes called perspective-taking, this kind of empathy can help in, say, a negotiation or in motivating people. A study at the University of Birmingham found, for example, that managers who are good at perspective-taking were able to move workers to give their best efforts.\nBut there can be a dark side to this sort of empathy – in fact, those who fall within the “Dark Triad” – narcissists, Machiavellians, and sociopaths (see Chapter 8 in Social Intelligence) – can be talented in this regard, while having no sympathy whatever for their victims. As Paul told me, a torturer needs this ability, if only to better calibrate his cruelty – and talented political operatives no doubt have this ability in abundance.\nKatrina’s devastation, we all saw, was amplified enormously by the lackadaisical response from the very agencies that were supposed to manage the emergency. As we all witnessed, leaders at the highest levels were weirdly detached, despite the abundant evidence on our TV screens that the disaster’s victims were doubly victimised by the indifference to their suffering.\nCertainly empathy qualifies as one critical measure of the right leader in a crisis, along with being cool under pressure. But exactly what kind of empathy should we look for? When it comes to the right leader for a crisis, cognitive empathy alone seems insufficient .\nThen, Paul told me, there’s “emotional empathy,” – when you feel physically along with the other person, as though their emotions were contagious. This emotional contagion, social neuroscience tells us, depends in large part on the mirror neuron system (see Chapter Three in Social Intelligence). Emotional empathy makes someone well-attuned to another person’s inner emotional world, a plus in any of a wide range of callings, from sales to nursing – let alone for any parent or lover.\nOne downside of emotional empathy occurs when people lack the ability to manage their own distressing emotions can be seen in the psychological exhaustion that leads to burnout. The purposeful detachment cultivated by those in medicine offers one way to inoculate against burnout. But the danger arises when detachment leads to indifference, rather than to well-calibrated caring.\nFinally, there’s what Paul calls “compassionate empathy,” which I’ve written about using the term “empathic concern” (see Chapter Six in Social Intelligence). With this kind of empathy we not only understand a person’s predicament and feel with them, but are spontaneously moved to help, if needed.\nPaul told me about his daughter, who works as a social worker in a large city hospital. In her situation, he said, she can’t afford to let emotional empathy overwhelm her. “My daughter’s clients don’t want her to cry when they’re crying,” as he put it. “They want her to help them figure out what to do now – how to arrange a funeral, how to deal with the loss of a child.” Empathic concern was the vital ingredient missing in the top-level response to Hurricane Katrina.\nRespectfully quoted from Daniel Goleman’s website below.']"	['<urn:uuid:9b27ca0b-e3fa-42f1-b2c7-6917e3661a4f>', '<urn:uuid:92f59514-bdeb-48cc-9173-1e39270a1369>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T02:03:54.148118	5	82	1420
51	How did street vendors sell warm gingerbread?	Some dealers would take fresh gingerbread hot from the oven, wrap it in heavy cloths, and sell it on the streets while it was still warm.	['Chaucer mentioned gingebreed (gingerbread) in his writings in 1386, however, he gave no description of the product. As with early versions, his experience was probably with that made from honey, spices, and breadcrumbs or ground almonds.\nIn Two Fifteenth-Century Cookery-Books, Gyngerbrede was made: Take a quart of hony, & sethe it, & skeme it clene; take Safroun, pouder Pepir, & throw ther-on; take gratyd Brede, & make it so chargeaunt that it wol be y-leched; then take pouder Canelle, & straw ther-on y-now; then make yt square, lyke as thou wolt leche it; take when thou lechyst hyt, an caste Box leaves a-bouyn, y-stkyd ther-on, on clowys. And if thou wolt haue it Red, coloure it with Saunderys y-now. Austin, Thomas. Two Fifteenth-Century Cookery Books. Harleian MS. 279…and Harl. MS. 4016. London, 1888. Early English Text Society, Oxford Series, No. 91.\nWhat did we just read? Old English aside, this basically translates into bread crumbs combined with honey, spices, and saffron. Such products were more candy-like and less gingerbread-like than we know today.\nIn old London, gingerbread was sold in stalls at all times of year. It varied in form from that of a cake to bars which were sliced, or it was made into nuts and sold by the dozen. Charles Carter’s version  in “long rolls or cakes” was copied later by Elizabeth Moxon in 1764.\nDesigns were sometimes stamped into the cakes as evidenced in John Murrell’s version penned in 1621. “Roule it in round cakes and print it with your moulds…”.\nWhile George Read’s book contained one receipt in 1854 in which “nuts” were made by encasing an almond in gingerbread dough, the term generally meant simply gingerbread baked into small balls [Frederick Nutt, 1790] or rolled and cut into rounds. Eliza Leslie instructed the cook to flour the hands and roll small pieces of dough into “little round balls”.\nThe selling of gingerbread in markets and stalls was a lucrative profession in former times. “In the days of the early Georges, for instance, gingerbread was hawked about by a smartly dressed lad…but older folk were also employed.”\nAs will be shown later, George Read agreed with men selling gingerbread, but there were accounts of women who worked as gingerbread sellers. By 1614, Ben Johnson had included the character of a gingerbread seller into his play, Bartholomew Fair.\n“Joan Trash, the gingerbread woman, crying her ‘Fine gilt gingerbread!’ The last refers to the custom of painting the top in fine gold leaf, a custom that, “we do not see” by the 1870’s. – Chatterbox. 1876. Boston.\nAlthough it had been the custom for decades by then, in 1822 cooks were cautioned that, “…the use of this poisonous material [gilt] for gilding gingerbread and sweetmeats cannot be too much reprobated”. – One Thousand Experiments in Chemistry.\nHaving established the making and selling of gingerbread as a profession in Britain and America, let’s note that the same was true of France and Germany. In 1878, when the treat had fallen slightly out of favor in the markets of London, it was noted to still be as popular as ever in those two countries. – The Lancet. Aug. 31, 1878.\nThe gingerbread seller has been illustrated in books and magazines from London and Paris attesting to the frequency with which the treats were sold.\nSome dealers took the gingerbread fresh and hot from the oven, wrapped it in heavy cloths, and took to the streets to sell it while yet warm from the oven.\nIn 1854, George Read claimed the origins of gingerbread were Asiatic or Eastern because, “…the natives of these countries are extremely fond of sweetmeats and spiced bread”. An appreciation of a delicacy doesn’t necessarily mean that culture was the first to make and enjoy it, and he did proclaim it to be universal though, carried to excess”, in Holland.\nThose who wished to ingratiate himself with a family often depends in no small degree, on the quality and quantity of presents which he makes in gingerbread.\nThe many references Read made to the making of gingerbread inferred it was made by men. “The receipt for it descended from father to son as an heirloom and was kept secret outside the family.”\nThe British also appreciated gingerbread and Read claimed lovers often made presents of gingerbread nuts and “fairings” to their mistresses and children would spend their last penny on gingerbread made into the shape of a, “horse, cock in breeches, or old man and woman”. Of the three, the only shape that became universal was the man and woman.\nThere is a myth, or let’s say at least, that I found no first hand documentation to back up the story, that the first gingerbread men were made for Queen Elizabeth I.\nShakespeare penned a line about spending one’s last penny in the world to buy gingerbread in Love’s Labour Lost. Wouldn’t it be rather romantic to think of him presenting his wife, Anne Hathaway, with a gift of gilt gingerbread?\nThe receipts Read gave were intended for commercial use although one he described as, “an old receipt”, [old in 1854] might be a quantity practical enough for family use as it kept rather well. He acknowledged that it was not fermented with yeast, as was bread, and the means of “gasifying” it was of comparatively recent origin.\nTake refined sugar, 6 lbs.; damask rise water, 3 pints, or enough to make a syrup of it, of the same consistence or thickness of treacle, which keep for use. Take ginger, coriander seed, caraway seed, of each, in fine powder, 2 oz.; fennel seed, aniseed, each, in fine powder, 1 oz.; cloves, in fine powder, 1 oz.; mix them well together in a mortar which reserve. Take of the former syrup 1 quart, of the reserved powder 2 oz. (more or less, as you would have it to taste of the spice); fine wheat flour, 3 quarts, or so much as may make it up into a pretty stiff paste; roll it out into thin square cakes and so bake it. This exceeds all other preparations of gingerbread whatsoever.\nThe color of the gingerbread ranged from golden brown to dark brown depending on the quality of treacle or syrup used in making it up.\nWhen gingerbread first contained a substance to make it lighter and therefore softer, the transformation was the result of the addition of pearl ash or potash. Keeping the dough some time before baking it gave the leavening agent time to act with the treacle producing carbonic gas. It was a slow process. Carbonate of magnesia and soda were also in time, and produced a product lighter and spongier than that made with potash. – Read, George. The Complete Biscuit and Gingerbread Baker’s Assistant. 1854. London.\nThe following should invoke interest among my readers since Christmas is looming on the horizon.\nGINGERBREAD CAKES – Richard Briggs. 1788. [The English Art of Cookery] Rub one pound of butter into three pounds of flour, one pound of sugar, two ounces of ginger beat fine and sifted, and a large nutmeg grated; then take a pound of treacle, a gill of cream, make them warm together, and make up the bread stiff, roll it out, and make it into thin cakes, or cut it round with a tea-cup or glas, or make it into nuts, or any form or shape you please, put it on oven-plates, and bake it in a slack oven.\nIn 1867, a receipt was published for a pound cake flavored as gingerbread which was probably lighter than anything in Read’s repertoire.\nPound Cake Gingerbread. One cup of sugar, two cups of molasses, one of butter, one cup of buttermilk or sour cream, four cups of flour, four eggs, a tablespoonful of ground ginger and one of cloves, a teaspoonful of soda dissolved in hot water, and half a teaspoonful of cream of tartar poured in last of all.\nThere were no further directions given with the recipe for how it was made. – Barringer, Maria Massey. Dixie Cookery; or, How I Managed my Table for Twelve Years.\nBarringer’s Light Gingerbread instructed the cook to mix the soda into the molasses and then mix with the remaining ingredients. I would suggest doing so with the pound cake. Mix the cream of tartar and spices with the flour. Cream the butter with the sugar and eggs. Mix the soda with the molasses, and add it to the dry ingredients with the buttermilk or sour cream. The soda could also be mixed with the buttermilk instead of the molasses as the maker prefers.']	['<urn:uuid:1f553821-f7d6-4dcb-97e3-7bf04dc4854d>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T02:03:54.148118	7	26	1436
52	college preparation courses grades impact uc csu eligibility minimum performance requirements implications	For UC/CSU eligibility, students must complete specific A-G requirements with grades of 'C' or better. These include 2 years of history/social science, 4 years of English, 3 years of math, 2 years of lab science, 2 years of the same foreign language, 1 year of visual/performing arts, and 1 year of college prep electives. While a 4.0 GPA represents straight As in regular courses, students can boost their GPA through honors and AP courses. However, not all high school graduation requirements align with A-G requirements. If a student receives a 'D' grade, they must either retake the course or, in some cases, can validate it by earning a 'C' or better in a higher-level course, though validation rules vary by subject and university system.	"[""Today’s Wonder of the Day was inspired by ALEX. ALEX Wonders, “What is gpa ” Thanks for WONDERing with us, ALEX!\nWhat do you want to do with your life after you graduate from high school? If you're like many kids, you may have your sights set on attending a particular college. And if you want to go to college, then you know what you need to do in high school: get good grades!\nHow do you keep track of your grades? Do you count the number of As and Bs you receive? Most kids instead monitor one particular number calculated by most schools. What are we talking about? Your GPA, of course!\nGPA stands for grade point average. It's one number that sums up how well you've performed academically in a particular semester (often called your term GPA) or over the course of your whole high school career (known as your cumulative GPA).\nMost grade point averages are calculated on a four-point scale. That means the best possible GPA is a 4.0. To calculate your GPA, you need to add up the points you receive for each individual grade you receive and then divide by the number of classes you have.\nFor example, a typical scale assigns grade values as follows: A = 4, B = 3, C = 2, D = 1, and F (or sometimes U) = 0. Grades with a minus or a plus are given a value above or below the corresponding letter grade. In many systems, an A- would be 3.7 points.\nIf you have five classes and you get an A in each class, your total points would be 20 (five As at four points each). Dividing by five (your number of classes), gives you a GPA of 4.0.\nWhat if you don't get all As? For example, let's assume you got two As and three Bs instead. That's a total of 17 points (two As at four points each and three Bs at three points each). Dividing by five (your number of classes) gives you a GPA of 3.4.\nIf you think of a B as a three and an A as a four, then you can see how a 3.4 GPA accurately reflects that your grades were slightly closer to Bs than As (three Bs and two As).\nNot all schools use a four-point GPA scale. Schools that use what is known as a weighted GPA system might have a five-point GPA scale. In a weighted system, honors and advanced placement (AP) classes are given higher point values to reflect that they are tougher classes.\nFor example, in many systems, honors classes will be given an extra half-point (0.50), whereas advanced placement classes will often be given a whole extra point (1.0). Thus, an A in an honors class would equate to a 4.50 and an A in an advanced placement class would equate to a 5.0.\nEven though the maximum GPA in a weighted system would be 5.0, most students would not be able to achieve a 5.0 GPA, because not all classes are usually available as advanced placement or honors courses. When applying to colleges, many schools will calculate all student GPAs on an unweighted scale to make comparisons between students easier.\nIs your GPA important? Of course, it is! Studies show higher GPAs correlate to higher future earnings. A high GPA will also open the door to many opportunities in terms of scholarships and colleges.\nIn the long run, though, your GPA is just one indicator of your academic performance. Work experience, extracurricular activities, community service, and many other factors can contribute to achieving your dreams for the future!"", 'Welcome! The Topic For Today Is… A-G Requirements! Requirements Validations & GPAs Honors Practice Miscellaneous 200 200 200 200 200 400 400 400 400 400 600 600 600 600 600 800 800 800 800 800 1000 1000 1000 1000 1000 Bonus Question: 5000 Points Requirements: 200 • Question: How many years of History/Social Science courses are students required to complete in order to meet the A-G Admission Requirement at most universities? • Answer: Students must complete at least two years of history/social science, which include one year of world history, cultures and geography and one year of U.S history. Requirements: 400 • Question: What is the minimum letter grade a student must receive in an A-G course in order to meet the A-G Admission Requirement at most universities? • Answer: A student must receive a grade of “C” or better in all A-G courses. Requirements: 600 • Question: How many years of Math are students recommended to take? • Answer: Students are recommended to complete 4 years of college-preparatory mathematics that include the topics covered in elementary and advanced Algebra and two- and three- dimensional geometry. Requirements: 800 • Question: What does each A-G letter represent? • Answer: A: Social Science, B: English, C: Mathematics, D: Laboratory Science, E: Language other than English, F: Visual and Performing Arts, G: College Preparatory Elective Requirements: 1000 • Question: How many years of History, English, Math, Lab Science, Language other than English, Visual and Performing Arts are students required to complete of each subject in order to meet the A-G Admission Requirement at most universities? • Answer: Students must complete 2 years of History, 4 years of English, 3 years of Math, 2 years of Lab Science, 2 years of Language other than English, 1 year of Visual and Performing Arts, 1 year of an elective. Validations & GPAs: 200 • Question: Are all courses completed in high school used for the CSU/UC GPA calculation? • Answer: No, only A-G courses completed from the summer before the sophomore year up to the classes completed in the summer following the junior year will be used to calculate the initial CSU/UC GPA. Validations & GPAs : 400 • Question: Which A-G course grades are not calculated in the initial CSU/UC admissions GPA? • Answer: Grades earned in A-G classes completed in middle school, the 9th grade, and the 12th grade are not factored into the initial GPA calculation. Validations & GPAs : 600 • Question: What is the benefit of A-G validations? • Answer: Validations allow students to receive credit toward the CSU/UC A-G admission requirement for specific A-G classes completed with a “D.” To receive the validation for these specific A-G classes, a student must receive a “C” or better in the second semester or in a higher level class. Ask your counselor or Cal-SOAP advisor for more details. Validations & GPAs : 800 • Question: A student earned a “D” in Algebra 1, “C” in Geometry, and “B” in Algebra 2. Has this student met the CSU/UC A-G eligibility requirement for the Math component? • Answer: Yes. According to the validation rule, a “C” or better received in Algebra 2 will validate a lower letter grade in Algebra 1. Validations & GPAs : 1000 • Question: What is the chemistry validation rule for the CSU and the UC? • Answer: The CSU will allow a second semester grade of “C” or better to validate a first semester grade of “D”. The UC will not allow a chemistry validation. Honors: 200 • Question: How can students boost their GPA and earn extra points? • Answer: Students can boost their GPA and earn extra points by completing A-G approved honors, AP, and IB classes with a “C” or better. Honors: 400 • Question: Does receiving a “D” in an AP or honors course allow for extra points in the student’s GPA calculation? • Answer: No, a grade of a “C” or better must be obtained in order to receive extra points towards a student’s GPA. Honors: 600 • Question: Which of the following best describe International Baccalaureate classes? A: They’re social science classes B: They’re honor’s level classes C: They’re classes for English Language Learners • Answer: International Baccalaureate classes are honor’s level classes. Honors: 800 • Question: Does passing an AP course or passing an AP exam guarantee college credit? • Answer: No, you must check with the university to see what their requirements are for accepting AP courses for college credit. Honors : 1000 • Question: Do all honors courses give a student extra points towards their GPA calculation? • Answer: No, only classes designated as A-G approved honors classes are eligible for extra points towards the GPA. Refer to the UC Doorways website to see which honors courses are approved at your high school. Practice: 200 • Question: If Ally took one year of world history and one year of US history along with 4 years of English is she A-G eligible for the A (history) and B (English) requirement? • Answer: Yes, she is A-G eligible for the A and B subject requirements. Practice: 400 • Question: If Nicole took AP Spanish, AP Literature, AP Calculus, AP US History, and Honors Chemistry, how many honors points can she receive towards her GPA? • Answer: Although all courses are validated for AP and honors extra points, Nicole can only receive 8 points towards her GPA calculation. Practice: 600 • Question: If Kevin took 1 year of French and 1 year of Spanish, has he completed the E category (Language other than English) of the A-G subject requirement? • Answer: No, the E category can only be fulfilled by taking two years of the same language. Practice: 800 • Question: How many different classes can Sam take to make up for the “D” she received in the second semester of Algebra 1? • Answer: She can take ? classes; she can retake Algebra 1 or she can take Algebra 2, trigonometry, math analysis, pre-calculus, calculus, or statistics. Practice: 1000 • Question: Maya received a “D” in her high school Spanish Literature class and a “C” in the Spanish Literature class she took at the local community college. Will the highest grade be used in her CSU/UC GPA calculation or will both grades be used? • Answer: Both grades will be used to calculate her GPA. If she had retaken the same Spanish Literature class at her high school, only the highest grade would be used to calculate her GPA. Miscellaneous: 200 • Question: How many extra grade points can you earn from passing approved honors, AP, and IB classes with a “C” or better? • Answer: You can earn a maximum of 8 points from passing approved honors, AP, and IB classes with a “C” or better. Miscellaneous : 400 • Question: If a class is required for high school graduation, is it an A-G requirement as well? • Answer: No, the classes that you need to graduate from high school do not necessarily meet the A-G Subject Requirements (for example, Physical Education). Miscellaneous : 600 • Question: Where can I find out if the courses I am taking are A-G eligible? • Answer: Course listings can be found at the UC Doorways website: https://doorways.ucop.ed/list Miscellaneous : 800 • Question: How many times can a course be repeated? • Answer: If applying to a University of California (UC) a course can only be repeated once but if applying to a California State University (CSU) it can be repeated as many times as necessary. Miscellaneous : 1000 • Question: Does a repeated course allow for improvements in my GPA? • Answer: Yes, but a course can only be repeated if a grade lower than a “C” was received. The best grade will be used in the GPA calculation Bonus Question: 5000 • Question: What is the rationale of the A-G requirements? List 4 points. • Answer: 1. To ensure students can participate fully in the first year program at the university in a broad variety of fields of study. 2. To prepare students for courses, majors and programs offered at the university. 3. To offer students a body of knowledge that will provide breath and perspective to new, more advanced studies. 4. To instill students with essential critical thinking and study skills.']"	['<urn:uuid:42870c9a-559d-40f6-9f72-b65d0952fb83>', '<urn:uuid:338adeb0-7891-4e24-bd79-61646c4c42b0>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-13T02:03:54.148118	12	124	1999
53	define anomie social disorganization consequences psychological medical effects	Anomie has multiple effects across different domains. In social terms, it refers to the breaking down of institutions like family and employment in inner-city neighborhoods, leading to the absence of social norms and conventions. Psychologically, it causes individuals to value personal interests above social ones and struggle to adapt to social control. In medical contexts, anomie manifests as a language disorder where individuals have difficulty naming things, particularly in aphasic syndromes and aging populations.	"[""Grouping based on economic or social class and characterized by the unequal distribution of wealth, power, and prestige.\nSegment of the population whose members are at a relatively similar economic level and who share attitudes, values, norms, and an identifiable lifestyle.\nCulture of Poverty\nA separate lower-class culture, characterized by apathy, cynicism, helplessness, and mistrust of social institutions, such as schools, government agencies, and the police, that is passed from one generation to the next.\nThe lowest social stratum in any country. whose members lack the education and skills needed to function successfully in modern society.\nSocial Structure Theory\nThe view that disadvantaged economic-class position is a primary cause of crime.\nSocial Disorganization Theory\nBranch of social structure theory that focuses on the breaking down of institutions such as the family, school, and employment in inner-city neighbourhoods.\nBranch of social structure theory that sees crime as a function of the conflict between people's goals and the means available to obtain them.\nThe anger, frustration, and resentment, experienced by people who believe they cannot achieve their goals through legitimate means.\nCultural Deviance Theory\nBranch of social structure theory that sees strain and social disorganization together resulting in a unique lower-class culture that conflicts with conventional social norms.\nA set of values, beliefs, and traditions unique to a particular social class or group within a larger society.\nProcess whereby values, beliefs, and traditions are handed down from one generation to the next.\nAn area undergoing a shift in population and structure, usually from middle-class residential to lower-class mixed use.\nAs working and middle class families flee inner-city poor areas, the most disadvantaged population is consolidated in urban ghettos.\nSocial control exerted by cohesive communities, based on mutual trust, including intervention in the supervision of children and maintenance of public order.\nA lack of norms or clear social standards. Because of rapidly shifting moral values, the individual has few guides to what is socially acceptable.\nView that anomie results when socially defined foals (such as wealth and power) are universally mandated but access to legitimate means (such as education and job opportunities) is stratified by class and status.\nInstitutional Anomie Theory\nThe view that anomie pervades modern culture because the drive for material wealth dominates and undermines social and community values.\nThe goal of accumulating material goods and wealth through individual competition; the process of being socialized to pursue material success and to believe it is achievable.\nGeneral Strain Theory (GST)\nThe view that multiple sources of strain interact with an individual's emotional traits and responses to produce criminality.\nNegative Affective States\nAnger, frustration and adverse emotions produced by a variety of sources of strain.\nValues, such as toughness and street smarts, that have evolved specifically to fit conditions in lower class environment.\nA value system adopted by lower class youths that is directly opposed to that of the larger society.\nA form of culture conflict experienced by lower class youths because social conditions prevent them from achieving success as defined by the larger society.\nMiddle-Class Measuring Rods\nThe standards by which authority figures, such as teachers and employers, evaluate lower class youngsters and often prejudge them negatively.\nIrrational hostility evidenced by young delinquents, who adopt norms directly opposed to middle class goals and standards that seem impossible to achieve.\nThe view that lower class youths, whose legitimate opportunities are limited, join gangs and pursue criminal careers as alternative means to achieve universal success goals."", 'Anomie is a concept that refers, in a general way, to the absence of law, norms or conventions. The word, as such, comes from the Greek ἀνομία (anomy).\nThis etymology shows its use in the field of both social sciences and psychology, in which it refers to the absence of norms or conventions in a society or person, or their disrespect or degradation by an individual or a group of individuals\nThe consequences of anomie range from maladjustment to social norms, to the transgression of laws and antisocial behaviors.\nAnomie, in medicine, can also refer to a language disorder characterized by difficulties in remembering the name of things. In this sense, the word is composed with the voices a- , which denotes denial, and the Latin nomen, which means ‘name’.\nIn the disciplines of social sciences and sociology, anomie is related to the lack of social norms or conventions or their degradation as a product of a set of factors.\nThe French sociologist Émile Durkheim considered that human nature needed an authority that would put limits (norms, conventions) on his passions and desires, since his absence could become destructive to the individual himself.\nDurkheim also refers to how inequalities resulting from the social and economic dynamics of capitalism influence social anomie. Individuals, feeling that the norms and conventions under which they live lack a sense of justice and fairness, cease to respect them because they believe they have lost meaning for them.\nIn fact, Robert K. Merton considers that anomie can be considered as a consequence of the dissociation between the cultural aspirations of a society and the means or paths that individuals have to reach them, which means that in order to achieve these goals individuals must Sometimes violating certain social norms, which results in a breakdown from which antisocial behaviors originate.\nAnomie in psychology\nAnomie is a psychological disturbance that makes it difficult for a person to behave according to the set of legal, cultural or moral norms and conventions that govern life in society.\nHuman behavior is constantly regulated by this set of explicit or implicit rules, which constitute a model of social control, which the individual has difficulty adapting, due to his own human nature.\nIn psychological anomie, the individual values his interests and individual motivations above social interests because he has ceased to respect them since for him all these norms and conventions have lost their meaning.\nAsylum-bearing anomie is a behavior that is characterized by the negative and erroneous vision that an individual develops of himself. In asylum-oriented anomie, the person has a distorted idea of themselves and their abilities, so that they do not feel in a position to succeed in a difficult or adverse situation, rather they react by demonstrating their limitations or disabilities. It can be suffered by an individual or an entire social group.\nAsylum-bearing anomie is a consequence of situations of marginalization and exclusion, whether social, racial or ethnic. Groups that suffer from this type of discrimination feel that they cannot achieve what they set out or want, nor satisfy their personal needs or motivations, although in reality they can. Therefore, it seriously affects the personal and economic growth of individuals. The opposite of asylum-bearing anomie is social resilience, which is the ability to adapt or overcome adversity.\nAnomie in medicine\nAnomia is known as a language disorder in which the individual has difficulty calling things by name. It can be presented partially or totally.\nAnomie is one of the main characteristics of aphasic syndromes. With age, it is considered normal for most people to suffer a moderate degree of anomie.']"	['<urn:uuid:a7ce954d-0ec8-48b7-8192-a8b922ef60ca>', '<urn:uuid:957a428b-130c-4dd9-b392-e03eacecd3a8>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T02:03:54.148118	8	74	1174
54	sun symbolism protection pyramids ancient egypt	The sun was considered a primary element of life in ancient Egypt, worshipped through various deities including Horus, Ra, and Amun-Ra. This solar symbolism was reflected in protection symbols like the uraeus cobra, which represented the 'fiery eye of Re' and was believed to spit fire at enemies. This connection between sun worship and protection ultimately influenced tomb architecture, as seen in the Step Pyramid of Djoser, which was designed with steps to allow the Pharaoh's soul to climb into heaven.	"['This symbol represents the horizon from which the sun emerged and\ndisappeared. The horizon thus embodied the idea of both sunrise and\nsunset. It is similar to the two peaks of the Djew\nor mountain symbol with solar disk in the center. Both the beginning\nand the end of each day was guarded by Aker, a double lion god. In the\nNew Kingdom, Harmakhet (""Horus in the\nHorizon"") became the god of the rising and setting sun. He was pictured\nas a falcon, or as a sphinx with the body of a lion. The Great Sphinx\nof Giza is an example of ""Horus in the Horizon"".\nThis symbol represents the Underworld or Land of the Dead. Originally\nit meant the horizon of the sun set. Later, it became the symbol of the\nwest bank of the Nile, where the sun set and also where the Egyptians\ntraditionally buried their dead.\nSymbol of eternal life. The gods are often seen holding an ankh to\nsomeone\'s lips this is considered to be an offering of ""The Breath of\nLife"". The breath you will need in the afterlife.\nThe atef crown was worn by Osiris. It is\nmade up of the white crown of Upper Egypt and the red feathers are\nrepresentative of Busiris, Osiris\'s cult center in the\nThe Ba is what we might call someones personality. It would leave the\nbody at the time of death. During the days the Ba would make itself\nuseful, at night it would return to the tomb. At this time, it would\nlook for the person to which it belonged. This would be the mummy,\nhowever, often the egyptians would supply the Ba with a statue in the\nlikeness of the deceased in case the mummy was lost\nDuring mummification the internal organs were removed and placed in\nfour containers. These containers often have human or animal-headed\nstoppers. The word, canopic, comes from the Greek name of the local god\nof Canopus in the Nile delta, who was represented as a human-headed\npot. Canopic jars can be made of limestone, alabaster, wood, pottery,\nor even cartonnage. The heads of the canopic jar represented the Four Sons of Horus\n. . . . .From\nto right they are;\nImsety: The human headed\nof the liver\nThe Red Crown. This was the crown that represented Lower\nIt is believed that the Djed is a rendering of a human backbone. It\nrepresents stability and strength. It was originally associated with\nthe creation god Ptah. Himself being called\nthe ""Noble Djed"". As the Osiris cults took\nhold it became known as the backbone of Osiris\n. A djed column is often painted on the bottom of coffins, where the\nbackbone of the deceased would lay, this identified the person with the\nking of the underworld, Osiris. It also\nacts as a sign of stability for the deceased\' journey\ninto the afterlife.\nWhich means mountain, the symbol suggests two peaks with the Nile\nvalley in the middle. The Egyptians believed that there was a cosmic\nmountain range that held up the heavens. This mountain range had two\npeaks, the western peak was called Manu, while the eastern peak was\ncalled Bakhu. It was on these peaks that heaven rested. Each peak of\nthis mountain chain was guarded by a lion deity, who\'s job it was to\nprotect the sun as it rose and set. The mountain was also a symbol of\nthe tomb and the afterlife, probably because most Egyptian tombs were\nlocated in the mountainous land bordering the Nile valley. In some\ntexts we find Anubis, the gaurdian of the\ntomb being referred to as ""He who is upon his\nmountain."" Sometimes we find Hathor takeing\non the attributes of a deity of the afterlife, at this time she is\ncalled ""Mistress of the Necropolis.""\nShe is rendered as the head of a cow protruding from a mountainside.\nFeather of Maat\nRepresents truth, justice, morality and balance. It was pharaoh\'s job\nto uphold Maat. When a pharaoh died, Maat was lost and the world was\nflung into chaos, only the coronation of a new pharaoh could\nFetish of Osiris\nAn animal skin hanging from a stick, this is a symbol of Osiris and Anubis.\nFlail and Crook\nA symbol of royalty, majesty and dominion.\nThe heb glyph represents an alabaster bowl. These bowls were used for\nspecial purposes like festivals or feasts.\nThe Heb-Sed glyph Is a combination of the heb\nglyph and the sed glyph. It represents the Heb-Sed or Jubilee festival. This\nfestival was celebrated on the 30th\nyear of a pharoah\'s rule. It was believed to renew the pharaohs\nstrength of rule over the two lands.\nThe White Crown. This was the crown of Upper Egypt\nThis symbol represents a heart. The Egyptian believed the heart was the\ncenter of all consciousness, even the center of life itself. When\nsomeone died it was said that their ""heart has departed."" It was the\nonly organ that was not removed from the body during mummification. In\nthe Book of the dead, it was the heart that\nwas weighed against the feather of Maat\nto see if an\nindividual was worthy of joining Osiris in\nrepresent the west or western desert AKA the land of the dead. The\nhorizon on which Ra dies at night and enters the underworld.\nThe ka is usually translated as ""soul"" or ""spirit"" The ka came into\nexistence when an individual was born. It was believed that the\nram-headed god Khnum crafted the ka on his\npotter\'s wheel at a persons birth. It was thought that when someone\ndied they ""met their ka"". A persons ka would live on after their body\nhad died. Some tombs included model houses as the ka needed a place to\nlive. Offerings of food and drink would be left at the tomb entrance so\nthe ka could eat and drink.\nThe blue crown was a ceremonial crown often worn in\nThis symbol represents a lamp or brazier on a stand from which a flame\nemerges. Fire was embodied in the sun and in its symbol the uraeus which spit fire. Fire also plays a\npart in the\nEgyptian concept of the underworld. There is one terrifying aspect of\nthe underworld which is similar to the christians concept of hell. Most\negyptians would like to avoid this place with its fiery lakes and\nrivers that are inhabited by fire demons.\nRepresents truth, justice, morality and balance. Deities are often seen\nstanding on this symbol, as if standing on a foundation\nThis symbol represents a heavy beaded necklace with a crescent shaped\nfront and a counter piece at the rear. It was a symbol associated with\nthe goddess Hathor and her son, Ihy. In\nfact , Hathor was known as the ""Great Menat"". We often see Hathor using the Menat as a conduit through\nwhich she passes her power. It was representative of the ideas of joy,\nlife, potency, fertility, birth, and rebirth. It was not uncommon in\nthe New Kingdom, to see the king offering the Menat to Hathor. This probably meant to represent the\nking symbolically with the goddess\' son, Ihy. This idea of divine\nassimilation was common, although the best examples are of the king\nrepresenting the falcon god, Horus.\nA scribes pallet. Writing was a very important skill to the ancient\nEgyptians. It was practiced by a group called scribes. The writing\nequipment used by scribes consisted of a palette, which held black and\nred pigments, a water jar, and a pen. To be a scribe was a favorable\nposition, even some kings and nobles are show proudly displaying\nShrine in which divine statues were kept, especially in temple\nsanctuaries. A small wooden naos was normally placed inside a\nmonolithic one in hard stone; the latter are\ntypical of the Late\nPeriod, and sometimes elaborately decorated. Also used as a term for\nThis symbol represents gold which was considered a divine metal, it was\nthought to be the flesh of the gods. Its polished surface was related\nto the brilliance of the sun. Gold was important to the afterlife as it\nrepresents aspects of immortality. By the New Kingdom, the royal burial\nchamber was called the ""House of Gold.""\nThe two ladies of upper and lower Egypt. Nekhbet the vulture goddess,\nprotrectress of Upper Egypt and Wadjet the cobra goddess associated\nwith lower Egypt and the Nile delta. There is a beautiful rendering\nof these two ladies in the White Chapel\nof Senwosret I.\nA goddess portrayed as a vulture. Protrectress of\nA striped headcloth worn by Pharaohs.\nThe Gods or having to do with the gods.\nThe Egyptians would put a notch in a palm branch to mark the passing of\na year in the life of a pharaoh. It symbolized the measure of time.\nThis symbol depicts the sky as a ceiling which drops at the ends, the\nsame way the real sky seems to reach for the horizon. This sign was\noften used in architectural motifs; the top of walls, and door frames.\nIt symbolizes the heavens.\nThe Egyptians believed that during creation this hill rose out of the\nsea of chaos to create dry land. The idea of this hill rising had a\nprofound effect on the egyptians, being used as every thing from temple\nlayouts to the possible inspiration behind the pyramids. A.K.A benben stone\nThe Double Crown, the red crown and the white crown put together to\nrepresent a unified Egypt. Although Egypt was not always a unified\nnation it was stronger that way.Therefore unification was desirable.\nNarmer (Menes), the founder of the First Dynasty around 3100 B.C., was\nthe first man recorded wearing this crown.\nThe sun was the primary element of life in ancient Egypt, we find this\nimportance reflected in the art and religion. Some of the most popular\ngods had a solar connection. The sun was first worshipped as Horus, then as Ra and\nlater as Amun-Ra There are many other\nrepresentations of the sun, including Khepri,\nrepresenting evening. During the reign\nof Akhenaten, the sun was worshipped as the Aten. A form of the sun disk with its rays\ndepicted as arms holding ankh signs. Another common form that the sun\ntakes is that of Horus\nBehudety, the winged sun disk flanked\nThis symbol means protection. Its origins are uncertain, but it is\nspeculated that it represents either a rolled up herdsman\'s shelter or\na papyrus life-preserver used by ancient egyptian boaters. Either way\nit is clearly a symbol of protection. From early times the sa plays an\nimportant part in jewelry design. It is often used in conjunction with\nsymbols, particularly the ankh, was and djed signs. We often find Taurt, the hippopotamus goddess of\nchildbirth, resting her paw on a standing sa sign.\nCalled the dung beetle because of its practice of rolling a ball of\ndung across the ground. The Egyptians observed this behavior and\nequated it with the ball of the sun being rolled across the sky. They\nconfused this balled food source with the egg sack that the female dung\nbeetle laid and buried in the sand. When the eggs hatched the dung\nbeetles would seem to appear from nowhere, making it a symbol of\nspontaneous creation. In this role it was\nassociated with the sunrise. Khepri was the\nscarab headed god.\nThis is a representation of the pavilion used in the Heb-Sed\nThis is a symbol of authority.\nThis is a rendering of the lungs attached to the windpipe. As a\nhieroglyph this symbol represents the unification of Upper and Lower\nEgypt. Other symbols are often added to further illustrate unification.\nThere are many fine examples of this\nsymbol throughout Egypt.\nIn these renderings we see the Sema\nbound with two plants, the\npapyrus and the lotus. The papyrus represents Lower Egypt and the lotus\nrepresents Upper Egypt.\nIn other representations we find two gods\nbinding the Sema together\nusing lotus and papyrus which represents the binding together of upper\nand lower Egypt.\nA Lotus Flower. This is a symbol of the sun, of creation and rebirth.\nBecause at night the flower closes and sinks underwater, at dawn it\nrises and opens again. According to one creation myth it was a giant\nlotus which first rose out of the watery chaos at the beginning of\ntime. From this giant lotus the sun itself rose on\nthe first day. A symbol of Upper Egypt\nA pool of water. The Egyptians portrayed bodies of water by means of\nequally spaced vertical wave lines. When these lines are inclosed by a\nrectangle it denotes a lake or pool. The Egyptians believed water was\nthe primeval matter from which aII creation began. Life in Egypt\'s\ndesert climate depended on water, and a pool of water would be a great\nluxury. There are many tomb paintings that show the deceased drinking\nfrom a pool in the afterlife.\nA loop of rope that has no beginning and no end, it symbolized\neternity. The sun disk is often depicted in the center of it. The shen\nalso seems to be a symbol of protection. It is often seen being\nclutched by deities in bird form, Horus the\nfalcon, Mut the vulture. Hovering over Pharaohs\nhead with their wings outstretched in a gesture of protection. The word\nshen comes from the word ""shenu"" which means ""encircle,"" and in its\nelongated form became the cartouche which\nMore commonly know as a cartouche. The shape represents a loop of rope\nin which a name is written. A protector of that name. See\nThe sistrum was a sacred percussion instrument used in the cult of Hathor. The sistrum consisted of a wooden or\nmetal frame fitted with loose strips of metal and disks which jingled\nwhen moved. This noise was thought to attract the attention of the\ngods. There are two types of sistrum, an iba, was shaped in a simple\nloop, like a closed horse-shoe with loose cross bars of metal above a\nHathor head and a long handle. The seseshet had the\nshape of a naos temple above a Hathor head, with ornamental loops on\nthe sides. The rattle was inside the box of the naos. They were usually\ncarried by women of high rank.\nThis bird is called the Lapwing, it is identified by its head crest,\nIts wings are pined back preventing it from flying. This symbol\nsignifies a group of people. It is often seen below the feet of a ruler\nto signify that the people are subjects of that ruler.\nKingdom, the symbol develops human arms which are held in the act of\ngiving praise. In this form it can be interpreted to mean\n""a group of\npeople giving praise.""\nThe exact origin of this symbol is unknown. In many respects it\nresembles an ankh except that its arms\nIts meaning is also reminiscent of the ankh,\nwe find the tiet being used as decoration when it appears with both the\nankh and the djed column, and\nlater with the was scepter. The tiet is\nwith Isis and is often called ""the knot of\nIsis"" or ""the blood of Isis."" It seems to be called ""the knot of Isis""\nbecause it resembles a knot used to secure the garments that the gods\nwore. The meaning of ""the blood of Isis"" is more obscured but it was\noften used as a funerary amulet made of a red stone or glass. In the\nLate Period the sign was associated with the goddesses Nephthys, Hathor,\nand Nut as well as with Isis. In all\ncases it seems to represent the ideas of resurrection\nThe sound eye of Horus. Symbolizes healing and\nThe cobra is an emblem of Lower Egypt. It is associated with the king,\nand kingdom of Lower Egypt. It is also associated with the sun and with\nmany deities. The cobra represented the ""fiery eye of Re"", in which two\nuraei can be seen on either side of a winged\nThe uraeus appears as a symbol worn on the\ncrown or headdress of royalty. It is used as a protective symbol, the\nEgyptians believed that the cobra would spit fire at\nany approaching enemies.\nLiterally translated it means ""to answer."" It is a small mummiform\nfigure placed in tombs to do work in the afterlife on behalf of the\ndeceased. In some tombs of the late New Kingdom whole gangs of ushabti\nworkers were included with different tools for doing different work. A\ncomplete collection would consist of 401 Ushabti: one for each day of\nthe year, 365 plus 36 foreman.\nThis is a symbol of power and dominion. The Was scepter is carried by\ndeities as a sign of their power. It is also seen being carried by\nkings and later by people of lesser stature in\nWinged Solar Disk\nThis is a form that the god Horus Behudety\n(Horus of Edfu) takes in his battles with Seth. The god Thoth\nused his magic to turn Horus into a sun-disk with splendid outstretched\nwings. The goddesses Nekhbet and Uazet in the form of uraeus\nsnakes joined him at his side.', 'When people think about where the pharaohs of Egypt were buried, the pyramids come instantly to mind. It is true that some rulers of ancient Egypt were buried in these large, elaborated tombs. But this didn’t start happening until around 2630 BCE. This is when building started on the Pyramid of Djoser. The thing is, there were quite a few pharaohs who lived, and died, before the idea of building pyramids was ever thought of. So where were the earlier pharaohs buried? And why was there a sudden switch to larger, more elaborate tombs?\nIf you were a ruler or a noble during the ancient period of Egyptian history, you were buried in something called a mastaba. This was a tomb that was shaped like a rectangle. It had sides that sloped outwards and was built from either stone or bricks that had been formed from mud. Because of the Egyptian’s belief in the afterlife, they needed a place where a person’s body could sit and not be disturbed. In Egyptian religion, you needed a body that had not rotted away or been disturbed by animals or other humans. If your body was destroyed, you were denied a place in the afterlife and eternal life was out of the question.\nAlthough it looked like a fairly uncomplicated structure from the surface, a mastaba was actually fairly complex. The bench-like structure that emerged from the sand was not where the body was actually located. Instead, the bodies of the deceased were placed in a sealed chamber that was located deep below the surface of the sand. The structure above had places where offerings could be brought by family members could be placed. The body inside was mummified. This was necessary because the way that the body was sheltered meant that it could not dry out naturally, the way it would if it was placed in the dry air of the desert.\nThe problem was that these tombs were large, obvious, and known to be the resting place of wealthy Egyptians. So, what’s a poor person to do? In many cases, grave robbers broke into the mastabas and carried off most of the contents that had any value. They often went so far as to destroy the carefully wrapped and preserved bodies as well. Because a defiled body meant no entrance into the afterlife, this was something that frightened and upset ancient Egyptians. They wanted a way to make sure that their graves would remain untouched.\nThis was one reason why many ancient Egyptians chose to be buried in tombs cut in rock walls instead of mastabas. In fact, by 1550 BCE, the use of mastabas was quite rare. However, they were not totally overlooked. In fact, the earliest pyramid was actually constructed to look like a stack of mastabas that had been placed one on top of another. The Step Pyramid of Djoser was designed to look like mastabas that had been stacked to form steps. It was believed that the steps would allow the soul of the Pharaoh Djoser to climb into heaven.']"	['<urn:uuid:aadf15f4-4176-4e04-bd29-fe5c2af4a1f2>', '<urn:uuid:af22d139-e341-4396-acde-28effac5df76>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-13T02:03:54.148118	6	81	3369
55	where does aidsvu get hiv data from	High quality HIV surveillance data for the city maps are provided by the local health departments in 33 cities. The county level data come from the Centers for Disease Control and Prevention. This is managed through a public-private-academic partnership that includes Emory University's Rollins School of Public Health and Gilead Sciences, Inc.	['Today, on National HIV Testing Day, AIDSVu released new maps visualizing the HIV epidemic in 33 U.S. cities. AIDSVu’s principal researcher, Dr. Patrick Sullivan, took some time to discuss how AIDSVu maps are changing the way people see HIV.\nQ: What makes this AIDSVu update unique from past data updates?\nA: By adding maps for 11 new cities, AIDSVu now maps HIV for 33 cities that make up nearly 60 percent of the U.S. epidemic. These maps are highly detailed and allow users to drill down to the ZIP code level, which is an unprecedented look at how the disease is affecting our communities and where it is having the greatest impact.\nFor example, with new maps in cities like Jackson and Richmond, we can now look at HIV prevalence in 19 Southern cities. The South is heavily impacted by HIV, accounting for almost half of all new HIV diagnoses in the U.S., and these new maps illustrate that profound impact. At the ZIP code level, we see that prevalence rates for some parts of Baton Rouge and Jacksonville are just as high as ZIP codes in New York and Washington, DC, and as high as national HIV prevalence rates in many Central and West African countries. Those are sobering statistics, but it’s vital to know and understand the geography of the disease.\nQ: Where do the data on AIDSVu come from?\nA: AIDSVu is a public-private-academic partnership in its fifth year of providing the most detailed, publicly available view of HIV prevalence in the United States. High quality HIV surveillance data for the city maps are provided by the local health departments in 33 cities. The county level data come from the Centers for Disease Control and Prevention, and will be updated this fall.\nThe AIDSVu project was created by Emory University’s Rollins School of Public Health and Gilead Sciences, Inc., and we are fortunate to be guided by an Advisory Committee and a Technical Advisory Group, with representatives from federal agencies, state health departments and non-governmental organizations working in HIV prevention, care and research.\nQ: Why did AIDSVu time the release to coincide with National HIV Testing Day?\nA: In the fight against the HIV epidemic, we have made strides when it comes to raising awareness and removing stigma. But the fact remains that there are still an estimated 1.1 million people in the United States who have HIV. One in six Americans still does not know that they are HIV-positive – and the only way to know your status is through testing.\nIf we can show people what HIV really looks like where they live, I think that adds a real sense of urgency to why it’s important to be tested. The CDC recommends that everyone between the ages of 13 and 64 be tested for HIV routinely, and AIDSVu maps are drawing attention to the “why.”\nQ: How do you hope to see AIDSVu being used?\nA: AIDSVu is a powerful tool for a number of audiences. It is key for policymakers at all levels because it allows for better resource allocation by demonstrating where the needs for prevention, testing and treatment are most critical. We’ve seen examples of AIDSVu being used around the country in some really smart ways.\n- In Alabama, a community-based organization used data from AIDSVu and other public data to determine where to place telemedicine centers to connect people living with HIV in rural areas to HIV specialists.\n- In Philadelphia, Brown University scientists used maps of the most impacted neighborhoods to direct HIV testing providers to those city blocks most impacted by HIV, where the testers offered HIV testing door to door as part of a geo-informed testing campaign.\n- In Denver, a community-based organization that provides support for women living with HIV used the AIDSVu maps to choose U.S. cities for educational seminars to reach the highest possible number of women.\nAIDSVu is also a great way for health care providers and the general public to better understand how HIV impacts their communities. AIDSVu shows that nearly every area in the U.S. is affected, which underscores the importance of HIV testing. If you live in a darkly shaded area, there’s no need to be anxious – remember that HIV is not transmitted through casual contact. However, by knowing their HIV status as part of their overall health and well-being, everyone can protect themselves – even in heavily impacted parts of the country. Similarly, people who live in lightly shaded areas need to know that HIV has no geographic boundaries, so living in an area with a lower prevalence rate does not mean you are protected from HIV. If users don’t know where to get tested for HIV, the AIDSVu site offers testing and care services locators for people interested in taking action.\nQ: What’s next for AIDSVu?\nA: We are working on a new neighborhood mapping pilot, which will be released in the coming weeks, that details HIV at a level that aligns with how people identify their place in a community – by community area in Chicago and by ward in Washington, DC. This fall, we will be updating state- and county-level-data and it will be followed by the release of state and county level new diagnosis data, which gives people a national snapshot of the HIV epidemic and new infections.\nAIDSVu will continue its mission of making HIV data widely available and locally relevant and hope to continue to be at the forefront of data visualization. You can sign up on to receive email notifications when new features or data are added to the site, and share your uses of AIDSVu with us at info@AIDSVu.org.']	['<urn:uuid:1f05b4a2-4ba6-40e5-9ff0-fe2deda1f5f5>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-13T02:03:54.148118	7	52	948
56	how many members knights of labor members participated in great southwestern strike 1886	Out of the total 700,000 Knights of Labor members in 1886, 200,000 workers participated in the Great Southwestern Strike against Jay Gould's railroad companies.	['The Noble and Holy Order of the Knights of Labor (1869-1949)\nBy: Michael Barga\nIntroduction: The Noble and Holy Order of the Knights of Labor were the most prominent labor organization of the 1880’s. Specifically, the organization grew between the end of the Depression and the beginning of the Great Upheaval (roughly 1879-1886) under the tenure of Grand Master Workman Terence V. Powderly. By the end of the 1880’s, their influence and membership dropped dramatically, and the last remnants of the Knights disbanded in 1949. Characterized by its oath-bound secrecy, its emphasis on autonomy of local Knights and non-violence, and its broad sense of solidarity, it is considered by many to be a failed experiment in the labor movement which did not capitalize on the action-mindedness of the Great Upheaval moment.\nBackground: The Knights of Labor were formed in 1869 by eight garment cutters in Philadelphia to replace the local union by Uriah Stephens. At the time, they were just a small part of the young modern labor movement which had materialized only within the last fifty years. The earliest unions were before industrialization and formulated out of the increasingly strained relationship between journeymen and masters in the skilled or artisan labor sector, a system reminiscent of the guild system. In the mid-1830’s, the General Trades’ Union allowed these wage earners to identify their shared grievances. As collective action picked up, employers felt the groups held too much power over individuals and maintained that economic demand, not employers themselves, truly decided wage levels.\nThe distinction between skilled and unskilled laborers was still made in the early 1870’s, yet changes created by industrialization placed the groups in greater contact, often in the factory. This opportunity to bridge the divide of the workers was part of the reason the Knights of Labor formed. The craft unions of previous times, composed almost solely of skilled workers, were ineffective. He saw the arrangement of labor and capital as a systemic problem that resembled the slavery of the past, and Stephens hoped for a brotherhood to provide education, mutual aid, and cooperation for challenging the labor-capital arrangement. Others, like the National Labor Union, had tried to organize a similar national and political movement starting in 1866, but the organization lost prominence after a number of disastrous political setbacks and the economic downturn of 1873.\nFormation and Early Years: The secrecy of Knights of Labor membership was considered a positive feature of the group by some and only something to be tolerated by others. The union also banned politicians, lawyers, and physicians since they were considered of low moral character or at high risk of breaking secrecy. The lack of distinction between skilled and unskilled workers departed from the early labor models in hopes to take advantage of the new industrialized arrangement. The Knights, originally a local Philadelphia union, had spread throughout the area in its initial few years, especially in New Jersey and the coal-mining regions of Pennsylvania. After the 1873 Depression subsided and unemployment declined, previous unions were re-created under the auspices of the Knights of Labor.\nIn 1878, it was deemed necessary to have a General Assembly which invited representatives from all the local assemblies. Stephens was at the helm at this first General Assembly, but he resigned within two years. Interestingly enough, the general principles of the Knights had not been explicitly declared despite its structural formation. The secrecy of the organization was the main reason for this slowness to communicate the mission of the union. The initiation practices and secrecy of even the name of the Knights of Labor were altered by 1879 to eliminate some of the religious overtones partly to accommodate Catholics. While this was a step towards reconciliation, the tension between the Catholic hierarchy and the Knights would significantly persist for almost another decade.\nBy 1879, Terence V. Powderly took over the position of Grand Master Workman with a membership of 9,300 workers who were diverse by trade including garment-cutters, miners, shoemakers, machinists, locomotive engineers, stationary engineers, glass-workers, moulders, printers, coopers, blacksmiths, boiler-makers, nail-packers, teachers, and carpenters.1 Powderly had helped found the Knights of Labor in the Scranton area in 1876.\nAfter the Knights lifted a ban on political discussion following a railroad strike in 1877, Powderly helped organize the “Greenback-Labor Party” in hopes of contending for local political offices. He quickly rose to Master Workman for the Scranton Knights and successfully navigated a period of severe divisiveness due to members’ differences in ethnicity and religion. In 1878, he was elected mayor of Scranton for the Greenback-Labor party.\nPowderly was not the only political success for the Knights of Labor. The organization, which now had spread to other regions of the United States, took a handful of other political offices in places like Maine and Massachusetts. Still, Powderly had made contact with many local assemblies and stood out to Knights of Labor leadership through his many various organizing activities. Such demonstrated commitment to the Order, even when his own local assembly membership had limited growth in 1879, complimented the recognition of Powderly’s other skills in writing and oration.\nUnder Grand Master Workman Powderly, the general assembly declared strikes an option of last resort and that the name and objects of the Order were made public in the early 1880’s. One of the more progressive moves of the Order at this time was declaring women to be admitted with equal standing as men. The ritualistic aspects of the Knights were also revised in hopes of increasing membership. The unexpected factor that appears to have boosted membership significantly was the strike victories in 1882 and 1885 that became associated with the Knights of Labor.\nThe Union Pacific Railroad had cut wages, yet through the aggressive leadership of Joseph R. Buchanan the original wages were restored. Buchanan reproduced the success in a number of other railroad strike incidents, all of which became associated nationally with the Knights of Labor despite their mostly local nature. The Knights of Labor had an explicitly anti-strike mentality, but the local autonomy of assemblies had allowed their name to become known as a powerful and assertive group, including financially, which could create sensational successes in assertive worker action. This hyped image was reinforced when local Knights called for help in an effort against notorious and unscrupulous railroad financier Jay Gould.\nThe national Knights of Labor leaders, including Powderly, recognized that the existence of the Order may be in danger if the 1885 strike was not supported. Despite the hope to keep the anti-strike mentality, Knights of Labor had been steadily laid off by Baron’s railroad companies in a seemingly deliberate effort to disband the local assemblies. The executive board called for Knights to strike and trains were stopped and the cars uncoupled, engines were “killed,” and widespread sabotage, in some cases leading to disorder and violence, spread throughout the Southwest.2 Gould realized the immediate threat to his entire transportation system and accepted a series of negotiations with the Knights of Labor’s national leadership. Such a demonstration of labor’s power had never occurred in the U.S., and the already inflated prestige of the order became all-the-more sensationalized.\nNewspapers across the country covered the story, and people of all trades were inspired by an immense confidence in the organization. Rumors ran wild that the membership was 2.5 million people and the treasury held 12 million dollars, and consequently, the number of new local assembly initiations was overwhelming to the national Knights of Labor. These new groups sang combative songs and hazardously took part in strikes, thinking the national organization could fuel a victory. Gould regrouped and easily defeated the largely unorganized strikes and assertive actions by local Knight assemblies. These failures were credited as defeats for the Order nationally even when no encouragement or approval had been given by Powderly or others in the executive board.\nBy 1886, there were an estimated 700,000 members in the Knights of Labor. While defeats had already begun, the Knights ultimate let-down to overenthusiastic supporters occurred in relation to the Haymarket Affair in Chicago. After a group of demonstrators were falsely convicted of setting of a bomb, many called for Powderly to challenge the authorities and assert the Knights of Labor power. Powderly had actually written to local assemblies to avoid the May 1st strikes and actions which had led to the Haymarket Affair disaster. Both those who disagreed with Powderly about the strikes beforehand and those who called for support after the actions became detractors of Powderly and the Knights of Labor leadership.\nWhile in the South there was still great numbers of the Order, many defected to more radical labor elements and groups which appealed to their skill more specifically, like the American Federation of Labor. Similar to the 1873 Depression, the Great Upheaval of 1886 brought in a period of declining employment stability for wage earners which had a negative effect on workers’ willingness to risk losing their jobs by joining or maintaining union membership. When Powderly lost re-election as Grand Master Workman in 1893, the Knights had fallen to a membership of roughly 75,000 and would never recover. By 1949, the last remnants of the Order would disband, and the Noble and Holy Order of the Knights of Labor would only have a presence in the history books.\nLooking back on the mix of local autonomy and progressive solidarity which were staples of the Knights of Labor, a particularly significant moment was the 1886 General Assembly in Richmond, VA. A black delegate of District Assembly Number 49, Franklin J. Ferrell, introduced Powderly to the convention. In Richmond, the local assemblies were separated by color, despite the fact that there was an official ban on color discrimination by the Knights of Labor. District Assembly no. 49 was needed support for Powderly, yet earlier in the convention local hotels and other institutions had given Ferrell trouble. This created a tension which Powderly could have dealt with in many ways and for many reasons.\nIt is debatable if the true motivations for having Farrell involved significantly in the assembly were for Powderly’s political expediency or on principle. Some believe Powderly and the Knights practiced a disguised discrimination model. Still, the incident was certainly a unique moment in the history of the Knights, a movement of tenuous solidarity for people of all backgrounds. Unfortunately, there was very little progressivism in terms of inclusion of Asian immigrants, and Powderly was in favor of closing the borders in this regard.\nAnother significant moment for the Knights of Labor was the 1888 reconciliation between the Knights of Labor and the Catholic Church. The ritualistic and masonic-like elements, in addition to the radical nature of the group, were met with great suspicion by the Roman Catholic Church. While measures were taken to lessen the measures offensive to Catholics, most clerics were opponents of the Order, especially in Canada where an official stand was taken against the organization. Through work with Cardinal Gibbons, Powderly got Vatican approval for membership by Catholics. While the Knights were on the decline at this point, it was an important step for friendly relations between the Catholic Church and the labor movement as a whole, setting the stage for the next generation of labor-priests and religious.\nConclusion: The Knights of Labor rose to prestige quickly in the 1880’s, and Powderly was considered the voice of labor, the head of an organization that could deal blows to even the likes of Jay Gould. While the reasons for the decline of the Order are debated, the economic conditions of the time, like for the National Labor Union, appear to be a factor. Also, the high level of local assembly autonomy appears to be a major contributing factor to both the union’s increased and eventual decreased membership.\nThe Knights are considered a failed experiment in the labor movement and yielded very few lasting contributions, yet defection to other unions, like the American Federation of Labor, may suggest that the energy of the labor movement was shifted rather than lost. In any case, the Noble and Holy Order of the Knights of Labor stand as a significant organization in a unique moment in the young history of the labor movement in the United States.\n1. “An Historical Sketch of the Knights of Labor” by Carroll D. Wright, The Quarterly Journal of Economics, Vol. 1, No. 2 (Jan. 1887): 149.\n2. Labor in America, Fourth Edition, by Foster Rhea Dulles & Melvyn Dubofsky, Heights, IL: Harlan Davidson, Inc., 1984: 133.\nGrand Master Workman: Terence Powderly and the Knights of Labor by Craig Phelan, Westport, CT: Greenwood Press, 2000.\n“Terence V. Powderly and Disguised Discrimination” by Herman D. Bloch, American Journal of Economics and Sociology, 33(2), April 1974: 145-160.\nAmerican Catholic History Classroom – http://cuomeka.wrlc.org/exhibits/show/knights/kol-intro/kol-intro3\nThe Making of American Exceptionalism: The Knights of Labor and Class Formation in the Nineteenth Century by Kim Voss, Ithaca, NY: Cornell University Press, 1993.\nKnights of Labor insignia – http://afge1504.org/page6.php\nRailroad Strikes – http://romitasx.tripod.com/id1.html\nFerrell Introduces Powderly – http://www.corbisimages.com/stock-photo/rights-managed/IH024307/engraving-of-knights-of-labor-assembly\nFor More Information: Visit the American Catholic History Classroom online at http://cuomeka.wrlc.org/exhibits/show/knights/kol-intro/kol-intro3 or see “An Historical Sketch of the Knights of Labor” by Carroll D. Wright, The Quarterly Journal of Economics, Vol. 1, No. 2 (Jan. 1887): 137-168.', 'On March 6, 1886, the Great Southwestern Strike began, marking the start of a year of worker revolt against the exploitation of Gilded Age capitalism. Over 200,000 railroad workers went on strike, but the failure to win helped usher in the decline of the Knights of Labor.\nThe widely and publicly loathed Jay Gould was one of the leading railroad capitalists and financiers in the United States and he had invested heavily in the massive expansion of railroads into the southwest after the Civil War. This rapid expansion gave workers some level of power and for awhile they achieved good wages. But Gould’s managers consistently sought to cut costs and gain power over workers they thought far too independent.\nBy 1885, Gould had succeeded in gaining control over many of his competitors and he started looking to cut labor costs. That year though, railroad workers had success striking against Gould-owned railroads. Management attempts to cut wages sent workers off the job and the Knights of Labor, a rising labor organization attempted to organize all workers, stepped in and helped negotiate a settlement that included rehiring strikers and paying back wages. Specifically, Terence Powderly personally sat down with Gould and convinced him to grant the workers’ demands. Defeating Gould was the biggest victory the Knights would ever achieve. Feeling hope that a national organization could fight for their demands, hundreds of thousands of American workers joined the Knights in the next few months.\nGould however was not going to take his defeat lying down. He was determined to crush the Knights. The managers’ war on independent labor continued and another strike quickly seemed likely. As these things often go, the strike itself started over an isolated incident. In Marshall, Texas, a Knights member attended a union meeting on work time. He was fired and his fellow workers walked off their job to demand he be rehired. The strike spread like wildfire among workers infuriated with Gould for the terrible wages, long hours, and dangerous working conditions along his rail line. Within days, the strike had spread across Texas and into Arkansas, Kansas, Missouri, and Illinois.\nThis became the largest of the nation’s 1400 strikes in 1886. Eventually, 200,000 workers (out of a total of 700,000 Knights members that year) went on strike. The strikers were hurt by the lack of solidarity from other unions, as the skilled labor union The Brotherhood of Engineers continued to work. On the other hand, one of the leaders Texas Farmers’ Alliance, William Lamb, declared a boycott of the railroads in support of the strikers, although this was controversial within the Alliance. The rapid growth of the Knights also was a problem. Powderly was generally opposed to strikes and the new members didn’t understand that the Knights actually had a pretty limited vision for labor reform that did not include very much direct action. While Powderly’s role in the strike is not entirely clear, he did not approve of the second strike, certainly did not grant it very strong support and stayed mostly hands off. In any case, when he did reach out to Gould to hash this new strike out, the plutocrat refused, seeking to crush the union entirely. Perhaps apocryphally, he said about the workers, “I can hire one half of the working class to kill the other half.”\nUltimately, the strikers had trouble maintaining the popular support necessary to overcome the overwhelming odds against organized labor in the Gilded Age. This certainly wasn’t because everyday people loved Jay Gould. He was largely considered a villain at the time as he is today. But people were so reliant on the railroad for goods and transportation that the shutdown of the system made their lives difficult. And if we know one thing about labor struggles today, it’s that for many people, their support of workers remains in the abstract and at the first moment they are personally inconvenienced, that theoretical support dries up. In popular opinion, strike leader Martin Irons began to be portrayed as Gould’s equal in oppressing the needs of the farmer, both monopolists of a sort, a sign of the limited ability for farmer-worker solidarity in these years.\nGould also worked with local politicians to crush the strike. The governor of Missouri called out the state militia, while the governor of Texas built on that by calling out the militia and the Texas Rangers. The governor of Kansas however refused, noting the lack of worker violence that Gould claimed was the reason for the troops. U.S. marshals also assisted in ensuring the trains ran. Gould also called in the Pinkertons. What did cause property destruction was the state repression, which led workers to retaliate by burning machine shops and letting train engines go cold, which delayed trains for hours until they warmed back up. In both Fort Worth, Texas and East St. Louis, Illinois, actual violence eventually took place against Gould’s hired thugs, leading to the death of at least nine workers. Shootouts began taking place between workers and trains running through the strike. But the violence caused by the Pinkertons and state forced workers back on the job. The strike was mostly over by May and accomplished little.\nBattle between workers and U.S. marshals at East St. Louis\nThis was the first major defeat for the Knights of Labor. Up to this point, it had seemed a growing force in American life. A few weeks after the strike’s defeat, the Knights convened a special assembly where it banned the organization from participating in strikes. This was a terrible idea as employers who had previously capitulated to the Knights immediately rolled back workers’ gains, knowing the central organization had taken away their best weapon to maintain those victories. Combined with the Haymarket incident a couple of months later (Powderly himself refused to do anything for the anarchists thrown in prison for it), the corporate-dominated media was able to paint the Knights as violent radicals and public sympathy turned against them. Moreover, workers felt betrayed by Powderly for his actions during the year. The organization would decline soon after, with a loss of 90 percent of members by 1890, and the American Federation of Labor would rise in its place as the primary union organization of American workers, or at least the ones the AFL would accept.\nMartin Irons would be banned from the Knights for leading this disastrous strike and was blacklisted from the railroad. He died in poverty in 1900.\nThis is the 135th post in this series. Previous posts are archived here.']	['<urn:uuid:49911a55-61f2-4e06-9439-d1b4be93529b>', '<urn:uuid:92aaf11c-b9cb-4be8-ba83-c66164db4ade>']	factoid	direct	long-search-query	similar-to-document	three-doc	novice	2025-05-13T02:03:54.148118	13	24	3297
57	minimum investment amount requirements reit vs angel group investment	For REITs, the minimum investment amount varies from REIT to REIT with no standard requirement. In contrast, for angel group investments, typical deal minimums range from $10,000 to $25,000 per deal.	"['You Asked, We Answered: Yield Alternatives for IRA Investors - Real Estate Investment Trusts (REITs)\nEstimated reading time: 3 minutes\nIn May\'s national webinar we discussed Real Estate Investment Trusts (REITs).\nParticipants who joined our free webinar had the opportunity to ask Bill Neville, Business Development Manager at The Entrust Group and Jon Bren, our guest and expert on REITs from KBS, questions regarding this popular investment strategy.\nContinue reading to see what questions were answered.\nQ: Can a REIT be formed in lieu of an LLC in order to invest in residential rental properties? Can my Roth IRA then be an investor in that REIT?\nA: Keep in mind a REIT is a managed investment and will have other investors investing in it also. There are requirements to establish a REIT, below are some of the requirements:\n- Must at least have 75% of its total holdings in real estate, cash or U.S. Treasuries\n- Must be a taxable corporate entity\n- Must have a managing board of trustees\n- Must have at least 100 share holders\n- May not have 50% of its shares be own by less than 5 individuals\n- Must at the least receive 75% of its gross income from rents from real property, interest on mortgages financing real property or from sales of real estate\n- Pay at least 90 percent of its taxable income in the form of dividends\nQ: What is the minimum dollar investment for a REIT?\nA: This depends on the REIT requirements and vary from REIT to REIT.\nQ: How a private placement REIT different from investing through crowdfunding websites?\nA: Crowdfunding portals are the result of the JOBS Act, seemingly to allow non-accredited investors to invest in private placements. However some of the crowdfunding portals only accept accredited investors. And, a crowdfunding portal could provide REITs as investments if they wanted to. A REIT is a type of investment, an asset. A crowdfunding portal is an avenue to invest in assets, including potentially a REIT.\nQ: With a Self-Directed IRA, can I invest in my own condo as an individual and still earn around 8-10% return? It gives me control and 100% liquidity opportunity whenever I want. In this context, what is the added value of REIT?\nA: Actually you can invest in a REIT since the REIT can invest in a condo you are interested in. This can eliminate the Prohibited Transaction rules since there is a managing board over the REIT. The only thing the REIT cannot do is offer you preferred treatment by having an investment in the REIT since that could be considered an indirect prohibited transaction.\nQ: Are there limitations on the types of REIT programs that can be invested in other than the regular prohibited person’s rules?\nA: It could vary from REIT to REIT. It is highly recommended to work with an investment advisor to vet out each REIT before investing.\nQ: Can a trust be a disqualified ""person""?\nA: Yes it can. Here’s a link to the IRS prohibited transaction FAQs which answers most of the prohibited transaction questions as well as lists the types of disqualified persons and entities. It is highly recommended to work with your legal counsel before proceeding with a transaction since engaging with prohibited transactions has tax consequences.\nThank you to everyone who participated in our national webinar. We encourage our readers to post any additional questions regarding Self-Directed IRAs or retirement planning in the comments section below.\nYou can view the full recording of this webinar here to get answers to questions which are not listed in this article.', 'WHAT IS AN ANGEL INVESTOR?\nAn angel investor is an individual who makes direct investments of personal funds into a venture, typically early-stage businesses. This is in contrast to venture capitalists who typically raise and invest third party funds. Because the capital is being invested at an early, risky stage in a business venture, angel investors must be capable of taking a loss of the entire investment; as a result most angel investors are relatively high-net-worth individuals.\nWHAT IS AN ANGEL GROUP?\nAn angel group is a formal or informal assembly of active angel investors who consider co-investing in businesses. Key characteristics of an angel group are: organization and control of the group by one or more member angels (who manage the group\'s processes and procedures), and collaboration by member angels in the investment process.\nWHEN AND WHERE DOES branch venture group MEET?\nMeetings are limited to angel members and invited guests only. Branch Venture Group meets once per month in Boston, MA.\nIS MEMBERSHIP IN branch venture group RESTRICTED?\nYes. Please see BECOME A MEMBER for a description of membership requirements above.\nWHAT IS branch venture group\'s POLICY ON GUESTS?\nGuests must be sponsored by an existing member and are welcome to attend one meeting given advanced notice to the Partners. Guests’ behavior must conform to Branch Venture Group’s Code of Conduct. Before investing in an opportunity presented at a group meeting, guests must be approved as members and agree to the terms of the Membership Agreement.\nMUST I MAKE A MINIMUM INVESTMENT EACH YEAR TO REMAIN A MEMBER?\nMembers are encouraged to be active investors, but since each investor’s threshold and interests differ, there is no strict investing requirement. In order to keep Branch Venture Group’s membership as active and vibrant as possible, the Partners reserve the right to refuse renewal for members who are neither investing nor making active contributions to the community.\nIS THERE A MINIMUM OR MAXIMUM AMOUNT OF MONEY I MAY INVEST?\nNo. Individual deal terms may impose per-angel minimums or per-angel maximums on a deal-by-deal basis. Typical deal minimums will range from $10,000 -$25,000.\nHOW ARE INVESTMENT DECISIONS MADE?\nBranch Venture Group members will pool their expertise, experience, and observations and work together on due diligence to review deals of interest, but in the end, each member will make his or her own personal investment decisions.\nDOES Branch venture group HAVE ITS OWN ""SIDE-CAR"" FUND?\nNo. Branch Venture Group encourages active rather than passive involvement by its angel members and does not provide a passive conduit for investing. However, we have the opportunity to syndicate deals and with other funds.\nHOW IS branch venture group FUNDED?\nThe administration of Branch Venture Group is funded by its members’ dues. Members of Branch Venture Group invest directly on their own behalf.\nDOES branch venture group MAKE MONEY OFF MY INVESTMENTS?\nNo. Branch Venture Group does not charge any management fee or receive carry on your investments, nor does Branch Venture Group receive (nor would it ever consider receiving) any finder or referral fees from the companies in which you invest. You keep all the returns you generate. Branch Venture Group\'s role is to provide its membership with the best opportunities in which to invest.\nHOW WILL PRESENTING COMPANIES BE SELECTED?\nA screening committee consisting of the Partners and interested members reviews plan submissions and meets to discuss and select the opportunities likely to be of greatest interest to the group. Any member is welcome to apply to join the screening committee.\nHOW SHOULD COMPANIES APPLY FOR FUNDING?\nGo to the Entrepreneurs section of this site, review our investment criteria, preferred stages and investment process and then visit our Gust profile here.\nARE THEIR GEOGRAPHICAL RESTRICTIONS ON WHERE branch venture group WILL INVEST?\nYes. Branch Venture Group focuses on companies located in Boston or within a few hour’s drive of the Boston area but we will consider opportunities from other regions in the United States.\nHOW IS DUE DILIGENCE CONDUCTED?\nMembers conduct due diligence themselves on a voluntary basis. A lead diligence coordinator is generally chosen, an agreed-upon diligence checklist is utilized (which is never less thorough than the minimum guidelines set by the Angel Capital Association), groupware and collaborative tools are utilized wherever possible to enhance efficiency for members and entrepreneurs, and diligence reports are shared openly within the member group (and with syndication partners from other groups, where appropriate). Diligence reports are not shared outside of the group or with the entrepreneur.\nHOW IS branch venture group RUN?\nThe Partners administer the group and welcome the input of Branch Venture Group members on how the group is administered. Branch Venture Group works in service of members and consults with members on matters relating to long-range issues and fundamental policy questions.\nHOW DO I LEARN MORE ABOUT ANGEL INVESTING?\nBranch Venture Group organizes programming for new members as an intensive introduction to angel investing. Branch Venture Group has speakers on relevant industry topics as part of the monthly meetings. Members also learn from other experienced angel investor group members, and the Partners can direct interested members to additional materials as needed.']"	['<urn:uuid:e8e034fb-ecf3-455a-95fe-8fdf3c272fc5>', '<urn:uuid:6ccfd985-b4a6-43ba-8ed7-970eec71b6a4>']	factoid	direct	long-search-query	similar-to-document	comparison	expert	2025-05-13T02:03:54.148118	9	31	1461
58	How did floor cleaning equipment evolve, and what maintenance helps prevent breakdowns?	Floor cleaning equipment evolved from manual mops and buckets in the 1950s to advanced autoscrubbers with features like solution tanks, scrubbing systems, moisture recovery, and drive systems. Modern machines are greener, safer, easier to service, and have fewer parts. For maintenance, active maintenance through cloud-based fleet management software helps prevent breakdowns by enabling real-time monitoring, remote troubleshooting, and scheduled maintenance. This proactive approach reduces downtime, keeps staff productive, and lowers maintenance costs compared to reactive maintenance.	"['The Evolution Of Autoscrubbers\nFrom early development to today\'s designs, a profile of these popular timesavers.\nWalking through Chicago’s Midway Airport at 1:00 in the morning, you may not see many people.\nWhat you will see are a lot of automatic scrubbers.\nAt that time in the morning, cleaning crews block off entire areas of the airport in order to clean the floors using these machines.\nSeeing all those “autoscrubbers,” as they are frequently called, you have to wonder how custodial workers cleaned large floor spaces such as those found in airports decades ago.\nAfter all, when were automatic scrubbers first invented?\nThe answer to the first question is easy.\nBefore the invention of these machines, floors in airports, train stations and other large facilities were dust mopped and then cleaned using mops and buckets.\nIt was slow, tedious work, and the results were often less than satisfactory.*\nAs to when automatic scrubbers were introduced, the first machines began to appear during the mid-1950s.\nThey looked like push lawn mowers containing a well for water and detergent, which the machine sprayed on the floor using jets.\nA set of rotary pads or brushes worked the combination into the floor, effectively removing soils.\nWhile the equipment certainly did not clean and dry the floors all in one pass, as autoscrubbers do today, these early machines were a very big advance in floorcare technology.\nThey used less water and less detergent and were overall more effective at cleaning floors than the old manual method.\nFloors also dried faster after cleaning.\nAnd, best of all, these machines improved worker productivity and morale tremendously.\nBy the 1970s, cleaning professionals had widely accepted these machines as a far more efficient and effective way to clean floors, and manufacturers had made significant advances in automatic scrubber technology.\nIn time, virtually all autoscrubbers shared the following three basic features:\n- A solution tank and water delivery system\n- A scrubbing system, which could be either rotary pads or, in recent years, cylindrical brushes\n- A moisture recovery system to vacuum up water/solution and soils into a recovery tank.\nMost of today’s machines now offer a fourth feature: A drive system that propels the autoscrubber forward, backward, left and right.\nWhen it comes to floor machines, greater mobility and maneuverability are typically high on the “want list” of floorcare technicians.\nHowever, greater mobility is not the only advance in automatic scrubber technology.\nEspecially during the past decade, manufacturers have developed machines that are:\nGreener: This applies to the types of chemicals used, as well as chemical and water usage.\nAccording to Paul Wildenberg of Charlotte Products/Enviro-Solutions, many floorcare chemicals are now green-certified, “and have proven [to be] both cost and performance effective.”\nToday’s machines also use less water and chemical, especially cylindrical brush systems.\nSafer: These machines are now safer for both the operator and others in the facility in which the machine is being used.\nSome of today’s automatic scrubbers have easy-access digital controls and greater visibility than older models, allowing users of varying heights to operate them, according to Doug Berjer, product manager for Tornado Industries, a manufacturer of automatic scrubbers.\nMore machines are also fitted with solenoid technology to control water flow.\nIn the past, water could continue to flow from these machines even when turned off, which was wasteful and increased the chances of a slip-and-fall accident.\nNow, water flows only when the machine is in use.\nEasier to service: A key advance in some floor machines is that they are now easier to service; operators can even handle many minor repair issues themselves.\nThis is important, according to David Frank, president of the American Institute for Cleaning Sciences, because it impacts the “total cost of ownership of the machine, which includes the initial purchase price, preventive maintenance, repairs, as well as availability of replacement parts.”\nManufactured with fewer parts: Berjer says that some machines have fewer mechanical parts now and that reducing the number of parts has also reduced the overall cost of ownership of some automatic scrubbers.\nFor instance, “The forward motion traction drive system on some machines is in a sealed gear unit that [not only] has fewer parts, but requires no maintenance whatsoever,” Berjer explains.\nAnother key improvement among automatic scrubbers in recent years (as well as among other types of floor machines, such as burnishers), is reduced size.\nBerjer adds that along with improved cleaning performance, the key reasons for this are enhanced worker productivity and safety.\n“According to ISSA studies, a 17-inch machine can clean nearly 13,000 square feet per hour, which is three times faster than a mop and bucket. Plus the floor is left clean and dry with a single pass, helping to prevent a slip or fall,” Berjer says.\nAnother trend that is already here for many manufacturers is battery-operated machines.\nThese systems improve the operator’s ability to clean larger spaces without stopping and eliminate the safety hazard posed by a cord dragging behind the machine.\nHowever, battery technology is changing, and more machines are now powered by maintenance-free batteries.\nWhile they may be more expensive initially, most cleaning technicians find that they require less maintenance overall and have a greater life cycle, so they are proving to be very cost effective.\nOther trends on the horizon in automatic scrubber technology include micro-machines that are even smaller than those available today; ""greener"" machines; safer machines; more effective machines that improve worker productivity by reducing the number of passes necessary to clean heavily soiled floors; and quieter machines.\n“However, improving worker productivity will continue to be a key goal of all scrubber manufacturers,” Berjer adds. “When you realize [that] many facilities are paying about the same now for floorcare as they did a decade ago, cleaning contractors are demanding machines that clean floors faster and more effectively than ever before.”\n*Science has proven that mopping floors, especially large floor areas, often moves soil from one area of the floor to another without actually removing it from the floor.\nRobert Kravitz is a former building service contractor and a frequent writer for the professional cleaning industry. He may be reached at RKravitz@RCN.com', 'ACTIVE MAINTENANCE FOR FLOOR CLEANING EQUIPMENT\nMachine down time is not only costly but has a negative impact on the overall well-being of indoor environments.\nFloor scrubbers and sweepers help cleaning staff with consistent and effective cleans and when not working properly, or at all, the health and safety of indoor environments suffers.\nIn a typical example of floor cleaning equipment down time, a machine can sit for at least a week waiting for repairs and parts. This is due to waiting for a service technician to be dispatched, trouble shoot, and order parts. Then there is the waiting game for parts to come in and the maintenance technician to return to fix the problem. This wait time is costly to BSC’s and building managers due to the job not getting done on schedule, and cleaning staff not being able to meet set expectations for building cleanliness.\nOn top of that is the cost of parts and service. Most floor cleaning equipment is bought outright, and such the cost of services and parts is up to the company that owns the equipment. Industry standards suggest that for the three-year useful life of a floor cleaning machine, service, parts, and trip charges can cost around $2K. That does not include ‘wear’ parts or battery replacement and repairs, nor does it account for money lost from downtime. For BSC’s and building managers that oversee entire fleets, this can be a very costly expense.\nConsequently, according to research from CleanLink the result of machine down time goes beyond just monetary costs as “Cleaning personnel are now spending extra time doing the same job. Hard floors or carpets are not getting as clean, which can lead to a less than desirable perception by building occupants.”\nDue to the importance of perception of clean and building safety, supporting building cleanliness has never been more important and active maintenance is key to supporting overall health and safety in buildings.\nPROACTIVE MAINTENANCE vs ACTIVE MAINTENANCE\nProactive Maintenance is the process of following manufactures guidelines for machine cleaning, parts inspections, and replacement, and making sure that these benchmarks are met. This is instead of reactive maintenance, meaning to not fix a problem until there is one. Proactive maintenance can prolong the life of a machine, support staff in efficient and productive cleans, and save companies money over the total life of the equipment.\nThe problem with having no plan, or reactive maintenance, is that “The unpredictability of reactive maintenance and breakdowns does not allow for improvements in key performance indicators, business drivers or the needs of a maintenance department,” according to Accelix. Reactive maintenance can ultimately lead to a multitude of losses and can contribute to poor overall building health.\nAs Accelix further notes, “To compete in the global marketplace, it is important for organizations to recognize the impact of maintenance on the business and its customers. With a program that focuses on establishing, tracking, and analyzing data on critical equipment, greater gains become possible.”\nTypical proactive maintenance routines include:\n- Setting up a clear “maintenance and service” schedule for cleaning staff to follow\n- Get the buy-in of cleaning staff\n- Track data including who uses machine, how often and for how long\n- Tracking when routine service is needed\n- Keeping up on routine cleaning and care of equipment\nActive Maintenance on the other hand takes proactive maintenance a step further. For the floor cleaning industry, this means access to SaaS models and subscription services that provide remote access to BSC’s and building managers, and in some cases technician teams who monitor equipment regularly. Cloud-based fleet management software make data tracking simple and in addition to tracking entire fleets, the platforms can send notifications based on machine activity or inactivity, look for maintenance or error notifications, and allow for troubleshooting to happen remotely.\nBENEFITS OF ACTIVE MAINTENANCE\nActive maintenance relies on fleet management software to help improve the overall performance of equipment resulting in a positive impact on overall building cleanliness and safety, as well as, supporting the productivity and efficiency of cleaning staff. By using equipment powered through SaaS and subscription programs, BSC’s and building managers have more knowledge and support in the maintenance of equipment. Benefits include:\nLess Machine Downtime\nCloud-based fleet management software gives end users real-time access to entire fleets through an app or computer. In the case of i-Synergy software by ICE Robotics, end users can track an entire fleet and monitor any service issues or machine activity. This allows BSC’s and building managers to maintain equipment and track when service or repairs are needed more accurately. The data is tracked, monitored and stored and can be viewed in easy-to-read reports that can be emailed to end users, making the maintenance of equipment and repair schedules even easier to keep track of and use.\nKeeps Staff Productive\nBecause end users are notified through reports and alerts, they can plan for machine maintenance and repairs. This means less machine downtime and less opportunity for cleaning staff to spend time trying to fix equipment themselves, a job they are typically not trained to do.\nDepending on the type of fleet management system, error codes can even be sent that help trouble shoot and solve the issue remotely without having to dispatch a technician to figure out the problem. The result is cleaning staff that can remain focused on an efficient and productive cleaning schedule and less time dealing with equipment issues.\nReduced Maintenance Costs\nWith active maintenance software, alerts are sent to end users giving them an immediate notification that a machine needs service. Notifications provide location of equipment and even error codes that help expedite repairs and maintenance. This type of data tracking also empowers BSC’s and building managers to create reports that track and notify in advance when routine maintenance and replacement of parts is necessary.\nBy collecting data, end users have valuable information at their fingertips that helps them figure out ahead of time when maintenance and repairs need to happen. Being able to plan means equipment is well maintained, machine life is extended and there is less down time, resulting in overall cost savings.\nBenefits of A Subscription Service to Active Maintenance\nIn the case of ICE Robotics’ subscription model there are even more benefits because as the distributor ICE Robotics has a stake in the equipment. This means BSC’s and building managers have a partner in data tracking, analysis, and machine repair and maintenance. Through fleet management software, ICE Robotics technicians monitor the equipment along with end users, BSC’s and building managers, providing added support in equipment maintenance.\nOn top of that, if alerts are sent due to machine malfunction, technicians are notified and can trouble shoot remotely and, in some cases, make a repair without having to go on site. Plus, fleet management and technical support are included in the monthly payment.\nAdded benefits lie in the ability of the distributor to provide data tracking and teams to manage the data and monitor performance. The information collected can be shared with manufacturers who then can build better equipment and send through software updates to improve performance. The relationship between distributor and client is incredibly important to the overall functioning of equipment and this results in cost savings and better performance.\nICE Robotics is an industry leading floor cleaning equipment manufacturer working to bring you people-first, collaborate cleaning solutions. For more information on our services please reach out to our client care team.']"	['<urn:uuid:8e88c77a-fcef-48c4-8312-3cf663a184a8>', '<urn:uuid:d18420ae-02e3-419f-8be3-ee0694ed9ce3>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-13T02:03:54.148118	12	76	2270
59	combined sewerage system versus private sewer laterals maintenance	Combined sewerage systems and private sewer laterals have different maintenance responsibilities and challenges. Combined systems carry both domestic sewage and stormwater in the same sewer, requiring larger infrastructure and involving higher maintenance costs due to the large quantity of sewage. These systems are maintained by public agencies. In contrast, private sewer laterals, which connect individual buildings to public sewer mains, are the responsibility of property owners. Property owners must maintain these laterals in safe and sanitary condition, though many only address issues during emergencies, leading to problems like root intrusion, earth settlement damage, and blockages that can cause system overflows.	['What is the Sewerage System?\nSewerage is the infrastructure that conveys sewerage, surface runoff using Sewer. It encompasses components such as receiving drains, manholes, pumping stations, stored overflows and screening chambers of the combined sewer or sanitary sewer.\nSewerage enters the entry sewerage treatment plant or at the point of discharge into the environment. It is the system of pipes, Chambers, manholes etc. that convey the sewerage and stormwater.\nTypes of Sewerage System\n- Combined System\n- Separate System\n- Partially Separate System\nCombined Sewerage System\nIn a combined system, the same sewer is intended to carry both the domestic sewage, industrial wastes as well as the surface and the stormwater flow.\nSituation for Adoption\n- Rainfall is even throughout the year.\n- Both the sanitary sewage and the stormwater have to be pumped.\n- The area to be severed is heavily built-up and space for laying pipes is not enough.\n- Effective or quicker flows have to be provided.\n- If the sewers are laid along with the overall development of the area, a combined system is preferred.\n- Rainwater keeps sewage fresh making it easier and more economical for treatment purpose.\n- Dilution also helps, this being in itself a method of treatment.\n- Automatic flushing is provided by water.\n- This is the simplest method of collection and house plumbing economies.\n- The bigger size of the sewer would involve larger excavation.\n- Overflowing under worst conditions may endanger public health.\n- Cost of pumping and treatment would increase due to the large quantity of sewage to be handled.\n- The dry weather flow is a small amount of the total flow, the large size of the sewer would often result in causing silting up due to low velocity of flow during the dry period of the year.\nIn a separate system, the domestic sewage and industrial wastes are carried in one set of sewers whereas the storm and surface water are carried in another set of the sewer.\nSituation for Adoption\n- Where rainfall is uneven.\n- Sanitary sewage is to have one outlet and other outlets for store or surface water are available.\n- Sanitary sewage is to be pumped.\n- Separate sewer must be placed deeper and the stormwater drains nearer the surface to economies excavation.\n- If the ground has steep slopes, it is easier to convert stormwater through an open drain to the natural stream.\n- Finance available are small but sanitary drainage is imperative.\n- If the subsoil is hard, it is difficult and costlier to lay combined sewer of large size.\n- If the sewers are laid before the area is developed, it is preferable to adopt a separate system.\n- Being smaller in size, the sewers are economical. The surface water may be taken in open or closed conduit or drains at or near the surface and discharged at suitable outlets, thus greatly simplifying the design of sewers of stormwater drains.\n- There is no risk of stream pollution as no storm overflows are to be provided.\n- The quantity of sewage to be treated is small, the disposal or the treatment works can be economically designed.\n- If the pumping of sewage at the treatment works is necessary, pumping cost would be much less as there is no need to pump the stormwater.\n- Unless laid at a steep gradient, self-cleaning velocity in the sewer cannot be assured and flushing shall have to be done. This may prove unsatisfactory and expensive.\n- Risk of encroachment by unauthorized rainwater collection and consequent overflows of sewage may be there.\n- Double house plumbing is another disadvantage. Two sewers or drains in a street leading to greater obstruction of traffic which repairs to any one of them are being carried out.\n- Maintenance costs of two systems are greater than that for one.\nPartially Separate System\nA partially separate system is a modification of the separate system in which the separate sewer discharging domestic sewage and industrial wastes also contain a portion of the surface drained from back-paved yards and roofs of the house.\nWhy we need partially separate system?\nIt may raise that a combined system of sewerage has not been found quite suitable under tropical Indian conditions for reasons outlined below.\n- Concentrated and heavy rainfall during the monsoon period, which at most places last for only 3 to 4 months in a year. Thus, there is a considerable variation in the quantity of sewerage flow during the twelve months of the year.\n- Inadequate amounts of waste of waters reaching the sewers because of a vast tract of intervening unsewered areas or due to the other reasons so that the dry weather flow (DWF) is generally a tiny proportion of the total flow.\n- The low economy and limited funds available.\n- Difficulties in the operation and maintenance of the system due to inadequate supervision of less qualified staff. Local bodies in charge of the work usually do not pay much attention to keeping trained and skilled staff.\nBecause of the reasons described above, it has been found necessary in practice to have a compromise between the separate and combined system and to adopt as for as possible the partially combined or partially separate system of sewerage.\nThe underground sewer system carries, only the sanitary or domestic sewerage into which may also be drained rainwater from the backyard and roofs of houses.\nIn contrast, the stormwater from the house front and the surface wash of streets and roads may separately discharge into a watercourse running near or to the same low lying area at some farther distance.\n- It simplifies the drainage of the houses.\n- It provides reasonable sizes of sewers and is economical.\n- The rainwater provides some safeguard against silting in the sewer.\n- Low velocity during the dry period.\n- Storm overflows may be found necessary.\nIf you found this article helpful, please be sure to share it with your friends.', 'What Are They and Who Is Responsible for Maintenance?\nPrivate sewer service laterals, which typically are not under the jurisdiction of public wastewater agencies, are not always sufficiently maintained and can contribute to inflow and infiltration (I/I) problems, cause blockages and play a role in sanitary sewer overflows (SSOs).\nWhat is a sewer lateral?\nThe private sewer service lateral (also known as a private lateral) is a pipe that takes an individual building’s wastewater to the public sewer main in the street. These private laterals are owned by the building owner and are essential for directing wastewater away from homes, churches, schools and businesses to a publicly owned wastewater collection and treatment system.\nAs illustrated in the diagram, there are both upper and lower private laterals. The upper lateral runs from the home or business to a cleanout at the sidewalk or sometimes the property line. The lower lateral runs from the cleanout to the sewer main in the street. If no cleanout is in place, the private lateral is usually not considered split between upper and lower portions.\nWho is responsible for private laterals?\nMIDWAY CITY SANITARY DISTRICT GENERAL REGULATION ORDINANCE NO.13 SECTION 19\nAll house connections, street laterals and appurtenances thereto shall be maintained in a safe and sanitary condition, and all devices or safeguards which are required by the ordinance shall be maintained in good working order. This is the financial reasonability of the property owner.\nExcept as hereinafter provided, all such house connections, street laterals, appurtenances and devices shall be maintained by the owner of the property served thereby.\nIn California, public agencies are required by the State Water Resources Control Board to manage and maintain the public portion of wastewater systems to minimize the likelihood of sanitary sewer overflows. Regular maintenance of agency-owned sewer lines typically includes scheduled cleaning, inspection and rehabilitation or replacement of defective lines to make them structurally sound and watertight. However, no similar statewide program exists for property owners to regularly clean, inspect and otherwise maintain the private laterals. Most often, property owners only attend to laterals in cases of complete stoppages or emergency failure.\nWhy are unmaintained sewer laterals cause for concern?\nMany older homes still rely on original sewer laterals that over time have become cracked, disjointed or damaged by roots, earth settlement or grease blockages.\nUnmaintained private laterals that become blocked or fail can contribute to SSOs of the public sewer system or to sewage backups into the building served by the private lateral. Some of the causes for concern for unmaintained private laterals are described below:\nRoot intrusion into the private lateral can contribute to SSOs in two ways. One way is that extra\nwater can enter the sewer system through cracks in private laterals created by roots. This extra\ngroundwater and storm water seeps in or “infiltrates” the public sewer mains, with the potential to\ncause wastewater collection and treatment system overloads such as SSOs. The other way is\nwhen a plumber cleans out a lateral blocked by roots, the plumber sometimes will push the root\nmass into the public sewer main. This mass can then cause a blockage in the public sewer main,\ncause wastewater to back up and result in an SSO.\nEarth settlement that results in cracks and leaks in the private lateral allows for the “inflow” of groundwater into the private lateral. This additional groundwater flows into the public sewer main, increases the wastewater flow in the collection system and has the potential to overload the system, creating SSOs.\nIf roof drains, irrigation drains and driveway drains are connected (illegally) to the private lateral, this increases the “infiltration” of rain water into the public sewer system, which can lead to system overloads.\nGrease build-up in the private lateral (like plaque in a person’s artery) can cause blockages that potentially result in on-site SSOs or backups of wastewater into the building that is served by the blocked private lateral.\nThe combination of inflow and infiltration from private laterals, across a wide area (neighborhoods or commercial districts), can significantly increase the wastewater flow in the public sewer main and to the treatment plant. Infiltration and inflow from private laterals are currently a major source of SSOs in some communities.\nWhat is the magnitude of the sewer lateral problem?\nThe magnitude varies considerably among different systems. It is estimated that private laterals can constitute as much as 50 percent of the total length of all the pipelines in some wastewater systems. Therefore, it is likely that poorly maintained laterals can contribute significantly to I/I problems or produce blockages and resulting SSOs. This is especially true during wet weather if roof or driveway drains are illegally adding flow to the private laterals.\nWhat are strategies for managing sewer laterals?\nProperty owners should be encouraged to inspect their private lateral upon property transfer and then at least once every five to 10 years to determine if it is defective or if a blockage is building up in the line. If problems exist, a contractor should be hired to correct the problem.']	['<urn:uuid:fb120b92-a9e9-495b-aaf2-ee7ffcedb016>', '<urn:uuid:e74fc693-e0af-4979-83cc-e0651d9c259a>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-13T02:03:54.148118	8	100	1841
60	compare size whirlpool galaxy ngc 4214	NGC 4214 is a small irregular galaxy with a diameter of about 9,300 light years, while the Whirlpool galaxy is much larger with a diameter of 60,000 light-years. However, the Whirlpool galaxy is still only about 35% of the Milky Way's size.	"['NGC 4214, MJ Block\nThe galaxy pictured above is NGC 4214. It is an irregular galaxy. Unlike ellipticals which have an oval or circular shape, irregular galaxies have a box-like shape. Irregular galaxies also have no spiral arms (unlike spiral galaxies). NGC 4214 is approximately 13 million light years away from us and has a diameter of about 9300 light years.\nIn the image above, we see lots of interstellar gas and dust surrounding a few knots of star formation. The blue in this image indicates newly-formed, high-mass stars. There are many low-mass stars in NGC 4214 as well, but it has a lot of glowing gas around these new star clusters.\nThis is a picture of NGC 4214 in the ultraviolet wavelength. Ultraviolet shows star formation well because new, young stars emit a lot of blue light. This is depicted by the black in the center of the image. There is a lot of star formation occurring in this galaxy in both the core and in a few knots on the outer edges of the galaxy.\nThis is another picture of NGC 4214, but this picture is a composite of pictures in six different wavelengths. The wavelengths are Infrared, R, H alpha, V, O III, and U. The infrared and the red filters indicate the high-mass stars (the dark red and orange-red in the photograph). The H alpha emission lines indicate ionized Hydrogen gas, which in this photograph is the yellow. New stars form, ionizing the Hydrogen gas surrounding the clusters. The O III emission lines indicate oxygen gas in the galaxy, which is blue in this photograph. We see this in the very center of the galaxy where it is too hot for the Hydrogen to recombine. But these even hotter stars allow Oxygen to combine. In the center of the galaxy, hundreds of massive blue stars are being formed, each with a brightness more than 10,000 times brighter than the sun. In the U filter, we can see the high-mass stars (which in this photograph are the very blue specks).\nThis is the light profile for NGC 4214. This shows us how bright the galaxy is as we move out from the center into the outer edges of the galaxy. An important thing to note about NGC 4214\'s light profile is that it has no bulge as normal galaxies do. If it had a bulge, the data points would increase greatly near the y-axis. As it is, the galaxy follows the same function from the center of the galaxy to the edge. The pink line in the center of the navy blue data points is the line of best fit for the data. This line can be described by the equation y = (-4.28+/.06)x + (9.20 +/-.05). The scale length found for NGC 4214 is 0.234 +/-.003 kpc along its major axis and 0.148 +/- .002 kpc along the minor axis. Since NGC 4214 is an irregular galaxy, the inclination cannot be computed from the major and minor axes. Since spiral galaxies studied by others in Astr212 have a scale length ranging from 0.6-1.8 kpc, it is obvious that NGC 4214 is a small irregular galaxy.\nThe picture above is of a nebula in the Large Magellanic Cloud called the Tarantula Nebula. The Tarantula Nebula is a nearby irregular galaxy that is similar to NGC 4214 and behaves similarly to the center of star formation as well. By comparing the linear diameter of the star formation region in the Tarantula Nebula to the center of star formation in NGC 4214, we can begin to grasp the size of NGC 4214. The linear diameter of the star formation of NGC 4214 is 380 Light years while the linear diameter of the star formation in the Tarantula Nebula is 720 Light years. This tells us that the Tarantula Nebula is twice as large as the central nebula in NGC 4214.\nNGC 4214 is classified as a IAM(s)m galaxy. The ""I"" stands for an irregular galaxy. The ""AB"" means that NGC 4214 may have an ovally distorted bar. Essentially, this means that the galaxy does not have a clear, distinct bar, but it does not have no bar either. The ""(s)"" means that the spiral arms start from the center of the galaxy. The ""m"" in the classification means that the galaxy has very little structure (or disk), but it is more coherent than an ""Ir"" galaxy.\nAll images were taken with the Calvin-Rehoboth Robotic telescope. Data were reduced in Maxim®. Raw images were calibrated with dark, bias, and flat images. Once the images were calibrated, median combine was used to combine all of the images taken in a filter into a single image. In order to make a single color image, the color combine process was used. For these data the color numbers were red 0.5, green 1, and blue 8. The Gamma stretch process was used to eliminate background light in the images as well. The gamma value used in these data was 0.35.\nThis research has made use of the NASA/IPAC Extragalactic Database (NED) which is operated by the Jet Propulsion Laboratory, California Institute of Technology, under contract with the National Aeronautics and Space Administration.\nFrommert, Hartmut and Kronberg, Christine. ""NGC 2070."" Students for the Exploration and Development of Space. <http://www.seds.org/MESSIER/xtra/ngc/n2070.html>\nGiving to Calvin\nMajors & Minors\nPeople at Calvin', 'The Whirlpool galaxy also goes by the name M51, M51a, and NGC 5194. It has the distinguishing note of being the first galaxy to ever get an official label designation for its well-known spiral shape.\nThe galaxy is familiar to many and the Whirlpool is thought to be a classic design for the description of a spiral galaxy.\nResearch of the Whirlpool galaxy has been ongoing as astronomers have used it to study and learn about the interaction that it has with a dwarf galaxy that is close to it known as M51b.\nThe Whirlpool galaxy is part of the “Hunting Dogs” constellation known as Canes Venatici, and is one of the galaxies in the sky that is so bright that it can be viewed with low tech binoculars and telescopes, and even the naked eye.\nWhen astronomers do use more sophisticated telescopes they can see the side of the Whirlpool spiral as it faces our Earth.\nWhen astronomers choose to display images of a galaxy they often select the Whirlpool galaxy alongside of the Milky Way. It has winding arms that span out in graceful displays, almost looking like it is dancing through the universe.\nThe “arms” are gas, stars, and dust in long delicate tendrils. These are important because they are the nurseries for stars, compressing the materials that are required for the clusters of newly born stars in its galaxy.\nScientists theorize that the Whirlpool galaxy had an interaction with the small, yellow-colored NGC 5195 galaxy that exists in one of the tips of the outermost arms, and the interaction has made the arms of M51 prominent.\nThe idea is based on the appearance of M51, which almost seems like the smaller galaxy is pulling on the arm, triggering the new formation of stars with the tidal forces involved in the encounter.\nThe Hubble Space Telescope has taken images that display NGC 5195 crossing the path of the Whirlpool galaxy, something that has been happening for hundreds of millions of years.\nGalaxy Type: Spiral\nDesignation: M51a or NGC 5194\nDiameter: 60,000 light-years\nMass: 160 billion M☉\nConstellation: Canes Venatici\nDistance to Galactic Center: 23 million light-years\nThe Whirlpool galaxy is another galaxy that has been easily viewed from Earth for millennia. It has a magnitude of 8.4 which makes it bright enough to be seen from all over the globe.\nHowever, in the past, many astronomers thought that the Milky Way galaxy was the limit of the universe and anything seen in the celestial skies was embedded in the Milky Way.\nAs time passed, astronomers believed that the other images that they saw in the night sky were “nebulae”. Charles Messier is credited for locating M51 in 1773, plotting it at 31 million light-years from our Earth in the Canes Venatici constellation.\nAlthough the discovery of the Whirlpool galaxy was in 1773, it took until 1845 for the galaxy to receive an official designation and be acknowledged as a spiral galaxy.\nThe 3rd Earl of Rosse, William Parsons, was incredibly wealthy and purchased the biggest optical telescope in the world at that time. Using the telescope, he viewed the Whirlpool galaxy to define the spiral shape.\nThe Whirpool galaxy remains as one of the most popular among scientists and astronomers and holds the credit for being the first spiral galaxy to be officially identified as such.\nThose that study the Whirlpool galaxy are particularly fascinated with its interactions with the dwarf galaxy neighbor, M51b.\nScientists have had a long-held debate over the distance of the Whirlpool galaxy from the Earth. Most astronomers agree that it is around 23 million light-years away, however, others had said that it could be in a range from 15-35 million light-years away.\nThose that settled on the 23 million light-year number did so from the calculations based on data that was collected from an exploding supernovae in the Whirlpool galaxy.\nThe event created small fluctuations in the instruments that are scientifically designed to detect cosmic objects, allowing sophisticated techniques of measurement.\nAstrophysicists are particularly fascinated with the large percentage of supernova star explosions that occur in the Whirlpool galaxy.\nThe Hubble Space Telescope has taken images of M51 showing the infrared light red tones and the abundance of hydrogen in the regions that have a high percentage of star formations.\nThe telescope has also captured pictures of the blue color of hot, young stars as well as the older stars that appear in a yellow color. They also continue to focus on the interactivity between the Whirlpool galaxy and M51b, the dwarf neighbor galaxy.\nThe fact that the Whirlpool galaxy is so close has offered scientists an incredible opportunity to watch its structure as it creates new stars.\nAstronomers have observed that, over time, the largest stars have an effect on the surrounding gas that causes it to radiate away.\nThis leaves the clusters of blue stars so that they remain within the arms of the Whirlpool galaxy, letting scientists accomplish long-term study.\nThe Whirlpool galaxy and its smaller neighbor, the dwarf galaxy of M51b have been merging for millions of years and the situation has caused the spiral arms of the Whirlpool galaxy to become misshapen.\nAstronomers don’t know what the original shape of the dwarf galaxy was.\nThe dwarf galaxy of M51b is giving the Whirlpool galaxy an incredible amount of raw material that has created a lot of production of new stars in the galaxy’s center.\nSimilar to the Milky Way galaxy, the Whirlpool galaxy has a supermassive black hole in its center. The black hole is surrounded by dust rings that are in a constant state of new star creation.\nThe activity is at such a high degree that it increases the brightness so that scientists have classified the Whirlpool galaxy as a “Seyfert galaxy.”\nThe companion dwarf galaxy M51b is eventually expected to become fully absorbed by the Whirlpool galaxy. The two are currently connected with a bridge of gas and dust.\nThe effect of the dwarf galaxy M51b on the Whirlpool galaxy has been noted by scientists. As M51b passes by the Whirlpool galaxy the gravitational pull of M51b generates waves within the pancake-shaped disk of the Whirlpool galaxy.\nThese waves can be compared to those that happen when you throw a rock into a pond and it causes ripples.\nThe waves pass through the gas clouds that are orbiting the disk and squeeze the gas to the inner edge of each of the Whirlpool galaxy’s arms.\nAs all of this occurs the dusty dark material appears like an Earth storm, with the eventual collapse of the dense clouds that herald the beginning of a star being born. This interaction has been coined as a “close encounter.”\nExploration and Study:\nIn 1845, William Parsons used a 72-inch reflector telescope, he created a drawing of the Whirlpool galaxy that depicted his observations.\nIt is considered to be a mid-19th century astronomy classic work and was used as a baseline for future studies. Parsons continued his work on what he thought was a “spiral nebula,” in within 5 years had discovered 14 more of the same type of objects.\nThis was during the time when astronomers weren’t sure if the nebula were part of the Milky Way galaxy or objects that were completely independent.\nIn the 1920’s, Edwin Hubble charted the M31 cosmic distance using the Cepheid variable stars. It was during that time that scientists began to acknowledge that there were distance galaxies apart from the Milky Way.\nIn 1994, 2005, and 2011, the Whirlpool galaxy has had a huge number of supernovas. The organization Skywatchers has been recording the events.\nScientists are unsure exactly why there has been such an increase in supernova activity. The 2011 supernova was examined after its brightest moment to see if they could figure out the source of the explosive behavior.\nUsing the Hubble Space Telescope, astronomers searched a supergiant yellow star that was present prior to the supernova explosion and then gone afterwards.\nThis was uncommon behavior as most supergiant stars don’t go nova when their lifecycle is done, however, the astronomer team indicated that there is a distinct possibility that this was a binary star.\nIn the case of a binary condition, the companion star would have been hot enough to pull some of the mass away from the supergiant.\nOver time, this would cause a destabilization of the star which could result in an explosion.\nThe Hubble Space Telescope doesn’t view the stars in ultraviolet light, which was the range required to see the blue companion star.\nThis situation, combined with the other supernova explosions has prompted scientists to recognize the requirement for additional study of the evolution of binary star explosions.\nThe Chanra X-Ray Observatory took an image in 2015 of the Whirlpool galaxy that involved over 250 hours of observation time.\nThe space telescope used five times the number of sources that had been used in prior studies which totaled to 500 X-ray sources. Astronomers believe that a minimum of ten of the sources were derived from black holes.\nFacts about Whirlpool Galaxy for Kids:\n- It’s estimated that the Whirlpool galaxy has over 100 billion stars.\n- Charles Messier was looking for comets in the celestial sky and, as he was trying to continue his charting process he discovered the Whirlpool galaxy in 1773.\n- William Parsons was incredibly wealthy and in 1845 he purchased one of the biggest and best telescopes in the world. He set up the telescope at Birr Castle, Ireland and observed the Whirlpool galaxy and its dwarf companion galaxy, identifying the spiral structure.\n- Scientists think than the companion dwarf galaxy, M51b and the Whirlpool galaxy have already passed each other and possibly passed through each other at least one time. There has been major disruption in the structure of M51b due to the cosmic dance that the two are having. The arms of the Whirlpool galaxy have also been distorted.\n- Astronomers believe that the collision with the dwarf companion galaxy of M51b is causing an explosion in the birth of new stars in the Whirlpool galaxy.\n- The Whirlpool galaxy has a mass that is huge. Its total mass is around 160 billion times that of our sun.\n- The mass of the Whirlpool galaxy might be huge, but the galaxy’s size is only about 35% of the Milky Way galaxy.\n- If you wanted to find the Whirlpool galaxy you would need to make sure that you go out on a dark night with very little light interference from towns or cities. Look towards the southeast of the Big Dipper to find the Canes Venatici constellation.\n- Astronomers know that the core of the Whirlpool galaxy is incredibly active. They believe that this may be the reason that it is so bright and demonstrates strong infrared spectrum emissions.\nThe Whirlpool galaxy, also known as M51, is one of the most visually appealing galaxies that we have discovered. It’s long been used in books, movies, television, and games as a source and backdrop for many science fiction stories.\nThe Homeworld series of games uses the Whirlpool galaxy as a basis, with the first game taking players on a trip through the galaxy to the core and outer rim.\nThe video game Ōkami has an appearance by the Whirlpool galaxy when the main character summons the galaxy by creating an oceanic whirlpool for travel.\nThe television series Roswell has three alien teenage protagonists that are supposed to originate from the Whirlwind Galaxy’s Five Worlds.\nDonald Moffitt’s novels The Genesis Quest and Second Genesis features humans from the future that all of the knowledge of humanity to the Virgo supercluster to aliens in the M51 galaxy.\nThe Mighty Morphin’ Power Ranges feature Master Vile, who is supposed to be the ruler of the Whirlpool galaxy.']"	['<urn:uuid:b86ad86e-2434-492c-a06a-205b3a5c7ed9>', '<urn:uuid:d0a0aca1-64ef-45ba-84b1-995a9adf31fe>']	factoid	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-13T02:03:54.148118	6	42	2868
61	What are the sources of marine debris and how deadly is it for whales?	Marine debris comes from both land and sea sources, with 80% originating from land and only 20% from ocean sources like shipping. According to UN estimates, there are about 13,000 pieces of debris per square kilometre in the world's oceans. This debris includes plastic sheets, covers, packaging, building materials, and fishing nets. The deadly impact is evident in cases like the 9.5-metre sperm whale found in Wakatobi, which had 5.9 kg of plastic items in its stomach, including 115 plastic cups, 19 pieces of hard plastic, plastic bags, and over 1,000 pieces of plastic raffia string. Globally, plastic debris kills an estimated 100,000 marine mammals annually.	['I was in the middle of giving a talk on the marine debris problem at a notable Californian marine research institute, when I drank the last of my water bottle, threw it onto the hall floor from the podium, and kept talking.\nThere it sat, to the surprise of several members of the audience. One well-meaning citizen then picked it up and returned it to the lectern.\n“Why did you do that?” I asked. “It just seemed wrong to leave it there,” he replied. Individuals know that something is wrong when the oceans are filled with debris. It is time for governments to act on that knowledge.\nBut sea debris comes from both land and sea sources and it’s hard to point to its exact origin and hence to get a polluter to take responsibility for cleaning it up.\nHow much is out there?\nThe search for Malaysian Airline flight MH370 was reportedly hindered by marine debris, which from satellite can appear quite similar to the remains of an aircraft. This sea junk doesn’t just impede missing flight searches, it has a severe impact on the health of the oceans, through entanglement and plastic pollution.\nMarine debris is manufactured or processed solid waste material developed by humans, so waste like floating logs, vegetation or sewage are not marine debris. In the last 25 years has seen the use of plastics increase 20 fold with dramatic impacts on marine debris levels.\nThe data available on the actual quantity of marine debris in the oceans is limited, usually revising estimates made over a decade ago by the UN Environment program. Of the debris that enters the ocean, approximately 70% of the inflow sinks and the other 30% floats or is suspended in the water column.\nMarine debris is found at background or ambient levels in all of the world’s oceans, with estimates of 13,000 pieces per square kilometre being proposed by UN Environment Program in 2005.\nBig pieces of marine debris include plastic sheets and covers, tarpaulins, crates, pallets, ropes, strapping and miscellaneous packaging, building materials, sealed drums and assorted industrial fishing nets, traps and lines.\nMarine debris is not uniformly distributed. In fact it usually starts to be noticed by humans when it builds to noticeable “hot spots” requiring clean up. The Pacific garbage patches are examples of oceanic garbage hotspots related to the north Pacific ocean gyres which cause garbage to accumulate. We know less about how debris accumulates in the Indian Ocean.\nMarine insurers recognise the occasional risk to shipping of a rogue floating container washed overboard, but rate the risk of hull loss as less than 1%. Ingestion of plastics into ship’s water intakes causing engine cooling issues and damage is more common, and can be costly, but rarely fatal.\nOnly 20% of marine debris is from ocean sources such as shipping, and 80% comes from land. Shipping has attempted to address waste at sea through marine pollution (MARPOL) International Convention for the Prevention of Pollution From Ships 1973/78. Primarily developed to address marine oil pollution, the convention requires ships to store and bring waste to port.\nHowever, the oceans are a common area that is a convenient dump for debris resulting from storms, floods and poor land and sea waste management practises.\nPrevention better than cure\nOne reason that marine debris does not get picked up is that it is too dispersed to collect it economically at sea.\nBecause the word marine is in there, the issue of whose responsibility is pushed to the door of government marine and fishery departments. Rarely is it seen as a waste issue, as waste is a local authority issue and their agency does not have anything to do with the sea.\nTo really clean up our oceans, we need to focus on stopping debris at the source on land through waste infrastructure.\nMy local coastal council deals with debris accumulation incidentally and instruct the garbage truck team to swing past the south end of the beach once a week where debris gathers. High densities of marine debris can be removed more cheaply per tonne than dispersed debris at sea.\nBut the failure of public municipal waste infrastructure is the primary source of chronic levels of marine debris near coastal megacities in Asia. In Jakarta, less than half of the city’s rubbish may reach landfill and the balance heads seaward via 13 rivers.\nSome Indian and Pacific ocean islands have municipal dumps at one end of the island, and the monthly high tide lifts the lot and another small debris island goes to sea.\nAnother method is to make polluters pay, or provide incentives for cleaning up. However even on land it is difficult to link irresponsible waste dumping to those who manufactured the product. But mishandling waste at other stages can put us right back where we started.\nBut it doesn’t always pay to do the right thing. An Alaskan fisher who brought a large fishing net he found floating in the Bering Sea back to town, and was faced US$10,000 for landfill disposal.\nGood ports are increasingly getting free receiving points for waste, but it still all too easy to leave it out there, a variation of “not in my backyard”, except in this case the backyard is really everyone’s.\nWho is responsible?\nUltimately the problem with sea debris is responsibility. There is a low sense of responsibility, ownership or recognition in government departments world wide that might encourage stewardship. Instead the problem is reduced to changing individual behaviour instead of government departments linking up to prevent debris entering the oceans.\nNational agencies need to relate with municipal authorities to bring attention to the need to prevent debris entering the water courses and then the sea. Ideally prevention will reduce or minimise the occurrence and cost of clean-up on the coasts.\nLong-term education is also an important but generally underrated part of the remedy. Education needs to produce awareness of marine debris and ways to prevent it entering the sea. Schools and volunteers can be involved in debris clean up, but it is preferable they gain an understanding of the need to prevent debris as well.', 'Originally published by WWF-Indonesia and WWF-Singapore. Written by Sharon Salim.\nLast Sunday, a 9.5-metre sperm whale was found stranded off the coast in Kapota Island, Wakatobi — dead. Cause of death: unknown. But what we do know: a staggering 5.9 kg of plastic items found in its stomach.\nWWF-Indonesia’s Dewi Satriani tell us more about this appalling incident.\nBelow, exclusive information povided by WWF-Indonesia on the timeline and all the things you need to know:\n#1 The whale was found by a local on Sunday evening\nWWF-Indonesia received a report from a community member of Kapota Island in Wakatobi on Sunday evening (Nov 18) informing them about an incident of a dead whale stranded off the coast.\nThe next Monday morning (Nov 19) at 7.30 AM local time, WWF-Indonesia, Wakatobi National Park, Wakatobi Marine and Fisheries Community Academy were deployed to the scene and discovered a whale carcass suspected to be a sperm whale.\n#2 The whale was already in an advanced state of decay\nThis is the reason why it was not possible to conduct a necropsy to investigate the cause of death. In this stage of decay (level 4), the carcass was already releasing an overpowering stench and the body was not intact. A necropsy can only be done if the stranded whale is still in level 2 stage of decay; when the skin still appears tight, emitting no smell, and the eyes still shine.\n#3 Identifying the whale was still possible\nEven though the whale carcass was no longer intact, WWF-Indonesia Marine Species Specialist Dwi Suprapti conducted a photo analysis of the carcass sent by the WWF-Indonesia team and identified the stranded whale as sperm whale (physeter macrocephalus). It has a block-shaped head, narrow jaw and teeth.\nSperm whale is also the largest toothed-whale, unlike other large-sized counterparts that are toothless. The possibility of it being a baleen whale was also ruled out thanks to the absence of prominent, iconic features of baleen like the rostral ridge and ventral grooves. This further confirms the fact that the stranded whale is a sperm whale (and not a blue whale as some news outlets have reported).\n#4 Finding a stranded whale in Wakatobi waters is not common\n“Whales do migrate through Wakatobi waters. In certain seasons we can see many different kinds of whales passing through Wakatobi islands, but stranding is not common. This is the only time we’ve found a stranded whale, which is also (alarming) because we found plastics in its abdomen,” Dewi revealed.\n#5 The locals decided to cut open the abdomen of the dead whale\nThe chunks of black material (left) are plastic raffia strings. A rubber-canvas sandal (right) was among the finds.\nTo be exact: there were 115 of plastic cups (750g), 19 pieces of hard plastic (140g), four plastic bottles (150g), 25 plastic bags (270g), a nylon sack (200g), and more than 1,000 pieces of plastic raffia string (3,260g) which totals to 5.9 kg inside the stomach, according to the identification result conducted by Wakatobi Marine and Fisheries Community Academy.\n“This discovery is deeply upsetting. It is a wake-up call for Indonesia about how plastic pollution is causing irreparable damage to our oceans and marine life. We urge businesses and governments to work together to address this issue urgently to prevent further plastic leakage into our oceans.” said Dwi Suprapti, Marine Species Specialist WWF-Indonesia.\n#6 There was no crowd management or public boundaries during the incidence\nSo as not to interfere or disturb the stranded whale, public members without any Personal Protective Equipment (PPE) including eye and face protection, gloves, and coveralls were advised to remain at a distance. This is a crucial protective measure to avoid any exposure of bacteria, virus or other dangerous microorganism emitted by the mammal and transmissible to human.\n#7 The whale was buried two days after it was discovered\nIt was buried on Tuesday (Nov 20) at Kolowawa Beach, North Kapota in Wakatobi for later retrieval of the bone specimen by the Wakatobi Marine and Fisheries Community Academy.\n#8 Plastic pollution is a transboundary problem that all countries share\nAlthough it remains uncertain whether the plastics were lodging the sperm whale’s ingestion organ or caused infections, we can’t turn a blind eye to how plastics production and pollution are still growing exponentially.\nEvery year, 8 million tonnes of plastics enter the ocean. By 2050, the total mass of ocean plastic will exceed that of fish. Plastic debris kills an estimated 100,000 marine mammals annually, as well as millions of birds and fish.\n#9: Beach clean-ups are not enough to solve the problem\nA big part of marine pollution comes from land sources, including rivers. An effective response to this crisis requires a global systemic change involving businesses and governments — and supported by consumers.\n#10 We cannot solve this issue alone\nKnowing that marine plastic pollution goes beyond Indonesia and the region, the global crisis requires global solutions.\nBusinesses need to take responsibility for the full life cycle of their products and play their part in helping governments deal with this issue.']	['<urn:uuid:25c25968-0bdf-4fed-b1c7-3d85d7d67a4b>', '<urn:uuid:d1103ef1-d057-45b2-9e68-b78458d5ce54>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T02:03:54.148118	14	106	1872
62	modern housing environmental protection methods impact	Modern housing developments and environmental protection methods show varying impacts across different cities. In East Austin, a new development project attempts to balance growth with environmental considerations by incorporating mixed-use spaces and integrating with existing infrastructure like trails and future rail lines, while offering affordable housing options. However, the Gorakhpur case study demonstrates that unchecked urban development can lead to environmental problems like flooding and loss of agricultural land. Their solution involves maintaining periurban agriculture zones, which serve multiple purposes: preventing flooding by preserving natural water absorption areas, providing local food security, and reducing energy footprints through local production. This approach also helps preserve local biodiversity and enables recycling of urban waste.	"[""Neighbors fight for more community benefits from huge East Austin development\nMonday, November 15, 2021 by Jonathan Lee\nA rezoning for a massive mixed-use development in East Austin went before the Planning Commission last Tuesday, showcasing intense negotiations between the applicant and neighbors trying to squeeze community benefits from the project.\nThe current proposal for the 15-acre site at 6705 and 6501 Regiene Road calls for a whopping 1 million total square feet of office, retail and restaurant space, in addition to artist workshops, a brewery and anywhere between 371 and 742 apartments – 10 percent of which will be affordable at 60 percent of the median family income.\n“The project is intended to be a mixed-use hub dedicated to Austin’s creative community,” said Leah Bojo, representing the applicant. Ten thousand square feet of commercial/industrial space will also be priced affordably for local artists and businesses.\nNeighbors, despite making inroads toward securing more concessions, did not support the rezoning. “We think you should deny this until there are better investments for communities of color that are left in this area,” said Nadia Barbot, co-chair of the East MLK Neighborhood Contact Team.\nThe change from Limited Industrial Services (LI) and Single Family-Standard Lot (SF-2) zoning to Limited Industrial Services-Planned Development Area (LI-PDA) zoning would allow 275 feet of building height near U.S. Highway 183 and 120 feet on the rest of the site. It would also double the allowable floor area ratio while cutting parking requirements in half. City staff does not support the 275-foot height allowance.\nThe project sits next to important multimodal infrastructure. The Walnut Creek Trail and the right of way for the future Project Connect Green Line regional rail run along the southern edge of the site. Bojo is in talks with Capital Metropolitan Transportation Authority about a rail station near the development.\nThis isn’t the only big project in the area. Adjacent to the site is Tech 3443, a 110-acre redevelopment proposal for the former Motorola chip factory site. Council approved the rezoning for Tech 3443 last year, permitting building heights ranging from 160 to 400 feet. In this case, the developer also worked with neighbors and offered extensive concessions.\nPlanning commissioners supported the Regiene Road project on the whole, but had reservations about allowing industrial use so close to future homes. In a stretch of convoluted motion-making, commissioners tried to limit industrial use, but no motion had enough votes to pass. This means the rezoning moves on to City Council without a recommendation.\n“It’s very complex, and it definitely deserves more public input,” Chair Todd Shaw said, adding that neighbors will have time before Council to continue negotiations.\nThose talks have been ongoing for over a year. When the commission first heard the case two weeks ago, several neighborhood representatives said they were close to striking a deal with Bojo. On Tuesday, neighbors still did not support the project, despite having secured affordability and worker protections, among other concessions.\n“We created a customized framework of community benefits and the developer continues to not counter or amend the project in any way,” Barbot said. “We believe the community deserves a small portion of the increased value the developer will receive with the additional height and the additional uses that this rezoning allows.”\nBojo said that the development, “even without those extra voluntary things, is an excellent asset to the community and is going to be a benefit in and of itself.” Though Bojo has declined to consider any off-site concessions the neighborhood has suggested, such as donations to local organizations, anything on-site is still on the table.\nTalks will likely continue until Council takes up the case either next month or early next year.\nThe Austin Monitor’s work is made possible by donations from the community. Though our reporting covers donors from time to time, we are careful to keep business and editorial efforts separate while maintaining transparency. A complete list of donors is available here, and our code of ethics is explained here.\nJoin Your Friends and Neighbors\nWe're a nonprofit news organization, and we put our service to you above all else. That will never change. But public-service journalism requires community support from readers like you. Will you join your friends and neighbors to support our work and mission?"", 'Urban flooding seems to be a recurrent event in the country, whether it is Mumbai, Guwahati, Srinagar or Chennai. We seem to be in a permanent fire fighting mode, waking up to every crisis only when it hits us hard and strong. Till then, we turn a lazy,blind eye to haphazard urbanisation, illegal encroachments, guzzling of water bodies by realtors and poor urban planning happening all around us. We lose lives, livelihoods, property, food production & so much more to this calamity. And climate change adds a new dimension to this growing cacophony. Can we reduce the impact of urban flooding, where we act well in time, rather than simply reacting after the disaster occurs?\nHere is the story of how Gorakhpur farmers began periurban agriculture, to be both food and flood efficient…\nGorakhpur, a secondary city located at the confluence of the Rapti and Rohin rivers in eastern Uttar Pradesh, is one of the fastest-growing cities in the mid-Gangetic plains. Here flooding is a common phenomenon, with the periurban areas prone to recurring floods and waterlogging. Caught between floods & water logging, loss of agricultural land & rapid urbanisation, the small and marginal farmers suffered.\nWas there a way to ensure local food supply, as well as maintain open areas to serve as flood buffers? The Gorakhpur Environmental Action Group (GEAG) definitely thought so. They devised a simple strategy: Strengthen livelihoods based on periurban agriculture. The aim was twofold: Reduce risks & vulnerabilities of the poor dependent on peri urban agriculture and also that of the city’s population affected by flooding.\nFarming at the city outskirts keeps areas that are vulnerable to flooding free from construction, and also ensures that the lands maintain their natural function of enhancing water storage and infiltration, thus reducing run off. Thus the capacity of the urban poor to cope with flood impact improves, as does their available sources of food & income. Other important outcomes include nutritional security, reduced energy footprint, improved income opportunities, local biodiversity preservation and recycling of urban waste.\nFarmers lose out\nSmall and marginal farmers are both socially and economically vulnerable. Floods and waterlogging result in decreased soil fertility and increased incidence of pests & diseases in the crops which severely impacts their food security. Periurban areas have also become the city’s waste & sewage dumping grounds, leading to health problems, deterioration of soil quality & contamination of groundwater. Increased cost of agricultural inputs makes agriculture almost unaffordable for these farmers whose net gains are very low. Also, these farmers do not have access to agricultural schemes directed at rural farmers, nor are they eligible for government subsidies on agricultural inputs, and neither do they have access to extension services. Loss of land for non agricultural purpose, also leads to distressed migration.\nOut of the box ideas to deal with flooding\nThe interventions were many. They included climate-resilient agriculture, a type of farming based on the principle of integrating household, livestock and agriculture, and seeks to enhance diversity and recycling in the farming systems; Integrated & diversified farming systems which means increasing the number and performance of functions; Time and space management that includes altering the timing of cropping cycles & sowing varieties that can withstand water inundation; Loft farming where farmers fill\nold tyres or sacks with soil and manure and plant seeds of climber vegetable crops thus saving the produce from spoiling in\nthe waterlogged fields; Raised low tunnel poly-houses for growing seedlings; Climber crops where they use support for creeping vegetables to climb up; Flood-resilient crop varieties that grow in flood and waterlogged conditions; Climate information advice that provide farmers with advance weather information; Promoting Low External Input Sustainable Agriculture (LEISA) which reduces the use of high-cost external inputs such as chemical pesticides and fertilisers, thereby increasing the net gains.\nOutcomes were seen at various levels, and it was felt that periurban agriculture should be undertaken by the entire community, as only then will its impacts be felt and sustainability achieved. Government schemes should be formulated in such a way that they encourage it’s at the community level, since the problems that farmers face are not individual but affect the whole community. Conservation of periurban villages is essential, not only for providing a low-cost supply of food to the urban market, but can also be developed for the services it provides to urban areas and also for agro-based industries, such as seed production, processing of dairy products and meat, tanneries, laundry services, carpentry, etc.\nThe experiences generated from this initiative will be used to develop and implement a policy framework. Apart from promoting LEISA techniques and other farm practices as ways of adapting to flood situations, the focus will also be on organising communities so that they can compete in markets. Through advocacy initiatives with city authorities, efforts will be made to implement regulatory frameworks that preserve periurban agricultural space and discourage change in land-use patterns. Lastly, experiences will be disseminated and shared in other areas with potential for periurban agriculture.\nThis article was first published by the Urban Agriculture Magazine, and has been republished here in partnership with them.']"	['<urn:uuid:7e84992f-7693-438e-8e73-94273e591e77>', '<urn:uuid:e8d5faad-d3fc-412e-be99-beab85107dd0>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T02:03:54.148118	6	112	1569
63	indigenous peoples land rights actions canada	Indigenous peoples have historically maintained important trade routes and lived sustainably on their traditional territories, including Treaty 1, 2, 3, and 5 Territories, which are home to various nations including the Anishinabek, Inninewak, and Dene peoples. Today, the Government of Canada is taking action through the United Nations Declaration on the Rights of Indigenous Peoples Act, which became law on June 21, 2021. This Act requires the development of measures to ensure federal laws align with the Declaration and the creation of an action plan by June 2023 to address injustices, combat prejudice, and eliminate discrimination against Indigenous peoples.	['Treaty 1 Territory and the National Homeland of the Red River Métis: Winnipeg Regional Office\nWe take time to acknowledge and honour Treaty 1 Territory and the Peoples and the lands that makeup Turtle Island. Lands which are home to the Anishinabek, Inninewak, Anishininwag, Dakota, Lakota and Dene peoples who, prior to contact with Europeans, created and maintained important trade routes, belonged to the land and respected non-human species, and thrived in a culture that was celebrated through language, ceremony, tradition and a sustainable economy. Also important is the recognition that Treaty 1 Territory is the National Homeland of the Red River Métis; a nation that came to be in the late 18th, early 19th century. A nation of people of mixed Indigenous and European ancestry with its own distinct culture, language and history. As we gather in Treaty 1 Territory it is important to recognize and honour Treaty 3 Territory, in particular Shoal Lake 40 First Nation, the source of Winnipeg’s clean drinking water. To honour the gift of water, Career Trek recognizes that in September of 2021, community members were finally able to drink clean water from their taps, ending a 24-year boil water advisory. We further recognize the importance and impact of the all-weather road that opened in June 2019. In addition to Treaty 3, we also acknowledge and honour Treaty 5 as it is this Territory which provides us with access to the electricity we use in both our personal and professional lives.\nTreaty 2 Territory: West Regional Office – Brandon, MB\nCareer Trek acknowledges, respects and has the honour of working on lands in Treaty 2 Territory, a treaty that was negotiated and entered into in 1871 at Manitoba House. It is important to recognize that the land on which we’re gathered is the home of the Dakota, Anishinabe, Inninewak , Anishininwag, Dene and Métis peoples. It is on these lands, that prior to European contact, Indigenous peoples established and maintained important trade routes, connected with and lived off the land, and thrived in a culture of language, ceremony, tradition and a sustainable livelihood.\nTreaty 5 Territory: North Regional Office – The Pas, MB\nCareer Trek acknowledges, respects and has the honour of working on lands in Treaty 5 Territory, a treaty that was first negotiated and entered into in 1875 in Berens River. Treaty 5 Territory is home to the Inninewak and Dene Peoples of the North who, prior to contact with Europeans, established and maintained important trade routes, connected with and lived off the land, and thrived in a culture that was celebrated through language, ceremony, tradition and a\nToday we are witness to the reawakening and revitalization of Indigenous languages, ceremony, tradition and connection to the land. This return to traditional ways of knowing is re-establishing the connection, place, livelihood and belonging for Indigenous Peoples who have inhabited Turtle Island since time immemorial.\nAs we think about and acknowledge Treaty 1, Treaty 2, Treaty 3, and Treaty 5 Territory, and the National Homeland of the Red River Métis, and as we move along the path of reconciliation it will be important to remember that we are on a journey of reconciliation because of a history, that until recently, hadn’t been told – the true history, a dark history of Canada’s relationship with Indigenous peoples.\nIn the era of truth and reconciliation it will be important to ensure there is a dedicated commitment from individuals and institutions to the 94 Calls to Action outlined by the Truth and Reconciliation Commission, the 46 articles of the United Nations Declaration on the Rights of Indigenous Peoples and the 231 Calls for Justice that came from the National Inquiry into Missing and Murdered Indigenous Women, Girls and Two-Spirit people.\nTaken from the Truth and Reconciliation Commission’s principles of reconciliation:\n“Together, Canadians must do more than just talk about reconciliation; we must learn how to practice reconciliation in our everyday lives – within ourselves and our families, and in our communities, governments, places of worship, schools, and workplaces. To do so constructively, Canadians must remain committed to the ongoing work of establishing and maintaining respectful relationships.” (Truth and Reconciliation (2015) Final Report. National Center for Truth and Reconciliation, University of Manitoba:Winnipeg,MB.P.16).', 'Download or consult our engagement kit that includes information on the Declaration, the Act and other resources.\nOn this page\nThe United Nations Declaration on the Rights of Indigenous Peoples Act became law and came into force on June 21, 2021.\nAs the next step, the Government of Canada is working in consultation and cooperation with First Nations, Inuit and Métis to:\n- Develop measures necessary to ensure the consistency of federal laws with the Declaration\n- Develop an action plan to achieve the objectives of the Declaration\n- Develop annual reports on progress and submit to, or table in, Parliament\nThe Act requires that the action plan be developed as soon as possible and no later than two years after the coming into force of the Act, which means it needs to be completed by June 2023.\nMoving forward together\nAs a first step, the Government of Canada will be working in partnership with First Nations, Inuit and Métis to implement the UN Declaration Act. There will be two engagement phases.\nPhase one focuses on working in partnership with Indigenous peoples to better understand their priorities to help shape the initial draft of an action plan and to begin to identify potential measures for aligning federal laws with the Declaration. Consistent with the Act’s requirements, this process will include:\n- First Nations, Inuit and Métis rights holders, including modern treaty signatories, self-governing nations and historic treaty partners, as well as with national and regional Indigenous representative organizations\n- Indigenous women, Elders, youth, persons with disabilities, 2SLGBTQQIA+ people, urban Indigenous people and other Indigenous organizations and groups\nPhase two focuses on continued engagement with Indigenous peoples to validate the draft action plan. The draft action plan will be informed by the priorities identified in phase one. This phase also includes opportunities for broader engagement, including with provinces and territories and industry. This phase is anticipated to start in the fall of 2022/winter of 2023.\nHow to participate\nThere are multiple ways for Indigenous peoples to participate in this engagement process:\n- Indigenous-led engagement sessions\n- A number of virtual sessions organized by Justice Canada\n- Online submission tool, email or mail\nSubmit your feedback\nTo submit your feedback, we invite First Nations, Inuit and Métis to fill out the following online submission tool. The series of questions and themes are designed to help the Government successfully meet the requirements of the Act and ensure that Indigenous priorities are identified.\nRead the Privacy statement and Terms of reference related to this online submission tool.\nFeedback and comments can also be submitted via email or mail.\nUN Declaration Act Implementation Secretariat\nDepartment of Justice Canada\n275 Sparks Street\nCanada K1A 0H8\nPotential topics for discussion\nWe welcome all views and perspectives on key priorities for the action plan, and measures to ensure the consistency of federal laws with the Declaration and annual reporting on progress.\nThe Act requires the action plan to include measures:\n- to address injustices, combat prejudice and eliminate all forms of violence, racism and discrimination against Indigenous peoples, including elders, youth, children, persons with disabilities, women, men and gender-diverse and two-spirit persons\n- to promote mutual respect and understanding, as well as good relations, including through human rights education\n- related to the monitoring, oversight, follow up, recourse or remedy or other accountability with respect to the implementation of the Declaration\n- for monitoring the implementation of the plan itself and for reviewing and amending the plan\nThe Act specifically outlines key areas that the Government of Canada must consult on and cooperate with Indigenous peoples in order to meet the requirements of the Act.\nTo assist, there are a series of questions, a list of themes of the Declaration, and an engagement kit available to help guide discussions and input related to each of the legal obligations contained in the Act.\nThis collaborative work will complement other initiatives underway across Canada with Indigenous partners to close socio-economic gaps, advance reconciliation and renew relationships based on the affirmation of rights, respect, co-operation and partnership.\n- Statement on the 15th anniversary of the United Nations Declaration on the Rights of Indigenous Peoples - September 13, 2022\n- Watch the video: The United Nations Declaration on the Rights of Indigenous Peoples explained\n- Annual Report 2022\n- Statement – June 21, 2022\n- Engagement and resource kit\n- Fact Sheet - The United Nations Declaration on the Rights of Indigenous Peoples Act\n- Declaration themes\n- Watch the video: Voices on the UN Declaration on the Rights of Indigenous Peoples\n- The Declaration in action\n- Bill C-15: What we learned report\nReport a problem on this page\n- Date modified:']	['<urn:uuid:53251698-5245-462d-9a41-f2ef734e98d9>', '<urn:uuid:9229eb2c-554f-4e0f-8b9e-d60a8f17a26e>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T02:03:54.148118	6	99	1485
64	What is unique about dispute resolution in IGO 3.0 licenses?	In IGO 3.0 licenses, disputes are resolved through mediation first, and if unsuccessful, through arbitration. This is different from other licenses because IGOs have privileges and immunities from national legal processes. The process begins with mediation, where a neutral third party helps resolve the dispute. If mediation fails, either party can initiate arbitration, which allows for remote participation. IGOs can designate specific mediation and arbitration rules in the copyright notice, with UNCITRAL arbitration rules applying by default.	"[""Intergovernmental organizations (IGOs) are using CC to share research, data, and educational materials they produce. IGOs, like all creators who want wide dissemination of their content, realize they can benefit greatly from the use of Creative Commons licenses--maximizing the impact of their resources and efforts. A number of IGOs believe that as publicly minded institutions, adopting an open licensing policy for at least some subset of their publications is the preferred mechanism for ensuring the broadest and most widespread use and reuse of the information they publish.\nThis page explains some of the benefits for IGOs choosing to publish content under Creative Commons licenses, clarifies some unique legal considerations, provides case study of IGOs already using CC, aggregates relevant frequently asked questions, and addresses common licensing scenarios and options available to IGOs.\nIGOs are unique in several respects from individuals and other organizations. Below are some common questions about how CC licenses work for IGOs.\nAnyone may use CC licenses for works they own, including IGOs. The reasons for doing so vary, and often include a desire to maximize the impact and utility of works for educational and informational purposes, and to enhance transparency.\nCreative Commons published a 3.0 ported license suite specifically intended for use by IGOs. These ported licenses -- known as the 3.0 IGO ported licenses -- grant all of the same permissions as our international (unported) 3.0 licenses; however, they have two unique provisions. First, unlike all other 3.0 licenses, where the licensor is an IGO then unless otherwise mutually agreed, disputes are resolved by mediation or, if that is unsuccessful, through arbitration. This provision was included in response to the challenges IGOs face with enforcing their copyright. IGOs have privileges and immunities from national legal processes, including judicial processes. Waiving that immunity so they can bring suit in a national legal forum can be exceedingly difficult. Instead, IGOs typically use mediation and arbitration as the preferred means to resolve legal disputes.\nSecond, unlike the other 3.0 licenses, the 3.0 IGO ported licenses contain a cure period just like CC's new 4.0 licenses. If a licensee fixes a license violation within 30 days of discovery, the license automatically reinstates. The inclusion of the provision is intended to help reduce the likelihood that mediation and arbitration become necessary.\nYou should be aware of the dispute resolution mechanism in the license, which is contained in Section 8(h). Disputes involving works licensed by IGOs under the licenses are resolved by mediation and arbitration.\nGenerally, mediation is a process used to avoid settling a dispute in court. The process typically involves a neutral third party (called a mediator) who tries to help the parties resolve the dispute. Mediation is not binding, however, unless the parties agree otherwise. Arbitration is also used to avoid settling a dispute in court, but the third party (called an arbitrator) has authority to make a decision in favor of one party. Arbitration tends to be more formal than mediation.\nBefore using any work licensed by an IGO under the IGO 3.0 ported licenses, be sure you understand what the mediation and arbitration processes are that have been chosen by the IGO and know what they mean for you. IGOs typically designate those in the copyright notice attached to the work.\nAssuming a violation of the license has occurred and the dispute cannot be amicably resolved, the process starts with mediation. The IGO/licensor sends a notice of mediation to the licensee designating the mediation rules if those are not already identified in the copyright notice accompanying the work. If mediation is unsuccessful, then either the licensor or licensee can chose to commence arbitration. If arbitration becomes necessary, then those proceedings allow for remote participation (e.g., by teleconference, written submissions, etc.) whenever practicable.\nIGOs have the ability to designate the particular mediation and arbitration rules in the copyright notice attached to the work, though the licensor and user of the work can always agree otherwise. If none is designated and no agreement is reached, then the mediation rules will be those identified in the notice of mediation sent to the licensee. If the matter progresses to arbitration, then unless otherwise stated in the copyright notice the rules that apply are the current Arbitration Rules of the United Nations Commission on International Trade Law (known as the UNCITRAL arbitration rules). The UNCITRAL rules are widely used by IGOs and others.\nNote that Creative Commons does not endorse any particular mediation or arbitration rules. Creative Commons has published special deeds for the IGO 3.0 ported licenses that emphasize that disputes are resolved by mediation and arbitration. You should always take note before using a work by an IGO whether the license used is an IGO 3.0 ported license.\nNo, the only differences are the mediation and arbitration processes and the ability to cure a violation and regain your rights as a licensee. CC and the IGOs took great care to ensure that the interpretation of the licenses are no different otherwise than the 3.0 international licenses. The adjudicating body (the mediation or arbitration tribunal) will interpret the scope of the license and remaining obligations in accordance with general principles of international law. Exceptions and limitations remain unregulated by those licenses as well.\nNote that the IGO 3.0 port is designed so that only IGOs as defined in the license are able to use mediation and arbitration. It is not available to anyone else using the licenses.""]"	['<urn:uuid:bb51e81b-3397-4b68-a8d7-cdcffa1f49f4>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T02:03:54.148118	10	77	907
65	how is henna grown in morocco and what jewish celebrations use it	In Morocco, henna is grown in the Draa Valley and harvested three times yearly, with the first harvest in late May/early June considered best for color. It requires extensive irrigation and grows best in temperatures between 35-45°C. In Jewish Moroccan culture, henna is used in traditional celebrations including baby showers, weddings and bat mitzvahs, where it's applied during special henna parties. According to tradition, a Jewish bride cannot do household chores until her henna fades.	"[""OMG it looks like chocolate!!\nIt's from a plant and goes well with sugar, but henna is not as yummy to eat as chocolate. Henna seems like a miraculous plant—its leaves are green, but the stain they leave is a dark reddish-brown. The layman and the henna artist both see magic. In Morocco, henna is quite literally magic—a plant endowed with supernatural properties that will bring luck and protection to anyone who uses it in any way. But despite its special powers, henna (Latin name Lawsonia inermis) is a humble shrub. Henna plants grow to a height of 3–5 meters (10–16 feet) and can be found all over North Africa, the Middle East, South Asia, Southeast Asia, and beyond.\nIn addition to its mystical properties, henna has ordinary chemical magic in the form of lawsone, the dye molecule in henna that binds with the protein in skin (and hair) to produce a reddish-brown color. The greatest amount of lawsone is found in the leaves of the plant, which are dried, ground into a powder, and then sifted for use on skin and hair. The drying process causes the cells to burst, making the lawsone available to bond with skin and dye it. Henna powder must be fresh, usually no older than 6–12 months. To maintain its freshness, the powder must be kept away from light, heat, and moisture. Those three elements release henna’s dyeing potential—too much of them can make henna stale and incapable of producing a stain. Henna leaves can be picked fresh, crushed, and applied directly to skin to get a stain, or a paste can be made from the powder and some type of liquid. After the paste is applied, it must remain on the skin for several hours to allow the dye molecules to bind with the skin. Henna stains more quickly and darkly on thicker as opposed to thinner skin; thus, hands and feet stain the best. Moving away from the extremities, henna stains less well.\nWho invented henna?\nPassionate arguments have been made by various countries claiming to be the birth- place of henna and henna-related arts, but research has concluded (with some trepidation) that henna’s geographical origin is probably Egypt, whose word for the plant was “henu,” the likely origin of the Arabic word “henna” that is used all over the world. In South Asia, henna is called “mehndi” or variations thereof. In West Africa, it is called “lelli,” and the Tuareg call it “anella.” The Arabic word “henna” has as its root the letters “HNA,” which mean “to dye red,” specifically to dye with henna. Persians call it “hinna,” a word derived from the Arabic.\nThe henna plant is not native to the Maghreb; it probably was introduced by the Arabs through trade—although it is unclear as to when that occurred, possibly when Islam was being introduced to Morocco or even earlier. It is possible to conclude that henna is not indigenous to the Maghreb because there is no Amazigh word for henna among any of the various tribes while there are Amazigh words for all other native plants. What is curious, however, is that the Tuareg and West Africans do have their own words, which may mean that henna grew naturally in West Africa and was familiar to the Tuareg and West Africans before any contact with traders from the East. There is a belief in the Sahara that henna came from the southeast, not from the north (i.e., North Africa). It is impossible to know for sure.\nA henna bush grows in Morocco\nThe hotter the temperature where henna is grown, the higher the lawsone content of the leaves. Henna grows indigenously in the tropical savanna and tropical arid zone, in latitudes between 15°N and 25°S, from Africa to the western Pacific rim, containing the highest lawsone concentration in areas where temperatures range between 35°C and 45°C (95°–113°F). In Morocco, henna is grown in the Draa Valley in the south. It used to be grown more widely throughout Morocco; but as the country developed, that land was deemed to be more productive for other forms of agriculture. The areas of Morocco hot enough to grow henna are too dry; therefore, successful henna cultivation requires extensive irrigation. Moroccan henna is usually harvested three times a year. The first harvest, at the end of May/beginning of June, is the most sought-after harvest as the leaves are said to give the best color. The leaves are pulled off the branch, not cut; those from the top branches of the plant are thought to have the highest lawsone content. Moroccan henna is grown by cooperatives that sell the dried leaves to a single wholesaler/distributor who then grinds and sifts the henna powder. Moroccan markets sell dried henna leaves and powder by the kilo (loose) or in pre-packed boxes. In the countryside, where only the leaves are available, it is common for women to wait to grind the leaves into powder until immediately before they want to use it."", 'Jewish Culture explore the many musical, experiential and ceremonial expressions of the Jewish experience through culture.\nExplore various expressions of Jewish culture including biblical, Hellenistic, Judeo-Arabic, Sephardic, Ashkenazic, Eastern European, American and Israeli. As we analyze Jewish culture across time and space, we will discuss how Jews both adopted the cultural assumptions of their neighbors and adapted these traditions to preserve\na distinct identity.\nHistory of Jewish Culture\nJewish Culture and History is an inter-disciplinary journal which brings together the best of current research in Jewish social history with innovative work in Jewish cultural studies. The journal includes cutting-edge research by younger scholars as well as established specialists, reviews of recent publications, and a documents section in each issue reproduces selected primary materials from archives and lesser-known resources for the study of Jewish culture and history. The journal explores previously neglected areas of the Jewish experience from a range of different perspectives including Jewish popular culture, social and political history, literary and cultural representation of Jews, and the global contexts of Jewish culture and history.\nJewish Culture And Traditions\nThe Jews have rich culture incorporating assorted ethnic backgrounds, languages and geographic locations. If you are an inquisitive vacationer and looking to explore the historical opulence of Jewish culture in Morocco, planning an excursion to the country with Morocco-Jewish-Tours.com is an ultimate option for you. Morocco Jewry has progressed as an amalgam of several cultures that have created the country itself, namely Arab, Jewish, Spanish, Berber and French. Here are given some of popular Jewish traditions.\n‘Mimouna’ is celebrated on the night after Passover ends, by most Moroccan Jews. Usually the celebration is during evening, but afternoon barbecues and picnics are also common.\nConventional Henna parties are an important part of the Jewish culture in Morocco and take place before special occasions like baby showers, Jewish wedding and Bat Mitzvah. During a Henna party, any older member of the family, usually the grandmother applies henna on the palms of the bride and the groom to emblematically bless them good health, wisdom, and security. It is believed that the henna protects the newlywed couple from demons. After the henna is applied to the groom and the bride, guests can also apply henna on their palms to bring good luck. As per the Jewish culture in Morocco, the bride is not allowed to do household chores till her henna fades off.\nJewish holidays in Israel are traditionally accompanied by very special meals, when families gather together and retell the ancient stories of the land of Israel. The Passover dinner, for example, comes to remind us the story of Exodus. When Moses lead his nation out of the life of slavery in Egypt, the people had only one night to get prepared for the journey. The dough for the bread patties they were making for the road did not have enough time to rise with the help of yeast. In Passover dinner, Jews still eat flat crispy bread, much like crackers called matza. Bread especially made for Saturday night dinner is called challah. It is a sweet fluffy white sesame seed topped bun in the form of a braid. Jews, whose ancestors lived in Ethiopia make a spongy sour dough bread called injera. A large flat bread by the name of Lafa, similarly to quesadilla is used to wrap meat, salad and condiments inside it.\nThough the Jews dress up according to the style of adopted nation, they also love to wear traditional clothing during celebrations like Mimouna, weddings, Bar Mitzvahs or intimate gatherings like Shabbat dinner. As per the Jewish culture in Morocco, men mostly dress in a white jellaba (also known as jellabiya) and women wear ornate kaftans.']"	['<urn:uuid:d2859a84-d3cf-45fa-b21d-458e919f90f9>', '<urn:uuid:c2061157-37dd-491c-a387-bae6bbe150e0>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T02:03:54.148118	12	75	1450
66	What's the connection between the Peace Day's education goals and World Water Day's environmental protection efforts in terms of creating long-term global impact?	Both days focus on long-term impact through different approaches. The International Day of Peace, particularly in 2013, emphasized peace education to minimize war and conflict by teaching peace values to students. Similarly, World Water Day addresses long-term environmental protection through education about ecosystem preservation, noting that 2/3 of world's forests are degraded and wetlands have largely disappeared since 1900. Both initiatives aim to create sustainable change - Peace Day through instilling peace values in future generations, and World Water Day through promoting ecosystem restoration and sustainable water management practices to ensure safe water access for all by 2030.	"['The International Day of Peace is observed annually on September 21. Also known as World Peace Day, the day is dedicated to enhancing cohesion and peace among all people across the world. The absence of war and violence is the key objective. The International Day of Peace was celebrated for the first time in 1982. Many countries, political, and military groups observe the day. In 2013, the Secretary-General of the United Nations dedicated the International Day of Peace to peace education. The aim was to minimize war and conflict by instilling the value of peace in students.\nHow International Day Came Into Existence?\nIn 1981, a declaration was made by United Nations General Assembly to set a date when peace will be commemorated. The aim was to have an International Day of Peace devoted in strengthening peace among people. The date was initially to coincide with the opening the yearly sittings of the UN General assembly. Nevertheless, in 2001, a new commemoration date was set for September 21 every year. The maiden celebration event was held on September 21, 1982. The main message was “right to peace by all people.”\nOther Events That Shaped the Peace Day\nIn 1983 a culture of peace to bring a strong bond among organizations, projects, and people was initiated. The aim was to increase peace for the generations as announced by the UN Secretary-General to retain the original vision. In 1996 a debate at Seanad Éireann in Ireland proposed for the expansion of the International Day of Peace to include Reconciliation.\nIn 2001, Secretary-General Kofi Annan wrote a message in honor of the International Peace Day. It was during this occasion that amendments were made, moving the IDP from third Tuesday of September to September 21. The General Assembly declared the Peace Day as a global ceasefire and non-violence.\nThe Peace Bell\nThe United Peace Bell was made from coins that were donated by children around the world except Africa. The bell is rung to inaugurate the Peace Day at UN Headquarters in New York City. The sound of the ringing bell acts as a reminder of the high cost of war. The sides of the bell have words advocating peace. The words read “Long live absolute world peace” Kofi Annan rang the bell in 2006 when his term was ending. Ban Ki-moon also rang the bell in 2007 to ask for a 24- hour cessation of conflict.\nActivities During The Peace Day\nDuring on the Peace Day, people all over the world engage in activities aimed at promoting peace. Events include inter-religious peace ceremonies, the lighting of candles, choirs presenting peace songs, convoy of cars for peace, planting of trees or art exhibitions to spearhead peace among communities.\nThe 2017 International Peace Day Celebrations\nThe theme was “Together for peace: Respect, Safety, and dignity for all.” The organizers sought to promote global peace and coexistence. An organization is known as “The Peace Crane Project” embarked on collecting one thousand cranes from students across the globe. The Cranes were to be displayed worldwide in venues commemorating the peace anniversary.', 'World Water Day is coming up one week from today. It\'s held annually on March 22, and is all about the importance of water.\nWater, food, air, and shelter are the fundamental building blocks for life, but so many do not have access to clean drinking water.\nEach year the UN picks different topics to focus on for World Water Day. This year they\'re looking at the connection of Nature and Water.\nThe day focuses on nature-based solutions to our current 21st Century water challenges.\nAccording to their website, ""Damaged ecosystems affect the quantity and quality of water available for human consumption. Today, 2.1 billion people live without safe drinking water at home; affecting their health, education and livelihoods.\nSustainable Development Goal 6 commits the world to ensuring that everyone has access to safe water by 2030, and includes targets on protecting the natural environment and reducing pollution.""\nHere are some interesting water facts you can share:\n- 2/3rds of the world\'s forests are degraded\n- 2/3rds of the world\'s wetlands have disappeared since 1900\n- 1.9 billion people live in areas where water is already scarce by 2050 this could increase to 3 billion\n- 80%of all the world\'s wastewater flows back into the rivers and oceans without being treated\n- Soil erosion results in 40 Billion tons of loose topsoil washed into our water every year\n- Floods, droughts, and storms have affected over 4 billion people and caused $1 Trillion of damage in the past 25 years\nThe UN is looking at nature based solutions to help the management of water resources. Restoring ecosystems and preserving our wetlands can help to create jobs and reduce the amount of damage.\nFor example, adding vegetation along the waters edge can act as a buffer when it rains too much and the water rises. It\'s about working with nature and it\'s natural course, rather than trying to bend nature to our will.\nIn order to celebrate World Water Day here are 10 things we can implement in our life.\n- Be grateful. If you wake up and clean water flows out of your tap, you are blessed. Every time you run the tap, remember how blessed you are. I find keeping this perspective helps with conservation efforts!\n- Turn off the tap. Whether you\'re brushing your teeth, your hands, the dishes. Turn the water off when not in immediate use. Keep a basin in your sink to wash your dishes.\n- Shower with a bucket. While your shower water heats up, use a bucket to catch the cold water. Use it to water your lawn, flush your toilet, or wash your dishes.\n- Shower less. Most people shower WAY too much. You shouldn\'t shower daily unless you\'re very sweaty or very dirty. It\'s not good for your skin. If you\'re prone to getting stinky just wash the stinky parts like the pits and feet, and save a full shower for later!\n- Run it full. Run your laundry or dishwasher when it\'s full to conserve water.\n- Change your landscape. Reduce the amount of grass you have and opt for drought friendly lawn space or convert it into an edible garden, so you\'re able to grow local food!\n- Keep it real. Use real plates and cloth napkins. It might sound like more water is being used in washing the items, but it actually takes 8 gallons of water to make one paper plate! WOW! The average dishwasher only uses 6 gallons of water. Look at all those water savings.\n- Stop the leaks. Monitor you water bill. If you see something spike, maybe you have a leak? This helps to save water and puts some money in your pocket.\n- Watch what you eat. Agriculture accounts for 65% of the world\'s fresh water. Opt for low water crops instead of high water crops. Reduce your meat and dairy intake. Try to add in more low water crops like corn, sweat potatoes, amaranth, and black-eyed peas.\n- Watch what you wear: Did you know it takes 2,700 gallons of water to make one cotton t-shirt!? Shop for clothing second hand, find contentment in what you already own, host a clothing swap with friends and know you\'re saving lots of water!\nFor more eye-opening water information, calculate your water footprint.']"	['<urn:uuid:6e0c1038-674b-4b03-8e37-45225b222ee6>', '<urn:uuid:3111b686-fa71-4fd2-82cf-cfb3e8e52c60>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T02:03:54.148118	23	98	1234
67	how does cell size affect what type of cell stem cells become	Cellular water efflux, which causes a decrease in cell volume and an increase in stiffness, influences mesenchymal stem cell differentiation propensity, specifically towards an osteogenic or adipogenic fate. MSCs undergoing multilineage differentiation exhibit alterations to their cell volume.	['You are hereNovember 6, 2017 | Mesenchymal Stem Cells\nCell Volume modulates Mesenchymal Stem Cell Fate\nReview of “Cell volume change through water efflux impacts cell stiffness and stem cell fate” from PNAS by Stuart P. Atkinson\nMultiple studies have established that external osmotic pressure prompts a reduction in cell volume via the efflux of water, leading to higher cell stiffness and altered cell mechanics/behavior [1-3]. With this knowledge in hand, researchers from the laboratory of Jennifer Lippincott-Schwartz (Howard Hughes Medical Institute, Virginia, USA) and David A. Weitz (Harvard University, Massachusetts, USA) asked whether cells alter their volume to modify cell behavior in response to additional stimuli. In their new study, Guo et al. describe their new findings, and report a striking link between cell volume and mesenchymal stem cell (MSC) fate .\nThe authors discovered that cell growth on a stiff growth substrate also leads to a reduction in cell volume and an increase in cell stiffness mediated by water efflux - a similar behavior to cells cultured under external osmotic pressure. Substrate stiffness, cell spread area, and external osmotic pressure all correlated to decreased cell volume and increased stiffness mediated by water efflux. This reduction in cell volume leads to an increased concentration of intracellular material and a phenomenon known as “molecular crowding” and an overall modification of cell mechanics and cell behavior.\nInterestingly, the study then established that cellular water efflux, accompanied by a decrease in cell volume and an increase in stiffness, prompted alterations to MSC differentiation propensity, specifically towards an osteogenic or adipogenic fate. This all suggests that the consequences of reduced cell volume influences stem cell fate, a hypothesis strengthened by the finding that MSCs undergoing multilineage differentiation exhibit alterations to their cell volume.\nThe reduction in cell volume and increase in cell stiffness can affect processes occurring both in the cytoplasm and in the nucleus that influence stem cell fate. These include protein folding and binding kinetics, cell structure, in-cell transport, protein expression, and, importantly, chromatin structure, and, therefore, transcription and gene expression patterns [3, 5]. The next quest is to understand how MSCs control this phenomenon and to discover whether this means of fate determination exists in other stem cell types.\nTo hear more about cell volume and stem cell fate, keep the Stem Cells Portal bookmarked!\n- Zhou EH, Trepat X, Park CY, et al., Universal behavior of the osmotically compressed cell and its analogy to the colloidal glass transition. Proc Natl Acad Sci U S A 2009;106:10632-7.\n- Oh D, Zidovska A, Xu Y, et al., Development of time-integrated multipoint moment analysis for spatially resolved fluctuation spectroscopy with high time resolution. Biophys J 2011;101:1546-54.\n- Irianto J, Swift J, Martins RP, et al., Osmotic challenge drives rapid and reversible chromatin condensation in chondrocytes. Biophys J 2013;104:759-69.\n- Guo M, Pegoraro AF, Mao A, et al., Cell volume change through water efflux impacts cell stiffness and stem cell fate. Proceedings of the National Academy of Sciences 2017.\n- Swift J, Ivanovska IL, Buxboim A, et al., Nuclear lamin-A scales with tissue stiffness and enhances matrix-directed differentiation. Science 2013;341:1240104.']	['<urn:uuid:a8206997-7a01-4b8b-9297-fb8b6d606d4e>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-13T02:03:54.148118	12	38	517
68	climate change effects ocean temperatures north atlantic fish populations consequences employment	Climate change is causing warming of the oceans, with most additional energy since 1970 going into the oceans. In New England, this warming has severely impacted fish populations, particularly in the Gulf of Maine which is warming faster than 99.9% of the global ocean. This has led to cold-water species migrating north, resulting in a 35% drop in wages and a 16% decline in fishing employment over a 20-year period.	"['AGW, , anthropogenic global warming\nHuman activities are adding greenhouse gases, particularly carbon dioxide, methane and nitrous oxide, to the atmosphere, which are enhancing the natural greenhouse effect. While the natural greenhouse effect is keeping average temperature on earth at about +15°C, this enhanced greenhouse effect is leading to a dangerous degree of global warming. A fast rise in average temperature of Earth could result in rising sea levels, melted glaciers, floods, droughts and other hazardous scenarios. This is why mitigation and adaptation to anthropogenic climate change is so important.\nHuman activities are adding greenhouse gases, particularly carbon dioxide, methane and nitrous oxide, to the atmosphere, which are enhancing the natural greenhouse effect. While the natural greenhouse effect is keeping average temperature on earth at about +15�C, this enhanced greenhouse effect is leading to a dangerous degree of global warming. A fast rise in average temperature of Earth could result in rising sea levels, melted glaciers, floods, droughts and other hazardous scenarios. This is why mitigation and adaptation to anthropogenic climate change is so important.\nGlobal warming and climate change are terms for the observed century-scale rise in the average temperature of the Earth\'s climate system and its related effects. Multiple lines of scientific evidence show that the climate system is warming. Although the increase of near-surface atmospheric temperature is the measure of global warming often reported in the popular press, most of the additional energy stored in the climate system since 1970 has gone into the oceans. The rest has melted ice and warmed the continents and atmosphere. Many of the observed changes since the 1950s are unprecedented over tens to thousands of years. Scientific understanding of global warming is increasing. The Intergovernmental Panel on Climate Change (IPCC) reported in 2014 that scientists were more than 95% certain that global warming is mostly being caused by human (anthropogenic) activities, mainly increasing concentrations of greenhouse gases such as methane and carbon dioxide (CO2). Human-made carbon dioxide continues to increase above levels not seen in hundreds of thousands of years. Methane and other, often much more potent, greenhouse gasses are also rising along with CO2. Currently, about half of the carbon dioxide released from the burning of fossil fuels remains in the atmosphere. The rest is absorbed by vegetation and the oceans. Climate model projections summarized in the report indicated that during the 21st century the global surface temperature is likely to rise a further 0.3 to 1.7 °C (0.5 to 3.1 °F) for their lowest emissions scenario and 2.6 to 4.8 °C (4.7 to 8.6 °F) for the highest emissions scenario. These findings have been recognized by the national science academies of the major industrialized nations and are not disputed by any scientific body of national or international standing. Future climate change and associated impacts will differ from region to region around the globe. Anticipated effects include warming global temperature, rising sea levels, changing precipitation, and expansion of deserts in the subtropics. Warming is expected to be greater over land than over the oceans and greatest in the Arctic, with the continuing retreat of glaciers, permafrost and sea ice. Other likely changes include more frequent extreme weather events including heat waves, droughts, heavy rainfall with floods and heavy snowfall; ocean acidification; and species extinctions due to shifting temperature regimes. Effects significant to humans include the threat to food security from decreasing crop yields and the abandonment of populated areas due to rising sea levels. Because the climate system has a large ""inertia"" and greenhouse gasses will stay in the atmosphere for a long time, many of these effects will not only exist for decades or centuries, but will persist for tens of thousands of years. Possible societal responses to global warming include mitigation by emissions reduction, adaptation to its effects, building systems resilient to its effects, and possible future climate engineering. Most countries are parties to the United Nations Framework Convention on Climate Change (UNFCCC),whose ultimate objective is to prevent dangerous anthropogenic climate change. Parties to the UNFCCC have agreed that deep cuts in emissions are required and that global warming should be limited to well below 2.0 °C (3.6 °F) relative to pre-industrial levels, with efforts made to limit warming to 1.5 °C (2.7 °F). Public reactions to global warming and concern about its effects are also increasing. A global 2015 Pew Research Center report showed a median of 54% consider it ""a very serious problem"". There are significant regional differences, with Americans and Chinese (whose economies are responsible for the greatest annual CO2 emissions) among the least concerned.\nSource: Wikipedia - Global warming', 'For decades, climate change has wreaked havoc on New England’s historic fishing communities. Now we know precisely how much.\nA new study from the University of Delaware shows that erratic changes in atmospheric pressure, a phenomenon known as the North Atlantic oscillation, were singularly responsible for a 13-percent loss in fishing revenue between 1996 and 2017, and a 16-percent loss of jobs in New England’s hardest-hit communities.\nLarge pressure increases occur naturally, but are exacerbated by man-made emissions. They raise sea temperatures, scramble ocean currents, and drive a rapidly changing climate in the northern hemisphere. The warmer weather causes fish to leave the region and seek colder waters.\nBecause fishermen are paid based on what they catch, fewer fish naturally means fewer earnings and jobs. Each unusual increase in pressure was associated with a 35-percent drop in wages, an effect that persisted for years. Over time, climate change took away 16 percent of jobs in New England’s most afflicted fishing communities.\n“The individuals who can’t weather these climate shocks are the small mom-and-pop businesses and smaller fishing establishments,” says economist Kimberly Oremus, the study’s author. “There are communities that are just not going to be fishing communities anymore.”\nNew England waters were once home to an abundance of bottom-dwelling fish, such as haddock, redfish, and especially cod. That fish was once so abundant that one could purportedly walk “across the Atlantic on their backs,” as the saying went.\nNo longer. Due to overfishing, cod stocks are nearly depleted. To avert complete collapse, the National Oceanic and Atmospheric Administration now sets limits on fish catches, or quotas. In the past decade, catches have plummeted, from 100 million pounds of cod in the early 1980s to a fraction today.\nAt the same time, the Gulf of Maine, a frigid inlet between Cape Cod and Nova Scotia, has been warming faster than 99.9 percent of the global ocean. As cold-water species like lobster migrate north, fisherman are left empty-handed, and wait for warm-water blue crabs to eventually take their place.\nSudden “shocks” of atmospheric pressure contribute to that change, but that isn’t evident until one to six years later, when squid, sea scallops, and other species are mature enough to be fished.\nOremus didn’t observe any correlation between these “signal lags” and lowered quotas, which means NOAA fishing managers may not be including climate shocks in their calculations. That could lead to unintentional overfishing, and perhaps eventual collapse of the stock, she says.\nAccording to Oremus’ analysis, between 1996 and 2017, each unusual shock reduced the total catch in New England fishing counties by 2 percent. Those 2-percent reductions accumulated to a 13-percent overall decline in fish revenue. That 13-percent decline in fish revenue resulted in a 13-percent drop in employment, and a 35-percent drop in wages, over a six-year period.\nAll told, those positive climate shocks were responsible for an on-average 16-percent decline in county-level fishing employment over that same 20-year period. Other factors included industry consolidation, market swings, regulation of overfishing, and an aging workforce.\nIronically, as climate shocks were negatively impacting fishing jobs, wages and businesses, Oremus observed that the mining industry was enjoying a boom. Although labor data suggests that fishers could have been leaving their boats for mining and fracking jobs, Oremus cautions that more research is needed to prove the connection.\nThe study is available in full here.\nJessica Fu contributed reporting.']"	['<urn:uuid:81e1e477-feac-431f-8008-913f914f363b>', '<urn:uuid:beecc560-57aa-4a00-addc-b6ea17133482>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T02:03:54.148118	11	70	1330
69	How do smart pumping systems and ballast water management both protect equipment?	Smart pumping systems protect equipment by detecting and safeguarding against adverse conditions like dry running and cavitation, while ballast water management protects ships through required management plans and record books. Both systems use control measures to prevent damage - smart pumps adjust automatically while ballast systems require specific procedures like water exchange at sea.	"['Smart Pumping Systems: The Time Is Now (Part 1)\nThe pump industry has begun to incorporate computer technology to operate, control and protect pumps and their systems. Smart pumping systems can match pump output exactly to system conditions and can detect and protect against unusual operating conditions. Through the use of a smart variable speed controller, these systems can significantly reduce pump-operating costs by eliminating the use of energy consuming control valves, as well as reducing life cycle costs. All of the major components of life cycle cost, such as operation, maintenance and installation, need to be evaluated when comparing smart systems to conventional systems.\nWhat is a smart system?\nThe pump industry has started to use computer technology to operate, control and protect pumps and their systems. These smart pump controllers incorporate microprocessors as part of their normal function.\nA smart pumping system must be capable of knowing when to adjust the process variable in response to system changes without manual intervention. The system must also be fault tolerant, to enable the system to recognize and safeguard itself from operating under conditions that may reduce its life. Adverse conditions like dry running, operation below the minimum flow, deadheading, runout condition and cavitation must all be recognized and reacted to before damage occurs. The system must also be capable of understanding when the system transient or unusual operating condition has cleared, thereby allowing normal pump operation to resume.\nA smart pumping system consists of a pump, variable speed drive or controller, instrumentation (when required), microprocessor and special software. The pump can be any standard centrifugal pump fitted with instrumentation to measure a process variable. More common applications control discharge pressure, flow or process level (either suction side or discharge side). However, there are advanced products available that can measure and control the process variable without the need for external instrumentation. The pump control software enables the controller to sense pump and process conditions and react accordingly. These systems can be designed to maintain constant values of speed, capacity, pressure, level or other process variables, and can be controlled either locally or through a distributive control system (DCS).\nThe value to the customer in using Smart-Pumping Systems lies in the reduced life cycle costs. The major components of life cycle cost are initial cost , operation, maintenance and installation.\nSystem Curve. A system curve is comprised of a static component and a dynamic component. The static component of the system curve does not change with flow rate. The dynamic component is essentially proportional to the square of the rate of flow. It is also a function of other variables such as pipe configuration/size, surface roughness, quantity and type of fittings/valves and fluid viscosity. These can be represented by a single system constant and the dynamic or frictional head can be expressed as: Hf = K Q2\nThe dynamic head constant K is a constant for a given system. However, if a control valve position changes in the system, then the constant K will also change.\nConventional System. Figure 1 shows a typical control scheme for a conventional pumping system. Note the system curve includes a static component of 6.1 meters. This is the change in elevation between the suction and discharge source. In this type of system the pump operates at a fixed speed and the pump performance curve is based on an impeller diameter pre-selected to match the system requirements as closely as possible. It is common practice to add a safety margin to the design point when it is difficult to accurately define system losses. This can result in an oversized pump that runs out too far on the curve and absorbs too much power. The total system head curve intersects the pump head-capacity curve at point “A” with the control valve wide open. The design flow for this system is 638 m3/hr so the friction head (point “B”) must be increased to 37 meters in order to operate at this point.\nThe most common method of varying the capacity in a conventional system is to introduce a variable resistance device that will alter the system friction curve; this is the main function of a control valve. Control valves are throttling devices, which use some of the available pumping energy to control the process. The amount of consumed energy will vary depending on the method of control, valve sizing and the operating point. In the U.S., a common control valve standard (PIP PCECV001) specifies that the control valve shall be 50-80% open at design flow, at least 10% open at minimum flow and no more than 90% open at maximum flow. Other methods base the amount of pressure drop on past plant practice or rules of thumb.\nVariable Speed System. In a variable speed system the controller will match the pump output to system head requirements without the need for throttling a control valve. Safety factors and pressure margins typically built into dynamic head systems can be eliminated and in some cases result in a lower-cost pump selection. Since the smart controller can adjust the pump speed to suit the required system conditions, only one impeller diameter needs to be stocked, thus lowering inventory costs. In variable speed systems the design point no longer needs to be based on a fixed speed. This yields a larger number of selections over a given pump range with a better chance of operating at or near the best efficiency flow. Variable speed control is most effective and efficient in all friction head systems. The effectiveness diminishes somewhat for applications having high static head and low dynamic head curves, since the intersection of the pump and system curve moves further to the left of the best efficiency flow(1).\nSystems with flat head curves can result in unstable flow, making control difficult. Advanced products control the process variable by adjusting pump torque rather than speed. This provides a steeper, easier–to-control performance curve.\nFigure 2 shows a variable speed system with system curve “A” identical to that shown in Figure 1. Head-Capacity curves are shown at various speeds. If the desired operating flow is 638 m3/hr it is shown that the pump can operate at a substantially lower speed and head. In this example, the savings of the variable speed system over a conventional system is represented by the difference in head between points “B” (Figure 1) and “C” (Figure 2).\nLife cycle operating cost\nThis difference in system head requirements can often translate to thousands of dollars in energy savings over the life of a pump. Table 1 exemplifies these savings and is based on a cooling tower installation(2) represented by Figures 1 and 2. Note that the total pump head has been reduced from 37 meters to 23 meters. As a result, the pump power has dropped nearly 30 kW and operating speed has been lowered by over 300 rpm.\nTable 1 shows the difference in energy costs between a conventional system and a variable speed system represents a 35.5% decrease in operating costs over the life of the pump.\nTo read the rest of this article and find out more about the comparison between smart pumping systems and conventional systems, including lower maintenance cost, initial cost and installation cost, please click here.\n- Casada, D., 1999; “Energy and Reliability Considerations for Adjustable Speed Driven Pumps”, Industrial Energy Technology Conference, Houston, TX, pp. 53-62.\n- Kratowicz, R., 2000; “Less is More”, Fluid Handling Systems Magazine, pp.30-33.', 'Ballast water discharge and the environment\nBallast water discharges by ships can have a negative impact on the marine environment. The discharge of ballast water and sediments by ships is governed globally under the Ballast Water Management Convention, since its entry into force in September 2017. It is also controlled through national regulations, which may be separate from the Convention, such as in the United States.\nCruise ships, large tankers, and bulk cargo carriers use a huge amount of ballast water, which is often taken on in the coastal waters in one region after ships discharge wastewater or unload cargo, and discharged at the next port of call, wherever more cargo is loaded. Ballast water discharge typically contains a variety of biological materials, including plants, animals, viruses, and bacteria. These materials often include non-native, nuisance, exotic species that can cause extensive ecological and economic damage to aquatic ecosystems, along with serious human health issues including death.\nThere are hundreds of organisms carried in ballast water that cause problematic ecological effects outside of their natural range. The International Maritime Organization (IMO) lists the ten most unwanted species as:\n- Cholera Vibrio cholerae (various strains)\n- Cladoceran Water Flea Cercopagis pengoi\n- Mitten Crab Eriocheir sinensis\n- Toxic algae (red/brown/green tides) (various species)\n- Round Goby Neogobius melanostomus\n- North American Comb Jelly Mnemiopsis leidyi\n- North Pacific Seastar Asterias amurensis\n- Zebra Mussel Dreissena polymorpha\n- Asian Kelp Undaria pinnatifida\n- European Green Crab Carcinus maenas\nBallast water issues by country\nThe ballast tanks in New Zealand carry animals and plants that kill ecosystems. Ballast tanks are only used in cargo ships there. Ballast water is controlled under the Biosecurity Act 1993.\nThe zebra mussel, which is native to the Caspian and Black Seas, arrived in Lake St. Clair in the ballast water of a transatlantic freighter in 1988. Within 10 years it had spread to all of the five neighbouring Great Lakes. The economic cost of this introduction has been estimated by the U.S. Fish and Wildlife Service at about $5 billion.\nBallast water discharges are believed to be the leading source of invasive species in U.S. marine waters, thus posing public health and environmental risks, as well as significant economic cost to industries such as water and power utilities, commercial and recreational fisheries, agriculture, and tourism. Studies suggest that the economic cost just from introduction of pest mollusks (zebra mussels, the Asian clam, and others) to U.S. aquatic ecosystems is more than $6 billion per year.\nCongress passed the National Invasive Species Act in 1996 in order to regulate ballast water discharges. The Coast Guard issued ballast water regulations in 2012. Under the authority of the Clean Water Act, the Environmental Protection Agency (EPA) published its latest Vessel General Permit in 2013. The permit sets numeric ballast water discharge limits for commercial vessels 79 feet (24 m) in length or greater. EPA issued a separate permit for smaller commercial vessels in 2014.\nAmong 818 ports in the Pacific region, Singapore alone accounts for an estimated of 26 percent of cross-region (long range) species exchange. Via targeted ballast management on Singapore and a few other ""influential"" ports, cross-region species exchange to/from the Pacific region can be combinatorially reduced.\nTo react to the growing concerns about environmental impact of ballast water discharge, the International Maritime Organization (IMO) adopted in 2004 the ""International Convention for the Control and Management of Ships\' Ballast Water and Sediments"" to control the environmental damage from ballast water. The Convention will require all ships to implement a ""Ballast water management plan"" including a ballast water record book and carrying out ballast water management procedures to a given standard. Guidelines are given for additional measures then the guidelines.\nThe goals of the convention are to minimise damage to the environment by:\n- Minimise the uptake of organisms during ballasting.\n- Minimising the uptake of sediments during ballasting.\n- Ballast water exchange while at sea (the ship should be minimum 200 nautical miles from shore with a depth of minimum 200 metres and can use the flow through or sequential method). At least 95 percent of the total ballast water should be exchanged.\n- Treatment of the ballast water by chemical or mechanical influences (UV-radiation, filter, deoxygenation, cavitation, ozone…)\nControl measures include:\n- International Ballast Water Management Certificate\n- Ballast water management plan\n- Ballast water record book\nThe IMO convention was ratified by enough countries and entered into force on September 8, 2017.\n- ""International Convention for the Control and Management of Ships\' Ballast Water and Sediments"". International Maritime Organization.\n- Living Beyond Our Means: Millennium Ecosystem Assessment, 2005. Statement from the Board.[full citation needed]\n- Statement of Catherine Hazlewood, The Ocean Conservancy, “Ballast Water Management: New International Standards and NISA Reauthorization,” Hearing, House Transportation and Infrastructure Subcommittee on Water Resources and Environment, 108th Cong., 2nd sess., March 25, 2004.\n- David Pimentel, Lori Lach, Rodolfo Zuniga, and Doug Morrison, “Environmental and Economic Costs Associated with Non-indigenous Species in the United States,” presented at AAAS Conference, Anaheim, CA, January 24, 1999.\n- United States. Pub. L. 104-332. October 26, 1996.\n- U.S. Coast Guard, Washington, D.C. ""Standards for Living Organisms in Ships’ Ballast Water Discharged in U.S. Waters."" Federal Register, 77 FR 17254, 2012-03-23.\n- ""Vessels Incidental Discharge Permitting"". National Pollutant Discharge Elimination System (NPDES). Washington, D.C.: U.S. Environmental Protection Agency (EPA). 2015-12-09.\n- EPA (2014-09-10). ""Final National Pollutant Discharge Elimination System (NPDES) Small Vessel General Permit for Discharges Incidental to the Normal Operation of Vessels Less Than 79 Feet."" Federal Register. 79 FR 53702.\n- Xu, Jian; Wickramarathne, Thanuka L.; Chawla, Nitesh V.; Grey, Erin K.; Steinhaeuser, Karsten; Keller, Reuben P.; Drake, John M.; Lodge, David M. (2014). ""Improving management of aquatic invasions by integrating shipping network, ecological, and environmental data"": 1699–1708. doi:10.1145/2623330.2623364.\n- ""Ballast Water Convention to Enter into Force in 2017"". Maritime Executive. 8 September 2016. Retrieved 14 September 2016.\n- Buck, Eugene H.(2012). ""Ballast Water Management to Combat Invasive Species."" U.S. Congressional Research Service. Report No. RL32344.']"	['<urn:uuid:a8f55057-bdaf-4dbd-bcc1-9aec8f3ce81c>', '<urn:uuid:e9749ea3-2ea0-436c-95fd-f59b7fb5adc2>']	factoid	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-13T02:03:54.148118	12	54	2246
70	router placement network security tips	For optimal router placement and network security, several key factors must be considered. Regarding placement, your router needs a clear line of sight for maximum output, and obstacles can slow down performance. You may need WiFi extenders or multiple routers for split-level homes or unusual floorplans. For security, it's essential to implement several measures: change the default router name (SSID) and create a strong password using a mix of numbers, letters, and symbols. Always use WPA2 encryption as it's more secure than WEP or WPA. Additionally, enable and maintain firewalls, keep all connected devices' software updated, and consider setting up a separate guest network. Regular scanning of external devices like USBs is also recommended to prevent malware infections.	['When setting up your home WiFi there are a number of options to choose from, and the router, ISP, and devices should all seamlessly complement each other. The right combination of products and services could mean the difference between fast download speeds and an unreliable internet connection.\nIf you are ready to make the most of your WiFi, carefully consider the needs of your home:\nThe Number of People\nHow many people will have access to WiFi at the same time? For those with a large family or a small business, your WiFi needs to be able to support multiple devices and applications all at once. Start by researching ISPs to find a solution with high speeds.\nBe careful because advertised speeds may not always be actual speeds. Asking your friends and neighbors for a recommendation may be a more accurate way to determine speed and reliability in your neighborhood.\nThe Types of Online Activities\nIf you only use your WiFi to access emails and Facebook, speed and download quota may not be a concern. While you should still always avoid dial-up unless there is no other option, you may prefer to look at the price. When it comes to high bandwidth activities such as gaming, streaming, and downloading, a little more caution should be taken.\nOnce you have found the right ISP package, consider your router. It needs to be able to match the top speed from your ISP, otherwise, you will never be able to get what you are paying for.\nSometimes your ISP will give you the option to rent the hardware. While the ongoing cost can be off-putting, the ease of installation, compatibility, and support may be worth it if you are not tech-savvy. Similarly to the router, your devices will also come with maximum speed allowances and even with fast internet, you could experience lag with the wrong pairing.\nThe Size of the Area\nYour WiFi may not run efficiently if the size of the area it is in is too large, or if it has obstacles in the way. Your router needs a clear line of sight for maximum output and the wrong positioning could slow you down. When purchasing this type of product you should look at the router range, and look for the letters “AC” in the description. Experiment with the placement of your router and try a WiFi extender if the problem still persists.\nIf you have a split level home or an unusual floorplan, you may consider purchasing two or more routers.\nThe security of your WiFi network should be a high priority. If you’re wondering, “How do I secure my WiFi?” just know, there are a few ways to do this. The first is to ensure you have a strong password and WiFi encryption. You should check your network and make sure your encryption is WPA2. No encryption, or using WEP or WPA protocols are not ideal situations for keeping your household data safe.\nKeep any hardware and software updated, including your router, internet security, systems, and applications.\nWiFi For Any Purpose\nTo get the most from your WiFi it is important to consider what you need it for, the size of the space, and the number of people needing access. Choosing an ISP, router, and devices that complement each other with similar top speeds will help you get the most efficient WiFi possible.\nSecurity should never be neglected, and encryption, updates, and passwords should be an important part of the mix.', 'Securing Your Home Network\nA protected home network means your family can use the Internet safely and securely.\nMost households now run networks of devices linked to the Internet, including computers, laptops, gaming devices, TVs, tablets, and smartphones that access wireless networks. To protect your home network and your family, you need to have the right tools in place and confidence that family members can use the Internet safely and securely.\nThe first step is to Keep a Clean Machine and make sure all of your Internet-enabled devices have the latest operating system, web browsers and security software. This includes mobile devices that access your wireless network.\nSecure Your Wireless Router: A wireless network means connecting an Internet access point – such as a cable or DSL modem – to a wireless router. Going wireless is a convenient way to allow multiple devices to connect to the Internet from different areas of your home. However, unless you secure your router, you’re vulnerable to people accessing information on your computer, using your Internet service for free and potentially using your network to commit cybercrimes.\nHere are ways to secure your wireless router:\nChange the name of your router: The default ID – called a service set identifier” (SSID) or “extended service set identifier” (ESSID ) – is assigned by the manufacturer. Change your router to a name that is unique to you and won’t be easily guessed by others.\nChange the pre-set password on your router: When creating a new password, make sure it is long and strong, using a mix of numbers, letters and symbols. Here are some tips for creating a secure password the easy way.\nReview security options: When choosing your router’s level of security, opt for WPA2, if available, or WPA. They are more secure than the WEP option.\nCreate a guest password: Some routers allow for guests to use the network via a separate password. If you have many visitors to your home, it’s a good idea to set up a guest network.\nUse a firewall: Firewalls help keep hackers from using your computer to send out your personal information without your permission. While anti-virus software scans incoming email and files, a firewall is like a guard, watching for attempts to access your system and blocking communications with sources you don’t permit. Your operating system and/or security software likely comes with a pre-installed firewall, but make sure you turn on these features.\nProtect Yourself with these STOP. THINK. CONNECT. Tips:\nKeep a clean machine: Having the latest security software, web browser, and operating system are the best defenses against viruses, malware, and other online threats.\nAutomate software updates: Many software programs will automatically connect and update to defend against known risks. Turn on automatic updates if that’s an available option.\nProtect all devices that connect to the Internet: Along with computers, smart phones, gaming systems, and other web-enabled devices also need protection from viruses and malware.\nPlug & scan: “USBs” and other external devices can be infected by viruses and malware. Use your security software to scan them.\nProtect your $$: When banking and shopping, check to be sure the sites is security enabled. Look for web addresses with “https://” or “shttp://”, which means the site takes extra measures to help secure your information. “Http://” is not secure. Want more security? Have the ability to turn your debit card off and turn it back on from your smart phone with CardValet, giving you an added layer of security against debit card fraud with user controls.\nBack it up: Protect your valuable work, music, photos, and other digital information by making an electronic copy and storing it safely.\nOctober is Cyber Security Awareness month. Take the time to learn more at staysafeonline.org']	['<urn:uuid:61ab1430-15d6-4e39-b742-b69baf5455db>', '<urn:uuid:3e90e3fe-c068-4721-b037-21733344b810>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-13T02:03:54.148118	5	118	1208
71	liver biopsy for fibrosis detection how invasive is it and what alternative methods exist	Liver biopsy remains the gold standard for diagnosing liver fibrosis but is an invasive procedure. As an alternative, non-invasive methods include ultrasound elastography which measures liver stiffness and can detect advanced fibrosis, potentially eliminating the need for biopsy. Research is also exploring the identification of specific biomarkers combined with non-invasive optical imaging as another alternative diagnostic approach.	"['Chronic liver diseases cover a spectrum of pathologies headed by a process that involves a progressive destruction and regeneration of liver functional parts leading to fibrosis and then cirrhosis. Oral dosage forms can help to increase patient compliance significantly improving the course of such chronic disorders and lipid-based oral medications have been shown to be suitable candidates. The aim of this project is to develop and characterize novel oral lipid-based formulations for chronic liver pathologies by means of validated and optimized in vitro models. In order to select the most efficient excipients for novel phospholipid-based antifibrotic therapies, the role of single lipids is investigated by the screening of lipid components and thorough quantitative and qualitative analyses of their effect are performed at a molecular and cellular level. For more information: Pharmaceutics (2019) 11:676\nThe possibility of an early diagnosis to detect the evolution of liver disease to liver fibrosis remains a Holy Grail in hepatology. The lack of accurate, reproducible, and easily applicable methods for the assessment of hepatic fibrosis has been the major limitation for both the clinical management and research in liver diseases. Liver biopsy remains the gold standard to reach the certainty of diagnosis. The nature of the procedure, however, creates a quest for a less invasive diagnostic modality through which the evolution of the disease and the effectiveness of the anti-fibrotic therapies could be followed. The identification of biomarkers specific for enzymes responsible for the early alterations of liver microstructure, combined with a non-invasive optical imaging modality, could guide the clinicians towards a timely therapeutic strategy. The biomarkers, indeed, could be also used as a tool to achieve a highly specific treatment in the fibrosis-affected areas in the liver. Liposomes represent an optimal platform to combine diagnostic accuracy and therapeutic efficiency. In this research project emphasis is placed on the development and physicochemical characterization as well as on the use of in vitro cell culture to test targeted liposomes designed to improve management strategies of this chronic fibro-proliferative disease. For more information: J Mater. Chem. B (2021) doi: 10.1039/D0TB02372H\nLiver fibrosis is the wound-healing response to chronic hepatic insults, often leading to the loss of organ function. It is characterized by the excessive deposition of scar tissue, a process driven by activated hepatic stellate cells (HSC). The broader aim of our research is to assess the impact of lipid-based therapeutics on extracellular vesicles isolated from hepatic cell lines, used as an in vitro model for the progression of liver fibrosis.\nThis project is financially supported by the Phospholipid Research Center. For more information:\nThe topic of this project is the development of a novel phospholipid-based depot formulation for sustained release of drugs. The main principle of the depot building is the aggregation of liposomes encapsulating a model drug. Besides the screening of appropriate formulations and the physico-chemical characterisation of the aggregates, the emphasis is the investigation of different encapsulation methods and following release studies. Another focus is the development of a novel manufacturing process and application system for this depot formulation compared to existing market products.\nThis project is financially supported by the Phospholipid Research Center. For more information: Colloids Surf B Biointerfaces (2018) 168:10-17; Pharmaceutics (2020) 12, 567; Eur J Pharm Biopharm (2022) 181:300-309.', ""Alcoholic Liver Disease\nAlcohol consumption is high in most Western countries. According to the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5), 8.5% of US adults are estimated to have alcohol use disorder in any 12-month period (see Alcohol Use Disorders and Rehabilitation). The male:female ratio is about 2:1. Disorders that occur in alcohol abusers, often in sequence, include\n(See also the 2018 American College of Gastroenterology’s practice guidelines for Alcoholic Liver Disease.)\nHepatocellular carcinoma may also develop in patients with cirrhosis, especially if iron accumulation coexists.\nThe main risk factors for alcoholic liver disease are\nAmong susceptible people, a linear correlation generally exists between the amount and duration of alcohol use and the development of liver disease.\nAlcohol content is estimated to be the beverage volume (in mL) multiplied by its percentage of alcohol. For example, the alcohol content of 45 mL of an 80-proof (40% alcohol) beverage is 18 mL by volume. Each mL contains about 0.79 g of alcohol. Although values can vary, the percentage of alcohol averages 2 to 7% for most beers and 10 to 15% for most wines. Thus, a 12-ounce (oz) glass of beer contains between about 5 to 20 g of alcohol, and a 5-oz glass of wine contains between about 12 to 18 g, and a 1 1/2-oz shot of hard liquor contains about 14 g.\nRisk of liver disease increases markedly for men who drink > 40 g, particularly > 80 g, of alcohol/day (eg, about 2 to 8 cans of beer, 3 to 6 shots of hard liquor, or 3 to 6 glasses of wine) for > 10 years. For cirrhosis to develop, consumption must usually be > 80 g/day for > 10 years. If consumption exceeds 230 g/day for 20 years, risk of cirrhosis is about 50%. But only some chronic alcohol abusers develop liver disease. Thus, variations in alcohol intake do not fully explain variations in susceptibility, indicating that other factors are involved.\nWomen are more susceptible to alcoholic liver disease, even after adjustment for body size. Women require only 20 to 40 g of alcohol to be at risk—half of that for men. Risk in women may be increased because they have less alcohol dehydrogenase in their gastric mucosa; thus, more intact alcohol reaches the liver.\nOther risk factors include iron accumulation in the liver (not necessarily related to iron intake) and concomitant hepatitis C.\nAlcohol (ethanol) is readily absorbed from the stomach, but most is absorbed from the small intestine. Alcohol cannot be stored. A small amount is degraded in transit through the gastric mucosa, but most is catabolized in the liver, primarily by alcohol dehydrogenase (ADH) but also by cytochrome P-450 2E1 (CYP2E1) and the microsomal enzyme oxidation system (MEOS).\nMetabolism via the ADH pathway involves the following:\nADH, a cytoplasmic enzyme, oxidizes alcohol into acetaldehyde. Genetic polymorphisms in ADH account for some individual differences in blood alcohol levels after the same alcohol intake but not in susceptibility to alcoholic liver disease.\nAcetaldehyde dehydrogenase (ALDH), a mitochondrial enzyme, then oxidizes acetaldehyde into acetate. Chronic alcohol consumption enhances acetate formation. Asians, who have lower levels of ALDH, are more susceptible to toxic acetaldehyde effects (eg, flushing); the effects are similar to those of disulfiram, which inhibits ALDH.\nThese oxidative reactions generate hydrogen, which converts nicotinamide-adenine dinucleotide (NAD) to its reduced form (NADH), increasing the redox potential (NADH/NAD) in the liver.\nThe increased redox potential inhibits fatty acid oxidation and gluconeogenesis, promoting fat accumulation in the liver.\nChronic alcoholism induces the MEOS (mainly in endoplasmic reticulum), increasing its activity. The main enzyme involved is CYP2E1. When induced, the MEOS pathway can account for 20% of alcohol metabolism. This pathway generates harmful reactive oxygen species, increasing oxidative stress and formation of oxygen-free radicals.\nFat (triglycerides) accumulates throughout the hepatocytes for the following reasons:\nExport of fat from the liver is decreased because hepatic fatty acid oxidation and lipoprotein production decrease.\nInput of fat is increased because the decrease in hepatic fat export increases peripheral lipolysis and triglyceride synthesis, resulting in hyperlipidemia.\nHepatic fat accumulation may predispose to subsequent oxidative damage.\nOxidative stress is increased by\nLiver hypermetabolism, caused by alcohol consumption\nFree radical–induced lipid peroxidative damage\nReduction in protective antioxidants (eg, glutathione, vitamins A and E), caused by alcohol-related undernutrition\nBinding of alcohol oxidation products, such as acetaldehyde, to liver cell proteins, forming neoantigens and resulting in inflammation\nAccumulation of neutrophils and other white blood cells (WBCs), which are attracted by lipid peroxidative damage and neoantigens\nInflammatory cytokines secreted by WBCs\nAccumulation of hepatic iron, if present, aggravates oxidative damage. Iron can accumulate in alcoholic liver disease through ingestion of iron-containing fortified wines; most often, the iron accumulation is modest. This condition must be differentiated from hereditary hemochromatosis.\nA vicious circle of worsening inflammation occurs: Cell necrosis and apoptosis result in hepatocyte loss, and subsequent attempts at regeneration result in fibrosis. Stellate (Ito) cells, which line blood channels (sinusoids) in the liver, proliferate and transform into myofibroblasts, producing an excess of type I collagen and extracellular matrix. As a result, the sinusoids narrow, limiting blood flow. Fibrosis narrows the terminal hepatic venules, compromising hepatic perfusion and thus contributing to portal hypertension. Extensive fibrosis is associated with an attempt at regeneration, resulting in liver nodules. This process culminates in cirrhosis.\nHepatic steatosis, alcoholic hepatitis, and cirrhosis are often considered separate, progressive manifestations of alcoholic liver disease. However, their features often overlap.\nHepatic steatosis (fatty liver) is the initial and most common consequence of excessive alcohol consumption. Hepatic steatosis is potentially reversible. Macrovesicular fat accumulates as large droplets of triglyceride and displaces the hepatocyte nucleus, most markedly in perivenular hepatocytes. The liver enlarges.\nAlcoholic hepatitis (steatohepatitis) is a combination of hepatic steatosis, diffuse liver inflammation, and liver necrosis (often focal)—all in various degrees of severity. The damaged hepatocytes are swollen with a granular cytoplasm (balloon degeneration) or contain fibrillar protein in the cytoplasm (Mallory or alcoholic hyaline bodies). Severely damaged hepatocytes become necrotic. Sinusoids and terminal hepatic venules are narrowed. Cirrhosis may also be present.\nAlcoholic cirrhosis is advanced liver disease characterized by extensive fibrosis that disrupts the normal liver architecture. The amount of fat present varies. Alcoholic hepatitis may coexist. The feeble compensatory attempt at hepatic regeneration produces relatively small nodules (micronodular cirrhosis). As a result, the liver usually shrinks. In time, even with abstinence, fibrosis forms broad bands, separating liver tissue into large nodules (macronodular cirrhosis—see Cirrhosis: Pathophysiology).\nSymptoms usually become apparent in patients during their 30s or 40s; severe problems appear about a decade later.\nHepatic steatosis is often asymptomatic. In one third of patients, the liver is enlarged and smooth, but it is not usually tender.\nAlcoholic hepatitis ranges from mild and reversible to life threatening. Most patients with moderate disease are undernourished and present with fatigue, fever, jaundice, right upper quadrant pain, tender hepatomegaly, and sometimes a hepatic bruit. About 40% deteriorate soon after hospitalization, with consequences ranging from mild (eg, increasing jaundice) to severe (eg, ascites, portosystemic encephalopathy, variceal bleeding, liver failure with hypoglycemia, coagulopathy). Other manifestations of cirrhosis may be present.\nCirrhosis, if compensated, may be asymptomatic. The liver is usually small; when the liver is enlarged, hepatic steatosis or hepatoma should be considered. Symptoms range from those of alcoholic hepatitis to the complications of end-stage liver disease, such as portal hypertension (often with esophageal varices and upper gastrointestinal bleeding, splenomegaly, ascites, and portosystemic encephalopathy). Portal hypertension may lead to intrapulmonary arteriovenous shunting with hypoxemia (hepatopulmonary syndrome), which may cause cyanosis and nail clubbing. Acute renal failure secondary to progressively decreasing renal blood flow (hepatorenal syndrome) may develop. Hepatocellular carcinoma develops in 10 to 15% of patients with alcoholic cirrhosis.\nChronic alcoholism, rather than liver disease, causes Dupuytren contracture of the palmar fascia, vascular spiders, myopathy, and peripheral neuropathy. In men, chronic alcoholism causes signs of hypogonadism and feminization (eg, smooth skin, lack of male-pattern baldness, gynecomastia, testicular atrophy, changes in pubic hair). Undernutrition may lead to multiple vitamin deficiencies (eg, of folate and thiamin), enlarged parotid glands, and white nails. In alcoholics, Wernicke encephalopathy and Korsakoff psychosis result mainly from thiamin deficiency. Pancreatitis is common. Hepatitis C occurs in > 25% of alcoholics; this combination markedly worsens the progression of liver disease.\nRarely, patients with hepatic steatosis or cirrhosis present with Zieve syndrome (hyperlipidemia, hemolytic anemia, and jaundice).\nAlcohol is suspected as the cause of liver disease in any patient who chronically consumes excess alcohol, particularly >80 g/day. When the patient's alcohol consumption is in doubt, history should be confirmed by family members. Patients can be screened for alcoholism using the CAGE questionnaire (need to Cut down, Annoyed by criticism, Guilty about drinking, and need for a morning Eye-opener). There is no specific test for alcoholic liver disease, but if the diagnosis is suspected, liver function tests (PT; serum bilirubin, aminotransferase, and albumin levels) and CBC are done to detect signs of liver injury and anemia.\nElevations of aminotransferases are moderate (< 300 IU/L) and do not reflect the extent of liver damage. The ratio of aspartate aminotransferase (AST) to alanine aminotransferase (ALT) is ≥ 2. The basis for low ALT is a dietary deficiency of pyridoxal phosphate (vitamin B6), which is needed for ALT to function. Its effect on AST is less pronounced. Serum gamma-glutamyl transpeptidase (GGT) increases, more because ethanol induces this enzyme than because patients have cholestasis or liver injury or use other drugs. Serum albumin may be low, usually reflecting undernutrition but occasionally reflecting otherwise obvious liver failure with deficient synthesis. Macrocytosis with an mean corpuscular volume > 100 fL reflects the direct effect of alcohol on bone marrow as well as macrocytic anemia resulting from folate deficiency, which is common among undernourished alcoholics. Indexes of the severity of liver disease are\nThrombocytopenia can result from the direct toxic effects of alcohol on bone marrow or from splenomegaly, which accompanies portal hypertension. Neutrophilic leukocytosis may result from alcoholic hepatitis, although coexisting infection (particularly pneumonia and spontaneous bacterial peritonitis) should also be suspected.\nImaging tests of the liver are not routinely needed for diagnosis. If done for other reasons, abdominal ultrasonography or CT may suggest hepatic steatosis or show splenomegaly, evidence of portal hypertension, or ascites. Ultrasound elastrography measures liver stiffness and thus detects advanced fibrosis. This valuable adjunct can obviate the need for liver biopsy to check for cirrhosis and help assess prognosis. Its exact role is under study.\nIf abnormalities suggest alcoholic liver disease, screening tests for other treatable forms of liver disease, especially viral hepatitis, should be done.\nBecause features of hepatic steatosis, alcoholic hepatitis, and cirrhosis overlap, describing the precise findings is more useful than assigning patients to a specific category, which can only be determined by liver biopsy.\nNot all experts agree on the indications for liver biopsy. Proposed indications include the following:\nLiver biopsy confirms liver disease, helps identify excessive alcohol use as the likely cause, and establishes the stage of liver injury. If iron accumulation is observed, measurement of the iron content and genetic testing can eliminate hereditary hemochromatosis as the cause.\nFor stable patients with cirrhosis, the American Association for the Study of Liver Diseases (AASLD) recommends that liver ultrasonography, with or without alpha-fetoprotein (AFP) measurement, should be done every 6 months to screen for hepatocellular carcinoma. They also suggest that surveillance not be done for patients with Child’s class C cirrhosis unless they are on the transplant waiting list because of the low anticipated survival for these patients (1).\nPrognosis is determined by the degree of hepatic fibrosis and inflammation. Hepatic steatosis and alcoholic hepatitis without fibrosis are reversible if alcohol is avoided. With abstinence, hepatic steatosis may completely resolve within 6 weeks. Fibrosis and cirrhosis are usually irreversible.\nCertain biopsy findings (eg, neutrophils, perivenular fibrosis) indicate a worse prognosis. Proposed quantitative indexes to predict severity and mortality use primarily laboratory features of liver failure such as prothrombin time, creatinine (for hepatorenal syndrome), and bilirubin levels. The Maddrey discriminant function may be used; it is calculated from the following formula:\nFor this formula, bilirubin level is measured in mg/dL (converted from bilirubin in micromol/L by dividing by 17). A value of > 32 is associated with a high short-term mortality rate (eg, after 1 month, 35% without encephalopathy and 45% with encephalopathy). Other indexes include the Model for End-Stage Liver Disease (MELD) score, Glasgow alcoholic hepatitis score, and Lille model. For patients ≥ 12 years of age, the MELD score is calculated using the following formula:\nOnce cirrhosis and its complications (eg, ascites, bleeding) develop, the 5-year survival rate is about 50%; survival is higher in patients who abstain and lower in patients who continue drinking.\nCoexisting iron accumulation or chronic hepatitis C increases risk of hepatocellular carcinoma.\nAbstinence is the mainstay of treatment; it prevents further damage from alcoholic liver disease and thus prolongs life. Because compliance is problematic, a compassionate team approach is essential. Behavioral and psychosocial interventions can help motivated patients; they include rehabilitation programs and support groups (see Alcohol Use Disorders and Rehabilitation: Maintenance), brief interventions by primary care physicians, and therapies that explore and clarify the motivation to abstain (motivational enhancement therapy).\nDrugs, if used, should only supplement other interventions. Opioid antagonists (naltrexone or nalmefene) and drugs that modulate gamma-aminobutyric acid receptors (baclofen or acamprosate) appear to have a short-term benefit by reducing the craving and withdrawal symptoms. Disulfiram inhibits aldehyde dehydrogenase, allowing acetaldehyde to accumulate; thus, drinking alcohol within 12 hours of taking disulfiram causes flushing and has other unpleasant effects. However, disulfiram has not been shown to promote abstinence and consequently is recommended only for certain patients.\nGeneral management emphasizes supportive care. A nutritious diet and vitamin supplements (especially B vitamins) are important during the first few days of abstinence. Alcohol withdrawal requires use of benzodiazepines (eg, diazepam). In patients with advanced alcoholic liver disease, excessive sedation can precipitate portosystemic encephalopathy and thus must be avoided.\nSevere acute alcoholic hepatitis commonly requires hospitalization, often in an intensive care unit, to facilitate enteral feeding (which can help manage nutritional deficiencies) and to manage specific complications (eg, infection, bleeding from esophageal varices, specific nutritional deficiencies, Wernicke encephalopathy, Korsakoff psychosis, electrolyte abnormalities, portal hypertension, ascites, portosystemic encephalopathy—see elsewhere in THE MANUAL ).\nCorticosteroids (eg, prednisolone 40 mg/day orally for 4 weeks, followed by tapered doses) improve outcome in patients who have severe acute alcoholic hepatitis (Maddrey discriminant function ≥ 32) and who do not have infection, gastrointestinal bleeding, renal failure, or pancreatitis (1).\nOther than corticosteroids and enteral feeding, few specific treatments are clearly established. Antioxidants (eg, S-adenosyl-l-methionine, phosphatidylcholine, metadoxine) show promise in ameliorating liver injury during early cirrhosis but require further study. Therapies directed at cytokines, particularly tumor necrosis factor (TNF)-alpha, and aiming to reduce inflammation have had mixed results in small trials. Pentoxifylline, a phosphodiesterase inhibitor that inhibits TNF-alpha synthesis, had mixed results in clinical trials in patients with severe alcoholic hepatitis. When biologic agents that inhibit TNF-alpha (eg, infliximab, etanercept) are used, risk of infection outweighs benefit. Drugs given to decrease fibrosis (eg, colchicine, penicillamine) and drugs given to normalize the hypermetabolic state of the alcoholic liver (eg, propylthiouracil) have no proven benefit. Antioxidant remedies, such as silymarin (milk thistle) and vitamins A and E, are ineffective.\nLiver transplantation can be considered if disease is severe. With transplantation, 5-year survival rates are comparable to those for nonalcoholic liver disease—as high as 80% in patients without active liver disease and 50% in those with acute alcoholic hepatitis. Because up to 50% of patients resume drinking after transplantation, most programs require 6 months of abstinence before transplantation is done; recent data suggest that earlier transplantation may offer a survival advantage, but currently, this approach is not standard of care.\nRambaldi A, Saconato HH, Christensen E, et al: Systematic review: Glucocorticosteroids for alcoholic hepatitis—A Cochrane Hepato-Biliary Group systematic review with meta-analyses and trial sequential analyses of randomized clinical trials. Aliment Pharmacol Ther 27(12):1167-1178, 2008. doi: 10.1111/j.1365-2036.2008.03685.x.\nRisk of alcoholic liver disease increases markedly in men if they ingest > 40 g, particularly > 80 g, of alcohol/day (eg, about 2 to 8 cans of beer, about 3 to 6 glasses of wine, or 3 to 6 shots of hard liquor) for > 10 years; risk increases markedly in women if they ingest about half that amount.\nScreen patients using the CAGE questionnaire, and when in doubt about the patient's alcohol consumption, consider asking family members.\nTo estimate prognosis, consider unfavorable histologic findings (eg, neutrophils, perivenular fibrosis) and use of a formula (eg, Maddrey discriminant function, Model for End-Stage Liver Disease [MELD] score).\nEmphasize abstinence, provide supportive care, and hospitalize and give corticosteroids to patients with severe acute alcoholic hepatitis.\nConsider transplantation for abstinent patients.\nDrugs Mentioned In This Article\n|Drug Name||Select Trade|\n|No US brand name|""]"	['<urn:uuid:52ce2b31-196a-4222-a887-a628df993b13>', '<urn:uuid:e2b474a2-b962-48c0-896b-5f03fcff9466>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-13T02:03:54.148118	14	57	3332
72	How fast is lab water testing and what makes it pure?	Ion-exchange chromatography can analyze water samples for multiple ions (F–, Cl–, Br–, NO2–, NO3–, PO43–, and SO42–) in just 15 minutes, compared to 1-2 days using other methods. The water purification process involves passing water through two containers - one with cation exchange resin that exchanges H+ ions with unwanted positive ions, and another with anion exchange resin that exchanges OH- ions with negative ions. These H+ and OH- ions then combine to form pure water molecules, effectively removing all dissolved impurities.	"['In ion-exchange chromatography (IEC) the stationary phase is a cross-linked polymer resin, usually divinylbenzene cross-linked polystyrene, with covalently attached ionic functional groups. As shown in Figure 1 for a styrene–divinylbenzene co-polymer modified for use as an ion-exchange resin, the ion-exchange sites—indicated here by R and shown in blue—are mostly in the para position and are not necessarily bound to all styrene units. The cross-linking is shown in red. The counterions to these fixed charges are mobile and can be displaced by ions that compete more favorably for the exchange sites.\nFigure 1: (left) styrene–divinylbenzene co-polymer modified for use as an ion-exchange resin. (right) The photo here shows an example of ion-exchange polymer beads. These beads are approximately 0.30–0.85 mm in diameter. Resins for use in ion-exchange chromatography are smaller, typically 5–11 mm in diameter.\nImagine if we had a tube whose surfaces were coated with an immobilized cation. These would have electrostatic attraction for anions. If a solution containing a mixture of positively and negatively charged groups flows through this tube, the anions would preferentially bind, and the cations in the solution would flow through (Figure 2).\nFigure 2: Separating action of amino acids in an anion exchange column.\n- This is the basis of ion exchange chromatography. The example above is termed ""anion exchange"" because the inert surface is interacting with anions\n- If the immobile surface was coated with anions, then the chromatography would be termed ""cation exchange"" chromatography (and cations would selectively bind and be removed from the solution flowing through\n- Strength of binding can be affected by pH, and salt concentration of the buffer. The ionic species ""stuck"" to the column can be removed (i.e. ""eluted"") and collected by changing one of these conditions. For example, we could lower the pH of the buffer and protonate anions. This would eliminate their electrostatic attraction to the immobilized cation surface. Or, we could increase the salt concentration of the buffer, the anions in the salt would ""compete off"" bound anions on the cation surface.\nA common detector used exclusively in liquid chromatography for ion chromatography is the conductivity detector. In this detector two closely spaced electrodes are placed after the separation column and changes in the conductivity of the solution passing between the two electrodes is monitored. In order to improve the sensitivity of this simple detector and ion suppressor is typically placed between the separation column and the conductivity detector. Pictures in Figure 3 is a schematic of an ion suppressor used , in this example for improving the detectability of a series of anions, X-.\nFigure 3. An ion suppressor used in ion chromatography. Image source currently unknown\nAs evident in Figure 3 an ion suppressor is based on simple acid base neutralization chemistry, i.e. H+ + OH- → H2O or 2H+ + CO32- → H2CO3. In the example in Figure 3, hydronium ions generated at the anode of an electroylsis cell pass through the ion exchange membrane where sodium ions exit due to charge balance and the hydronium ions neutralize the hydroxide ions in the mobile phase, thus reducing the net ion concentration and conductivity of the solution passing to the conductivity detector.\nAn example of the effect of an ion suppressor is illustrated in Figure 4 for a separation of a series of anions.\nFigure 5: An example of the expected improvement in detectability in the separation of a series of anions with an ion suppressor. Image source currently unknown.\nIon-exchange chromatography is an important technique for the analysis of anions and cations in environmental samples. For example, an IEC analysis for the anions F–, Cl–, Br–, NO2–, NO3–, PO43–, and SO42– takes approximately 15 minutes. A complete analysis of the same set of anions by a combination of potentiometry and spectrophotometry requires 1–2 days.', 'Q. What is softening of water? Describe deionization method of water softening with diagram.\nAns. Softening of water: The process of removing the hardness causing salts from water is called softening of water. Hard water is unsuitable for industries as well as for domestic purposes. Therefore, it is necessary to remove the hardness of water before using in various industries, boilers etc. The softening of hard water can be done by:\nRemoval of hardness causing salts of calcium and magnesium.\nConversion of these salts into soluble sodium salts.\nCombination of the above methods. The hardness causing salts can be removed mainly by three methods.\n(a) Lime-soda method\n(b) Zeolite method and\n(c) Ion-exchange methods.\nIon Exchange process (Determination or Deionization):\nThe substances capable of exchange of ions with the electrolyte solution are called ion exchangers. The process of ion exchange is the property of a ion exchangers to exchange an ion in its structure with ion in solution that is brought in contact with it. Since all the cations and anions are completely removed in this process, so it is known as ""complete deionization"". Ion exchange are organic cross, linked, insoluble, high molecular weight, polymers with a porous structure, and the ""functional structure"" attached to the chains are responsible for the ion-exchange properties. The ion exchange or ion-exchanger resins may be classified as:\nCations exchangers: Resins containing acidic functional groups like (-COOH, -SO3H etc.) are capable of exchanging their H+ ions with other cations which comes in their contact; and hence they are termed as cations exchangers e.g. styrene divine benzene copolymers which on sulphonation.\nThey can be represented as R-H+ where R- represents the insoluble polymer matrix. The exchange reactions are:\n2R- - H+ + Mg+2 = R2Mg + 2H+\n2R- - H+ + Ca+2 = R2Ca + 2H+\nAmberlite IR-120; and Do vex -50 are commercially available cations exchange resins.\nAnion exchangers: Resin containing basic functional group like ammonia or quaternary ammonium or quaternary phosphonium or hydroxides or hydrochloride are capable of exchanging their anions in water and therefore are known as anion exchangers e.g. styrene benzene or ammine formaldehyde copolymers. These after treatment with dilute 2NaOH solution, become capable to exchange their OH- anions with other anions present in water. They can be represented as R+ OH- where R+\n2R+ + OH- + SO4-2 = R2SO4 + 2OH-\nR+OH- + Cl- = RCl + OH-\nProcess: The apparatus used for softening hard water by this method consists of two containers, one containing cations exchange resin and the other anions exchange resin over a bed of gravel. Hard water is first passed through the cations exchange resin. The cations present in hard water get exchanged with H+ ions of the resin. The water coming out of the first container thus contains free yyhjions in acidic in nature. It is now passed through the anion exchange resin. These resin exchangers the anions with OH- ions.\nR+ - OH- + Cl- = RCl + OH-\nThe OH- ions thus formed combine with free H+ ions to form H2O molecules. all cations and anions that were present in hard water. It is known as deionised or demineralised water.']"	['<urn:uuid:74b8f632-75e4-4ce6-aec1-6fa65755de76>', '<urn:uuid:1927f62a-a6ee-46e3-9c17-8a94d482331e>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T02:03:54.148118	11	82	1159
73	what type baskets dat so la lee known for	Dat-So-La-Lee is probably best known for her degikup (or 'day-gee-coop') baskets, which begin with a small circular base, extend up and out to a maximum circumference, then become smaller until the opening at the top is roughly the same diameter as the base.	['The information below has been compiled from a variety of sources. If the reader has access to information that can be documented and that will correct or add to this woman’s biographical information, please contact the Nevada Women’s History Project.\nAt A Glance:\nBorn: About 1829\nDied: December, 1925\nMaiden Name: Dabuda\nRace/Nationality/Ethnic Background: Native American (Washoe Tribe)\nMarried: Assu, Charlie Keyser\nChildren: Two; died in childhood\nPrimary City and County of Residence and Work:\nCarson City (Ormsby County)\nMajor Fields of Work: Basket Weaver\nOther Role Identities: Wife, Mother, Household servant\nDat-So-La-Lee was a Washo (or Washoe) Indian woman who was born near the place that became the mining town of Sheridan in Carson Valley . She was also known by her given name Dabuda. Her birth date is believed to have been 1829. Her father’s name was DA DA uongala and her mother’s name is unknown. Dat-So-La-Lee lived in and around Carson City , Carson Valley , and Lake Tahoe\nSometime around 1899 Dabuda became known as Dat-So-La-Lee. This nickname suited her nicely. It was musical like her weaving.\nRecords provided by Dr. S.L. Lee indicate she was first married into the family of “Lame Tom”, who was called Assu and possibly died of consumption. No children from Dabuda’s marriages apparently survived to adulthood.\nIn her earlier years, Dabuda washed clothes and cooked for the miners and their wives. In 1871, she went to the mining town of Monitor in Alpine County, California, and worked for the Harris Cohn family. She worked as a servant.\nIn 1888 Dat-So-La-Lee married Charlie Keyser, who was part Washo and took his name from the family which owned the Keyser and Elrod Ranch in Nevada ‘s Carson Valley . At this time she took the name Louisa Keyser. Charlie was twenty-four years younger and an expert arrow craftsman.\nLouisa came to Abe Cohn’s attention in 1895 when he bought four willow-covered bottles she had made. He later became her sponsor, business manager, and press agent. In 1899 her baskets were being carefully recorded, in a ledger separate from the family’s business ledger, by Amy and Abe Cohn who recognized how skillfully they were made.\nDat-So-La-Lee and Charlie led a comfortable life with Abe and Amy Cohn. From 1895 until Charlie’s death in 1928, all of their expenses were taken care of by the Cohns. They traveled to Lake Tahoe every summer where Cohn had provided another home for them near Tahoe Tavern and Louisa (Dat-So-La-Lee) traveled extensively with the Cohns to arts and crafts exhibits. In return for their providing room and board, the Cohns received Dat-So-La-Lee’s baskets. For pleasure she liked the games the Indians played with wood or bone dice hidden in the hands or under baskets and the new games of chance the white men brought to Nevada . Sometimes she played late into the night.\nDat-So-La-Lee is probably best known for her degikup or “day-gee-coop” baskets. This type begins with a small, circular base, extends up and out to a maximum circumference, then becomes smaller until the opening at the top is roughly the same diameter as the base. She wove baskets for Cohn’s Emporium for approximately thirty years until her death in 1925. It is now generally accepted that some of Louisa’s designs were inspired by other weavers, probably Pomo and Miwok Indians. Most of her designs were her own. She used symbols like words to tell a story.\nDat-So-La-Lee lived during a time that saw an enormous amount of change for her people. She used her hand print, which was copyrighted, to certify bills of sale. The receipts included the hand print, a description of the basket, stitches to the inch, design, and time involved in its construction – a lovely gesture devised by Abe and Amy Cohn.\nDat-So-La-Lee was a member of the southern Washo group associated with Carson Valley and Alpine County. Her native people of the Great Basin , the traditional Washoe homeland, have been making baskets for several thousand years. The “Hokan” speaking Washo people apparently entered the Great Basin of the American West via a California route perhaps as many as 4,000 years ago. Though the Washo inhabited areas of eastern California , the tribe is more commonly associated with western Nevada . According to Jane Green Hickson,\n“Before the white men came, the Washoe camped by the shores of Lake Tahoe and Washoe Lake, on the banks of the Truckee, Carson, and Walker Rivers, and near springs in the Pine Nut Hills.\nFor food, they hunted rabbits, antelope, and mud hen, fished the lakes and streams, brought back fly larvae from Mono Lake , hiked to the western slope of the Sierras for acorns, collected seeds from the grasses, and gathered pine nuts.\nThe men did the hunting and fishing and made arrows, tools, and blankets; while the women gathered and prepared the plant and insect foods, tended the children, and made baskets.”\nEsther Summerfield, writing for the Nevada State Historical Society, says, in part:\n“Myriads of stars shine over the graves of our ancestors. Dat-So-La-Lee had seen some 96 winters, mostly in the Carson Valley , when death came. Last of the famed Washo basket-weavers, her unexpressed dreams and her love of beauty were woven into her masterpieces. Her baskets were unsurpassed for their artistic conception and symbolical significance. She gathered all known materials, with the aid of her husband. This work was tedious and required careful attention. Her materials were cured, seasoned and tied up ahead for the next year’s work. She was among the last of those Washo weavers whose ancient art had been practiced by countless generations.\nHer memories and her visions are beautifully woven into her baskets and will live on to remind us of the history and unique tribal artistry of her people, the Washo Indians.”\nDat-So-La-Lee died in December 1925 and was buried at the Stewart Indian cemetery in Carson City.\nResearched and written by Marcia Cuccaro. Posted to web site January 2016.\n- 1829 – Dabuda born to Washo people about this time.\n- 1840s – Dabuda married Assu.\n- 1844 – Dabuda (16) and nephew see white faces.\n- 1848 – 1852 Dabuda and Assu welcome two children during these years.\n- 1858 – 1860 Dabuda’s and Assu’s children died about this time.\n- 1850 – 1900 Traditional 10,000 year old Washo way of life destroyed –\nDabuda (31 to 71) jerked from Prehistoric Age to Frontier America.\n- 1871 – Dabuda (42) worked as servant for Harris Cohn.\n- 1870s – Assu died of consumption or exposure during this time.\n- 1877 – Dabuda (48) worked at Elrod – Keyser Ranch.\n- 1888 – Dabuda (59) married Charley Keyser, took Louisa as English name.\n- 1895 – Louisa promised all of her baskets to Abe Cohn for a house, food and clothes.\n- 1899 – Louisa given nickname “Dat-So-La-Lee”.\n- 1899 – 1914 Considered Dat-So-La-Lee’s (70-85) classical phase of weaving.\n- 1919 – Dat-So-La-Lee (90) traveled to St. Louis , Missouri by train with Abe Cohn, his wife, and an Indian girl to the Arts and Crafts Exhibition.\n- 1925 – Dat-So-La-Lee died (96); buried at Stewart Indian Cemetery near Carson City, Nevada.\nSources of Information:\n- Burton , Henrietta K., “A Study of the Methods Used To Conserve the Art of the Washoe Indian Basketry.” Unpublished manuscript. May, 1932.\n- Chase, Don M.; Purdy, Carl; McNaughton, Clara. Basket-Maker Artists . Sebastopol , California , 1977.\n- Cohn, Abraham. Papers. Nevada Historical Society, Reno.\n- Cohodas, Marvin. “The Breitholle Collection of Washoe Basketry.” American Indian Art Magazine. 9. Autumn, 1984.\n- Codohas, Martin. “Dat-So-La-Lee and the Degikup.” Halcyon , A Journal of the Humanities. 1982.\n- Codohas, Marvin. “Washoe Basketry.” American Indian Basketry Magazine. July 2, 1983.\n- Codohas, Marvin. Washoe Fancy Basketry 1895-1935. The Fine Arts Gallery of the University of British Columbia . 1979.\n- Hickson, Jane Green. Dat-So-La-Lee, Queen of the Washoe Basketmakers. Popular Series #3. Nevada State Museum, December 1967.\n- Kern, Norval C., Jr. “A Presentation of Sculpture: A Synthesis of a Design Alphabet Derived From The Art Forms Of A Primitive People,” Ed. D. Diss. New York University , 1969.\n- Mack, Effie Mona. “Dat-So-La-Lee, World-Renowned Washoe Basket Weaver.” Parts 1, 2. Nevada Magazine. February-March 1946.\n- McNaughton, Clara . “ Native Indian Basketry .” The New West. October, 1912.\n- Obituary. Carson City Daily Appeal. January 29, 1934.\n- “Scrapbook.” Number 194-G-22. Nevada State Museum . Carson City, Nevada.\n- Westergard, Dixie . Dat-So-La-Lee: Washo Indian Basketmaker. Reno, Nevada 1999.']	['<urn:uuid:179e5051-e9a4-45de-becf-8343dc0b65ae>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T02:03:54.148118	9	43	1427
74	I'm interested in magnetic materials and their applications. Can you tell me how ferromagnetic substances are used in modern technology?	Ferromagnetic substances are used in recording tapes and disks for audio, video, and computer signals. They are also used in permanent magnets, which are found in motors, antenna, and speakers. Recent development in strong magnets has enabled communication equipment and computers to be miniaturized.	"['|The Challenger Accident|\nChallenger, launched at 11:38 am EST|\nafter a freezing Florida night,\nexploded 73 seconds after liftoff\ndue to the failure of an O-ring seal\non the right solid rocket booster\nkilling 7 passengers.\nRedesign of the seal and modification\nof the space shuttle took almost 3 years.\nThe replacement Endeavor resumed\nflight Sept. 29, 1988.\nA material undergoes transition under the influence of temperature and pressure, and these changes are physical in nature, because their molecules remain intact. During our school days, we were asked to distinguish physical and chemical changes. At that stage, we began to think in more details than what our senses have detected. Having the ability to distinguish physical properties from chemical properties is indeed a good beginning in the study of materials.\nThe Challenger disaster is due to a human failure to recognize the limitation of the property of o-ring material.\n|Phase Transition at Constant Pressure|\nHeat of vaporization\nHeat of fussion\nPhase transitions from solid to liquid, and from liquid to vapor absorb heat. The temperature of a system usually does not change as long as two phases are present. The (phase) transition temperature from solid to liquid is called the melting point whereas the temperature at which the vapor pressure of a liquid equals 1 atm (101.3 kPa) is called the boiling point. Thus, the measured boiling point depends on the atmosphere pressure. For compounds that decompose at high temperature, boiling point can be either specified at lower pressure or be replaced by the decomposition temperature. Thus, conditions as well as the value of boiling point listed in literature must be taken into account for application considerations. Boiling points of mixtures change with composition. The boiling points of some common mixtures are listed in handbooks, and boiling points can be used to assess the composition of a mixture or the purity of a compound.\nHowever, a glassy material becomes soft in a wide range of temperatures. The temperature at which the material becomes soft (behave molten like) is called glassy temperature, but it may be a range of temperatures. Behavior of a substance as the temperature changes must be carefully considered in its applications. Behavior of a mixture as temperature rises is different from its components. There is no theoretical way to predict the behavior of a mixture from its components, even if its exact composition is known. Addition of one or more materials usually changes the melting or glassy temperature of a substance. Thus, we often employ a blend (mixture) of materials whose behavior is acceptable within the desirable range of temperatures. Antifreeze for automobile radiator and deicing liquid for airplanes are examples of this application.\nThe behavior of mixtures as temperature and pressure change often requires a phase diagram to represent, and there are several models of two-component systems. A phase diagram of a many-component system requires a lengthy study.\nOne-component phase diagrams for water and carbon dioxide are given here.\nAt pressure below 5.1 atm, solid and gas carbon dioxide coexist, but the vapor pressure depends on the temperature. The variation of vapor pressure is represented by a line, which separates the Dry ice phase from the CO2 Gas phase. The vapor pressure of dry ice at 194.6 K (-78.5°C) the pressure is 1 atm, and at 216.7 K (-56.4°C) the pressure is 5.11 atm. The line separates the conditions for the formation of solid and vapor. A similar curve borders between liquid and gas CO2, whereas a line separates dry ice from the liquid phase. At 216.7 K, vapor pressures of solid and liquid CO2 are the same, 5.11 atm. At, 5.11 atm and 216.7 K, all three phases coexist, and the condition is called the triple point.\nPhase rules in soil science\nPhase equilibria in one-component systems pdf file\nPhase equilibrium: Pure substance Chapter 5 of a book\nPhase and phase diagrams\nChemical energy Use FIND to search for ""tin disease"" after\n|Linear thermal expansion|\ncoefficient per K at room\ntemperature of some substances\n|Electric resistance of some|\n|P (white)||109 Note the range of 1015|\nThe reciprocal of electric conductance is called electric resistance; thus, the higher the conductance, the lower the resistance. Electric resistance for some familiar materials are given in the table here. Note the large range of 1015 among these substances. Aluminium and copper are very good conductors, and their resistances are very low, in the order of 10-8, almost 100 times smaller than that of tungsten, W. Germanium, Ge, and silicon, Si, are typical semiconductors, whereas sulfur and phosphorous are insulation material.\nIron, cobalt and nickel are some ferromagnetic substances, there are some other alloys and oxides that behave this way. They possess a spontaneous magnetic moment. A magnetic field is present in these materials even in the absence of an external magnetic field. However, ferromagnetism is temperature dependent, and above the so called Curie temperatures of the substances, magnetism vanishes. The Curie temperature or Curie point of a substance is unique. The Curie points for Fe, Co, and Ni are 1043, 1400, and 630 K respectively.\nFerromagnetism are due to the presence of magnetic domains in the substance, and when these domains line up parallel to each other, they give a net magnetic field. If the domains line up antiparallel to each other at the Curie point, the substance is said to be antiferromagnetic. The magnetic susceptibility reaches a maximum at Curie temperature for antferromagnetic material. For example, FeO, MnO, CoO, NiO, FeF2, FeCl2, a-Mn, Cr2O3 etc. are some of the antiferromagnetic substances.\nFerromagnetic substances play important roles in recording tapes and disks for audio, video, and computer signals. Furthermore, ferromagnetic materials are used in permanent magnets, which are used in motors, antenna, and speakers. Recent development in strong magnets enables communication equipment and computers to be miniaturized.\n|Refractive index of |\nsome common substances\nDifference in refractive indexes of lights of different wavelengths can be separated using a prism. Refractive indexes for some familiar substances are given in a box. It should also be kept in mind that index of refraction changes with dissolved substance and concentration.\nWashing and cleaning also involve solubility, and the formulation of an effective cleaning agent requires the knowledge of many substances. Substances can be classified according to polarity. Water, ammonia (NH3), and methanol (CH3OH) are polar, because their molecules have negative and positive sites, whereas methane (CH4), gasoline, and motor oil are non-polar. Regarding solubility, a rule of thumb reads like dissolves like, which means that polar solvents dissolve polar substances and non-polar solvents dissolve non-polar substances. An organic compound with a polar group attached to non-polar chain will bring water molecules to a non-polar surface, and hence it can be used as a detergent or wetting agent. This rule of thumb has potential for both domestic and industrial applications.\nSurface tension results from intermolecular attraction, the higher of which, the higher surface tension. Energy required to stretch the surface by some defined unit is called surface tension, and whose unit is N.m/m2 (= N/m). Like viscosity, surface tension decrease with increase temperature. Surface tension causes the dew and raindrops to be round, and the idea to manufacture perfect spheres in zero gravity zone is making use of surface tension. Soap reduces surface tension of water and soapy water forms bubbles when air is blown into it.\nDescribe an application of a material based on any one of the physical properties.\nPoint out two physical properties that has not been mentioned here.\nWhat substance has the highest dielectric constant?\nWhat is a beam of polarized light?\nGive a sketch of the molecular structure of an aminoacid.']"	['<urn:uuid:f8e1f254-5669-4d3f-bd1d-2797d03f0c0e>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T02:03:54.148118	20	44	1273
75	As a harbor manager, what's the winter storage protocol for boats?	Each boat must be picked and carried 100 to 200 feet to be placed in winter storage at least once a year. The Link-Belt crane lifts the boats out of the water and swings them over a dock area, using outriggers for the initial lift and rubber tires for transportation.	"['The Link-Belt crane hoisting the U.S. Coast Guard’s marine science vessel up out of the water for maintenance.\nThe U.S. Coast Guard Academy, which has its own harbor with a fleet of both sailboats, motorized boats and the U.S. Coast Guard competition-ready sailing yacht, purchased a new Link-Belt RTC8065 Series II 65-ton (59 t) telescopic boom rough-terrain crane from Wood\'s CRW. The crane features 115 ft. (35 m) of boom, two drums, block and ball, a flat deck design and safety reels.\nThis crane was purchased specifically to lift each Coast Guard vessel out of the water either for periodic maintenance or for winter storage. The crane was delivered in January 2013 and replaced an aging P & H crane, which had been in service for more than 20 years.\nSome of these vessels are very challenging to move.\nThere are sailboats as long as 44-ft. (13.4 m) that have a 9.5-ft. (2.9 m) keel, masts that are approximately 70 ft. (21 m) in height and can weigh up to 27,000 lbs. (12,247 kg). Some of the smaller boats range down to 32 ft. (9.7 m) in length and weigh as little as 9,000 lbs. (4,082 kg).\nThe crew that maintains and operates the harbor are responsible for approximately 40 boats. Most of the time these boats are simply being lifted up out of the water and swung out over a dock area where maintenance can be performed. But at least once a year, each boat must be picked and carried 100 to 200 ft. (30.5 to 61 m) to be placed in winter storage.\n""The pick and carry capability of the Link-Belt was one of the most important features the Coast Guard Academy had for buying requirements,"" said Marc Varricchione of Wood\'s CRW.\nThe Link-Belt RTC8065 Series II is ideal for the crew at the academy\'s harbor. When lifting boats up out of the water, the outriggers are put in place to make the pick feasible and if the boat needs to be transported, the rubber-tired feature of the crane comes into play for transportation. One of the biggest appeals that the academy had for this particular Link-Belt rough-terrain crane was its size and turning radius. There is very little space between the edge of the dock that the crane works from to pick up the boats and a maintenance building that sits less than 100 ft. from the edge of the dock. This makes maneuverability an important necessity to the crane.\nThe Link-Belt is equipped with all-wheel steer, which gives the crane maximum steering ability in a minimum amount of space. It can almost turn 360 degrees in its own footprint.\nAccording to Jim Hartley, one of the crane operators, ""This crane does a very nice job fitting our needs. Its maneuverability far exceeds what I had to work with previously. It has a turning radius of only 24 feet. The controls are set up in a way that really gives me a sense of confidence while operating the crane. From a selfish viewpoint, I really love the cab. Starting with the dramatically improved visibility; it has a curved glass windshield. Basically, the windshield and ceiling of the cab are made of one continuous piece of glass. So there is nothing there to block your view. Also, the cab features air conditioning, which our old crane certainly did not have.""\nAccording to Allen Kruger, waterfront director, ""As a team we are extremely pleased with this machine. When we decided to make the purchase the operators and staff spec\'d out exactly what they were looking for to best suit their needs and this Link-Belt is certainly meeting all of that criteria. Wood\'s CRW has worked closely with us throughout the entire process, even providing us with a crane on a short term basis, between the time when our old crane was taken out of commission and the new crane being delivered. Marc Varricchione, our representative with Wood\'s CRW, the dealership that sold the machine to us, has made sure that everything has gone very smoothly; delivery, setup, training of our people, and basically making sure that all of our needs have been satisfied.""\nToday\'s top stories']"	['<urn:uuid:2a46852d-7255-4370-9fcb-4de6db9144c7>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-13T02:03:54.148118	11	50	697
76	monocular diplopia causes versus falls related visual problems elderly	Monocular diplopia can be caused by irregular astigmatism, subluxated clear lenses, poorly fitting contact lenses, early cataract, iridodialysis, polycoria, large iridotomies, and macular disorders. In elderly individuals, visual problems like cataracts are a significant physical risk factor for falls, which can lead to serious injuries including fractures, head trauma, and loss of independence.	"['Presentation on theme: ""Approach to a patient with diplopia""— Presentation transcript:\n1 Approach to a patient with diplopia Dr. R.R.BattuNarayana Nethralaya\n2 What does the faculty of BSV require? Perfect ( or near perfect ) alignment of the visual axes simultaneously on the object of regardPerfect ( or near perfect ) retinal correspondencePerfect central ( or paracentral ) fusional capability.Perfect ( or near perfect ) alignment of the retinal receptorsPerfect ( or near perfect ) optics to allow only one image to be formed on the retina and the same single image to be formed on the other\n3 What is Diplopia ?It is when more than one image ( two ) of the object of regard are seen simultaneouslyThis occurs when….(Mechanisms)More than one image of the object of regard is formed in the retinae of one or both eyes ( monocular diplopia)The eyes lose their simultaneous alignment with the object of regard in one or more directions ( or distances ) of gaze (incomitance of ocular alignment – binocular diplopia)The eyes although aligned, send images to the brain which disallow fusion ( aniseikonia )Local retinocerebral adaptations to misalignments in early life go askew (paradoxical diplopia, loss of suppression)Rarely, purely cerebral mechanisms\n4 Monocular vs Binocular Diplopia Key questionIs the double vision present even on monocular eye closure?\n5 Monocular diplopiaMore than one image of the object of regard is formed in the retinae of one or both eyes…..Irregular astigmatism ( nebular scars, haze, corneal distortion)Subluxated clear lensesPoorly fitting contact lensesEarly cataractIridodialysis, polycoria, large iridotomiesMacular disorders – edema, CNVM etc\n6 Binocular DiplopiaThe eyes lose their simultaneous alignment with the object of regard in one or more directions ( or distances ) of gaze (incomitance of ocular alignment – binocular diplopia)Key cluesAnomalous Head PositionVision Blurry in one gaze position, better in anotherVestibular signsLong tract signsObviously misaligned eyes, proptosisPresence of partial ptosisNystagmus\n7 Questions to be asked Is there a mis alignment? If so, in which directions ( or distances ) of gaze?Which are the hypofunctioning ( and hyperfunctioning ) muscles?Do they have a neurogenic pattern, or a restrictive pattern or a neuromuscular pattern or a myogenic pattern?\n9 Diplopia - Key questions Is the diplopia more for distance or near? Is the diplopia predominantly horizontal or vertical?In which direction of gaze are the images maximally separated?To which eye does the “outer” image belong?Is there a predominant tilt?In which position of gaze does the tilt increase maximally?\n10 Diplopia chartingDiplopia is maximum ( separation of images) in the field of action of the paralysed muscle.The false image ( the image belonging to the eye with the hypofunctioning muscle ) is always peripherally situatedHigher in upgaze, lower in downgaze, on the right in right gaze and on the left in left gaze\n11 Hess Charting Based on the principle of confusion Allows for identifying the position of one eye, while the other eye fixes in different positions of gaze.Effectively demonstrates Sherrington’s and Hering’s lawsAllows for more objective follow up also.\n12 The cover-uncover and alternate cover tests Probably the most important objective tests to evaluate muscle palsiesMeasurements with a prism bar allow for measurementMeasure in the 9 cardinal gaze positionsDistance and near\n13 Versions & Ductions Allow to assess actual rotation limits Allow assessment of underactions and overactions of synergists\n14 Saccadic Velocity“Floating saccades” are suggestive of a nerve palsy or paresisIndirectly “oblique saccade” testing can be done.Normal saccadic velocity with limitation indicates a restricted muscle\n15 Forced Duction Testing Allows to assess forced movement in direction of restrictionImportant in Blow out fractures, TED, long standing strabismus with contracturesImportant to lift the globe and rotate\n16 Force Generation Testing Allows to identify residual power in a suspected paretic muscle. Usually done to direct management6th N palsyRecess – resect or muscle transposition\n17 Pointers to primary orbital disease Restrictive muscle hypofunctionProptosisSigns of orbital inflammationSigns of anterior segment, lid and adnexal hyperemia or inflammation\n18 Neurological diseaseLook for supranuclear, nuclear and infranuclear patternsLook for sensory ( visual ) abnormalitiesLook for nystagmusLook for vestibular – auditory symptomsLook for other cranial nerve involvementLook for long tract signs\n19 CNS and orbital imaging Done for obvious neurological patternsOrbital inflammatory disease, proptosisOccasionally may avoid or delayPupil sparing 3rd in a diabetic6th Nerve in a hypertensive, image if no spontaneous recovery in a few weeks\n20 ImagingCTMRIFat suppressionStir sequencesMRA vs CT angio\n21 Ancillary tests Tests for myasthenia Tests of thyroid function X- ray chestBloods\n22 Aniseikonia Occurs when image size disparity exceeds 5% Previously seen in monocular aphakiaMay occur following keratorefractive surgery\n23 Convergence insufficiency Classically for nearCould be primary or secondary', 'When a person grows older, mobility concerns come in to play. Falls are one of the major health problems amongst senior citizens and a common cause of injury in this age group; an event which is most of the times accidental and seen mainly in elderly individuals.\nRecurrent falls are a critical cause of morbidity and mortality in elderly and a marker of poor physical and cognitive status. Studies in Indian population have shown that incidence of falls in elderly (60 years of age and above) ranges from 14 per cent to 52 per cent.\nAn Indian study has found that after a fall, fracture is seen in 21 per cent and other forms of injury in nearly 80 per cent. Falls in elderly are one of the major contributors of hip fracture, spine injuries and severe head trauma. They are also major cause for hospitalisation of elderly and need for long-term institutional care.\nMedications are one of the most common causes of falls in elderly. Taking five or more medicines (Polypharmacy) has been shown to be directly related to increased risk of falls.\nMedicines that act on the nervous system, particularly, sedatives, psychoactive drugs, anti-depressants, anti-histamines, medications affecting BP are some of the common reasons contributing to falls.\nFalls are generally a combination of several risk factors which can affect and threaten the ability of a person to continue with daily routine activities for a living.\nPhysical risk factors:\n- Previous history of having fallen\n- Chronic medical conditions like diabetes, thyroid problems, anaemia\n- Neurological problems like stroke, Parkinson disease\n- Vertigo and Balance disorders\n- Muscle weakness\n- Mood disorders like depression and psychiatric issues\n- Impaired mental status like cognitive impairment and dementia\n- Visual problems like cataract\nEnvironmental risk factors:\n- Poor lighting\n- Slippery floors\n- Cluttered walkways\n- Wearing improper footwear\n- Incorrect use of the walking aids such as canes or walkers\n- Lack of grab bars in bathrooms\n- Uneven and broken sidewalks\n- Irregular steps\n- Unpredictable level changes\nFalls can range from trivial to major injuries in the elders which are mentioned below:\n- Physical injuries like (muscle pains, scratches and wounds, hematomas, lacerations, fractures, head injuries leading to intracranial bleeding) and in severe cases can lead to death.\n- Self-imposed limitation of activity\n- Social isolation\n- Loss of independence\n- Poor quality of life\nIt is of utmost importance that the elderly report any fall that they may have had, when visiting their doctor for a check-up. Consultation with a Geriatrician (Elderly Care Specialist Physician) may benefit, as they are experts in preventing and managing falls.\nGait and balance training, medication optimisation, muscle strengthening exercises, home hazard modifications, correction of underlying problems like cataract are some of the preventive measures. An efficient preventive strategy is to educate patient and caregiver.\nThe patient at risk and family members should be educated about the multifactorial nature of falls, the specific risk factors and recommended interventions. Vitamin-D supplementation has been shown to be effective in preventing falls; but it should be taken only after reviewing with a doctor.\nDepending on the severity of the fall and associated injury, the treatment options may vary. If there is a fracture, orthopaedic intervention and corrective surgery is needed.\nEvery individual with a fall needs to undergo detailed evaluation and assessment like review of medications, gait and balance assessment and other tests as per the need, medication optimization, gait and balance training, muscle strengthening exercises, using assistive devices like walkers, vitamin D supplementation, better home safety measures like hand rails, grab bars and raised toilet seats.\nTreatment of underlying cardiovascular or neurologic disorder and education of patient and caregivers regarding falls are important steps.\n- Frequent discussion with them about their current health conditions\n- Take them for regular eye check-up\n- Keep a watch on whether they are holding onto walls, furniture or taking help of someone when they are walking or face any difficulty in walking or getting up from a chair\n- Talk about medications\n- Eating healthy and food those provide nutrition\n- Wiping up the spills immediately\n- Keeping pathways in a good and clean condition\n- If they have difficulty in their daily activities or have balance problems or have fallen in the past, seek a consultation with a Geriatrician\n- Ensure that the elderly people are using assistive devices like canes or walkers, as prescribed by doctors\nThe author is a Consultant Geriatrics at Fortis Hospital, Mulund']"	['<urn:uuid:26c57bdc-d355-4e18-b6a1-02e9e4e89f23>', '<urn:uuid:802fd89e-7aff-420b-b735-d8d434009491>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-13T02:03:54.148118	9	53	1514
77	where is wild florida pennyroyal plant found	The Wild Pennyroyal or Florida Pennyroyal plant (Piloblephis rigida) is only found naturally in Florida.	['Mint is a nectar producing plant with over 30 species and hybrids. Probably one of the best known herbs, it grows wild around the world and is widely cultivated. It is a member of the nectar-producing (melliferous) Lamiaceae family that include basil, oregano, rosemary, sage, thyme and lavender, many of which produce excellent single flower honey.\nMint honey is a rare treat. It has a strong flavor and distinctive aroma which varies depending upon the variety of mint it was made from. It is a common wild flower and is often a contributor to wild flower honey, but seldom in quantities sufficient to make a single flower honey. Fortunately, mint is cultivated for its essential oil and these large fields of mint make the monofloral variety of honey much easier to find. When looking for mint honey, start with the countries and regions known for producing the essential oil of mint.\nMint is generally known for its slightly sweet taste and the cool sensation it leaves in your mouth. It has been used for thousands of years as a culinary herb and for its aromatic and therapeutic properties. Mint is used for tea and therapeutic products and the essential oil of mint is used as an aromatic agent in chewing gum, dental products, pharmaceutical products, foods and drinks.\nMint is said to be named after the nymph, Minthe who was seduced by the Greek god of the underworld, Pluto. His wife, in a fit of jealous rage, trampled the nymph to dust. Pluto, unable to reverse his wife’s actions and save her, partly succeeded by turning her into mint, a plant that becomes even more aromatic if stepped upon. Mint was often strewn on floors as a natural air freshener that was revitalized as people walked on it.\nTranslations: Italian: miele di menta; French: le miel de menthe; Spain: miel de menta; Romania: miere de menta; German: minze honig; Croatia: Metvičin med\nPeppermint honey is a rare find these days in spite of the fact that mint it is widely cultivated. Peppermint (Mentha × piperita) is a naturally occurring hybrid mint, a cross between watermint (Mentha aquatica) and spearmint (Mentha spicata). It is found wild occasionally with its parent species. Peppermint honey crystallizes into a dense, fine buttery consistency, easily softened in warm water. It has one of the most powerful aromas of all the mints. Surprisingly, the honey does not taste like like what you might expect, peppermint candy. In fact, it has a strong musky aroma, which when combined with the menthol packs a powerful punch. The flavor is strong too, but it surprisingly good. It is medium sweet with a lingering after taste. It is a amber colored in liquid form and lightens to a beautiful caramel when crystallized.\nThe plant is a native of the Mediterranean and is cultivated in Japan, China, Australia and Great Britain. It is widespread in northern Italy, and particularly well known regionally as Mint Pancalieri (Torino). Peppermint honey found in Italy (Miele di Menta) is produced from these northern Italian Peppermint crops. United States is responsible for 75% of all world production. The history of mint agriculture in the United States began in the late 1700’s. It is cultivated on a large scale in the states of Oregan, Indiana, Idaho, Ohio and Michigan.\nThe ancient Egyptians, Greeks, and Romans used peppermint extensively in foods such as wines and meat and also in medicines. Additionally, it was used as crowns and decorations at feasts. Aristotle referred to it as a an aphrodisiac. In the eighteenth century, the peppermint was first utilized for commercial cultivation.\nPeppermint aka: Balm Mint, Brandy Mint, Curled Mint, Lamb Mint\nTranslations: German: Pfefferminzhonig\nSpearmint – (Mentha spicata) Has a medium amber color and crystallizes easily. It has a medium strong aroma supported by a touch of menthol and a spearmint nuance. It is medium sweet with a short aftertaste.\nOne of the oldest recorded mints, it has long been used therapeutically for digestion and other illnesses. A native of the Mediterranean but is now also cultivated in the USA, UK, Italy, Yugoslavia, Hungary, Bulgaria, South Africa, Thailand, Vietnam as well as Argentina, Australia, Brazil, China, Egypt, France, Japan, Korea, Morocco, New Zealand, Paraguay, Romania, Russia, Taiwan. United States is a primary producer of spearmint oil with spearmint cultivation localized in the states of Indiana and Michigan. Spearmint honey may be found in both those states.\nPennyroyal – (Mentha pulegium) 100 kg/ha honey; (Danube Delta – Romania). It is the tiniest of the mint plants. Pennyroyal honey is strongly flavored and works well with savory dishes. The name Pennyroyal is derived ultimately from the Latin, Pulegium (fleabane) as described by the ancient Roman naturalist, Pliny, for its ability to ward off fleas. It worked so well it was called Royal Fleabane, or in the original latin, “Pulegium regium” which was corrupted to, “Penny Royal” or Pennyroyal. It had such a strong aroma Pliny notes it was effective for reviving the fainted when stored in vinegar. It has an agreeable, strong pungent aroma comparable to camphor and similar to spearmint. Originating in France and Germany, it is found growing wild in Europe, Asia, New Zealand, North and South America and is cultivated in Morocco and USA. Pennyroyal honey may be found in New Zealand.\nThe name Pennyroyal is popular. There is another honey produced in the Canary Islands called “miel de poleo” which is tranlsated as Pennyroyal honey. It is produced from the nectar of “poleo de pinar” (Pine Tree Pennyroyal), a plant with a strong minty and menthol fragrance. This plant produces a similar tasting honey and belongs to the same family, Lamiaceae, but is an entirely different species, Bystropogon origanifolius. It is a rare plant found naturally only in the beautiful Canary Islands off the coast of Spain.\nThere is yet another Pennyroyal honey that was produced in Florida around the turn of the 20th century, and may be produced still, from the abundant Wild Pennyroyal or Florida Pennyroyal plant, of the same plant family Lamiaceae. However, this too is of an entirely different species, Piloblephis rigida (W. Bartram ex Bentham). This plant is only found naturally in Florida. In the book, The ABC and XYZ of Bee Culture, the honey is described as, “…clear, and light in color, perhaps the whitest of all honeys in the state, the body and flavor both being fine. Some Beemen here maintain that pennyroyal honey is the finest in the state…” (see reference below)\nThe plant was identified by the father and son naturalists, William and John Bartram in the late 1700’s. Coincidentally, these are the same naturalists who identified the plant source of Tupelo honey, Ogeechee Tupelo (Nyssa ogeche W. Bartram ex Marshall).\nTranslations: Spanish: miel de poleo; Italian: miele di mentuccia\nOther mint honeys from around the world are obtained from wild or cultivated mint. A few popular ones are mentioned here. Availability of these honeys variable and possible only from local beekeepers selling directly or through local farmers markets.\n- Corn Mint or Field Mint – (Mentha arvensis) (AKA पुदीना/ Pudina in Hindi, Wild Mint, Japanese Mint) is native to Europe and western and central Asia, east to the Himalaya and eastern Siberia, and North America. It is cultivated in India, China, Taiwan, Thailand, Japan and Brazil.\n- Water Mint Honey – (Mentha aquatica) Grows in the Danube Delta – Romania. It is a native of Europe, northwest Africa (Morroco) and southwest Asia.\n- Horse Mint Honey – (Mentha longifolia) Native to Europe, Western and central Asia, North and South Africa.\nPeppermint blossom Image Credit: By H. Zell., Bergamot blossom Image Credit: By Magnus Manske, Pine Tree Pennyroyal Image Credit: By Tigerente. These image files are licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license.\nWild Pennyroyal Image Credit: By Artcons\nSpecific chemical markers of Mentha species: The ester, Methyl syringate (MSYR) and vomifoliol. MSYR has been found in many monofloral honeys and is found in high concentrations in asphodel honey.\nThe ABC and XYZ of Bee Culture. An Encyclopedia Pertaining to Scientific and Practical Culture of Bees by Amos Ives Root, Ernest Rob Root Published in Medina, Ohio by the A.I. Root Company 1917 2nd Edition.']	['<urn:uuid:f0395bb0-5a69-47e5-b81c-cb1d40d085d2>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-13T02:03:54.148118	7	15	1370
78	compare teacher student rights academic freedom versus religious activity in schools what can each group do	Regarding academic freedom, teachers (faculty members) have the right to use their scholarly judgment about topics, readings, and arguments in their classes, while students have the right to provide feedback and disagree with professors without fear of reprisal. Both groups have the right to have their work judged by disciplinary standards, not personal or political views. Regarding religious activity in schools, which is governed by First Amendment protections, neither teachers nor students can be forced into any particular religion by the government. Government actions affecting religious activity in schools must pass three tests: having a secular purpose, not primarily advancing/inhibiting religion, and avoiding excessive entanglement with religion. For example, allowing exclusive Christian prayers in school settings would be unconstitutional as it would promote one religion over others.	"['The Department of Political Science is committed to academic freedom as a foundational value of higher education and research. As Debra Satz, Dean of Humanities and Sciences, writes “academic freedom is, in the first instance, the freedom of the scholarly community to pursue, disseminate, and openly discuss their work.”\nAcademic freedom is a prerequisite for the duties associated with the role of a scholar. These duties include the creation, preservation, and dissemination of knowledge in the spirit of truth-seeking.\nAcademic freedom is intended to protect the rights of faculty. It includes both individual and collective rights. Individual faculty members have the right to pursue research and teaching within their areas of expertise without coercive external interference or pressure (e.g. from the state, donors, university trustees, corporations, interest groups, or online communities).\nAs researchers, faculty members have the right to use their best scholarly judgment about the research questions they pursue and what they publish. Academic freedom includes the right of faculty members to have their work assessed according to the internal standards of the discipline and not, for instance, on the basis of their personal or political views, or on the basis of the political implications of their results.\nAs teachers, faculty members have the right to use their best scholarly and pedagogical judgment about which topics to cover, which readings to assign or omit, and which arguments to foreground in their classes.\nAcademic freedom is more limited than the freedom of speech protected by the First Amendment of the United States Constitution. To adapt an example from the legal scholar Robert post: “Although the First Amendment would prohibit government sanctioning an editorialist for the New York Times is he were inclined to write that [the astrological signs of world leaders explain the incidence of inter-state wars], no [political science] department could survive if it were unable to deny tenure to a young scholar similarly convinced.” Of course, scholars have constitutional rights to express outlandish views as private citizens or in public discourse. They do not have the same protections if they propagate such views in their position as scholars.\nCollectively, faculty members have the right to adjudicate the work of both colleagues and students on the basis of the internal standards of their discipline, without coercive external interference or pressure (e.g. from the state, donors, university trustees, corporations, interest groups, or online communities). This includes, but is not limited to, departmental decisions on hiring and promotion, as well as decisions within the broader discipline (e.g. by faculty acting as journal editors and reviewers or evaluators of grant applications).\nAs Dean Satz has stated, students also possess a set of rights as members of the Stanford academic community. They have the right to provide feedback on teaching and to disagree with their professors without fear of reprisal. They have the right to have their work judged by the internal standards of the relevant discipline and not, for instance, on the basis of their personal or political views, or on the basis of the political implications of their results.\nRESOURCES ON ACADEMIC FREEDOM AT STANFORD\nAmerican Association of University Professors (AAUP), Resources on Academic Freedom.\nDebra Satz, Dean’s Message on Academic Freedom and Free Speech (March 2021).', ""Freedom of Religion - Explained\nWhat is Freedom of Religion?\nIf you still have questions or prefer to get help directly from an agent, please submit a request.\nWe’ll get back to you as soon as possible.\n- Marketing, Advertising, Sales & PR\n- Accounting, Taxation, and Reporting\n- Professionalism & Career Development\nLaw, Transactions, & Risk Management\nGovernment, Legal System, Administrative Law, & Constitutional Law Legal Disputes - Civil & Criminal Law Agency Law HR, Employment, Labor, & Discrimination Business Entities, Corporate Governance & Ownership Business Transactions, Antitrust, & Securities Law Real Estate, Personal, & Intellectual Property Commercial Law: Contract, Payments, Security Interests, & Bankruptcy Consumer Protection Insurance & Risk Management Immigration Law Environmental Protection Law Inheritance, Estates, and Trusts\n- Business Management & Operations\n- Economics, Finance, & Analytics\nTable of ContentsWhat are the Establishment Clause and Free Exercise Clauses?What is the Establishment Clause?Discussion QuestionWhat is the Free Exercise Clause?What is the Secular Purpose test of the 1st Amendment?Discussion QuestionWhat is the Primary Effect test of the 1st Amendment?Discussion QuestionWhat is the Excessive Entanglement test of the 1st Amendment?Discussion QuestionPractice QuestionAcademic Research\nWhat is Freedom of Religion?\nThe freedom of religion portion of the 1st Amendment is made up of the Establishment Clause and the Free Exercise Clause.\nThe 1st Amendment states that, Congress shall make no law respecting an establishment of religion or prohibiting the free exercise thereof.\nNext Article: Businesses and Freedom of Religion Back to: CONSTITUTIONAL LAW\nWhat is the Establishment Clause?\nThe first provision for religious protection under the US Constitution is known as the Establishment Clause.\nIt stands for the principle that the government should not force any particular religion(s) onto its citizens.\nHistorically, why did the Framers seek to prohibit the Government from establishing a state religion? How has this clause been interpreted with regard to government actions and religious activity?\n- 1)The framers believed that any government interventions in the religious affairs of citizens would necessarily infringe on their religious freedom. 2) The establishment of religion clause of the First Amendment means at least neither a state nor the federal government can set up a church. Neither can press laws which aid one religion, aid all religions or prefer one religion to another. According to Jefferson (First Amendment) clause against establishment of religion by a law intended to erect a wall of separation between church and state......... The wall must be kept high and impregnable.\nWhat is the Free Exercise Clause?\nThe second provision of the 1st Amendment addressing religion is known as the Free Exercise Clause.\nIt provides that the government cannot prohibit individuals from practicing any religion.\nThe Free Exercise Clause has been the subject of significant litigation charging the government with discriminating against an individuals' religious practices. E\nstablished in Lemon v. Kurtzman, the common law test for determining whether a government statute runs afoul of the Free Exercise Clause by unduly restricting the free exercise of religion is whether the law\n- Has a secular purpose,\n- What is the primary effect of the law, and\n- Is there excessive entanglement with religion.\nIf a law or government action violates any one of the above elements, it is an unconstitutional infringement of the 1st Amendments religious protections.\nWhat is the Secular Purpose test of the 1st Amendment?\nDoes the statute or government action affecting religion have a secular (non-religious) purpose?\nIf the purpose of the statute or government action is to somehow promote any single or particular group of religions, the statute is unconstitutional.\nIf the purpose of the statute or government action is not to promote religion, then move onto the next step.\n- Example: If a government allows a manger scene on government property during Christmas and denies citizens the right to put a menorah or minaret, this action would not have a secular purpose.\n- What is the 1st Amendment?\n- How does freedom of religion affect business practice?\n- What is the protection of Freedom of Speech?\n- Speech with Limited or No Protection\n- What is Obscene Speech?\n- What are Fighting Words?\n- What is Commercial Speech?\n- What is Defamation?\n- What is Political Speech?\n- Overbreadth and Overly Broad Laws\n- Freedom of the Press\n- Freedom of Assembly\nCan you think of any examples of government action that have been challenged as effectively promoting a particular religious practice? What about discriminating?\n- In Lund vs Rown County, NC, the 4th Circuit Court of Appeals limited the ability of County Commissioners to open meetings with Christian prayer the prothelatyzed to attendees. In Employment Division v Smith, the US Supreme Court held that the local government could fire an employee for smoking Peyote as part of a religious ceremony. Per the court, a state may accommodate a religious practice, but it is not required to do so.\nWhat is the Primary Effect test of the 1st Amendment?\nIs the primary effect of the statute or government action to advance or inhibit religion?\nEven if the purpose of the statute or action is secular, it may violate the Establishment Clause if the primary effect is to somehow advance or inhibit a religion among the citizens.\nThe word primary is of particular importance. It leaves room for statutes or actions that only incidentally promote a particular religion.\nIf the primary effect is something other than advancing or inhibiting religion, move on to the next step.\nCan you think of any examples of laws or government actions that have a purpose other than promoting religion, but do have an incidental effect of promoting a religious practice?\n- Recently in Maryland-National Capital Park and Planning Commission v. American Humanist Association, the US Supreme Court held in a 5-2 decision that allowing a war memorial that is shaped as the Latin Cross, (Christian cross) does not have the effect of promoting one religion above another.\nWhat is the Excessive Entanglement test of the 1st Amendment?\nDoes the statute cause excessive government entanglement with religion?\nEven if the statute only has a secondary effect upon religion, it may still result in too much government involvement with religious practice to comply with the Establishment Clause. This is a floating standard that greatly depends on the specific law and how it affects religious practice.\n- Example: In Tilton v. Richardson (1971) the Supreme Court held that providing one-time grants to religious colleges and universities to build facilities did not violate the Establishment Clause, as there was not excessive entanglement with religion.\n1. How do you feel about allowing a court this level of autonomy for determining when a statute or government action is simply too closely related to religious activity?\n- Look back on the material covering Judicial Activism and Judicial Restraint for the policy leanings of court involvement in judicial interpretation.\n2. Do you think that the government should pass laws that have an effect on any single religion? Can you think of any examples of a law that has a secular purpose but has an effect of promoting a religion? How would you measure whether an effect on promoting religion is strong enough to be considered a primary effect? At what point do you think government influence on religious practice becomes excessive entanglement?\n- Your argument may focus on judicial autonomy and theories of judicial activism or restraint. Many would argue that the government allowing for open prayer in public meetings where the individuals delivering the prayer are exclusively Christians would unduly promote Christianity above other religions.\nMary is the mayor of Small Town, Texas. She is a Christian and believes that everyone else in the world should be as well. She proposes a resolution to the city council to make Christianity the official religion of Small Town. As part of her proposal, she lays out a plan to convert part of the city hall into a sanctuary to hold Christian services on Sunday. What are the Constitutional issues implicated by these proposals?\n- Adopting Mary's proposal would be a violation of the 1st amendment of the Constitution. The Establishment Clause directs that the government should not favor one religion over the other. This proposal has the specific purpose of promoting religion, which fails the common law test. Opening up a Government facility to religious-related activity would be okay if it is non-specific in the religion. The question would be whether opening up the facility to this type of service would be excessive entanglement with religious practice.\n- Esbeck, Carl H., The Establishment Clause as a Structural Restraint on Governmental Power. Iowa Law Review, Vol. 84, pp. 1-113, 1998. Available at SSRN: https://ssrn.com/abstract=186692\n- Muoz, Vincent Phillip, The Original Meaning of the Free Exercise Clause: The Evidence from the First Congress (2008). Harvard Journal of Law and Public Policy, Vol. 31, No. 3, pp. 1083-1120, 2008. Available at SSRN: https://ssrn.com/abstract=1150780\n- Esbeck, Carl H., When Accommodations for Religion Violate the Establishment Clause: Regularizing the Supreme Court's Analysis. West Virginia Law Review, Vol. 110, p.359, 2007; University of Missouri School of Law Legal Studies Research Paper No. 2007-09. Available at SSRN: https://ssrn.com/abstract=992885\n- Corbin, Caroline Mala, Intentional Discrimination in Establishment Clause Jurisprudence (March 27, 2015). Alabama Law Review, Forthcoming. Available at SSRN: https://ssrn.com/abstract=2585220\n- Garry, Patrick M., Coordinating the Exercise and Establishment Clauses: A Narrow Establishment Clause Test for Government Funding of Prisoner Rehabilitation Services by Religious Providers (June 30, 2008). Ave Maria Law Review 387, Vol. 6, No. 2, 2008. Available at SSRN: https://ssrn.com/abstract=1153474\n- Garry, Patrick M., The Democratic Aspect of the Establishment Clause: A Refutation of the Argument that the Clause Serves to Protect Religious or Nonreligious Minorities. Mercer Law Review, Vol. 59, No. 2, 2008. Available at SSRN: https://ssrn.com/abstract=1130303\n- Garry, Patrick M., Distorting the Establishment Clause Into an Individual Dissenters Right (2012). 7 Charleston Law Review, 2012. Available at SSRN: https://ssrn.com/abstract=2336213\n- Huigens, Kyron, Science, Freedom of Conscience and the Establishment Clause (1989). 13 U. Puget Sound L. Rev. 356 (1989).. Available at SSRN: https://ssrn.com/abstract=3055866\n- Chapman, Nathan S., The Establishment Clause, State Action, and Town of Greece (February 15, 2016). William & Mary Bill of Rights, Vol. 24, 2015; UGA Legal Studies Research Paper No. 2016-06. Available at SSRN: https://ssrn.com/abstract=2732794""]"	['<urn:uuid:bb249800-1f1d-44d2-862a-fcb7db985bad>', '<urn:uuid:74019223-e77e-4392-b2af-caeda64fbd6a>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-13T02:03:54.148118	16	127	2235
79	steps develop effective dividend policy	Steps to develop effective dividend policy include: assessing financial position to determine dividend payment capacity, considering growth prospects and investment opportunities, understanding shareholder preferences, and clearly communicating the policy to shareholders.	['Dividend policy refers to the decisions companies make regarding the amount and timing of dividend payments to shareholders. This policy has a significant impact on shareholder wealth and can influence the overall value of a company.\nHow Dividend Policy Affects Shareholder Wealth\nDividend policy affects shareholder wealth in several ways. First, regular dividend payments are often seen as a signal of a company’s stability and financial health. Companies that consistently pay dividends are perceived as reliable and trustworthy, leading to a positive impact on shareholder confidence and, ultimately, the stock price.\nSecond, dividend payments provide shareholders with a tangible return on their investment. A company that pays regular dividends can attract long-term investors who are looking for steady income, thereby increasing demand for the stock and potentially driving up its price.\nOn the other hand, companies that do not pay dividends may be perceived as riskier or as lacking investment opportunities. This can lead to a decrease in shareholder confidence and a negative impact on the stock price.\nFactors Influencing Dividend Policy\nSeveral factors influence a company’s dividend policy, including its financial position, cash flow, growth prospects, and tax considerations. Additionally, management’s perception of the company’s future financing needs and the preferences of shareholders can also play a role in determining the dividend policy.\nFor example, a company with stable earnings and excess cash flow may choose to adopt a more liberal dividend policy, distributing a larger proportion of its earnings to shareholders. Conversely, a company with high growth potential may choose to reinvest its earnings to fund future expansion, resulting in a more conservative or nonexistent dividend policy.\nOverall, dividend policy has a significant impact on shareholder wealth. By carefully considering the financial position and growth prospects of a company, as well as the preferences of its shareholders, management can implement a dividend policy that enhances shareholder value and contributes to the long-term success of the business.\nHow to Develop an Effective Dividend Policy\nDeveloping an effective dividend policy requires careful consideration of various factors. Here are some steps to help companies create a dividend policy that maximizes shareholder wealth:\n- Assess the financial position: Evaluate the company’s financial position, including cash flow, earnings stability, and capital requirements, to determine the capacity for dividend payments.\n- Consider growth prospects: Assess the company’s growth prospects and investment opportunities to determine the appropriate balance between reinvesting earnings and distributing dividends.\n- Understand shareholder preferences: Take into account the preferences of different types of shareholders, such as income-oriented investors and growth-oriented investors, when determining the dividend policy.\n- Communicate the policy: Clearly communicate the dividend policy to shareholders to manage expectations and build confidence in the company’s financial stability and outlook.\nWhat is a dividend policy?\nA dividend policy refers to the decisions companies make regarding the amount and timing of dividend payments to shareholders.\nHow does dividend policy impact shareholder wealth?\nDividend policy can impact shareholder wealth by influencing the perceived stability and financial health of the company, as well as providing shareholders with a tangible return on their investment.\nWhat factors influence dividend policy?\nFactors that influence dividend policy include the company’s financial position, cash flow, growth prospects, tax considerations, future financing needs, and shareholder preferences.']	['<urn:uuid:ea72984b-a54e-492f-ad69-38831db78a06>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-13T02:03:54.148118	5	31	536
80	agronomist studying nitrogen uptake efficiency what factors affect nitrate leaching in irrigated sandy soils and high NO3-N irrigation water	Three main factors affect nitrate leaching in these conditions: 1) Drainage timing - about 50% of nitrate loss occurs in May-June, 30% in July-August, and 20% in September-October, with spring months showing higher losses due to excess precipitation. 2) Nitrogen availability - applying optimal N rates rather than too high or too low amounts minimizes leaching while maintaining yields. 3) The uptake efficiency of irrigation water NO3-N is approximately 80% across different NO3-N concentrations and irrigation levels, though this high efficiency requires minimal residual soil NO3-N.	['Field trials show the fertilizer value of nitrogen in irrigation water\nCalifornia Agriculture 71(2):62-67. https://doi.org/10.3733/ca.2017a0010\nPublished online April 12, 2017\nIncreased regulatory activity designed to protect groundwater from degradation by nitrate-nitrogen (NO3-N) is focusing attention on the efficiency of agricultural use of nitrogen (N). One area drawing scrutiny is the way in which growers consider the NO3-N concentration of irrigation water when determining N fertilizer rates. Four drip-irrigated field studies were conducted in the Salinas Valley evaluating the impact of irrigation water NO3-N concentration and irrigation efficiency on the N uptake efficiency of lettuce and broccoli crops. Irrigation with water NO3-N concentrations from 2 to 45 milligrams per liter were compared with periodic fertigation of N fertilizer. The effect of irrigation efficiency was determined by comparing an efficient (110% to 120% of crop evapotranspiration, ETc) and an inefficient (160% to 200% of ETc) irrigation treatment. Across these trials, NO3-N from irrigation water was at least as efficiently used as fertilizer N; the uptake efficiency of irrigation water NO3-N averaged approximately 80%, and it was not affected by NO3-N concentration or irrigation efficiency.\nCalifornia agriculture faces increasing regulatory pressure to improve nitrogen (N) management to protect groundwater quality. Groundwater in agricultural regions, such as the Salinas Valley and the Tulare Lake Basin, has been adversely impacted by agricultural practices, with nitrate-N (NO3-N) in many wells exceeding the federal drinking water standard of 10 mg/L (Harter et al. 2012). The threat to groundwater is particularly acute in the Salinas Valley, where the intensive production of vegetable crops has resulted in an estimated net loading (fertilizer N application – N removal with crop harvest) of > 100 lb/ac (> 112 kg/ha) of N annually (Rosenstock et al. 2014).\nLevels of NO3-N in irrigation wells in the Salinas Valley commonly range from 10 to 40 mg/L. Given the typical volume of irrigation water applied to vegetable fields, NO3-N in irrigation water could represent a substantial fraction of crop N requirements, provided that crops can efficiently use this N source. Indeed, the concept of “pump and fertilize” (substituting irrigation water NO3-N for fertilizer N) has been suggested as a remediation technique to improve groundwater quality in agricultural regions (Harter et al. 2012).\nInexpensive nitrate test strips allow on-farm estimation of irrigation water NO3-N concentration. In Salinas Valley irrigation wells, levels of NO3-N commonly range from 10 to 40 mg/L, which could supply a substantial portion of crop N requirements.\nCooperative Extension publications from around the country (Bauder et al. 2011; DeLaune and Trostle 2012; Hopkins et al. 2007) agree that the fertilizer value of irrigation water NO3-N can be significant, but they differ as to what fraction of water NO3-N should be credited against the fertilizer N recommendation. There is a paucity of field data documenting the efficiency of crop utilization of irrigation water N. Francis and Schepers (1994) documented that corn could use irrigation water NO3-N, but in their study N uptake efficiency from irrigation water was low, which they attributed to the timing of irrigation relative to crop N demand and the availability of N from other sources. Martin et al. (1982) suggested that uptake efficiency of irrigation water NO3-N could actually be higher than from fertilizer N, but their conclusion was based on a computer simulation, not on field trials.\nWith this near total lack of relevant field data, California growers have legitimate concerns about the degree to which irrigation water NO3-N can substitute for fertilizer N. Two questions commonly asked by growers are whether plants can effectively use N at the low concentrations common in irrigation water, and to what degree irrigation inefficiency reduces water NO3-N availability. We undertook this study to document the agronomic value of irrigation water NO3-N in the production of vegetable crops under field conditions representative of the Salinas Valley.\nIrrigation water NO3-N trials\nFour field trials were conducted at the U.S. Department of Agriculture Agricultural Research Service (USDA-ARS) facility near Salinas between 2013 and 2015. The soil was a Chualar sandy loam. Before planting, fields were sprinkler-irrigated to leach residual soil NO3-N so that all trials were conducted with low background soil N availability. The well water used for pre-plant leaching as well as for all in-season irrigation ranged between 2 and 4 mg/L NO3-N over the course of this study. The experimental design for each trial was a randomized complete block, with four replications. Individual plots consisted of four beds, each 40 inches (1 meter) wide and 40 feet (12.2 meters) long, with all data collected from the middle two beds.\nCrisphead lettuce ‘Telluride’ was seeded on May 16, 2013, in two rows per bed and germinated using sprinklers. A soil anticrustant solution containing 17 lb/ac (19 kg/ha) of N was applied to all treatments at planting to improve germination. After plants were thinned to a final in-row spacing of approximately 12 inches (30 centimeters), drip tape was installed on top of the beds and the field was drip-irrigated for the rest of the season.\nCrop growth and N uptake were compared across a range of treatments simulating different irrigation water NO3-N concentrations during the drip-irrigated phase of the crop. The different NO3-N concentrations were achieved by using water-powered proportional injectors to enrich all drip-applied water to 12, 25 or 45 mg/L NO3-N. Injected NO3-N was a blend of Ca(NO3)2 and NaNO3 to maintain a cation balance similar to groundwater (Ca:Na milliequivalent ratio of 1.0). A water sample was collected from each treatment during each irrigation to confirm that the target NO3-N concentrations were achieved. Additionally, an unfertilized control and a fertilized control treatment were included; both were irrigated using water containing only 2 mg/L NO3-N. The fertilized control received five fertigations of ammonium nitrate solution (AN-20) totaling 150 lb/ac (168 kg/ha) of N. Also, all treatments were fertilized with potassium thiosulfate (KTS) in two fertigations of 30 lb/ac (34 kg/ha) of K each.\nCalculating the N in irrigation water\nCalculation of the amount of nitrogen in irrigation water requires knowledge of both the N concentration and the volume of water applied. Laboratory analysis for nitrate in water is commonly reported as milligrams per liter (mg/L) or parts per million (ppm); these units are numerically the same: 1 mg/L equals 1 ppm. Labs may report concentration either as nitrate (NO3−) or nitrate-N (NO3-N); the conversion between the two is\nTo convert NO3-N concentration to mass of N applied, this equation can be used:\nNitrate is usually the only form of N present in irrigation water in an agronomically significant amount, so it is the only N form reported on the typical water test. However, recycled municipal wastewater, which is increasingly being used for irrigation in California, can contain more ammonium N (NH4-N) than NO3-N, as well as some organic forms of N that become relatively quickly available in soil. Wastewater treatment plants routinely test for these other N sources in addition to NO3-N, and this information is publicly available. One should consider all forms of N when estimating the amount of plant-available N in recycled water.\nEach N treatment was evaluated at two levels of irrigation to observe the interaction between irrigation efficiency and crop uptake of irrigation water NO3-N. The lower level of irrigation, 110% of crop evapotranspiration (ETc), was chosen to represent efficient management with minimal leaching. The higher level of irrigation, 160% of ETc, was chosen to represent less efficient irrigation management; we have observed a number of Salinas Valley vegetable fields in which irrigation reached as high as 200% of ETc (Smith et al. 2016). Applying 160% of ETc generated an estimated leaching fraction of 37% (Cahn and Bali 2015). ETc was estimated by multiplying reference evapotranspiration (ETo) values obtained from the CIMIS weather station located on the USDA-ARS facility by crop coefficients calculated by the method described by Johnson et al. (2016). Irrigation was applied twice weekly. Data on ETc and irrigation volume are given in table 1. Precipitation was an insignificant factor, with < 0.2 inches (< 0.5 cm) received in any trial.\nTABLE 1. Inches of crop evapotranspiration (ETc) and irrigation applied during the drip-irrigated portion of the field trials\nA second trial of the same structure was conducted in 2014. Broccoli ‘Patron’ was seeded on Aug. 18 in two rows per bed and germinated with sprinkler irrigation following an anticrustant application containing 23 lb/ac (26 kg/ha) of N. After crop establishment and bed cultivation, the trial was converted to surface drip irrigation. The irrigation levels evaluated were 110% and 190% of ETc. The fertilized control treatment received three fertigations of AN-20 totaling 220 lb/ac (246 kg/ha) of N. All treatments were also fertigated with KTS in two applications of 25 lb/ac (28 kg/ha) of K.\nAn on-site CIMIS weather station provided accurate evapotranspiration data. Each N treatment was evaluated at two levels of irrigation — 110% and 160% of ETc.\nTwo trials were conducted in 2015 to directly compare the uptake efficiency of irrigation water NO3-N to that of fertilizer N. In the spring trial, crisphead lettuce ‘Telluride’ was seeded and germinated as previously described. After converting the field to drip irrigation, four levels of fertigation (a seasonal total of 0, 20, 60 and 150 lb/ac [0, 22, 67 and 168 kg/ha] of N from AN-20, applied in three equal fertigations) were compared at each of two irrigation levels (110% and 180% of ETc). In each irrigation treatment, three concentrations of irrigation water NO3-N (14, 25 and 45 mg/L) without any AN-20 fertigation were also evaluated. In the fall trial, broccoli ‘Patron’ was grown. The treatments were similar to the lettuce trial, with the exception that the seasonal AN-20 fertigation levels were 0, 40, 80 and 200 lb/ac (0, 45, 90 and 224 kg/ha) of N. The irrigation levels evaluated were 120% and 200% of ETc.\nIn all trials, plots were harvested when the highest fertilizer N rate treatment reached commercial maturity. Aboveground fresh and dry biomass and whole-plant N concentration were determined. From these data, crop N uptake was calculated. Uptake efficiency of irrigation water NO3-N was calculated as the increase in crop N uptake above the unfertilized control divided by the amount of NO3-N in the applied water.\nUptake efficiency of NO3-N\nLettuce biomass and crop N uptake increased linearly with increasing irrigation water NO3-N concentration in the 2013 trial (fig. 1). Across the NO3-N enrichment levels, uptake efficiency of irrigation water NO3-N was 85%, and it was similar between the levels of irrigation (which received 7.0 and 10.1 inches [18 and 26 centimeters] of drip irrigation in the 110% and 160% ETc treatments, respectively). The amount of N applied in the 45 mg/L water treatment at 160% of ETc (91 lb/ac, or 102 kg/ha) was sufficient to maximize crop productivity, producing fresh biomass equivalent to the biomass of the fertilized control receiving 150 lb/ac (168 kg/ha) of N from AN-20.\nFig. 1. Effect of irrigation water NO3-N on lettuce biomass and aboveground N uptake, 2013 trial; water NO3-N concentrations were 2, 12, 25 and 45 mg/L.\nResults of the 2014 broccoli trial were similar, with crop biomass and N uptake increasing linearly with increasing irrigation water NO3-N concentration (fig. 2). Uptake efficiency of irrigation water NO3-N was again high (78%) across NO3-N concentrations and irrigation levels. However, given the much higher N requirement of broccoli compared to lettuce, even the 45 mg/L NO3-N water treatment was insufficient to maximize crop productivity.\nFig. 2. Effect of irrigation water NO3-N on broccoli biomass and aboveground N uptake, 2014 trial; water NO3-N concentrations were 2, 12, 25 and 45 mg/L.\nThe 2015 trials clearly demonstrated that irrigation water NO3-N was at least as effectively used by the crop as fertilizer N. The regression lines in figures 3 and 4 indicate the crop response to fertigation with AN-20 at the two levels of irrigation; all regressions were highly significant (p < 0.001). The fact that the irrigation water NO3-N treatments generally placed above the fertilizer response line for their respective irrigation regimes suggested that a higher N uptake efficiency was achieved with irrigation water NO3-N than with N from fertigated AN-20. This was most pronounced in the broccoli trial (fig. 4), where the N uptake efficiency for fertilizer was substantially lower under the high irrigation level (200% of ETc).\nFig. 3. Comparison of lettuce response to N fertilizer (solid and dashed lines) with crop response to irrigation water NO3-N, 2015 trial; water NO3-N concentrations were 14, 25 and 45 mg/L.\nFig. 4. Comparison of broccoli response to N fertilizer (solid and dashed lines) with crop response to irrigation water NO3-N, 2015 trial; water NO3-N concentrations were 14, 25 and 45 mg/L.\nAveraged across all field trials, the N uptake efficiency of irrigation water NO3-N was remarkably high, averaging approximately 80% (fig. 5). Neither NO3-N concentration nor irrigation level significantly influenced N uptake efficiency. It must be noted that the high N uptake efficiency in these trials was attributable to the fact that residual soil NO3-N in these fields had been deliberately minimized by heavy preplant leaching in order to maximize the uptake efficiency of both fertilizer N and water NO3-N. In typical production fields, higher levels of residual soil NO3-N are common, and N uptake efficiency of applied N, whether from irrigation water or fertilizer, would likely be lower.\nFig. 5. Influence of irrigation water NO3-N concentration (mg/L, across irrigation levels) and irrigation level (across water NO3-N concentrations) on the mean N uptake efficiency of irrigation water NO3-N across the four field trials. Bars represent the 95% confidence interval of the measurement.\nCalculating “fertilizer credits”\nThese field trials unequivocally demonstrated that vegetable crops can effectively use NO3-N from irrigation water, even at relatively low concentration. The important question is how can growers safely estimate an appropriate fertilizer credit for irrigation water NO3-N. In answering that question, it is important to distinguish between N uptake efficiency and a fertilizer credit. N uptake efficiency refers to the fraction of applied N taken up by the crop. N uptake efficiency from either fertilizer or irrigation water is affected by overall soil N availability (all sources, including residual soil NO3-N and soil N mineralization); as total N availability increases, N uptake efficiency from either fertilizer or irrigation water will decline. A fertilizer credit is the comparison of the relative availability of N from irrigation water and from fertilizer N.\nDrip irrigation increases irrigation efficiency and simplifies the determination of the “fertilizer credit” for irrigation water NO3-N.\nSeveral factors need to be considered in calculating a fertilizer credit. First, the stability of the irrigation water NO3-N concentration over time is important. In general, surface water sources have reasonably low but stable NO3-N, typically < 5 mg/L. Water districts usually have historical records that provide good estimates of NO3-N concentration for the current season. Nitrate concentration in irrigation wells may be more variable, so periodic monitoring within a growing season may be appropriate. Growers who use several wells of differing NO3-N concentration to irrigate a field would need to monitor the NO3-N concentration of the blended water. This can be accomplished by collecting water in a covered bucket using a drip emitter connected to the irrigation main line; this sample can be tested using nitrate-sensitive colorimetric test strips.\nSecond, it may be necessary to consider irrigation inefficiency when calculating a fertilizer credit, depending on the details of the irrigation management. In this study, drip irrigation was used, with frequent irrigation at relatively low volume, typically < 0.6 inches (1.5 centimeters) per application; even in the high irrigation treatment (160% to 200% of ETc), the volume of leachate from individual irrigations was small. Under these conditions, N uptake efficiency was similar in the high and low irrigation regimes, indicating that the crops were able to remove a substantial amount of NO3-N even from the fraction of applied water that eventually leached. This phenomenon may relate to the residence time of applied water within the active root zone. With low volume leaching events, it may take several irrigation cycles before water moves below the root zone, giving the crop the opportunity to take up applied NO3-N. In a fertigation trial with bell pepper, Scholberg et al. (2009) found that increasing fertilizer retention time from 1 to just 3 days quadrupled fertilizer N uptake efficiency.\nConversely, when irrigation management features large leaching events, particularly early in the season when crop N uptake is slow and before a substantial root system has developed, crop access to and use of irrigation water NO3-N would be limited, and this should be considered in the fertilizer credit calculation. In the context of vegetable production, irrigation to germinate seeded crops or to establish transplants would be particularly vulnerable to inefficiency. It may be appropriate not to credit any of the irrigation water NO3-N applied during crop establishment. Presidedress soil nitrate testing (PSNT, a valuable practice to evaluate N fertilizer requirements; Hartz 2003) would capture any N contribution from irrigation water still in the root zone following establishment. From that point forward, crediting 100% of irrigation water NO3-N against the assumed fertilizer N requirement would be a reasonable practice if in-season irrigation were managed efficiently. Where in-season irrigation results in large leaching events, a smaller fertilizer credit could be justified. However, it should be acknowledged that large leaching events may similarly restrict crop recovery of fertilizer N.\nThese field trials documented that NO3-N in irrigation water is effectively used by crops. Growers can confidently adjust their fertilization practices to reflect the agronomic value of this N source. In doing so they will reduce the potential for N loading to groundwater.', 'By: Jessica Wayment\nNitrate leaching is a major concern in coarse-textured agricultural soils because it can cause economic losses for farmers and contaminate groundwater. While some nitrate leaching may be inevitable when growing corn on sandy soils, there are several management strategies that can be implemented to limit nitrate loss. Here are some key takeaways from a recent five-year study looking at three major factors impacting nitrate leaching: drainage, nitrogen availability, and cropping system.\nDrainageNitrate leaching occurs when rainfall, irrigation, or the two combined exceed the water holding capacity of the soil and the excess water drains below the root zone, carrying nitrate with it. Climate models for Minnesota project a greater likelihood of extreme precipitation events during the spring and summer months. This means that it will become increasingly more challenging to ensure that precipitation and irrigation events do not overlap and exacerbate nitrate leaching. While we cannot do much about the rain, we can manage irrigation by applying only what the crop needs when the weather forecast demands it to minimize drainage. In our study on an irrigated sandy soil at the Rosholt Research Farm in Westport, Minnesota from 2017 to 2021, we observed that 50% of the nitrate loss happened in May and June, 30% in July and August, and 20% in September and October. Nitrate loss was higher during the spring months because precipitation often exceeded crop needs. During the summer months, water and nitrogen use by an actively growing crop minimized the potential for nitrate loss.\nNitrogen availabilityAnother factor that affects nitrate leaching is the amount of nitrogen present in the soil. When nitrogen is applied at the rate suggested by the University of Minnesota’s corn fertilizer guidelines, leaching potential decreases compared to when nitrogen is applied above (or below) the optimal rate. While you would assume that applying nitrogen below the optimum rate would reduce nitrate leaching, our study results showed that this reduced crop growth, water use, and grain yield, which resulted in more water drainage and nitrate leaching. Applying more nitrogen than what the crop needs to optimize growth and yield results in reduced economic returns and more nitrate in the soil, which is available for leaching. Therefore, applying the optimal nitrogen rate is the best strategy to maximize profit and minimize nitrate leaching. An additional way to further reduce nitrate leaching is to split-apply nitrogen during the crop growth stages when the crop’s nitrogen uptake is highest. Previous research clearly show a consistent large yield benefit when split-applying nitrogen. For this reason, in this study we applied 25% of the total nitrogen at V2, 25% at V6, and the remaining 50% at V10.\nCropping SystemSoils naturally convert organic nitrogen in the organic matter into inorganic nitrogen through mineralization. In irrigated sandy soils, this pool of nitrogen is much lower than other soil types and is not sufficient for corn production, so additional nitrogen inputs are needed to meet the crop’s nutritional demands. However, we learned from this study that nitrogen fertilizer is not the only factor influencing nitrate leaching. Annual cropping systems grown in sandy soils are simply “leaky.” The amount of nitrate leaching during the soybean year of a corn-soybean rotation, when no nitrogen fertilizer is applied, is similar to nitrate leaching observed during the corn year. Similarly, nitrate leaching was roughly the same in our continuous corn system compared to the corn-soybean rotation (even though, over two growing seasons, the amount of nitrogen applied at the economic optimum nitrogen rate for continuous corn is more than double the rate for corn following soybean).\nCover cropsUsing cover crops may be a way to fix a leaky annual cropping system and improve soil health, but unfortunately cover crops are not a silver-bullet solution to our water quality problems. Two cover crops that have shown potential to reduce nitrate leaching are winter rye and kura clover. Rye is a winter-hardy annual grass and is one of the most common cover crops grown in Minnesota, and it grows well in sandy soils. It is typically planted in the fall and terminated in the spring before planting the cash crop. Kura clover is a perennial winter-hardy legume that is intercropped as a living mulch with strip-till after one to two years of fallow for establishment.\nA well-established stand of rye can act as a nitrogen “scavenger” to reduce nitrate leaching while no cash crop is actively growing. Rye is considered low risk compared to other cover cropping systems because it has lower seed and management costs, and there is typically little impact on grain yield. There are two keys for proper rye management:\nGet a good stand establishment in the fall: Planting rye as early as possible and ensuring good seed-to-soil contact are the best strategies to help get your rye cover crop off to a good start.\n- Don’t terminate the rye too late in the spring: The timing of rye termination in the spring is crucial because you want the nitrogen that the rye scavenged to be available in time for uptake by the cash crop. You need to give the terminated rye residue enough time to decompose and release the nitrogen back into the soil before the cash crop needs it.\nPost a comment\nReport Abusive Comment']	['<urn:uuid:d2775de1-7603-44a1-bf72-d1c2327f6e10>', '<urn:uuid:cfdd75e7-4233-40c7-a7b6-4e0eaec1ca72>']	factoid	with-premise	long-search-query	similar-to-document	three-doc	expert	2025-05-13T02:03:54.148118	19	86	3803
81	As an aircraft mechanic working on both radial and racing engines, I'm curious about the relationship between push rod installation and connecting rod failures - what are the proper installation procedures for both, and what are the potential consequences of improper installation?	For push rods, proper installation requires following specific procedures. In opposed engines with hydraulic lifters, the correct push rod length must be selected to maintain proper running clearance between valve stems and rocker arms. For radial engines like Pratt & Whitney models, which have solid lifters, push rod length is adjusted using different thickness washers under the push rod socket. A special tool is required for installing push rod sockets, and incorrect interference fit can cause socket detachment and subsequent damage. For connecting rods, the critical factor is proper bolt installation through measuring bolt stretch rather than just torque. The correct stretch measurement is around 0.006 inches, which equals approximately 9,600 pounds of clamping load. Improper installation in either case can lead to catastrophic failures - push rod problems in radial engines can cause valve failures leading to engine shutdown in flight, while incorrect rod bolt installation can result in expensive rebuilds due to rod failures.	['Both radial and opposed engines must have the proper running clearance between the valve stem and the rocker arm. Most opposed engines have hydraulic lifters therefore the correct push rod length must be selected to maintain the proper running clearance. At overhaul, the correct push rod length is installed for each cylinder. When a cylinder is changed in the field, the replacement cylinder may have a different valve seat depth or valve seat width than the cylinder removed therefore it may be necessary to change the push rod in order to get the correct clearance. It is very important to collapse the hydraulic lifters by bleeding all the oil out, then check the clearance and select the proper push rod by part number. Before setting the valves, be sure that the proper torque and tightening sequence was used during the cylinder installation.\nPratt and Whitney R985, R1340 and R1830 radial engines are the same principle but have solid lifters therefore have an adjusting screw and lock nut on the rocker arm. Different push rod lengths are also required on these engines only the push rod length is adjusted by adding or removing different thickness washers from under the push rod socket. This procedure is also done during overhaul to obtain the correct valve clearances. If the rods are the correct length, the adjusting screw should also be within limits, no less than 2 threads and no more than 5 threads showing on the end of the adjusting screw after the lock nut has been tightened.\nCAUTION There is a removal and inspection procedure and a special tool is required for installing a push rod socket. The installation of the push rod socket without the correct interference fi t can cause damage to the push rod and socket. In some instances the socket has come off. An improperly installed socket causes the rocker arm valve adjustment screw to be out of limits or worse yet, allowing the jam nut on the adjusting screw to come loose and pound a hole in the rocker cover. After adjusting the valve clearance to the proper limits, make sure you have at least 10 thou clearance between the valve spring outer washer and the rocker arm even though you have clearance between the valve stem and the rocker adjustment screw. We have seen evidence where the rocker arm comes in contact with the valve spring washer putting pressure to one side of the valve and valve keepers causing the keepers and valve stem to wear until the valve and valve guide fails. When this happens, the valve breaks loose and pounds a hole in the piston and usually exits through the cylinder head. This also causes damage to the link rod as well as filling the engine with metal. Radial engines will quit in flight as a result of a broken push rod, rocker arm or a loose or broken adjustment screw. If this occurs in any one cylinder, the valves won’t open, especially the exhaust valve. The combustion fi res back through the intake into the supercharger, disturbing the fuel distribution to the rest of the cylinders. Any pilot that has had this experience will tell you that it is almost impossible to keep the engine running. By close inspections and working to the proper manuals and table of limits this type of failure can be avoided.', 'How to eliminate connecting rod failures\nEngine failures that many racers blame on the connecting rods are often actually installation mistakes. When their characteristics are correctly understood, connecting rods can be one of the strongest parts of a modern racing engine.\nLet’s do a quick mental exercise: How many times-either at the track or while watching television-have you heard a driver or crewchief explain away a blown engine with the phrase, “we broke a rod”?\nPlenty of times, probably. But what does that mean? Connecting rods built for racing are manufactured with as much or more precision (and expense) as any other piece that goes in the engine. Rods are often the most stressed pieces in an engine, yet many Winston Cup engine builders will tell you they have rarely seen a rod failure that is truly the fault of the rod.\nThe fault can come from any number of places, but the failure often occurs at the rod because of the stresses involved. Often, the fault from an early rod failure comes from sheer human error. Simple things such as improper rod bolt installation, rough handling, or over-cycling the rods can lead to expensive rebuilds. Fortunately, these mistakes are easy to prevent if you have a solid understanding of what types of forces you expect your connecting rods to live through (horsepower, reciprocating weight, expected life cycle, etc.) and how to install them properly.\nSince almost all forms of stock car racing limit rods to stainless steel in the interest of cost containment, this article will concentrate only on processes involving that material. For stainless rods, there are several options for construction, namely forged, powder forged, billet, and fully machined.\nPowder-forged rods are used almost exclusively in OEM applications. Powder forging allows for a cheap, fairly rugged rod, but it isn’t the quality of a standard forging. You will be hard-pressed to find a purpose-built racing rod that has been powder forged. However, if you are racing a strictly stock class that doesn’t allow aftermarket rods, this might be what you are stuck with. As long as they are balanced, you aren’t likely to exceed a powder-forged rod’s horsepower limitations in a stock engine.\nNext up the durability scale are forged rods. “The advantage of forging is that steel, just like wood, has a grain direction,” explains Jack Sparks of Carrillo Industries. “When put in the proper position, it enhances the fatigue properties of the part. Forging a rod allows you to manipulate the grain direction in the steel. The optimum is to have longitudinal grain flow (up and down) in the blade area and cylindrical grain flow around the big end.”\nThe problem with a forged rod also comes from the act of forging. The forging process leaves an inconsistent, rough surface on the rod. This roughness creates a multitude of stress points on the rod. “If you disregard the fasteners, most failures in a connecting rod are generated from some sort of inclusion or stress riser on the surface,” Sparks explains. “It’s like putting a notch in a coat hanger. When you bend it, it is going to break at the notch.”\nA billet rod is created by literally machining the rod out of a solid chunk of metal. The machining process creates a smooth surface that is (unless there is a problem in the manufacturing process or simply a bad design) free of surface inclusions. By avoiding the forging process altogether you have avoided a weakness, but you haven’t been able to take advantage of strength by influencing the grain flow.\nEnter what is commonly known as the “fully machined” rod. This process produces the strongest rod (gauging material strength only) by taking the best traits of the forging and billet processes and combining them. A fully machined rod starts out as a large forging (over six pounds in the case of most of Carrillo’s rods) and then is machined into its final form much like a billet rod. This produces a rod that has the optimum grain structure of a forging along with the smooth exterior that you find in billet pieces. It’s the best of both worlds.\nFasteners, no matter what type, are the greatest weakness in a connecting rod. However, if installed correctly, most fasteners from quality manufacturers are capable of handling the stresses they are designed for. Before we talk about correct installation, we first need to have an understanding of what’s out there.\nThe fasteners used to hold the two pieces of the big end of the rod together come in two designs. Thru-bolt designs have a complete bolt and nut to clamp the rod together. A cap-screw design eliminates the nut, instead utilizing threads in the rod for the bolt to thread into. A thru-bolt design requires flat faces to be cut into the big end of the rod for each bolt (one for the head of the bolt and one for the nut). Eliminating the flat for the nut makes the cap screw that much stronger. Additionally, threading the bolt directly into the body of the rod also helps rigidity.\nNow, it’s time for the good stuff. Sparks says the most common reason for rod bolt failures is failing to install the rod bolts so that they achieve the proper clamping load. That’s the complicated way to say “aren’t torqued right,” but that can be misleading because it’s the use of a torque wrench that causes most of the problems.\nThe only way to make sure a bolt is exerting the proper clamping load is to measure stretch, not torque. On this point, Sparks can get downright militant. “Carrillo and every other quality rod manufacturer strongly suggest that the bolt is installed by stretch,” he says. “Using a torque wrench only measures friction, and there are a lot of variables that can cause you to get the torque reading you are looking for without achieving the stretch that you need.\n“I get calls all the time from people trying to get by with just torquing the rod bolts and asking what lube to use or what torque number they should look for with a particular lube. I tell them they need to be checking stretch, and then it won’t matter if they are using peanut butter as a lubricant.”\nOf course, Sparks admits that peanut butter makes a much better sandwich than it does a lubricant, but he makes his point. Take, for example, moly-based lubes, which are most commonly used for rod bolts. Moly lubes have a metallic base. It’s good because it doesn’t come off easily, but that also means that every time you pre-fit the rod bolts you are polishing those threads, and every time you add another coat of moly lube, you are also making those threads progressively slicker. If you use the same torque number throughout an engine build, the amount of stretch on the rod bolts will be less every time the rods are assembled.\n“Rod bolts are designed very carefully, so we have a pretty good idea after a multitude of tests that around 0.006 stretch equals approximately 9,600 pounds of clamping load. You have a lot of variables when assembling your connecting rods. They include the actual torque wrench, the lubricant, quality of mating surfaces between the bolt and the rod, and even the operator. If you try to go only by torque, you have to take all of these factors into account, but if you measure bolt stretch, all of these variables are suddenly not a part of the equation.”\nThe lubricant’s main job is to keep the bolts from galling in the rods so the engine can be easily disassembled when it is time for a rebuild. Make sure you use a lubricant that doesn’t wipe off easily or boil away under heat, and use it to coat both the threads and the underside of the head of the bolt.\nMany engine builders try to get away with assembling the connecting rods with only a torque wrench for one simple reason: It can be a pain to measure rod bolt stretch.\nThe most critical time to measure stretch is on final assembly, but that is when the rods are located in the cylinder bores and being bolted to the crank. It is easy to measure stretch when the rods are sitting on the work bench; it’s not quite so easy when they are in the block.\nSparks understands this and offers a practical solution for the engine builder. As you build an engine, you will have the rods together and back apart several times. As you do this, you will be polishing the threads on both the bolt and rod, making all more consistent. Keep track of your stretch and torque numbers to make sure that all the bolts torque to the same number. Now, when you are ready for final assembly, you know that all your bolts stretch at the same torque number. Check the stretch on one set of rod bolts and mark your torque reading. You can safely torque the rest of the bolts to that number and be assured you have the correct amount of stretch.']	['<urn:uuid:b8146df9-8abf-4585-afd8-d9e3b24ce1ca>', '<urn:uuid:b7626c1f-34a3-4af2-be32-e4bec4bb71c8>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T02:03:54.148118	42	156	2094
82	I'm learning to use English shots, will low deflection help?	Low deflection cues are particularly helpful for English shots as they require less compensation in aim, especially on longer shots. They are more forgiving of minor off-center hits and make it easier for players to learn and practice sidespin technique. However, while they can enhance learning and practice, they should be viewed as a tool to improve technique rather than a magic solution. The effectiveness ultimately depends on the player's fundamental skills.	['While you likely won’t find one in the assortment of house cues on the wall of your favored pool hall, low deflection (LD) pool cues are highly popular among experienced players. You may have even seen a pool shark or two showing off the shiny new LD cue they just bought. If you’re wondering what the fuss is about – or even what distinguishes a low deflection cue from a standard one – you’re not alone. One of the most common questions asked by novice players is, “What is a low deflection pool cue?”\nA low deflection cue is any pool cue specifically designed to reduce or minimize ‘squirt’ – divergence of the cue ball away from the stick line. Low deflection cues accomplish this by virtue of their lighter, more flexible shafts. This allows for greater accuracy, particularly with shots involving ‘English’ or sidespin.\nWhether you’re in the market for a new cue and want to know if low-deflection is the right choice, or you’re simply curious about how they compare with standard cues, you’ve come to the right place. Read on to find all the information you need about low deflection pool cues, how different types are made, and how they can impact your game for the better.\nUnderstanding Cue Ball Squirt\nCue ball squirt (often referred to as deflection), as noted above, is when the path of the cue ball deviates from the path of the cue stick after a stroke. While some level of squirt occurs on every shot, its effects are most pronounced (and most troublesome) when English or spin is applied.\nWith English Comes Deflection\nStriking the cue ball straight-on will produce so little squirt that it will have virtually no effect on your shot. Striking the cue ball even slightly to the left or right of the vertical axis can yield enough squirt to blow your shot or miss the object ball entirely. This is especially true on longer shots, as the initial angle deviation will only widen as the cue ball travels along the table.\nStandard cues generate significantly more squirt than do low deflection cues, primarily due to their higher mass near the tip, or “end-mass.” Players using standard cues simply need to adjust their aim more to compensate for squirt. More on that later – for now, let’s focus on the mechanics of squirt.\nSome find it helpful to compare squirt to the crosswinds an archer or marksman has to contend with and account for. The dynamic is similar in terms of the overall effect on the projectile and the compensation required to offset it. But unlike crosswind, squirt is a result of the shot itself.\nWhat Causes Squirt?\nWhen the tip of a cue stick strikes a cue ball off-center (as when imparting English), a small sideways force is exerted on the cue ball, causing it to deflect, or ‘squirt’ at an angle away from the stick line. If the cue ball is struck right of center, it will squirt left; if struck left of center, it will squirt right.\nAs the cue tip impacts the cue ball and initiates both the forward trajectory and the spin of the ball, the tip remains in contact with the ball for a tiny fraction of a second. During this time, the spinning cue ball pushes against the cue tip. Since, as Newton discovered, every action has an equal and opposite reaction, the cue tip pushes back – producing the sideways force that results in squirt.\nBy this token, a cue stick with more end-mass will exert a greater sideways force on the cue ball. The stiffness of the cue shaft will also contribute to this sideways force. This brings us to low deflection cues.\nHow Do Low Deflection Cues Work?\nLow deflection pool cues are designed to have less mass and more flexibility at the top of the shaft, thereby reducing the amount of lateral force exerted on the cue ball during an off-center stroke. In short, low deflection cues work by minimizing the factors that cause cue ball squirt – namely end-mass and shaft stiffness.\nThe benefit of added flexibility is twofold. Firstly, it allows the tip to more easily flex away from the side of the cue ball on impact. Secondly, it limits the amount of “effective mass” engaged in the lateral energy exchange. The stiffness of a standard cue allows more of the shaft’s mass to be involved in the sideways force, creating more squirt. A flexible cue restricts the effective mass to the top several inches of the shaft, reducing squirt.\nNow, let’s take a look at the various ways cue stick manufacturers go about reducing end-mass and increasing flexibility.\nTypes of Low Deflection Cues\nThere are several methods used for reducing a cue’s end-mass. Some are more effective than others and performance cue makers typically offer a range of options. In general, you get what you pay for in terms of cue quality and squirt reduction. We’ll start with the most basic methods and work our way up to the high-end, lowest-deflection cue designs.\nModified Ferrule – Using a shorter ferrule helps cut down on end-mass to some degree. Some designs also feature ferrules made of wood rather than the plastic found on standard cues. This is because wood is lighter and has less mass than plastic. Many ferrules feature a small cushion disc to further absorb stroke impact.\nSmaller Tip/Skinny Shaft Taper – Most LD cues have tips with a diameter of between 11.75mm and 12.75mm. Naturally, the skinnier the top of the shaft is, the less end-mass it has – and the less squirt it will cause. However, the narrower tip makes it easier to commit inaccuracy errors like accidental English or miscues. If you’re looking to expose flaws in your accuracy, this might be helpful! But for most players, this isn’t the preferred solution to the problem of squirt.\nHollow Shaft – One common design proven to significantly reduce squirt is the hollowed-out shaft. A hole is drilled 5 or 6 inches into the top of the shaft, drastically lowering the end-mass and giving the shaft a degree of flexibility. The first-ever LD shaft on the market, created in the mid-90s by Predator (then called Clawson Cues) utilized this design. This game-changing shaft, the Predator 314, is still widely used today. Here’s the latest generation model – the Predator 314-3.\nLighter Shaft Core – Some cue makers hollow out all or a portion of the shaft and refill the core with lightweight materials like balsa wood, foam rubber, or carbon fiber. This reduces end-mass but also helps absorb impact and increase durability. One tried-and-true example of such a cue is this Lucasi Sneaky Pete.\nCarbon Fiber Shaft – More and more cue manufacturers are constructing shafts entirely from lightweight carbon fiber. This gives them a built-in advantage over heavier solid-maple cue shafts. Regardless, carbon fiber shafts still often feature hollow tips or foam cores to cut down on end-mass. Carbon fiber shafts cost more to manufacture, so tend to be a little more expensive than other LD shafts. This Cuetec Cynergy CT-15K is a mid-range example.\nBenefits & Drawbacks of Low Deflection Cues\nIf you’re looking to expand your game into the world of English, or if you’re having trouble utilizing spin without sacrificing accuracy, buying an LD cue may seem like a no-brainer. But it may not be the best fit for every player. Here are some of the biggest advantages and drawbacks of LD cues.\nImproved Accuracy – Shots with English require less compensation in aim. This eliminates much of the guesswork and simplifies the geometry of longer shots.\nEasier Skill Development – The increased accuracy makes it easier for newer players to learn and practice sidespin technique.\nHigh-Quality Construction – Like other performance cues, LD cues are generally well-made and tend to be durable and long-lasting.\nMore Forgiving – Unless you choose a cue with a very small tip (11.75mm or smaller), minor errors in aim, such as slightly off-center hits, won’t have as much of an effect on accuracy.\nCost – Because they’re specially designed, LD cues tend to be a little pricey, typically ranging from about $200-$500. Some high-end options are much more expensive. However, a quality LD cue can last for years if properly cared for, so it may be worth the investment.\nLess Effective Jumping – Due to their low end-mass, LD cues are not ideal for jump shots. You’ll have to apply more stroke speed to compensate for the lack of mass, which can potentially hinder accuracy. The same principle applies to breaking. This is why many serious players also purchase a dedicated jump-break cue.\nFeel – This isn’t a mark against LD cues themselves, but it’s worth mentioning. Whether it’s the lighter shaft or the sensation of hollowness on impact, some players simply don’t like the feel of LD cues. If you’re used to the solid, heavy feel of a standard cue, this may throw you off.\nDo Pros Use Low Deflection Shafts?\nMany professional pool players swear by low deflection cues, but this is more a matter of preference than anything else. The choice of a cue is a very personalized decision and most pros, while they have the skills to play well with any cue, tend to use whatever they’re most comfortable with.\nYes, lots of pros use LD shafts, but plenty still win tournaments using traditional solid-maple designs or cues they’ve had custom built.\nWill a Low Deflection Cue Help Me Play Better Pool?\nA good low deflection cue is a worthwhile investment if you’re looking to expand your game and play at a higher level – but don’t expect a cue to do all the work for you. The surest path to playing better pool is to play a lot of pool. That being said, an LD cue is an excellent learning tool and can help take much of the frustration out of practicing complicated shots.\nIf you’re a beginner or intermediate player and you’re looking to purchase your first cue, an LD cue is a good place to start. But if you can, find another player with an LD cue and ask to try it out for a day.\nIf it makes little or no difference, if it hinders your performance, or if you simply don’t like the way it feels, you’ll know you can stick with a standard cue and be just fine.\nLow deflection pool cues can help you to be more consistent with your aim and improve your overall accuracy by reducing cue ball squirt. This especially comes in handy for distance shots and more precise cue ball control on shots involving English.\nAs such, LD shafts are very valuable for players itching to develop their sidespin skills and technique. However, the efficacy of an LD cue ultimately comes down to how well you play to begin with, so it should be viewed as a tool to enhance learning and practice – not a magic bullet.']	['<urn:uuid:aa3bd392-db80-444d-83c2-e2b358aedf6f>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-13T02:03:54.148118	10	72	1840
83	I'm looking into public support for different revolutions. Was there a difference between how much popular backing the Paris Commune had compared to the Decembrist uprising in Russia?	Yes, there was a clear difference in popular support. The Paris Commune demonstrated significant proletarian unity, being a 'splendid example of unanimity' among workers despite the proletariat being split into numerous sects. In contrast, the Decembrist uprising failed specifically because of 'lack of mass support' - some plotters defected, others failed to carry out their assignments, and they were unable to gain broader popular backing for their cause.	"['Zagranichnaya Gazeta, No. 2 March 23, 1908.\nPublished according to the text in Zagranichnaya Gazeta.\nSource: Lenin Collected Works, Progress Publishers, 1972, Moscow, Volume 13, pages 475-478.\nTranslated: Bernard Isaacs\nTranscription\\Markup: R. Cymbala\nPublic Domain: Lenin Internet Archive (2004). You may freely copy, distribute, display and perform this work; as well as make derivative and commercial works. Please credit “Marxists Internet Archive” as your source.\nOther Formats: Text • README\nAfter the coup d état, which marked the end of the revolution of 4848, France fell under the yoke of the Napoleonic regime for a period of 18 years. This regime brought upon the country not only economic ruin but national humiliation. In rising against the old regime the proletariat under took two tasks—one of them national and the other of a class character—the liberation of France from the German invasion and the socialist emancipation of the workers from capitalism. This union of two tasks forms a unique feature of the Commune.\nThe bourgeoisie had formed a “government of national defence” and the proletariat had to fight for national independence under its leadership. Actually, it was a government of “national betrayal” which saw its mission in fighting the Paris proletariat. But the proletariat, blinded by patriotic illusions, did not perceive this. The patriotic idea had its origin in the Great Revolution of the eighteenth century; it swayed the minds of the socialists of the Commune; and Blanqui, for example, undoubtedly a revolutionary and an ardent supporter of socialism, could find no better title for his newspaper than the bourgeois cry: “The country is in danger!”\nCombining contradictory tasks—patriotism and socialism—was the fatal mistake of the French socialists. In the Manifesto of the International, issued in September 1870, Marx had warned the French proletariat against being misled by a false national idea; the Great Revolution, class antagonisms had sharpened, and whereas at that time the struggle against the whole of European reaction united the entire revolutionary nation, now the proletariat could no longer combine its interests with the interests of other classes hostile to it; let the bourgeoisie bear the responsibility for the national humiliation—the task of the proletariat was to fight for the socialist emancipation of labour from the yoke of the bourgeoisie.\nAnd indeed the true nature of bourgeois “patriotism” was not long in revealing itself. Having concluded an ignominious peace with the Prussians, the Versailles government proceeded to its immediate task—it launched an attack to wrest the arms that terrified it from the hands of the Paris proletariat. The workers replied by proclaiming the Commune and civil war.\nAlthough the socialist proletariat was split up into numerous sects, the Commune was a splendid example of the unanimity with which the proletariat was able to accomplish the democratic tasks which the bourgeoisie could only proclaim. Without any particularly complex legislation, in a simple, straightforward manner, the proletariat, which had seized power, carried out the democratisation of the social system, abolished the bureaucracy, and made all official posts elective.\nBut two mistakes destroyed the fruits of the splendid victory. The proletariat stopped half-way: instead of setting about “expropriating the expropriators”, it allowed itself to be led astray by dreams of establishing a higher justice in the country united by a common national task; such institutions as the banks, for example, were not taken over, and Proudhonist theories about a “just exchange”, etc., still prevailed among the socialists. The second mistake was excessive magnanimity on the part of the proletariat: instead of destroying its enemies it sought to exert moral influence on them; it underestimated the significance of direct military operations in civil war, and instead of launching a resolute offensive against Versailles that would have crowned its victory in Paris, it tarried and gave the Versailles government time to gather the dark forces and prepare for the blood-soaked week of May.\nBut despite all its mistakes the Commune was a superb example of the great proletarian movement of the nineteenth century. Marx set a high value on the historic significance of the Commune—if, during the treacherous attempt by the Versailles gang to seize the arms of the Paris proletariat, the workers had allowed themselves to be disarmed without a fight, the disastrous effect of the demoralisation, that this weakness would have caused in the proletarian movement, would have been far, far greater than the losses suffered by the working class in the battle to defend its arms. The sacrifices of the Commune, heavy as they were, are made up for by its significance for the general struggle of the proletariat: it stirred the socialist movement throughout Europe, it demonstrated the strength of civil war, it dispelled patriotic illusions, and destroyed the naïve belief in any efforts of the bourgeoisie for common national aims. The Commune taught the European proletariat to pose concretely the tasks of the socialist revolution.\nThe lesson learnt by the proletariat will not be forgotten. The working class will make use of it, as it has already done in Russia during the December uprising.\nThe period that preceded the Russian revolution and prepared it bears a certain resemblance to the period of the Napoleonic yoke in France. In Russia, too, the autocratic clique has brought upon the country economic ruin and national humiliation. But the outbreak of revolution was held back for a long time, since social development had not yet created the conditions for a mass movement and, notwithstanding all the courage displayed, the isolated actions against the government in the pre-revolutionary period broke against the apathy of the masses. Only the Social-Democrats, by strenuous and systematic work, educated the masses to the level of the higher forms of struggle—mass actions and armed civil war.\nThe Social-Democrats were able to shatter the “common national” and “patriotic” delusions of the young proletariat and later, when the Manifesto of October 17th had been wrested from the tsar due to their direct intervention, the proletariat began vigorous preparation for the next, inevitable phase of the revolution—the armed uprising. Having shed “common national” illusions, it concentrated its class forces in its own mass organisations—the Soviets of Workers’ and Soldiers’ Deputies, etc. And notwithstanding all the differences in the aims and tasks of the Russian revolution, compared with the French revolution of 1871, the Russian proletariat had to resort to the same method of struggle as that first used by the Paris Commune—civil war. Mindful of the lessons of the Commune, it knew that the proletariat should not ignore peaceful methods of struggle—they serve its ordinary, day-to-day interests, they are necessary in periods of preparation for revolution—but it must never forget that in certain conditions the class struggle assumes the form of armed conflict and civil war; there are times when the interests of the proletariat call for ruthless extermination of its enemies in open armed clashes. This was first demonstrated by the French proletariat in the Commune and brilliantly confirmed by the Russian proletariat in the December uprising.\nAnd although these magnificent uprisings of the working class were crushed, there will be another uprising, in face of which the forces of the enemies of the proletariat will prove ineffective, and from which the socialist proletariat will emerge completely victorious.\n The article “Lessons of the Commune” published in Zagranichnaya Gazeta (Foreign Gazette), No. 2, March23, 1908 is the verbatim report of a speech made by Lenin. The editors of the newspaper introduced the article with the following remark: “An international meeting was held in Geneva on March 18 to commemorate three proletarian anniversaries: the twenty-fifth anniversary of the death of Marx, the sixtieth anniversary of the March revolution of 1848, and the anniversary of the Paris Commune. Comrade Lenin on behalf of the R.S.D.L.P. spoke at the meeting on the significance of the Commune.”\nZagranichnaya Gazeta—a newspaper published by a group of Russian emigrants in Geneva in March-April 1908.\n See K. Marx and F. Engels, Selected Works, Vol. 1, 1958, p. 497.\n For Marx’s evaluation of the historical role of the Paris Commune, as a forerunner of the new society, see The Civil War In France (K. Marx and F. Engels, Selected Works, Vol. 1, 1958, pp. 473-545) and letters to Kugelmann for April 12 and 17, 1871 (K. Marx and F. Engels, Selected Correspondence, Moscow, pp. 318-20).\n This refers to the Manifesto of October 17th, 1905 in which the tsar, frightened by the revolution, promised the people civic liberties and a constitution.', '(1796–1843), army officer who conspired to over-throw Nicholas I.\nNikita Muraviev was one of the army officers involved in the Decembrist movement to overthrow Tsar Nicholas I. He is best known for the constitution he drafted for a new Russian state. Although he did not actually participate in the uprising on December 14, 1825, he was condemned to death when it failed. His sentence was later commuted to twenty years at hard labor in the Nerchinsk mines. He died in Irkutsk Province.\nIn 1813, after studying at Moscow University, Muraviev embarked on a military career, and in 1816 he joined with other aristocratic young officers in organizing a secret society called the Union of Salvation. Led by Paul Pestel, it was renamed the Union of Welfare a year later. Stimulated by the French Revolution (1789) and the Napoleonic Wars (1812–1815), the officers had been influenced by the liberal ideas of French and German philosophers while serving in Europe or attending European universities. The new Russian literature, with its moral and social protest against Russia\'s backwardness, also was an important influence, especially the works of Nikolai Novikov, Alexander Radishchev, and the poets Alexander Pushkin and Alexander Griboyedov. The Arzamas group, an informal literary society founded around 1815, attracted several men who later became Decembrists, including Nikita Muraviev, Nikolai Turgenev, and Mikhail Orlov.\nEconomic stagnation, high taxation, and the need for major reforms motivated Muraviev and the other Decembrists to take action. They advocated the establishment of representative democracy but disagreed on the form it should take: Muraviev favored a constitutional monarchy; Pestel, a democratic republic. To get rid of tsarist agents and members who were either too dictatorial or too conservative, the organizers dissolved the Union of Welfare in 1821 and set up two new groups: The Northern Society, centered in St. Petersburg, was headed by Muraviev and Nicholas Turgenev, an official in the Ministry of Finance. The more radical Southern Society was dominated by Pestel. During the interregnum between Alexander I and Nicholas I, the two societies plotted the coup.\nMuraviev was the ideologist for the Northern Society, drafting propaganda and a constitution that was found among his papers following his arrest. The uncompleted constitutional project reveals the strong impact of the American constitution. Like Pestel, he envisioned a republic: ""The Russian nation is free and independent. It cannot be the property of a person or a family. The people are the source of supreme power. And to them belongs the sole right to formulate the fundamental law."" Muraviev advocated a constitutional monarchy along the lines of the thirteen original states of North America, separation of powers, civil liberties, and the emancipation of the serfs. Although his constitution guaranteed the equality of all citizens before the law, the landed classes were recognized as having special rights and interests. Thus Muraviev rejected Pestel\'s idea of universal suffrage; only property-holders would be allowed to vote and to seek elective office.\nWhat distinguishes Muraviev\'s draft constitution is its advocacy of federalism, an idea not echoed by any major political movement in Russia until the twentieth century. Muraviev argued that ""vast territories and a huge standing army are in themselves obstacles to freedom."" Too much of a nationalist to call for the breakup of the empire, however, Muraviev urged that Russia adopt a federalist system as a way to reconcile ""national greatness with civic freedom.""\nThe Decembrist uprising failed because of the plotters\' incompetence and lack of mass support. Some defected, and others, at the last minute, failed to carry out their assignments. Five of their leaders, including the poet Kondraty Ryleyev, were executed. Despite the stricter censorship Nicholas I imposed after the crushed rebellion, the memory of the Decembrists inspired many writers and revolutionaries, especially the political refugee Alexander Herzen, who established the journal The Bell (Kolokol ) in London in 1857 to ""propagate free ideas within Russia.""\nSee also: decembrist movement and rebellion; empire, ussr as; nicholas i\n""Muraviev, Nikita."" Encyclopedia of Russian History. . Encyclopedia.com. (February 17, 2019). https://www.encyclopedia.com/history/encyclopedias-almanacs-transcripts-and-maps/muraviev-nikita\n""Muraviev, Nikita."" Encyclopedia of Russian History. . Retrieved February 17, 2019 from Encyclopedia.com: https://www.encyclopedia.com/history/encyclopedias-almanacs-transcripts-and-maps/muraviev-nikita\nEncyclopedia.com gives you the ability to cite reference entries and articles according to common styles from the Modern Language Association (MLA), The Chicago Manual of Style, and the American Psychological Association (APA).\nWithin the “Cite this article” tool, pick a style to see how all available information looks when formatted according to that style. Then, copy and paste the text into your bibliography or works cited list.\nBecause each style has its own formatting nuances that evolve over time and not all information is available for every reference entry or article, Encyclopedia.com cannot guarantee each citation it generates. Therefore, it’s best to use Encyclopedia.com citations as a starting point before checking the style against your school or publication’s requirements and the most-recent information available at these sites:\nModern Language Association\nThe Chicago Manual of Style\nAmerican Psychological Association\n- Most online reference entries and articles do not have page numbers. Therefore, that information is unavailable for most Encyclopedia.com content. However, the date of retrieval is often important. Refer to each style’s convention regarding the best way to format page numbers and retrieval dates.\n- In addition to the MLA, Chicago, and APA styles, your school, university, publication, or institution may have its own requirements for citations. Therefore, be sure to refer to those guidelines when editing your bibliography or works cited list.']"	['<urn:uuid:2ae5f71f-d9e3-474c-8849-70ec20b893f7>', '<urn:uuid:1e4c2154-3004-41ee-b3be-205f0bfa1817>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T02:03:54.148118	28	68	2306
84	My mechanic says I need Stage 1 heads. Are they special?	Stage 1 heads are not special castings - they are just regular heads of that year with larger valves installed. Any BBB (big block Buick) head can be converted into a 'Stage 1 head'.	['If you are questioning what year motor to start with for your big block Buick (BBB) build-up, this thread is for you. (If you are looking for a ’70 SF motor to build “because it has 370 HP,” you REALLY need to read this thread!) I’ll cut to the chase and give you the answer right off. With one major exception, it really does not matter. The exception is the 75-76 heads. They are junk, due to their compression-killing open combustion chambers. Everything else is 95% comparable between every BBB ever made. If you are concerned about factory horsepower levels, the fact is that the iron “hard parts” (block, crank, rods, heads, and manifolds—the stuff you might re-use in a rebuild) of a BBB have very, very little to do with the power output of the engine. Now, as Paul Harvey would say, here is the rest of the story… Crank and rods All BBB cranks and rods are identical, equal, and interchangeable. Blocks 1967 through early 71 blocks have a smaller oil pick-up passage. This is easily drilled out during machine work. 75-76 blocks are slightly heavier (about 10 lbs.) For anything under 500HP, this is just dead weight. And there is only a small horsepower window between where using a 75-76 block would be beneficial before you get to the point where you need a girdle no matter what. If they pass a sonic check, 400-430 blocks CAN be bored out to 455, but you better send your machinist a Christmas card after that. Heads As stated, 75-6 heads are junk, due to the open combustion chambers. For the drop in compression between 70 and 71, most of the change was done through a deeper dished piston. The 71-74 heads have slightly larger combustion chambers, but not by much. They flow just a well as earlier heads. The emissions passages in 72-up heads do not affect flow. 67 and early-68 heads are known as “big port heads,” due to slightly larger intake passages. Untouched, they do flow a little better than later heads, but if you are doing any porting, it is just a “head start” for the porter. Any port job will go way beyond the flow of big port heads. Stage 1 heads are just the same castings as regular heads of that year with larger valves installed. Any BBB head can become a “Stage 1 head.” 67-69 BBBs oiled the valve train through oil passages in the block and heads, into the rocker shafts. 70-up oiled through the pushrods. You can use through-the-pushrod oiling on any BBB, but, 455 blocks continued to have this oil passage in the driver’s side of the block. So, if using early (67-9) heads on a 70-up block you need to block this oil passage, either in the block deck, or in the head. The passage is in the front rocker shaft pedestal, right next to the rocker shaft bolt hole. Manifolds All intake manifolds will flow equally. As with the heads, any emissions passages added to later intakes do not affect flow. If you are putting a 67-71 intake on later heads, you need to block the AIR passages at each corner of each head, as the earlier intakes don’t cover them. There are also issues with the exhaust crossover changing in, I believe, 72. Some (all?) 67 intakes do not have the cast-in bosses to mount the throttle cable bracket. 67-69 heads use 3/8” intake bolts and 70-up use 7/16” bolts. Obviously, you use whatever bolts fit the heads. The bigger bolts go through the bolt holes in the earlier intakes just fine. All exhaust manifolds flow equally. Some 67 manifolds do flow a little better, but not a big difference. If I missed anything, or others have a different take on any of this, chime in!']	['<urn:uuid:cca3a83e-cc98-493c-a63e-580e7a82c459>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-13T02:03:54.148118	11	34	639
85	calculate proper shower floor drainage angle	To calculate the slope, measure from the drain opening to the furthest wall and round up to the nearest foot. Multiply this measurement by 1/4 inch. For example, if the drain is 2-foot 8-inches from the furthest wall, the shower floor should slope 3/4 inch from that wall to the drain.	['- 1 Do you need a shower liner on concrete floor?\n- 2 How do you Slope a curbless shower?\n- 3 How do you make a curbless shower?\n- 4 Can a shower floor be concrete?\n- 5 How thick does a concrete shower pan need to be?\n- 6 Is wire mesh necessary in shower pan?\n- 7 Do I need a mortar bed for shower pan?\n- 8 Do doorless showers get cold?\n- 9 Is a curbless shower worth it?\n- 10 How much does a curbless shower cost?\n- 11 What is a zero threshold shower?\n- 12 What is a zero entry shower?\n- 13 How do you Slope a shower floor?\nDo you need a shower liner on concrete floor?\nJRC3 Member. It will need to be lined or it will wick into the slab and studding. The liner also makes the walls around the pan water-tight, that’s why the liner normally goes up at least 3″ above the pan behind the tile and backer material. Moisture gets behind the wall tile and is eventually caught by the liner.\nHow do you Slope a curbless shower?\nThe drain in a curbless walk-in shower does not have to be in the middle. Linear drains can be placed along any edge. If the floor is gently and unnoticeably sloped, 1/8″ to 3/16″ per foot, the water can be directed to a side or a corner. A curbless walk-in shower floor area might benefit from a smaller-scale tile.\nHow do you make a curbless shower?\nInstall your vinyl shower pan (or whatever waterproofing system you prefer) and tie it into your FHA drain; water test as required by local codes. Install mud bed sloped to drain followed by non-slip floor tile, 2×2” or smaller. Install a “water dam” just inside the shower on the sloped floor.\nCan a shower floor be concrete?\nNot only are concrete shower walls and floors practical and easy to clean, with no grout joints where ugly mold and mildew can grow, they also permit unlimited design options. A concrete wall and floor overlay was hand applied right over this old tile shower to give it a completely grout-free decorative surface.\nHow thick does a concrete shower pan need to be?\nAs a general rule, a shower curb foundation is 3 to 4 inches thick and 4 to 6 inches tall, with finish material adding to these dimensions.\nIs wire mesh necessary in shower pan?\nBefore installing the final layer of mortar on the shower pan you will need to install expanded wire mesh on the shower curb, over the PVC pan liner. The wire mesh will cover the entire curb from the inside of the shower pan at the top of the mortar floor over the top of the curb and down to the bathroom floor.\nDo I need a mortar bed for shower pan?\nMost installers recommend putting a bed of mortar down for the shower base to sit on. Check the manufacturer’s installation instructions and see what they recommend in case the use of mortar is discouraged.\nDo doorless showers get cold?\nWalk-in showers can even take up less space than their traditional counterparts, since they don’t require walls or curbs. Since there’s no shower enclosure to capture the steam from the warm water, on a cold day (and particularly in a large bathroom), a walk-in shower can be drafty or downright frigid.\nIs a curbless shower worth it?\nCurbless showers add visual appeal and maximize space. Getting rid of the shower barrier allows the bathroom floor to flow from wall-to-wall creating a seamless look and significantly larger appearance. Lastly, curbless showers do a good job of keeping water within the splash zone if installed properly.\nHow much does a curbless shower cost?\n“A typical walk-in shower with a curb and door costs about $2,500 to $5,000 on average,” says the home services company Angie’s List. “Installers say the special modifications for a curbless shower add another $500 to $700 or more depending on the tile type and shower size.”\nWhat is a zero threshold shower?\nA zero–threshold shower is a walk-in shower in which the bathroom floor and shower floor are flush, with no dam. There is no sill to step over, and this makes it easy for a wheelchair to roll right into the shower.\nWhat is a zero entry shower?\nOpen-concept, or curbless showers are a concept that’s growing in popularity – and for good reason. They offer accessibility – and lots of flexibility in design. And when we remove the barriers for our shower, we can also create a seamless look in the bathroom, making it look and feel bigger.\nHow do you Slope a shower floor?\n1) Measure from the drain opening to the furthest wall, then round up to the nearest foot. Calculate the slope by multiplying the measurement by 1/4 inch. For instance, if the drain opening measures 2-foot 8-inches from the furthest wall, then the shower floor should slope 3/4 inch from that wall to the drain.']	['<urn:uuid:5255b109-0163-4131-a101-75fdbac15ef3>']	open-ended	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-13T02:03:54.148118	6	51	846
86	What are the physical characteristics of feral hogs, and what economic impacts do they cause in the United States?	Feral hogs can reach a shoulder height of 36 inches and weigh between 100 to over 400 pounds. They have four continuously growing tusks, poor eyesight, but keen sense of hearing and smell. Regarding economic impacts, these animals cause significant damage, with restoration of ecosystems and losses to agriculture and livestock estimated to exceed US$800 million in the United States each year.	"['Feral Hogs in Southlake Early Spanish Explorers probably were the first to introduce hogs in Texas over 300 years ago. As colonization increased, hog numbers subsequently increased. They provided an important source of cured meat and lard for settlers. Feral hogs may appear basically the same as domestic hogs and will vary in color and coat pattern.\nA mature feral hog may reach a shoulder height of 36 inches and weigh from 100 to over 400 pounds. Hogs have four continuously growing tusks (two on top, two on bottom) and the contact causes a continuous sharpening of the lower tusks. They have relatively poor eyesight but have keen sense of hearing and smell. Feral hogs are distributed throughout much of Texas generally inhabiting the white-tailed deer range, with the highest population densities occurring in East, South, North and Central Texas. The current population in Texas alone is in excess of 1.5 million.\nCity of Southlake\'s Hog Management Plan Trapping is the most common method utilized by landowners and municipalities. This method allows for the safe and humane animal removal. The Southlake Department of Public Safety has taken the position that live trapping is the appropriate method of animal control. The animals would be baited and trapped in large groups.\nThese groups would then be removed from the city and humanely euthanized. This option, like others will be a slow and deliberate population control program. This will be a sustained program over months and years to control overall hog populations within Southlake. The hogs have lived in this area for many years which is why total eradication is highly unlikely, but management and control of hog populations is possible through efforts of everyone working together for a common goal. If you have questions please call the Southlake Police Department at 817-748-8114.\nFrequently Asked Questions\nWhat do feral hogs eat?\nIn a word, everything. Feral hogs are omnivorous, meaning they eat both plant and animal matter. They are very opportunistic feeders and much of their diet is based on availability. Foods include, grasses, forbs, roots and tubers, browse, acorns, fruits and bulbs. Animal matter includes invertebrates (insects, snails and earthworms), reptiles and birds. Hogs will also feed on domestic animals if given the opportunity.\nDo feral hogs carry diseases?\nYes. In general, wild hogs carry various diseases. They include pseudo rabies, swine brucellosis, tuberculosis, bubonic plague, tularemia, hog cholera, hoof and mouth disease.\nHow do I keep from contacting disease?\nMost of these disease are transmitted by contact, avoiding contact with wild or feral hogs is the only sure way of avoiding contracting these diseases.\nAre feral hogs dangerous? All wild animals have the potential of being dangerous, especially when wounded or cornered. In a natural state, feral hogs will prefer to run and escape danger, and are not considered dangerous. Extreme caution should be maintained when encountering any wounded, cornered, trapped animal or females with young. ""Their razorsharp tusks combined with their lightning speed can cause serious injury"".\nCan I wipe out a hog population through trapping?\nThe feral hog has managed to survive, adapt, and increase their numbers despite attempts at population control. While it is possible to keep the population in check with continuous control, it is highly unlikely to eradicate a hog population within an established range. The City has established a plan to help control the feral hog population. You can find it’s basic components on the other side of this brochure.', 'Feral swine -- Control -- Oregon, Feral swine -- Environmental aspects -- Oregon, Wild boar -- Control -- Oregon, Oregon Invasive Species Council\nFeral swine are defined as free roaming animals of the genus Sus that are not being held under domestic management or confinement. Swine have spread from Europe and Russia to habitats around the world via human introduction. Currently, feral swine populations are established on every continent except Antarctica. Unlike other large mammal invaders, swine have a high reproductive capacity and are omnivorous, which allows for a quick assimilation into most habitats. Once a breeding population is established in an area, the population can quickly increase and negatively impact the ecosystem. A successful invasion of feral swine is difficult, and sometimes impossible, to reverse. A feral swine pest risk assessment for Oregon, released in 2004, designated feral swine as a very high-risk species due to high potential for establishment, environmental and economic impacts, and disease transmission to wildlife, livestock and humans. Economic impacts on ecosystems and disease transmission to wildlife are difficult to assess, but restoration of ecosystems and losses to agriculture and livestock have been estimated to exceed US$800 million in the United States each year. Environmental impacts include facilitation of noxious weed invasions, shifts in dominant plant species, reduction of forest regeneration, and soil erosion. Facilitation of noxious weeds and erosion due to feral swine rooting are documented in Oregon. Feral swine in Oregon have not been implicated in disease transmission to humans, but the recent E. coli outbreak from spinach grown on a California farm that caused three deaths has been genetically traced to feral swine excrement deposited in spinach fields. The feral swine population in Oregon is currently small and dispersed. Few disturbances have been documented but state and federal biologists report regular occurrence of disturbances due to feral swine. Actions to prevent the effects of an invasion fall into three categories: management, control or eradication. Of the three categories, only eradication efforts have successfully slowed or reversed the effects of swine invasions. Case studies from California, Australia, Hawaii, the Galapagos Islands and the Channel Islands off the coast of California show that management and control efforts, while effective in the short term, have not successfully kept small feral swine populations from increasing to levels that are unmanageable and uncontrollable. A four-year feral swine eradication plan is proposed. The Plan includes recommended legislative changes to facilitate eradication, outreach and education, population assessment, rapid response, and eradication elements. A 0.5 FTE position is required at the Oregon Department of Fish and Wildlife to implement the plan. Eradication of feral swine in Oregon is estimated to require a four-year, $1.29 million effort. Follow-up control of new releases and escapes will require a maintenance effort estimated at less than $50,000 per year (excluding contingency funds for emergency response). These costs are small relative to the value of the $3.6 billion Oregon agriculture and livestock industries and the investment Oregon has made in riparian restoration efforts. Sustained control of feral swine in Oregon will require a longterm commitment that will include annual domestic swine marking, education, and monitoring.\nSytsma, Mark and Rouhe, Arick Christopher, ""Feral Swine Action Plan for Oregon"" (2007). Center for Lakes and Reservoirs Publications and Presentations. 14.']"	['<urn:uuid:f0ed966d-9041-49d0-9c39-d81c37b873dc>', '<urn:uuid:905df633-f756-4dc8-9e51-957bd08668f9>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-13T02:03:54.148118	19	62	1117
87	medici family political social influence florence	The Medici family's influence in Florence combined both political and social aspects. Politically, Giovanni di Bicci de'Medici opposed the conservative Albizzi family and worked towards a more equitable taxation system. Socially, he contributed to Florence's development through charitable projects and architectural patronage, including funding the Spedale Innocenti hospital and renovating San Lorenzo Church. His success transformed the Medici from obscurity to prominence, establishing two family branches that would rule Florence and Tuscany for three centuries. This rise of the Medici paralleled broader societal changes in medieval Europe, where wealthy middle-class families gained increasing political rights and social influence through their financial power.	"['Giovanni di Bicci de Medici was the first member of the Medici dynasty to accumulate a great fortune, bringing his family from obscurity to fame. Born in Florence in 1360, he founded an important bank with branches all over Europe, lending money to kings and popes.\nGiovanni di Bicci de Medici – the dawn of power\nThe achievements of Giovanni di Bicci had great repercussions throughout history. Not only in Florence but the whole of Italy. His father known as “Bicci”, was a well-to-do wool merchant, but Giovanni had other ambitions. He aimed high. He learned the trade working in his uncle’s bank, eventually buying it and building it into the most powerful bank in Europe.\nThe dowry of his newly married wife Piccarda helped him to start his business. She turned out to be the perfect match, a strong woman from a wealthy family who gave him two sons who brought forward the Medici dynasty and fortune: Cosimo (he will be known as Cosimo the Elder) and Lorenzo.\nA powerful friend – the antipope Baldassarre Cossa\nGiovanni di Bicci needed powerful connections to achieve his ambitions. One of these was his good friend Baldassare Cossa.\nCossa (1370-1419) was elected pope as Giovanni XXIII, with the help of Giovanni’s money. He was a controversial character, who loved women and money more than religion. But most important for Giovanni was the fact that he granted the Medici bank the right to collect Vatican taxes, and taking a percentage of them.\nCossa’s fortune didn’t last long however. He was deposed and incarcerated in 1415, but Giovanni didn’t desert him in his misfortune. He paid for his liberation. The ‘antipope’ died in Florence, where Giovanni’s son Cosimo paid for his grand tomb in Florence’s Baptistry. A beautiful tomb by MIchelozzo and Donatello that can still be seen today.Sailko, CC BY-SA 3.0, Link\nGreat ambition and civic duty in Florence\nGiovanni di Bicci de Medici got involved in Florentine politics late in life. Though he never had a major political role, his money and connections did give him enormous power. Politically he opposed the more ‘conservative’ Albizzi family, and contributed to a more just and proportional system of taxation.\nHe offered a significant contribution to the betterment of his beloved city. His generosity included charitable projects such as the Spedale Innocenti (a hospital for orphans) employing Filippo Brunelleschi for the job. Brunelleschi was Giovanni’s favourite architect, and he chose him for the renovation of San Lorenzo Church, which at that time was falling into ruins.\nThese are just some examples that show how the Medici family shaped the city of Florence throughout the centuries.\nBy the time he died of natural causes in 1429, the 69 year old Giovanni had succeeded in redeeming the image of the Medici, and created a solid base from which the fortune of the Medici dynasty would grow. He is buried in the Church of San Lorenzo in Florence.\nGiovanni di Bicci de Medici – legacy\nGiovanni’s children sired the two historical branches of the Medici family that ruled over Florence and Tuscany for three centuries. The first branch (being from Cosimo the Elder and Contessina de’ Bardi, grandparents of Lorenzo il Magnifico) died out at the end of the 16th century. The second branch (that of Lorenzo the Elder) would survive through the first half of the 18th century.\nThere’s much more to know about the Medici family: if you’re interested, check out some facts and curiosities about this famous dynasty, and how it features in the TV series Medici: Masters of Florence.', 'After the economic upheavals following the Black Death, the dissolution of the feudal system was set in motion, and Europe was on course toward the emergence of the modern nation state. Feudal strictures were gradually reduced, many peasants were freed from the obligation to pay feudal dues, and many towns were granted the right of self-governance. The growth of commerce led to the emergence of a middle class, or ""Bourgeoisie"" (a word originating from the French ""bourg"" for a medieval town or village, sharing the same roots as the German ""burg""). Unlike the peasants of the earlier feudal system, the middle class had enforceable legal rights to their capital which they could defend in courts, and they had much greater opportunity for upward mobility.\nRoyalty had traditionally been dependent on the nobility, but while the clergy and nobility had been exempt from taxation, the growing middle class was not. Royalty consequently began to court the favor of prosperous members of the middle class. Middle class representatives were appointed to positions in councils, assemblies and parliaments, where they were granted new rights and privileges in exchange for their agreement to finance the royals through taxation. This process ultimately led to the emergence of the nation-state.\nThe growth of trade and the re-emergence of banking in Europe had already begun during the Crusades, during which time Italian city-states such as Venice, Genoa and Pisa had prospered by supplying ships, and by receiving rich cargo of spices, silk, jewels, perfumes and other valuable goods from the East. The Italian city-states were not the only ones to prosper from the expansion of trade. Many German, English, and French cities also grew and thrived. In the late 13th century Germans founded the Hanseatic League, an alliance of trading cities and merchant guilds who provided protection for its members. The Hanseatic cities had their own legal systems, and the league sent its own foreign service agents to make deals with trading partners.\nTrade routes connected cities throughout Europe, and accelerated the growth of banking and commerce. Traders and financiers began using letters of credit and foreign exchange documents, converting currencies, storing and transporting the money. When possible they also engaged in money lending. The Vatican\'s prohibition of lending money and charging interest (""usury"") complicated matters, but it had been ignored in the Knight\'s Templars favor, and the proto-bankers of medieval Europe often managed to find exceptions as well. For example, profiting from time differences in foreign exchange contracts tended to fall outside the official definition of usury, and consequently became a popular practice.\nThe first medieval merchant banks were originally started by Italian grain merchants, notably the Lombard bankers. Jews that had been expelled from Spain also began to enter Italy and the trade, where they had an advantage over the native Italians who were forbidden by the Christian anti-usury laws from lending money at interest. Merchant bankers began settling trades as middlemen, and holding deposits against notes of exchange. The funds being held were often used by the merchant bankers for their own trades, which meant that they would often have had insufficient funds if their depositors had simultaneously demanded to withdraw their money. The words ""bank"" and ""bankrupt"" illustrate these origins: Bank is derived from ""banca,"" Italian for bench (i.e., the counters over which the merchant bankers made trades), and the term bankrupt is derived from ""banca rotta,"" meaning ""broken bench.""\nBy the middle of the 13th century the Italian merchant bankers devised clever legal maneuvers to avoid the ban on usury. For example, making loans with no interest, but requiring that the loan be insured against loss or delay in repayment. Merchant bankers such as the Cahorsins and Lombards became prosperous using such techniques. The activities of the increasingly powerful Italian merchant bankers aroused a great deal of opposition. King Martin I of Aragon had them expelled in 1401, and Henry IV declared a nationwide ban on their profiteering. The Italian bankers were also expelled from Flanders and Paris later in the same decade. Nonetheless, their power and influence continued to grow.\nIn 1403 the bankers won a ruling that made money-lending at interest legal in Florence. A number of powerful Genoese families founded the Bank of Saint George in 1407, and the Bank remained a powerful force for four centuries. The Bank governed many of Genoa\'s overseas territories and lent large sums of money to powerful European rulers.\nFlorence was the source of many of the most famous and powerful banking families, notably the Medicis, who had founded the legendary Medici Bank in 1397. Giovanni di Bicci de\'Medici, the founder of the Medici Bank, anticipated and supported the return of the Pope to Rome, and in 1413 the papacy rewarded de\'Medici by appointing him the Pope\'s banker. Interest on the Pope\'s huge debts — as well as favorable contracts and mining rights — made the Medici Bank one of the wealthiest multi-national businesses in Europe. The Italian bankers became known as God\'s Bankers, or the Pope\'s Usurers.\nThis history of money will continue in Part VII, featuring the Great Bullion Famine, the rise of the Fugger banking dynasty, and the beginning of the Age of Exploration...']"	['<urn:uuid:1deb62c7-cda1-494e-9c26-029235325ffd>', '<urn:uuid:9a01e317-8d97-4d21-b3cc-f86021fa584b>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-13T02:03:54.148118	6	102	1459
88	I'm trying to set up my first raised garden bed and I'm confused about what soil to use. What's the best medium for filling garden beds, and why shouldn't I use regular potting mix?	For garden beds, you should avoid using bags of potting mix or planting mix as they are expensive, retain too much moisture, don't hold fertility well, and are intended for smaller containers. Planting mix is basically just potting mix with additional sand. Instead, the best approach is to build up your existing soil by adding organic matter or compost.	['Our role as both grower and retailer is a uncommon combination in our business. A beautiful shopping surroundings brimming with climatized plants and backed by a lifetime of expertise that can assist you succeed. As with most hobbies, you’ll be able to put as much into gardening as you could have the time and inclination. You can plant a small container with cheerful flowers to place exterior your entrance door, or you possibly can construct a greenhouse and have tomatoes and peppers that flourish through the winter. Making an enormous pot of soup from freshly picked components is a pleasure.\nGarden pests are generally crops, fungi, or animals (frequently bugs) that interact in activity that the gardener considers undesirable. Aphids, spider mites, slugs, snails, ants, birds, and even cats are generally thought of to be backyard pests. ] to explain gardening utilizing intermediate expertise (greater than tools, less than harvesters), particularly of natural varieties. Gardening is effectively scaled up to feed entire villages of over one hundred people from specialised plots. A variant is the community garden which offers plots to city dwellers; see additional in allotment (gardening).\nCaring for crops can even do wonders on your personal wellbeing, an abundance of scientific analysis suggests. The physical train can contribute to a wholesome weight and blood strain ranges, and simply interacting with flora can improve your mood and mental well being. Even with efficient use of space, a backyard takes away a portion of your lawn or patio.\nIt is also essential to make use of the best medium for filling the beds. Donâ€™t use bags of potting mix or planting combine as they’re costly, retain moisture, donâ€™t hold fertility, and are intended for smaller containers. Bags of planting mix are principally potting combine with extra sand in it. Instead, if attainable, construct up your current soil by adding natural matter or compost.\nIn urban or developed areas that could be devoid of wetlands, bird baths and other options can appeal to considerable numbers of birds to the garden so long as they’re properly designed and maintained. It can also be essential that any water options designed for the benefit of wildlife not turn into predatory â€œtrapsâ€ (until you are attempting to help native predators). Surround water options with low vegetation that gives good visibility while offering easy escape routes. Stumps and huge logs can be used to create natural planters for woodland crops.\nOf all crops, herbs in all probability have one of the best return on your gardening dollars. Fresh herbs will take a meal from plain to gourmet for just pennies.\nCompanion planting is not an exact science, and the explanation why some plants get along while others don’t isn’t always clear. To quite a few gardeners and farmers, trial and error has shown that some crops certainly don’t make good neighbors. Several reasons dictate why greens must be planted collectively and apart. The points embody cross-contamination, launch of certain compounds and extreme shade solid by taller plants over smaller neighbors.\nKeep reading to be taught what you are able to do in your garden to make it a extra friendly and sustainable environment for the animals that call it home. What does it take to be an ecological gardener, past filling landscapes with a various number of native vegetation? As major producers, plants type the base of the garden meals internet that supports animal life.\nHome and garden events list (summer time 2018 edition)\nBut letâ€™s face it, most people eat vegetables as a result of theyâ€™re good for us, not essentially as a result of we like them. On the other hand, develop one or two issues each year that you have by no means tried before.\n“To plant a garden is to believe in tomorrow.”\nThe University of Georgia Cooperative Extension explains that soil in containers has much less capillary pull to encourage moisture downward, and the shortage of porous soil encourages fungi that may rot your plantsâ€™ roots. Your plants should have the right amount of air circulating through the soil to develop correctly. Once you assess environmental factors, similar to growing zone and soil, itâ€™s time to focus in your planting strategy. Soil could be sandy, clay, or loamy, which determines how well your plants develop and how inclined you might be to experiencing plant illnesses and root rot. To decide what youâ€™re working with, grab some soil, moist it, and attempt to kind a ball.']	['<urn:uuid:cffa0769-7a77-4fe8-8b98-844410588804>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-13T02:03:54.148118	34	59	744
89	What unique contributions do medical family therapy and music therapy make to healthcare treatment and patient recovery?	Medical Family Therapy contributes to healthcare by addressing the biopsychosocial-spiritual aspects of health, collaborating with medical professionals, helping prevent caregiver burnout, and influencing healthcare policy. It focuses on treating health issues within the context of family systems and supports both patients and healthcare providers. Music Therapy uniquely contributes by using musical interventions to achieve specific health outcomes, such as helping patients regain speech after brain injury, reducing asthma episodes, improving motor function in Parkinson's disease patients, reducing pain in hospitalized patients, improving sleep patterns in premature infants, and lessening the effects of dementia in older adults. Both therapies are evidence-based and contribute to comprehensive patient care, but through different therapeutic tools and approaches.	"[""Medical Family Therapy : Conceptual Clarification and Consensus for an Emerging Profession\n- ECU Author/Contributor (non-ECU co-authors, if there are any, appear on document)\n- Lisa E. Tyndall (Creator)\n- East Carolina University (ECU )\n- Web Site: http://www.ecu.edu/lib/\nAbstract: The term Medical Family Therapy (MedFT) was coined in the early 1990s (McDaniel, Hepworth, & Doherty, 1992a) by a team of family therapists and a family psychologist. Since then there has been growing interest in MedFT, including the expansion of training programs and an increasing prevalence in the academic literature. While this growth is exciting, if MedFT is going to continue to move forward and gain credibility in the healthcare system, its practitioners, researchers, and scholars must first establish a common lexicon, that can thereby ground the MedFT's professional identity, regulatory oversight, and scientist-practitioner models. The first article presented in this dissertation highlights the available literature on MedFT and identifies ways to further MedFT research initiatives and possibilities. The second article is based upon based on responses from an expert panel of MedFTs and includes an analysis of their perspectives on how MedFT should be defined, practiced, and taught. The first article is a non-systemic literature review that illustrates the state of MedFT as well as reports on the similarities and differences present in its myriad of available definitions. Additionally, the article presents the theoretical foundation and skill set of MedFTs as found in the applied clinical literature and foundational research. Researchers who have studied MedFT interventions or incorporated MedFTs as interventionists in models of clinical care are also reviewed. Overall, 65 articles were reviewed and three distinct themes emerged from the process: 1) the inception of MedFT, 2) MedFT skills and applications, and 3) MedFT Effectiveness and Efficacy Research. During the review of these articles, variations in the definition of MedFT included or excluded concepts such as: collaboration, family systems, or the biopsychosocial perspective. These variations appeared to reflect the qualifications and educational background of the practitioners, the focus and generalizability of the research. Additionally, these variations will affect the future of MedFT as either an orientation to be practiced by a wide variety of professions or a profession to be licensed independently. Upon reviewing the literature and articulating the existing gaps, it is clear that the most salient need for future research is a cohesive definition of MedFT, quality science that demonstrates its effectiveness, and educational guidelines for those desiring to be MedFT practitioners. Therefore, three recommendations are made: 1) those with expertise in MedFT must reach a consensus on a definition from which practice, training, and research can grow, 2) the MedFT intervention framework must be strengthened through research, and 3) agreement must also be reached on a MedFT curriculum with which to train future practitioners and scholars. The second article is the results of a research study conducted to address two of the recommendations suggested in the literature review. A modified Delphi (Dalkey, 1972; Linstone & Turoff, 1975) study was conducted bringing together 37 panelists with MedFT expertise to identify the current definition of MedFT, its scope of practice, and educational competencies believed to be essential to those who practice it. After analyzing these data, we discovered that several of the foundational elements of MedFT discussed in McDaniel et al. (1992a) still hold true, including the importance of collaboration, the connection to marriage and family therapy as a parent discipline, and the overarching goals of agency and communion. The biopsychosocial (BPS) model (Engel 1977; 1980) also a foundational element of MedFT (McDaniel et al., 1992a), remained fundamental; however, the expert panel also argued for the inclusion of the spiritual dimension of health to be addressed. Panelists endorsed MedFT as primarily an orientation, a way of thinking; leaving it open to be practiced in a wide array of settings with a variety of conditions. However, some panelists also believed MedFT to be a developing profession. Also discovered was a general consensus for what a core MedFT curriculum would include. MedFT students should have a strong theoretical base and clinical skill set in family systems theory and the BPS framework, as well as comfort and skill working within medical settings and collaborating with medical professionals. MedFTs should be familiar with a variety of diseases, illnesses, disorders, and disabilities, as well as have taken courses in areas such as psychopharmacology, MedFT theory, medical culture and collaboration, and families and illness. Panelists called for MedFTs to be involved in the creation of healthcare policy, but also provide psychosocial support to medical professionals in an effort to help them to avoid caregiver burnout, compassion fatigue, and improve patient care. Recommendations for future research, clinical practice, and education in MedFT are offered.\n- Language: English\n- Date: 2010\n- Psychology, Behavioral, Behavioral Sciences\n|Title||Location & Link||Type of Relationship\n|Medical Family Therapy : Conceptual Clarification and Consensus for an Emerging Profession||http://thescholarship.ecu.edu/bitstream/handle/10342/2794/Tyndall_ecu_0600D_10115.pdf||The described resource references, cites, or otherwise points to the related resource."", 'What is Music Therapy?\nMusic Therapy is the clinical and evidence-based use of music interventions to accomplish individualized goals within a therapeutic relationship by a credentialed professional who has completed an approved music therapy program.\nMusic Therapy is an established health profession in which music is used within a therapeutic relationship to address physical, emotional, cognitive, and social needs of individuals. After assessing the strengths and needs of each client, the qualified music therapist provides the indicated treatment including creating, singing, moving to, and/or listening to music. Through musical involvement in the therapeutic context, clients\' abilities are strengthened and transferred to other areas of their lives. Music therapy also provides avenues for communication that can be helpful to those who find it difficult to express themselves in words. Research in music therapy supports its effectiveness in many areas such as: overall physical rehabilitation and facilitating movement, increasing people\'s motivation to become engaged in their treatment, providing emotional support for clients and their families, and providing an outlet for expression of feelings.\nWhat Music Therapy Is... and Is Not\nThe American Music Therapy Association (AMTA) supports music for all and applauds the efforts of individuals who share their music-making and time; we say the more music the better! But clinical music therapy is the only professional, research-based discipline that actively applies supportive science to the creative, emotional, and energizing experiences of music for health treatment and educational goals. Below are a few important facts about music therapy and the credentialed music therapists who practice it:\n- Music therapists must have a bachelor’s degree or higher in music therapy from one of AMTA’s 72 approved colleges and universities, including 1200 hours of clinical training.\n- Music therapists must hold the MT-BC credential, issued through the Certification Board for Music Therapists, which protects the public by ensuring competent practice and requiring continuing education. Some states also require licensure for board-certified music therapists.\n- Music Therapy is an evidence-based health profession with a strong research foundation.\n- Music Therapy degrees require knowledge in psychology, medicine, and music.\nThese examples of therapeutic music are noteworthy, but are not clinical music therapy:\n- A person with Alzheimer’s listening to an iPod with headphones of his/her favorite songs\n- Groups such as Bedside Musicians, Musicians on Call, Music Practitioners, Sound Healers, and Music Thanatologists\n- Celebrities performing at hospitals and/or schools\n- A piano player in the lobby of a hospital\n- Nurses playing background music for patients\n- Artists in residence\n- Arts educators\n- A high school student playing guitar in a nursing home\n- A choir singing on the pediatric floor of a hospital\nFinally, here are examples what credentialed music therapists do:\n- Work with Congresswoman Giffords to regain her speech after surviving a bullet wound to her brain.\n- Work with older adults to lessen the effects of dementia.\n- Work with children and adults to reduce asthma episodes.\n- Work with hospitalized patients to reduce pain.\n- Work with children who have autism to improve communication capabilities.\n- Work with premature infants to improve sleep patterns and increase weight gain.\n- Work with people who have Parkinson’s disease to improve motor function.\nAMTA’s mission is to advance public awareness of the benefits of music therapy and increase access to quality music therapy services in a rapidly changing world. In consideration of the diversity of music used in healthcare, special education, and other settings, AMTA unequivocally recommends the unique knowledge and skill of board certified music therapists.\nBradt, J., Magee, W.L., Dileo, C., Wheeler, B.L., & McGilloway, E. (2010). Music therapy for acquired brain injury. Cochrane Database of Systematic Reviews, 2010(7), doi: 10.1002/14651858.CD006787.pub2.\nClair, A. A., Lyons, K., & Hamburg, J. (2012). A feasibility study of the effects of music and movement on physical function, quality of life, depression, and anxiety in patients with Parkinson disease. Music and Medicine, 4 (1), 49-55.\nQuotes about Music Therapy\n- Dr. Sanjay Gupta:\n""On this day, I was playing the patient. An intensive, exhaustive seven-hour schedule was presented, full of physical therapy, speech, recreational, occupational and my personal favorite – music therapy."" - CNN, February 2011\n""Music therapy helps speech, but also motor skills, memory and balance. Also emotionally uplifting."" - Twitter, May 2011\n- Jodi Picoult (Author of the bestselling book Sing You Home):\n""Music therapy, to me, is music performance without the ego. It’s not about entertainment as much as its about empathizing. If you can use music to slip past the pain and gather insight into the workings of someone else’s mind, you can begin to fix a problem. ""\n- Michael Greene, President & CEO of NARAS - 1997 Grammy Awards:\n""When we look at the body of evidence that the arts contribute to our society, it\'s absolutely astounding. Music Therapists are breaking down the walls of silence and affliction of autism, Alzheimer\'s and Parkinson\'s disease.""\n- Sen. Harry Reid (D-Nev.):\n- Sen. Harry Reid:\n""Music helps all types of people to remain forever young."" He noted that Congress had never before ""directly addressed the question of music"" as preventive medicine and as ""a therapeutic tool for those suffering from Alzheimer\'s disease and related dementias, strokes and depression.""\n- SUPERIOR, WI Telegram, Aug. 14, 1991.\n- Mickey Hart (Grateful Dead):\n""(Rhythm) is there in the cycles of the seasons, in the migrations of the birds and animals, in the fruiting and withering of plants, and in the birth, maturation and death of ourselves,"" Hart told a Senate panel studying music therapy.\n- REUTERS, Aug. 1, 1991.\n- Ida Goldman (90-year-old testifying at Senate hearings):\n""Before I had surgery, they told me I could never walk again. But when I sat and listened to music, I forgot all about the pain,"" said Goldman, who walked with assistance during the hearing.\n- REUTERS, Aug. 1, 1991.\n- Sen. Harry Reid:\n- Dr. Oliver Sacks (""Awakenings""):\nDr. Sacks reports that patients with neurological disorders who cannot talk or move are often able to sing, and sometimes even dance, to music. Its advocates say music therapy also can help ease the trauma of grieving, lessen depression and provide an outlet for people who are otherwise withdrawn.\n- ST. Louis Post Dispatch.\n- Dr. Clive Robbins (Nordoff-Robbins Music Therapy Clinic):\n""Almost all children respond to music. Music is an open-sesame, and if you can use it carefully and appropriately, you can reach into that child\'s potential for development."" Nordoff-Robbins uses music therapy to help 100 handicapped children learn and to relate and communicate with others.\n- Barbara Crowe (past president of the National Association for Music Therapy):\n- Oliver Sacks, M.D.:\n- Mathew Lee (Acting Director, Rusk Institute, New York):']"	['<urn:uuid:a5f3dc26-184a-4b81-badd-e2aa9bc2d1fc>', '<urn:uuid:5965de03-6b3a-4315-acc6-33d3682be80b>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T02:03:54.148118	17	113	1933
90	how does restricted stock buy back price work for startup founders	With a typical restricted stock grant, if a founder pays $.001 per share for restricted stock, the company can buy it back at $.001 per share. The company retains this buy-back right which lapses progressively over time as the stock vests.	"['Restricted stock is the main mechanism whereby a founding team will make sure that its members earn their sweat fairness. Being fundamental to startups, it is worth understanding. Let\'s see what it will be.\nRestricted stock is stock that is owned but could be forfeited if a founder leaves a company before it has vested.\nThe startup will typically grant such stock to a founder and develop the right to purchase it back at cost if the service relationship between the company and the founder should end. This arrangement can double whether the founder is an employee or contractor in relation to services performed.\nWith a typical restricted stock grant, if a founder pays $.001 per share for restricted stock, the company can buy it back at bucks.001 per share.\nBut not realistic.\nThe buy-back right lapses progressively occasion.\nFor example, Founder A is granted 1 million shares of restricted stock at cash.001 per share, or $1,000 total, with the startup retaining a buy-back right at $.001 per share that lapses as to 1/48th of the shares terrible month of Founder A\'s service tenure. The buy-back right initially holds true for 100% on the shares earned in the scholarship. If Founder A ceased employed for the startup the next day of getting the grant, the startup could buy all of the stock back at $.001 per share, or $1,000 finish. After one month of service by Founder A, the buy-back right would lapse as to 1/48th of your shares (i.e., as to 20,833 shares). If Founder A left at that time, the could buy back basically the 20,833 vested gives you. And so lets start work on each month of service tenure just before 1 million shares are fully vested at the finish of 48 months of service.\nIn technical legal terms, this isn\'t strictly dress yourself in as ""vesting."" Technically, the stock is owned but sometimes be forfeited by what called a ""repurchase option"" held the particular company.\nThe repurchase option could be triggered by any event that causes the service relationship in between your founder and the company to end. The founder might be fired. Or quit. Or why not be forced give up. Or collapse. Whatever the cause (depending, of course, from the wording of your stock purchase agreement), the startup can normally exercise its option to obtain back any shares that are unvested associated with the date of termination.\nWhen stock tied a new continuing service relationship might be forfeited in this manner, an 83(b) election normally in order to be be filed to avoid adverse tax consequences down the road for that founder.\nHow Is restricted Stock Applied in a Startup?\nWe are usually using entitlement to live ""Co Founder Collaboration Agreement India"" to relate to the recipient of restricted stock. Such stock grants can be generated to any person, even though a author. Normally, startups reserve such grants for founders and very key people. Why? Because anyone who gets restricted stock (in contrast for you to some stock option grant) immediately becomes a shareholder and have all the rights of an shareholder. Startups should stop being too loose about providing people with this stature.\nRestricted stock usually cannot make sense for every solo founder unless a team will shortly be brought .\nFor a team of founders, though, it will be the rule pertaining to which lot only occasional exceptions.\nEven if founders do not use restricted stock, VCs will impose vesting to them at first funding, perhaps not if you wish to all their stock but as to several. Investors can\'t legally force this on founders but will insist on it as a complaint that to funding. If founders bypass the VCs, this needless to say is not an issue.\nRestricted stock can be used as to a new founders and not others. Is actually no legal rule saying each founder must create the same vesting requirements. One can be granted stock without restrictions any kind of kind (100% vested), another can be granted stock that is, say, 20% immediately vested with the remaining 80% depending upon vesting, so next on. The is negotiable among creators.\nVesting do not have to necessarily be over a 4-year age. It can be 2, 3, 5, one more number which renders sense to your founders.\nThe rate of vesting can vary as well. It can be monthly, quarterly, annually, or other increment. Annual vesting for founders is relatively rare nearly all founders will not want a one-year delay between vesting points as they build value in the organization. In this sense, restricted stock grants differ significantly from stock option grants, which face longer vesting gaps or initial ""cliffs."" But, again, this almost all negotiable and arrangements alter.\nFounders may also attempt to barter acceleration provisions if termination of their service relationship is without cause or if perhaps they resign for grounds. If they include such clauses involving their documentation, ""cause"" normally always be defined to utilise to reasonable cases certainly where an founder isn\'t performing proper duties. Otherwise, it becomes nearly impossible to get rid associated with an non-performing founder without running the risk of a court case.\nAll service relationships within a startup context should normally be terminable at will, whether not really a no-cause termination triggers a stock acceleration.\nVCs typically resist acceleration provisions. When agree these in any form, it will likely remain in a narrower form than founders would prefer, items example by saying in which a founder are able to get accelerated vesting only should a founder is fired from a stated period after an alteration of control (""double-trigger"" acceleration).\nRestricted stock is used by startups organized as corporations. It may possibly be done via ""restricted units"" in an LLC membership context but this a lot more unusual. The LLC a excellent vehicle for many small company purposes, and also for startups in the right cases, but tends turn out to be a clumsy vehicle to handle the rights of a founding team that in order to put strings on equity grants. be done in an LLC but only by injecting into them the very complexity that a majority of people who flock with regard to an LLC seek to avoid. The hho booster is to be able to be complex anyway, will be normally advisable to use the corporation format.\nAll in all, restricted stock is often a valuable tool for startups to utilization in setting up important founder incentives. Founders should take advantage of this tool wisely under the guidance from the good business lawyer.']"	['<urn:uuid:c3242822-3b3e-47db-8abd-af0faed8ac6b>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-13T02:03:54.148118	11	41	1096
91	How long after exercising should you eat to get the best results?	Research shows the post-workout window is actually the entire 24 hours after training, with the key time to eat ideally occurring within 4 hours after finishing exercise. While the body's cells are most receptive to replenishment within the first 30 minutes after intense training, studies have shown that whether you eat protein before, immediately after, or several hours after your workout, your muscle protein synthesis will be about the same.	"['You might want to think twice before you rush to slug down another post-workout smoothie. Your pre-workout routine of oatmeal and fruit? It might not be helping in the way you think. And the extra BCAAs you’re drinking during your workout? The real impact is likely only on how much money you have in your wallet.\nFrom building muscle to surviving your endurance runs, the rules of workout nutrition have completely changed. But, there’s one big problem: few people are aware of what really helps you fuel before a workout and recover afterward.\nWhich is why this is both your warning and a sigh of relief. The latest breakthroughs have rewritten the script, and that’s good news for anyone who likes to exercise. Gone are the days of carb-loading or rushing to have protein within 30 minutes of finishing your workout.\nIn fact, both nutrient timing and workout nutrition needs have liberating truth: Neither matters as much as we once believed.\nSo, while you might look at the past as wasted, it’s best to view these new rules for what they are: a serious fitness upgrade that makes it easier than ever to eat the right way to fuel performance, strip away fat, or even build extra muscle without all the extra, unnecessary eating.\nConfusion 101: Are Sports Drinks Better Than Water?\nIf you really want to know why the advice has been so misguided, you don’t have to look any farther than the sports drink aisle at your grocery store.\nFor most weekend warriors, the need for a sports drink (think Gatorade, Powerade, or any other energetic adjective + “ade”) isn’t as real as the ads make it seem.\nYes, there can be benefits to sports drinks. But, the liquid rejuvenation is limited to a very select group of exercises that deplete their bodies of certain nutrients.\nAnd, for most gym-goers, runners, and weekend warriors, it’s rare that you ever push your body to the point of needing the type of energy locked inside the bottle.\nYou see, most people’s workouts fall into one of 2 categories:\n- High intensity but shorter duration (think less than 1 hour of gym activity)\n- Lower or moderate intensity for a longer duration (think 1-2 hour runs)\nIn both of these cases, the only necessary hydration is water. If you want a little boost, then you might want to sip on some electrolytes (think more sodium and potassium than you’ll find in sports drinks, as well as calcium and magnesium), and a few carbs to help with hydration — but not the 30 grams of sugar packed into your favorite sports drink.\nWhen you’re working out at a high intensity and for longer periods of time (think more than 2 to 3 hours), that’s when sports drinks offer the most benefits because they refill what is lost during that type of extreme condition.\nIf you regularly sweat out 2 to 3 percent of your body’s weight during long duration, intense exercise—3 to 6 pounds, for most of us—you probably need more sodium. That’s what a sports drink provides.\nThe same goes for the minerals you lose through heavy sweating. For example, most athletes know about electrolytes. In particular—potassium, magnesium, and sodium—are essential (and have the name “electrolyte) because your body needs them to transmit electrical signals from your brain to your muscles. This is what allows your body to function.\nBut, the same type of research that was used to formulate products like Gatorade was also the basis of your workout strategy. In other words, Gatorade was designed more for high-level athletes than high-level executives, mothers, fathers, and typical gym-goers.\nThis was the basis of nutrient timing theory: The high carb amounts. The immediate need for protein. The fear of fats slowing down recovery.\nThe reality? None of it was really designed for your body.\nDo You Have To Eat Directly After Your Workout?\nLet’s set one thing clear: What food you put into your body is still very important and determines how hard you can exercise and how well you recover.\nThe bigger issue is exactly what you should be eating, or maybe, more importantly, when you should be eating it.\nThe idea of the “anabolic window” or that you need to eat as soon as possible after finishing your workouts is one of the most misleading pieces of fitness advice that has persevered for decades.\nIt’s based on a fear-driven, scientifically-debunked mentality that your muscles live in an hourglass, and with each passing second of eating before or after a workout you were losing out on improvement.\nFor the past 20 years, the prevailing idea was that you had about 30 to 60 minutes to eat something after your workout. If not, your body would become catabolic (a state of stress) and you would lose muscle, not recover fast enough, and fail to see the benefits from all your hard work and time invested.\nWhen you think about it, the theory seems crazy. How could the human body have such a small window for recovery?\nThat was the question exercise physiologist Dr. Brad Schoenfeld aimed to solve.\nHe reviewed a large number of studies that examined nutrient timing and set out to answer a simple question: Is there such thing as the “anabolic window?”\nTurns out there is—but it’s much bigger than anyone ever suggested. And the timing of your meals after a workout isn’t even the biggest indicator of your success. (More on that in a moment.)\nWhen Should You Eat After Your Workout?\nAfter you exercise you burn up your main energy store of carbohydrates, also known as glycogen. So, it only makes sense that you need to refuel glycogen by eating lots of carbs to replace what was lost.\nBut, when food was consumed in a shorter window of time after a workout there was no significant difference than when it was consumed after a long delay.\nIn fact, the research would go as far as suggesting that your post-workout window is actually the entire 24 hours after you train, with the key time to eat ideally occurring anywhere within 4 hours after you finish your last set, stop your run, or end your athletic event.\nNot exactly the same message as slug your protein shake before your muscles shrink.\nHow did this massive misunderstanding occur?\nIt goes back to the sports drink phenomenon. The “glycogen emptying” idea wasn’t really applicable to the average person. In reality, it takes a tremendous effort to completely deplete your glycogen stores.\nExtreme marathoners can do it. Bodybuilders who train twice per day can do it. NFL athletes who play a 3-hour game can do it.\nBut you? It’s a different story.\nMost people don’t’ go to the gym completely fasted or do workouts that completely tap-out your energy reserves (even if you feel exhausted). And yet, those were the test conditions used to determine what to eat after your workout.\nWhile it might feel like your body needs food immediately, the ROI of rushing to or even forcing food into your system is minimal: you won’t see added strength, additional muscle, faster fat loss, improved endurance, or a boost in recovery.\nThe new rules of nutrient timing focus on the bigger picture. If you want to perform and look your best, then you need to consider three factors: what you eat before your workout, what you eat after, and what type of activity you perform.\nHow to Fuel Your Workouts The Right Way\nJust because the timing of your post-workout meal has been reduced from urgent to “apply on your time,” doesn’t mean the entire concept of nutrient timing is dead.\nIn fact, it’s just the opposite. There’s never been a clearer idea of exactly what you should be eating to help your body. And the biggest breakthrough is clear. Protein is the new carbs.\nIt used to be that you needed to fuel up with carbs prior to your workout and then replenish after your workout. This all ties back to glycogen as a primary source of energy and fuel for your body. Most research tested the benefits of using carbohydrates as fuel and then tested different amounts of carbs.\nBut, even that rationale was a bit flawed. Nutrient timing should focus on three aspects that help improve your performance and appearance.\nGlycogen replenishment: Glycogen is your fuel. The more you have the harder you can push your body for longer periods of time.\nProtein breakdown: If you want to gain muscle, protein synthesis (anabolism) has to be greater than protein breakdown (catabolism).\nProtein Building – Protein Breakdown = Muscle Growth or Loss\nSo, it only makes sense that you want to slow the breakdown process.\nProtein synthesis: Eating protein after a workout is supposed to optimize the other side of the same equation by increasing muscle protein synthesis, the process that helps you repair and rebuild muscle.\nCombined, all three of these factors influence how hard you can train (endurance, strength, work capacity), how well you recover, and your ability to build muscle and burn fat. So it only makes sense that what you eat should target any or all of these goals.\nDo Carbs Help Your Workouts?\nCarbs are a great source of fuel for your body. But, eating more carbs doesn’t necessarily mean you’ll have more energy. And that’s because depleting glycogen is actually very difficult.\nFor example, let’s say you did a full-body workout of 9 exercises, performed 3 sets of each exercise (so 27 sets total), and pushed at a high intensity of 80 percent of your 1 rep max. That’d be a grueling workout, but when researchers tested this exact protocol, they found that it only depleted about one-third of total glycogen stores.\nEven crazier? When a similar workout was tested and followed with no food, about 75 percent of the depleted glycogen was replenished within 6 hours.\nSo what’s going on? Your body is protective of your energy. The more you deplete your glycogen, the faster resynthesis occurs. The higher your intensity, the quicker you recharge. Even in marathon runners and endurance athletes, complete resynthesis is usually complete within 24 hours.\nThat’s not a call to avoid carbs. They are important and necessary, and if you’re exercising they need to be a part of your plan.\nBut, the extreme nature of pre-workout (carb-loading) and post-workout (insulin-spiking) carb needs were overblown. You don’t need to fuel up with hundreds of grams of fuel pre, during, and post-workout because you’re not tapping out your glycogen.\nWhen your tank is empty, you’ll know it without question. So, your ideal carb plan will ultimately depend on the type of activity you perform.\nHow Much Protein Should You Eat After a Workout?\nWhen eating protein and carbs was compared to carbs alone, it instantly became clear that protein is your body’s best friend. Adding protein improved recovery, muscle protein synthesis, and protein breakdown.\nBut most interesting? When protein and carbs (25 grams of protein and 50 grams of carbs) was compared to just protein alone (25 grams), there was no additional benefit in terms of muscle protein synthesis or muscle protein breakdown when the carbs were added.\nThe verdict: Protein is the new king of workout nutrition.\nAnd it doesn’t end there. While we know that protein is important for preventing muscle protein breakdown and fueling muscle protein synthesis, and some carbs (but not too much) are good for glycogen, how much you eat around your workout should not be your primary consideration.\nResearch shows that the most important dietary factor for performance and appearance was not how much protein or carbs you had before or after your workout, but rather how much you ate in the entire day.\nIn essence, even if your pre- or post-workout nutrition was less than optimal (say, if you’re in a rush to get to work), as long as you still ate the right amount of nutrients (proteins, carbs, and fats) for the entire day, then you would still see benefits.\nThe Best Pre- and Post-Workout Nutrition Plan\nTiming nutrition around your workout is a good idea for both fueling your performance and helping recovery. But, you don’t need to stress the timing as much as we once thought. Instead, the urgency of nutrition depends more on the activity you perform and whether you eat something before you exercise.\nWhen you enjoy a pre-workout meal, that will determine what you need after a workout. That’s because eating before your workout ensures that your insulin, amino acid, and glucose levels are still going to be high several hours after the workout.\nMost mixed meals will keep your insulin levels high enough to stop protein breakdown for 4-6 hours. A 45-gram dose of whey protein will do the same for about two hours. Most studies have shown that if you eat protein before, immediately after, or several hours after your workout, your muscle protein synthesis will be about the same.\nTranslation: choose a pre- and post-workout nutrition approach that works for you.\nIf you don’t like to eat before a workout, then don’t. But you’ll want to emphasize that post-workout meal more because you won’t have protein or carbs in your system.\nIf you do like a meal before exercise, there’s no rush to refuel immediately after. Not to mention, if you load up on carbs (such as with oatmeal or some fruit), depending on your type of activity you might not even need post-workout carbs.\nThe closer your meal is to the training bout, the longer your window following the session. And both are dependent on your primary training goal. Meaning there isn’t a gold standard for what you should be eating around your workouts. Instead, you should fuel your body based on the type of activity you perform.\nAnd remember, as long as you consume enough protein by the end of the day, your body generally has no trouble growing new muscle tissue, recovering, or having the energy needed to push through and become better.\nTo help you figure out your needs, use the activity chart below — based on the latest research — to help determine exactly what you need for your body and your goals.\nThe Ultimate Guide to Workout Nutrition\nYour Goal: Endurance Sports\nExamples: Long-distance track and cycling events, marathons, basketball, soccer, MMA\nWhat to eat: Carbohydrates for replenishing muscle glycogen, maintaining stamina, and maintaining energy during your event.\nWhat to remember: It’s easy to argue that nutrient timing is most important for endurance athletes because of the duration and demands of the activity. Performance is the main goal, therefore making carbohydrates more important as a fuel source during the activity and after for recovery. Protein, while useful for minimizing protein loss, is not as essential in the moment for these athletes, but is still important for recovery and retention of muscle.\nYour Nutrition Plan\n- The Focus: carbs and protein\n- The dose: 0.2-0.25 g/lb target bodyweight for both protein and carbs\nDuring your workout\n- For every hour of endurance activity, consume 8-15 g protein and about 15-30 grams of carbs. Liquids and gels are usually best for this.\nYour Goal: Strength/Power Sports\nExamples: Olympic weightlifting, football, powerlifting, bodybuilding, high-intensity intervals\nWhat to eat: Protein for optimizing muscle recovery and growth and minimizing muscle damage\nWhat to remember: Based on the length of time and type of activity, muscle glycogen is not depleted to the extent of endurance sports. Protein is important for supporting strength and muscle growth while minimizing muscle damage and loss. Carbohydrates are important, but less so, and are generally taken care of by meeting total daily calorie and macronutrient goals.\nYour Nutrition Plan\n- A balanced, full meal consisting of carbs and protein, 0.2-0.25 g/lb target bodyweight for both protein and carbs\nYour Goal: Weight-Loss\nExamples: Any type of activity geared towards losing weight. This is your typical cardiovascular type of activity (walking, treadmill, stairstepper) or weight training. NOTE: This is not high-intensity work or something like CrossFit, which is more likely to fit into the strength or endurance categories.\nWhat to eat: Fewer calories (calorie deficit) and more protein\nWhat to remember: The most important thing to keep in mind is you must burn more calories than you bring into your body. Create a calorie deficit first, and then worry about dialing in your pre- and post-workout nutrition.\nYour Nutrition Plan\n- Eat a balanced, full meal consisting of carbs and protein, 0.2-0.25 g/lb target bodyweight for both protein and carbs\nYour Next Steps\nRemember that nutrient timing should focus on three core aspects: glycogen replenishment, protein breakdown, and protein synthesis. And rather than stressing over timing, focus on giving your body the proper nutrition based on what type of activity you perform.\nHave questions? Share them in the comments below.\nOr if you’re looking for more personalization and hands-on support our online coaching program may be right for you. Every client is assigned two coaches — one for nutrition and one for fitness. Find out more here.', 'Eating to Recover: How and What to Eat Post Workout\nFor years the most important meal of the day has been said to be breakfast, but I believe that to be wrong. I suspect most of the readers here could agree that with all the effort you put into your training that your post-workout nutrition just may be more important to you than breakfast. In fact, I believe your post-workout meal is one of the most important meals you can have all day.\nThe reason I believe this is because in a hard workout, you can use up all your stored glycogen, easily sweat over two liters of water, and break down both muscle and red blood cells. This is why what you as an athlete consume in the minutes and hours after your training or competition is so crucial to both performance and recovery. This week I will look at nutrition recovery and explain it\'s importance and the best ways to use it to maximize your results.\nRecovery is defined in the Webster dictionary as ""the act of regaining or returning toward a normal or healthy state."" In recovery nutrition the foods we choose to consume have been shown to positively affect recovery and an athlete\'s future performance.\nThe Goals and Timeframe of Post Exercise Nutrition\n“Recovery” covers a range of processes that include:\n- Replenishing the muscle and liver glycogen stores\n- Consuming protein to assist with muscle repair\n- Restoring fluid and electrolytes lost in sweat\n- Supporting the immune system to handle the damage\nIt has been determined that the body\'s cells are most receptive to replenishment, particularly glycogen stores, within the first thirty minutes after intense training. Essentially the clock starts ticking for an athlete as soon as they enter their cool down. This is just one part of the equation as recovery nutrition can be broken into two stages - stage one occurs within thirty minutes of the workout and stage two occurs one to two hours after exercise.\nReplenishing the Body\'s Glycogen Stores\nIngesting carbohydrates after training is critical in replenishing glycogen stores and initiating muscle glycogen synthesis. If we don\'t replenish these stores an athlete\'s training performance can be compromised in future sessions.\nIt is recommended that 1.2g per kilogram of bodyweight of carbohydrates be consumed post exercise. Sometimes it is not practical for an athlete to consume such a large quantity of carbohydrates straight after exercise, as they may have difficulties tolerating food or drink. There is a way to combat this, though. Research in the International Journal of Sport Nutrition and Exercise Metabolism has shown that ingesting protein (0.2-0.4g/kg) and a smaller amount of carbohydrates together (around 0.8g/kg) can result in a similar effect, stimulating an endogenous insulin release that actually replenishes glycogen stores at a similar rate as ingesting 1.2g/kg of bodyweight of carbohydrates does.\nBuilding and Repairing Muscle\nMuscle protein is broken down due to high intensity or prolonged exercise. This makes recovery nutrition important in helping to rebuild. Once the recovery phase begins the catabolic processes reduce while anabolic processes increase and continue on for at least 24 hours after training. The ingestion of essential amino acids from quality protein sources has been shown to help with the muscle-building process. Even though research continues into the type, amount, and timing of protein consumption to obtain the maximum results, most agree that athletes who do either endurance or resistance type training will get the desired benefit by consuming 20-30g of high quality protein within the first hour post exercise.\nRehydrating the Body\nMost athletes will finish a competition or training session in some kind of fluid deficit. If this deficit is not corrected it can have a significant impact on future training sessions. In order to rectify this deficit an athlete should aim to consume 125-150% of the estimated fluid lost over a four to six hour period post exercise.\nIncluding sodium into recovery fluid can assist in replacing the electrolytes lost through excessive sweating. Rather than just losing the fluid through excessive urination, the addition of electrolytes can help the body retain the water consumed. In order to rehydrate effectively, 50-80mmol of sodium should be added. This can be achieved by adding extra electrolytes to commercial sports drinks or consuming fluids created with this ratio. Another alternative is consuming foods that contain sodium along with recovery fluids to achieve this required amount.\nSupporting the Immune System\nIntensive training can suppress the immune system. This suppression occurs while training is in progress and can continue to affect the efficiency of the immune system for hours afterward. This is obviously a concern for athletes as these hours of decreased immune function could allow an athlete to pick up an infection. Vitamins C and E, zinc, glutamine, and probiotics have all been touted to aid in the protection and support of the immune system. None have been proven to provide a bulletproof defense. There has been research, though, stating that if adequate glycogen stores are maintained pre and post exercise that this can reduce the disturbance of immune system markers as the consumption of carbohydrates can help reduce the bodies stress hormone response to exercise.\nUsing Supplements or Whole Foods to Meet Your Goals\nThese days there is a supplement for everything and for some athletes this means that they can get lazy and become totally reliant on sports supplements to meet their recovery goals. Unfortunately this can mean that some athletes end up doubling up on specific nutrients. While inherently this not a bad thing, some minerals taken in excess can cause toxic symptoms, so an athlete needs to be aware of what is in his or her supplements.\nAthletes are generally advised to obtain real food options to aid in recovery unless constrained for time. This is because it also allows an athlete to meet the daily nutritional needs of essential vitamins and minerals, and also stock up on much needed antioxidants like vitamins C and E that help reduce oxidization caused from the stress of exercise.\nPractical considerations of recovery nutrition also need to be taken into account. Issues like a lack of appetite, unavailability of food, and being unprepared can all play a part in an athlete failing to meet recovery goals. With a little planning these challenges can be nothing more than bumps in the road. Once an athlete notices the benefits, these bumps should be even further minimized, making for a consistent and competitive athlete through something as simple as eating the right foods at the right time.\n1. M. Beelen, Burke, L.M., Gibala, M.J., and van Loon, L.J.C. Nutritional strategies to promote postexercise recovery. International Journal of Sport Nutrition and Exercise Metabolism. 2010; 20:515-532.\n2. Hayes A and Cribb P. J. Effect of whey protein isolate on strength, body composition and muscle hypertrophy during resistance training. Current Opinion in Clinical Nutrition and Metabolic Care 2008, 11:40-44\n3. Nieman DC. Influence of carbohydrate on the immune response to intensive, prolonged exercise. Exerc Immunol Rev. 1998;4:64-76.\n4. ""Recovery Nutrition."" Sports Dieticians Australia. June 2012\nPhotos courtesy of Shutterstock.']"	['<urn:uuid:8a814cd6-27b0-4409-b5c6-b658b7469570>', '<urn:uuid:8d0a08b5-6b6b-4055-8eb9-d38da98e5c8e>']	factoid	direct	concise-and-natural	distant-from-document	three-doc	novice	2025-05-13T02:03:54.148118	12	70	4031
92	effects of fiber intake on belly fat blood sugar disease risk	Fiber intake has significant effects on belly fat reduction and disease prevention. Research from Wake Forest Baptist Medical Center found that soluble fiber specifically reduces harmful deep belly fat that surrounds organs and is associated with high blood pressure, diabetes, and fatty liver disease. Additionally, fiber helps stabilize blood sugar levels and reduces cravings by slowing down carbohydrate breakdown. Studies show that high fiber intake lowers risk for various conditions including hypertension, diabetes, dementia, depression, and certain cancers. In fact, research following over 387,000 people found that those with the highest fiber intake had a 22% lower mortality risk and an 80% greater chance of living a long, healthy life.	"['As a caregiver, it is important to be aware that fiber intake is the key to successful aging according to a recent study from Australia. Seniors with the highest intake of fiber had an almost 80% greater chance of living a long and healthy life.\nEven the scientists were surprised by the results. Fiber had a greater impact than any other dietary factor they studied, including sugar consumption. Adults who ate more complex carbohydrates significantly lowered their risk for hypertension, diabetes, dementia, depression, and disabilities.\nIf you want to live a longer and enjoy a more active life without eating like a rabbit, study these practical tips. You’ll learn new facts about fiber and how to include more roughage in your diet.\nFacts about Fiber\n1) Understand your needs. The average North American eats about 15 grams of fiber a day, which is far below the recommended levels. Individual requirements decrease slightly with age, and vary from a minimum of 21 to 25 grams for women and 30 to 38 grams for men.\n2) Cover both bases. Fiber comes in two forms, soluble and insoluble, depending on whether it dissolves in water. Soluble fiber lowers cholesterol and protects your heart. Insoluble fiber enhances your digestion and helps you stay regular. Eating a variety of foods provides enough of both.\n3) Know the benefits. Foods rich in fiber make you feel full with fewer calories so you can manage your weight. They also help you stabilize your blood sugar and reduce your risk for certain cancers, heart disease, and diabetes.\n4) Take it gradually. Give your digestive system time to adapt to more fiber. Add about 2 to 3 grams a week to avoid bloating and diarrhea.\n5) Drink more water. Drinking plenty of water will maximize the digestive benefits of fiber, and minimize constipation and gas. Stay hydrated around the clock.\nEating More Fiber\n1) Breakfast hearty. It’s easier to take in more fiber when you start early. Prepare a bean burrito or whip up a smoothie with kale and fresh fruit.\n2) Adjust the menu. Bulk up the dishes you love with a few simple tweaks. Stir shredded cabbage or carrots into chili and meatloaf. Top off your pizza with mushrooms and onions. Sprinkle a cup of beans into your salad.\n3) Switch to whole grains. Trade in ultra-processed white bread and rice for their healthier versions. Order brown rice when you’re eating out. Make sandwiches with whole wheat wraps.\n4) Cook fast. Some cooking methods, like boiling, rob vegetables of fiber and important nutrients. Steaming or microwaving will let you enjoy all their goodness.\n5) Bake at home. You don’t even have to give up cookies and cake to make room for fiber. Rely on your own oven instead of supermarket brands so you can use ingredients like oats and whole wheat flour.\n6) Keep the peels. Stop wasting apple and potato skins. Many vegetables and fruits have edible peels and seeds packed with fiber.\n7) Snack smart. Use snacks as well as meals to help you reach your fiber goals. Dip raw vegetables into hummus. Create a trail mix with dried fruit, nuts, and seeds.\n8) Read labels. Manufacturers know that consumers are looking to eat more fiber. Check the label to see how many grams a product really contains. When an item says it’s an excellent source of fiber, it must contain at least 5 grams per serving.\nThe verdict is clear. Fiber can help manage weight now, and help protect from heart conditions and other diseases with the aging process. To help reach the golden years with fewer health issues and higher functioning, eat more complex carbohydrates like vegetables, nuts, breads, and fruits.\nHappiness is elusive without a sense of self-worth. If you’re not happy with yourself, how can you be happy about life in general? Self-worth is about believing that you have value as a person. It’s not how you compare to others. It’s your belief in your intrinsic value as a human being. We’re all important. Some of us just require a little more convincing than others.\nMaximize your feelings of self-worth with these 11 ways:\n1) Forgive yourself. You have plenty of guilt and resentment toward yourself that you’ve been carrying around for years. You chose the wrong major in college or broke up with your significant other via text. Give yourself a break and vow to do better in the future.\n2) Volunteer. Spending your time in a valuable and selfless way is a quick means to raise your opinion about yourself. Just two hours a week has been shown to make a difference.\n3) Avoid gossiping. Gossip probably has gotten you in trouble in the past, and you know you shouldn’t do it. Try pointing out positive traits and actions in others, instead. You’ll both feel better!\n4) Face a few of your smaller fears. We feel bad that we’re afraid of things that we know we shouldn’t fear. Face a few small fears and overcome them. You’ll be impressed with yourself.\n5) Set personal boundaries and enforce them. There’s no reason to allow others to use or manipulate you. Decide what you consider to be reasonable behavior and enforce it. Word will spread, and your life will be smoother.\n6) Set goals you can achieve. It’s important to have big goals, but small goals are important, too. Big goals lead to big accomplishments, but smaller goals can provide a great lift to your self-worth. Make it easy to succeed on a regular basis. Completing your daily to-do list is an example of completing a smaller goal.\n7) Define your values and live by them. It’s important to determine what you’re willing to do and what you’re not willing to do, as well as what’s important and unimportant to you. If you’re able to live by these values, you won’t have a reason to think badly of yourself.\nMost people don’t take the time to define their values. They rely on a vague sense of right and wrong from childhood. Give it some thought and put your ideas down on paper. When you’re stuck with a tough decision, pull out the list.\n8) Be willing to make changes in your life. The same life will provide the same results. Some changes will be necessary if your goal is to enhance your self-worth.\n9) Voice your opinion. Do you keep your opinion to yourself because you think no one cares? Maybe you put everyone else’s happiness above your own. Neither is good for your self-image. Your opinion matters. Share it.\n10) Minimize time spent with others who don’t appreciate you. Do you have a friend or family member that drags down your self-worth? Think about letting them go or spending less time with them.\n11) Make decisions. Indecisiveness creates stress and frustration. It’s also a way of avoiding responsibility. Take a reasonable amount of time and then pull the trigger. You’ll feel more in control of your life.\nWhen you think negatively about yourself, you lower your expectations. You even begin to look for reasons to support those thoughts. This is the ultimate in self-defeating behaviors. Your feelings of self-worth influence your ability to be happy and successful. Be proud of who you are as a person. Life will be easier and more enjoyable.\nWhen caring for others, it can be easy to forget about our own needs. It may take more than an apple a day to keep the doctor away, but a healthy diet and other simple lifestyle changes can keep you from becoming ill. Learn how to develop habits that will keep you fit and strong.\nMany experts blame the Standard American Diet (SAD) for high rates of obesity, diabetes, depression, and other serious conditions. Good nutrition can strengthen your immune system and lower your risk for many illnesses.\n1. Eat more produce. Fruits and vegetables are nutrient dense and light in calories. They’ll boost your immune system and help you stay hydrated. Plus, all that fiber can lower your risk of diabetes.\n2. Focus on whole foods. Processed foods are usually loaded with excessive fat, sugar, and salt. Try eating foods in their natural state.\n3. Limit alcohol. Too many cocktails can damage your liver and other organs. Most experts recommend up to one drink a day for women and two for men.\n4. Manage your weight. Carrying around too many pounds increases your risk of heart conditions, arthritis, and certain cancers. Stay slim by watching calories and leading an active life.\nOther Lifestyle Changes\nHere are a few more changes to go along with your balanced diet. They’ll have a major impact on your body and mind.\n5. Move around. Physical activity strengthens your heart and muscles. Aim to exercise at least 3 days a week. Take the stairs instead of the elevator.\n6. Sit less. Research suggests that the longer you sit, the poorer your health may be even if you exercise. If you have a desk job, try taking walking breaks every half hour. Cut back on your TV time.\n7. Do yoga. While any form of exercise and relaxation can be beneficial, yoga seems especially powerful. A study at Massachusetts General Hospital recorded a whopping 43% reduction in healthcare use among patients who studied yoga for a year.\n8. Deal with stress. If yoga is not your cup of tea, there are other ways to keep tension from piling up. Book a massage or listen to gentle music.\n9. Be happy. The more you’re satisfied with your life, the less you’ll need your doctor. On a scale of 1 to 6, a patient could expect an 11% decrease in doctor visits for each level of higher life satisfaction, according to one University of Michigan study.\n10. Adopt a pet. Holding your cat is good for mental and physical well being. The CDC says pets help people lower their blood pressure, cholesterol, and triglyceride levels. They also provide an antidote to loneliness.\n11. Connect with others. Speaking of loneliness, support from humans helps too. Close social ties can help you catch fewer colds, and may even extend your life.\n12. Sleep well. Adequate rest and sleep is vital to healing. Turn off the computer and TV in the evening and go to bed on time.\n13. Quit smoking. Giving up tobacco may be the most important thing you can do for your health. It takes an average of 5 to 10 attempts to quit for good, so hang in there.\nWorking through grief and loss, it is importance to maintain our physical health but mental and emotional health are just as critical. Exercise prevention whenever possible. Mental and emotional health issues can be very debilitating and challenging to treat.\nPlace a premium on your mental and emotional well being:\n1. Spend time with others socially. Your mental and emotional health won’t be optimal if you split all of your time between work and home. Spend some time in the company of others. Wednesday afternoon staff meetings don’t count! Humans are social beings and social interaction is a component of good mental health.\n2. Train your brain. Keep your brain active and healthy. Buy a book of crossword puzzles or brainteasers. Play chess or learn a musical instrument. One of the best ways to preserve your brain health is to challenge it each day.\n3. Take time for yourself. Between work, family, and maintaining a home, it can be challenging to find personal time. Nevertheless, spend a little time each day doing something that you enjoy. Even if it’s just sitting alone on your deck with a cup of coffee in the morning. Social activity is important. A little solitude is important, too.\n4. Spend time with positive people. Your thoughts, attitudes, and expectations mirror those of the people around you. By spending time with positive people, you’ll be more positive, too. Whom do you spend time with now\n5. Find a job that you enjoy. Besides sleep, you spend more time working than you do on any other single activity. If your job makes you miserable, your emotional health is sure to suffer. Money isn’t everything. Consider finding a career that you enjoy. You’ll be happier and more positive about the future.\n6. Share your time and yourself with your community. This is also a great social activity. You feel better about yourself and increase your sense of self-worth when you help others. There are plenty of opportunities to volunteer. Investigate a few and make a decision.\n7. Spend more time doing things you love to do. What makes you happy? Spend more time doing it. Do you love golf? Build a weekly round of golf into your schedule. How can you not be happier if you spend more time doing things that make you happy?\n8. Get sufficient sleep. Sleep is necessary for good health, both physical and mental. Those that lose the ability to sleep due to accident or illness die within several weeks. Your ability to manage stress, make smart decisions, and regulate your emotions are dependent on getting enough sleep.\n9. Have a compelling future. For your emotional health to be at its best, you need something to look forward to. It might be a date on Saturday night, a vacation, or retiring. It doesn’t matter what it is, but you need something in the future that makes you smile when you think of it.\nGive you mental health as much attention as you do your physical health. Strive to live a well-rounded life filled with enjoyable activities and people. Get enough sleep and find constructive ways to deal with stress. With good mental health, you’ll be able to handle any challenge that life throws at you.\nRemember to get professional help if you’re unable to maintain good mental health. You wouldn’t attempt to treat a physical disease by yourself. Put as much importance on your mental health. Avail yourself of expert assistance if necessary.', 'We tend to think of fiber as something that keeps us ""regular,"" but that\'s only part of the picture. In fact, fiber is an overlooked anti-aging medicine.\nFor nine years, researchers tracked more than 219,000 men and 168,000 women between the ages of 50 and 71, and found that those who ate the most fiber enjoyed longer lives. The study, published in the Archives of Internal Medicine, found this: Men who ate 29 grams (compared to 19) of fiber daily, and women who ate 26 grams (compared to 11) daily, were 22 percent less likely to die for any reason.\nIt isn\'t all that difficult to get that extra 10-15 grams each day. One-half cup of navy beans contains 9-10 grams; one tablespoon of chia seeds packs approximately 3-6 grams; and supplements are another source. And those small additions can have a big impact. Types of Fiber\nPlant foods contain a combination of soluble (able to mix with water) and insoluble fiber, and while we need both, each one does a slightly different job. Soluble fiber binds with toxins and makes it possible for them to be eliminated from our bodies. It also reduces cholesterol. Insoluble fiber passes through the digestive tract like a street cleaner, and helps to keep us ""regular.""\nSoluble fiber also increases bulk in the stomach, creating a sense of fullness; and it slows down the breakdown of carbohydrates, helping stabilize blood-sugar levels and reduce cravings. Both these actions help with weight loss, especially with fat around the abdomen.\nDeep belly fat that surrounds our organs is associated with high blood pressure, diabetes, and fatty liver disease. Researchers at Wake Forest Baptist Medical Center in Winston-Salem, N.C., tracked fiber intake and measured belly fat with CT scans in more than 1,000 people, and found that eating soluble fiber significantly reduces\nthis harmful fat.\nCapsules, chewable tablets, powders, and liquid supplements offer fiber from a variety of sources, including psyllium husks, different fruits and vegetables, legumes, grains, and seeds. In supplements, amounts of fiber per serving vary from approximately 2 to 12 grams. When choosing fiber supplements, consider the amount of fiber in your diet and add enough to reach your daily requirement.\nWhen increasing your fiber intake, do so gradually, in small amounts, and drink plenty of water to avoid digestive upset. And take fiber at a different time of day than other supplements. This will prevent nutrients from binding with fiber and being excreted.\nDaily Fiber Requirements\nMen over 50\nWomen over 50\nSource: Institutes of Medicine\nWhere’s The Fiber?\nAll plant foods contain fiber, including sprouted and whole-grain nutrition bars and breads. These are some examples of fiber content.\n1 serving Cereals: Mass-market “high fiber” brands may contain artificial sweeteners and preservatives and added fiber extracts, whereas health-food store brands usually contain fiber in a more natural form, from whole grains, bran, flax seed, and/or chia.\n½ cup navy beans\n½ cup pinto or black beans, lentils or split peas\n½ cup garbanzo, kidney, lima or white beans\n1 Asian pear\n1 cup fresh or frozen red raspberries or blackberries\n1 Tbs. chia seed (exact content of seeds differs by variety; check labels)\n2 Tbs. ground flax seed (exact content of seeds differs by variety;\n1 medium (3-inch diameter) apple\n1 medium potato with skin\n1 medium sweet potato (5 inches long, 2-inch diameter; flesh only)\n1 medium banana (7 to 8 inches long)\n½ cup brown rice, long grain\nSource: USDA National Nutrient Database\nGarden of life RAw fiber is ideal for those who want an alternative to psyllium, which can be too harsh for some. Raw, vegan, organic, and gluten-free, RAW Fiber has soluble and insoluble fibers from sprouted seeds, grains, and legumes.\nTrying to lose weight? Natural factors pgx ultra matrix plus soothe digest is a patent-pending fiber blend (water-soluble) that has been shown to reduce appetite, regulate blood sugar, and help with weight loss.\nHave a little fun with your fiber supplement: Nutrition NOw Fiber gummies marry the delicious taste of fruit-flavored gummy candy with high-quality fiber (4 gm per serving). Great for travel, this product is new and just hitting health food store shelves.']"	['<urn:uuid:8a7a0eec-f9f6-487b-bcc9-467a76d655ca>', '<urn:uuid:f5759e72-d8d6-4dc2-8a3c-e846dc51ea58>']	open-ended	direct	long-search-query	similar-to-document	comparison	expert	2025-05-13T02:03:54.148118	11	110	3030
93	How do trees affect grass growth, and how do green roofs impact buildings?	Trees affect grass growth by changing both light quantity and quality - shifting light to 'far red' wavelengths that reduce photosynthesis, resulting in thinner grass leaves and stems, and creating moisture problems that increase disease risk. As for green roofs' impact on buildings, they provide better insulation, reduce power consumption, retain stormwater, and improve air quality. However, they also introduce risks like potential water damage from leaks, additional fire load from combustible components, increased structural load from landscaping and water, and vulnerability to wind and seismic forces.	"['Lawn – Growing in Shade\nOK. I admit it. I have given up. I came to the conclusion that grass just would not grow in the corner of my lawn near the ornamental cherry trees. The shade there is so dense that my grass, no matter how lush in April, thinned out to bare soil for five years running. This spring I planted hydrangea, white impatiens, golden hosta and variegated ivy there and the spot looks better than it ever has.\nTrees and shade look nice in the landscape. However, it is difficult to grow grass under trees because not only the quantity but also the quality of the light changes in the shade. In full sun, light is in the “near red” range of wavelengths. In the shade it shifts to the “far red,” which is less effective for photosynthesis. The result is a reduction in the food needed for turf growth. Leaves and stems of grass plants are thinner in shade. Moisture from dew under trees takes longer to evaporate. The additional moisture may contribute to an increase in disease.\nWith the deck stacked against it, it is remarkable that any grass grows under trees. If you let experience, rather than me, be your teacher, these tips will help you grow grass in the shade. Go to How Much Light Does a Lawn Need before you read further.\nCHOOSE RIGHT GRASS Under high pine tree shade, bermudagrass and zoysiagrass can grow successfully. Beneath the shade of a dogwood or an oak, both fail miserably. Use turfgrasses adapted to your area of Georgia. Fine and tall fescue are adapted to north Georgia. Dependent on the variety; zoysiagrass, centipedegrass, and bermudagrass can be grown throughout the state.\nCool season grasses –Fescue is the most shade-tolerant grass that is adapted to the Atlanta area but there is little difference between the various named fescue seed brands. Fine fescues may survive in some situations. When establishing cool-season grasses, it is best to seed or sod early enough in the fall so there is sufficient time for the turfgrass to mature before leaves cover the ground. During the fall remove leaves by raking, blowing or bagging when mowing to prevent smothering of the grass. Cool-season turfgrasses will grow long after deciduous trees have dropped their leaves.\nWarm season grasses – In general bermudagrass is intolerant to shade. Two newer cultivars, TifGrand and Celebration have shown good persistence under shade conditions. These cultivars may be options in environments where the grass receives 6 hours or better of filtered to intermittent shade. Likewise, centipedegrass needs the same light conditions as these two bermudagrass cultivars.\nSome St. Augustinegrass cultivars have suitable tolerance to shade. Examples include Palmetto, Mercedes, and Raleigh.\nSimilarly, some zoysiagrass cultivars are tolerant to limited light environments. Zoysiagrass cultivars that have good tolerance to shade include El Toro, JaMur, Zeon, and Zorro. These species have performed well under a heavy hardwood shade for 5 years. Cavalier has also been reported to have good shade tolerance while the older cultivar, Meyer, has fair tolerance.\nPRUNE TREES Selective pruning and thinning of limbs within the tree canopy will allow better sunlight penetration. Single tall trees should have all limbs removed below 10 feet. Rake and remove leaves or needles before they accumulate.\nMOW HIGHER Grass in the shade should be cut approximately 1/2 to 1 inch higher than the grass growing in full sunlight. This will allow more leaf area to intercept the limited amount of sunlight. Avoid scalping turf in the spring. Decline of turf in shade often begins after a single episode of scalping.\nWATER MORE Tree roots compete with the turfgrass for available soil moisture. In addition, the distribution of moisture from rainfall is poor under a tree canopy. The tree leaves block or deflect the rainfall. This yields an uneven soil moisture pattern. As a rule of thumb, turfgrass in shade needs 1 inch or more of water per week. In the absence of rainfall, wet the soil to a 6-inch depth. This may take 2 to 3 hours for one irrigation application.\nMINIMIZE TRAFFIC During the summer months, minimize the traffic and activities in the shaded grass areas. This will reduce the wear stress on the turf. If the grass is in a path where traffic can not be avoided, place stepping stones or pine bark to mark the trail.\nLIMIT FERTILIZER Grass grown in the shade should receive half the fertilizer than that in the sun. Follow the fertilizer schedule at my Lawn Care Calendars page\nLIMIT WEEDKILLERS Use broadleaf weed controls sparingly on shaded lawns. Spot-spray special weeds like wild violet. Crabgrass controls are not necessary in a shade environment since crabgrass needs a high light intensity to germinate.\nUSE GROUNDCOVER OR MULCH In areas where even fescue fails, think about using light-colored groundcovers like I did. A layer of pinestraw or pine chips looks much better than bare dirt. As one of my gardener buddies says, “Nature always bats last!”\nsee also Using Mondo Grass for a Lawn', ""Sustainability Series: Green Roofs\nArticle information and share options\nWith increased urbanization, green roofs are becoming a major trend. They offer many advantages to building owners and occupiers, the general public, and the environment: better insulation, reduced power consumption, stormwater retention, improved air quality, and a biodiverse environment offering aesthetic diversity. However, green roofs may introduce property considerations and concerns that owners and occupiers should be aware of when reviewing their property and business risks.\nGreen roofs comprise specific vegetation designed and installed on a series of components, including moisture retention material, a drainage system, a root barrier, and a protective layer for both the insulation and underlying roof surface.\nProperty Risk Considerations\nLet's take a closer look at a few features that will likely be of interest to commercial property insurers when evaluating the building's overall risk profile.\nDownload our guide\n1. Potential for water leakage damage\nThe irrigation pipework may be susceptible to freezing. Leaks in waterproofing membranes due to root growth or temperature fluctuations may expose the underlying roof structure to damage or corrosion. Drainage systems may become blocked by soil and vegetation. These risks are particularly relevant in the construction phase.\nLoss Prevention Tips: Penetrations in the roof should be minimized. It is important to include leak detection systems below the waterproofing membrane. Size gutters to accommodate both rainfall and irrigation runoff and maintain them regularly, with specific inspection points included. Consider the impact of roof gradient on the density of growth media and its propensity to shift or slide during heavy rain events.\n2. Fire load\nGreen roofs and their supporting components (e.g., vegetation and waterproofing membrane) typically add combustible loading to a roof and increase the potential for ignition from exposing fires or other ignition sources. In some cases, the design of a green roof may encompass space for recreational activities, introducing ignition sources such as lighting, electrical installations, barbecues/grills, and smoking.\nLoss Prevention Tips: Limit the overall fire risk by maintaining adequate moisture content of vegetation, regular removal of dead vegetation, and careful design and placement of noncombustible fire breaks. Include manual firefighting equipment such as fire hydrants, fire hose reels, and portable fire extinguishers, along with access provisions for the fire brigade. Potential ignition from electrical equipment, smoking, or other sources should be identified and carefully managed.\n3. Susceptibility to collapse\nGreen roofs introduce live loads associated with landscaping, precipitation, induced saturation due to irrigation, and periodic replacement of new soil and growth of vegetation.\nLoss Prevention Tips: The load carrying ability of concrete roofs versus all other types (e.g., long span steel) should be considered over the roof lifecycle. Some roofs may be susceptible to deformation over time, affecting the operational efficiency of certain components that originally functioned properly. Future changes may affect the structural integrity and should undergo formal review. Concrete roofs are generally more resilient than other roofs to changes that increase live loading.\n4. Damage from natural hazards\nGreen roofs are susceptible to natural hazard perils such as seismic forces and wind uplift pressures. Vegetation or potentially the entire roof system may be damaged, requiring replacement.\nLoss Prevention Tip: The system, including any moisture/root barriers, should be properly secured to structural elements (growth media should not be relied upon). Green roofs should not generally be installed in areas with elevated wind exposures such as coastal areas subject to hurricanes.\nGreen Roofs provide many environmental benefits, especially in urban locations where vegetation may be scarce. However, if a green roof is to be installed, ensure that the considerations outlined above are incorporated in the design, installation, and future maintenance requirements to mitigate the potential for loss.""]"	['<urn:uuid:5602fd49-4529-4678-8e9d-4f83d73bd47e>', '<urn:uuid:adefe97e-66fa-495a-b3a2-8e24a422fce1>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-13T02:03:54.148118	13	87	1448
94	I study disaster response mechanisms and notice both Lindblad Expeditions and EEI's Mutual Assistance Program involve coordination between multiple organizations - how do their approaches to crisis management differ in terms of funding and resource allocation?	The two programs take distinctly different approaches. Lindblad Expeditions' Galapagos relief effort focuses on financial aid through a matching program where every $1 donated is matched 3:1 by multiple entities (Wanderlust Fund, Lindblad Expeditions, and their Board of Directors), with funds distributed as microloans through a local non-profit. In contrast, EEI's Mutual Assistance Program coordinates physical resources and personnel, allowing utilities to 'borrow' restoration workers including line workers, tree trimmers, and support staff from other companies. The Mutual Assistance Program involves coordinating accommodation, equipment transport, and 16-hour work shifts, while the Galapagos relief focuses on sustainable business development and meeting basic community needs.	"['Lindblad Expeditions and Island Conservation launch Galapagos Island Relief Fund to help the Galapagos community build toward a brighter, more certain, and sustainable future after COVID-19 devastated the tourism economy.\nFOR IMMEDIATE RELEASE – December 17, 2020\nImages available upon request.\nLindblad Expeditions, a global provider of expedition cruises, and Island Conservation, an international not-for-profit conservation organization, have launched the Galápagos Island Relief Fund—to provide immediate financial relief for the people of the Galápagos Islands who have been devastated by the socio-economic impact of the COVID-19 pandemic.\nTourism accounts for more than 80% of the Galápagos economy, so in March, when travel restrictions suddenly halted the tourism industry, it caused profound hardship for families, businesses, and the communities at large. In the first two months of the pandemic alone, the islands’ economy lost $50 million, almost a quarter of the annual income; and the Galápagos Chamber of Tourism recently reported that visitors between July and October dropped a staggering 95%. The economic devastation also poses a serious threat to the conservation of this extraordinary and pristine archipelago.\nDonations received by the Galápagos Island Relief Fund will enable Fundación Un Cambio Por La Vida (FUNCAVID), a local non-profit organization to disseminate microloans to the people of the Galapagos..\nThe monies will provide immediate financial relief, address the basic needs of vulnerable families, allow children to stay in school, and help sustainable businesses survive, grow, and begin developing new income sources. These lines of credit will support hundreds of community enterprises and initiatives in the areas of local entrepreneurship and strengthening existing businesses, sustainable food production and marketing, emergency aid for vulnerable families to meet basic living costs, and education. So, the community can continue its inspiring journey of protecting the unique ecosystems and wildlife of their island home.\n“The people of Galápagos are struggling more than I’ve seen in my 53-year relationship with the Islands,” stated Sven Lindblad, Founder and CEO of Lindblad Expeditions. “Stimulating the local economy through community micro-loans has the power to activate a thriving system – helping entrepreneurs and small business owners develop new ideas to complement sustainable tourism, and meeting the needs of the local community long after the pandemic is over.”\nThe main fundraising mechanism will be outreach to Lindblad guests who have voyaged with them to Galápagos. The small ship cruise line has also set up a matching program, where every $1 donated to the Galapagos Island Relief Fund (up to $50,000) will be matched 3:1 by an equivalent dollar-for-dollar donation from each: Sven and Kristin Lindblad’s Wanderlust Fund, Lindblad Expeditions, and the Lindblad Expeditions Board of Directors.\n“The Galapagos Island Relief Fund will enable Galapagos community members to further their education, create new businesses, and increase local food security—steps that are essential to building a more sustainable way of life on the islands, in which, people and wildlife can thrive,” explained Karl Campbell, Island Conservation’s Executive Director of Latin America.”\nEvery dollar raised (100% of funds donated) will stay in Galápagos. Persons interested in supporting the Galápagos Island Relief Fund may do so here. All donations (through Island Conservation, a 501(c)3 charity) are tax deductible to the fullest extent allowed by law.\nDaniel Proano, FUNCAVID Project Coordinator noted, “Together we will transform the strongest crisis that has impacted Galapagos and the world into an opportunity building bridges to bring our heritage to a higher level of resilience. Through your donation, we will be able to empower the dreams and creativity of the islanders, driving a productive reactivation through innovative ventures, with a social focus and the use of good environmental practices.”\nFUNCAVID was established in 2009 by the Proaño family, after a family calamity, with the purpose of promoting positive changes and improvements in the life of the Galapagos people. The Foundation aims to bring together, include and convene without exception all the citizens of Galapagos to be part of the realization of changes that raise awareness, promote and motivate the protection of life, the recovery of the values of the island culture and good living in order to generate spaces for discussion and monitoring of the construction of a sustainable community with efficient public policies and clear citizen agreements.\nAbout Island Conservation\nIsland Conservation’s mission is to prevent extinctions by removing invasive species from islands. Working alongside local communities, government management agencies, and conservation organizations, Island Conservation focuses on removing a primary threat to island wildlife – introduced invasive vertebrates – saving threatened plants and animals, restoring island ecosystems, and supporting island communities. Since 1994, they have successfully restored 64 islands worldwide, benefiting 1195 populations of 487 species and subspecies.\nAbout Lindblad Expeditions\nLindblad Expeditions is an expedition travel company that focuses on ship-based voyages through its Lindblad Expeditions brand and on land-based travel through its subsidiary, Natural Habitat Adventures, an adventure travel and ecotourism company with a focus on responsible nature travel.\nLindblad Expeditions works in partnership with National Geographic to inspire people to explore and care about the planet. The organizations work in tandem to produce innovative marine expedition programs and to promote conservation and sustainable tourism around the world. The partnership’s educationally oriented voyages allow guests to interact with and learn from leading scientists, naturalists and researchers while discovering stunning natural environments, above and below the sea, through state-of-the-art exploration tools.\nInterviews are available with:\n- Island Conservation: Victor Carrion, Ecuador Program Manager (available for Spanish speaking interviews)\n- Lindblad Expeditions: Amy Berquist, VP, Conservation, Education & Sustainability\n- FUNCAVID: Daniel Proaño, Project Coordinator; Patricio Proaño, President (available for Spanish speaking interviews)\n- Lindblad Expeditions and Island Conservation Launch Galapagos Island Relief Fund – December 17, 2020\n- Island Restoration Set to Save Coral Reefs – October 19, 2020\n- Emerging Research on Tetiaroa Atoll – October 16, 2020\n- Restoring Tetiaroa Atoll from Ridge to Reef – October 16, 2020\n- Press Release: Pursuing Sustainable Development for Island Communities by Removing Invasive Species – August 13, 2020\n- Press Release: Advancing Drone Technology for People and the Planet – July 1, 2020\n- Restoring Balance and Building Climate Resilience on Tetiaroa Atoll – June 24, 2020\n- Fishing Community Strives for Healthy Oceans Surrounding Pajaros Uno Island – June 24, 2020\n- The Plight of the Pink-footed Shearwater – June 24, 2020\n- Monitoring Lehua Island After Dark – June 24, 2020\nView All Posts', ""You’ve seen the dramatic images – roofless buildings, flipped cars, broken windows, a line of power poles pulled down. Just a few testaments of how powerful storms can leave astonishing destruction.\nAs communities pick up, clean up and recover, the massive task of restoring electric power is often beyond the capabilities of local utilities. So major disasters are answered by Edison Electric Institute’s (EEI) Mutual Assistance Program, a voluntary partnership of investor-owned electric companies from across the country that mobilizes to help speed restoration to major outage events.\nElectric companies impacted can, in effect, “borrow” restoration workers from other utilities in the region. This can include line workers, tree trimmers, damage assessors, and even call center support. The objective is to restore power quickly and safely by clearing debris, rebuilding power lines and replacing poles.\nHow Xcel Energy answers mutual assistance requests\nKelly Bloch, Regional Vice President for Distribution Operations with Xcel Energy, states, “We’re always watching the weather to see where a major event may happen. We’re pretty good at estimating when a request will come our way. Then we determine our ability to respond by how vulnerable we may be to storms in our area and maintaining our ongoing projects.”\nUsually, Xcel Energy contractors are first asked to participate in a Mutual Assistance request. When a Mutual Assistance call requires a more considerable amount of support, Xcel Energy employees are engaged. In most years, Xcel Energy answers one to four Mutual Assistance requests.\nWhat’s it like for line workers offering mutual assistance?\nResponding to a Mutual Assistance request, Xcel Energy sends a turn-key group. This group includes line workers along with the full support they need to be as productive as possible. This involves managers, mechanics, safety and occasionally damage assessment staff, all working as a self-sufficient team on location.\nCritical to each Mutual Assistance project is a staging site. Here’s where the host utility provides a secure place for the crews to use as a base for their operations. In addition to being able to access the materials needed to perform the restoration work there are often giant tents with cots, mess halls, portable showers and other necessities are placed. Or, buses will shuttle workers to and from local hotels or college dormitories.\nCrews typically work 16-hour shifts with eight hours for rest. Large Mutual Assistance efforts such as with Hurricanes can last two to three weeks.\nOperating in the new COVID-19 reality\nWith the new safety and social distancing requirements of COVID-19, Mutual Assistance programs have adapted. Now, more separated accommodations are needed, along with smaller gathering places. For example, sharing specifics about the local power system and other details previously took place at a meeting at the staging area. These are now sent via electronic documents and voice messages. While crews miss out on some of the camaraderie, the new process is proving to be effective.\nClick on above image to watch video.\nOur Puerto Rico story\nOne of the most prolific Mutual Assistance projects was for Hurricane Maria, a devastating Category 4 storm that made landfall in Puerto Rico in September 2017. All critical infrastructure and least 80 percent of the island's energy grid was severely damaged.\nNearly 60 power utilities provided approximately 3,000 line workers and support personnel to take on the task in tropical heat and ruined infrastructure.\nFor Xcel Energy, the project started with six weeks of prep, moving trucks and equipment to the Gulf Coast and then onto barges for 10-days of ocean travel. Crews flew to Puerto Rico where three teams rotated three-week durations. Line workers rose to the challenge and made positive impacts with many island communities.\nOther Mega-Mutual Assistance responses\n- Superstorm Sandy in October 2012, 10 million customers lost power across 24 states in the Northeast and Mid-Atlantic. 80 utilities respond.\n- The June 2012 Derecho—a sudden and widespread storm with peak wind gusts ranging from 80–100 miles per hour—four million people lose power across Ohio and the Mid-Atlantic. 30,000, company workers and crews from as far away as Canada, Texas, and Wyoming.\n- In August 2011, Hurricane Irene hit the East Coast, nine million customers without power. Nearly 50,000 electric company restoration workers respond.\n- Hurricane Katrina hit the Gulf of Mexico in August 2005, damaging a 400-mile section of coastline across Louisiana, Mississippi, Alabama and western Florida, destroying the energy grid. More than 46,000 electric company restoration workers and contractors respond.\nCome home safe\nKelly Bloch continues, “For us, safety is the overall priority. Making sure the public is kept safe during traumatic times and keeping our crews safe as they do what they do best, get the power back on.”\nBe ready for an outage\nTo help keep you and your family safe during storms and outages, use our resource page.""]"	['<urn:uuid:9bd4ad22-ae08-422e-a0ab-e201daf9ba0f>', '<urn:uuid:b8b8817a-6b3a-45fd-bc13-59a69bdbaad3>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-13T02:03:54.148118	36	103	1855
95	As a researcher studying racial disparities during COVID-19, I'm curious about how food insecurity rates compared between different racial groups in Massachusetts, and how did these disparities relate to broader socioeconomic impacts across the United States?	During the COVID-19 pandemic, there were significant racial disparities in food insecurity rates in Massachusetts, with White and Asian communities having much lower rates (around 5%) compared to Latinx households (24%). Black households also experienced a significant jump in food insecurity in 2020. These local patterns reflected broader national disparities in adverse socioeconomic outcomes, where food scarcity rates for racial/ethnic minorities (except Non-Hispanic Asians) were more than two times higher (16.2-19.3%) than for Non-Hispanic Whites (7.2%), with one out of five Non-Hispanic Blacks reporting food scarcity.	['Despite the fact that Massachusetts had a strong economy before the COVID-19 pandemic hit, we still had not done nearly enough to leverage this aggregate wealth to ensure that all families had access to healthy, safe, affordable food—as of 2019, roughly 8 percent of adults in Massachusetts were food insecure. The State of Hunger in Massachusetts details many of the ways in which Black and Latinx families experienced these food-related challenges at disproportionately high rates even when the economy was stronger. To complement that report, this research brief provides new insights into how these longstanding food insecurity challenges have been exacerbated by the current pandemic. We pair this analysis with a discussion of actions policymakers could take to remove barriers and expand eligibility for food assistance to avoid even greater hardship and long-term harm to the people of this Commonwealth.\nImpact of COVID-19 Pandemic on the Economy and Food Security\nBefore the COVID-19 pandemic, the Massachusetts unemployment rate was at historically low levels and below the US average. However, not everyone benefited equally from this strong economy. The high cost of living meant that many families—disproportionately families of color—were struggling to find adequate housing and pay for food, health care, child care and other critical expenses, even as they held full-time jobs. The coronavirus crisis only served to intensify this situation. In March 2020, as a critical part of the public health strategy to stop the spread of COVID-19, state and local officials shut down large swaths of our economy that relied on face-to-face interactions. Sectors hardest hit include food service, retail, tourism and education. Between March and April, the state unemployment rate increased almost six-fold and eclipsed the national unemployment rate. By June, Massachusetts had the highest unemployment rate in the country. Despite some recent improvements, we still have the 6th highest unemployment rate in the country. Lessons from prior downturns tell us that vulnerable workers in lower-wage, public-facing jobs with fluctuating hours—like retail and food service—will have some of the slowest recovery, unless different policy choices are made.\nWith economic pain mounting, Congress passed the Coronavirus Aid, Relief, and Economic Security (CARES) Act in March 2020, which provided expanded unemployment assistance for many who lost jobs. The federal response helped stave off hardship for many families, but not for everyone. Some people never received any assistance because they did not qualify, including undocumented immigrants (who pay into the state unemployment system) and some recent college graduates. Moreover, the CARES Act’s Pandemic Unemployment Compensation provision, which automatically added $600 per week to all unemployment assistance recipients’ benefits, ended in July, causing a new spike in families struggling to afford food.\nMassachusetts has traditionally had lower food insecurity than the national average and for the population overall this remains true during the crisis. However, the pandemic caused food insecurity to double, and this has affected our communities in different ways (see Definitions and Methodology for food insecurity definitions and estimation procedures).\nOne example of the how different types of households experience food insecurity is that it tends to be higher among households with children. In Massachusetts, there is a five percentage point gap between households with children (19% of which are food insecure) and households without children (14% of which are food insecure).\nEven greater disparities emerge when we compare food insecurity across racial/ethnic groups, with Black and Latinx communities experiencing food insecurity at higher rates both during times of relative economic stability and crisis, due to persistent systemic racism and discrimination. Systemic racism, perpetuated by longstanding discriminatory institutions and public policies, has and continues to disempower people of color, especially Black and Latinx people, by restricting access to opportunity and resources like jobs, income, education, workplace protections and wealth. These, in turn, lead to greater vulnerability to economic shocks.\nAs demonstrated in The State of Hunger report and reinforced by this new analysis of trends during the pandemic, White and Asian communities have consistently had much lower rates of food insecurity than Black and Latinx populations. During the years preceding the pandemic, food insecurity for White and Asian households hovered around 5 percent. By contrast, food insecurity among Latinx households was 24 percent. For Black households, in the years leading up to the current crisis, food insecurity trended downward, before jumping in 2020. A robust evidence base, including research from Children’s HealthWatch and a new study from the Massachusetts Immigrant and Refugee Advocacy Coalition, demonstrates that immigrants also disproportionately struggle with food insecurity, especially more recent immigrants and undocumented immigrants.\nAs food insecurity has risen, so has the share of households relying on free meals or groceries to keep food on the table. While 5.6 percent of respondents to the Household Pulse Survey in May said they or someone in their household received free meals or groceries within the past seven days, this figure increased to 7.5 percent and then to 8 percent in June and July, respectively.\nAmong households who received free meals or groceries, the most common source was school-based or children’s meal programs. This may have included Pandemic-Electronic Benefits Transfer, although these data do not explicitly identify the program (detailed below). From May through July, an average of 161,125 adult respondents lived in households that received free food through a school-based or children’s meal program each week. Food banks and food pantries were the second most common source of free food, followed by family or friends, or other community-based programs.\nApplications for public assistance also rose dramatically early on in the crisis, with a 400 percent increase in weekly applications for Supplemental Nutrition Assistance Program (SNAP), Transitional Aid to Families with Dependent Children (TAFDC) and Emergency Aid to the Elderly, Disabled and Children (EAEDC). SNAP accounted for the largest share of increased applications, suggesting that the pandemic and the economic slowdown had an almost immediate effect on access to food and demand for food assistance. While the situation is dire, the uptick in SNAP applications reflects the proper functioning of the program—SNAP enrollment is intended to increase in a downturn. This may be further evidenced by the second surge in SNAP applications in early August, just as the additional weekly $600 in unemployment assistance provided for in the CARES Act expired. Not only does this demonstrate that SNAP is filling a critical gap, it is also indicative of just how tenuous the economic position of many Massachusetts families truly is.\nSNAP is an entitlement program, meaning that the federal government must provide food support to all families that are eligible (many immigrants are not currently eligible). In response to the crisis, the federal government expanded SNAP in a few key ways. Congress appropriated $16 billion for additional SNAP benefits, and allowed states to automatically provide the maximum monthly benefit to all families during the pandemic. A significant shortcoming of this maximum monthly benefit provision is that the families with the lowest incomes who already received the maximum benefit did not receive any additional help during these tough times. Congress also temporarily waived work requirements typically used to determine eligibility for SNAP. Lastly, in recognition of the fact that many parents need to provide more meals for children who are home from school (often with less employment income), the government created a new program called Pandemic EBT (Electronic Benefits Transfer), which automatically provides extra money for food (no extra registration process required) to families with school-age children who qualify for free or reduced-price meals at school.\nCalls to food assistance hotlines have also risen during the pandemic. Project Bread, a Massachusetts statewide anti-hunger organization that connects people to reliable sources of food, while advocating for policies that make food more accessible so that no one goes hungry, coordinates a hotline for people struggling with food insecurity. The FoodSource Hotline assists callers with the SNAP information and the application process and connects them to other federal nutrition programs and community food resources. Data from the FoodSource Hotline show that calls from around the state increased more than five times comparing July and August 2019 to the same period in 2020. The implementation of the state’s Pandemic EBT program prompted a large share of the new calls. There was also an increase in calls about SNAP (including SNAP pre-screens, application assistance and help with recertification).\nThe FoodSource Hotline is just one example of many where local organizations are rising to the challenge and serving scores of families experiencing food insecurity in dire times. Massachusetts hunger relief organizations are both serving struggling communities and simultaneously pushing for a stronger, coordinated federal response to the hunger crisis. In the absence of more effective leadership from the government, vulnerable communities and families will continue to suffer.\nPolicy Directions for Greater Food Security and Equity\nAs in the State of Hunger report, this brief demonstrates that racial and ethnic inequities have led to adverse outcomes of the current crisis, as acutely evidenced by food insecurity rates. A savvy reader of both the State of Hunger report and this brief will recognize that many similar recommendations for systemic change and policy changes are represented in both publications. While the COVID-19 pandemic and the economic crisis it precipitated has certainly increased the prevalence of food insecurity, the fundamental sources of these inequities have not changed. Thus, knowing COVID-19 did not create but rather deepened inequity in food access, the solutions must also dive deeper than the pandemic, while simultaneously helping households today.\nMany new programs initiated in response the COVID crisis—such as Pandemic EBT and Federal Pandemic Unemployment Compensation—along with greater flexibilities in SNAP, WIC (the Supplemental Nutrition Program for Women, Infants, and Children), and school nutrition, have made a significant difference in mitigating food insecurity for households with children. While these innovations have been crucial, many came only after considerable pressure from families, advocates, lawmakers and other key stakeholders. The lack of urgency from federal administrators has caused these interventions to come too late, end too soon and generate confusion in the absence of sufficient guidance or support. Food-insecure families need these programs and flexibilities to be made permanent to ensure access to food and other supports throughout the duration of this current crisis and beyond. Here are four specific action areas for greater food security, well-being and racial equity:\n1. Promote systemic change that is responsive to historical racism and structural inequity.\n2. Strengthen federal nutrition programs.\n3. Address other co-existing hardships faced by food-insecure households, such as limited access to housing, child care, utilities, transportation, health care and employment.\n4. Prioritize equity-based approaches in future research and data collection.\nDefinitions and Methodology:\nThe analysis contained in this research brief predominantly relies on two major data sources: The Current Population Survey (CPS) Food Security Supplement (FSS) for data before the pandemic, and the Census Bureau Household Pulse Survey (HHPS) for data during the pandemic. The CPS FSS is the standard data that the United States Department of Agriculture (USDA) uses to produce official government statistics on food insecurity in the US. The USDA defines food insecurity as a household-level economic and social condition of limited or uncertain access to adequate food for all household members to lead active, healthy lives. The CPS FSS survey instrument includes 10 questions for all households and eight additional questions for households with children, which are collectively used to assign a food security status (on a four-tier scale) to each household. The top two tiers of food security (high food security and marginal food security) officially correspond to “food secure,” while the bottom two tiers of food security (low food security and very low food security) are used to designate a household as “food insecure.” Before the 10 or 18 questions used to designate a food security status are asked, there is a screener question about food sufficiency. There are four response options: always had enough of the types of food we wanted, always had enough but not always the types of food we wanted, sometimes did not have enough food, or often did not have enough food to eat. This question about food sufficiency is also included in the HHPS, but other food security questions are not included.\nTo obtain 2020 data about food insecurity, we extrapolate food insecurity rates in the HHPS using the relationship between food security and food sufficiency observed in the CPS FSS data prior to the crisis (specifically, in the pooled 2015-2019 CPS FSS data). Researchers at Northwestern University’s Institute for Policy Research (IPR) developed this method, and details are available here. One critical assumption underlying the extrapolation of food security rates from the HHPS data is that the relationship between food sufficiency and food security before the COVID-19 pandemic is similar to their relationship during the crisis. By comparing results from this method with various data sources, including the National Health Interview Survey and the COVID Impact survey, Northwestern University researchers suggest the relationship between food security and food sufficiency does predict rates of food insecurity during the current crisis reasonably well.\nThere are a few important ways in which the data in this research brief are harmonized and transformed that lead our food insecurity measure to differ from the official CPS FSS measure of food insecurity including the unit of analysis and the recall period. As a result, food insecurity data in this brief representing years 2001-2019 is fundamentally different from food insecurity data for the year 2020 and comparisons across these time periods should be interpreted as our best approximation of food insecurity in 2020. The official USDA food insecurity metric evaluates food insecurity at the level of the household, while the HHPS data assesses food sufficiency for adults 18 years and older. To harmonize the data, we produce food insecurity estimates that correspond to the population 18 and older. (While all data used in this brief from the HHPS reflect the adult population, some measures assess characteristics in the households of the adult respondents and so we describe trends among households, although the data actually correspond to share of the adult population living in households with a certain characteristic (e.g., households with/without children, households receiving free meals/groceries). Some researchers reweight the HHPS data using pseudo-household weights to obtain household-level estimates, but this can introduce more imprecision to the analysis, particularly at the state level.) Also, the official food insecurity questions ask about food security over the course of a full year, while the food sufficiency question in the HHPS only ask about the prior seven days. This likely biases our estimates of food insecurity during the pandemic downward. The procedure we undertake likely does not compromise the validity of the results; however, it should be interpreted with caution. Other researchers have adapted these methods as well, and while there are many different measures of food access and security, new analyses consistently show a large negative shock to food security since the onset of the COVID-19 pandemic.', 'Racial disparities in the adverse socio-economic outcomes of COVID-19 in the United States\nRacial disparities in socio-economic and health outcomes are a topic of concern in the United States. As Deepthi Kolady and Weiwei Zhang show, they have also played a major role in the adverse socio-economic outcomes of COVID-19 since April 2020.\nRacial disparities refer to the unequal distribution of resources, power, and economic opportunity across races in a society. Racial disparities in socio-economic outcomes such as education, employment, income/wealth, housing, mobility and rates of poverty affect the well-being of all Americans.\nAmong other things, unfavorable socio-economic environments adversely affect health outcomes. In the United States, COVID-19 mortality rates were highest among African American, Hispanic/Latino, and Native Americans (Tai et al., 2021). While this is known, detailed examination of the extent of racial disparities in the adverse socio-economic outcomes of COVID-19 is lacking.\nPre-pandemic racial disparities in adverse socio-economic outcomes\nPoverty rates among Black people (18.8%) and Hispanics (15.7%) were much higher than among Non-Hispanic Whites (7.3%) in 2019 (Bowdler and Harris 2022). Unemployment rates were highest for Blacks and American Indian and Alaska Native (6.1%) followed by Hispanics (4.3%), and lowest for Whites (3.3%) in 2019. Household food insecurity was highest for Black people (19.1%), followed by Hispanics (15.6%) in 2019 and lowest for Non-Hispanic Whites (7.9%) (Coleman-Jensen et al., 2020). Pre-pandemic levels of adverse socio-economic outcomes were higher for racial and ethnic minorities, Blacks especially, than for Non-Hispanic Whites.\nRacial disparities in adverse socio-economic outcomes since the COVID-19 pandemic\nIn a recent study (Zhang and Kolady 2022), we used real-time population survey data to demonstrate the extent of unequal and adverse socio-economic impacts of the COVID-19 pandemic on minority groups in the United States. We used Household Pulse Survey (HPS) Public Use Files (PUF), a bi-weekly online survey, developed by the U.S. Census Bureau that started on April 23, 2020. HPS data collection is implemented in phases and cycles with multiple cycles in one phase (Table 1).\nOur study focused on the following adverse socio-economic outcomes: loss of employment/ income, food scarcity, housing insecurity, and perceived unmet needs of mental health. Housing insecurity includes two outcomes: having difficulties with rent or mortgage payments and the likelihood of an eviction or foreclosure. Food scarcity measured the extent of non-availability of food (often/sometimes not enough to eat).\nWe analyzed the extent of adverse socio-economic outcomes for the following five groups: Non-Hispanic Whites, Non-Hispanic Blacks, Non-Hispanic Asians, Non-Hispanic other minorities, and Hispanics. Non-Hispanic other minorities include participants who identified as American Indian and Alaska Natives (AIANs), another race/ethnicity, two or more races (Table 2).\nOur analysis showed that employment/income loss was highest for Hispanics (26.7%) followed by Non-Hispanic Blacks (22.9%) and Non-Hispanic others (21.6%). Employment/income loss for Non-Hispanic Asians (15.7%) was close to the population average (16.3%).\nThe prevalence of food scarcity for racial/ethnic minorities except Non-Hispanic Asians was more than two times higher (16.2–19.3%) than for Non-Hispanic Whites (7.2%). One out of five Non-Hispanic Blacks reported food scarcity (19.3%).\nThe prevalence estimates of missing payments for all minority groups were more than two times higher than the estimate for Non-Hispanic Whites (7.4%): they were highest among Non-Hispanic Black people (21.5%), followed by Hispanics (16.2%), Non-Hispanic Asians (15.1%) and Non-Hispanic others (14.9%).\nAs per our analysis, 10.9% of the total population reported not receiving mental health services when they needed counseling or therapy from a mental health professional in the previous four weeks. Non-Hispanic others had the highest percentage of unmet mental health service needs (16.4%). The other groups were close to the population average with the lowest percentage of unmet mental health service needs among Non-Hispanic Asians (6.7%).\nIt is clear from our study that racial/ethnic disparities in adverse socio-economic outcomes were present during the COVID-19 pandemic, with ethnic and racial minority groups being at a disadvantage, although in different degrees. These differences may have contributed to the racial disparities in mortality rates and infections over the period. To reduce these racial disparities in health outcomes, the first imperative is to address racial disparities in socio-economic outcomes.\nBowdler, J. and Harris, B. (2022). Racial Inequality in the United States. U.S. Department of Treasury. Available at Racial Inequality in the United States | U.S. Department of the Treasury. Accessed 12/18/2022.\nColeman-Jensen, A., Rabbitt, M.P., Gregory, C.A. and Singh, A. 2020. Household Food Security in the United States in 2019, ERR-275, U.S. Department of Agriculture, Economic Research Service\nTai, D. B. G., Shah, A., Doubeni, C. A., Sia, I. G., & Wieland, M. L. (2021). The Disproportionate Impact of COVID-19 on Racial and Ethnic Minorities in the United States. Clinical Infectious Diseases, 72(4), 703-706. doi:10.1093/cid/ciaa815\nZhang, W. Kolady, D. 2022. The COVID-19 Pandemic’s Unequal Socioeconomic Impacts on Minority Groups in the United States. Demographic Research, 47: 1019-1032. DOI: 10.4054/DemRes.2022.47.33']	['<urn:uuid:485a3c13-5f9b-47be-839d-92d6b0d70830>', '<urn:uuid:723b78a9-6de6-4677-9152-f5b02251ba36>']	factoid	with-premise	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-13T02:03:54.148118	36	86	3276
96	meningitis emergency medical care symptoms when seek treatment	Medical care should be sought immediately if someone experiences the hallmark signs of meningitis: sudden fever, severe headache, and stiff neck. Additional symptoms requiring emergency care include nausea, vomiting, double vision, drowsiness, and sensitivity to bright light. It's crucial not to wait until a rash develops and to seek immediate medical attention, as delay in treatment can lead to severe consequences including permanent disability and death.	['What is meningitis and meninges?\nBrain and spinal cord are protected by a layer known as meninges. An inflammation or a swelling in meninges is termed as meningitis. Meningitis is a life-threatening infection and if left untreated, it can cause brain swelling, permanent disability and even death.\nWhat are the causes and types of meningitis?\nMeninges protect the brain and spinal cord from harmful agents and acts as a shock absorber. However, sometimes these protective layers get infected by some agents such as bacteria, viruses, etc and results in infection.\nWhen such agents enter the membrane, they become isolated from the body’s immune system and starts spreading fast. By the time body fights back, the infection spreads and becomes worse. The blood vessels becomes leaky as the body fights the infection and the infection can even spread in the brain and spinal cord, causing life threatening consequences.\nThere are various agents responsible for causing meningitis. Meningitis is classified into different types on the basis of different causative agents. They are as follows-\nWhat is bacterial meningitis?\nThere are certain bacteria that act as the main cause of meningitis. These bacteria can enter the bloodstream and reach the brain and spinal cord, thus infecting the meninges. This results in bacterial meningitis.\nBacterial meningitis can also occur when the bacteria directly invades the meninges. Many different strains are responsible of causing bacterial meningitis. Some of them are:\n- Streptococcus pneumoniae (pneumococcus)\n- Listeria monocytogenes\n- Group B Streptococcus\n- Neisseria meningitides\n- Haemophilus influenzae\nBacterial meningitis is a very serious form of meningitis and can even cause death in some cases if the condition worsens.\nWhat is viral meningitis?\nViral meningitis is the most common type of meningitis but it is not as severe as bacterial meningitis. It usually takes a few weeks for a person to become well without any treatment but consulting a healthcare professional is important as some cases can be or become severe.\nThere are different types of viruses that can cause viral meningitis. Some of them are:\n- Mumps virus\n- Herpes viruses(Herpes simplex virus and varicella- zoster virus)\n- Measles virus\n- Influenza virus\nWhat is fungal meningitis?\nFungal meningitis is a rare type of meningitis and is caused by the spreading of fungus in the meninges. The fungus can enter the layers through blood. People suffering from HIV infection or cancer are at a higher risk of developing fungal meningitis. People in Africa suffer mostly from fungal meningitis.\nDifferent types of fungus responsible for causing fungal meningitis are as follows:\nMeningitis suffering by fungus can even spread to different organs such as lungs, etc.\nWhat are the risk factors for meningitis?\nAny person can suffer from meningitis. However, there are certain specific factors that can cause meningitis.\nSome common risk factors are as follows:\nPeople with certain immune deficiency are more prone to suffer from meningitis. Some of the immunodeficiency diseases are as follows HIV AIDS, autoimmune disease, etc.\nMeningitis is a communicable disease and can even spread when people live in close quarters such as dormitories, boarding schools, etc.\nPregnant women are at a higher risk of getting meningitis. It can even spread to her unborn child. Pregnancy increases the risk of listeriosis in women.\nPeople of all age are at a risk of developing meningitis. However, children below the age of 5 and young adults are at higher risk of suffering from the infection.\nWhat are the symptoms of meningitis?\nMeningitis shows its symptoms in a person soon after the infection develops. The most common symptoms of meningitis are fever and headache followed by stiff neck.\nSome more symptoms associated with it are:\n- Extreme sensitivity to bright light (photophobia)\nSymptoms of meningitis depend upon the cause of the infection.\nSymptoms of viral meningitis\nSymptoms of viral meningitis include:\n- Decreased appetite\n- Fever and pain\n- Stiff neck, etc.\nSymptoms of Bacterial meningitis\nSymptoms of bacterial meningitis include:\n- Altered mental status\nA person should immediately consult a doctor if he or she experiences any of the above symptoms. Delay in treatment of meningitis can lead to severe consequences.\nHow is meningitis diagnosed?\nAs soon as the symptoms are noticed, the person should consult a doctor for accurate diagnosis. Diagnosis usually starts with a physical examination in which your doctor will check for the signs of infection near head, ear, throat and skin and near the spinal cord.\nAfter physical examination, the doctor will advise you for certain diagnostic tests which may further help him to understand the nature of meningitis and develop an appropriate treatment plan.\nDiagnostic tests and methods for meningitis\nCommon diagnostic tests for meningitis are as follows:\nBlood samples are collected from patients and are tested for the presence of microorganisms, especially bacteria.\nGram stain may also be performed to understand the physiology of bacteria under the microscope.\nMagnetic resonance (MR) and Computerized tomography (CT) scans are used to observe the swelling and inflammation in the head.\nSpinal tap (Lumbar puncture)\nFor proper diagnosis, a sample of cerebrospinal fluid (CSF) is taken with the help of spinal tap. People with meningitis have low sugar level in CSF. This test also helps the doctor to identify the exact bacteria that has caused meningitis in that patient.\nWhat are the complications of meningitis?\nGenerally, people recover fully from meningitis. However, in some cases, a case can become very severe and it can lead to various complications such as:\n- Partial or full loss of hearing\n- Epilepsy (recurrent seizures)\n- Problem with memory and concentration\n- Partial or full loss of vision\n- Loss of limbs due to spreading of infection in the body\n- Bone and joint problems like arthritis\n- Kidney problems, etc.\nWhat are the treatment options for meningitis?\nTreatment for meningitis depends upon its cause.\nTreatment of viral meningitis\nViral meningitis cannot be cured by antibiotics and can improve on their own in some weeks. The most common treatment for viral meningitis is:\n- Complete bed rest\n- Intake of fluids\n- Over-the-counter medicines for fever and body aches.\nSome doctors can even prescribe corticosteroids that can help in reducing brain swelling and anticonvulsant medicine to control seizures.\nTreatment of bacterial meningitis\nBacterial meningitis is the most severe form of meningitis and should be immediately treated after diagnosis.\nMedications such as intravenous antibiotics, corticosteroids, etc are prescribed which can help in reducing and controlling the symptoms of the infection such as swelling and seizures.\nHow is meningitis prevented?\nMeningitis is a serious infection and can lead to severe consequences. It is therefore important to follow certain basic things to maintain a healthy lifestyle such as:\n- Taking proper rest\n- Avoiding alcohol and smoking\n- Avoiding contact with sick people\n- Taking medications at proper time interval\nApart from the above points, the most important preventive measure for meningitis is vaccination. There are some vaccines that can protect a child from developing meningitis such as:\n- Meningococcal vaccine\n- Pneumococcal conjugate vaccine\n- Haemophilus influenzae type B vaccine', 'Can meningitis be misdiagnosed?\nCan meningitis be misdiagnosed?\nMeningitis can be misdiagnosed, especially in young children and babies. Many of the symptoms of bacterial meningitis are overlooked in children and babies. For example, symptoms, such as a stiff neck and headache can be difficult to recognize in an infant.\nWhat are 2 abnormal signs that occur with meningitis?\nThe hallmark signs of meningitis include some or all of the following: sudden fever, severe headache, nausea or vomiting, double vision, drowsiness, sensitivity to bright light, and a stiff neck. Encephalitis can be characterized by fever, seizures, change in behavior, and confusion and disorientation.\nWhat is the most serious form of meningitis?\nBacterial meningitis, the most serious form of meningitis, can also be contagious, especially if it’s meningococcal meningitis. It’s spread through extended contact with an infected person.\nHow long can meningitis go unnoticed?\nViral meningitis tends to cause flu-like symptoms, such as fever and headache, and may be so mild that the illness goes undiagnosed. Most cases of viral meningitis resolve completely within 7 to 10 days, without any complications or need for treatment.\nDoes meningitis show up in CBC?\nMeningitis blood tests A complete blood count (CBC) or total protein count check for heightened levels of certain cells and proteins that can suggest a meningitis infection. A procalcitonin blood test can also help your doctor tell if an infection is more likely caused by either bacteria or a virus.\nWhen should I worry about meningitis?\nYou should get medical advice as soon as possible if you’re concerned that you or your child could have meningitis. Trust your instincts and do not wait until a rash develops. Call 999 for an ambulance or go to your nearest A&E immediately if you think you or your child might be seriously ill.\nWhat labs are abnormal with meningitis?\nIn people with meningitis, the CSF often shows a low sugar (glucose) level along with an increased white blood cell count and increased protein. CSF analysis may also help your doctor identify which bacterium caused the meningitis.\nWhat is meningitis and what are the symptoms?\nViral meningitis symptoms may also develop quickly or over several days. Fever, severe headache, and neck stiffness are the hallmark symptoms of meningitis. Others include: Nausea and vomiting. Confusion and disorientation (acting “goofy”) Drowsiness or sluggishness. Sensitivity to bright light. Poor appetite.\nWhen do you get meningitis or ventriculitis?\nAlthough many of these patients present with clinical symptoms during hospitalization, ventriculitis and meningitis may develop after hospital discharge or even many years later.\nWhen to go to the ER for meningitis?\nThese are additional signs and symptoms of meningococcal infection that has entered the bloodstream: If you suspect that your child or someone you know has meningitis, seek medical care right away. Call the doctor and describe the signs and symptoms. Go immediately to the nearest emergency room if a doctor is not reachable right away.\nWhen to see a doctor for meningioma symptoms?\nMost signs and symptoms of a meningioma evolve slowly, but sometimes a meningioma requires emergency care. Make an appointment to see your doctor if you have persistent signs and symptoms that concern you, such as headaches that worsen over time.\nWhat are the signs and symptoms of aseptic meningitis?\nThe symptoms of aseptic meningitis can vary due to the virus or medical condition that caused it. Sometimes symptoms won’t emerge until the condition has run its course. General symptoms of aseptic meningitis in children and adults include: Infants and toddlers may show the following symptoms:\nCan a person be contagious with viral meningitis?\nMeningitis isn’t always the result of an infection. It can develop as a result of head injury or brain surgery. It can also be caused by certain medications, lupus, or cancer. Non-infectious meningitis is not contagious. Viral meningitis is the most common type, but it’s not usually life-threatening.\nWhat causes the symptoms of meningitis in children?\nIt can be caused by fungi, parasites, or even injury. Most often, it’s caused by viral or bacterial infection. Children are especially vulnerable to bacterial meningitis. Symptoms begin within a week after exposure.\nWhat causes meningitis in the United States of America?\nMost cases of meningitis in the United States are caused by a viral infection, but bacterial, parasitic and fungal infections are other causes.']	['<urn:uuid:74bf2a1f-e339-48da-8b1f-6f986ad702bf>', '<urn:uuid:3a73ed80-5464-41f7-b97c-aa6b149860d5>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-13T02:03:54.148118	8	66	1890
97	as a dog expert comparing working breeds which dog sheds more heavily newfoundland or pembroke corgi	While both breeds shed, the Newfoundland dog sheds more heavily. Newfoundlands have a thick double-layer coat that sheds profusely year-round and goes through extreme seasonal shedding called 'coat blows' twice per year. In contrast, Pembroke Corgis have a shorter double coat that requires only routine grooming, with no mention of extreme seasonal shedding.	['Main Difference – Pembroke vs Cardigan Corgi\nBoth Pembroke and Cardigan Corgi are dwarf dogs sharing some common features of the famous dog breed Dachshunds. Both dog breeds are corgi type dogs. The word Corgi is thought to be a Celtic word for dog used by Scottish people. Pembroke and Cardigan Corgi were first identified as two different dog breeds in 1934. The main difference between Pembroke and Cardigan Corgi is their size since Pembroke is quite smaller than Cardigan Corgi. This article elaborates more details on the difference between Pembroke and Cardigan Corgi.\nKey Areas Covered\n– Facts, Features, Characteristics, Behavior\n2. Cardigan Corgi\n– Facts, Features, Characteristics, Behavior\n3. What is the Difference Between Pembroke and Cardigan Corgi\nPembroke – Facts, Features, Characteristics, and Behavior\nPembroke is also known as Pembroke Welsh Corgi as this dog breed is often considered as a branch of Cardigan Corgi family. These dogs first originated in Pembrokeshire, Wales back in the year 1107. However, they were recognized as a separate breed in 1934. These dogs were primarily used as cattle driving dogs and watchdogs. However, at present, Pembroke is valued as an excellent companion due to their loyalty.\nThe height at withers of an adult female/ male dog is between 10-12 inches. An adult male may weigh about 13.5 kg and an adult female may weigh around 12.5 kg. The coat is short to medium in length and composed of an undercoat and coarser outer coat. The coat may be black and tan, fawn, red, or sable colored with white markings on chest, leg, neck, and face. The lifespan of Pembrokes is around 12-15 years. The ears are pointed and erect. The body is long with characteristically short legs. The tail is short and docked. This dog breed is an ideal small housedog owing to their friendly and intelligent behavior. They are loud attentive alarm barkers. Routing grooming, obedience training and low to moderate exercise are needed.\nCardigan Corgi – Facts, Features, Characteristics, and Behavior\nThe Cardigan Corgi is also known as Cardigan Welsh Corgi as this dog breed was first bred in Cardiganshire in Wales around 1200. They share some common features with Dachshund, hence, it is believed that these both dog breeds have some common ancestry. The same common ancestry gave rise to other corgi type dog breeds including Shipperke, Swedish Vulhunds, and Finnish Spitz dogs. This dog breed was initially used as guard dogs and cattle driving dogs. They also worked as pests hunters. However, later on, they became one of the best companion dog breeds due to their unique physical characteristics and some outstanding behaviors including intelligence, quick learning ability, devotion, and activeness.\nBoth male and female Cardigan Corgis have heights about 10.5 – 12.5 inches. The average weight of an adult male is around 13.5- 17 kg, whereas that of an adult female is about 11.5-15.5 kg. They have double coats of medium length. The coat may exist in few colors including black, red, sable, and brindle with white areas on chest, legs, neck, face, and tail tip. The lifespan is around 12-15 years. These dogs are sturdy and agile due to the presence of long body with short legs. The tail is long and thick. Early training, moderate exercise and low to moderate grooming are needed.\nDifference Between Pembroke and Cardigan Corgi\nPembroke: Pembrokes originated from Pembrokeshire, Wales.\nCardigan Corgi: Cardigan Corgi originated from Cardiganshire, Wales.\nHeight at Withers\nPembroke: The height at withers is 10-12 inches.\nCardigan Corgi: The height at withers is 10.5 – 12.5 inches.\nPembroke: Pembroke is shorter than Cardigan Corgi.\nCardigan Corgi: Cardigan Corgi is longer than Pembroke.\nPembroke: Pembroke has straighter legs.\nCardigan Corgi: Cardigan Corgi has bowed legs due to chondrodystrophic conformation.\nPembroke: Pembroke has smaller and more pointed ears.\nCardigan Corgi: Cardigan Corgi has larger ears, which are not very pointed.\nPembroke: Pembrokes are not as calm as Cardigan Corgis.\nCardigan Corgi: Cardigan Corgis are calmer than Pembrokes.\nPembroke: The coat may be present in black and tan, fawn, red, and sable with white markings on chest, leg, neck, and face.\nCardigan Corgi: The coat may exist in few colors including black, red, sable, and brindle with white areas on chest, legs, neck, face, and tail tip.\nPembroke: The coat is short to medium in length, and composed of an undercoat and coarser outer coat.\nCardigan Corgi: This breed has a double coat with medium length.\nPembroke: Routine grooming is necessary.\nCardigan Corgi: Low to moderate grooming is necessary.\nBoth Pembroke and Cardigan Corgi have common ancestry of dachshunds and related dog breeds. Both dog breeds belong to the corgi family and originated in Wales. Pembrokes are more active and Cardigan Cordi. The difference between Pembroke and Cardigan Corgi can be identified by looking at their body size, ears, and legs. Pembrokes are smaller than Cardigan Corgis and have large pointed ears with more straight legs, unlike Cardigan Corgis. However, both dog breeds are more popular and used as companion dogs owing to their loyal companionship and intelligence.\n1. Bell, Jerold, et al. Veterinary medical guide to dog and cat breeds. CRC Press, 2012.\n2. Palika, Liz. The Howell book of dogs: the definitive reference to 300 breeds and varieties. John Wiley & Sons, 2007.', 'The Newfoundland dog is nothing short of massive in every way.\nThese dogs may now be considered somewhat iconic due to the runaway success of the “Peter Pan” story, but once upon a time they were simply humble working dogs that assisted people with hunting, guarding, and – most famously – water rescues.\nIt is hard to believe it just by looking at them, but the gigantic and hairy Newfoundland dog is actually a great swimmer!\nBut it is all that hair that we are going to talk about in this article. Specifically, we will look at whether Newfoundland dogs shed and, if so, how much do they shed and how can you prepare for that.\n- 1 Do Newfoundland Dogs Shed?\n- 2 Learn About Newfoundland Coat Blows and Shedding\n- 3 Meet the Newfoundland Dog Breed\n- 4 Understanding the Layers of the Newfoundland’s Working Dog Coat\n- 5 Why You Should Never Ever Shave a Newfoundland Dog’s Coat\n- 6 Why Do Newfoundland Dogs Shed More Seasonally?\n- 7 What About Shedding the Outer Coat Layer?\n- 8 Do Newfoundland Puppies Shed and Blow Coat?\n- 9 How Often Do You Have to Brush and Groom Your Newfoundland Dog?\nDo Newfoundland Dogs Shed?\nDo Newfoundland dogs shed? Boy, do they! Ask any Newfoundland dog owner about their enormous shedding canine and how they manage it and you are likely to be listening to tips for quite some time!\nNewfoundland dogs are not just big in terms of how much they weigh and how tall they stand. These dogs have the thick, double-layer, seasonally shedding coats that nearly all working group dogs have. This can add up to a lot of shedding year-round and seasonally for your Newfoundland dog and you.\nLearn About Newfoundland Coat Blows and Shedding\nAs this short owner-made YouTube video explains, approximately twice per year, the Newfoundland dog goes through a type of seasonal shedding called “blowing coat.”\nThis is exactly what it sounds like. The coat sheds out profusely and for a period of time, it can feel like you are literally surrounded in a snowstorm of sorts made of fur.\nLuckily, there is a lot you can do to keep even these extreme seasonal coat sheds manageable for you and your family, not to mention your clothing and home furnishings.\nMeet the Newfoundland Dog Breed\nSo let’s spend a moment getting acquainted with the Newfoundland dog breed and their thick and luxurious working-dog coat.\nAccording to the American Kennel Club (AKC) breed profile, the Newfoundland dog can easily weigh 100 to 150+ pounds and stand 28 inches tall (paw pads to shoulders).\nAnd every inch of the Newfie, as breeders and owners like to call this dog breed, is covered in dog hair!\nIn other words, the Newfoundland has the double-layer coat that is characteristic of nearly every purebred dog breed that has ever worked alongside people in any job.\nAccording to the Antarctica Newfoundland Club, the Newfoundland dog excels at several different jobs, including hauling, carting, hunting, guarding, and protection and conducting water rescues, often at long distances from land.\nSpecifically, Newfoundlands have made a name for themselves over the centuries for their ability to rescue people set adrift at sea when their vessels sank. Clearly, the coat of this dog must be specialized to do such icy cold and dangerous work!\nUnderstanding the Layers of the Newfoundland’s Working Dog Coat\nMost working dog breeds – that is, dogs bred specifically to do jobs alongside people – tend to be bred with an extra-protective coat that non-working dogs often lack.\nAccording to the Newfoundland Club of America (NCA) breed standard, the Newfoundland is a dog that should be “heavily coated.”\nThe breed standard also notes that this working dog coat type has two layers. Each layer is specialized to keep that dog protected, warm, and safe while out working.\nOuter coat layer\nThe outer layer of a working dog’s coat may be long, short or medium in length. But generally speaking, it will always be somewhat water-repellant, coarse, and thick.\nThis is because the outer coat layer is designed to help rain, ice, or snow (or seawater in the Newfoundland dog’s case) sheet right off the dog’s body instead of soaking through to the skin.\nThe coarseness of the outer layer can also serve as a buffer against pests, abrasion, and injury while the dog is working.\nAnd the coat can help protect the dog’s skin from getting sunburned or windburned.\nInner coat layer\nIn direct contrast to what you just read, the inner layer of a working dog’s coat generally has a completely different quality. It is thick and soft and downy.\nIf this description makes you think of a fluffy down comforter, you’ve already got the right idea. This inner layer is the dog’s insulation against cold from any number of sources.\nWhy You Should Never Ever Shave a Newfoundland Dog’s Coat\nIf this is your first time coping with the extreme shedding that Newfoundland dogs can go through seasonally, you may have had more than a passing fantasy about simply shaving your dog and being done with it.\nBut this is something you should NEVER do for several reasons.\nYour dog will lose the coat’s protection\nAll the protective properties you just read about – protection from sun, wind, rain, ice, snow, abrasion, pests, and injury – go away the moment you shave your Newfoundland dog’s thick, double-layer coat.\nIt can be a dangerous thing to be a dog without a coat. The impact would be similar to if you decided to go out and about in the world wearing just your birthday suit.\nSuddenly danger would lurk around every corner. Every place you sat down or walked around, everyone you met and everything you did could cause you harm.\nJust like your body needs clothing to protect your skin from dangerous, your dog needs their coat to stay safe and healthy.\nThe coat will never grow back properly\nAt this point, you might be thinking that even if you shaved your Newfoundland during the hottest months or during the seasonal sheds, the coat would just grow back, right?\nUnfortunately, the fact that the coat will grow back is not enough. Once the coat is shaved, it will never again grow back in two separate and distinct layers like it is now.\nYour Newfoundland’s body will not understand that it needs to grow the downy underlayer and keep that separate from the coarse and water-repellant outer layer. The two layers won’t grow in stages.\nRather, the two layers of the coat will grow in together, with the two different hair types mixing and intermingling. This means the coat will have neither it’s insulating nor its protective properties intact.\nFor you, this will mean that for the rest of your dog’s life you will have to take extra precautions to protect your Newfoundland from getting sunburn, windburn, heat exhaustion, injury and abrasion, and cold.\nIt is far better to learn how to deal with the seasonal shedding than to have to deal with all of these dangers every day for the rest of your dog’s time with you.\nWhy Do Newfoundland Dogs Shed More Seasonally?\nTo understand why Newfoundland dogs go through a seasonal shedding cycle, it helps to remember that one function of all that fur is to keep a dog that works in cold climates warm while doing outside work.\nAs this dog’s breed name implies, Newfoundland dogs may have gotten their modern breed start in Newfoundland, Canada – an area that gets very long, icy, and snowy winters.\nAs PetMD points out, not only have Newfoundland working dogs historically rescued people from deep-sea boating accidents, but they have actually towed boats in the ocean back to shore.\nThese dogs are so strong, they have often been tasked with the job of jumping into the ocean, gripping ship leads (long ropes), and actually pulling boats in to shore when the water got too choppy for accurate navigation.\nKnowing this makes it very easy to understand why a dog might need a very thick winter coat to do this type of work during seasonal storms!\nBut what about when summer arrives? Canada’s summer season is not long but it can get quite hot. All that thick insulating fur might cause a Newfoundland dog to overheat while working during the summer months.\nAnd this is why Newfoundlands do a seasonal shed.\nCold to warm-season shed\nThe biggest coat blow, or seasonal shed, the period is usually during the transition from the cold season to the warm season.\nThis is because your dog has to drop quite a bit of the thick, insulating undercoat rather quickly to keep from overheating as the temperature rises.\nYour dog will shed out a great deal of the undercoat over a short time period, often in big chunks of hair at one time, which is where the nickname “coat blow” comes from.\nAs My Brown Newfies owner blog points out, the shedding isn’t actually triggered by the temperature increasing outside. It is triggered by an increase in daylight hours.\nSo if you live in an area where you have a long winter season, your dog might start the coat blow well before you feel the warmer temperatures starting to creep in.\nThis is normal because the seasonal sheds are connected with your dog’s internal circadian rhythms.\nWarm to cold season shed\nA slightly less impactful coat blow, or seasonal shed, often happens during the transition from the warm season back to the cold season again.\nAt this time, the coat will be replenishing its most important insulating properties by shedding out the well-used summer undercoat and growing in a fresh and thick undercoat.\nWhat About Shedding the Outer Coat Layer?\nThe Newfoundland dog sheds every single day and all day long. This dog can shed while sleeping and often sheds lightly when patted.\nThe outer guard hairs, with their water-resistant and protective properties, are continually being replenished and renewed.\nFor this reason, there needs to be some amount of consistent shedding going on. If a hair gets damaged, it needs to be replaced without delay.\nDo Newfoundland Puppies Shed and Blow Coat?\nAs the South Central Newfoundland Dog Club explains, Newfoundland puppies will typically only go through one single coat blow period.\nThis happens when the puppy coat starts shedding out and the adult coat grows in.\nBecause Newfoundland dogs are giant breed dogs, they can take a lot longer to grow up fully than do smaller breed dogs. This means the coat transition may not happen until your dog is two or even three years old.\nThe Newfie puppy coat is a single layer coat that is softer than the adult coat. The coat will blow out in patches and can get kind of messy for a time until all the puppy hair has shed out and all the adult hair has grown in.\nThis is also the right time to get your Newfoundland puppy used to be brushed and groomed and trimmed and checked. It will be a lot harder to brush and groom your massive adult dog, so you want to have a dog that is happy to allow you to do this.\nHow Often Do You Have to Brush and Groom Your Newfoundland Dog?\nOne big question many aspiring Newfoundland dog owners have is about the frequency of brushing and grooming their dog.\nThe frequency is up to you. However, the more you brush your dog now, the less shed hair you will have to sweep up or vacuum up later.\nEither way, the hair is coming out, so it is really just a question of whether you want the shed, dead hair to come out in the dog brush, or onto the couch cushions or your clothing.\nMany experienced Newfoundland owners make time for at least a daily brushing, often at a particular time of day so the dog gets used to the routine and comes to look forward to this time with you.\nBy understanding in advance what you are choosing by picking a giant Newfoundland dog with a thick, double-layer, shedding coat type, you can prepare yourself with the right brushing and grooming supplies to care for your dog.']	['<urn:uuid:54c698de-2cfc-46e5-9d55-d4dca39b9bc4>', '<urn:uuid:eb18a671-4576-453e-9d64-32cc6abebb41>']	factoid	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-13T02:03:54.148118	16	53	2929
98	I'm curious about liver disease severity testing - what's the difference between the ALBI score and IL-18 testing when it comes to checking how bad liver problems are in cancer patients?	The ALBI (albumin-bilirubin) grade and IL-18 polymorphism testing represent different approaches to assessing liver disease severity in cancer patients. The ALBI score is an evidence-based scoring system that allows objective assessment of liver dysfunction severity across different treatments. Meanwhile, IL-18 polymorphism analysis serves as a marker for predicting progression to cirrhosis in chronic HCV patients and subsequent development of hepatocellular carcinoma (HCC) in cirrhotic patients. Specifically, the GG genotype and G allele of IL-18 have been shown to be valuable predictive markers, with cirrhotic patients carrying CC and GC genotypes having a 1.17-fold higher risk of developing HCC compared to those with GG genotype.	"['Interleukin-18 polymorphism as a diagnostic tumor marker for hepatocellular carcinoma in patients with hepatitis C-related cirrhosis\nEgyptian Liver Journal volume 10, Article number: 51 (2020)\nEgypt has the highest hepatitis C virus prevalence worldwide where about 24% of the people are estimated to carry HCV and more than 50% of blood donors have anti-HCV in some towns. The burden of hepatocellular carcinoma has been increasing in Egypt with a doubling in the incidence rate in the past 10 years. Thus, the aim of the present study was to analyze the interleukin-18 single nucleotide polymorphisms (SNPs) as a diagnostic tumor marker for hepatocellular carcinoma in patients with hepatitis C-related cirrhosis.\nThis study included 33 hepatocellular carcinoma (HCC) complicating HCV-related cirrhosis patients, 37 cirrhotic patients without HCC (cirrhosis group), and 20 healthy individuals who were included as a control for 9 months of follow-up. SNPs of the IL-18 gene were genotyped by polymerase chain reaction. There was a statistically significant difference in the GG genotype in the HCC group in comparison with the control group (P = 0.04). There was a statistically significant difference in the G allele in the cirrhosis and HCC groups in comparison with the control group (p1 < 0.001 and p2 = 0.03, respectively). Patients with GC genotype have a risk for developing HCC by 6.33-folds more than those with GG genotype while patients with GC genotype have a risk for developing cirrhosis by 5.43-folds more than those with GG genotype, and cirrhotic patients with CC and GC genotype had a risk for developing HCC by 1.17-folds more than those with GG genotype.\nOur findings revealed that the analysis of IL-18 single nucleotide gene polymorphism could be a valuable marker for the prediction of progress towards cirrhosis in chronic HCV patients and also to subsequent development of HCC in HCV cirrhotic patients proved by the results of both GG genotype and its G allele; also, cirrhotic patients with CC and GC genotype have a risk for developing HCC by 1.17-folds more than those with GG genotype.\nEgypt has the highest hepatitis C virus (HCV) prevalence worldwide where about 24% of the people are estimated to carry HCV . Viral hepatitis was estimated to be the 7th leading cause of mortality globally. About half of this mortality is attributed to hepatitis C virus (HCV) . HCC is the most common primary liver cancer with over one million new cases worldwide annually. Globally, it is the third leading cause of cancer-related deaths . Alpha fetoprotein (AFP) is still the most widely used tumor biomarker currently available for detection and clinical follow-up of patients with HCC with a sensitivity of 41–65% and a specificity of 80–94%. Internationally, AFP cutoff level of 200 ng/mL is indicative of HCC . Acute and chronic viral hepatitis as well as patients with cirrhosis caused by hepatitis C may be associated with slightly high AFP levels. However, this widely used marker does not yield satisfactory results in the early diagnosis of HCC limiting the universality of its application due to its low positive rate, false-positive results, and finally false-negative results . As for the diagnosis of HCC, the European Association for the Study of the Liver (EASL) panel of experts and the recently updated American Association for the Study of Liver Diseases (AASLD) guidelines have proposed that imaging technique computed tomography and/or magnetic resonance imaging (CT or MRI) showing the HCC radiological hallmark, contrast uptake in the arterial phase, and washout in the venous/late phases could diagnose tumors of 1–2 cm in diameter or above . IL-18 is an 18-kDa cytokine, originally known as interferon-γ (IFN-γ)-inducing factor. This cytokine is mainly produced by activated macrophages and Kupffer cells and can promote IFN-γ production . Also, it participates in chronic hepatic inflammation, leading to carcinogenesis. It was reported that the serum level of IL-18 is a useful biological marker of tumor invasiveness and an independent prognostic factor for survival among patients with HCC . Furthermore, the serum level of IL-18 is increased in patients with HCV-related stage IV HCC compared with patients with earlier-stage HCC . IL-18 polymorphism has been proposed as a possible prognostic factor for reduced survival in patients with HCC . IL-18 polymorphism has been clearly demonstrated that it contributes to tumor progression and metastasis. Although genetic predisposition is one of the factors critical for HCC progression, few studies have focused on IL-18 single nucleotide polymorphisms (SNPs) in patients with HCC. Moreover, research on the combined effect of IL-18 SNPs and HCV infection on the risk and clinicopathologic development of HCC remains scanty .\nThus, the aim of the present study was to analyze the interleukin-18 polymorphism as a diagnostic tumor marker for hepatocellular carcinoma in patients with hepatitis C-related cirrhosis.\nThe present study was a cross-sectional study.\nThe study was conducted at Mansoura Specialized Medical Hospital outpatient clinics without recruitment for 9 months of follow-up.\nThis study included 33 HCC complicating HCV-related cirrhosis patients (HCC group), 37 cirrhotic patients without HCC (cirrhosis group), and 20 healthy individuals who were included as a control (control group), and all of them were from Mansoura Specialized Medical Hospital outpatient clinics for 9 months. SNPs of the IL-18 gene were genotyped by polymerase chain reaction (PCR) restriction fragment-length polymorphism assays.\nThe inclusion criteria were as follows:\nHCC complicating HCV-related cirrhosis patients and cirrhotic patients without HCC\nBoth groups are naive\nBoth gender (62.82 years mean age for the HCC group and 57.78 mean age for the cirrhosis group)\nAll stages of Child-Pugh\nThe exclusion criteria were as follows:\nPatients with a history of cancers other than the liver\nPrevious liver transplantation\nPatients co-infected with HIV or HBV\nOther organ failures (heart failure and renal failure)\nAfter taking a consent of the patients with ensuring the confidentiality of patients and control data, permission was taken from the head of the concerned department. All patients were subjected to history taking (name, age, sex of the patient, and smoking), previous exposure to HCV infection, history of liver disease, cirrhosis, history of medications and/toxin exposure, history of chronic diseases (DM, HTN, and other diseases), and family history of HCC. Physical examination includes a general examination for signs of liver cirrhosis (vital signs jaundice, general appearance, and spider nevi); local abdominal examination for the liver, spleen, and presence or absence of ascites; investigations—laboratory tests for virology markers HBsAg, HCV Ab (ELISA), and HCV PCR+ve in all patients and biochemical tests for liver function tests [S. albumin, S. bilirubin, prothrombin time, INR ratio, ALT, AST], serum level of alpha fetoprotein, complete blood count, S. creatinine, and IL-18 polymorphism genotyped by polymerase chain reaction (PCR) restriction fragment-length polymorphism assays with its variants and alleles A, C, and G; and radiology, specially assessing liver (cirrhosis or HCC on top of cirrhosis): abdominal US, triphasic CT abdomen, and liver biopsy.\nThe study size is determined by the statistician.\nThe collected data were coded and fed into the SPSS system (Statistical Package for Social Sciences) ver. 22.\nThis study included 33 HCC complicating HCV-related cirrhosis patients (HCC group), 37 cirrhotic patients without HCC (cirrhosis group), and 20 healthy individuals who were included as a control (control group).\nDescriptive data, outcome data, and main results\nOur study included 33 HCC complicating HCV-related cirrhosis patients (HCC group), 37 cirrhotic patients without HCC (cirrhosis group), and 20 healthy individuals who were included as a control (control group). The HCC patients were 26 (78.8%) males and 7 (21.2%) females, and the cirrhotic patients were 14 males (37.8%) and 23 females (62.2%) while the control group included 15 (75%) males and 5 (25%) females.\nThe current study is conducted aiming to analyze IL-18 single nucleotide gene polymorphism and its value in predicting HCC among HCV-related cirrhotic patients by studying 33 HCC patients with HCV-related cirrhosis, 37 cirrhotic patients without HCC, and 20 healthy individuals properly selected as a control.\nOf interest, the presence of GG genotype is more in healthy control than in HCC patients (P = 0.04) (Table 5). A finding that could consider the presence of genotype GG of IL-18 as a good predictive marker against HCC development evidenced by lack of difference between the other genotypes (AA, AC, CC, and GC) in the studied groups and each other or the control.\nOf interest, Estfanous et al.  reported that IL-18 polymorphism GG genotype and G allele were significantly associated with a lower risk of chronic HCV infection.\nFurthermore, we find that G allele can be a protective factor against cirrhosis HCC development. This is not matching with Bouzgarrou et al.  who reported that IL-18 polymorphism C allele was associated with a higher risk of cirrhosis and HCC.\nThere were scanty studies of IL-18 single nucleotide gene polymorphism in HCV patients with or without cirrhosis. Previous studies of HCC in HBV patients confirmed abstinence of significant association of different genotypes of IL-18 in the studied patients.\nDai et al.  reported that GG genotype carriers may increase the risk of HCC in healthy populations and the risk of LC in chronic hepatitis B carriers while Zhang and colleagues  reported that the AA genotype and A allele frequencies of IL-18 SNP were positively correlated with HBV-related HCC.\nA previous study conducted by Bao and colleagues  proved that GC genotype and C allele significantly associated with decreased HCC risk.\nIn contrast to our results, Bakr et al.  proved that IL-18 polymorphism AA and GG genotypes were significantly related to a higher risk of developing HCC, and GC genotype and C allele were significantly associated with a lower risk of developing HCV-related cirrhotic patient.\nLau and colleagues  reported that the IL-18 polymorphism with GC+CC genotypes and G allele could be factors that increase the risk of HCC compared with those carrying the wild-type GG.\nThe explanation for the disparity of results between us and other studies may be attributed to the variation in genetic background between different ethnicities, different environmental factors, exposure to different carcinogens in different populations, and to a somewhat smaller sample size of our study population.\nFinally, analysis of IL-18 single nucleotide gene polymorphism could be a valuable marker for prediction of progress towards cirrhosis in chronic HCV patients and also to subsequent development of HCC in HCV cirrhotic patients proved by the results of both GG genotype and its G allele in our studied patients.\nElastography was not done as it is very expensive for our patients. Also, the relatively small number of patients was due to the difficulty in acceptance by patients to be included in a research study in addition to the high expense of the kits.\nOur results should be interpreted with caution because of several limitations. Firstly, though we recruited 90 samples in this study, the sample size of each group was relatively small which may restrict its detailed subgroup analysis by the clinical index. Secondly, considering we just controlled four factors (D.M., gender, smoking, and obesity), other factors including environmental background, treatment protocols, and living habits may cause some bias. Thirdly, all participants were all from Mansoura Specialized Medical Hospital outpatient clinics, Egypt, which may not stand for all the Egyptian population.\nThe fundamental experiments should be further conducted to validate our results and explore the possible mechanism.\nAnalysis of IL-18 single nucleotide gene polymorphism could be a valuable marker for prediction of progress towards cirrhosis in chronic HCV patients and also to subsequent development of HCC in HCV cirrhotic patients proved by the results of both GG genotype and its G allele; also, cirrhotic patients with CC and GC genotype have a risk for developing HCC by 1.17-folds more than those with GG genotype.\nAvailability of data and materials\nThe datasets used and/or analyzed during the current study are available from the corresponding author on reasonable request.\nHepatitis C virus\nSingle nucleotide polymorphisms\nMagnetic resonance imaging\nHepatitis B surface antigen\nEnzyme-linked immunosorbent assay\nPolymerase chain reaction\nStatistical Package for the Social Sciences\nArea under the curve\nOmar A, Abou-Alfa GK, Khairt A et al (2013) Risk factors for developing hepatocellular carcinoma in Egypt. Chin Clin Oncol 2(4):43\nKouyoumjian SP, Chemaitelly H, Abu-Raddad LJ (2018) Characterizing hepatitis C virus epidemiology in Egypt: systematic reviews, meta-analyses, and meta-regressions. Sci Rep 8(1):1661\nMurata S, Mine T, Ueda T et al (2013) Transcatheter arterial chemoembolization based on hepatic hemodynamics for hepatocellular carcinoma. Sci World J 479805:1–8\nBehne T, Copur MS (2012) Biomarkers for hepatocellular carcinoma. Int J Hepatol 2012:859076\nZhao YJ, Qiang JU, Guan-Cheng LI (2013) Tumor markers for hepatocellular carcinoma. Mol Clin Oncol 1(4):593–598\nBruix J, Sherman M (2011) Management of hepatocellular carcinoma: an update. Hepatology 53:1020\nYue M, Wang JJ, Tang SD et al (2013) Association of interleukin-18 gene polymorphisms with the outcomes of hepatitis C virus infection in high-risk Chinese Han population. Immunol Lett 154:54–60\nTangkijvanich P, Thong-Ngam D, Mahachai V et al (2007) Role of serum interleukin-18 as a prognostic factor in patients with hepatocellular carcinoma. World J Gastroenterol 13:4345–4349\nShiraki T, Takayama E, Magari H et al (2011) Altered cytokine levels and increased CD4+CD57+ T cells in the peripheral blood of hepatitis C virus-related hepatocellular carcinoma patients. Oncol Rep 26:201–208\nChen TP, Lee HL, Huang YH et al (2016) Association of intercellular adhesion molecule-1 single nucleotide polymorphisms with hepatocellular carcinoma susceptibility and clinicopathologic development. Tumour Biol 37(2):2067–2074\nLau H-K, Hsieh M-J, Yang S-F et al (2016) Association between interleukin-18 polymorphisms and hepatocellular carcinoma occurrence and clinical progression. Int J Med Sci 13(7):556–561\nEstfanous SZK, Ali SA, Seif SM, Soror SHA (2019) Inflammasome genes’ polymorphisms in Egyptian chronic hepatitis C patients: influence on vulnerability to infection and response to treatment. Mediators Inflamm 2019:3273645\nBouzgarrou N, Hassen E, Schvoerer E et al (2008) Association of interleukin-18 polymorphisms and plasma level with the outcome of chronic HCV infection. J Med Virol 80:607–614\nDai ZJ, Liu X-H, Wang M et al (2017) IL-18 polymorphisms contribute to hepatitis B virus-related cirrhosis and hepatocellular carcinoma susceptibility in Chinese population: a case-control study. Oncotarget 8(46):81350–81360\nZhang QX, Yao YQ, Li SL et al (2016) Association between interleukin-18 gene polymorphisms and hepatocellular carcinoma caused by hepatitis B virus. Zhonghua Gan Zang Bing Za Zhi 24(5):352–357\nBao J, Lu Y, Deng Y et al (2015) Association between IL-18 polymorphisms, serum levels and HBV-related hepatocellular carcinoma in a Chinese population: a retrospective case-control study. Cancer Cell Int 15:72\nBakr NM, Awad A, Moustafa E (2018) Association of genetic variants in the interleukin-18 gene promoter with risk of hepatocellular carcinoma and metastasis in patients with hepatitis C virus infection. IUBMB Life 70(2):165–174\nThanks to every person shared in this work and the soul of Dr. Ayman A. Eldesoky.\nEthics approval and consent to participate\nThe study protocol was investigated and approved by the Medical Ethics Research Team, Faculty of Medicine, Mansoura University (code number MS/16.12.39).\nEvery case, after guaranteeing privacy, has given informed written consent.\nConsent for publication\nThe authors declare that they have no competing interests.\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nThe present study was cross sectional in nature and our patients were selected from the hepatology outpatient clinics in Mansoura Specialized Medical Hospital.\nAbout this article\nCite this article\nEldesoky, A.A., Ahmed, N.A.F., Zaghloul, H.E. et al. Interleukin-18 polymorphism as a diagnostic tumor marker for hepatocellular carcinoma in patients with hepatitis C-related cirrhosis. Egypt Liver Journal 10, 51 (2020). https://doi.org/10.1186/s43066-020-00062-8', 'Staging and prognostic factors in hepatocellular carcinoma\n- Steven A Curley, MD, FACS\nSteven A Curley, MD, FACS\n- Professor of Surgery\n- Baylor College of Medicine\n- Carlton C Barnett, Jr, MD\nCarlton C Barnett, Jr, MD\n- Professor of Surgery\n- University of Colorado at Denver\n- Eddie K Abdalla, MD\nEddie K Abdalla, MD\n- Professor of Surgery\n- The Lebanese American University\nHepatocellular carcinoma (HCC) is an aggressive tumor that frequently occurs in the setting of chronic liver disease and cirrhosis. (See ""Epidemiology and etiologic associations of hepatocellular carcinoma"".) It is typically diagnosed late in the course of these diseases, and the median survival following diagnosis ranges from approximately 6 to 20 months . Available therapeutic options for HCC are dictated by the complex interplay of tumor stage and the extent of underlying liver disease.\nThis topic review will provide an overview of staging and prognostic scoring systems for HCC. Surgical and nonsurgical treatments for HCC are discussed separately. (See ""Overview of treatment approaches for hepatocellular carcinoma"".)\nSTAGING AND PROGNOSTIC SCORING SYSTEMS\nA number of systems have been proposed to predict the prognosis for hepatocellular carcinoma (HCC), none of which has been universally adopted [2-8]. These schema variably incorporate four features that have been recognized as being important determinants of survival: the severity of underlying liver disease, the size of the tumor, extension of the tumor into adjacent structures, and the presence of metastases [2,3,9-11]; some (eg, the Hong Kong and French prognostic staging systems) also incorporate performance status [5,8]. The four most commonly used systems are the tumor, node, metastasis (TNM), Okuda and Barcelona Clinic Liver Cancer (BCLC) systems, and the Cancer of the Liver Italian Program (CLIP) score. A new, evidence-based score – the ALBI (albumin-bilirubin) grade – if independently validated, may allow more objective assessment of the severity of liver dysfunction in patients with HCC across a wide spectrum of treatments. (See \'ALBI score\' below.)\nTumor, node, metastasis (TNM) staging — The current combined TNM staging classification of the American Joint Committee on Cancer (AJCC)/Union for international Cancer Control (UICC) was revised in 2010 (table 1) . Like the 2002 classification, this system recognizes the most important predictors of prognosis: the number of tumors, and the presence and extent of vascular invasion within the tumor [13,14]. However, compared with the 2002 staging system, there are some reclassification changes, primarily surrounding the better prognosis for multiple HCC versus HCC with major vascular invasion.\nFive-year survival rates after complete resection for HCC, based upon the 2010 TNM staging system, are as follows :To continue reading this article, you must log in with your personal, hospital, or group practice subscription. For more information on subscription options, click below on the option that best describes you:\n- A new prognostic system for hepatocellular carcinoma: a retrospective study of 435 patients: the Cancer of the Liver Italian Program (CLIP) investigators. Hepatology 1998; 28:751.\n- Okuda K, Ohtsuki T, Obata H, et al. Natural history of hepatocellular carcinoma and prognosis in relation to treatment. Study of 850 patients. Cancer 1985; 56:918.\n- Prospective validation of the CLIP score: a new prognostic system for patients with cirrhosis and hepatocellular carcinoma. The Cancer of the Liver Italian Program (CLIP) Investigators. Hepatology 2000; 31:840.\n- Farinati F, Rinaldi M, Gianni S, Naccarato R. How should patients with hepatocellular carcinoma be staged? Validation of a new prognostic system. Cancer 2000; 89:2266.\n- Chevret S, Trinchet JC, Mathieu D, et al. A new prognostic classification for predicting survival in patients with hepatocellular carcinoma. Groupe d\'Etude et de Traitement du Carcinome Hépatocellulaire. J Hepatol 1999; 31:133.\n- Llovet JM, Brú C, Bruix J. Prognosis of hepatocellular carcinoma: the BCLC staging classification. Semin Liver Dis 1999; 19:329.\n- Yang JD, Kim WR, Park KW, et al. Model to estimate survival in ambulatory patients with hepatocellular carcinoma. Hepatology 2012; 56:614.\n- Yau T, Tang VY, Yao TJ, et al. Development of Hong Kong Liver Cancer staging system with treatment stratification for patients with hepatocellular carcinoma. Gastroenterology 2014; 146:1691.\n- Tang ZY. Liver cancer. In: Manual of Clinical Oncology, Pollock RE (Ed), Wiley-Liss, Inc, New York 1999. p.407.\n- Primack A, Vogel CL, Kyalwazi SK, et al. A staging system for hepatocellular carcinoma: prognostic factors in Ugandan patients. Cancer 1975; 35:1357.\n- Lai CL, Lam KC, Wong KP, et al. Clinical features of hepatocellular carcinoma: review of 211 patients in Hong Kong. Cancer 1981; 47:2746.\n- American Joint Committee on Cancer. American Joint Committee on Cancer Staging Manual, 7th, Edge SB, Byrd DR, Compton CC, et al (Eds), Springer, New York 2010. p.175.\n- Ikai I, Yamaoka Y, Yamamoto Y, et al. Surgical intervention for patients with stage IV-A hepatocellular carcinoma without lymph node metastasis: proposal as a standard therapy. Ann Surg 1998; 227:433.\n- Vauthey JN, Lauwers GY, Esnaola NF, et al. Simplified staging for hepatocellular carcinoma. J Clin Oncol 2002; 20:1527.\n- Abou-Alfa GK, Pawlik, Shindoh J, and Vauthey J-N.. Liver. In: AJCC Cancer Staging Manual, 8th, Amin MB (Ed), AJCC, Chicago 2017. p.287.\n- Shindoh J, Andreou A, Aloia TA, et al. Microvascular invasion does not predict long-term survival in hepatocellular carcinoma up to 2 cm: reappraisal of the staging system for solitary tumors. Ann Surg Oncol 2013; 20:1223.\n- Ishak K, Baptista A, Bianchi L, et al. Histological grading and staging of chronic hepatitis. J Hepatol 1995; 22:696.\n- Nzeako UC, Goodman ZD, Ishak KG. Hepatocellular carcinoma in cirrhotic and noncirrhotic livers. A clinico-histopathologic study of 804 North American patients. Am J Clin Pathol 1996; 105:65.\n- Kosuge T, Makuuchi M, Takayama T, et al. Long-term results after resection of hepatocellular carcinoma: experience of 480 cases. Hepatogastroenterology 1993; 40:328.\n- Bilimoria MM, Lauwers GY, Doherty DA, et al. Underlying liver disease, not tumor factors, predicts long-term survival after resection of hepatocellular carcinoma. Arch Surg 2001; 136:528.\n- Vauthey JN, Ribero D, Abdalla EK, et al. Outcomes of liver transplantation in 490 patients with hepatocellular carcinoma: validation of a uniform staging after surgical treatment. J Am Coll Surg 2007; 204:1016.\n- Poon RT, Fan ST, Lo CM, et al. Long-term prognosis after resection of hepatocellular carcinoma associated with hepatitis B-related cirrhosis. J Clin Oncol 2000; 18:1094.\n- Levy I, Sherman M, Liver Cancer Study Group of the University of Toronto. Staging of hepatocellular carcinoma: assessment of the CLIP, Okuda, and Child-Pugh staging systems in a cohort of 257 patients in Toronto. Gut 2002; 50:881.\n- Ueno S, Tanabe G, Sako K, et al. Discrimination value of the new western prognostic system (CLIP score) for hepatocellular carcinoma in 662 Japanese patients. Cancer of the Liver Italian Program. Hepatology 2001; 34:529.\n- Villa E, Colantoni A, Cammà C, et al. Estrogen receptor classification for hepatocellular carcinoma: comparison with clinical staging systems. J Clin Oncol 2003; 21:441.\n- Cho YK, Chung JW, Kim JK, et al. Comparison of 7 staging systems for patients with hepatocellular carcinoma undergoing transarterial chemoembolization. Cancer 2008; 112:352.\n- Huo TI, Huang YH, Lin HC, et al. Proposal of a modified Cancer of the Liver Italian Program staging system based on the model for end-stage liver disease for patients with hepatocellular carcinoma undergoing loco-regional therapy. Am J Gastroenterol 2006; 101:975.\n- Yu SJ. A concise review of updated guidelines regarding the management of hepatocellular carcinoma around the world: 2010-2016. Clin Mol Hepatol 2016; 22:7.\n- Torzilli G, Belghiti J, Kokudo N, et al. A snapshot of the effective indications and results of surgery for hepatocellular carcinoma in tertiary referral centers: is it adherent to the EASL/AASLD recommendations?: an observational study of the HCC East-West study group. Ann Surg 2013; 257:929.\n- Marrero JA, Fontana RJ, Barrat A, et al. Prognosis of hepatocellular carcinoma: comparison of 7 staging systems in an American cohort. Hepatology 2005; 41:707.\n- Cillo U, Vitale A, Grigoletto F, et al. Prospective validation of the Barcelona Clinic Liver Cancer staging system. J Hepatol 2006; 44:723.\n- Kudo M, Chung H, Osaki Y. Prognostic staging system for hepatocellular carcinoma (CLIP score): its value and limitations, and a proposal for a new staging system, the Japan Integrated Staging Score (JIS score). J Gastroenterol 2003; 38:207.\n- Vitale A, Morales RR, Zanus G, et al. Barcelona Clinic Liver Cancer staging and transplant survival benefit for patients with hepatocellular carcinoma: a multicentre, cohort study. Lancet Oncol 2011; 12:654.\n- Johnson PJ, Berhane S, Kagebayashi C, et al. Assessment of liver function in patients with hepatocellular carcinoma: a new evidence-based approach-the ALBI grade. J Clin Oncol 2015; 33:550.\n- Mehta N, Heimbach J, Harnois DM, et al. Validation of a Risk Estimation of Tumor Recurrence After Transplant (RETREAT) Score for Hepatocellular Carcinoma Recurrence After Liver Transplant. JAMA Oncol 2017; 3:493.\n- Lee JH, Cho Y, Kim HY, et al. Serum Tumor Markers Provide Refined Prognostication in Selecting Liver Transplantation Candidate for Hepatocellular Carcinoma Patients Beyond the Milan Criteria. Ann Surg 2016; 263:842.\n- Sherman M. Staging for hepatocellular carcinoma: complex and confusing. Gastroenterology 2014; 146:1599.\n- Liu PH, Hsu CY, Hsia CY, et al. Prognosis of hepatocellular carcinoma: Assessment of eleven staging systems. J Hepatol 2016; 64:601.\n- Vauthey JN, Dixon E, Abdalla EK, et al. Pretreatment assessment of hepatocellular carcinoma: expert consensus statement. HPB (Oxford) 2010; 12:289.\n- Lee YT. Primary carcinoma of the liver: diagnosis, prognosis, and management. J Surg Oncol 1983; 22:17.\n- Stuart KE, Anand AJ, Jenkins RL. Hepatocellular carcinoma in the United States. Prognostic features, treatment outcome, and survival. Cancer 1996; 77:2217.\n- Rustgi VK. Epidemiology of hepatocellular carcinoma. Gastroenterol Clin North Am 1987; 16:545.\n- Rolfes DB. Fibrolamellar carcinoma of the liver. In: Neoplasms of the Liver, Okuda K, Shak KG (Eds), Springer-Verlag, Tokyo 1987. p.137.\n- Okuda K, Musha H, Nakajima Y, et al. Clinicopathologic features of encapsulated hepatocellular carcinoma: a study of 26 cases. Cancer 1977; 40:1240.\n- Cillo U, Vitale A, Bassanello M, et al. Liver transplantation for the treatment of moderately or well-differentiated hepatocellular carcinoma. Ann Surg 2004; 239:150.\n- Tangkijvanich P, Anukulkarnkusol N, Suwangool P, et al. Clinical characteristics and prognosis of hepatocellular carcinoma: analysis based on serum alpha-fetoprotein levels. J Clin Gastroenterol 2000; 31:302.\n- Ebara M, Ohto M, Shinagawa T, et al. Natural history of minute hepatocellular carcinoma smaller than three centimeters complicating cirrhosis. A study in 22 patients. Gastroenterology 1986; 90:289.\n- Matsumoto Y, Suzuki T, Asada I, et al. Clinical classification of hepatoma in Japan according to serial changes in serum alpha-fetoprotein levels. Cancer 1982; 49:354.\n- Hanazaki K, Kajikawa S, Koide N, et al. Prognostic factors after hepatic resection for hepatocellular carcinoma with hepatitis C viral infection: univariate and multivariate analysis. Am J Gastroenterol 2001; 96:1243.\n- Tyson GL, Duan Z, Kramer JR, et al. Level of α-fetoprotein predicts mortality among patients with hepatitis C-related hepatocellular carcinoma. Clin Gastroenterol Hepatol 2011; 9:989.\n- Santambrogio R, Opocher E, Costa M, et al. Hepatic resection for ""BCLC stage A"" hepatocellular carcinoma. The prognostic role of alpha-fetoprotein. Ann Surg Oncol 2012; 19:426.\n- Yamamoto K, Imamura H, Matsuyama Y, et al. Significance of alpha-fetoprotein and des-gamma-carboxy prothrombin in patients with hepatocellular carcinoma undergoing hepatectomy. Ann Surg Oncol 2009; 16:2795.\n- Shim JH, Yoon DL, Han S, et al. Is serum alpha-fetoprotein useful for predicting recurrence and mortality specific to hepatocellular carcinoma after hepatectomy? A test based on propensity scores and competing risks analysis. Ann Surg Oncol 2012; 19:3687.\n- Kiriyama S, Uchiyama K, Ueno M, et al. Triple positive tumor markers for hepatocellular carcinoma are useful predictors of poor survival. Ann Surg 2011; 254:984.\n- Toyoda H, Kumada T, Kaneoka Y, et al. Prognostic value of pretreatment levels of tumor markers for hepatocellular carcinoma on survival after curative treatment of patients with HCC. J Hepatol 2008; 49:223.\n- Kim HS, Park JW, Jang JS, et al. Prognostic values of alpha-fetoprotein and protein induced by vitamin K absence or antagonist-II in hepatitis B virus-related hepatocellular carcinoma: a prospective study. J Clin Gastroenterol 2009; 43:482.\n- Villa E, Camellini L, Dugani A, et al. Variant estrogen receptor messenger RNA species detected in human primary hepatocellular carcinoma. Cancer Res 1995; 55:498.\n- Cescon M, Cucchetti A, Grazi GL, et al. Role of hepatitis B virus infection in the prognosis after hepatectomy for hepatocellular carcinoma in patients with cirrhosis: a Western dual-center experience. Arch Surg 2009; 144:906.\n- Chen JH, Chau GY, Lui WY, et al. Surgical results in patients with hepatitis B-related hepatocellular carcinoma and positive hepatitis B early antigen. World J Surg 2000; 24:383.\n- Sun HC, Zhang W, Qin LX, et al. Positive serum hepatitis B e antigen is associated with higher risk of early recurrence and poorer survival in patients after curative resection of hepatitis B-related hepatocellular carcinoma. J Hepatol 2007; 47:684.\n- Kubo S, Hirohashi K, Yamazaki O, et al. Effect of the presence of hepatitis B e antigen on prognosis after liver resection for hepatocellular carcinoma in patients with chronic hepatitis B. World J Surg 2002; 26:555.\n- Li J, Zhou J, Yang PH, et al. Nomograms for survival prediction in patients undergoing liver resection for hepatitis B virus related early stage hepatocellular carcinoma. Eur J Cancer 2016; 62:86.\n- Sasaki Y, Yamada T, Tanaka H, et al. Risk of recurrence in a long-term follow-up after surgery in 417 patients with hepatitis B- or hepatitis C-related hepatocellular carcinoma. Ann Surg 2006; 244:771.\n- Roayaie S, Haim MB, Emre S, et al. Comparison of surgical outcomes for hepatocellular carcinoma in patients with hepatitis B versus hepatitis C: a western experience. Ann Surg Oncol 2000; 7:764.\n- Utsunomiya T, Shimada M, Kudo M, et al. A comparison of the surgical outcomes among patients with HBV-positive, HCV-positive, and non-B non-C hepatocellular carcinoma: a nationwide study of 11,950 patients. Ann Surg 2015; 261:513.\n- Kim BK, Park JY, Kim DY, et al. Persistent hepatitis B viral replication affects recurrence of hepatocellular carcinoma after curative resection. Liver Int 2008; 28:393.\n- Wu JC, Huang YH, Chau GY, et al. Risk factors for early and late recurrence in hepatitis B-related hepatocellular carcinoma. J Hepatol 2009; 51:890.\n- Wang YG, Wang P, Wang B, et al. Diabetes mellitus and poorer prognosis in hepatocellular carcinoma: a systematic review and meta-analysis. PLoS One 2014; 9:e95485.\n- Kaseb AO, Hassan MM, Lin E, et al. V-CLIP: Integrating plasma vascular endothelial growth factor into a new scoring system to stratify patients with advanced hepatocellular carcinoma for clinical trials. Cancer 2011; 117:2478.\n- Kaseb AO, Morris JS, Hassan MM, et al. Clinical and prognostic implications of plasma insulin-like growth factor-1 and vascular endothelial growth factor in patients with hepatocellular carcinoma. J Clin Oncol 2011; 29:3892.\n- Kaseb AO, Abbruzzese JL, Vauthey JN, et al. I-CLIP: improved stratification of advanced hepatocellular carcinoma patients by integrating plasma IGF-1 into CLIP score. Oncology 2011; 80:373.\n- Sun H, Teng M, Liu J, et al. FOXM1 expression predicts the prognosis in hepatocellular carcinoma patients after orthotopic liver transplantation combined with the Milan criteria. Cancer Lett 2011; 306:214.\n- Schmitz KJ, Sotiropoulos GC, Baba HA, et al. AKR1B10 expression is associated with less aggressive hepatocellular carcinoma: a clinicopathological study of 168 cases. Liver Int 2011; 31:810.\n- Nault JC, De Reyniès A, Villanueva A, et al. A hepatocellular carcinoma 5-gene score associated with survival of patients after liver resection. Gastroenterology 2013; 145:176.\n- Hoshida Y, Villanueva A, Kobayashi M, et al. Gene expression in fixed tissues and outcome in hepatocellular carcinoma. N Engl J Med 2008; 359:1995.\n- Tsuchiya M, Parker JS, Kono H, et al. Gene expression in nontumoral liver tissue and recurrence-free survival in hepatitis C virus-positive hepatocellular carcinoma. Mol Cancer 2010; 9:74.\n- Qiu J, Peng B, Tang Y, et al. CpG Methylation Signature Predicts Recurrence in Early-Stage Hepatocellular Carcinoma: Results From a Multicenter Study. J Clin Oncol 2017; 35:734.\n- Kornberg A, Küpper B, Tannapfel A, et al. Patients with non-[18 F]fludeoxyglucose-avid advanced hepatocellular carcinoma on clinical staging may achieve long-term recurrence-free survival after liver transplantation. Liver Transpl 2012; 18:53.\n- Cascales Campos P, Ramirez P, Gonzalez R, et al. Value of 18-FDG-positron emission tomography/computed tomography before and after transarterial chemoembolization in patients with hepatocellular carcinoma undergoing liver transplantation: initial results. Transplant Proc 2011; 43:2213.\n- Lee JH, Park JY, Kim DY, et al. Prognostic value of 18F-FDG PET for hepatocellular carcinoma patients treated with sorafenib. Liver Int 2011; 31:1144.\n- Kim JW, Seong J, Yun M, et al. Usefulness of positron emission tomography with fluorine-18-fluorodeoxyglucose in predicting treatment response in unresectable hepatocellular carcinoma patients treated with external beam radiotherapy. Int J Radiat Oncol Biol Phys 2012; 82:1172.\n- Cheung TT, Chan SC, Ho CL, et al. Can positron emission tomography with the dual tracers [11 C]acetate and [18 F]fludeoxyglucose predict microvascular invasion in hepatocellular carcinoma? Liver Transpl 2011; 17:1218.\n- Minagawa M, Makuuchi M. Treatment of hepatocellular carcinoma accompanied by portal vein tumor thrombus. World J Gastroenterol 2006; 12:7561.\n- Roayaie S, Jibara G, Taouli B, Schwartz M. Resection of hepatocellular carcinoma with macroscopic vascular invasion. Ann Surg Oncol 2013; 20:3754.\n- Poon RT, Fan ST. Evaluation of the new AJCC/UICC staging system for hepatocellular carcinoma after hepatic resection in Chinese patients. Surg Oncol Clin N Am 2003; 12:35.\n- Lei HJ, Chau GY, Lui WY, et al. Prognostic value and clinical relevance of the 6th Edition 2002 American Joint Committee on Cancer staging system in patients with resectable hepatocellular carcinoma. J Am Coll Surg 2006; 203:426.\n- STAGING AND PROGNOSTIC SCORING SYSTEMS\n- Tumor, node, metastasis (TNM) staging\n- Okuda system\n- CLIP score\n- The Barcelona staging classification\n- ALBI score\n- RETREAT and MoRAL scores\n- Choice of staging system\n- OTHER FACTORS INFLUENCING SURVIVAL\n- High versus low-incidence regions\n- Tumor histology\n- Serum alpha-fetoprotein level\n- Variant estrogen receptors\n- Hepatitis B and C\n- - Antiviral therapy for HBV-related HCC\n- Diabetes mellitus\n- New prognostic markers under investigation\n- Clinical implications']"	['<urn:uuid:3555e18d-42b8-42e9-851c-ae0d46de702e>', '<urn:uuid:ec78455d-b93a-48ce-af62-afde3eaa9dbe>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-13T02:03:54.148118	31	104	5512
99	I've been hearing about metabolic syndrome lately and I'm worried about my health. Can you tell me what health problems it includes?	Metabolic syndrome includes several risk factors for cardiovascular diseases: abdominal obesity, hypertension, high triglyceride, low HDL cholesterol, and high fasting glucose.	"[""|Shen, Jian - TUFTS UNIVERSITY|\n|Arnett, Donna - UNIV. OF ALABAMA-BIRMINGH|\n|Peacock, James - UNIV. OF MINNESOTA|\n|Aldi, Kraja - WASHINGTON UNIVERSITY|\n|Hixson, James - UNIV. OF TEXAS|\n|Tsai, Michael - UNIV. OF MINNESOTA|\n|Kabagambe, Edmond - UNIV. OF ALABAMA-BIRMING|\n|Lai, Chao Qiang|\n|Stratka, Robert - UNIV. OF MINNESOTA|\nSubmitted to: Journal of Nutrition\nPublication Type: Peer Reviewed Journal\nPublication Acceptance Date: May 21, 2007\nPublication Date: August 1, 2007\nCitation: Shen, J., Arnett, D.K., Peacock, J.M., Parnell, L.D., Aldi, K., Hixson, J.E., Tsai, M.Y., Kabagambe, E.K., Lai, C., Stratka, R.J., Ordovas, J.M. 2007. Interleukin 1B genetic polymorphisms interact with polyunsaturated fatty acids to affect risk of the metabolic syndrome in the GOLDN Study. Journal of Nutrition. 137(8):1846-1851. Interpretive Summary: Metabolic syndrome (MetS) is the cluster of risk factors for cardiovascular diseases (CVD), including abdominal obesity, hypertension, high triglyceride, low HDL cholesterol and high fasting glucose. MetS substantially increases the risk of CVD and type II diabetes and therefore has become one of the major public health threats in the United States. The etiology of MetS involves multiple genetic and environmental factors, with diet being the major environmental factor. In this paper, researchers found that genetic mutations at the IL1B gene which encodes protein playing an important role in inflammatory pathway significantly affected individual risk to MetS. Furthermore, researchers demonstrated that the genetic impact is influenced by dietary fat, using fatty acids composition of the erythrocyte membrane as a biomarker of dietary intake. The deleterious effects of the IL1B gene mutation only occurs among subjects with low polyunsaturated fatty acid intake, suggesting potential unhealthy dietary habit. The results provide insight into causal role of chronic inflammation in the development of MetS. The knowledge gained from this research underscores the importance of gene and environmental interaction in MetS process.\nTechnical Abstract: Chronic inflammation has been identified as an important component of the metabolic syndrome (MetS). Therefore, environmental and genetic factors contributing to the variation of inflammatory responses could affect individuals’ susceptibility to the MetS. We investigated the association between common IL1' genetic polymorphisms, inflammation and the MetS, and its modulation by diet-related variables (i.e., erythrocyte membrane fatty acid composition) in a White US population. IL1' SNPs (-1473G>C, -511G>A, -31T>C, 3966C>T, 6054G>A), clinical and biochemical measurements were characterized in a total of 1120 subjects (540 males and 580 females) participating in the Genetics of Lipid Lowering Drugs and Diet Network (GOLDN) Study. The 6054 G>A SNP was significantly associated with plasma C-reactive protein (P=0.054), adiponectin (P=0.021) and the prevalence of MetS (P=0.004). Moreover, there was a significant interaction between the 6054G>A SNP and erythrocyte membrane (n-3) PUFA (P=0.019). Among subjects with low (n-3) PUFA content (below the median), the 6054 G allele was significantly associated with increased risk of the MetS (OR: 3.29, 95%CI: 1.49-7.26 for GG and OR: 1.95, 95%CI: 0.85-4.46 for GA, P<0.001) compared to the AA genotype, but there was no significant genotype associations among subjects with high (n-3) PUFA content (above the median). Further analyses supported a significant haplotype global effect on the MetS (P=0.017) among subjects with low (n-3) PUFA content. These results suggest that IL1' genetic variants are associated with measures of chronic inflammation and the MetS risk, and the genetic influences are more evident among subjects with low (n-3) PUFA intake.""]"	['<urn:uuid:3fa44d80-a5b9-4e17-896e-130d0374d913>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-13T02:03:54.148118	22	21	546
100	Which works better for treating inflammation - goat products or turmeric?	Both have demonstrated anti-inflammatory effects. Goat whey showed anti-inflammatory properties in experimental studies of intestinal inflammation, helping reduce pro-inflammatory substances like interleukin, TNF-alpha and nitric oxide. Curcumin from turmeric has also proven inflammation-dampening effects and has been successfully used to treat inflammatory conditions like osteoarthritis, with clinical evidence showing pain reduction within 8-12 weeks using standardized turmeric extracts. While both are beneficial, curcumin has more extensive research supporting its anti-inflammatory effects across multiple conditions.	['This study evaluated the intestinal anti-inflammatory ramifications of goat whey within a mouse style of colitis induced by 2,4-dinitrobenzenesulfonic acid that resembles human IBD. DNBS style of intestinal irritation, and these observations had been verified by its immunomodulatory properties in vitro. Jointly, our outcomes indicate that goat whey could possess applications for the treating IBD. Launch Inflammatory colon disease (IBD) is normally several chronic inflammatory colon circumstances, including Crohn’s disease and ulcerative colitis, that are due to an excessive disease fighting capability response. The main symptom is normally chronic swelling from the gastrointestinal system with intervals of exacerbation accompanied by intervals of remission [1,2]. IBD can be associated with a rise in pro-inflammatory cytokines such as for example interleukin (IL)-1, IL-6, IL-17, tumour necrosis element (TNF)-, prostaglandins and nitric oxide (NO), which might impair colonic muscle tissue contraction and hurdle function. These inflammatory mediators are essential for the era and maintenance of swelling [3,4]. Activation of Mitogen Activated Proteins Kinases (MAPK) pathway continues to be implicated in the pathogenesis of IBD. Activated MAPKs can bind and stimulate additional kinase focuses on, translocate towards the nucleus and activate transcription of pro-inflammatory genes. Among the well-studied the different parts of MAPK signaling pathway may be the nuclear transcription element kappa B (NF-kB), which highly influences the span of mucosal swelling [5,6]. Activation of NF-B could be initiated by different stimuli including bacterial cell wall structure components such as for example lipopolysaccharides, pro-inflammatory cytokines, infections and DNA harming agents . The existing pharmacological remedies for IBD consist of 5-aminosalicylic acidity, azathioprine, mercaptopurines, cyclosporine and corticosteroids [8,9]. The introduction of natural treatments such as for example monoclonal anti-TNF- antibodies, that have proven clinical efficacy, signifies a major latest progress in IBD therapy, but these remedies also increase the chance of disease and hypersensitivity [10,11]. Although these pharmacological improvements have improved the grade of existence of individuals, the induction of remission of the condition can be accompanied by unwanted effects due to the medicines [12,13]. Alternatively or a go with to pharmacological remedies, the consequences of practical foods have already been thoroughly studied, plus some contain substances that possess intestinal anti-inflammatory activity. With this feeling, goat milk and its own derivatives are encouraging practical foods for wellness advertising and disease avoidance. They contain bioactive parts such as for example peptides, fatty-conjugated linoleic acidity (CLA) and oligosaccharides, amongst others. Whey proteins and bioactive peptides that are 1314241-44-5 released by hydrolysis may possess beneficial biological results on your body, such as for example antioxidant and anti-hypertensive results . There’s been raising interest during the last 10 years in CLA, which is principally found in milk products from ruminants. Several health benefits happen to be related to Rabbit Polyclonal to Cytochrome P450 19A1 CLA such as for example anticarcinogenic, anti-atherogenic, antidiabetic, anti-inflammatory, 1314241-44-5 antioxidant and immunomodulatory results based on research including cell lines, pet experimental versions [15C17] and modulation from the immune system response in individuals with Crohn’s disease . Furthermore, 500 mg/kg/d of goat dairy oligosaccharides [19,20] displays guarantee for reducing intestinal swelling. Actually, goat dairy oligosaccharides have already been shown to show significant intestinal anti-inflammatory results in experimental types of mouse colitis [19,20]. Lately, we published a report showing that this dental administration of goat dairy and goat yogurt before and following the induction of colitis by acetic acidity ameliorated intestinal swelling in rats . Therefore, the purpose of this research was to measure the ramifications of goat whey on intestinal swelling induced by 2,4-dinitrobenzenesulfonic acidity (DNBS) in mice as well as the mobile reactions in the Natural 264 and CMT-93 cell lines. Components 1314241-44-5 and strategies Ethics This research was conducted relative to the Information for the Treatment and Usage of Lab Pets (NIH Publication No: 85C23, modified 1985), as well as the process was accepted by the Ethics Committee on Pet Experimentation from the College or university of Granada (Spain) (Ref. No. EAEC: 2010C286). Collection and 1314241-44-5 characterization of goat whey The dairy was extracted from crossbred Pardo-Alpine goats over around 50 (10) times of lactation. The pet diet implemented the recommendations from the NRC (2007) and fulfilled the dietary requirements for lactating goats. The dairy was collected on the Experimental Device of S?o Jo?o carry out CaririPB owned by the Federal College or university of Paraba (UFPB, Brazil)..', 'If you do a quick search about Turmeric on the Internet you may begin to wonder what it can’t do. The list of benefits reported is extensive and the diseases it is purported to treat are numerous. This super-spice is popping up in all sorts of interesting ways from Turmeric lattes to fermented drinks. It also has a growing body of research to back up some of the widely quoted benefits. However, it is important to note that there is a difference between food ingestion and a standardized supplement and in this article, I specifically look at standardized curcumin, one of the key active components of turmeric, and the role it has to play in pain management.\nChronic pain is defined as pain that extends beyond normal healing time or 3 months, whichever comes first. It is characterized by spontaneous pain, increased responsiveness to noxious (painful) input and painful responses to innocuous stimulus e.g. light touch or normal movement. There is both peripheral and central nervous system involvement in all types of chronic pain including neuropathic, inflammatory and nociceptive, however, changes within the central nervous system and brain are commonplace and can predominate.\nCurcumin has a wide range of therapeutic benefits that have potential in the management of pain including antioxidant, immune modulating, anti-spasmodic, anti-inflammatory and antidepressant. These effects, however, have to be translated into clinical benefit and accordingly, I have looked at the research in different types of pain below.\nNeuropathic Pain Treatment\nNeuropathic pain arises from injury or trauma to the nerves, which can result from factors such as exposure to drugs (e.g. chemotherapy agents), metabolic disease (e.g. diabetes type 1 and 2), or lesions (e.g. multiple sclerosis). Preliminary research suggests that curcumin may help prevent pain associated with diabetic neuropathy when taken in combination with some blood sugar lowering medication.\nPersistent pain occurs in 50% of individuals post surgery, with opioids being the most common strategy for pain management. In a previous article I have discussed how extended use of opioids post-surgery can actually contribute to further pain development and so short-term use only is recommended. Fortunately, curcumin has a dose-dependent effect on post-surgical pain and can improve pain threshold and recovery rates. The other key factor is that curcumin can potentially reduce the risk of opioid tolerance and dependence – a definite bonus in my opinion!\nCurcumin is well known for its inflammation dampening effects and I have previously written about effective use in highly inflammatory conditions such as psoriasis and rheumatoid arthritis. Osteoarthritis is another area where use of curcumin may benefit pain levels, and also reduce the use of analgesic pain medications such as paracetamol and non-steroidal anti-inflammatories (NSAIDs), which with ongoing use are associated with significant side effects.\nSeveral high-level reviews of the clinical evidence indicate that osteoarthritic pain can be reduced in 8-12 weeks of treatment using standardized turmeric extracts. The doses of curcumin involved were higher than most over the counter supplements currently available however the range was quite wide (400-1000mg curcumin), which means it is possible to achieve these results with lower end therapeutic doses.\nPain and mood\nDepression commonly occurs with chronic pain, some studies saying it is evident in up to 70% of people with chronic pain. The association is bi-directional; those with depression may be more prone to increased pain perception and conversely, those with chronic pain are more likely to develop depression. Here curcumin may play dual roles, helping to relieve both chronic pain and depression through independent effects. While studies haven’t been specifically run in a patient population with both chronic pain and depression, independent studies in pain and also depression show positive benefit.\nWith all the indications for curcumin outlined above, it is important to note that individual prescribing is important both for efficacy but also for safety. Fortunately, it is generally regarded as safe, even higher dose ranges however it is important to check in with either a naturopath, holistic nutritionist (with training in supplement prescribing) or pharmacist about any potential risks associated with medication co-prescribing. I have been specific in my recommendation about healthcare providers as it is important to ask a professional who knows about curcumin and any associated risks. Your GP or specialist may simply not know, as this is not their area of expertise.\nWhile I have only included research specific to curcumin, one of the active components of turmeric, daily inclusion of fresh or powdered turmeric is great for general health and wellbeing. For inspiration, I have included a couple of recipes below. If you are looking for a therapeutic effect from turmeric, via curcumin supplementation I recommend you a. source a quality product and b. run it past your naturopath because while it is a super-spice, there may be something more specific or appropriate to your health or pain issue.\nThis dressing is a great addition to salads for anyone with inflammatory conditions or just to maintain good health. The recipe is as follows:\n4 cloves garlic (optional)\n¼ cup + 2 tablespoons extra virgin olive oil OR cold pressed, unrefined avocado oil\n3 tablespoons lemon juice\n2 teaspoons raw honey (optional)\n2 teaspoons powdered turmeric\n½ teaspoon Dijon mustard\n⅛ teaspoon black pepper\n¼ unrefined salt\nPlace all ingredients in a mini processor and process until smooth and creamy. Alternatively, crush the garlic into a jar and add all the other ingredients. Shake well. Double or triple quantities can be made and kept in the fridge.\nCauliflower Florets with Ginger, Turmeric, and Cumin\n1 large head cauliflower\nSalt and pepper to taste\n2 tablespoons olive oil, divided\n1 teaspoon freshly grated ginger\n1 teaspoon ground cumin\n1/2 teaspoon ground turmeric\nSmall handful of parsley, chopped\nPreheat the oven to 180°C.\nRinse the cauliflower and break it up into larger florets. Use the stalk as well, just cutting it up into large chunks. Season each steak with salt and pepper on both sides. Cover an oven tray in a coating of olive oil, about 1 tablespoon. Spread the florets over the tray. Whisk together the remaining 1 tablespoon of olive oil, ginger, cumin, and turmeric. Brush or spoon the mixture on the cauliflower florets.\nRoast in the oven until tender, about 15-20 minutes. Garnish with parsley and serve.\nDaily, J. W., Yang, M., & Park, S. (2016). Efficacy of turmeric extracts and curcumin for alleviating the symptoms of joint arthritis: a systematic review and meta-analysis of randomized clinical trials. Journal of medicinal food, 19(8), 717-729.\nSun, J., Chen, F., Braun, C., Zhou, Y. Q., Rittner, H., Tian, Y. K., … & Ye, D. W. (2018). Role of curcumin in the management of pathological pain. Phytomedicine.']	['<urn:uuid:10f01491-a4cd-4874-bd7e-1f49d25be5af>', '<urn:uuid:cc2358e5-354a-423e-a44d-da8f80da815a>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-13T02:03:54.148118	11	74	1823
