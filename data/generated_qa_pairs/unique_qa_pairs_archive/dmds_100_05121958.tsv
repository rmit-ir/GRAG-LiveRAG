qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	what caused smooth mars northern hemisphere formation	The smooth Borealis basin that covers the northern hemisphere of Mars was likely created when Mars was struck by a Pluto-sized body about four billion years ago. This massive impact is thought to be the cause of the difference between the Martian hemispheres.	"[""Vastitas Borealis (Latin, 'northern waste' ) is the largest lowland region of Mars. It is in the northerly latitudes of the planet and encircles the northern polar region.  Vastitas Borealis is often simply referred to as the northern plains , northern lowlands or the North polar erg  of Mars The North Polar Basin, or Borealis basin, is a large basin in the northern hemisphere of Mars that covers 40% of the planet.It is named for the North Polar Basin on Earth, due to its similar location Also if the proposed Borealis basin is a depression created by an impact, it would be the largest impact crater known in the Solar System. An object that large could have hit Mars sometime during the process of the Solar System accretion\nThis theory suggests that Mars was struck by a Pluto-sized body about four billion years ago. The event is thought to be the cause of the difference between the Martian hemispheres. It made the smooth Borealis basin that covers 40% of the planet Borealis and Borouge announce global launch of new flagship brand Anteo™ Anteo delivers easy processability at lower extruder pressure, better sealing integrity and improved puncture resistance in combination with strong optics for enhanced shelf appeal South Pole-Aitken basin is within the scope of WikiProject Astronomy, which collaborates on articles related to Astronomy on Wikipedia. Start This article has been rated as Start-Class on the project's quality scale\nNorth Polar Basin/Borealis Basin (disputed) - Mars The distinctive mark of an impact crater is the presence of rock which has undergone shock-metamorphic. North Polar Basin (Mars) Save The North Polar Basin , more commonly known as the Borealis Basin, is a large basin in the northern hemisphere of Mars that covers 40% of the planet North Polar Basin. large basin in the northern hemisphere of Mars. North Polar Basin或Borealis basin; Statements. instance of. landform. 0 references. location\n. The Elysium is the smaller volcanic provence in the upper (northern) of the eastern hemisphere. Lowlands dominate the northern portion of both hemispheres. (see Vastitas Borealis The entire wikipedia with video and photo galleries for each article. Find something interesting to watch in seconds. The smooth Borealis basin in the northern. A giant northern basin that covers about 40 percent of Mars' surface, sometimes called the Borealis basin, is the remains of a colossal impact early in the solar system's formation, the new analysis suggests Borealis Planitia is a large basin on Mercury with a smooth floor, thought to be similar to a lunar mare . It is centered at 73.4° N, 79.5° W. The name is Latin for Northern Plain. References topo Borealis Planitia is a large basin on Mercury with a smooth floor, thought to be similar to a lunar.\nSome of the evidence is the abundance of extensive fracturing and igneous activity of late Noachian to early Hesperian age. A counter argument to the endogenic hypothesis is the possibility of those tectonic events occurring in the Borealis basin due to the post-impact weakening of the crust The Mars Ocean Hypothesis is a hypothesis that nearly a third of the surface of Mars was once covered by an ocean earlier in its history. The ocean, which is called Oceanus Borealis, would have filled the Vastitas Borealis basin in the northern hemisphere. [33\nIn the first of two episodes looking at the Borealis, we're examining the past, present, and potential future of this infamous ship from the Half-Life series. Watch Part 2 Here: https://www. Borealis basin Save The North Polar Basin , more commonly known as the Borealis Basin, is a large basin in the northern hemisphere of Mars that covers 40% of the planet Borealis Basin or North Polar Basin, a basin on the planet Mars Borealis quadrangle , an area on the planet Mercury Borealis Planitia , a basin within the quadrangl Close. This video is unavailable Feb 21, 2017 · Did Mars Once Have Three Moons? That giant feature, known as Borealis Basin, was likely created by a large impact that could have kicked up enough debris to form many moons. Image credit.\nExplore Mars using Google Maps API. Satellite images, terrain and infrared maps of Mars North Polar Basin/Borealis Basin (disputed) - Mars - Diameter: 10,600 km mark of an impact crater is the presence of rock which has on selected content. Hellas Planitia of Hellas Bassin is een groot, min of meer rond inslagbassin in het hoogland op het zuidelijk halfrond van Mars.Het ligt rond 42,7° Zuiderbreedte, 70,0° Oosterlengte Hemipodus borealis in uska species han Annelida nga ginhulagway ni Johnson hadton 1901. An Hemipodus borealis in nahilalakip ha genus nga Hemipodus , ngan familia nga Glyceridae .   Waray hini subspecies nga nakalista\nAs a result, Vastitas Borealis lies within the North Polar Basin, while Utopia Planitia, another The original version of this page is from Wikipedia, you can edit. Inuit are the descendants of what anthropologists call the Thule which twice over-wintered in Foxe Basin, Some Inuit looked into the aurora borealis,. Continue reading What is Mars Made Of? Skip to content This is thought to have been responsible for the Martian hemispheric dichotomy and created the smooth Borealis basin that now covers 40. Albedoformationer • Atmosfär • Kanaler • Klimat • Vatten • Liv • North Polar Basin • Kaosterräng Regioner Cydonia • Planum Boreum • Planum Australe • Cerberus-hemisfären • Vastitas Borealis • Iani Chaos • Kvadranter • Tharsis • Ultimi Scopuli • Eridaniasjön • Olympia Undae • Elysium Planiti\nFrom Wikipedia, the free encyclopedia. OMERS, officially the Ontario Municipal Employees Retirement System, is a pension fund created by statute in 1962 to handle the retirement benefits of local government employees in the province of Ontario, Canada The smooth Borealis basin in the northern hemisphere may be a giant impact feature, covering 40% of the planet. Mars' rotational period and seasonal cycles are likewise similar to those of Earth. Jupiter- Jupiter is the fifth planet from the Sun and the largest planet within the Solar System.[13\nTwo distinct basins are recognized within the Vastitas Borealis: the North Polar Basin and Utopia Planitia. Some scientists have speculated the plains were covered by an ocean at some point in Mars' history and putative shorelines have been suggested for its southern edges. Today these mildly sloping plains are marked by ridges, low hills, and. Although it is not an officially recognized feature, the North Polar Basin makes up most of the lowlands in the Northern Hemisphere of Mars. As a result, Vastitas Borealis lies within the North Polar Basin, while Utopia Planitia, another very large basin, is adjacent to it Kondado ang Skagit County (Prinanses: Comté de Skagit, Kinatsila: Condado de Skagit) sa Estados Unidos. Nahimutang ni sa estado sa Washington , sa amihanan-kasadpang bahin sa nasod, 3,700 km sa kasadpan sa Washington, D.C. Adunay 116,901 ka molupyo. [2 Bathygnathus on sukupuuttoon kuollut varhaisella permikaudella elänyt synapsidi suku, joka todennäköisesti kuuluu heimoon Sphenacodontidae.Sukuun luetaan yksi laji, Bathygnathus borealis\nMarikh(♂) (daripada bahasa Arab atau Parsi: مريخ) merupakan planet yang keempat letaknya dari Matahari dalam Sistem Suria.Marikh terkenal dengan warna permukaannya yang merah apabila dilihat dari langit, terutamanya pada waktu malam Espesye sa mamipero nga una nga gihulagway ni Titian Peale ni adtong 1848 ang Lissodelphis borealis. Ang Lissodelphis borealis sakop sa kahenera nga Lissodelphis sa kabanay nga Delphinidae .   Giklaseklase sa IUCN ang espesye sa kinaminosang kalabotan . [1 Image source: Wikipedia the Canada Basin, Makarov Basin, the Amundsen Basin, and the Nansen Basin. or Aurora Borealis (part of the tourist attraction) About. Media in category Ennadai Lake The following 4 files are in this category, out of 4 total. Aurora Borealis - Lodge Solar Panels, Ennadai Lake, Nunavut.jpg 1,100 × 733; 516 K Athabasca and Beaver River basins. Athabasca River basin. Borealis Canoe Club library. Although some Lakeland waters lie in the Athabasca River basin, I have.\nThe Basin of the Columbia River in Picturesque (The Guide and Index to the Microform Edition of the John Muir Papers, 1986, page Wikipedia has an article. The Borealis Basin makes up 40% of the planet's surface, taking up almost the entire northern hemisphere. Mars is covered by craters from objects like asteroids and meteorites hitting the planet. Today, 43,000 such craters have been found and that only includes the large ones Media in category BWS photos in Wikipedia articles Canyonlands National Park, Utah, USA, Monument Basin from White Rim, 2002 Aurora Borealis, Gillam.\nFishes of the Amur River basin. Zapiski Imperatorskoi Akademii Nauk de St.-Petersbourg (Ser. 8) 24(9): 1-270, Pls. 1-3. (Russian) Reference page. Links . Ophicephalus argus in Catalog of Fishes, Eschmeyer, W.N., Fricke, R. & van der Laan, R. (eds.) 2019. Catalog of Fishes electronic version, accessed on April 28, 2017. Channa argus Report. Szathmáry, L. (1990): A Multivariate Analysis of Upper Skull from Europe. Third Symposium on Upper Paleolithic, Mesolithic and Neolithic Populations of Europe and the Mediterranean Basin, Budapest. Abstract, 3. Szathmáry, L. (1990): Boreal-Atlantic Change in the populations of Carpathian Basin\nWikipedia states that Teotihuacan was the largest city in the pre-Columbian Americas, with a population estimated at 125,000 or more, making it at least the sixth largest city in the world during its epoch A geographical basin is a bowl shaped depression or dip in the Earth's surface, either oval or circular in shape. Some basins are empty while others contain water, and some are formed nearly instantaneously while others take thousands of years to form Hudson Bay basin. Coverage: Waters flowing into Hudson Bay. Borealis Paddling Expedition 2005. Arctic Ocean basin. Dubawnt River. Selwyn Lake to Beverly Lake. Mars 3 je sovjetska sonda lansirana s ciljem istraživanja Marsa.Lansiran je 28. maja 1971. Identična kao i prethodnik Mars 2,  sonda se sastojala od orbitera i lendera\nIn order for something akin to the aurora borealis to be found at that time of year, at that time of day, in that part of the country, localized entirely within Skinner's kitchen, he would likely have to set up a plasma chamber inside an evacuated glass vessel This type of traditional greenhouses answered fairly well in the countries of the Mediterranean basin, is confronted to the intense nocturnal cooling, which sometimes results in the reversal of the internal temperatures and complications of overheating and hygrometric variations According to the seasons Hesperian Chronology and Stratigraphy - Description and Name Origin... not used by the ICS timescale The Hesperian System is named after Hesperia Planum, a moderately cratered highland region northeast of the Hellas basin cover roughly 30% of the Martian surface they are most prominent in Hesperia Planum, Syrtis Major Planum, Lunae Planum, Malea Planum, and the Syria-Solis-Sinai Plana in. Hyphessobrycon borealis Zarske, Le Bail W.M. 2016. A new species of Hyphessobrycon Durbin (Characiformes: Characidae) from rio Aripuanã, rio Madeira basin. Destiny 2 is the sequel to Bungie's 2014 FPS/MMORPG Destiny.It was released for PS4 / Xbox One on September 6, 2017, and for PC on October 24 of the same year\nPark je známy aj svojou populáciou vtákov. Mnohé migrujúce druhy sa v parku zdržujú koncom jari a v lete. Možno tu pozorovať chocholáče (bombycilla), skaliariky (oenanthe) kolibkárika severského (phylloscopus borealis), smrečiara krivonosého (pinicola enucleator) a tiež snehule (lagopus) a labute malé (cygnus bewickii) Opis i mitovi. Mars je sin Jupitera, vrhovnog boga, i njegove žene - Junone.Prema drugom mitu, sin je Junone i cvijeta kojeg je nosila Flora.Rimljani su kasnije poistovijetili ovoga boga sa grčkim bogom Aresom, te se tako bog ratara, koji su često bili prisiljeni ratovati, pretvorio u boga rata 4.7 46.2 Haystack Observatory Simeiz Vallis -13.2 64.3 Simeiz Observatory Plains Borealis Planitia 73.4 79.5 Chryse Planitia shows Wikipedia Planitia. Il nome specifico 'borealis' 'è latino per boreale, del nord. Descrizione. I chiurli boreali sono piccoli, di circa 30 centimetri di lunghezza. Gli adulti hanno lunghe gambe grigiastro scuro, il becco è lungo e leggermente curvo verso il basso. Le parti superiori sono screziate di marrone, la pancia e le parti inferiori del piumaggio sono. Home > PowerPoint Templates > Aurora borealis near fairbanks ak PowerPoint Template With Aurora Borealis Near Fairbanks Ak Themed Background And A Wine Colored Foreground Desig\nBorealis AG er Europas nest største produsent av plast, med hovedkvarter i Wien i Østerrike. Borealis AG er et internasjonalt selskap med 40 års erfaring i produksjon av polyetylen - (PE) og polypropylen - (PP) løsninger for infrastruktur, bil- og det avanserte emballasjemarkedet A new species of Hyphessobrycon (Characiformes, Characidae) from the upper Guaviare River, Orinoco River Basin, Colombia. ZooKeys 668: 123-138. doi : 10.3897/zookeys.668.11489 Reference page Basin - centro abitato degli Stati Uniti d'America, nello stato del Montana Basin - centro abitato degli Stati Uniti d'America, nello stato del Wyoming Basin Street - strada di New Orlean It lies on the slope of Mount Hor in a basin among the mountains which form the eastern flank of Arabah (Wadi Araba), the large valley running from the Dead Sea to the Gulf of Aqaba. Petra has been a UNESCO World Heritage Site since 1985. @Wikipedia\nBad Astronomy « Yeah, If it can power the aurora borealis, it can power other stuff, too. Update: in researching my own question on wikipedia, it turns out it's already been done. In. Provavelmente, Neotamandua borealis, um fóssil encontrado na Colômbia, no sítio paleontológico de La Venta, foi o ancestral do tamanduá-bandeira. [17 Hyphessobrycon borealis (Zarske, Le Bail & Géry, 2006) Hyphessobrycon boulengeri (Eigenmann, 1907) Hyphessobrycon cachimbensis (Travassos, 1964) Hyphessobrycon catableptus (Durbin, 1909) Hyphessobrycon coelestinus (Myers, 1929) Hyphessobrycon columbianus (Zarske & Géry, 2002) Hyphessobrycon compressus (Meek, 1904 The plan was to then hand the golden key to the Uighur, so that they could access the treasure house of the Tarim Basin. However, his younger daughter misplaced the key. In retaliation, the supernatural being held his daughter captive in the Tarim Basin, forming what we now know as the Taklamakan Desert\nAurora Borealis, Elementary Art. Wikipedia, the free encyclopedia wo das Great Basin mit der verbliebenen Grube eines ausgetrockneten Lahontan Lake liegt. Es. Phoenix landed in the far North of Mars on May 25, 2008 at 68.22 N and 125.7 W (234.3 E) in Vastitas Borealis. Wide view from Phoenix lander Solar panels are visible. Curiosity Rove Мерку́рій — найближча до Сонця планета Сонячної системи.Обертається навколо Сонця за 87,969 земних діб\nRazorback sucker (Xyrauchen texanus) is an endemic catostomid of the Colorado River Basin. Razorback sucker transbasin movement through lake Powell, Utah 5 km along strike to the east of Razorback Ridge Deposit and would be easily accessible to any mining infrastructure developed at Razorback Along windswept beaches and cliffs, visitors experience where water meets land and sky, culture meets culture, and past meets present. The 21 islands and 12 miles of mainland host a unique blend of cultural and natural resources. Lighthouses shine over Lake Superior and the new wilderness areas. Native American Star Mythology Waupee and the Star Maiden White Hawk The Star Maidens and the Corona Borealis: Shawnee legends about a man who married a star The African Congo Basin is a network of Flicker-Aurora Borealis at the Arctic Circle -Creative Wikipedia text is.""]"	['<urn:uuid:776167c3-de72-4098-8964-406526eff8c1>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	7	43	2508
2	What is porcelain used for and how tough is it?	Porcelain is used as building material in tiles, large panels, and in various applications from residential to highest traffic commercial settings. It is completely vitrified, hard, impermeable, and resistant to wear. Full body porcelain tiles carry color throughout their thickness, making them virtually impervious to wear, and can withstand cleaning pressures up to 1,450 PSI if the grout is in good condition.	"['Porcelain (otherwise called china or fine china) is a ceramic material made by warming materials, for the most part incorporating earth as kaolin, in an oven to temperatures between 1,200 °c (2,192 °f) and 1,400 °c (2,552 °f). The durability, quality, and translucence of porcelain emerges primarily from the shaping of glass and the mineral mullite inside the let go body at these high temperatures. For the reasons of exchange, the Combined Nomenclature of the European Communities characterizes porcelain as being ""totally vitrified, hard, impermeable (even before coating), white or misleadingly hued, translucent (with the exception of when of extensive thickness), and thunderous.\nKaolin is the essential material from which porcelain is made, despite the fact that earth minerals may represent just a little extent of the entirety. The statement ""glue"" is an old term for both the unfired and let go material. A more basic phrasing nowadays for the unfired material is ""body"", for instance, when purchasing materials a potter may request a measure of porcelain body from a seller.\nDirts utilized for porcelain are by and large of lower versatility and are shorter than numerous other ceramics muds. They wet rapidly, implying that little changes in the substance of water can deliver huge changes in workability. Subsequently, the scope of water substance inside which these dirts could be met expectations is exceptionally thin and the misfortune or addition of water amid capacity and tossing or framing must be precisely controlled to keep the earth from getting to be excessively wet or excessively dry to manipulate.the assembling of porcelain got to be very sorted out and the oven destinations, those exhumed from this period, could fire upwards of 25,000 products. While Xing Ware is viewed as among the best of the Tang porcelain furnaces, Ding Ware turned into the head porcelain of Song Dynasty. By the Ming Dynasty (1368–1644), porcelain symbolization was being sent out to Europe. Probably the most well-known Chinese porcelain workmanship styles touched base in Europe amid this period, for example, the pined for blue-and-white products.\nThe glues created by joining together earth and powdered glass (frit) were called Frittenporzellan in Germany and frita in Spain. In France they were known as pâte tendre and in England as ""delicate glue"".\nIn spite of the fact that initially created in England since 1748 to contend with foreign porcelain, bone china is currently made around the world. The English had perused the letters of Jesuit teacher Francois Xavier d\'entrecolles, which depicted Chinese porcelain fabricating insider facts in subtle element. One journalist has theorized that a misconception of the content could perhaps have been in charge of the first endeavors to utilize bone-powder as a part of English porcelain, in spite of the fact that this is not backed via scientists and historians.tubs with porcelain covering don\'t scratch, rust, stain or consume (and are impervious to harm from substance cleaners), and hence got to be by and large utilized. The procedure Buick utilized remains comprehensively the same right up \'til the present time.\nPorcelain could be utilized as a building material, typically as tiles or expansive rectangular boards. Cutting edge porcelain tiles are by and large created to various perceived universal principles and definitions. Producers are found over the world with Italy being the worldwide pioneer, delivering in excess of 380 million square meters in 2006. Memorable illustrations of rooms enhanced altogether in porcelain tiles might be found in a few European royal residences including ones at Capodimonte, Naples, the Royal Palace of Madrid and the adjacent Royal Palace of Aranjuez.\nDespite the fact that initially created in England since 1748 to contend with foreign porcelain, bone china is currently made around the world. The English had perused the letters of Jesuit teacher Francois Xavier d\'entrecolles, which portrayed Chinese porcelain producing privileged insights in point of interest. One author has estimated that a mistaken assumption of the content could potentially have been in charge of the first endeavors to utilize bone-cinder as an element of English porcelain, despite the fact that this is not underpinned via scientists and antiqu', 'Porcelain tile Care and maintenance\nMost types of tiles that are made from clay or a mixture of clay and other materials and then kiln-fired, are considered to be a part of the larger classification called “Ceramic Tiles”. These tiles can be split into two groups, porcelain tiles and non-porcelain tiles. These non-porcelain tiles are frequently referred to as ceramic tiles by themselves, separate from porcelain tiles - confusing!\nNon-porcelain ceramic tiles are generally made from red or white clay fired in a kiln. They are almost always finished with a durable glaze, which carries the color and pattern. These tiles are used in both wall and floor applications. Non-porcelain ceramic tiles are usually suitable for very light to moderate traffic and generally have a relatively high water absorption rating making them less frost resistant and they are more prone to wear and chipping than porcelain tiles. Porcelain tile is more scratch resistant than ceramic tile. Also, porcelain tile is fired at higher temperatures than ceramic, resulting in superior durability and stain resistance.\nPorcelain unglazed tile is generally made by the dust pressed method from porcelain clays which result in a tile that is dense, impervious, fine grained and smooth, with a sharply formed face. Porcelain tiles usually have a much lower water absorption rate (less than 0.5%) than non-porcelain ceramic tiles making them frost resistant. Full body porcelain tiles carry the color and pattern through the entire thickness of the tile making them virtually impervious to wear and are suitable for any application from residential to the highest traffic commercial or industrial applications. Because porcelain tile is fire-hardened and quite hard, it can be cleaned at pressures up to 1, 450 PSI if the grout is in good condition.\nPorcelain unglazed tiles are similar to glazed tile, except that their surface is not coated. Full-body porcelain unglazed tiles do not show wear because their color extends throughout the tile, making them ideal for commercial applications.\nPEI classes range from 0 to 5. The Porcelain Enamel Institute rating scale is not a measurement of quality. It is a scale that clearly indicates the areas of use each manufacturer recommends and has designed their tile to fit. A PEI 2 tile has been designed for areas where very low traffic and soiling is anticipated. In most cases the aesthetic detailing of these tiles is of prime consideration. You will often find high gloss levels, vibrant colorations and metallic elements in this group of tile. Conversely, a PEI 5 tile has been designed for abusive extra heavy foot traffic:\n- PEI Class 0 - No Foot Traffic: Wall tile only and should not be used on floors\n- PEI Class 1 - Very light traffic: Very low foot traffic, bare or stocking feet only. (Master bath, spa bathroom).\n- PEI Class 2 - Light Traffic: Slipper or soft-soled shoes. Second level main bathroom areas, bedrooms.\n- PEI Class 3 - Light to Moderate Traffic: Any residential area with the possible exception of some entries and kitchens if extremely heavy or abrasive traffic is anticipated.\n- PEI Class 4 - Moderate to Heavy Traffic: High foot traffic, areas where abrasive or outside dirt could be tracked. Residential entry, kitchen, balcony, and countertop.\n- PEI Class 5 - Heavy Traffic: Ceramic tile suggested for residential, commercial and institutional floor subjected to heavy traffic.\nSealing Ceramic and Porcelain Tile & Grout\nGlazed tiles are coated with a liquid glass, which is then baked into the surface of the clay. The glaze provides an unlimited array of colors and designs as well as protects the tile from staining. A glazed tile is already stain proof, so there is no purpose to putting on a sealer. However, the grout joint between the tiles is usually very porous and generally made of a cement-based material. Therefore, grout joints typically will need to be sealed and maintained properly to prevent stains and discoloration. Impregnating sealers such as All-Purpose Grout Sealer go into the grout joint and protect against water and oil-based stains. Most industry professionals recognize that grout is best protected with a fluorochemical-based sealer, such as Impregnator Pro or All-Purpose Grout Sealer. If the grout joint is epoxy, a sealer is not necessary.\nUnglazed porcelain tile should be protected with a penetrating sealer, such as Sta-Clene, Bullet Proof or Impregnator Pro, including the grout lines. The penetrating sealer is an invisible, stain resistant shield that is absorbed into the surface.\nCementitious grout must be sealed to prevent or minimize staining. Leaving these surfaces unsealed may greatly hinder the ability to completely remove stains in the future. Allow new installations to cure for 72 hours prior to applying sealer.\nFor a natural looking protector on interior surfaces (heavy duty protection, water and oil repellency):\n- Sweep or vacuum all dust, dirt and debris.\n- Mask off and protect any baseboards or adjacent areas to avoid splashing and overspray onto surfaces not intended to be treated.\n- Ensure that the surface is clean, dry and residue-free.\n- Allow sealer to completely penetrate into the grout, 5-15 minutes.\n- Liberally apply a second coat of Grout Sealer following steps 4-5.\n- Wipe up all sealer from the surface of the tile. Use a clean, dry, lint-free, cotton towel or mop to remove excess sealer. Or, go over the floor with a cotton bonnet on a low-speed buffer.\n- If sealer was not completely wiped off and a residue appears, wipe entire surface with a towel dampened with sealer. Use a white, non-abrasive nylon brush or pad to loosen residue and follow with a clean, white absorbent towel to remove.\n- A full cure is achieved after 24-48 hours; foot traffic may begin in 4 hours. Cover with red rosin paper, if foot traffic must resume before the recommended time periods have passed.\n- Expected coverage is 600-1, 000 sq. ft. per gallon based upon grout joint width.\n- A 3-5 year re-application is needed for interior surfaces.\nCleaning Ceramic and Porcelain Tile & Grout\n- Keeping ceramic, porcelain tile & grout free of dust and dry, sandy soil will minimize scratches, wear patterns and grout soiling that can develop from everyday use and traffic.\n- Use walk-off mats to trap abrasive soil before it gets into the house or building.\n- Sweep, dust or vacuum surfaces regularly to remove loose soil and dust.\n- Clean the tile & grout using warm water and a clean nonabrasive cloth sponge or mop.\n- Use a neutral cleaner such as Stone & Tile Cleaner or Revitalizer that is specially formulated for ceramic, porcelain tile & grout to help remove soils that sweeping, dusting, vacuuming or damp mopping leave behind.\n- Do not use ordinary household cleaners, as you may degrade the sealer that was applied to the grout to protect against stains.\nCountertops and Vanities\nUse a ph-balanced cleaner such as Stone & Tile Cleaner or Revitalizer to keep surfaces clean from everyday soils and stains.']"	['<urn:uuid:33fbbd4a-d9b6-4308-9e4f-fce735defa52>', '<urn:uuid:f4754a5a-41a0-48fd-bd9f-22ee43519bbe>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:58:41.790432	10	62	1842
3	What makes invasive plants grow so quickly?	Invasive plants grow rapidly and out of control because they are away from the diseases and predators that helped keep them in check in their home environments.	['Non-native or “exotic” species of plants that are introduced into new ecosystems, i.e. to regions of the world where they have not lived before, may or may not become invasive.\nNon-native or “exotic” species of plants that are introduced into new ecosystems, i.e. to regions of the world where they have not lived before, may or may not become invasive. Some scientists estimate that about 25-30% of the non-native species that have been introduced to the United States have now become invasive.\nInvasive plants, essentially, are species that grow very rapidly and out of control. Away from the diseases and predators that helped to keep them in check in their home environments, they are able to grow and reproduce so aggressively that they quickly out-compete native species—damaging the ecosystem because their dominance reduces biodiversity—eliminating many of the plants and animals that used to live there. Often, they spread extensively enough to form “monocultures,” or areas where they literally are the only plant growing.\nJust as not all native plants have the same range, a particular exotic or introduced species might be invasive in one part of the United States but not another. Or it may take some time for that species to get established in different regions, so it may already be invasive in one area or state, but not yet have had the same impact elsewhere. Because our study of invasiveness is relatively new, and plants constantly continue to colonize new areas, it is quite likely that the range within which any given introduced plant species is recognized as being “invasive” will grow over time.\nSO BAD, that they are Now Illegal\nCurrently, in Massachusetts, approximately 140 species of plants are legally designated as noxious or invasive in the Commonwealth. These plants are considered so damaging to both our environment and our economy that it is now illegal to import, propagate, or sell any of these plants in the state. Regulated by the Department of Agricultural Resources, the plants that were chosen for this notable status were drawn from the Federal Noxious Weed List as well as through the work of the Massachusetts Invasive Plant Advisory Group— a collaborative group representing federal and state governments, private land trusts and conservation organizations, the nursery and landscaping industries, the scientific community and academia. So take a good look at the Massachusetts Prohibited Plant List and get to know the species considered to be the most harmful of all.\nYet, even though the propagation, sale, and distribution of these species is now illegal, most are growing prolifically and running rampant in various parts of our state and in our neighborhoods here in Cambridge. They are doing, and will continue to do, a fantastic job of increasing their numbers all on their own. That is, unless we all learn to recognize them and to take corrective action-- concerted steps to control them and minimize their numbers.\nAnd finally, note that many ecological gardeners and landscapers consider additional non-native species to be invasive, even though they might not to have made it into the status of illegality. The best advice for all of us who garden and steward land is:\nIf you don’t know it, don’t grow it.\nGet to know the plants you garden with. Find out if they are native or non-native. Learn a little bit about their history and growing characteristics. Emphasize natives in your planting for the health of our common ecosystem. And when you decide to plant a non-native, be sure it isn’t known or suspected of being invasive. You will love the results—more songbirds, butterflies, and lots of beauty all around.\nFind resource materials to learn more about invasive plants.']	['<urn:uuid:dbb11154-3bba-43a6-aca5-838ae6662ecc>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T19:58:41.790432	7	27	612
4	What security feature do malicious HTML files embedded in PDFs manage to get around when targeting users?	HTML files embedded in PDFs can bypass Adobe's security features, allowing attackers to convince victims to run malicious files on their computers.	"[""In our previous blog post, Attachments in Phishing 101, we introduced how attackers use attachments in a malicious manner. In this post, we introduce two common email attachments and show how these files are used to deliver malware or redirect you to dangerous websites. If you opened attachments (e.g. PDFs, spreadsheets) in the past and nothing bad happened, you will likely continue opening attachments in the future without much care. Nevertheless, you are likely to be more careful with attachments with an unfamiliar file format. To keep you safe from attackers, it's important to recognize suspicious behavior in a familiar-looking file attachment. We will show you step by step how malicious Excel and malicious PDF attachments can behave in Windows environment.\nUsing Excel macros is a common way for attackers to run malicious commands on a victim's computer. Macros can be very useful for task automation in Excel, but they also enable other commands to be run with little to no user interaction. In some companies, macro-enabled files are blocked in an email (this is a good security measure). Sometimes attackers use Excel version 4.0 macros, also known as XLM-macros. This way the file is not interpreted as a macro-enabled document, unlike a modern Visual Basic Application (VBA) -based macro document, and therefore is not blocked. Here’s a clip of one example of malicious macros in Excel:\nIn the video, the victim receives an urgent .xls file that requires immediate attention. Creating a sense of urgency is a social engineering technique with the aim to cause an emotional reaction in the victim. Once the victim previews the file, they see a message about the file being created with an old version of Excel, thus requiring macros to work. This message is a picture with text in it, not a real info message from Excel.\nThe unsuspecting victim opens the file in Excel and follows the instructions by enabling both editing and macro content. The “Enable editing” and “Enable content” buttons are meant as a security feature, which blocks malicious scripts and dynamic content until the buttons are pressed. These messages are often ignored by victims. You should always critically evaluate before enabling content and macros, especially if the file comes unexpectedly. Once the content is enabled, the macros are executed automatically. In this case, the malicious command script directs the victim to our scary website. The script can just as easily include other malicious purposes, like a way for the attacker to open a connection to the victims' computer.\nPDFs are a popular document format online. It would be safe to say that you have, at least once, received an email with a PDF attachment. Below you can see what can happen if you open a malicious PDF file. This PDF is designed to deliver malware by the creative use of legitimate features and circumventing a security feature in Adobe Acrobat.\nAn urgent task is received from an attacker impersonating a high-ranking, important person. The victim is asked to take immediate action on the attached PDF file. Opening the attachment, the victim is asked to update their PDF reader before continuing. By approving the update and clicking OK, they let the PDF reader automatically open another file embedded inside the PDF. The HTML file embedded in the PDF bypasses Adobe’s security features and convinces the victim to run a malicious file on their computer. An HTML file is opened in the default web browser, showing Adobe Acrobat’s latest features.\nA moment later the victim is asked to run a file called “Adobe-Reader-DC-Updater-v190609.hta” from public.adobecc.com. Notice the .hta file used instead of .exe. HTA is used in this example, as it can bypass security measures in Windows which usually prevent dangerous .exe files to run. Everything on this website is fabricated by the attacker to look like the real deal, which is why malicious activity can be difficult to detect. Clicking “Run” allows the browser to download and run the malicious file. This time, it only opens another tab on the victim's browser and navigates to our example page. A real attack could install file-encrypting ransomware, steal sensitive files, or spy on the victim.\nThese attachments are just examples of how an attacker could use popular file types to deliver malware or take you to a dangerous website. Both examples misuse real features built into the file formats (XLS or PDF). Keep in mind these examples require multiple actions from the users, but you can never be sure if a file is able to instantly run malicious code when viewed. Always aim to verify the authenticity of files you receive via email. We teach users to recognize dangerous emails with malicious files with gamified phishing training, and we use real-life examples to keep up with the latest phishing trends. Want to learn more about how we teach about phishing attachments at Hoxhunt? Head out to our Knowledge Base or request a demo to hear more. Stay safe!""]"	['<urn:uuid:2a820148-2d3e-46ce-a6bb-2675d57260fe>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	17	22	826
5	residential rehab cost length effectiveness compare	Residential rehab costs can range from $2,000 to $20,000 for basic treatment, potentially doubling for longer stays of 2-3 months. Research shows that at least 90 days or more are needed to effectively treat substance addiction, with long-term residential treatment increasing sobriety chances. Like diabetes or hypertension, addiction is a chronic disease requiring ongoing management. The therapeutic community model typically involves 6-12 month stays and has proven particularly effective, offering 24-hour monitoring, comprehensive support services, and focusing on helping clients examine damaging beliefs while adopting constructive behavioral patterns.	"[""As you discharge from inpatient treatment, you will receive recommendations for follow-up care and ongoing recovery support to strengthen your sobriety and reduce the risk of relapse. Like diabetes or hypertension, addiction is a chronic disease. Regaining your health means learning to manage your symptoms, first within the structure of an inpatient rehab program and eventually in your home environment where you are in charge of maintaining and strengthening your recovery. Drug Rehab Near Me\nThis kind of treatment is known as Cognitive Behavioral Therapy (CBT), because it introduces the patient to new and healthier ways of thinking (“cognitive”) and acting (“behavioral”). The National Institute on Alcohol Abuse and Alcoholism says that the success of alcohol treatment depends on “changing a person’s behaviors and expectations about alcohol.” Mom Left Job and Fell Into Alcoholism\nMany substance abuse treatment centers are affordable, or even free, for some clients. Many communities offer free counseling or rehab drug treatment. These types of programs usually take place at outpatient drug rehab centers. For affordable inpatient and outpatient care, utilizing an insurance provider can help you cover the cost of rehab. Many insurance plans cover substance abuse treatment completely. Speak with your insurance provider, or a representative from a potential program to find more details. The Recovery Village accepts a variety of insurance plans and works with clients to develop payment plans so they can make healing a priority. The Recovery Village’s insurance verification tool can help you narrow down your options.\nDrug detox: Detox, short for detoxification, is the first phase in many substance abuse treatment programs. During detox, patients are monitored by professionals during their withdrawal from drugs. Medications, nutritional supplementation and fluid replacement may be provided to relieve withdrawal symptoms. At the same time, counseling is provided to encourage the patient to move forward to the next phase of rehabilitation.\nOur highly qualified treatment team possess extensive clinical experience in treating alcohol addiction, and are able to deliver a wide range of established techniques to help you to address your alcohol addiction symptoms, and resolve the underlying causes and triggers for your alcohol addiction. We ensure that each individual who seeks support with us is placed at the centre of their alcohol addiction treatment and rehabilitation journey and is involved in any decisions that are made about their care. This ensures that you benefit from a truly collaborative and personalised treatment experience and the most positive outcomes for you as an individual. Our non-judgemental, highly compassionate addiction treatment environments provide you with the ideal setting in which to address your challenges and achieve and full and sustainable recovery.\nWhat emerges from relationships with poorly defined boundaries is a survival mentality where family members assume roles to help cope with stress. Though these roles can temporarily lessen stress, they increase confusion and anxiety because the underlying issue of the substance use is never directly dealt with. Rehab can help you understand where these boundaries get tangled up and show you ways to keep them healthy.\nAlcohol addiction treatment at Priory is delivered as part of a comprehensive Addiction Treatment Programme. Our Addiction Treatment Programmes typically last for 28 days, and consist of you staying at one of our nationwide hospital sites on a residential basis, for the duration of this time. During treatment, you will have the opportunity to undergo a medically assisted withdrawal detoxification process if this is required, before undergoing intensive individual and group addiction therapy in order to address the source of your addictive behaviours, increase your self-awareness and take steps towards recovery. Whilst 28 days is the recommended treatment time for alcohol addiction, treatment lengths at Priory can be flexible according to your unique needs, requirements and commitments.\nScientific research since the mid-1970s shows that drug abuse treatment can help many drug-using offenders change their attitudes, beliefs, and behaviors towards drug abuse; avoid relapse; and successfully remove themselves from a life of substance abuse and crime. Many of the principles of treating drug addiction are similar for people within the criminal justice system as for those in the general population. However, many offenders don’t have access to the types of services they need. Treatment that is of poor quality or is not well suited to the needs of offenders may not be effective at reducing drug use and criminal behavior. Robin Williams: Alcohol, Cocaine, & Rehab\nOne of the brain areas still maturing during adolescence is the prefrontal cortex—the part of the brain that allows people to assess situations, make sound decisions, and keep emotions and desires under control. The fact that this critical part of a teen's brain is still a work in progress puts them at increased risk for making poor decisions, such as trying drugs or continuing to take them. Introducing drugs during this period of development may cause brain changes that have profound and long-lasting consequences.\nResearch has identified differences in how the reward center of the brain responds to alcohol in heavy and light drinkers. In either group, alcohol caused the release of naturally occurring feel-good endorphins in the two brain regions linked to reward processing. Once addicted, alcohol withdrawal presents dangerous physical and psychological issues.9\nCocaine is a stimulant drug that causes dangerous physical effects such as rapid heart rate and increased blood pressure. Cocaine is extremely addictive due to its short half-life and method of action. It keeps a steady stream of dopamine in the brain while users are high, preventing further dopamine production and closing down dopamine receptors. When withdrawal sets in, the brain starts to crave the lost dopamine the drug once provided, making it extremely hard to recover from.10\nAfter occasional use comes alcohol abuse. This stage involves more frequent use of alcohol and you may be drinking more than the recommended amount on a regular basis. You might start drinking for more than just social reasons. Maybe you are drinking alcohol to make you feel better or different. You might be using it to boost your confidence or to alleviate feelings of stress or anxiety. This can lead to a cycle of abuse and an emotional attachment to alcohol.\nMany people and families in the United States do not have the extra income to pay for health care. Medicaid is set up for low-income families with little to no resources available to them. Medicaid is available to people of all ages who fit the criteria and are eligible for coverage. The program is funded by the state and the federal government and currently all of the states within the US participate in the program. Each state does not have to follow the eligibility criteria, specifically as to what is laid out. Each person applying must be a US citizen or a legal permanent resident, and this also applies to low income adults, their children, and persons with disabilities. Having a low income is not the only requirement needed for eligibility and coverage.\nDespite ongoing efforts to combat addiction, there has been evidence of clinics billing patients for treatments that may not guarantee their recovery. This is a major problem as there are numerous claims of fraud in drug rehabilitation centers, where these centers are billing insurance companies for under delivering much needed medical treatment while exhausting patients' insurance benefits. In California, there are movements and laws regarding this matter, particularly the California Insurance Fraud Prevention Act (IFPA) which declares it unlawful to unknowingly conduct such businesses.\nWhen a person receives a diagnosis of alcoholism, the next important step is getting that person to appropriate alcoholism treatment. Unfortunately, there is a variety of reasons alcoholics are reluctant to seek treatment including, the belief that therapy will not work, fear of being stereotyped and complete denial they have a problem at all. The first thing alcoholic individuals and their loved ones should understand is that alcoholism is a disease. In addition, just as some diseases cause pain, alcoholism produces responses such as fear of withdrawal and severe cravings. It is also good for alcoholics to understand that treatment can be challenging but that it is all worth it to achieve a successful recovery. Intervention by a loved one is usually a turning point for alcoholic individuals, often providing them with the motivation to seek the help they need. While it is important an alcoholic's loved ones express their support, they will also need to be firm in their insistence that the person seeks treatment.\nAntabuse is a bridge between your two lives. On the one hand, you have the life that you know. It's not what's good for you, but it's what you know. On the other hand, you have the life that you want to get to. It's better for you, but you don't know how to live there. You don't know how to relax, reward yourself, and celebrate without using drugs or alcohol. Antabuse helps you live in that life long enough so that you can develop new habits and coping skills.\nAddiction treatment at Priory is based on the world-renowned 12-Step approach, which is an abstinence-based addiction treatment model that was first pioneered by the organisation Alcoholics Anonymous (AA). The 12-Step philosophy provides a set of guiding principles for the addiction treatment and rehabilitation process, and focuses on your motivation to change your unhealthy behaviours and thought patterns, whilst also drawing upon elements of spirituality within the treatment and recovery process.\nOur recreation director, a professional personal trainer, schedules outdoor and indoor activities that vary from season to season. Expansive grounds and a heated swimming pool allows you to enjoy the great outdoors during the summer. We also offer crafting workshops and computer skills workshops and much more, all ways to explore new or past interests that fell to the side due to alcohol addiction.\nLike cocaine, crystal meth acts on the dopamine level in the brain but provides an additional touch of mimicking norepinephrine. The result? Neurons release more of both, while training your brain to need more in order to survive. The hangover and withdrawals last days and can break down a person mentally and physically. Addicts suffer psychosis, hallucinations, memory loss, severe depression and sometimes suicide.12 Addiction and Recovery: A How to Guide | Shawn Kingsbury | TEDxUIdaho\nInpatient addiction treatment focuses on stabilization and assessment of your health to ensure you are ready--physically, psychologically and emotionally--to learn about core recovery concepts and to begin practicing recovery principles. Each day, you will be given a schedule of treatment activities, appointments and services tailored to meet your specific recovery needs and goals. Learn more about what happens in a typical day of inpatient addiction treatment. Robin Williams: Alcohol, Cocaine, & Rehab\nThere are many factors that contribute to drug addiction: genetic makeup, family background, social influences, neurological factors, and environmental issues. Having a close family member who is addicted to drugs, or growing up in an environment where drug use is widely accepted, can increase your chances of dependence and drug addiction. A co-occurring mental illness makes you vulnerable to addictive drug use.\nDenial is common among those suffering with alcoholism. Your loved ones may have tried to discuss the problem with you, but you were unable to see things as clearly as them. Denial is one of the body’s defence mechanisms and is employed by the brain to protect you from a harsh reality. It can be useful for a short period but if it continues, can end up causing harm.\nBut perhaps the biggest indicators of an alcohol problem are the withdrawal symptoms if a problem drinker goes without alcohol. A casual or moderate drinker can cut off their intake of alcohol with no adverse effects. If a problem drinker tries to do the same, they may feel some effects of withdrawal within eight hours of their last drink, such as the following:\nHigh Success Rates. Most drug rehabs keep track of the recidivism, or relapse, rates of their patients and the most effective programs keep in close contact with clients as much as possible after they are graduated from treatment. The success rates for different drugs and situations can help patients compare the efficacy of different theories behind addiction treatment. The Discovery House - the best drug rehab center in Los Angeles, Virtual Tour.\nDrug addiction is a chronic disease, and relapse is one of its major symptoms. It’s important for a recovering addict to realize that relapse is the rule rather than the exception. Relapse prevention therapy can help addicts learn how to avoid lapses, or how to minimize the severity of a relapse if they do slip. The sooner you seek help after a relapse, the sooner you’ll get back on track with your recovery program.\nResidential drug treatment can be broadly divided into two camps: 12-step programs and therapeutic communities. Twelve-step programs are a nonclinical support-group and faith-based approach to treating addiction. Therapy typically involves the use of cognitive-behavioral therapy, an approach that looks at the relationship between thoughts, feelings and behaviors, addressing the root cause of maladaptive behavior. Cognitive-behavioral therapy treats addiction as a behavior rather than a disease, and so is subsequently curable, or rather, unlearnable. Cognitive-behavioral therapy programs recognize that, for some individuals, controlled use is a more realistic possibility.\nAnother factor to consider in choosing between inpatient and outpatient rehab options is whether you have a healthy and supportive home environment where your recovery will be a priority. If you do, outpatient treatment could be a good fit. Otherwise, a residential treatment program where you will have a built-in system of support will probably be the most effective option.\nA large body of scientific evidence has been gathered in recent years to show that addiction can run in families. In fact, children of alcohol-addicted parents are four times more likely to develop alcohol addiction in later life than those born to parents without alcohol addictions. How this works is complex, and there is no one ‘alcohol gene’ to blame for this; instead a number of genetic variations, which mean some individuals are more pre-disposed to alcoholism than others.\nGenetics make up about 50% of the risk for alcohol dependence, but they by no means tell the whole story. Genetic history is often hard to distinguish, but if parents are regular heavy drinkers, or they drink to reduce stress and depression, it is likely that their children will grow up believing that these behaviours are normal and possibly harmless. But environmental influence doesn’t come only from the home; peer pressure from friends, colleagues and partners can also encourage new and difficult patterns of drinking which can lead to dependency or co-dependency."", 'Time can heal all wounds, the saying goes, and that wisdom certainly applies to addiction recovery as well. For people who address their addiction with the help of professional drug rehabilitation, there are plenty of options to choose from, and selecting one may seem like a never-ending challenge. But it doesn’t need to be if you think about what you really need to heal from substance abuse.\nThe residential treatment model is popular among people who need more time to address their illness and develop the skills needed to live without addictive substances.\nThink of residential rehab as the place where you get the time and space you need to address the psychological effects of an alcohol or drug addiction and learn new skills that can help you make the changes you need. You also have time to figure out how to rebuild your life from there.\nResidential rehabilitation, or residential rehab for short, offers clients services and activities in a residential setting that are all designed to support their recovery from drug or alcohol addiction. Clients live on-site at a facility that offers 24-hour monitoring and a setting that is free from outside distractions. Such a climate can help guide a client into earn to maintain a sober lifestyle. Addiction care and medical professionals are available 24 hours a day to help clients with their needs.\nPeople who have had a detox and must now choose which treatment program to enter may want to consider a residential rehab, especially if their addiction is severe.\nClients typically voluntarily enter a residential rehabilitation program; other clients may have to enter after a court order is issued. Programs of this kind are also offered for specific populations, such as adolescents, women, members of the LGBTQ+ community, veterans, homeless individuals, people with severe mental health disorders, and people in the criminal justice system.\nResidential programs also are ideal for people who do not have a stable place to live, a job, or limited to no family support.\nHow long someone stays in residential treatment just depends on the person. How long clients stay in treatment depends on several factors, including:\nBefore it is determined that a residential setting is appropriate, an assessment, which takes place after the detoxification process, will review these factors and others such as medical history. The minimum required stay at a residential rehab is 30 days. Clients can stay even longer than a year, depending on the needs of the person who needs addiction care.\nAddiction is a chronic, relapsing brain disease, says the National Institute on Drug Abuse (NIDA). But, as the government agency notes, addiction is treatable.\nA long-term residential treatment model is recommended for people who have severe addictions to opiates/opioids (OxyContin, heroin), cocaine, methamphetamines, and alcohol among other drugs. People who have engaged in polysubstance use, when two more drugs are used at the same time, also may spend more time in residential rehab.\nResearch shows that at least 90 days or more are needed to treat a substance addiction. Long-term residential treatment increases the chances of achieving sobriety.\nAccording to the National Institute on Drug Abuse (NIDA), the most widely known long-term residential treatment model is the therapeutic community (TC), where clients can plan to stay between six and 12 months. In this arrangement, according to NIDA:\nThis kind of community also has a strong focus on helping to “resocialize” the individual and draws upon everyone, including the staff and other residents, to enhance the treatment experience.\nReady to get Help?\nWe’re here 24/7. Pick up the phone.\nYou can expect a range of inpatient services in an environment that promote long-term recovery.\nTreatment services offered at a residential treatment center often include:\nThis is typically the place people in recovery start the process of overcoming addiction. Medical detox is the first step to any successful addiction recovery. In many cases, withdrawal symptoms are often too difficult to handle without professional detox treatment.\nEducation helps clients understand what substance abuse and addiction are. It also raises awareness of the warning signs of addiction and information on how different addictive drugs and substances affect the mind and body, helping to prevent relapse in the future. Clients also learn about the effects addiction can have on one’s physical and mental health as well as their personal and professional relationships.\nPeople in active addiction and alcoholism often neglect their health and well-being. Nutrition is important to helping people regain physical, mental, and spiritual well-being.\nMuch of addressing addiction and substance abuse involves changing one’s thoughts and behaviors. Behavioral therapies, such as cognitive behavioral therapy (CBT), helps clients address their negative thoughts and actions associated with addiction. With CBT, clients learn practical strategies and skills to change old habits or replace them altogether.\nAddiction affects more than just the person who is going through it. Rehab for couples helps relationships maintain stability through recovery from substance abuse, and it is effective when either one or both partners are recovering.\nRecovery from substance abuse often starts with personal accountability. Personalized treatment for clients can help them address emotional and social issues that contribute to their desire to use drugs and alcohol. This includes helping them see the problems they have and motivating them to change course.\nAs it’s often said, addiction is a family disease. Clients and their families can both work on recovery from substance abuse together. Family therapy can be the starting point to figuring out how to move forward and obtain healing that can benefit everyone in the family unit.\nClients who have a similar journey and experiences in common can provide invaluable support to each other. Group therapy sessions can ensure clients they don’t have to walk the path to recovery alone. They also provide opportunities for growth and support, including connecting with others and building friendships with people who abstain from drugs and alcohol.\nThis training can involve teaching clients job skills, social skills, communication skills, anger management, stress management, goal setting, and money management among others needed as clients learn or relearn how to be a part of society.\nClients who must take medications as directed by a physician or other medical professional can rest assured that they will receive what they need while they are in residential care. In many cases, medications are issued to help manage the physical symptoms of withdrawal or help clients abstain from substance use.\nClients can be reassured that centers, such as Pathway to Hope in Fort Lauderdale, Florida, continue to support clients after they leave their facilities. In residential treatment, there are alumni programs in place that are designed to help prevent relapse and ensure post-addiction treatment success.\nThe 12-step fellowships allow participants to reflect on their past experiences openly and honestly with growth being the goal.\nNIDA advises that long-term residential treatment can feel confrontational at times, possibly because of the intense focus on the client. As it notes, activities in the therapeutic community “Are designed to help residents examine damaging beliefs, self-concepts, and destructive patterns of behavior and adopt new, more harmonious and constructive ways to interact with others.”\nOther comprehensive support services, such as employment training, are offered on-site.\nResidential treatment is just one model for addiction recovery. There are others including intensive outpatient treatment and partial hospitalization. Residential treatment may be ideal in many situations, but some considerations must be made before committing to a program of this kind.\nOne is the cost. A longer stay in treatment means the costs will be higher. Costs can be affected by the severity of a person’s addiction, the goals they want to achieve, and the person’s response to treatment, among other things. Consider whether your insurance plan will cover these, and if so, which costs will be paid for.\nThere are reports that the basic level of residential treatment can run between $2,000 and $20,000. If it runs longer, such as during a two- or three-month span, costs could double, so keep that in mind when considering how much you can spend or cover with an insurance plan or another payment plan.\nThere is a strong correlation between the kind of treatment one receives and how long they receive it. Research suggests that residential treatment is the most effective form of addiction treatment.\nWhen considering residential treatment, think about the long-term and what goals you or your loved one wants to achieve while in a facility. Time spent in drug rehab or alcohol rehab is your special time to get focused on recovery and putting addiction behind you, so make the most of it and put effort into it. NIDA informs that along with stopping drug abuse, the main goal of treatment is to return individuals to productive functioning in the family unit, the workplace, and the community in which they live.\nThe outcome of substance abuse treatment; however, largely depends on the person. NIDA writes on the issue, “… Individual treatment outcomes depend on the extent and nature of the patient’s problems, the appropriateness of treatment and related services used to address those problems, and the quality of interaction between the patient and his or her treatment providers.”\nPathway to Hope, a Delphi Behavioral Health Group facility, specializes in helping people who are battling with substance addiction. We don’t just treat the substance addiction; we also treat the thought patterns and behaviors that can prompt individuals to abuse substances with effective treatments that focus on the roots of your addiction and mental health condition.\nFor detox and residential treatment, we will connect you with our sister facility, Arete Recovery. After that, you’ll come to Pathway to Hope for outpatient services. Don’t delay. If you need addiction treatment, now is a good time to seek it.\nNational Institute on Drug Abuse. Principles of Drug Addiction Treatment: A Research-Based Guide (Third Edition). Retrieved March 2018 from https://www.drugabuse.gov/publications/principles-drug-addiction-treatment-research-based-guide-third-edition/principles-effective-treatment\nNational Institute on Drug Abuse, (July 2015).What Are Therapeutic Communities?. National Institute on Drug Abuse. Retrieved January 2017 from https://www.drugabuse.gov/publications/research-reports/therapeutic-communities/what-are-therapeutic-communities']"	['<urn:uuid:3d563bab-f1d4-4032-8b44-aecef4b2a15a>', '<urn:uuid:0db58a4c-7ef7-4ab7-b730-c00da5c51831>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T19:58:41.790432	6	88	4097
6	How does having real users test software help development?	The fastest way to deliver usable software is in cooperation with a user. No services should be developed without an interested consumer committed to providing feedback. Additionally, a formal Experience Design Research phase has a profound impact on identifying a hypothesis focused on unique user value.	"[""User Centric Approach\nThe fastest way to deliver usable software is in cooperation with a user. No services should be developed without an interested consumer committed to providing feedback.\nA formal Experience Design Research phase has a profound impact on identifying a hypothesis focused on unique user value\nMicroservices as a Best Practice\nMicroservices architecture is a convergence of concepts such as Service oriented Architecture (SOA), continuous delivery, virtualization and domain driven design. It creates technical and organizational scalability in performance through the distribution of tasks and in development by enabling teams to work autonomously from concept through to deployment.\n- They are defined by the principles of Domain Driven Design and represent business concepts\n- They are focused on doing one thing well\n- They are preferably only large enough to do this one thing\n- They are developed, evolved and executed autonomously\n- They provide language neutral interfaces\n- They do not rely on intelligence with the communication system (smart endpoints, dumb pipes)\n- They do rely on infrastructure support for composition and orchestration; however, the services themselves are agnostic of the infrastructure in which they are hosted and orchestrated.\nMicroservices in OSDU\nFor more on Microservices review the preferred architecture style in OSDU\nAligned Autonomous Teams\nDeveloping software effectively in a large distributed company requires empowering independent teams to develop software aligned through common goals and principles.\nContinuous Delivery and Feedback\nBuild software in such a way that it can be released into production at any time in a manner that provides immediate feedback.\nAdopt Hypothesis Driven Development: Development of new products and services as a series of experiments to validate a hypothesis of value and scale\nMake Security Everyone's Concern\nSecurity is a shared responsibility; it has to be valued as a business concern, considered at the time of design, implemented during development, validated through testing and monitored and reacted to during operations.\nThis requires a competency program targeted at each skill combined with shared practices throughout the DevOps process.\nLeverage Cloud Provider Expertise\nSecurity is a shared responsibility; providing the best security in the cloud requires understanding of what controls the cloud provider delivers and leveraging/complimenting those controls to the best of our ability.\nProtect Data As the most Valuable Asset\nData is the most critical asset of a data ecosystem. Access should be entitled, usage should be governed and the control should remain the responsibility of the owner.\nPut Customers in Control of Data and Users\nAvoid users and services that operate with elevated privileges. Customers should manage their own users and entitle their own data.\nMonitor Everything Important to the User\nReliability should always be observed from the perspective of the user and not the system. There might need to be separate monitoring for the different user categories\nFavor Managed Offerings\nWhen possible, select a service with a well-defined support model so that we can focus our attention on the technology that we develop rather than on what we consume. Once our technology has matured, then we can consider hosting dependent services if it achieves greater transparency, or improved compliance in operations.\nUse Robust and Fault Tolerant Patterns\nAchieving a step change in both system reliability and release agility in a highly distributed environment requires design and implementation that are resilient in the presence of failure.\nMicroservices operate in a distributed environment and need to be designed for failure. When successfully accomplished, microservices bring resiliency to a system through the isolation of fault.\nAutomate at Every Step\nA microservices architecture is comprised of many moving parts deployed and operated in a distributed environment. Scalable systems development and evolution requires removing the human element whenever possible in development, assurance, deployment, and operations.\nSupport Multiple Clouds\nAvoid the least Common Denominator\nPoly-cloud does not mean cloud agnostic. Make informed trade-off decisions, on a service by service basis, on when to use a portable solution vs. accommodating solutions that are optimized for cost, performance and reliability. The type of application and its needs for performance, latency and data volume will determine how closely the application need to run on bare metal.\nAdopt Community (Open Source) APIs\nWe need to aim for a Provider API. When possible, select an Open Source API that has Cloud optimized implementations as a means of balancing portability and optimization.""]"	['<urn:uuid:e940b5ac-e439-4237-979e-4bdf6b9466e8>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	9	46	718
7	What kinds of rocket engines are being used in space vehicles today?	Current rocket propulsion systems include liquid, solid-propellant, hybrid and electrical rocket propulsion systems. These are installed in both expendable and reusable launch vehicles and spacecraft.	['Session 1. Current and Future Space Launch Systems, Launch Vehicles, Their Components and Systems\n- analysis of technical solutions for current and future space launch systems and launch vehicles, including expendable and reusable launch vehicles;\n- analysis of present-day trends of launch services global market development;\n- analysis of technical solutions for launch vehicles critical subsystems (control, thermostating, power-supply systems);\n- Integrated launch vehicle (ILV) assemblies testing methods and facilities (test-bench facilities, monitoring real time computer systems, measurement and control of test equipment and pilot designs);\n- optimal reliability provision for LV and launch complex;\n- development of antiasteroid protection of the Earth;\n- near-Earth space disposal of anthropogenic wastes;\n- systems and technologies of injection;\n- ballistics, dynamics, aerogasdynamics, heat exchange and heat shielding, loading and durability of modern and future space-rocket hardware.\n|Chairman:||Alexander Mashchenko –|\n|First Deputy General Designer – Deputy General Director on Logistics, Yuzhnoye SDO, IAA Academician, Ukraine.|\n|Coordinators:||Alexander Kushnarev –|\n|Deputy General Designer – General Director of LV and SLS System Engineering, IAA Academician, Yuzhnoye SDO, Ukraine.|\n|Evgeniy Baranov –|\n|Deputy Chief Designer and Head of System Design of Rockets and Missiles Design Office – Chief of Design Complex of Complex Space and Missile Systems Development, Yuzhnoye SDO, Ukraine.|\n|Vladimir Sirenko –|\n|Head of Design and Theoretical Complexes of Design and Calculation of Ballistics, Aerodynamics, Heat-mass Exchange and Durability Division, Yuzhnoye SDO, IAA Corresponding Member, Ukraine.|\n|Sergey Davydov –|\n|Head of Aircraft Planning and Design Department, Dnepropetrovsk National University, professor, Ukraine.|\n|Secretary:||Alexander Berdnik –|\n|Group Leader, Yuzhnoye SDO Ukraine.|\nSession 2. Current and Future Space Satellite Systems\n- technical configuration of current satellites and spacecraft;\n- onboard and ground support systems for data acquisition, spacecraft control and testing;\n- space missions and technologies for Earth environment data acquisition;\n- space communication technologies and systems, including fixed and mobile satellite communication, TV- and broadcasting, fixed and broadband interactive-multimedia satellite services;\n- scientific missions to research in the field of Earth sciences, astronomy and astrophysics, fundamental physics and astrophysics;\n- current and future navigation systems;\n- power supply of spacecraft.\n|Chairman:||Alexander Makarov –|\n|Deputy General Designer of Production and Operation of Space vehicles, Satellite, Controlling and Management-information Systems of Space-rocket and Ground Segments, Yuzhnoye SDO, IAA Academician, Ukraine.|\n|Coordinators::||Volodymyr Maslyey –|\n|Chief Designer – Head of Spacecraft, Measuring Systems and Telecommunications Planning and Design Office, Yuzhnoye SDO, Ukraine.|\n|Vasily Stasiuk –|\n|Deputy Head of the Center of Reception and Processing of Specific Information and Navigation Control Field (CRPSI and NC), the National Center for Management and Testing of Space Resources (NCMTSR), Ukraine.|\n|Yuriy Shovkoplyas –|\n|Deputy Chief Designer of Spacecraft, Measuring Systems and Telecommunications Planning and Design Office, Yuzhnoye SDO, Ukraine.|\n|Lukyan Anatychuk –|\n|Director of Thermoelectricity Institute at NAS and DES in Ukraine, NASU Academician, Ukraine.|\n|Secretary:||Oleg Dotsenko –|\n|Group Leader, Yuzhnoye SDO, Ukraine.|\nSession 3. Future Rocket Engines and Propulsion Systems\n- technological solutions used to develop current liquid, solid-propellant, hybrid and electrical rocket propulsion systems, installed in expendable and reusable launch vehicles and spacecraft;\n- future propulsion systems for launchers.\n|Chairman:||Vladimir Schulga –|\n|Deputy General Designer of Rocket Engine Development Planning and Design Office, Yuzhnoye SDO, IAA corresponding member, Ukraine.|\n|Coordinators:||Yury Mitikov –|\n|Head of Propulsion Engineering Department, Associate Professor, Dnepropetrovsk National University, Ukraine.|\n|Anatoly Kirichenko –|\n|Deputy Chief Designer – Head of Solid-propellant Rocket Engine Design Office, Yuzhnoye SDO, Ukraine.|\n|Vladimir Durachenko –|\n|Head of Department of of Spacecraft, Measuring Systems and Telecommunications Planning and Design Office, Yuzhnoye SDO, Ukraine.|\n|Secretary:||Andrey Kuchta –|\n|Engineer and designer of the I -st category, Yuzhnoye SDO, Ukraine.|\nSession 4. Materials and Technologies\n- recent technologies and advanced studies on new materials and structures used for expendable and reusable space transportation systems and spacecraft, as well as methods of their construction;\n- new rocket and space materials and technologies quality assessment techniques;\n- space-rocketry nanotechnologies.\n|Chairman:||Alexander Potapov –|\n|Head of Advanced Materials and Technologies Complex, Yuzhnoye SDO, Academician of Manufacturing Science Academy,Candidate of Technical Science, IAA Academician, Ukraine.|\n|Coordinators:||Anatoliy Sanin –|\n|Head of Aircraft Technology and Production of Physical and Technical Faculty, Doctor of Engineering, Dnepropetrovsk National University, Ukraine.|\n|Yevgeny Djur –|\n|Professor of Aircraft Technology and Production of Physical and Technical Faculty, Doctor of Engineering, Dnepropetrovsk National University, IAA corresponding member, Ukraine.|\n|Nataliya Kalinina –|\n|Professor of Aircraft Technology and Production of Physical and Technical Faculty, Dnepropetrovsk National University, Doctor of Engineering, IAA Corresponding Member, Ukraine.|\n|Secretary:||Oksana Pronzevich –|\n|Leading researcher Yuzhnoye SDO, Ukraine.|\nSession 5. Space for Humankind\n- space exploration: political, economic and juridical aspects;\n- space exploration and international cooperation;\n- space activity influence on social and economic development and society daily life;\n- methods and future of youth space education;\n- distance design and education through the Internet;\n- development and implementation of new information technologies of international and regional standardization of space technologies, foreground tasks and issues;\n- innovation technologies and inventions;\n- space technologies in human history;\n- museums, historical and museum and historical and technical activity.\n|Chairman:||Alexander Novikov –|\n|Director of Rocket and Space, Training and Research Center, Yuzhnoye SDO, Candidate of Technical Science, IAA Academician, Ukraine.|\n|Coordinators:||Irina Dyachuk –|\n|Director of Zhitomir Astronautics museum named after S.P.Korolev, Doctor of Philosophy, Associate Professor, Ukraine.|\n|Yuriy Ziatdinov –|\n|Director of State Aviation Museum of Ukraine named after O.K.Antonov.|\n|Irina Fedorenko –|\n|Head of Organizational Department NAECUY, Candidate of History, Dnepr, Ukraine.|\n|Victor Perlik –|\n|Professor, Honored Worker of Science and Technology in the city Dnepr, Ukraine.|\n|Valeriy Kapinus –|\n|Chief of Specialized Department, Yuzhnoye SDO, Ukraine.|\n|Secretary:||Nina Zykova –|\n|Head of Postgraduate Studies Department, Yuzhnoye SDO, Ukraine.|\nSession 6. Ground complexes, launching equipment and their operation\n- Research, evaluation, prediction and improvement of technical, economic and ergonomic characteristics of the launching and ground equipment at various stages and phases of the life cycle, as well as technical support in view of functioning the “person- technique” systems.\n- Development of theoretical foundations, models and methods of substantiation the technical requirements to the complexes and launching equipment to ensure their durability and survivability, achieving a high level of reliability and maintainability.\n- Development and improvement of theoretical and experimental methods to determine the feasibility and rational ways of using the new physical principles, technical solutions and processes in the advanced systems and launch equipment creation and operation.\n- Information technology use for the design and development of space-rocket systems.\n- Research, development and improvement of the stress-strain state calculation methods, dynamic processes modeling during transportation and installation of rocket-space systems, acoustic, gas-dynamic and heat and mass exchange processes in the operation of launchers and launching sites.\n- Development of principles, ways of construction and technical solutions of ground complexes, launch equipment, designed to ensure the required levels of technical, operational, economic, ergonomic and ecological characteristics.\n- Development and improvement of methods and devices to ensure explosion and fire safety, neutralization of gas-drainage systems, vapour or spills of toxic rocket fuel components for creating the required environmental conditions.\n- Development of methodology and program planning techniques, justification, creation, testing, evaluation and improvement of operating systems and the operating and technical specifications of ground complexes and launching equipment.\n- Testing and operation of space-rocket complexes.\n- Creation and improvement of methods and control means, diagnostics and forecasting of them in the process of operation and recovery complexes.\n- Development of forecasting methods to identify causes of the complex emergency state.\n- Calculation and modeling methods development of the hydrodynamic and heat and mass exchange processes in filling systems, providing refueling, dosing and preparation of low- and high-boiling propellant components for temperature and gas content.\n- Research and development of devices to ensure temperature-humidity conditions of aircraft, structures and equipment of launching complexes.\n- Research on the justification, evaluation and improvement the training equipment and development of methods to increase the efficiency of their use.\n|Chairman:||Pavel Degtyarenko –|\n|Chief Designer – Head of System Design of Missiles and Missile Systems Design Office, Yuzhnoye SDO, Ukraine, Corresponding member of IAA.|\n|Coordinators:||Konstantin Bezruchko –|\n|Doctor of Technical Sciences, Professor of NAU “KhAI” named after N. E. Zhukovsky, Honored worker of Space Industry of Ukraine, Corresponding member of IAA.|\n|Deputy General Designer, Flight Development Tests and Flight Operations.|\n|Head of Design Complex for Creation the Ground-based Complexes, Systems and Components of Missile Systems and Space-rocket Complexes, Yuzhnoye SDO, Ukraine.|\n|Sergei Davydenko –|\n|Head of Complex for Development, Support the Systems and Components Development, Testing and Operation of the Missile and Space-rocket Complexes, Yuzhnoye SDO, Ukraine.|\n|Viktor Frolov –|\n|Deputy Head of Design Complex for Creation the Ground-based Complexes, Systems and Components of Missile Systems and Space-rocket Complexes, Yuzhnoye SDO, Ukraine.|\n|Head of Sector.|\nThe Round table „Lunar research and industrial base„ will be held within the Conference frameworks.']	['<urn:uuid:9e867835-bf73-4ad1-ad83-9082824fd2d4>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T19:58:41.790432	12	25	1450
8	education background academic degree karen price what university	Karen Price received her Ph.D. in Biostatistics in 2001 from Baylor University.	"['Karen Price is a Research Advisor at Eli Lilly and Company. Karen received her Ph.D. in Biostatistics in 2001 from Baylor University. She is currently leading the Bayesian methods efforts within the Advanced Analytics/Science Driven Adaptive Program team. She chairs the DIA Bayesian Scientific Working Group, which includes members in Industry, Regulatory, and Academia. The group\'s mission is to ensure that Bayesian methods are well-understood, accepted, and broadly utilized throughout medical product development. Her research interests include Bayesian meta-analysis, Bayesian methods for safety signal detection and evaluation, and Bayesian design and analysis of clinical trials.\nHong H, Fu H, Price K, and Carlin B. Incorporation of individual-patient data in network meta-analysis for multiple continuous endpoints, with application to diabetes treatment. Statistics in Medicine, 2015.\nXia A and Price K. Bayesian Applications for Drug Safety Evaluation. Chapter in Quantitative Evaluation of Safety in Drug Development. CRC Press, edited by Jiang and Xia, 2015.\nLiu-Siefert H, Siemers E, Price K, Han B, Selzler K, Henley D, Sundell K, Aisen P, Cummings J, Raskin J, Mohs R, and the Alzheimer\'s Disease Neuroimaging Initiative. ""Cognitive Impairment Precedes and Predicts Functional Impairment in Mild Alzheimer\'s Disease."" Journal of Alzheimer\'s Disease, 47: 205-214, 2015.\nLiu-Seifert H, Simers E, Sundell K, Price K, Han B, Selzler K, Aisen P, Cummings J, Raskin J, Mohs R. ""Cognitive and Functional Decline and Their Relationship in Patients with Mild Alzheimer\'s Dementia."" Journal of Alzheimer\'s Disease. 43: 949-955, 2015.\nPrice K and LaVange L. Bayesian Methods in Medical Product Development and Regulatory Review. Pharmaceutical Statistics, 2014.\nPrice K, Xia A, Lakshminarayanan M, Madigan D, Manner D, Scott J, Stamey J, and Thompson L. Bayesian methods for design and analysis of safety trials. Pharmaceutical Statistics, 2014.\nOhlssen D, Price K, Xia A, Hong H, Kerman J, Fu H, Quartey G, Heimann C, Ma H, Carlin B. Guidance on the implementation and reporting of a drug safety Bayesian network meta-analysis. Pharmaceutical Statistics, 2014.\nStamey J, Beavers D, Faries D, Price K, Seaman J. Bayesian modeling of cost-effectiveness studies with unmeasured confounding: a simulation study. Pharmaceutical Statistics, 2014.\nFu H, Price K, Nilsson M, and Ruberg S. Identifying Potential Adverse Events Dose-Response Relationships via Bayesian Indirect and Mixed Treatment Comparison Models. Journal of Biopharmaceutical Statistics.\nBennett M, Crowe B, Price K, Stamey J, Seaman J. Comparison of Bayesian and Frequentist Meta-Analytical Approaches for Analyzing Time to Event Data. Journal of Biopharmaceutical Statistics.\nMyers J, Wielage R, Han B, Price K, Gahn J, Paget M-A, Happich M. The Efficacy of Duloxetine, Non-sterodial Anti-inflammatory Drugs, and Opioids in Osteoarthritis: A Systematic Literature Review and Meta-analysis. Submitted to Osteoarthritis and Cartilage.\nFaries D, Peng X, Pawaskar M, Price K, Stamey J, and Seaman J. Evaluating the Impact of Unmeasured Confounding with Internal Validation Data: An Example Cost Evaluation in Type 2 Diabetes. Accepted Value In Health.vMehta N, Sheetz M, Price K, Comiskey L, Amruita S, Iqbal N, Mohler E, Reilly M. Selective PKC b Inhibition with Ruboxistaurin and Endothelial Function in Type-2 Diabetes, submited to Diabetes Care.\nTesfaye S, Tanden R, Bastyr EJ III, Kles KA, Skljarevski V, Price KL, Ruboxistaurin Study Group. Factors the Impact Symptomatic Diabetic Peripheral Neuropathy in Placebo-Administered Patients From Two 1-Year Clinical Trials. Diabetes Care. 2007; 30: 2626-2632.\nKincaid J, Price K, Jimenez M, Skljarevski V. Correlation of Vibratory Quantitative Sensory Testing and Nerve Conduction Studies in Patients with Diabetes. Muscle and Nerve. 2007; 36: 821-827.\nMcGill JB, King GL, Berg PH, Price KL, Kles KA, Bastyr EJ, and Hyslop DL. Clinical safety of the selective PKC-ß inhibitor, Ruboxistaurin. Expert Opinion on Drug Safety. 2006; 5(6): 835-845.\nKing GL, McGill JB, Hyslop DL, Berg PH, Price KL, Kles KA, Bastyr EJ, Vignati L. Clinical Safety of the PKC-ß Inhibitor, Ruboxistaurin in Patients with Diabetic Microvascular Complications. Diabetologia 2006; 49(suppl 1): 657.']"	['<urn:uuid:746d798a-2f75-4063-91a5-731206a6eb7b>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	8	12	629
9	Which hurts usability more: too quick or too slow interfaces?	Too slow interfaces are a much bigger usability problem, with 99% of all usability problems related to response times being caused by interfaces being too slow. While being too fast can cause issues (like missing screen changes or difficulty understanding rapid updates), it is far less common than being too slow.	['Given that 99% of all usability problems related to response times are caused by user interfaces being too slow, it might be dangerous for me to write about those few cases where the computer is too fast. After all,\ncomputers are usually too slow, and\nsnappy response times enhance usability. Human memory decays rapidly, so people might forget some of what they were doing while waiting for a slow computer.\nFurther, while young users are notoriously impatient, even older users won’t linger long on a slow site.\nCaveats enough, I hope; a snappier user experience is almost always better.\nThat said, there is such a thing as too fast. My latest example came a few days ago when we were user-testing tablet applications for the next update of our courses on Usability of Mobile Websites and Apps and Visual Design for Mobile and Tablet: The user repeatedly failed to connect to the correct WiFi network, because she repeatedly touched the wrong entry in the list.\nMaking such a mistake once might be excused as an example of the inherent problem in gestural interfaces — that people sometimes touch a spot that’s a bit off the target.\nBut this user hit the wrong target several times. Why? The intended target kept moving without her noticing it because the tablet was continually scanning the airwaves and adding or removing entries on its list of available WiFi networks.\nThe interaction went as follows:\nThe user scanned the list to locate the intended network (for which she had the proper login info).\nShe started moving her hand toward the target, intending to touch the line for the network found in step 1.\nDuring the fraction of a second that her finger was moving toward the tablet surface, the list would gain or lose an entry, causing the intended target to jump to a different place on the screen.\nThe user selected a different target than intended because the new network was now occupying the spot of the one she wanted (and she didn’t notice that she was touching the wrong name because her finger obscured the target).\nThe user attempted to log in to the selected network, but was refused because she was using credentials from a different network.\nAlthough these steps occurred in rapid succession, the poor user repeated the exact same sequence so many times that I figured out what was happening without needing to replay the usability video.\n(Of course, obeying a basic tenet of usability testing, we couldn’t tell the unfortunate test participant what she was doing wrong; we had to sit in silence and watch her fail repeatedly until she finally got lucky enough to select the network during a period in which the WiFi software didn’t detect any updates to the list.)\nFast Might Be Missed\nWhen the screen changes in the proverbial blink of an eye, users might blink and miss the change. Or, as in my WiFi example, they might not be looking at that particular part of the screen during the brief interval in which the change occurs.\nWhen changes happen on their own — without any user prompting — people don’t even know to look for them and are likely to miss things that change rapidly. To alleviate this problem, one useful tactic is to slow down the change, using, for example, an animation that lasts a second or two. But don’t over signal; there’s nothing more annoying than an aggressively blinking screen.\nRapid screen updates are great when they’re the result of user-initiated actions. When you touch a slider on a tablet, for example, you want it to highlight immediately to indicate that it has been activated. And, as you move the slider, you want the corresponding screen elements to update immediately.\nAs long as users do something, they know to look for the result. But even in these cases, there’s a potential usability issue if the update is far removed from the action. Changes outside the user’s field of vision might be overlooked and require stronger signaling than simply replacing the old information with something new. A classic example is e-commerce shopping carts that are positioned in the screen’s far corner and simply count up when the user clicks Add to Cart. In testing, we often find that users overlook such subtle updates when they occur far away from the button they clicked.\nFast Might Be Difficult\nA second class of usability problems occurs when users do notice a rapid change, but can’t understand what happened quickly enough. This often happens in carousel, rotators, and other auto-forwarding design elements. Once you decide that something might be of interest, it’s yanked off the screen — replaced by something you don’t want.\nThis is particularly problematic for slower users, such as international customers who don’t read your language well or old or disabled users who might need extra time dealing with the user interface and are thus disproportionally harmed by rapidly changing screens.\nLet’s establish a simple usability principle: avoid taunting the customers.\nAnd, to circle back to the beginning, it’s important to remember that, as problems go, being too fast is far less common than being too slow. Whether fast or slow, the point is to pay attention to the time aspect of the user experience. After all, a key differentiator of interaction design compared to most other forms of design is that we deal with time-changing phenomena.\nShare this article: Twitter | LinkedIn | Google+ | Email', 'It looks like there is great interest to quantifying performance impact on business, linking response time to income and customer satisfaction. A lot of information was published, for example, Aberdeen Group report Customers Are Won or Lost in One Second or Gomez whitepaper Why Web Performance Matters: Is Your Site Driving Customers Away? There is no doubt that there is a strong correlation between response times and business metrics and it is very good to have such documents to justify performance engineering efforts – and some simplification may be good from the practical point of view – but we should keep in mind that the relationship is not so simple and linear and it may be cases when it would matter.\nResponse times may be considered as usability requirements and are based on the basic principles of human-computer interaction. As long ago as 1968, Robert Miller’s paper Response Time in Man-Computer Conversational Transactions described three threshold levels of human attention. Jakob Nielsen believes that Miller’s guidelines are fundamental for human-computer interaction, so they are still valid and not likely to change with whatever technology comes next. These three thresholds are:\n- Users view response time as instantaneous (0.1-0.2 second)\n- Users feel they are interacting freely with the information (1-5 seconds)\n- Users are focused on the dialog (5-10 seconds)\nUsers view response time as instantaneous (0.1-0.2 second): Users feel that they directly manipulate objects in the user interface. For example, the time from the moment the user selects a column in a table until that column highlights or the time between typing a symbol and its appearance on the screen. Robert Miller reported that threshold as 0.1 seconds. According to Peter Bickford 0.2 second forms the mental boundary between events that seem to happen together and those that appear as echoes of each other.\nAlthough it is a quite important threshold, it is often beyond the reach of application developers. That kind of interaction is provided by operating system, browser, or interface libraries, and usually happens on the client side, without interaction with servers (except for dumb terminals, that is rather an exception for business systems today). However new rich web interfaces may make this threshold important for consideration. For example, if there is logic processing user input so screen navigation or symbol typing becomes slow, it may cause user frustration even with relatively small response times.\nUsers feel they are interacting freely with the information (1-5 seconds) : They notice the delay, but feel that the computer is “working” on the command. The user’s flow of thought stays uninterrupted. Robert Miller reported this threshold as one-two seconds.\nPeter Sevcik identified two key factors impacting this threshold: the number of elements viewed and the repetitiveness of the task. The number of elements viewed is, for example, the number of items, fields, or paragraphs the user looks at. The amount of time the user is willing to wait appears to be a function of the perceived complexity of the request.\nBack in 1960s through 1980s the terminal interface was rather simple and a typical task was data entry, often one element at a time. So earlier researchers reported that one to two seconds was the threshold to keep maximal productivity. Modern complex user interfaces with many elements may have higher response times without adversely impacting user productivity. Users also interact with applications at a certain pace depending on how repetitive each task is. Some are highly repetitive; others require the user to think and make choices before proceeding to the next screen. The more repetitive the task is the better should be response time.\nThat is the threshold that gives us response time usability goals for most user-interactive applications. Response times above this threshold degrade productivity. Exact numbers depend on many difficult-to-formalize factors, such as the number and types of elements viewed or repetitiveness of the task, but a goal of two to five seconds is reasonable for most typical business applications.\nThere are researchers who suggest that response time expectations increase with time. Forrester research of 2009 suggests two second response time; in 2006 similar research suggested four seconds (both research efforts were sponsored by Akamai, a provider of web accelerating solutions). While the trend probably exists (at least for the Internet and mobile applications, where expectations changed a lot recently), the approach of this research was often questioned because they just asked users. It is known that user perception of time may be misleading. Also, as mentioned earlier, response time expectations depends on the number of elements viewed, the repetitiveness of the task, user assumptions of what the system is doing, and interface interactions with the user. Stating a standard without specification of what page we are talking about may be overgeneralization.\nUsers are focused on the dialog (5-10 seconds): They keep their attention on the task. Robert Miller reported threshold as 10 seconds. Users will probably need to reorient themselves when they return to the task after a delay above this threshold, so productivity suffers. Or, if we are talking about Web sites, it is the threshold when users start abandoning the site.\nPeter Bickford investigated user reactions when, after 27 almost instantaneous responses, there was a two-minute wait loop for the 28th time for the same operation. It took only 8.5 seconds for half the subjects to either walk out or hit the reboot. Switching to a watch cursor during the wait delayed the subject’s departure for about 20 seconds. An animated watch cursor was good for more than a minute, and a progress bar kept users waiting until the end. Bickford’s results were widely used for setting response times requirements for web applications.\nThat is the threshold that gives us response time usability requirements for most user-interactive applications. Response times above this threshold cause users to lose focus and lead to frustration. Exact numbers vary significantly depending on the interface used, but it looks like response times should not be more than eight to 10 seconds in most cases. Still, the threshold shouldn’t be applied blindly; in many cases, significantly higher response times may be acceptable when appropriate user interface is implemented to alleviate the problem.\nSo while there is a strong correlation between response times and business metrics, it is definitely not a linear function. We are touching here the psychology of human-computer interaction and it is definitely not a single-dimension issue. It is very context-specific and published data should be used carefully with understanding what really stands behind them. The main practical conclusion is that you may have a point when further performance improvement won’t make much sense: you have increasing costs of performance improvement with diminishing business value. Although it looks like most existing systems haven’t reached this point yet.\nThe last fourteen years Alex Podelko (@apodelko) worked as a performance engineer and architect for several companies. Currently he is Consulting Member of Technical Staff at Oracle, responsible for performance testing and optimization of Hyperion products. Alex currently serves as a director for the Computer Measurement Group (CMG). He maintains a collection of performance-related links and documents.']	['<urn:uuid:09940dff-f695-4874-a74e-f2024020dee3>', '<urn:uuid:5e6fbaa0-673d-4cc2-80f3-a9b69256ed16>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T19:58:41.790432	10	51	2100
10	How do T cells and ENaC channels sense mechanical forces in our bodies?	Both T cells and ENaC channels use mechanical sensing mechanisms but in different ways. T cells, which are immune system cells, use their cytoskeleton to exert and sense forces while hunting cancer cells, allowing them to grasp and deliver a 'kiss of death' to target cells. ENaC channels, on the other hand, are mechanosensitive ion channels that sense mechanical forces through a different mechanism - they have special pores that can open and shut in response to physical forces, allowing chemicals to move in and out of cells. These channels are important for processes like blood pressure regulation and touch sensation.	"['Mechanical forces rule biological processes, from the contractions of the pump-like heart, to muscles that resemble strings and pullies, and cells that carry out microscopic tugs-of-war.\nPreviously, these mechanical aspects of biology have been largely ignored, not least due to a lack of technology allowing for intricate mechanical measurements.\nBut better tools are being developed, and these enable tracking of mechanical activity in cells and tissues.\nAnd because of this visibility, new drugs and treatments are starting to emerge.\nMechanobiology is the science of how cells and tissues sense and respond to mechanical forces.\nJust like we humans have muscles and bones that give us the ability to exert forces, each of our cells also has a skeleton: the cytoskeleton. This network of fibres allows cells to exert and resist forces, and enables them to move.\nWatching T cells\nT cells are part of our immune system: they can act like cellular assassins, killing other cells such as those infected with viruses, or cancer cells.\nAt the microscale (about one hundredth of a human hair), we can visualise and follow T cells “hunting” for cancer cells as they move and push their way through tissues. This applies the approach known as 3D traction force microscopy (TFM).\nUpon finding a cancer cell, a T cell firmly grasps its target, and delivers a “kiss of death”.\nTechniques known as dual pipette aspiration (DPA) and optical tweezers (see video below) allow us to grab individual cells, and pair them together in a controlled manner. This allows us to understand and illustrate the mechanics behind this deadly “kiss”.\nUsing mechanobiological techniques to see how T cells find and kill cancer cells, may allow better targeting of anti-cancer immunotherapies.\nThe first immunotherapy targeting cancer using a patient’s own T cells was approved just recently by the United States Food and Drug Administration (FDA).\nSensing the force\nCells use force sensors to detect and distinguish between many of the physical signals that they experience.\nA major class of force sensors are “mechanosensitive ion channels”. These are holes, or pores, in the surface of the cell that can open and shut.\nWhen the cell senses a physical force or a mechanical stimulus (in essence, like a microscopic prod), these pores can open. Chemicals move in and out, and a tiny electrical current is conducted across the cell wall. This can be measured by attaching small electrodes to the surface of a cell.\nMany types of cells and tissues have such sensors, and respond to changes in mechanical loads. These include the neurons that underpin our sense of touch, metastasising cancer cells and the cells that maintain our cartilage in bones.\nThe drug EVENITY – which aims to prevent bone loss in osteoporosis – acts through this pathway. The drug blocks sclerostin, a key factor that naturally inhibits bone formation based on the mechanosensing functions of bone-forming cells.\nTrialled in mice travelling to the International Space Station, the treatment has now passed human clinical trials and is awaiting FDA approval for use in the treatment of osteoporotic patients in the United States.\nOrgan on a chip\nOrgan-on-chip technology is designed to aid drug development, disease modelling and personalised medicine. Each individual unit is made of a transparent material known as a polymer: it’s about the size of a USB stick, and consists of hollow channels lined by living human cells.\nThese chips differ from other laboratory tests such as cell culture, in that they can mimic the physiology and mechanics of how cells interact with living tissues (rather than just looking at responses in individual cells).\nFor example, organs-on-chips can recreate the architecture of human organs at the microscopic level, including the intestines, kidney, skin, bone marrow and areas of the brain.\nAn example using lung tissue is described in the video below. This technology provides a way to see diseases in tissues, and is an alternative to animal testing for drug development.\nMany researchers and biotechnology companies hope that technology such as organs-on-chips will accelerate the development of new drugs, and advance personalised medicine.\nUsing existing techniques, clinical studies can take years to complete and testing a single compound can cost many millions of dollars. Also, pre-clinical animal studies often fail to predict human responses because animal models do not always accurately mimic human biological responses.\nThe agreement may expand in the future to cover additional organ-chips, including kidney, lung and intestine models.\nMechanobiology is integrating physical sciences into biology and driving the development of new technologies. Watching cells in motion, understanding and measuring forces at the cellular scale, and creating mini models of human tissues in the lab are just the beginning.', ""1.A.6 The Epithelial Na+ Channel (ENaC) Family\nEpithelial sodium channels facilitate Na⁺ reabsorption across the apical membranes of epithelia in the distal nephron, respiratory and reproductive tracts and exocrine glands, and hence they have a role in fluid volume homeostasis, osmolarity and arterial blood pressure regulation (Enuka et al. 2012). Acid-sensing ion channels are broadly distributed in the nervous system where they contribute to sensory processes (Schuhmacher et al. 2015). ENaC family members are from animals with no recognizable homologues in other eukaryotes or bacteria. The vertebrate ENaC proteins from epithelial cells cluster tightly together on the phylogenetic tree; voltage-insensitive ENaC homologues are also found in the brain. The many sequenced C. elegans proteins, including the worm degenerins, are distantly related to the vertebrate proteins as well as to each other. At least some of these proteins form part of a mechano-transducing complex for touch sensitivity, but others function in chemosensory transduction pathways (Ben-Shahar, 2011). D. melanogaster also has many ENaC family paralogues, some closely related to each other, others very distant in sequence. Other members of the ENaC family, the acid-sensing and/or mechanosensory ion channels, ASIC1-4, are homo- or hetero-oligomeric neuronal Zn2+ and H+-gated, mechanosensitive channels that mediate pain sensation in response to tissue acidosis. Two extracellular histidines (his-162 and his-339) potentiate Zn2+ activation while another (his-72) mediates pH sensitivity (Baron et al., 2001). ASIC1-4 also mediate light touch sensation and are excited by hair movement. The homologous Helix aspersa (FMRF-amide)-activated Na+ channel is the first peptide neurotransmitter-gated ionotropic receptor to be sequenced. Salty taste is mediated by an ENaC channel in the fungiform papillae in the dorsal epithelium of the anterior tongue. Activation of acid-sensing ion channel 1a (ASIC1a) occurs in response to surface trafficking (Chai et al., 2010). The stress response protein, SERP1, regulates ENaC biogenesis (Faria et al., 2012).\nEpithelial Na+ channel (ENaC)/degenerin family members are involved in mechanosensation, blood pressure control, pain sensation, and the expression of fear. They display a form of desensitization (Roy et al. 2013). Members all exhibit the same apparent topology, each with N- and C-termini on the inside of the cell, two amphipathic transmembrane spanning segments, M1 and M2, and a large extracellular loop. The extracellular domains contain numerous highly conserved cysteine residues. They are proposed to serve a receptor function. Welsh et al. (2002) present three models whereby members of the ENaC family sense mechanostimulation. Their preferred model involves tethering the channel protein to extracellular matrix proteins such as collagens and/or intracellular cystoskeletal proteins such as α- and β-tubulins. Carnally et al., 2008 have presented evidence, based on the X-ray crystal structure, that ASIC1a assembles as a heterotrimer. Carattino (2011) has reviewed the structural mechanisms underlying the function of epithelial sodium channel/acid-sensing ion channel. Opening of the ion conductive pathway involves coordinated rotation of the second transmembrane-spanning domains (Tolino et al., 2011). The second TMS modulates channel gating in response to shear stress (Abi-Antoun et al., 2011). ASIC- and ENaC-types of Na+ channels exhibit different conformational changes (Hanukoglu 2016). The ion selectivity filter has been proposed (Hanukoglu 2016).\nMammalian ENaC is important for the maintenance of Na+ balance and the regulation of blood pressure. Three homologous ENaC subunits, α, β and γ, have been shown to assemble to form the highly Na+-selective channel. Only the dehydrated form of Na+ (or Li+) is transported. The stoichiometry of the three subunits is αβγ in a heterotrimeric architecture, and they form a triangular pyramid-shaped funnel (Edelheit et al. 2014). A structural model has been proposed in which the properties of the channel are conferred by the second TMS together with the preceding hydrophobic region that may loop into the membrane as do the P-regions of VIC family members. The selectivity filter of the epithelial Na+ channel α-subunit is at least in part determined by residues Ser580 to Ser592 following the second TMS. Residues conferring cation selectivity are in both M2 and the preceding loop. Negatively charged residues in M2 of the mammalian α-subunit are important, as two substitutions, αE595C and αD602C confer K+ permeability (Sheng et al., 2001b).\nThe C-terminus of each ENaC subunit contains a PPXY motif which when mutated or deleted in either the β- or γ-ENaC subunit leads to Liddle's syndrome, a human autosomal dominant form of hypertension. In this disease, the mutation induces abnormally high levels of channel expression due to a loss of interaction with the inhibitory Nedd4 protein. Nedd4 regulates the activity of the epithelial Na+ channel in normal people but not in those suffering from Liddle's syndrome. Multiple WW domains in Nedd4 mediate the interaction with all three subunits of ENaC, α, β and γ, and WW domains 2-4 are most important for this interaction (Snyder et al., 2001). Cys palmitoylation of the β subunit modulates gating of the epithelial sodium channel (Mueller et al., 2010).\nAcid-sensing ion channels (ASICs) have been implicated in perception of pain, ischaemic stroke, mechanosensation, learning and memory. They are implicated in touch, pain, digestive function, baroreception, blood volume control and hearing (Chen and Wong 2013). Jasti et al. (2007) reported the low-pH crystal structure of a chicken ASIC1 deletion mutant at 1.9 Å resolution. Each subunit of the chalice-shaped homotrimer is composed of short amino and carboxy termini, and two transmembrane helices. A bound chloride ion is present. A disulphide-rich, multidomain extracellular region is enriched in acidic residues with carboxyl-carboxylate pairs, suggesting that at least one carboxyl group bears a proton. Electrophysiological studies on aspartate-to-asparagine mutants confirmed that these carboxyl-carboxylate pairs participate in proton sensing. Between the acidic residues and the transmembrane pore lies a disulphide-rich 'thumb' domain poised to couple the binding of protons to the opening of the ion channel. The results demonstrated that proton activation involves long-range conformational changes. The Akt and Sgk protein kinases are components of an insulin signaling pathway that increases Na+ absorption by up-regulating membrane expression of ENaC via a regulatory system that involves inhibition of Nedd4-2 (Lee et al., 2007).\nGonzales et al. (2009) presented the structure of a functional acid-sensing ion channel in a desensitized state at 3 Å resolution, the location and composition of the approximately 8 Å thick desensitization gate, and the trigonal antiprism coordination of caesium ions bound in the extracellular vestibule. Comparison of the acid-sensing ion channel structure with the ATP-gated P2X(4) receptor revealed similarity in pore architecture and aqueous vestibules, suggesting that there are unanticipated yet common structural and mechanistic principles (Gonzales et al., 2009). ENaCs have been used to form solid-state nanopores with diameters in the range of 150-200 nm and a thickness <1 micron which could serve as a platform to enhance the throughput of ion-channel characterization using Black Lipid Membranes ().\nThe activity of the epithelial sodium channel (ENaC) is modulated by multiple external factors, including proteases, cations, anions and shear stress. The resolved crystal structure of acid-sensing ion channel 1 (ASIC1), and mutagenesis studies suggest that the large extracellular region is involved in recognizing external signals that regulate channel gating. The thumb domain in the extracellular region of ASIC1 has a cylinder-like structure with a loop at its base that is in proximity to the tract connecting the extracellular region to the transmembrane domains. This loop has been proposed to have a role in transmitting proton-induced conformational changes within the extracellular region to the gate. Shi et al. (2011) examined whether loops at the base of the thumb domains within ENaC subunits have a similar role in transmitting conformational changes induced by external Na+ and shear stress. Mutations at selected sites within this loop in each of the subunits altered channel responses to both external Na+ and shear stress. The most robust changes were observed at the site adjacent to a conserved Tyr residue. In the context of channels that have a low open probability due to retention of an inhibitory tract, mutations in the loop activated channels in a subunit-specific manner. This loop may have a role in modulating channel gating in response to external stimuli, consistent with the hypothesis that external signals trigger movements within the extracellular regions of ENaC subunits that are transmitted to the channel gate (Shi et al., 2011).\nAs noted above, epithelial sodium channels (ENaC) consist of three homologous subunits. Channels composed solely of alpha and beta subunits (αβ-channels) exhibit a very high open probability (Po) and reduced sensitivity to amiloride, in contrast to channels composed of alpha and gamma subunits or of all three subunits (i.e., αγ- and αβγ-channels). A mutant channel comprised of alpha and beta subunits, and a chimeric gamma subunit where the region immediately preceding (beta12 and wrist) and encompassing the second transmembrane domain (TMS2) has been replaced with the corresponding region of the beta subunit (gamma-betaTMS2) and showed characteristics reminiscent of αβ-channels, including a reduced potency of amiloride block and a loss of Na+ self-inhibition (reflecting an increased Po) (Shi and Kleyman 2013). Substitutions at key pore-lining residues of the γβ-TMS2 chimera enhanced the Na+ self-inhibition response, whereas key γ-subunit substitutions reduced the response. Furthermore, multiple sites within the TMS2 domain of the γ-subunit were required to confer high amiloride potency. Thus, pore-lining residues in the γ-subunit are important for proper channel gating and its interaction with amiloride.\nAcid-sensing ion channels (ASICs) are cation selective proton-gated channels expressed in neurons that participate in diverse physiological processes including nociception, synaptic plasticity, learning, and memory. ASIC subunits contain intracellular N- and C- termini, two transmembrane domains that constitute the pore and a large extracellular loop with defined domains termed the finger, beta-ball, thumb, palm, and knuckle. Krauson and Carattino 2016 examined the contribution of the finger, beta-ball and thumb domains to activation and desensitization. The beta-ball and thumb domains reside apart in the resting state, but they become closer to each other in response to extracellular acidification. The thumb domain probably moves upon continuous exposure to an acidic extracellular milieu assisting with the closing of the pore during channel desensitization.\nThe ENaC Family has been reviewed by Hanukoglu and Hanukoglu 2016. ENaC dependent reabsorption of Na in kidney tubules regulates extracellular fluid (ECF) volume and blood pressure by modulating osmolarity. In multi-ciliated cells, ENaC is located in cilia and plays an essential role in the regulation of epithelial surface liquid volume necessary for cilial transport of mucus and gametes in the respiratory and reproductive tracts, respectively. The subunits that form ENaC (named as alpha, beta, gamma and delta, encoded by genes SCNN1A, SCNN1B, SCNN1G, and SCNN1D) are in the ENaC/Degenerin superfamily. The earliest appearance of ENaC orthologs is in the genomes of the most ancient vertebrate taxon, Cyclostomata (jawless vertebrates) including lampreys, followed by earliest representatives of Gnathostomata (jawed vertebrates) including cartilaginous sharks. Among Euteleostomi (bony vertebrates), Actinopterygii (ray finned-fishes) branch has lost ENaC genes. Yet, most animals in the Sarcopterygii (lobe-finned fish) branch including Tetrapoda, amphibians and amniotes (lizards, crocodiles, birds, and mammals), have four ENaC paralogs (Hanukoglu and Hanukoglu 2016).\nThe generalized transport reaction for Na+ channels is:\nNa+ (out) → Na+ (in).\nThat for the degenerins is:\nCation (out) → cation (in).""]"	['<urn:uuid:058c91cd-7163-4c9f-a0bc-d699cd99636d>', '<urn:uuid:0454bf8b-ae24-412e-95a9-7955c6d137fb>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T19:58:41.790432	13	101	2609
11	How do materials change during leather seat making and coffee roasting?	Both materials undergo significant physical transformations. In leather seat making, the leather absorbs water during wet casing, then is molded to shape and dried. After dying, it changes color and requires multiple coats of finish to seal it. In coffee roasting, beans go through several changes - they start as green beans, then turn yellow and brown while absorbing heat (endothermic reaction). During first crack, they release heat and steam (exothermic reaction), double in size, and shed their silver skin. The beans continue to change through an endothermic phase until potentially reaching second crack.	['I’ve had several folks ask the process of hand tooling. Here’s a look at what goes into one of my custom hand tooled leather motorcycle seats. It’s a lengthy process and the reason why most “affordable” commercially made seats are either plain or embossed using a machine.\nLike any project you must first take measurements and create a pattern to cut the leather from. This is followed by careful hand cutting and punching of any necessary mounting holes. In this case, I am making a seat with both a top and bottom so each piece requires a slightly different process.\nFor the top, I wet case the leather overnight. This is the foundation for deep cuts and quality tooling. After the leather has absorbed the water and returned to it’s original color, I transfer my design using a stylus. I typically draw out a design on paper first and then transfer it to the leather which allows for a clean image.\nOnce the image is transferred, I cut the outline using a swivel knife. This project took approximately 6 hours to cut the dragon into the seat. When the outline is complete the tooling begins. This is where you start to get the 3D effect that brings your artwork to life. I use a variety of leather tools for this work including various mauls, bevelers, lifters, shaders, bargrounders, backgrounders and modeling tools. All in all, the tooling took approx. 5 hours (dragon’s have a crazy amount of scales).\nNow is the time to mold both the top and bottom parts of the seat to attain the final shape. This is done by soaking the leather and molding the leather to the seat pan and foam using your hands and a bone folder. Once it’s molded you clamp it down and let it dry overnight. After it dries completely, massage a good coat of neatsfoot oil into the piece to restore the oils lost in the tooling process to avoid future cracking.\nThe two pieces are now ready to be dyed. I used a honey colored oil based dye for this seat and applied it using a small airbrush. I’ve found that you get a more consistent result, better coverage and a greater range of tones with this method. The airbrush is a tool that takes a long time to master and I learn something new each time I use it. The black burst was applied with a brush. Once the dye is applied I let it dry for 24 hours.\nIn order to keep the leather from slipping or bubbling I apply a good coat of adhesive to the underside of the leather and clamp it down for several hours. Since the bottom of this seat required copper rivets I set them after gluing the seat bottom but prior to gluing down the foam and top leather.\nNow is the time to address the seats edge and ready it for lacing. I prefer a smooth, beveled, black edge on seats prior to lacing to help reduce abrasion. The edge is trimmed, sanded, burnished, dyed black, sealed and allowed to dry.\nFor this project, the entire edge is laced. I only use Kangaroo lace which is 10 X stronger than it’s bovine counterpart. This step adds strength and durability along with a classic look but is comes at a price. Lacing for this seat added an additional 6 hours.\nThe final steps are applying an acrylic based finish to seal the leather and keep the dye from bleeding or rubbing off on clothing. I have found that best results are achieved by applying two thin coats with an airbrush vs. a heavy coat with a sponge, cloth or wool. I allow this to dry for several hours and then apply antique paste. To insure that the paste is dry I allow it to sit overnight and then apply two more coats of finish.\nThanks for reading. I hope that you now have a better understanding of hand tooling and seat building.', 'What affects coffee during roasting\nControlling variables during roasting process\nCoffee roasting includes adjusting many variables to create your perfect roast profile. By changing factors including temperature, length of roast, and airflow, you can highlight sweetness, emphasize acidity, or create a well-balanced roast. also drum speed, you can affect the amount of time that coffee beans are exposed to direct heat.Now you can change all those variable, Should you roast a Colombian Nariño the same way you would an Ethiopian Sidamo? Probably not.Producing countries have different climates, soil types, altitudes, density, moisture, – and all that leads to very different coffees. The beans will react differently to heat, plus you will want to accentuate specific characteristics. In other words, you need to roast them differently.So before you create a profile and put your coffee in the roaster, you need as much information as possible about the beans. And today, I’m going to take you through some of the main origin-based variables to consider. At each stage of the coffee supply chain, the moisture content of a green bean must diminish – or the bean might become moldy, defective, and less valuable than before. Ensuring a bean dries correctly is essential in order to optimize its quality potential and minimize the chance of problems.Roasters, near the end of the supply chain, have two tasks when it comes to managing moisture content. On the one hand, they must maintain the lots they store onsite within a narrow moisture range that is acceptable to their quality standards.\nOn the other hand, and in the span of a few minutes, the roaster is responsible for driving the last remaining bits of moisture out of the bean via the application of intense heat and pressure. In these minutes, the coffee is exposed to the most energy it will experience at any point in the coffee supply chain and the roast is set up for either success or failure.It’s easy to see why roasters should care about the moisture content of their coffee. But how useful is a number supplied by an importer, and how can roasters integrate moisture content readings into their craft? I spoke with Fred Seeber of Shore Measuring Systems, a supplier of moisture content meters, about measuring and making sense of moisture content in green coffee.\nThere is no official standard for ideal moisture content in green coffee, although the ICO recommends 11% as a good target. However, it’s commonly accepted that 10-12% is a reasonable range. Anything less than 10% is likely to result in loss of cup quality, while humidity at higher levels begins to create a risk of mold growth. Yet a coffee’s humidity is not static. While the pre-export drying process drastically increases a bean’s stability, changes in moisture content are still possible. Environmental factors, such as being in a particularly humid or hot location, are a common cause of this. Before getting into the technical details of measuring moisture content, it’s worth digging a little deeper into why it’s worth measuring moisture content. Knowing this will help you establish protocols suited to your specific needs.\nFor roasters of a certain scale, it’s simple: you pay for coffee by weight; the more water in that coffee, the more you’ve paid for water which you’re going to burn off anyway. common situation roasters find themselves in: “So, [an importer] sends you a sample, and… it’s showing 11.5% moisture in that sample. Then when your container shows up, that’s 40,000 pounds, and all of the sudden you discover it could be 13% moisture. Well, you just got blanked for two percentage points of water of a commodity that’s four bucks a pound… that’s [a lot] of money.” For the smaller, quality-focused roaster, those kinds of calculations may or may not be relevant. But moisture content still plays an indirect role in a roaster’s costs, regardless of whether or not they’re buying a few containers or a few bags.\nThere is no direct link between a coffee’s quality and its moisture content. A 10% humidity coffee is not necessarily better or worse than a 12% coffee. However, over time, green coffee will gradually lose vibrance. This will eventually result in the dreaded “past crop” flavor, and this process is associated with the drying out of the coffee. Therefore, even for a small roaster, it’s important to keep track of moisture content. If you paid for an 85-point coffee at 12% moisture, by the time it reaches 10% moisture it may be more like an 83-point coffee. Yet, you still paid 85-point prices for it originally.\nBy comparing moisture content loss with quality degradation over time, you can make smart buying and consumption decisions with your green lots. And, when combined with water activity measurements, you can even predict the shelf life of your green coffee. Again, precision here is key: you want to track your coffee through a narrow range of percentage points over a long time frame. Lastly, you may think to yourself that you don’t need to measure moisture content yourself, since your importer supplies those numbers already. Fred cautions against this thinking. He points out that coffee is shipped on water and that ports can often be warm and humid, which will affect moisture readings. So, if you’re a roaster in a dry part of the United States but your importer is located in New Orleans or Houston, and is taking moisture readings from lots right as they arrive, those numbers might not be applicable to you by the time your coffee arrives at your facility. Elevation, or altitude, is of immense importance for coffee roasting – but what we’re really talking about is density. When coffee is grown at cooler temperatures (which, most of the time, means higher elevation), the cherries ripen slower and so develop more sugars. This leads to more complex sweetness, but also to harder, denser beans. When you have beans of different densities, they also react differently to the heat. Soft, low-density beans tend to have more air pockets inside them, which can slow down heat transference. To avoid scorching the outsides of the beans, you should use a lower initial charge temperature. We also recommends extending the length of the roast for these coffees. Knowing what altitude your coffee is grown at, how far it is from the equator, and the temperature on the farm will help you to anticipate the density. When roasting, it’s important to consider not just the structure of the bean, but also the flavor of it. And this can vary greatly. “We will never have an Ethiopian with the same type of acidity like that Kenya AA Kamwangi we once had,” Tom tells me, “and it will be very difficult to find a Colombia with the stone fruit, tea-like flavors of the Yirgacheffe coffees.”\nyou can expect well-balanced coffees from the Americas, with more chocolate and hazelnut notes appearing in Brazil. In East Africa, coffees tend to be clean, juicy, and fruity. Some regions lean more towards sweetness (like Burundi), while others are more acidic (like Kenya). Indonesia is often known for its heavy body and earthy tones. Yet there are so many flavor variations within one region, as a result of micro climates, terroir, varieties, production and processing methods, and more. Sulawesi, Indonesia is famous for its spice notes, while Bali has a more citric profile. A Panamanian Geisha will taste different from a Panamanian Bourbon. Brazil is so large, you can fit much of Europe in it – and it has a wide variety of profiles to match. And as Tom points out, some countries have multiple harvest seasons. it’s the roaster’s job to preserve what makes an origin special and “let the coffee speak”. Knowing the profile of the coffee origin will help you anticipate which flavors will be most prominent – and how you can emphasize them. roast graph data into two types of curves: control curves and reading curves. Control curves are variables that you directly control during the roast, such as the heat settings, airflow, and gas flow. Reading curves are temperature readings. Since the variables are constantly changing, they are recorded as line graphs.\nBut what reading curves do you need to know? the key ones are bean temperature, air/environment temperature, and rate of rise curves – although you can also measure bean color, air, and gas pressure for even greater insights. Denser, higher-altitude coffees are associated with greater acidity, and you’ll often hear this described in terms of fruit notes – mandarin, grapefruit, plum, blueberry, and so on. This is a highly prized trait, and if you’re roasting a coffee that has this quality, you may want to accentuate it. (Bear in mind, however, that while acidity can be good, underdeveloped and sour notes are not. There is a fine line.) the more acidity and fruitiness you will throw away. A faster Rate of Rise (RoR) is also recommended by many roasters for emphasizing acidity. On the other hand, if you want more sweetness – say you have a natural Bourbon from Burundi – then Willem Boot, CEO of Boot Coffee, recommends opting for a lower RoR. Sweet Maria’s also experimented with stretching out the drying phase of the roast, and found that it could highlight this quality. as for body, stretching out first crack could open up a more syrupy mouthfeel in a coffee. It’s important to remember that the qualities you want to highlight will all depend on the coffee itself, and its unique, overall profile. Roasting is a complex skill; there are no simple rules. These guidelines are just starting points for creating your roast profiles. Knowing the altitude, temperature, terroir, and origin profile is a great start to creating a roast profile for a coffee. “It’s about a commitment to get to know the origin and bring the best to the surface,”\nBut it’s only a start.\nBean temperature(the blue line)\nThe bean temperature curve will look a bit like a check mark; once it starts going up (something called the turning point – more on that to come!) it should always continue going up. If not, you risk stalling your coffee and developing bread-like, doughy flavours\nROR(other blue line which is vice vers)\nThe rate of rise curve is linked to bean temperature, but there’s a subtle difference: it measures the rate at which bean temperature changes. This will give you far earlier indications of temperature changes and, in turn, allow you more control over the roast. It has a very different shape to the bean temperature curve, rising sharply from zero shortly after the turning point.\nAir temperature(RED line)\nAir temperature is variable measures the environment inside the drum. It’s useful to know because much of the heat transfer in coffee roasting is via air. This line will follow a similar shape to the bean temperature curve.\nKey Stages on Your Roast Graph\nNow we know what the roast graph measures, you can start reading and interpreting these lines. To do so, you want to pay attention to several key points on the graph: charge temp, turning point, first crack, and end temp.\nThis is the temperature of your drum just before you add the coffee. By manipulating this, you can speed up or slow down the rate of rise and, in turn, choose how much acidity to accentuate. You should also pay attention to bean density and processing method when selecting this.\nAs you add the cold beans to the roaster, the heat inside the machine will dramatically fall before starting to rise again. The point at which it begins to rise is called turning point.\nOne of coffee roasting most famous moments is first crack. First crack signals that the beans are almost ready. As the beans expand and moisture evaporates, steam develops inside the beans. This steam then forms pressure that cracks the beans open.\nFirst crack it’s a moment that has been given almost mythical status in coffee roasting – and it deserves it. A key stage in any roast, understanding it will give you insight into how flavors and aromas are developing.\nAs the name suggests, this is the temperature at the end of your roast.\nBy understanding what’s going on inside the roaster at these key points, you’ll be able to start evaluating the impact of them on your beans. For example, by manipulating charge temp you can speed up or slow down your roast. The duration of first crack can affect body. Roasting graphs may, at first, be challenging. There’s a lot of data to collect and understand. However, as you start to work with air temperature, rate of rise, first crack, and more, you will begin to gain real mastery over how your coffee beans develop during roasting. So don’t be intimidated by these charts – start recording those temperatures and see how it helps you as a roaster. As heat is applied to the coffee beans, they go through endothermic and exothermic reactions. Up until first crack, the beans absorb the heat (an endothermic reaction). The moisture dissipates and the color changes from green to yellow and then brown. The aroma will be cereal-like: think toast, popcorn, or grass. As for first crack, this is a brief exothermic reaction: the beans release heat (energy) in the form of that steam we mentioned above, along with carbon dioxide. The bean will have doubled in size and shed the majority of their silver skin, but oils won’t yet be present. After first crack, it switches back to an endothermic reaction until second crack, the final exothermic reaction (if you choose to roast your beans that far).']	['<urn:uuid:4ffb5b51-b170-4d03-beff-a8b9ee1665a1>', '<urn:uuid:7823bc81-5b4b-4fbd-964d-bca4d098a9dd>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T19:58:41.790432	11	94	2950
12	What happened to Mercedes Sosa when her home country was taken over by a military government?	Mercedes Sosa, an Argentinian folk singer, was forced into self-exile when her home country was taken over by a military dictatorship. Despite this, she continued to dedicate her life to being the voice of the voiceless through her activism and music.	['13 Kids Books That Celebrate Latinx Stories\nEach year, we observe National Hispanic Heritage Month from September 15 to October 15 by celebrating the history, the culture, and the myriad contributions of those from Spain, Mexico, Puerto Rico, Argentina, and more. This year, include your child in the festivities by adding some of these books that celebrate Latinx stories into your storytime rotation! From board books for babies to award-winning picture books for big kids, there’s no doubt that you’ll find so many libros that your niños will adore!\nWritten and illustrated by Paloma Valdivia\nIf you love the classic children’s book Runaway Bunny, you’ll for-sure adore this beautiful bilingual tale of a mama and her tot imagining themselves as a variety of animals and showing how precious their bond is. This picture book’s unique illustrations are charming and unforgettable, while the words are sweet and poetic, making Nosotros Means Us a just-right read for little ones up to 5 years old—and a fantastic way to start building up your bub’s bilingual vocabulary.\nWritten by Leonarda Carranza, illustrated by Rafael Mayani\nThis poignant, empowering, and important children’s book is a must-have addition to any child’s home library. Author Leonarda Carranza skillfully addresses racism and microaggressions through the experiences of a young child and her granny (abuelita) who are running errands. The intergenerational pair shows bravery, grace, and empowerment—while teaching readers that treating others poorly because of their differences is wrong. There’s little wonder why Abuelita and Me has snagged numerous awards, including being named one of the best books of 2022 by Kirkus and the International Latino Book Awards.\nWritten by Sonia Sotomayor, illustrated by Rafael López\nWhat do you get when you cross a Latina Supreme Court Justice with an award-winning Latino artist? The answer: Justice Sonia Sotomayor’s bestselling children’s book Just Ask! This Amazon Teachers’ Pick celebrates how the differences and varying abilities among all of us should be viewed as special powers, not detriments. Through fresh and spunky illustrations, readers witness children working together to build a community garden, each asking poignant questions along the way.\nWritten by Matt de la Peña, illustrated by Loren Long\nGet the tissues ready! This bestselling and award-winning picture book masterfully hugs your heart, stirring up all the love inside with its poetic prose and gorgeous illustrations. Here, readers are given magnificent glimpses of the many ways we experience love throughout childhood and beyond. (Love is the sound of the first voices we ever hear; it is the color of the night sky over a happy home; it is the echo of summer laughter.) This tender tale is sure to be a new classic. (You can also pick up the Spanish language version Amor.)\nWritten by Patty Rodriguez & Ariana Stein, illustrated by Citlali Reyes\nPart of the Lil’ Libros bilingual board book series, Counting With Frida introduces little ones to the iconic Mexican painter Frida Kahlo…all while teaching them how to count to 10 in English and Spanish. Simple and sweet, each of the lively illustrations lean on Kahlo’s famous style and known works. Bonus: The length of this board book is perfect for a baby’s limited attention span! (Find more number books for babies and toddlers.)\nWritten and illustrated by Carlos Aponte\nArmed with an old photo and a full heart, a young boy named Carlitos sets on a journey through the heart of Old San Juan to find his papi. Across the Bay is not your typical wrapped-up-in-a-bow children’s book. Instead, it gracefully and honestly illustrates the true meaning of family and home, which is based on author/illustrator Carlos Aponte’s own childhood in Puerto Rico. This enchanting picture book is perfect for the 3- to 7-year-old set.\nWritten and illustrated by Melisa Fernández Nitscheoung\nInvite your child into the world of Argentinian folk singer extraordinaire Mercedes Sosa, who turned her artistry into activism with songs that spoke to what it truly means to battle injustices in Latin America. Her music was so powerful, in fact, that she was forced into self-exile when her home country was taken over by a military dictatorship. But that didn’t stop Sosa from dedicating her life to being the voice of the voiceless. This bright and breathtaking book is designed to inspire and empower young readers (age 4 and up) across the globe…including yours!\nWritten by Yamile Saied Méndez, illustrated by Jaime Kim\nIt’s easy to see why Where Are You From? has garnered so many prestigious book awards! Not only are the illustrations captivating and heart-warming, the story is a lyrical powerhouse. Here, a little girl who’s always fielding the title’s question, but doesn’t have a simple answer, learns all about self-acceptance with the help of her beloved abuelo (grandfather). This is a wonderful story for children 4 and up to enjoy, but especially for those who’ve ever felt they didn’t belong. The Spanish-language edition, De Dónde Eres?, is also available.\nWritten by Carmen Agra Deedy, illustrated by Eugene Yelchin\nDon’t let this playful story about a sing-songy bird fool you! The Rooster Who Would Not Be Quiet! is a fantastic picture book that skillfully teaches children about human rights and the consequences of oppression. Geared toward kindergarteners through second graders, this Amazon Teachers’ Pick is set in a happy, but noisy village of La Paz where the mayor outlaws loud singing in the street…then at home…then everywhere! But Rooster just won’t stop and when the mayor threatens the town’s favorite bird, a singing revolution begins!\nWritten and illustrated by Andrea Cáceres\nWhen Aurora arrived in America from Venezuela, she learned to speak English…but her beloved pup, Nena, did not. That means, Nena doesn’t understand sit, or wait, or treat, but she definitely knows siéntate, espera, and postre! This picture book is a sweet vehicle to explore code switching, learning a second language, and having to translate for others. My Dog Just Speaks Spanish is also a fun tool to help young readers practice reading—and learn English or Spanish terms!\nWritten by Alyssa Reynoso-Morris, illustrated by Mariyah Rahman\nAbuela says, “plátanos are love.” I thought they were food. But Abuela says they feed us in more ways than one. This is more than just a delicious story about cooking plátanos (plantains), it’s about tradition, community, and most importantly intergenerational love between a doting abuela and her granddaughters. Written for children 4 and up, this joyous and lyrical picture book is sprinkled with Spanish words and heaps of warmth.\nWritten by David Bowles, illustrated by, Erika Meza\nAvailable in English and Spanish, this multi-award-winning picture book tells the heartfelt tale of a father and son living on the U.S./Mexico border, and their weekend tradition of going to their favorite places on The Other Side. The pair visit a cherished restaurant, devour sweet treats…and drop off much-needed supplies to friends who are seeking asylum. The luminous watercolors and gentle words weave weighty realities into the everyday…and come together perfectly for children aged 4 to 8.\nWritten by Naibe Reynoso, illustrated by Jone Leal\nSpecially designed for the youngest “readers,” this is the bilingual board book version of the original, Be Bold, Be Brave: 11 Latinas Who Made U.S. History. Within the pages, your little one will be introduced to notable Latinas such as Supreme Court Justice Sonia Sotomayor, astronaut Ellen Ochoa, famed singer Selena, Oscar-winner Rita Moreno, and seven more. This is the ideal book to start introducing your child to the countless Latinas who’ve made a difference in the world!\nMore Children’s Books\n- The Best Spanish-English Bilingual Books for Babies\n- Books That Teach Little Kids About Race, Justice, and Equality\n- Kids’ Books That Celebrate Amazing Women in History\n- Children’s Books That Teach Empathy and Kindness\nHave questions about a Happiest Baby product? Our consultants would be happy to help! Submit your questions here.\nDisclaimer: The information on our site is NOT medical advice for any specific person or condition. It is only meant as general information. If you have any medical questions and concerns about your child or yourself, please contact your health provider.']	['<urn:uuid:c8051784-f4a6-40c0-b1de-9c77315cc7a7>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	16	41	1345
13	what problem does socrates solve with his doctrine of recollection about learning truth	Socrates solves the difficulty of how one can seek truth through his doctrine of Recollection. The problem is that a person cannot seek what they already know (since they know it), and cannot seek what they don't know (since they don't know what to seek for). The doctrine of Recollection interprets all learning as remembering - an ignorant person just needs a reminder to become conscious of what they already know within themselves.	"['Philosophical Fragments (Danish: Philosophiske Smuler eller En Smule Philosophi) was a Christian philosophic work written by Danish philosopher Søren Kierkegaard in 1844. It is a discussion of ""how"" a person arrives at Absolute Truth vs historical or philosophical truths.\n- Propositio: The question is asked in ignorance, by one who does not even know what can have led him to ask it.\nA Project of ThoughtEdit\n- How far does the Truth admit of being learned? (…)For what a man knows he cannot seek, since he knows it; and what he does not know he cannot seek, since he does not even know for what to seek. Socrates thinks the difficulty through in the doctrine of Recollection, by which all learning and inquiry is interpreted as a kind of remembering; one who is ignorant needs only a reminder to help him come to himself in the consciousness of what he knows. Thus the Truth is not introduced into the individual from without, but was within him.\n- p. 8\nThe God as Teacher and Saviour: An Essay of the ImaginationEdit\n- The Moment makes its appearance when an eternal resolve comes into relation with an incommensurable occasion. Unless this is realized I we shall be thrown back on Socrates, and shall then have neither the God as Teacher, nor an Eternal Purpose, nor the Moment. Moved by love, the God is thus eternally resolved to reveal himself. But as love is the motive so love must also be the end; for it would be a contradiction for the God to have a motive and an end which did not correspond. His love is a love of the learner, and his aim is to win him.\n- p. 20\nThe Absolute Paradox: A Metaphysical CrotchetEdit\n- I always reason from existence, not toward existence, whether I move in the sphere of palpable sensible fact or in the realm of thought. I do not for example prove that a stone exists, but that some existing thing is a stone. The procedure in a court of justice does not prove that a criminal exists, but that the accused, whose existence is given, is a criminal.\n- p. 31\nThe Case of the Contemporary DiscipleEdit\n- The God has thus made his appearance as Teacher (for we now resume our story), and has assumed the form of a servant. To send another in his place, one high in his confidence, could not satisfy him; just as it could not satisfy the noble king to send in his stead even the most trusted man in his kingdom. But the God had also another reason; for between man and man the Socratic relationship is the highest and truest. If the God had not come himself, all the relations would have remained on the Socratic level; we would not have had the Moment, and we would have lost the Paradox. The God’s servant-form however is not a mere disguise, but is actual; it is not a parastatic body but an actual body; and from the hour that in the omnipotent purpose of his omnipotent love the God become a servant, he has so to speak imprisoned himself in his resolve, and is now bound to go on (to speak foolishly) whether it pleases him or no.\n- p. 42\n- If this fact has been naturalized, birth is no longer merely birth, but is at the same time a new birth, so that one who has never before been in existence is born anew -- in being born the first time. In the individual life the hypothesis of naturalization is expressed in the principle that the individual is born with faith; in the life of the race it must be expressed in the proposition that the human race, after the introduction of this fact, has become an entirely different race, though determined in continuity with the first. In that event the race ought to adopt a new name; for there is nothing inhuman about faith as we have proposed to conceive it, as a birth within a birth (the new birth); but if it were as the proposed objection would conceive it, it would be a fabulous monstrosity.\n- p. 71-72\n- A good man wishes to serve humanity by presenting a probability-proof, so as to help it accept the improbable. He is successful beyond all measure; deeply moved, he receives congratulations and addresses of thanksgiving, not only from the quality, who know\n- p. 82\n- The projected hypothesis indisputably makes an advance upon Socrates, which is apparent at every point. Whether it is therefore more true than the Socratic doctrine is an entirely different question, which cannot be decided in the same breath, since we have here assumed a new organ: Faith; a new presupposition: the consciousness of Sin; a new decision: the Moment; and a new Teacher: the God in Time. Without these I certainly never would have dared present myself for inspection before that master of Irony, admired through the centuries, whom I approach with a palpitating enthusiasm that yields to none. But to make an advance upon Socrates and yet say essentially the same things as he, only not nearly so well -- that at least is not Socratic.\n- p. 82']"	['<urn:uuid:7b631fc4-7d94-49dc-9f2a-6d0e808317f8>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T19:58:41.790432	13	73	880
14	regular maintenance procedures reduce risk factory equipment insurance rates	Regular maintenance procedures that can help reduce insurance rates include bundling wires together, keeping floors and working areas tidy, and replacing bad bearings and belts. Cleanliness and maintenance are considered the most important preventive measures, even more than installing heat warning devices, spark detectors, and flame suppressant systems. These diligent maintenance practices can lead to lower premiums by reducing accidents that may cause harm to employees and facilities.	['Three Secrets to Lowering Insurance Premiums\nAgribusiness insurance experts reveal top money-saving strategies\nMost insurance agencies offer credits to lower insurance premiums based on a variety of factors, such as driving records, workers’ compensation history and recorded training programs. Three agribusiness insurance experts revealed to Feed & Grain their top tips for saving money on insurance costs every month.\n- Raise your deductible\nOne way to control premiums is to take the highest deductible that the insured is comfortable handling because the higher the deductible, the lower the cost.\nAccording to John Quirk, director of marketing for Rural Mutual Insurance based in Madison, WI, “Depending on the type of buildings the insured has they may be able to save some premium by electing to go with an actual cash value settlement rather than a replacement cost settlement. Some companies may also allow the insured to exclude coverage on buildings that they may elect not to rebuild in event of a major loss.\nThis concept applies to vehicles as well. The insured may even elect not to have physical damage coverage on some or all of their vehicles in order to save premium dollars.\n- Create a safety culture\nAccording to Dick Anderson, property/casualty products and client service manager of Ag States Group, Inver Grove Heights, MN, having a well-documented safety program in place is the best way to reap insurance savings.\n“It’s more than ‘do they have proper OSHA manuals and do they document training?’” Anderson says. “You can see it by walking through the facility if things are neat and orderly, fire extinguishers are charged and properly positioned. We check if employees are wearing goggles and gloves when working with anhydrous. We talk with employees about how they do their job, and that’s why it’s so important to maintain a safety culture throughout all levels of the business and reinforce that with employees.”\nAnderson adds, “It’s beneficial to dedicate people within the organization to be safety directors or contact people. Investigate accidents — not only what caused them, but what could be done in the future to prevent accidents like that. This all helps build a safety culture, which can lower premiums.”\n- Take preventive maintenance measures\nInsuring a feed and grain facility is not like insuring a normal building. An office building, for example, has a very low rate because it’s unlikely an explosion or fire will occur. But in this industry, there are a lot of hazards that have potential to cause disaster. But regularly conducting facility maintenance will help avoid those disasters and can help lower your insurance premiums.\nAccording to Steven Simmons, risk management manager for Nationwide Agribusiness of Columbus, OH, installing heat warning devices, spark detectors and flame suppressant systems are all recommended prevention mechanisms, but the most important thing is cleanliness and maintenance.\n“Bundle wires together, keep floors and working areas tidy, replace bad bearings and betls,” says Simmons. “Diligent maintenance can lead to lower premiums by reducing accidents that may cause harm to your employees and the facility.”']	['<urn:uuid:8f06cb4a-8fe9-4a9f-9bcb-ef75e1367877>']	open-ended	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	9	68	506
15	traditional percussion instruments ching gong atsimevu drum size dimensions comparison	The ching and atsimevu differ significantly in their dimensions. The ching is a brass gong ranging from approximately 35-40 cm (13-16 in) in diameter, with an inward-sloping rim of approximately 8-10 cm (3-5 in) deep and is approximately 3 mm thick. In contrast, the atsimevu drum is much larger, ranging from four and a half to six feet tall (approximately 137-183 cm), with a head ranging from nine inches to one foot in diameter (approximately 23-30.5 cm). The atsimevu is approximately 15 inches around the middle, with an opening about nine inches in diameter.	"['The kkwaenggwari is a small, brass gong that has a diameter of approximately 19–22 cm (7–8 in.), and a rim of approximately 3–4 cm (1 in.). It is played with a wooden mallet with a bare wooden disc attached at the tip. The length of the mallet may vary, depending on the purpose of the music, but it is thinner than the mallet used for the ching (large gong). The kkwaenggwari is sometimes known as the sogŭm, literally “small metal”. Nowadays, the kkwaenggwari is made of a combination of copper and zinc, its tone much clearer when the percentage of copper is higher (60–70%). Kkwaenggwaris with a larger percentage of zinc produce a lower, darker tone that does not resonate as well.\nThe changgo is an hourglass drum that is the most widely played of all Korean instruments, and most basic in the sense that it is the one percussion instrument on which a complete changdan (rhythmic cycle) is played out. Its body is usually made of paulownia wood (odong namu), although pottery, metal, ceramic, and plastic bodies also exist, and its heads are made of animal skin. In earlier times, the hourglass-shaped body of the drum was sometimes made by conjoining two or three separated pieces (bowl-shaped parts connected in the middle by a third module), but these days, the body is made of one whole piece. The skins of the changgo are attached to its hollow body by a rope that is looped alternately through the eight metal hooks around the rim of either head. The tension of the drumheads of the changgo can be adjusted by moving leather straps that encase the ensuing V-shape laces. The gungpyon (or pukpyon), usually placed on the left side when the changgo sits horizontally, is covered with cowhide or deer hide, producing a low tone. The chaepyon (right side), is covered with dog hide or horsehide, and usually produces a higher tone. The changgo has been standardized into two types: larger, heavier ones used in court and orchestral music, and smaller, lighter ones used in the genre of p’ungmul nori (farmer’s band music, sometimes known as nongak). Larger changgos may measure over 60 cm (23–24 in.) in length and have a diameter of over 30 cm (11–12 in.); smaller ones are approximately one third less. Changgos used in court music were usually painted red, the royal color, while changgos used in folk music are the natural wood color as they are rarely painted (except for oil or varnish).\nThe word puk in Korean is the generic term for the word “drum,” and there are several kinds of puks in Korean music. However, the most common are the p’ungmul puk used in farmer’s band music, and the sori puk used to accompany p’ansori singing (Korean traditional narrative storytelling). The puk is a shallow, double-headed barrel drum with a wooden body made of paulownia or poplar, and heads made of deer hide, horsehide, or cowhide, although cowhide is most common. The size of the puk varies from region to region and according to purpose (sori puks may by larger than the puks used in farmer’s band music), but the heads generally range from 35–40 cm in diameter (13–15 in.). They are approximately 20–25 cm deep (7–9 in.). The skins of p’ansori puks are permanently nailed around the body of the drum, while the skins of the p’ungmul puks are attached to each other by lacing leather strings across the body of the drum.\nThe ching is the larger of two gongs used in Korean percussion music. It is made of brass and ranges in size from approximately 35–40 cm (13–16 in) in diameter, with an inward-sloping rim of approximately 8–10 cm (3–5 in) deep. It is approximately 3 mm thick. The size of the ching varies according to its usage: the ching used in p’ungmul nori (farmer’s band music) is usually smaller than the ching used in court, Buddhist, or ritual music, in which cases it is sometimes referred to as the taegŭm (literally, “large metal”), as opposed to the kkwaenggwari, the small gong, which is sometimes known as the sogŭm (“small metal”).\nContent adapted from the Virtual Instrument Museum of Wesleyan University of Middletown, Connecticut.', 'Geographic Region: Africa\nCountry of origin: Ghana\nClimatic type: Tropical\nSvH No.: 211.2\nThe atsimevu is the largest of the Eve drums. It is constructed of wood, either as one solid carved piece or, as is more common , of wooden slats bound by metal rings. The wood used for the barrel drums, a light mahogany native to West Africa, is known as logo in Eve. (Galeota 1985: 59) The drums range in size from four and one half to six feet tall (Galeota 1985: 04) with a head ranging in size from nine inches to one foot in diameter. (Pantaleoni 1971: 46) The drum is approximately 15 inches around the middle, and the opening is about nine inches in diameter. (Ladzekpo)\nThe name atsimevu comes from a description of the way the drum is supported by the vudetsi. (Ladzekpo) Modern construction of this drum and others like it is similar to the construction of wooden barrels. In the past these drums were carved from solid tree trunks. This still happens at times, but the scarcity of large trees makes it more efficient to use wooden slats bound by metal rings. (Pantaleoni 1971: 46) Early drums of this type were actually made from used commercial barrels. Laurance Mensaga Nutakor, a drum maker from Abor, born 1911, is reputed to be the first to make a barrel drum from raw materials. (Galeota 1985: 30)\nThe atsimevu is tuned low in relation to the ensemble. Proper tuning is achieved one of two ways. In dry conditions, the drum is sometimes turned upside down and filled with water, allowing the planks to absorb the moisture. The expansion of the wood tightens the pegs in their sockets, thus pulling the drum head taut. (Galeota 1985: 03) A more common technique is to drive the wooden pegs farther into their sockets, achieving the same purpose.\nSound is created on the atsimevu by striking the drum with the full hand, a stick, the fingers or combinations of the above. (Ladzekpo) Additional sounds are produced by damping the membrane to affect resonance and striking differing positions on the drum head. (Ladzekpo) Leaning the drum on a stand creates a comfortable angle at which to play, as well as leaving the bottom open to provide resonance. The sticks used are approximately ten inches long and made of thick wood. (Galeota 1985: 03) The eight different strokes played on the atsimevu are referred to by the corresponding vocable syllables de, te, ge, tsi, to, ka, dza, and dzi. (Ladzekpo) Although the atsimevu in the Eve drum ensemble always plays the leading role, the style of lead drumming varies throughout the repertoire. When accompanying singing, the atsimevu outlines the song\'s melodic and rhythmic structure. (Galeota 1985: 10) When played for dancing, the atsimevu leads the ensemble by playing rhythms that signal dance movements.\nThe music of the Eve drum ensemble is transmitted almost exclusively aurally. The vocables, however, are similar to notation in that they serve as a tool for learning and remembering rhythms and patterns.\nThe atsimevu plays the leading role in the drum ensemble of the Eve. Its most important function is to interact with the dancers while setting the pace of the music. (Locke 1979: 501) While engaging in a musical dialogue with the supporting drums sogo and kidi, the atsimevu plays rhythms that signal dance movements.\nGaleota, Joseph. 1985. ""Drum Making Among the Southern Eve People of Ghana and Togo."" M.A. thesis, Wesleyan University.\nLast Modified: 14-Sep-2010TOP']"	['<urn:uuid:6aff6675-2504-4ca2-b08e-b9d40c7ffcd8>', '<urn:uuid:0b763f3f-c87d-42da-ab75-02ccbd37642c>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-12T19:58:41.790432	10	94	1284
16	What do archaeological findings reveal about Korean soybean history?	The oldest preserved soybeans found in archaeological sites in Korea dated about 1000 BCE. Specifically, radiocarbon dating of soybean samples recovered through flotation during excavations at the Early Mumun period Okbang site indicated soybean was cultivated as a food crop in around 1000-900 BCE.	['A research team has found evidence suggesting soybeans were cultivated in Japan about 5,000 years ago.\nThe team, which includes officials of Yamanashi Prefectural Museum of Art, announced recently traces of the nation’s oldest cultivated species of soybeans had been found in a clay pot excavated in Hokuto, Yamanashi Prefecture.\nThe pot, which dates back to the Jomon period (ca 10,000 B.C.-ca 300 B.C.), had a cavity on the tip of a fractured handle in which the researchers believe a soybean became embedded.\nTraces of soybeans have previously been found in late Jomon period pottery dating back about 3,500 years that has been excavated in Kumamoto and other prefectures in Kyushu.\nHowever, the pot excavated from Sakenomiba remains in Nagasakacho, Hokuto, in 1995, dates back a further 1,500 years.\n“This finding shows that people living in the Jomon period had access to a greater variety of foods, by cultivating plants as well as hunting and foraging, than previously believed,” one researcher said.\nA copy of the soybean trace was created using silicon resin through the so-called replica SEM method, after being examined with an electron microscope.\nThe researchers said the soybean was 11.9 millimeters long, 5.7 millimeters wide and 3.7 millimeters thick. Judging from the size and shape of the soybean, they believe it was a cultivated species.\n(Yomiuri, Oct. 22, 2007)\nAccording to Origin, History and Uses of Soybean (Glycine Max) by Lance Gibson and Garren Benson, “The first domestication of soybean has been traced to the eastern half of North China in the eleventh century B.C. or perhaps a bit earlier.”\nAccording to the ancient Chinese myth, in 2853 BCE, the legendary Emperor Shennong of China declared the five sacred plants to be: rice, wheat, barley, millet…and soybeans.\nHowever, long before written records, soybeans were a crucial crop in eastern Asia, and soybean in China was used as a food and a component of drugs, 5,000 years ago. The wild ancestor of the soybean is Glycine soja (previously called G. ussuriensis), a legume native to central China. Wild-size soybeans have been found in the Yellow River basin of China. Today in China, soybean production is concentrated in Manchuria (Heilungkiang, Kirin, Liaoning) and Shantung. Soybeans are also grown extensively in the provinces of Anhwei, Honan, Hopei, Kansu, Kiangsu, Shansi, Shensi and Szechwan.\nGeneral literature and sources state that soybeans were introduced into several countries including Japan from about the first century CE to the Age of Discovery (15-16th century), due to the establishment of sea and land trade routes. The earliest Japanese textual reference to the soybean is in the classic Kojiki (Records of Ancient Matters), which was completed in 712 BC. Until the finds mentioned in the above news article, the best current evidence on the Japanese Archipelago suggested soybean cultivation occurred in the early Yayoi period. In the light of the new finds, soybean cultivation may have arrived in Japan earlier during the Jomon period or may even have been domesticated locally independent of Chinese sources, the finds are even earlier than those of Korea. Prior to fermented products such as soy sauce, tempeh, natto, and miso, soy was considered sacred for its use in crop rotation as a method of fixing nitrogen.\nThe oldest preserved soybeans found in archaeological sites in Korea dated about 1000 BCE, radiocarbon dating of soybean samples recovered through flotation during excavations at the Early Mumun period Okbang site in Korea indicated soybean was cultivated as a food crop in around 1000–900 BCE.\n“This reports details on the morphometrics of more than 900 archaeological soybeans across 22 sites in (northern) China, South Korea and Japan, including 7 directly dated by AMS. This provides the first really good archaeological dataset for making inferences about soybean domestication, and includes a summary of the soybean metrics from Wangchenggang published in a Chinese monograph in 2007 by Zhao Zhijun (and in more detail his 2010 book collection) , previously suggested to indicate some size increase by the Longshan period (i.e. 2500-1900 BC). The data reported here suggests large, truly domesticated soybeans present in middle Jomon Japan Shimoyakebe (near Tokyo), from the Third Millennium BC. These are significantly larger than those from the Yellow River valley of similar age, the Longshan period, which are also probably enlarged by selection under cultivation (at least the population from Wangchengang appear enlarged while some other Longshan samples still fall in the wild type range). In Korea a measured population form the Middle Chulmun is perhaps marginally enlarged while those of the later Mumum had clearly undergone selection for size increase. The overall impression that selection for seed size increase in soybeans was a protracted process and one that was uneven in different regions, and it may be that in some areas wild population continued to be exploited or proximity of wild populations and early cultivation methods did not lends themselves to selecting for larger seed size. This then complements data emerging from several crops for a protracted process of evolution of domestication traits (see previous, for example my articles in Annals of Botany or Evolution, and for a discussion of protracted domestication processes in the New World tropics see the recent Current Anthropology by Piperno). It also strongly points to a Jomon domestication independent from that in northern China, and one in which seed size evolved more quickly.”\nSources and readings:\n“Soybean find suggests 5,000 year cultivation”, Yomiuri, Oct. 22, 2007\nThe Archaeobotanist | “Soybean archaeobotany: multiple origins and not coincident with cereals” (6 Dec, 2011)\nM. M. Lager, The Useful Soybean (1945); J. P. Houck et al., Soybeans and Their Products (1972).\nA Special Report on History of Soybeans and Soyfoods in China and Around the World | A Chapter from the Unpublished Manuscript, History of Soybeans and Soyfoods: 1100 B.C. to the 1980s by William Shurtleff and Akiko Aoyagi Retr. from Soyinfocenter.com\nOn the domestication of the Soybean (National Soybean Research Laboratory)']	['<urn:uuid:d55e3f1e-d3c1-4cc4-aadb-bfa4b64303db>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	9	44	984
17	how narrow people protect their self worth	Narrow people protect their self-worth by exclusion and denial - they retract their sense of self from regions they cannot securely possess and look with negative feelings or hate towards people who don't resemble them or over whom they have no influence.	"['The problem of self-esteem\nAs presented in\nInventive personality type, Annie Reich defined self-esteem as ""the expression of discrepancy or harmony between self-representation and the wishful concept of the self."" William James gave a similar definition in Chapter 10 of his The Principles of Psychology, and formulated it as, ""Self-esteem = Success / Pretensions.""\nJames (1890) viewed self-esteem as an evaluative process; he argued that self-esteem, at its simplest, could be measured as the ratio of a person\'s successes to his or her pretensions. Pretensions are viewed as goals, purposes, or aims, whereas successes constitute the perception of the attainment of those goals. As people attain more of their pretensions, the ratio grows larger and self-esteem becomes correspondingly stronger. Pretensions also add a vulnerability component to self-esteem in that these are the areas where the individual is proposed to be most competent. If he or she comes up short in the percerption of goal-attainment, or in comparison with others in the same pretension arena, self-esteem suffers. A realization of shortcomings in an area that was not important to the individual, however, would not result in a devaluation of personal worth (Bernet et al., 1993, pg. 141).\nThe following is an excerpt from James\'s discussion of the problem of self-esteem:\nRivalry and Conflict of the Different Selves.\nWith most objects of desire, physical nature restricts our choice to but\none of many represented goods, and even so it is here. I am often confronted\nby the necessity of standing by one of my empirical selves and relinquishing\nthe rest. Not that I would not, if I could, be both handsome and fat and\nwell dressed, and a great athlete, and make a million a year, be a wit,\na bon-vivant, and a lady-killer, as well as a philosopher; a\nphilanthropist, statesman, warrior, and African explorer, as well as a\n\'tone-poet\' and saint. But the thing is simply impossible. The millionaire\'s\nwork would run counter to the saint\'s; the bon-vivant and the philanthropist\nwould trip each other up; the philosopher and the lady-killer could not\nwell keep house in the same [p. 310] tenement of clay. Such different characters\nmay conceivably at the outset of life be alike possible to a man.\nBut to make any one of them actual, the rest must more or less be suppressed.\nSo the seeker of his truest, strongest, deepest self must review the list\ncarefully, and pick out the one on which to stake his salvation. All other\nselves thereupon become unreal, but the fortunes of this self are real.\nIts failures are real failures, its triumphs real triumphs, carrying shame\nand gladness with them. This is as strong an example as there is of that\nselective industry of the mind on which I insisted some pages back (p.\n284 ff.). Our thought, incessantly deciding, among many things of a kind,\nwhich ones for it shall be realities, here chooses one of many possible\nselves or characters, and forthwith reckons it no shame to fail in any\nof those not adopted expressly as its own.\nI, who for the time have staked my all on being a psychologist, am mortified\nif others know much more psychology than I. But I am contented to wallow\nin the grossest ignorance of Greek. My deficiencies there give me no sense\nof personal humiliation at all. Had I \'pretensions\' to be a linguist, it\nwould have been just the reverse. So we have the paradox of a man shamed\nto death because he is only the second pugilist or the second oarsman in\nthe world. That he is able to beat the whole population of the globe minus\none is nothing; he has \'pitted\' himself to beat that one; and as long as\nhe doesn\'t do that nothing else counts. He is to his own regard as if he\nwere not, indeed he is not.\nYonder puny fellow, however, whom every one can beat, suffers no chagrin\nabout it, for he has long ago abandoned the attempt to \'carry that line,\'\nas the merchants say, of self at all. With no attempt there can be no failure;\nwith no failure no humiliation. So our self-feeling in this world depends\nentirely on what we back ourselves to be and do. It is determined\nby the ratio of our actualities to our supposed potentialities; a fraction\nof which our pretensions are the denominator and the numerator our success:\nSuch a fraction may be\nincreased [p. 311] as well by diminishing the denominator as by increasing\nthe numerator. To give up pretensions is as blessed\na relief as to get them gratified; and where disappointment is incessant\nand the struggle unending, this is what men will always do. The history\nof evangelical theology, with its conviction of sin, its self-despair,\nand its abandonment of salvation by works, is the deepest of possible examples,\nbut we meet others in every walk of life. There is the strangest lightness\nabout the heart when one\'s nothingness in a particular line is once accepted\nin good faith. All is not bitterness in the lot of the lover sent\naway by the final inexorable \'No.\' Many Bostonians, crede experto\n(and inhabitants of other cities, too, I fear), would be happier women\nand men to-day, if they could once for all abandon the notion of keeping\nup a Musical Self, and without shame let people hear them call a symphony\na nuisance. How pleasant is the day when we give up striving to be young,\n- or slender! Thank God! we say, those illusions are gone. Everything\nadded to the Self is a burden as well as a pride. A certain man who lost\nevery penny during our civil war went and actually rolled in the dust,\nsaying he had not felt so free and happy since he was born.\nSelf-esteem = Success / Pretensions.\nOnce more, then, our self-feeling is in our power. As Carlyle says:\n""Make thy claim of wages a zero, then hast thou the world under thy feet.\nWell did the wisest of our time write, it is only with renunciation\nthat life, properly speaking, can be said to begin.""\nNeither threats nor pleadings can move a man unless they touch some\none of his potential or actual selves. Only thus can we, as a rule, get\na \'purchase\' on another\'s will. The first care of diplomatists and monarchs\nand all who wish to rule or influence is, accordingly, to find out their\nvictim\'s strongest principle of self-regard, so as to make that the [p.\n312] fulcrum of all appeals. But if a man has given up those things which\nare subject to foreign fate, and ceased to regard them as parts of himself\nat all, we are well-nigh powerless over him. The Stoic receipt for contentment\nwas to dispossess yourself in advance of all that was out of your own power,\n- then fortune\'s shocks might rain down unfelt. Epictetus exhorts us, by\nthus narrowing and at the same time solidifying our Self to make it invulnerable:\n""I must die; well, but must I die groaning too? I will speak what appears\nto be right, and if the despot says, then I will put you to death, I will\nreply, \'When did I ever tell you that I was immortal? You will do your\npart and I mine; it is yours to kill and mine to die intrepid; yours to\nbanish, mine to depart untroubled.\' How do we act in a voyage? We choose\nthe pilot, the sailors, the hour. Afterwards comes a storm. What have I\nto care for? My part is performed. This matter belongs to the pilot. But\nthe ship is sinking; what then have I to do? That which alone I can do\n- submit to being drowned without fear, without clamor or accusing of God,\nbut as one who knows that what is born must likewise die.""\nThis Stoic fashion, though efficacious and heroic enough in its place\nand time, is, it must be confessed, only possible as an habitual mood of\nthe soul to narrow and unsympathetic characters. It proceeds altogether\nby exclusion. If I am a Stoic, the goods I cannot appropriate cease to\nbe my goods, and the temptation lies very near to deny that they\nare goods at all. We find this mode of protecting the Self by exclusion\nand denial very common among people who are in other respects not Stoics.\nAll narrow people intrench their Me, they retract it, - from\nthe region of what they cannot securely possess. People who don\'t resemble\nthem, or who treat them with indifference, people over whom they gain no\ninfluence, are people on whose existence, however meritorious it may intrinsically\nbe, they look with chill negation, if not with positive hate. Who will\nnot be mine I will exclude from existence altogether; that is, as far as\n[p. 313] I can make it so, such people shall be as if they were not.\nThus may a certain absoluteness and definiteness in the outline of my Me\nconsole me for the smallness of its content.\nSympathetic people, on the contrary, proceed by the entirely opposite\nway of expansion and inclusion. The outline of their self often gets uncertain\nenough, but for this the spread of its content more than atones. Nil\nhumani a me alienum. Let them despise this little person of mine, and\ntreat me like a dog, I shall not negate them so long as I have a\nsoul in my body. They are realities as much as I am. What positive good\nis in them shall be mine too, etc., etc. The magnanimity of these expansive\nnatures is often touching indeed. Such persons can feel a sort of delicate\nrapture in thinking that, however sick, ill-favored, mean-conditioned,\nand generally forsaken they may be, they yet are integral parts of the\nwhole of this brave world, have a fellow\'s share in the strength of the\ndray-horses, the happiness of the young people, the wisdom of the wise\nones, and are not altogether without part or lot in the good fortunes of\nthe Vanderbilts and the Hohenzollerns themselves. Thus either by negating\nor by embracing, the Ego may seek to establish itself in reality. He who,\nwith Marcus Aurelius, can truly say, ""O Universe, I wish all that thou\nwishest,"" has a self from which every trace of negativeness and obstructiveness\nhas been removed - no wind can blow except to fill its sails.\nBernet, Christine Z., Rick E. Ingram, and Brenda R. Johnson, (1993). Self-esteem, in (Ed.) Charles G. Costello, Symptoms of Depression . New York: Wiley.\nJames, William, (1890). The Principles of Psychology. Copied, with permission, from The Principles of Psychology.']"	['<urn:uuid:f2f1c2fe-08c9-442e-8335-ab374fd507af>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T19:58:41.790432	7	42	1788
18	What happens to donated organs and what are possible complications?	Donated organs go to the patients who need them most urgently and are the best match - donors cannot choose recipients as this is regulated by law to ensure equal healthcare access. During surgery, doctors may discover an organ is unsuitable for transplantation due to quality issues or unexpected disease - in such cases, the organ can be used for scientific research. For living donors, there are surgical risks including pain, infection, blood loss requiring transfusions, blood clots, allergic reactions to anesthesia, pneumonia, and injury to surrounding tissues. Additionally, living donors may experience psychological symptoms like anxiety, depression, or regret, and could face challenges with insurance coverage and employment.	['If you donate organs and tissues after death, you can help persons who are very sick. They can keep living if they receive a donor kidney or liver. And thanks to the eye tissue of a donor, they can see again. In the Netherlands, more than 1,000 persons are on an organ waiting list. About 150 individuals die every year while waiting for an organ. A deceased donor donates three organs on average, so that every year these organ donors save the lives of about 800 persons. One would think that this takes care of everyone on the waiting list, but that is not the case. The waiting list just keeps getting longer.\nNo, not everyone can donate their organs or tissues after death. A doctor will assess upon your death whether your organs or tissues are suitable for donation.\nBut what everyone can do is specify their choice in the Donor registry – even if you are sick, sometimes even if you have (or have had) cancer, have received a blood transfusion or are taking medications. It is always a good thing to specify your choice. An organ or tissue that is damaged by disease or medication can sometimes become ineligible for transplantation, but other organs or tissues may still be suitable.\nThings can also change in the future, as medical science is able to solve more issues. While today you may not able to donate because of a disease, in a few years that same disease may no longer constitute a problem.\nYes, you can. You can donate if you are sick or taking medications. Upon your death, the doctor will assess which organs or tissues can still be used for a patient. Sometimes organs or tissues are no longer suitable for transplantation, for instance due to damage or disease, but other organs or tissues may still be suitable for transplantation.\nNo, you yourself cannot decide who gets your organs or tissues after your death. That is stipulated by law. One reason for it is that in the Netherlands, everyone must be able to get the same level of healthcare. This is why you cannot be the one determining who you will or will not donate to. An organ goes to the patient on the waiting list who needs it most urgently, and who is the best fit for the organ.\nEveryone can specify their choice in the Donor registry. Whether you are homosexual or heterosexual makes no difference, so you may be eligible to donate organs.\nYes, even if you have an unhealthy lifestyle, it can be helpful to give a ‘yes’ in the Donor registry. A person on the waiting list for a new organ can die if the organ does not arrive on time. Lungs of a smoker may be better than the person’s own lungs, and the liver of a drinker can sometimes help a liver patient. So if you smoke or drink, it doesn’t necessarily mean you cannot donate your organs. What is important is that the organ still be suitable for transplantation. A doctor will properly assess this in advance.\nOrgan and tissue donation can be done from birth to old age. A doctor will assess the suitability of every organ or tissue. Skin donation is only possible from age 20. The heart of an 80-year-old is generally no longer suitable for donation, but that person may still be able to donate kidneys.\nYes, all the organs and tissues that you can donate are listed on the Donor form. You may want to donate some organs and tissues but not others. You can specify this on the form.\nOrgan donation can only be done when people die at the hospital, for example because of a severe stroke or an accident. In this way, a person may be able to donate organs after their death. Authorisation is always needed from the donor or from their family. Only when it is certain that a patient is going to die, does a doctor take a look in the Donor registry to see the choices that were made.\nDid you give a ‘yes’ in the Donor registry? Or has your family said ‘yes’? Only then can the hospital start the testing. The physicians may take an ECG or blood tests, and ensure that the body stays at the right temperature. Organ donation can take between 10 and 24 hours, sometimes even longer. Did you give a ‘no’ in the Donor registry? Then there will be no donation and no testing.\nDonation of tissues works out more often than donation of organs. In such cases it doesn’t matter whether you die at home or at the hospital. Special teams harvest the tissues at a hospital or another suitable space, following strict rules.\nAfter a patient dies, a doctor takes a look in the Donor registry to see the choices that were made. Have you not specified a choice? In that case, your family will decide about donation. Has permission been given for donation? In that case, the doctor will report the donor to the Dutch Transplant Foundation, and the donation process will get started. Altogether, tissue donation takes between 8 and 28 hours.\nAfter your death, you can donate the following organs and tissues:\nOrgans: liver, heart, lungs, pancreas, small intestines, kidneys\nTissues: eye tissue, heart valves, large vessels, skin, bone, cartilage and tendons\nThe chances are not that large. Organ donation is quite exceptional and does not happen often. How come? An organ donor must always die at the intensive care unit (ICU) of a hospital and be receiving artificial respiration, and the organs must still be suitable for transplantation.\nIn 2018, 273 persons gave organs after their death. Every year, about 153,000 people die. This means that the chances of becoming an organ donor are 1 in 560.\nDefinitely. Organs and tissues are only harvested when doctors are 100% sure that a person has died. Death is diagnosed according to strict rules, and this is done by several physicians.\nDonation and family\nNo. Family cannot change your choice just like that. The doctor will almost always honour the choice you made in the Donor registry. That is what you wanted. If there is a very good reason, a doctor may decide differently.\nAn organ or tissue donation must be done with the utmost care, so it takes time. Altogether, organ donation can take between 10 and 24 hours, sometimes even longer.\nThe time between diagnosing brain death and harvesting organs is between 4 and 12 hours. If the heart and blood circulation have stopped, there is less time. As soon as blood is no longer circulating through the body, organs quickly become less suitable for transplantation. This is why harvesting of organs begins quickly after death is diagnosed.\nIf the body of a tissue donor is refrigerated within six hours, tissues can still be harvested for transplantation up to 24 hours after death. The tissue donation procedure can thus last between 8 and 28 hours.\nCertainly. There is always someone for the family at the hospital. During the entire duration of the donation process, a hospital staff member (such as the transplant coordinator) will be in touch with the family.\nYes. Removing organs and tissues is always done with great care. Doctors consider it very important for a donor to look presentable after the operation. They do not remove anything from places that are visible when a body is viewed. Stitches with bandages are applied on operated areas.\nYes, there is always time to say goodbye. Family can be with the donor before and after the donation. After the donation procedure, the family itself determines what happens with their loved one. The family can also choose the viewing location, at a funeral centre or at home.\nYes, you can. A funeral or cremation can take place on the day chosen by the family and does not need to be postponed because of the organ or tissue donation.\nNo, there are no costs involved. The family of the donor doesn’t pay anything for donation.\nDonation and religion\nDonation and religion can be compatible. Most faiths approve of organ donation. If you have any doubts about this, it is important that you discuss it with someone – perhaps a pastor, an imam or someone else knowledgeable about your religion – so that you can arrive at a joint perspective about organ and tissue donation.\nYou can only be included in the Donor registry if you yourself specified your choice in the Donor registry. Anyone from the age of 12 can specify their choice in the Donor registry. There are four choices:\nChoice 1: Yes, I grant permission\nYou want to become a donor. You can donate the following organs: pancreas, intestines, heart, liver, lungs and kidneys;\nand the following tissues: blood vessels, bone, heart valves, cartilage, tendons, skin and eye tissue.\nYou may want to donate some organs and tissues but not others. You can specify this on the form.\nChoice 2: No, I do not grant permission\nYou do not want to become a donor.\nChoice 3: My partner or family will decide\nUpon your death, your partner or family are allowed to make the choice.\nChoice 4: The person I have appointed will decide\nYou want someone else to decide for you after your death.\nEveryone in the Netherlands will receive information from the government about the new law.\nIf after 1 July 2020, you still haven’t specified a choice in the Donor registry, you will receive a letter asking whether you want to specify your choice.\nIf you do not specify anything after the first letter, you will receive a reminder after six weeks.\nIf again you do not specify anything, ‘No objection against organ donation’ will be listed under your name. You will get another letter to confirm this. ‘No objection against organ donation’ means that your organs can go to a patient after your death.\nIt remains important that you yourself specify a choice about organ donation. If you have already done so, your choice will remain valid.\nThe choices are the same as in the current law. Something will change only if you haven’t made a choice yet. Nothing changes even after you receive a letter after 1 July 2020 in which you are asked to specify a choice.\nIf you do not specify a choice in the Donor registry, ‘No objection against organ and tissue donation’ will appear under your name. This means that your organs can go to a patient after your death. The doctor at the hospital will discuss this with your family. If your family is very sure and can explain to the doctor that you really did not want to be a donor, then you will not be a donor. It is therefore very important for your partner and family to know what your choice is, and especially for you yourself to specify it.\nIt is important for you to specify in the Donor registry whether you do or do not want to become a donor. You can specify this choice in three different ways:\nWhen it comes to organ and tissue donation, a person must be able to understand what it is about. The person must also be able to understand the consequences of the choice they make. If the person cannot do that, then they are legally incapable for organ donation.\nAt present, a legally incapable person cannot be a donor. That will change when the new donor law goes into effect on 1 July 2020.\nWhen an organ or tissue is harvested, a doctor can discover that it is not suitable for transplantation after all, for instance because the quality of the organ or tissue is not good enough or because an unexpected damage or disease is found. In such cases, the organ can be used for scientific research. Through this research, physicians learn more about transplantation. This is different than donating your entire body to science.\nSince 1 October 2019, the donor pass is no longer issued. A doctor is not going to look for your pass when you pass away; a doctor is obliged to search the Donor Register, where your choice is registered. For that reason, the donor pass is no longer sent together with a confirmation of the registration.', 'Living donation involves anesthesia and major surgery and their associated risks.\nIn the majority of cases, the surgical removal of the donor’s kidney (called a donor nephrectomy) is done by laparoscopic surgery which involves two to three small incisions in the abdomen and one larger incision (6-9 centimeters) through which the kidney is removed. Surgery is performed under general anesthesia. In a small number of cases, a laparoscopic nephrectomy is not an option, so the surgery involves a five to seven inch incision on the side of the chest and upper abdomen.\nSurgical complications can include pain, infection, blood loss (requiring transfusions), blood clots, allergic reactions to anesthesia, pneumonia, injury to surrounding tissue or other organs, and even death.\nAfter surgery, the remaining kidney will grow slightly larger to make up for some of the function of both kidneys. Some people are born with one kidney or lose one due to injury and are able to live full lives with little or no effect. However, people with one kidney may be at a greater risk of high blood pressure, proteinuria (protein in the urine) and reduced kidney function. In very rare cases, a kidney donor may experience loss of kidney function in the years following their kidney donation. Should this occur, priority is given to a prior living donor according to national organ transplant waiting list regulations.\nPlease note that there has been no national systematic long-term data collection on the risks associated with living organ donation. Based upon limited information that is currently available, overall risks are considered to be low – 95 to 96 percent of donors have no complications.\nA study by Johns Hopkins University School of Medicine that followed 80,000 living kidney donors over a 15-year period found that there was no increase in mortality rates among donors compared to healthy people with both kidneys.\nIn a 2015 paper published in the Journal of the American Society of Nephrology, lifetime risk for the average person of ESRD was 326/10,000 (about 1 in 30), 90/10,000 (about 1 in 110) for those who donated a kidney, and 14/10,000 (about 1 in 700) for healthy non-donors. The reason kidney donors have a lower risk of ESRD compared to the general population is that kidney donors are typically healthier than the average person due to the donor screening process. When donors and healthy non-donors are compared, there is an implied ESRD risk increase of 76/10,000 from donating a kidney. The following are informative papers related to long term risks of donating a kidney: (Source: National Kidney Registry)\nJournal of the American Society of Nephrology – 2015\nReassessing Medical Risk in Living Kidney Donors\nJournal of the American Medical Association – 2014\nRisk of End-Stage Renal Disease Following Live Kidney Donation\nNew England Journal of Medicine – 2009\nLong-Term Consequences of Kidney Donation\nSome medical organizations, such as the American Academy of Sports Medicine and the American Academy of Family Physicians, suggest that a person with one kidney should refrain from sports involving high contact or frequent collisions. Individuals who participate in these sports should exercise caution, be aware of possible consequences, and wear appropriate protective equipment.\nNegative psychological symptoms are also possible during the healing process and even years after the donation. Your donated organ may not function in the recipient after it is transplanted. You and/or the transplant recipient may have medical problems from the surgery. Scarring or other aspects of the donation process could possibly contribute to problems with body image. You may have feelings of regret, resentment, or anger. You may have symptoms of anxiety or depression. Treatment for these conditions can be lengthy, costly, and could possibly include the use of medications with risks and side effects.\nSome donors have reported difficulty in getting, affording, or keeping health, disability, or life insurance. It is important that you talk with your own insurance carriers before making a decision about being a living donor. Your premiums could increase. If you do not have health insurance, serving as a donor could be considered a pre-existing condition if you apply for insurance later.\nIf you work, talk with your employer about any existing leave policies before committing to living donation. Also, fully think about the financial impact on your family, especially if you and/or whoever serves as your caregiver during the donation recovery process may face lost wages.\nSource: UNOS Living Donation']	['<urn:uuid:7c861db9-4840-4731-a188-4e0ecdf4fefd>', '<urn:uuid:253c5264-e495-4819-a25e-5239ad27e9d8>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:58:41.790432	10	109	2792
19	I'm curious about workplace privacy - how is personal information of employees protected in companies nowadays?	EnPro has a high respect for employee privacy and their right to protect personal information. The company policy controls the use and access to company information that affects individuals' employment status. This information must be carefully protected and should be shared only with those who have legitimate need for it.	['Human rights are basic rights, freedoms and standards of treatment regarded as belonging to all persons, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status or any other status. EnPro respects and supports internationally recognized human rights, and this policy is guided by the principles found in the U.N. Guiding Principles for Business and Human Rights. We strive to promote these rights in our relationships with our employees, suppliers, vendors, and stakeholders, and require compliance with our human rights policy by all.\nEqual Employment Opportunity\nEnPro values diversity and is committed to providing equal opportunity for hiring, promotions, transfers, and training opportunities, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status. All personnel will be required to carry out the spirit and intent of this policy. The successful implementation and effectiveness of this policy is the responsibility of the respective managers and supervisors, with the assistance of Corporate Human Resources. More detailed information is available to EnPro employees in our Equal Employment Opportunity Policy.\nIntolerance for Discrimination and Harassment\nTo ensure a productive working environment and encourage mutual respect among employees, EnPro is committed to providing all employees a workplace free from all forms of discrimination, including harassment based on race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran. It is EnPro’s policy to ensure that the work environment is free from any such discrimination or harassment. More specifically, it is against EnPro policy for any employee (including co-workers, supervisors, managers and other personnel) or any third-party (including customers, independent reps and vendors) to harass an employee in violation of this policy. More detailed information is available to EnPro employees in our EnPro’s General Harassment in the Work Environment Policy.\nFreedom of Association\nEnPro respects an employees’ right to join or form a labor union, and to collective bargaining, without fear of reprisal, intimidation or harassment. We are committed to engaging in open and constructive communication with union representatives.\nSafe and Healthy Work Environment\nEnPro is committed to excellence and continual improvement in our Environmental, Health, and Safety (EHS) practices and performance. Periodic audits of EnPro’s operations are conducted to ensure compliance with EnPro’s EHS policies and applicable EHS laws, regulations and standards. In addition, we believe the right to water as a fundamental human right and all office spaces are supplied with clean drinking water, clean and accessible restrooms, adequate lighting and ventilation, emergency exits, and first aid equipment.\nWages and Benefits\nWe ensure full compliance with all wage, hour, overtime and benefits laws, and are committed to ensuring fair wages for all employees. Our wages and benefits are competitive within our industry and local labor market, and working hours contribute to a healthy work-life balance.\nWe are compliant with labor laws which prohibit the exploitation of children and use of illegal child labor. Company personnel must ensure that all employees are legally eligible for employment and meet the applicable minimum legal age. We prohibit the hiring of individuals under 18 years of age for positions in which hazardous work is required. Verifiable documentation of each employee’s date of birth or other legitimate means of confirming each employee’s age must be maintained, as required by law.\nPrivacy and Security\nWe are committed to maintaining a safe work environment for all employees, free from intimidation, harassment, threats or acts of a violent nature, and any such conduct is not tolerated. An employee should report all threats of (or actual) violence, both direct and indirect, as soon as possible to his/her immediate supervisor or any other member of management. This includes threats by employees, as well as threats by customers, vendors, solicitors, or other members of the public. Reports or incidents warranting confidentiality will be handled appropriately and information will be disclosed to others only on a need-to-know basis.\nEnPro has a high respect for employee privacy as well, and our employees’ right to protect their personal information. Therefore, it is our policy to control the use and access to company information that affect individuals’ employment status. We believe that this information must be carefully protected and should be shared only with those who have legitimate need for it.\nGuidance and Reporting\nEnPro complies with all labor and employment laws and is dedicated to maintaining a workplace where respect for individual rights and safety is of the utmost importance. In our efforts to educate all employees about our policies on appropriate workplace behavior and treatment of others, we require all new hires to read and acknowledge our Code of Business Conduct. On an annual basis, all active employees are required to recertify that they have read and acknowledge the Code or complete a training module regarding the code.\nIn the event of a violation of any of these human rights policies, employees are provided with our EnTegrity Assistance Line at the time of hire and are instructed to report violations of any unethical nature, including human rights, without fear of retaliation. Reports may be made anonymously, and all reports are kept confidential to the extent practicable, consistent with applicable laws and the need to conduct an adequate investigation.']	['<urn:uuid:ea3027c3-7fb3-4e37-8994-e69e0f8eae7f>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	16	50	868
20	I'm curious about historical traditions and farming - what was the original Hebrew meaning of the word jubilee, and how do modern farmers protect beneficial insects like bees?	The word jubilee comes from the Hebrew word 'yobhel' meaning ram's horn, which was used as a trumpet to proclaim the jubilee year. Modern farmers protect beneficial insects like bees by using safer pest control measures, such as applying pesticides in the evening when bees aren't active, using floating row covers, and planting flowering herbs that attract beneficial insects while controlling pests naturally.	"['jubilee(redirected from Jubilees)\nAlso found in: Thesaurus, Legal, Encyclopedia, Wikipedia.\na. A specially celebrated anniversary, especially a 50th anniversary.\nb. The celebration of such an anniversary.\n2. A season or occasion of joyful celebration.\n3. Jubilation; rejoicing.\n4. often Jubilee Bible In the Hebrew Scriptures, a year of rest to be observed by the Israelites every 50th year, during which slaves were to be set free, alienated property restored to the former owners, and the lands left untilled.\n5. often Jubilee Roman Catholic Church A year during which plenary indulgence may be obtained by the performance of certain pious acts.\n1. a time or season for rejoicing\n2. a special anniversary, esp a 25th or 50th one\n3. (Roman Catholic Church) RC Church a specially appointed period, now ordinarily every 25th year, in which special indulgences are granted\n4. (Judaism) Old Testament a year that was to be observed every 50th year, during which Hebrew slaves were to be liberated, alienated property was to be restored, etc\n5. a less common word for jubilation\n[C14: from Old French jubile, from Late Latin jubilaeus, from Late Greek iōbēlaios, from Hebrew yōbhēl ram\'s horn, used for the proclamation of the year of jubilee; influenced by Latin jūbilāre to shout for joy]\nju•bi•lee(ˈdʒu bəˌli, ˌdʒu bəˈli)\n1. the celebration of any of certain anniversaries, as the 25th, 50th, 60th, or 75th.\n2. the completion of 50 years of existence, activity, or the like.\n3. any season or occasion of rejoicing or festivity.\n4. rejoicing or jubilation.\n5. (in the Roman Catholic Church)\na. Also called ju′bilee year′. an appointed year or other period, ordinarily every 25 years, in which a plenary indulgence is granted upon repentance and the performance of certain acts.\nb. the plenary indulgence granted.\n6. a yearlong period observed by Jews in ancient times every 50 years, during which Jewish slaves were freed, alienated lands restored to the original owner, and the fields left untilled. Lev. 25. Compare sabbatical year (def. 2).\n7. an African-American folk song concerned with future happiness or deliverance from tribulation.adj.\n8. flambé: cherries jubilee.\n[1350–1400; Middle English < Middle French jubile < Late Latin jūbilaeus < Late Greek iōbēlaîos (with assimilation to Latin jūbilāre to shout for joy) « Hebrew yōbhēl ram\'s horn, jubilee]\njubilee- Comes from Hebrew yobhel, ""ram\'s horn,"" which was used as a trumpet to proclaim the jubilee, a year of emancipation and restoration (every 50 years).\nSee also related terms for trumpet.\nSwitch to new thesaurus\n|Noun||1.||jubilee - a special anniversary (or the celebration of it)|\nanniversary, day of remembrance - the date on which an event occurred in some previous year (or the celebration of it)\ndiamond jubilee - an anniversary celebrating the passage of 60 years\nsilver jubilee - an anniversary celebrating the passage of 25 years\nn → Jubiläum nt\na celebration of a special anniversary (especially the 25th, 50th or 60th) of some event, eg the succession of a king or queen. The king celebrated his golden jubilee (= fiftieth anniversary of his succession) last year. jubileum, jubelfees; jubeljaar يوبيل юбилей jubileu jubileum, výročí das Jubiläum jubilæum πανηγυρική επέτειος, ιωβηλαίο jubileo, aniversario juubel سالروز؛ بزرگداشت riemujuhla jubilé חֲגִיגַת יוֹבֵל जयन्ती jubilej jubileum perayaan istimewa memperingati sesuatu fagnaðarhátíð, afmælishátíð giubileo; anniversario 記念祭 기념(축)제 jubiliejus jubileja jubli jubileumjubileumjubileusz پنځوسم تلين: ياپنڅوسمه كاليزه: جشن، دميلى ورځ jubileu jubileu юбилей jubileum jubilej jubilej jubileum งานฉลองครบรอบ 25 ปี 50 ปี หรือ 60 ปี kutlama, jübile 週年紀念 ювілей خوشی کی تقریب lễ kỉ niệm 周年纪念（特别是 25、50或60周年纪念）', 'Listen To The Article\nSince 1998, scientists, conservationists, and farmers have noticed an alarming trend. European honeybee populations are declining at rapid rates. Researchers believe several factors are at play here: viruses spread through the colonies, loss of habitat, migratory habits, and increased pesticide use.\nOne class of pesticides, in particular, has been implicated in the decline of bee populations. The European Council recently voted to ban this group of pesticides, neonicotinoids, in an effort to restore bee colonies.\nOver 70 percent of the world’s crops are at least partially pollinated by bees. Without these pollinators, we would face a substantial loss of crop diversity. In your own garden, a loss of pollinators means no more zucchini, pumpkins, summer squash, apples, berries, or melons. Some crops, such as carrots and onions, rely on bees to produce seed, rather than edible plant parts.\nThe need for safer pest control measures is apparent, but you might be wondering how to control pests without harming bees. Read on for a roundup of ideas.\nOrganic Pest Control\nMany pesticides are labeled as safe for organic use. These products are usually derived from plants or other natural materials and they might break down faster in the soil than synthetic products. However, just because a product is labeled “organic,” doesn’t mean it won’t harm bees. Below is a list of common organic pesticides, fungicides, and herbicides, and their toxicity to bees, according to the Xerces Society. When using a pesticide, always opt for the least toxic products.\n- Bacillus thuringiensis\n- Kaolin clay\n- Corn gluten\n- Gibberellic acid\n- Boric acid\n- Horticultural vinegar\n- Lime sulfur and sulfur\n- Diatomaceous earth\n- Insecticidal soap and oil\n- Copper sulfate\nHow you apply pesticides can also make a difference in their toxicity to bees. Pesticides kill bees in several ways. First, bees absorb the chemicals through their exoskeletons when they’re exposed to pesticides in the air. Pesticides can contaminate pollen, nectar, and even dew. When bees come in contact with these substances, they can be killed. Finally, bees sometimes take contaminated pollen and nectar back to the hive, where it harms the other bees.\nIf you must apply pesticides, apply them only to the affected plants, and preferably when the plants aren’t in bloom. Apply them late in the evening when bees aren’t active. Spray pesticides during dry conditions, if possible, since dew can retain the toxins.\nWhenever possible, focus your efforts on prevention strategies, rather than resorting to pesticides to eliminate problems. Organic gardeners take a holistic approach, knowing that no one strategy can combat every pest. When used in combination, though, the following techniques can minimize the pests in your garden.\nFloating Row Covers. Floating row covers are lightweight agricultural fabrics that allow water and sun to permeate while keeping out the bad guys. Install them at planting time and secure them with rocks or pins. Allow enough slack for the fabric to expand as the plants grow. Floating row covers won’t eliminate pests that live in the soil, but they will keep out beetles, flea beetles, and many other pests. Additionally, floating row covers warm the soil and keep it moist so seeds germinate faster. They also encourage faster growth in young seedlings. Remove row covers when summer heat arrives or when plants start to produce flowers so bees can pollinate them.\nHand Picking. This strategy might not appeal to squeamish types, but if you were the sort of kid who loved bugs, this might be right up your alley. Simply stroll through your garden every morning or evening and pick off caterpillars, beetles, snails, and slugs. Drop the pests in a bucket of soapy water. Obviously, this technique won’t work for flea beetles or small pests, but it’s a fast and simple way to control most of the pests in your garden.\nTraps. Veteran gardeners seem to collect creative methods for trapping insects. Wrap a paper collar around tender young seedlings. Push the collar down into the soil at least 3 inches, allowing another 3 inches to remain above ground. Collars are effective at deterring cutworms and other soil-dwelling pests. Some gardeners lay crumpled aluminum foil under plants to keep cutworms, slugs, and snails out. Others place shallow bowls of beer or soapy water in the garden to attract and drown insects. Traps work with varying effectiveness, but are worth a try. Don’t leave anything toxic, such as mothballs, in the garden where children and animals might get it.\nCrop Rotation. One of the simplest ways to control pests and diseases is by rotating your crops from year to year. Some insects are generalists, nibbling their way through the garden like it’s a buffet. Most, though, prefer some plants over others. Cucumber beetles prefer cucurbits, tomato hornworms love tomatoes. Move the crops around and you’ll confuse the pests a bit. Combine crop rotation with inter-planting and you double your odds of success. Inter-planting is the technique of planting two crops together to repel, confuse or deter pests. For example, onions and garlic seem to repel many pests. Plant them among your more vulnerable plants for added protection.\nInvite Beneficial Insects To The Garden. Not every insect is a pest. In fact, some insects can help your garden by feeding on the bad guys. Ladybugs, lacewings, and praying mantises are three insects worth keeping around. Plant flowering herbs in your garden, such as dill, angelica, cilantro, and basil. Beneficial insects often lay their eggs on these plants and their larva feed on them.\nTidy Up. Keeping a tidy garden has more than aesthetic benefits. A clean garden harbors fewer pests. Weeds and grasses growing near your garden can harbor aphids and other pests that not only feed on plants, but carry diseases, such as aster yellows. Overripe produce is sure to attract pests as it rots. Always harvest produce as it ripens and freeze, compost, or give away what you can’t use immediately.\nFinally, sometimes the simplest solution is to take a live and let live approach. A few damaged leaves are rarely fatal to plants and hardly worth the risk of using pesticides. If a specific crop is particularly troublesome, perhaps it’s time to drop it from your vegetable garden inventory. A few simple changes this summer can mean a healthier garden for both you and the bees.']"	['<urn:uuid:c714d0f5-1658-44f9-b481-e80ef8ae703d>', '<urn:uuid:f853b51c-7089-4e7f-99e0-3b3f4818ae02>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T19:58:41.790432	28	63	1640
21	How do contracts and agreements affect behavior?	Contracts and agreements, even when unenforceable, can significantly shape behavior through multiple mechanisms. For informal risk-sharing networks, contracts can either complement or substitute existing arrangements - some evidence shows that formal insurance can crowd out informal networks by creating negative externalities, while other evidence suggests it can strengthen networks by helping manage specific risks. Additionally, even very incomplete or unenforceable contracts can establish relationship-specific norms that influence behavior. The most effective contracts often include an unenforceable 'handshake' agreement to take optimal actions, suggesting that establishing norms can effectively substitute for weak enforceable restrictions.	"[""This paper addresses whether the index-based livestock insurance and informal insurance network substitute or complement using the data collected in rural Ethiopia, Borena zone, which is the southern part of the country.\nIndex insurance has attracted world wide attention as it does not suffer from classical incentive problems, such as moral hazard and adverse selection, traditional insurance faced. Although it is expected to be diffused rapidly, the overall uptake rate is not so high.\nRecent theoretical advancements argue that low uptake can be partly explained by the existing of informal insurance network. For example, de Janvry et al. (2014) demonstrates individuals who are connected with informal networks can free-ride on their friends' insurance payout, which creates externatilty, each member expects others to be formally insured, resulting in a socially suboptimal level of insurance coverage. So, unless insurance is sold to a group, which can adjust these externalities through group coordination, informal insurance networks may lead to lower uptake of formal insurance. In a detailed theoretical model, Boucher and Delpierre (2013) show that formal insurance induces behaviroal changes, leading riskier choices (increasing residual risks), so the risk that informal networks must absorb increases, generating a negative externality on network members, and each will leave the informal risk-sharing group.\nYet there is also counterargument. Dercon et al. (2014) model that most index insurance products are designed to target aggregate shocks that affect an entire community, but basis risk remains and such idiosyncratic shocks should be recovered by individual farmers. If individuals within a group can commit to offer mutual protection to each other against such idiosyncratic shocks, then index products offer better value to farmers. They argue that the presence of basis risk makes index insurance a complement to informal insurance networks sharing, and that the demand for index insurance should therefore increases when there is within group risk sharing.\nChemin also discusses that one of the constraints on uptake of microinsurance is trustworthiness of insurance company. It found that support for social learning in groups from early adopters who have tested the insurance system before, and thus alleviate fears of non-reimbursement, implying that group can resolves some constraints.\nAlthough one of the remedy for low uptake seems to provide index insurance to groups, we know little about when whether the index insurance crowed-in or crowed-out informal risk-sharing networks when sold to individuals is scarce.\nThe primary purpose of this article is to seek empirical support for the alternative theoretical predictions on the demand for group index insurance.\nOur study site is in Borene region in southern Ethiopia, about 550 km away from Addis Abeba, Capital of the country. Borena is located near to Kenya boundary. From this region, we selected 17 core areas as study site and collected the baseline data from 515 households in 2012. In 2014, 514 households with some replacement is again covered by the survey.\nAs an index, the normalized differenced vegetation index, which is a numerical indicator for by satellite was used. The indemnity payout will be done if the cummulative NDVI falls below the 15th percentile of historical distribution since 1981.\nIn our study, regions are divided into eight Woreda, which is an administrative unit of Ethiopia, and premium rate is set to be the same within the Woreda. Actual premium payout is the woreda-specific premium rate multiplied by total insured herd value which varies with the species of animals.\nThe region is comprised of arid and semi-arid ecological zones with four seasons: a short rainy season (October to November); and a short dry season (December to February) a long rainy season (March to May); a long dry season (June to September); IBLI is marketed and sold during two periods per year, directly preceding each rainy season (August-September and January-February), with coverage lasting one year and the potential for two indemnity payouts, one after each dry season During each sales period, a household decides whether to buy IBLI and how many animals to insure. If a household buys IBLI in the August-September sales period, it is insured from October 1 to September 30 of the following year and may receive indemnity payouts in March and/or October of the year following purchase. Note that if a pastoral household buys IBLI not only in the August-September sales period but also the following January-February sales period, then insurance coverage periods for the two contracts overlap from March to September, and the household may receive indemnity payouts for both contracts in October.\nMore precisely, 𝑇𝐼𝐻𝑉= # of camel insured ∗15,000+ (# of cows insured)∗5,000 + (# of goats and sheep insured)∗700 and 𝑃𝑟𝑒𝑚𝑖𝑢𝑚 𝑝𝑎𝑦𝑚𝑒𝑛𝑡=Woreda specific insurance premium rates ∗ 𝑇𝐼𝐻𝑉.\nWe apply the random matching within sample method, implemented by Santos and Barrett (2011) and Maertens and Barrett (2013), to elicit one’s network and willingness to form informal risk sharing networks. Since within and outside sub-region network will have different implications about IBLI-informal risk sharing nexus, as distance increases, lower correlations exist in terms of shocks, hence more useful in terms of informal insurance. To know differential IBLI impacts, then we may want to add 3 randomly drawn households to the matches from the sub-sample outside each location.\nBasically, knowing is important, but not necessary condition for lending.\nHere is the result of ivprobit. (1) does not include the far dummy, while (2) includes it. Both show negative, but insignificant: purchase of IBLI and link formation is neither substitute nor complement.\nThis is extension. I have estimated multivariate probit (three equations: ) and include the predicted variable in the regressors. The results show that if other buy IBLI, pastrorist does not buy IBLI by him/her self. –an indication of free-riding. Also, not robust, but if other buy IBLI, one is more willing to form informal networks with hi/her. Again, purchase of IBLI and link formation is neither substitute nor complement, though signs turns to be positive.\nVasilaky: Formal insurance may encourage additional risk taking, creating residual idiosyncratic risks that the group will incur. (2) nd question is to know closeness of the match and strategic action when one thought that the match is reliable.\nIs the demand of the index-based livestock insurance and informal insurance network substitute or complement?\nIs the demand of the index-based\nlivestock insurance and informal insurance\nnetwork substitute or complement?\n(with Chris Barrett and Munenobu Ikegami)\nIndex insurance attracts attention as the next financial\nSeveral studies discuss that formal insurance may crowd\nout informal insurance networks (de Janvry et al. 2013 ;\nBoucher and Delpierre, 2014;)\nFree-riding: well-connected individuals can free-ride on their\ngroup-members' insurance payout, resulting in a socially\nsuboptimal level of coverage\nMoral hazard: a greater degree of formal insurance allows for\nexcessive risk-taking, which informal networks should absorb,\nimposing a negative externality on network members-\ncrowding-out of informal risk-sharing\nCounterargument is also provided to explain that the\ndemand of the index insurance can be complementary to\ninformal insurance networks (Berhane, et al., 2014:\nChemin, 2014; Dercon et al., 2014; Mobarak and\nRosenzweig, 2013;) .\nBasis risk and crowed-in: the difference between the losses\nactually incurred and the losses insured= idiosyncratic risk of\nincomplete compensation pooled and managed within an\ninformal risk-sharing group\nIncreased trust: social learning in groups from early adopters\nwho have tested the system before, and thus alleviate fears of\nEmpirical evidence on whether the index insurance\ncrowed-in or crowed-out informal risk-sharing networks\nwhen sold to individuals is scarce, and it is theoretically\nOur paper aims to provide empirical evidence to this\nissue, by using the data collected in Borena, Ethiopia.\n17 Study sites in Borena-Southern Ethiopia (near to Kenya Boundary)\n514 households from Round 3\nDesign of IBLI\nIBLI insures against area average herd loss predicted based on the index\nfitted to past livestock mortality data.\nIndex: Normalized Differenced Vegetation Index (NDVI) –\na numerical indicator of the degree of greenness recorded by satellite\nPayout rule: if the index falls below the 15th percentile of historical\ndistribution since 1981.\nZNDVI: Deviation of NDVI from long-term averageNDVI (Feb 2009, Dekad 3)\nDesign of IBLI\nTiming of Purchase: before rainy seasons (two times in a\nCoverage: 1 year\nTiming of Payout: after dry seasons (two times in a year)\nDesign of IBLI\n𝑊𝑜𝑟𝑒𝑑𝑎 𝑠𝑝𝑒𝑐𝑖𝑓𝑖𝑐 𝑖𝑛𝑠𝑢𝑟𝑎𝑛𝑐𝑒 𝑝𝑟𝑒𝑚𝑖𝑢𝑚 𝑟𝑎𝑡𝑒𝑠 ∗ 𝑇𝐼𝐻𝑉.\n(9.75% for Dilo, 8.71% for Teltele, 7.54% for Yabello, 9.49% for Dire, 8.58% for Arero, 9.36% for Dhas,\nand 11.05% for Miyo and Moyale, depending on differences in expected mortality risk)\nTotal insured herd value (TIHV):\n# 𝑜𝑓 𝑐𝑎𝑚𝑒𝑙 𝑖𝑛𝑠𝑢𝑟𝑒𝑑 ∗ 15,000 + (# 𝑜𝑓 𝑐𝑜𝑤𝑠 𝑖𝑛𝑠𝑢𝑟𝑒𝑑) ∗\n5,000 + (# 𝑜𝑓 𝑔𝑜𝑎𝑡𝑠 𝑎𝑛𝑑 𝑠ℎ𝑒𝑒𝑝 𝑖𝑛𝑠𝑢𝑟𝑒𝑑) ∗ 700\nMax: 0.5*TIHV Min: Premium payment\n(depending on the severity of the drought)\nWe want to explore the impact of informal insurance on the\nuptake of IBLI or vice versa.\nFormation of informal networks/uptake of IBLI is clearly endogenous\nMeasuring informal network is often problematic (Santos and\nBarrett, 2011; Maertens and Barrett, 2013)\nCensus is costly, and infeasible\nNetwork within sampling method (either list up certain number or not)\nartificially truncates the network, and resultant network data are non-\nOpen question tends to elicit only strong network link\nApply “random matching within a sample” method\nA household is randomly matched with 5 near neighbors and 3\nnon-near neighbors within a sample\nTwo questions: (1) Do you know (the match)? (2) If yes, are\nyou willing to transfer cattle as a loan if the match asked for it.\nA dummy, representing a link, equal to 1 if the answer to (2) is\nThis is a hypothetical question, but hopefully, this may not be\na problem as informal asset transfers among Boran pastoralists\nare generally small. Also, there is evidence that the inferred\ndeterminants of insurance networks derived from this\napproach closely match those obtained from analysis of real\ninsurance relations among the same population (Santos and\nBasic model (via ivprobit)\nLINK 𝑖𝑗 = 𝜔 + 𝑎𝑥𝑗 + 𝑏𝑥𝑖 + 𝛽 𝐼𝐵𝐿𝐼𝑖 + 𝜏𝑖𝑗 + 𝜓𝑖𝑗\nLINK: 1 if there is the possibility of transferring cattle as a loan if the\nmatch asked for it between a household i and j,\nXi: characteristics of household i,\nXj: characteristics of matched household j,\nΤ: characteristics to replect relationships between i and j\n𝐼𝐵𝐿𝐼𝑖: the predicted IBLI uptake of previous one year (instrumented\nwith some exogenous variables, such as the discount coupon\nrecipient (assigned randomly: RCT) dummy)\n𝛽>0 is complementary; 𝛽<0 is supplement\nKnowing and lending\nKnow No Yes Total\nNo 1,153 474 1,627\n70.87% 29.13% 100%\nYes 633 1,844 2,477\n25.56% 74.44% 100%\nHave heard name, but never met\nNo 450 1,484 1,934\n23.27% 76.73 100%\nYes 183 360 543\n33.7% 66.3% 100%\nNo 599 1,365 1,964\n30.5% 69.5% 100%\nYes 34 479 513\n6.63% 93.37% 100%\nBasic model (IVprobit)\nIBLI: =1 if purchase IBLI at either 3 or 4 sales period\nControl: HHsize, Head male (=1), Head age and its squared, Head’s completed\nyears of education, risk preference dummies, same clan (=1), study site fixed\neffect for both own and mathed\nIV: dummy to receive discount coupons at either 3 or 4 sales period\nVARIABLES Link Link\n𝐼𝐵𝐿𝐼𝑖 -0.057 -0.022\nNot simultaneous decision. Given others’ previous purchase\n(1) (2) (3) (4)\nLink 𝐼𝐵𝐿𝐼𝑖 Link 𝐼𝐵𝐿𝐼𝑖\n𝐼𝐵𝐿𝐼𝑖 0.141 0.134\n𝐼𝐵𝐿𝐼𝑗𝑅3 0.108 -0.301** 0.065 -0.312**\n(0.150) (0.152) (0.165) (0.152)\n*** p<0.01, ** p<0.05, * p<0.1\nSome indication of free-riding:\nnegative coefficient of others’ IBLI purchase on own purchase\npositive coefficient of others’ IBLI purchase on link formation (lend\nno robust results on whether the own purchase of IBLI crowed-out\ninformal risk sharing network (insignificant coefficient of own IBLI\npurchase on link formation, though sign is positive)\nSome other findings:\nIf the match is in the same clan, prob (link) is positive and significant\nOthers’ wealth measured in TLU does not affect own purchase\nMore risk averse households tend to buy IBLI\nDiscount coupons positively affect the uptake of IBLI\nIt seems important to investigate whether the free-riding is\ndriven by the fact that the subject knows very well about the\neconomic conditions of the matches.\nCai et al. (2015) show positive network effects are driven by\nthe diffusion of insurance knowledge rather than purchase\nVasilaky et al. (2014) show groups in which individuals knew of\none another's assets were less likely to purchase their\ninsurance within a group (in line with Boucher and Delpierre,\nWe will add two questions in R4: (1) do you think the match\nbought IBLI six month ago? (2) how many cows do you think\nthe match herds?"", 'The Effect of Safe Experience on a Warning\'s Impact: Sex, Drugs and Rock-n-Roll\n(with Greg Barron and Jennifer Stack) OBHDP (2008)\nAbstract: In many contexts we are warned against engaging in risky behavior only after having past safe experience. We examine the effect of safe experience on a warning\'s impact by comparing warnings received after having safe personal experience with those received before people start making choices. A series of five experiments studies this question with a paradigm that combines both descriptive information (i.e. the warning) and experiential information (safe outcomes). The results demonstrate two separate advantages to an early warning that go beyond the warning\'s mere informational content. When an early warning coincides with the beginning of a decision-making process, the warning is both weighted more heavily in future decisions (the Primacy Effect) and induces safer behavior that becomes the status quo for future choices (the Initial History Effect). While both effects operate indirectly through choice inertia, the primacy effect also operates directly on choices. This pattern of behavior is inconsistent with the ""ideal"" Bayesian for whom the order of information revelation does not influence subsequent behavior. The effect was robust across settings with and without forgone payoffs and when the consequences for risk taking are delayed until the end of the experiment. The results imply that, even after being adequately warned, some people may continue to take risks simply because they incurred good outcomes from the same choice in the past. Implications for policy and theory are discussed.\nDirected Altruism and Enforced Reciprocity in Social Networks\n(with Markus Mobius, Tanya Rosenblat and Q.A. Do) QJE (2009)\nAbstract: We conduct online field experiments in large real-world social networks in order to decompose prosocial giving into three components: (1) baseline altruism toward randomly selected strangers, (2) directed altruism that favors friends over random strangers, and (3) giving motivated by the prospect of future interaction. Directed altruism increases giving to friends by 52 percent relative to random strangers, while future interaction effects increase giving by an additional 24 percent when giving is socially efficient. This finding suggests that future interaction affects giving through a repeated game mechanism where agents can be rewarded for granting efficiency enhancing favors. We also find that subjects with higher baseline altruism have friends with higher baseline altruism.\nWhat Do We Expect From Our Friends?\n(with Markus Mobius, Tanya Rosenblat and Q.A. Do) JEEA (2010)\nAbstract: We conduct a field experiment in a large real-world social network to examine how subjects expect to be treated by their friends and by strangers who make allocation decisions in modified dictator games. While recipients\' beliefs accurately account for the extent to which friends will choose more generous allocations than strangers (i.e. directed altruism), recipients are not able to anticipate individual differences in the baseline altruism of allocators (measured by giving to an unnamed recipient, which is predictive of generosity towards named recipients). Recipients who are direct friends with the allocator, or even recipients with many common friends, are no more accurate in recognizing intrinsically altruistic allocators. Recipient beliefs are significantly less accurate than the predictions of an econometrician who knows the allocator\'s demographic characteristics and social distance, suggesting recipients do not have information on unobservable characteristics of the allocator.\nThe Role of Experience in the Gambler\'s Fallacy\n(with Greg Barron) JBDM (2010)\nAbstract: Recent papers have demonstrated that the way people acquire information about a decision problem, by experience or by abstract description, can affect their behavior. We examined the role of experience over time in the emergence of the Gambler\'s Fallacy in binary prediction tasks. Theories of the Gambler\'s Fallacy and models of binary prediction suggest that recency bias, elicited by experience over time, may play a significant role. An experiment compared a condition where participants sequentially predicted the colored outcomes of a virtual roulette wheel spin with a condition where the wheel\'s past outcomes were presented all at once. In a third condition outcomes were presented sequentially in an automatic fashion without intervening predictions. Subjects were yoked so that the same history of outcomes was observed in all conditions. The results revealed the Gambler\'s Fallacy when outcomes were experienced (with or without predictions). However, the Gambler\'s Fallacy was attenuated when the same outcomes were presented all at once. Observing the Gambler\'s Fallacy in the third condition suggests that the presentation of information over time is a significant antecedent of the bias. A second experiment demonstrated that, while the bias can emerge with an all-at-once presentation that makes recent outcomes salient (Burns and Corpus, 2004), the bias did not emerge when the presentation did not draw attention to recent outcomes.\nKidneys For Sale: Who Finds That Repugnant, and Why?\n(with Alvin E. Roth) AJT (2010)\nAbstract: The shortage of transplant kidneys has spurred debate about legalizing monetary payments to donors to increase the number of available kidneys. However, buying and selling organs faces widespread disapproval. We survey a representative sample of Americans to assess disapproval for several forms of kidney market, and to understand why individuals disapprove by identifying factors that predict disapproval, including disapproval of markets for other body parts, dislike of increased scope for markets and distrust of markets generally. Our results suggest that while the public is potentially receptive to compensating kidney donors, among those who oppose it, general disapproval toward certain kinds of transactions is at least as important as concern about specific policy details. Between 51% and 63% of respondents approve of the various potential kidney markets we investigate, and between 42% and 58% want such markets to be legal. A total of 38% of respondents disapprove of at least one market. Respondents who distrust markets generally are not more disapproving of kidneymarkets; howeverwe find significant correlations between kidney market disapproval and attitudes reflecting disapproval toward certain transactions - including both other body markets andmarket encroachment into traditionally nonmarket exchanges, such as food preparation.\nNorms and Contracting\n(with Judd Kessler) MS (2012)\nAbstract: We argue that very incomplete contracts do more than their enforceable components imply, they can also induce relationship-specific norms. We find experimentally, across four games, that the most effective contract always includes an unenforceable ""handshake"" agreement to take the first best action. In three games, a totally unenforceable contract consisting of only a handshake agreement is (at least weakly) optimal. These results are particularly strong in games with strategic complements, where even selfish subjects increase their actions. Our results highlight an alternative explanation for contractual incompleteness: establishing a norm can effectively substitute for weak enforceable restrictions.\nContractual and Organizational Structure with Reciprocal Agents\n(with Florian Englmaier) AEJ-Micro (2012)\nAbstract: Empirically, compensation systems generate substantial effort despite weak monetary incentives. We consider reciprocal motivations as a source of incentives. We solve for the optimal contract in the basic principal-agent problem and show that reciprocal motivations and explicit performance-based pay are substitutes. A firm endogenously determines the mix of the two sources of incentives to best induce effort from the agent. Analyzing extended versions of the model allows us to examine how organizational structure impacts the effectiveness of reciprocity and to derive specific empirical predictions. We use the UK-WERS workplace compensation data set to confirm the predictions of our extended model.\nWhy Do Firms Use Non-Linear Incentive Schemes? Experimental Evidence on Sorting and Overconfidence\n(with Ian Larkin) AEJ-Micro (2012)\nAbstract: Non-linear incentive schemes are commonly used to determine employee pay, despite their distortionary impact. We investigate possible reasons for their widespread use by examining the relationship between convex pay schemes and overconfidence. In a laboratory experiment, subjects chose between a piece rate and a convex pay scheme. We find that overconfident subjects are more likely than others to choose the convex scheme, even when it leads to lower pay. Overconfident subjects also persist in making the mistake despite clear feedback. These results suggest non-linear pay schemes may help companies select and retain overconfident workers, and may reduce the wage bill.\nContracts, Biases and Consumption of Access Services\n(with Ozge Sahin) (forthcoming)\nAbstract: We study theoretically and empirically the consumption of access services. We demonstrate that consumption is affected by contract structure (pay-per-use vs. three part tariffs) even if the optimal consumption plans are identical. We find that a majority of individuals correctly use a threshold policy that is similar to a nearly optimal heuristic, however they use the free units too quickly leading to overconsumption and lost surplus. These errors are partially driven by mistaken beliefs about the value distribution. We also measure subjects\' willingness to pay for a contract with free access units, and we find that nearly half of subjects are willing to pay at least the full per-unit price, with a substantial fraction willing to overpay. In response, the optimal firm strategy offers a three-part tariff at a very small discount, which increases revenue by 8-15% compared to only offering a pay-per-use contract.\nTemptation in Vote-Selling: Evidence from a Field Experiment in the Philippines\n(with Allen Hicken, Nico Ravanilla and Dean Yang) (2014)\nAbstract: We test the predictions of a behavioral model of transactional electoral politics in the context of a randomized anti-vote-selling intervention in the Philippines. We model selling one’s vote as a temptation good: it creates positive utility for the future self at the moment of voting, but not for past selves who anticipate the vote-sale. We also allow keeping or breaking promises regarding vote-selling to affect utility. Voters who are at least partially sophisticated about their vote-selling temptation can thus use promises not to vote-sell as a commitment device. An invitation to promise not to vote-sell is taken up by a majority of respondents, reduces vote-selling, and has a larger effect in electoral races with smaller vote-buying payments. The more effective promise treatment reduces vote-selling in the smallest-stakes election by 10.9 percentage points. Inviting voters to make another type of promise – to accept vote-buying payments, but to nonetheless “vote your conscience” – is significantly less effective. The results are consistent with voters being partially (but not fully) sophisticated about their vote-selling temptation.\nCapacity Investment in Supply Chains: Contracts and the Hold-Up Problem\n(with Andrew Davis) (2014)\nAbstract: Suppliers are often reluctant to invest in capacity if they feel that they will be unable to recover their initial investment costs in subsequent negotiations with buyers. In theory, a number of different coordinating contracts can solve this form of hold-up problem and induce first best investment levels by the supplier. In this study, we experimentally evaluate the performance of these contracts in a two-stage supply chain. We develop a novel experimental design where retailers and suppliers bargain over contract terms, and both roles have the ability to make multiple back-and-forth offers while also providing feedback on the offers they receive. Our main result suggests that an option contract is best at increasing investment levels, and thus increasing overall supply chain profits. Furthermore, after investigating the evolution of offers during bargaining, we observe that participants tend to place particular emphasis on ""superficial fairness."" Specifically, participants focus more on setting a wholesale price that is in the middle of the available contracting space, while largely ignoring the coordinating contract parameter. We show that this behavioral tendency drives the favorable performance of the option contract, as there is a large set of coordinating terms which, conditional on having a superficially fair wholesale price, generate the proper incentives for suppliers to invest, and thus increase the total expected supply chain profit.\nProcedural Fairness and the Cost of Control\n(with Judd Kessler) (2014)\nAbstract: A large and growing literature has demonstrated that imposing control on agents has the potential to backfire, leading agents to withhold effort. Consistent with principles of procedural fairness, we find that the way in which control is imposed — in particular whether control is imposed symmetrically on both principals and agents and whether both parties have a say in whether control is imposed — affects how agents respond to control. In our setting, control leads agents to withhold effort only when control is imposed unilaterally with an asymmetric affect on the agent.\nA Meeting of the Minds: Contracts and Social Norms\n(with Erin Krupka and Ming Jiang) (2014)\nAbstract: Using coordination games we elicit social norms directly for two different games where either an agreement to take the first best action has been reached or where no such agreement exists. We combine the norms and choice data to predict changes in behavior and demonstrate that including social norms as a utility component significantly improves predictive performance. We estimate that honoring an agreement in the Double Dictator Game is worth giving up approximately 20% of their earnings, and more than 300% in the Bertrand Game. We show that informal agreements affect behaviour through their effect on social norms.\nBargaining in Supply Chains\n(with William Lovejoy) (2013)\nAbstract: We study experimentally bargaining in a multiple-tier supply chain with horizontal competition and sequential bargaining between tiers. Our treatments vary the cost difference between firms in tiers 1 and 2, with larger cost differences reflecting increased bargaining power. We measure how these underlying costs influence the efficiency, negotiated prices and profit distribution across the supply chain, and the extent to which these outcomes are influenced by personal characteristics such as risk aversion, altruism, selfishness, inequity aversion and social welfare concerns. We find that Retailer profits are hurt by decreased competition in either the Manufacturer or the Supplier tier. Additionally, Manufacturers and Suppliers benefit by decreased competition in their tier, while Manufacturers are hurt by decreased competition in the Supplier tier. We find that the Balanced Principal model of supply chain bargaining does a good job explaining our data, and significantly out performs the common assumption of leader-follower negotiations. Finally, we find that the structural issue of cost differentials dominates personal characteristics in explaining outcomes.\nGift Exchange in the Lab - It is not (only) how much you give...\n(with Florian Englmaier) (2012)\nAbstract: An important aspect in determining the effectiveness of gift exchange relations is the ability of the worker to ""repay the gift"" to the employer. To test this hypothesis, we conduct a real effort laboratory experiment where we vary the wage and the effect of the worker\'s effort on the manager\'s payoff. Furthermore we collect additional information that allows us to control for the workers\' ability and whether they can be classified as reciprocal or not. Our agency model of reciprocal motivation predicts high and low ability workers are differently affected by our experimental variations. These predictions are borne out by our results. Furthermore, we document that exactly those individuals we classify as reciprocal are the ones driving these results.\nManagerial Payoff and Gift Exchange in the Field\n(with Florian Englmaier) (2012)\nAbstract: We conduct a field experiment where we vary both the presence of a gift exchange wage and the effect of the worker\'s effort on the manager\'s payoff. The results indicate a strong complementarity between the initial wage gift and the agent\'s ability to ""repay the gift"". We collect information on ability to control for differences and on reciprocal inclination to show that gift exchange is more effective with more reciprocal agents. We present a simple principal-agent model with reciprocal subjects that motivates our empirical findings. Our results offer an avenue to reconcile the recent conflicting evidence on the efficacy of gift exchange outside the lab; we suggest that the significance of gift exchange relations depends on details of the environment.\nDoes Experience Always Pay? Welfare and Distribution Effects in Games with Heterogeneously Experienced Players\n(with Robert Slonim) 2006']"	['<urn:uuid:8099ad7d-551e-4a12-afc6-292e8012d0ee>', '<urn:uuid:3aef2478-dc7b-4976-b098-3eace4e17205>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T19:58:41.790432	7	93	4653
22	polarized light art creation process materials and display methods evolution from early work to current techniques	The creation process of polarized light art has evolved significantly. In early work, the artist used cracker wrappers, candy wrappers, and Scotch tape. Currently, sheets of cellophane cut to shapes are used, with artwork sizes ranging from 3x3 inches to 60x90 inches. The creation starts with a line drawing that becomes a pattern, then pieces of cellophane and birefringent materials are cut into tiny shapes following the drawing lines, sometimes using multiple layers at varying angles. For display, there are several types: some work like traditional paintings or stained glass without special mechanisms, while others are interactive and viewed through filters or polarized sunglasses. Some pieces change automatically using either a rotating polarizing filter and motor or a special liquid crystal panel with electronic circuits. Without polarizing light, the works appear as neutral gray sketches or clear Plexiglas, but transform into colorful moving pictures when properly illuminated.	"['Artist breaks barriers by painting with light\nThe world of art is changing.\nAnd then there\'s Polage.\nAustine Wood Comarow\'s paintings in polarized light, a medium in which she has been working for more than four decades, go on view Saturday in a retrospective exhibit at Lakeview Museum.\nThe Las Vegas artist will be in Peoria this week to oversee the installation of her unique work.\nWood creates a palette of pure, bright colors using light with cellophane and polarizing filters. Not surprisingly, the filters also can change the angles of viewing and affect the content.\nTo see one of her drawings without the polarizing light is to see a neutral gray sketch - but that all changes with the polarizing filters, and the drawings become colorful moving pictures.\n""Months of laborious care go into making a single major artwork, which, when finished, appears to be nothing more than a flat piece of clear, colorless Plexiglas, with some opacity added, due to an almost unnoticeable, seemingly sanded and lightly etched jigsaw patterning on the work\'s surface,"" James Mann, curator-at-large for the Las Vegas Art Museum, explains in a catalog that accompanies the exhibit.\n""The piece looks like a blank slate, a true tabula rasa, and yet its surface possesses the potential to unlock and release a picture (or pictures) with more shades, patches or bursts of color than a Pointillist painting.""\nIn her earliest work, Comarow used cracker and candy wrappers and Scotch tape. Now she uses sheets of cellophane cut to shapes.\nSome of her early works are 3 inches by 3 inches, while some of the most recent are 60 inches by 60 inches. She\'s even created a piece that\'s 60 inches by 90 inches, although it won\'t be on display here.\nShe also uses back lighting and reflective techniques in some of her pieces.\nWhen Comarow exhibited a few pieces at Lakeview in May 2007 as part of the ""Trompe L\'Oeil: The Art of Illusion"" show, she displayed some of her clear Plexiglas works that changed color and content when viewed through special filters.\nFor that show, Maui Jim sunglasses were made available to viewers. This time, a sample painting in the Lakeview lobby and a large pair of sunglasses will allow visitors to experiment with the brilliance of colors and changing forms.\nA lover of nature, Comarow incorporates many images of the natural world in her paintings and then bathes them in light. One series depicts the Greek goddess Gaia as Mother Earth.\nComarow was born in Louisville, Ky. Her father was in the foreign service and the family traveled around a lot.\nShe did her earliest light painting 42 years ago and exhibited in the Boston Museum of Science in 1986. More recently her work has been on view at the Las Vegas Art Museum.\nMann calls Comarow\'s kinetic light paintings, made possible by scientist Edwin Land\'s development of light-polarizing, synthetic filters 70 years ago, ""a revolutionary new art.""\n""They are, without exaggeration and improbable as it may seem, something quite wholly new in the history of visual art,"" he writes.\nTheo Jean Kenyon can be reached at (309) 686-3190 email@example.com.', 'FREQUENTLY ASKED QUESTIONS\nPolage Art & Austine Wood Comarow\nWhere does the color come from?\nThere are no colors in the raw materials, just as there is no color in a prism. It is only the interaction of light that is polarized passing through the molecules of “optically active” (birefringent) materials such as cellophane. Linear polarized light beams are all vibrating in parallel. When a polarized beam enters a cellulose molecule, part of the energy of the beam rotates. But, when light travels through such an optically active material, The energy vibrating on one direction travels through the molecule faster than the energy vibrating in a different direction. These portions of the light beam interfere with each other and create elliptically polarized light. This light appears colorless when viewed with the naked eye, but when viewed through a linear polarizing filter, such as polarized sunglasses, appears to have a color. That color shifts as the polarizer is rotated.\nI rarely think about the physics of the materials I use any more than a painter thinks about the chemistry of paints. But many scientists and engineers are fascinated with my work and have become avid collectors. Some physics departments have bought my art to demonstrate principles of light. For an example, for an advanced discussion of the physics of my art, please read an article by Donald H. Lyons, Professor Emeritus, Department of Physics, University of Massachusetts.\nHow do you display a Polage® art work?\nThere are several ”types” of Polage art that have evolved over the years. Some are just like a painting or stained glass and require no special interaction or mechanism. Some, perhaps the most fun, are interactive and are viewed though a filter or polarized sunglasses. Some change constantly and automatically by themselves. Those use either a rotating polarizing filter and motor or a special liquid crystal panel and electronic circuit. Many of my motorized pieces can be lifted off the light box, turned around and shown interactively. Be sure to switch the motor off when the image is the brightest.\nHow do you design a Polage® art work?\nI constantly sketch in a sketchbook that is always with me. Since my art always starts with a line drawing, I adapt, combine and otherwise integrate my sketches into a “cartoon” which becomes the pattern for a Polage artwork. I put the full size drawing under a sheet of glass or acrylic (Plexiglas®) onto which I have adhered a sheet of polarizing filter. Then I use pieces of cellophane and other special birefringent materials drawn from an array of different thicknesses to cut tiny shapes, following the lines of my drawing. I may use multiple layers, some at varying angles to each other. If there are to be silhouettes, I layer cut-outs of polarizing filter over the collage of clear birefringent pieces. I often have a copy of the drawing onto which I record the details of what I did so that I can refer to the notes to make another piece using the same drawing. That is how I have made “editions” over the years, although most of the work I do now are one of a kind (with the exception of the commissions I do for Maui Jim). Finally, I laminate a proprietary protective layer over the entire surface to protect it from moisture and UV light.\nMy Polage® art isn’t changing in the lightbox any more.\nFirst, be sure the motor switch is turned on. It is very common for this switch to be turned off by accident. Some light boxes have a small black toggle switch on the back. Some have switches on the side or bottom. If you are certain the switch is turned on, listen closely to the light box and see if you can hear a difference between when the motor switch is on and when it is off. If you need a new motor, it is HIGHLY recommended you return the light box to the studio. We will completely refurbish the light box to the newest standards and be sure it is operating properly. While we discourage self-repair, some people are very handy and familiar with electrical wiring. If you absolutely want to replace the motor yourself, for 12 inch and 19 inch light boxes, motor replacement kits are available here.\nReplacing motors in larger light boxes is much more complex. Please call the studio if you have an issue with a light box over 19 inches in diameter. Liquid crystal Polage artworks must be returned to the studio for repair. Please call.\nMy very old Polage® art seems to be delaminating.\nSome very early work that used primitive materials can usually be fully restored even after suffering delamination or moisture attack. Please call or email us photographs of the damage and we can discuss what is needed for us to restore the work.\nDo you license your art for reproduction?\nAustine selectively licenses her art for reproduction and other uses. For information, please contact the studio at (702) 260-1600 Tuesday to Friday, 8 am to 3:30 pm Pacific Time. All of Austine’s art is protected by U.S. and International copyrights. The marks “Polage” and “Austine” are registered on the U.S. Trademark Office’s Principle register. Unauthorized reproduction is strictly forbidden. Links to videos and images on this website are permitted, however.\nDo you teach people how to make polarized light art?\nSorry, I do not currently give instruction . This may change in the future. If you are interested, please send me an email and I will put you on a list to notify if I decide to give classes or write a “How To” book.\nDo you have a patent on your techniques?\nThe basic Polage process is not patented, but there are certain aspects of it that are proprietary. My images are all protected under U.S Copyright law. I also have registered trademarks on the marks “Polage” and “Austine”. My husband is a Patent and Intellectual Property attorney and enforces my IP rights worldwide.\nI saw something like this in a sunglass store. Are those made the same way?\nI have a exclusive agreement with Maui Jim, Inc., manufacturer of high quality sunglasses. Since 1997, I have created a wide variety of original works for them that are displayed in optical retail shops throughout the world. I have also developed a method for making small, simple point of purchase displays that are visible only through polarized sunglass lenses.\nWhere can I buy a Polage® art work?\nAll my work is available through my website. Custom commissions may be arranged by calling the studio Tuesday through Friday, 8 am to 3:30 pm Pacific Time. I am also represented by a select group of fine art galleries. Please note, the price for my artwork is the same whether you purchase it here online or at a gallery.\nDo you ever hire assistants?\nOn very rare occasions I may hire art and technical assistants. However, all my current art assistants have worked for me for many years, some as long as 30 years. So, please do not expect any turn-over soon.\nDo you have light boxes that run on 220 volts?\nWe can make special light boxes that will operate outside the U.S. on 220 volts, 50 Hz.\nThe Liquid Crystal Polages will operate on either 110 volts or 220 volts.']"	['<urn:uuid:869649cd-6b82-4afc-8cf9-931e5eef45a1>', '<urn:uuid:ead5c319-05ba-4bac-8abc-d358b5bbadc9>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T19:58:41.790432	16	147	1756
23	Which came first: steam engines on farms or mass production?	Mass production came first, as it was used in Venice several hundred years before steam engines appeared in farming. The Venice Arsenal mass-produced ships using pre-manufactured parts and assembly lines, producing nearly one ship per day. In contrast, steam engines weren't used in farming until much later - they first appeared as stationary threshing machines on very large farms, and weren't self-propelled until Henry Ford encountered one as a twelve-year-old boy, which would have been in the late 19th century.	"['Mass production (also called flow production, repetitive flow production, series production, or serial production) is the production of large amounts of standardized products, including and especially on assembly lines. The concepts of mass production are applied to various kinds of products, from fluids and particulates handled in bulk (such as food, fuel, chemicals, and mined minerals) to discrete solid parts (such as fasteners) to assemblies of such parts (such as household appliances and automobiles).\nMass production of assemblies typically uses electric-motor-powered moving tracks or conveyor belts to move partially complete products to workers, who perform simple repetitive tasks. It improves on earlier high-throughput, continuous-flow mass production made possible by the steam engine.\nMass production of fluid and particulate matter typically involves pipes with pumps or augers to transfer partially complete product between vessels.\nMass production is capital intensive and energy intensive, as it uses a high proportion of machinery and energy in relation to workers. It is also usually automated to the highest extent possible. With fewer labour costs and a faster rate of production, capital and energy are increased while total expenditure per unit of product is decreased. However, the machinery that is needed to set up a mass production line (such as robots and machine presses) is so expensive that there must be some assurance that the product is to be successful to attain profits.\nOne of the descriptions of mass production is that the craftsmanship is in the workbench itself, not the training of the worker; for example, rather than having a skilled worker measure every dimension of each part of the product against the plans or the other parts as it is being formed, there are jigs and gauge blocks that are ready at hand to ensure that the part is made to fit this set-up. It has already been checked that the finished part will be to specifications to fit all the other finished parts - and it will be made more quickly, with no time spent on finishing the parts to fit one another. This is the specialized capital required for mass production; each workbench is different and each set of tools at each workbench limited to those necessary to make one part. As each of these parts is uniformly and consistently constructed, interchangeability of components is thus another hallmark of mass produced goods.\nMass production systems are usually organized into assembly lines. The assemblies pass by on a conveyor, or if they are heavy, hung from an overhead monorail.\nIn a factory for a complex product, rather than one assembly line, there may be many auxiliary assembly lines feeding sub-assemblies (i.e. car engines or seats) to a backbone ""main"" assembly line. A diagram of a typical mass-production factory looks more like the skeleton of a fish than a single line.\nThe economies of mass production come from several sources. The primary cause is a reduction of nonproductive effort of all types. In craft production, the craftsman must bustle about a shop, getting parts and assembling them. He must locate and use many tools many times for varying tasks. In mass production, each worker repeats one or a few related tasks that use the same tool to perform identical or near-identical operations on a stream of products. The exact tool and parts are always at hand, having been moved down the assembly line consecutively. The worker spends little or no time retrieving and/or preparing materials and tools, and so the time taken to manufacture a product using mass production is shorter than when using traditional methods.\nThe probability of human error and variation is also reduced, as tasks are predominantly carried out by machinery. A reduction in labour costs, as well as an increased rate of production, enables a company to produce a larger quantity of one product at a lower cost than using traditional, non-linear methods.\nHowever, mass production is inflexible because it is difficult to alter a design or production process after a production line is implemented. Also, all products produced on one production line will be identical or very similar, and introducing variety to satisfy individual tastes is not easy. However, some variety can be achieved by applying different finishes and decorations at the end of the production line if necessary.\nVertical integration is a business practice that involves gaining complete control over a product\'s production, from raw materials to final assembly.\nIn the age of mass production, this caused shipping and trade problems in that shipping systems were unable to transport huge volumes of finished automobiles (in Henry Ford\'s case) without causing damage, and also government policies imposed trade barriers on finished units.\nMass production was popularized in the 1910s and 1920s  by Henry Ford\'s Ford Motor Company, which introduced electric motors to the then-well-known technique of chain or sequential production and, in the process, began a new era often called the ""second industrial revolution."" Ford\'s contribution to mass production was synthetic in nature, collating and improving upon existing methods of sequential production and applying electric power to them, resulting in extremely-high-throughput, continuous-flow mass production, making the Model T affordable and, as such, an instant hit.\nAlthough the Ford Motor Company brought mass production to new heights, it was a synthesizer and extrapolator of ideas rather than being the first creator of mass production. Ships had been mass-produced using pre-manufactured parts and assembly lines in Venice several hundred years earlier. The Venice Arsenal apparently produced nearly one ship every day, in what was effectively the world\'s first factory which, at its height, employed 16,000 people.\nDuring the Industrial Revolution simple mass production techniques were used at the Portsmouth Block Mills to manufacture ships\' pulley blocks for the Royal Navy during the Napoleonic Wars. These were also used in the manufacture of clocks and watches, and in the manufacture of small arms.\nDuring the American Civil War the Springfield Armory started to mass produce guns, using interchangeable parts on a large scale. The interchangeable part in manufacturing gun was strongly advocated by Eli Whitney. For this reason, the term Armory practice is occasionally used to refer to mass production. Soon after the war the American System of Watch Manufacturing showed that these techniques could be successfully applied even when very high precision was required. Later, in the 1890s, dollar watches traded off lower precision for much lower manufacturing costs.\nTaking a look back at the history of American manufacturing, the key features of mass production were the perfect interchangeability of parts in the goods produced, long production runs and large quantity of outputs that were homogeneous. These key features were developed out of the earlier non-mechanized factory system known as the American system.\nWhile the preceding American system of manufacturing relied on steam power, mass production factories were electrified and used sophisticated machinery. Adoption of these techniques coincided with the birth of the second industrial revolution in the US and its emergence as the dominant industrial superpower in the 20th century. Countries that were quick to follow in its wake (e.g. Germany and Japan) enjoyed high rates of growth.\nFrench political thinker and historian Alexis de Tocqueville identified one of the key reasons mass production was able to succeed so quickly in America, namely that of the homogeneous consumer base. De Tocqueville wrote in his Democracy in America (1835) that ""The absence in the United States of those vast accumulations of wealth which favor the expenditures of large sums on articles of mere luxury... impact to the productions of American industry a character distinct from that of other countries\' industries. [Production is geared toward] articles suited to the wants of the whole people"".\n| This page is a candidate to be copied to the Simple English Wiktionary.\nThe information in this article appears to fit better in a dictionary than an encyclopedia. Wikipedia is not a dictionary, but Wiktionary is. Please make sure that this article fits the Wiktionary criteria for inclusion.If this article can be changed to be more than a dictionary entry, please do so and remove this message.\nMass production means to produce goods in large quantities, using machinery.', 'Have you heard the story of the man who almost made a fortune in the soft-drink industry? He invented 6-Up.\nAll right, I know it’s a very old joke, but it illustrates a point: there are a lot more near-misses in capitalism than bull’s eyes. Many of these near-misses come about through simple bad timing or bad luck (RCA’s SelectaVision, for instance, blown out of the water by the VCR and laser disc). Others result from technological overreach (Howard Hughes’s Spruce Goose).\nBut others happen because innovators fail to fully conceptualize the new technology they are dealing with and rely on models from the old technology they seek to replace. The first mechanical pencil sharpener was a Rube Goldbergian contraption that sought to imitate a human hand wielding a penknife. Not surprisingly it didn’t work very well.\nOr consider Henry Ford’s Fordson tractor. While Ford no more invented the tractor than he invented the automobile, his Fordson tractor, like his Model T, revolutionized an industry, brought a powerful new technology within the reach of millions, changed an age-old way of life forever, and had vast economic consequences.\nBut while the Model T made Ford one of the richest men in the world, the Fordson tractor was, finally, a financial failure. The reason it failed, perhaps, was that Henry Ford hated farming and focused too much on simply replacing the horse and not enough on what the horse actually did for the farmer.\nAmerican agriculture, from its beginning, had been different from its European antecedents. In Europe land was expensive and labor cheap; in America it was exactly the reverse. Because of this reality, early American farmers often had a startling lack of interest in husbandry but were very receptive to laborsaving machinery.\nAt the time of the Revolution, farmers still had little in the way of equipment unknown to the Romans two thousand years earlier. It was reckoned that two men and a boy, using two or three horses or twice as many oxen, could plow only an acre or two a day.\nBut as early as 1788 Thomas Jefferson was working on the right mathematical curve for a plow to turn the earth with maximum efficiency (his equation, elegance itself on paper, was not successful in the field). Most farmers continued to use simple wooden plows, while the wealthier could afford cast-iron ones.\nThen in 1819 Jethro Wood introduced cast-iron plows with replaceable parts, bringing them within reach of the average farmer. Two decades later John Deere introduced the steel plow, a great improvement on the cast-iron model and a capitalist bull’s eye of the first order. The John Deere Company used the motto “He gave to the world the steel plow” for well over a century (of course, as more than one farmer noted, “He may have given it to the world, but I had to buy mine!”).\nThere is much more to farming, to be sure, than plowing, and the Industrial Revolution also gave the farmer mechanical seed drillers, cultivators, reapers, and threshers. All this had a radical effect on productivity. In 1822 it had taken fifty to sixty man-hours to produce twenty bushels of wheat on an acre; by 1890 it took just eight to ten man-hours.\nThe number of “horse-hours,” however, had greatly increased, and by 1880 the number of horses and mules on American farms exceeded twelve million and was climbing quickly.\nAt that time the only alternative to the horse was steam. At first portable steam engines were employed on very large farms for threshing. In truth they were portable only in the sense that they were not permanently situated and could be moved, very slowly, by large teams of horses.\nSomebody soon had the idea of using a steam engine’s own power to move it from farm to farm. It was exactly such an engine that the young Henry Ford encountered one day when riding in a wagon with his father. It was the first self-propelled device that Ford had ever seen, and he was wild with excitement as only a twelve-yearold boy can be.\n“The engine had stopped,” Ford wrote half a century later, “to let us pass with our horses and I was off the wagon and talking to the engineer before my father...knew what I was up to.” The engineer cheerfully explained how everything worked and made a lifelong impression on Ford. Indeed, “from the time I saw that road engine...right forward to today, my great interest has been in making a machine that would travel roads.”\nSteam, however, was not well adapted to farm use. Its energy output is low per unit of weight, and thus steam engines capable of doing farm work were very heavy and expensive. Very few farmers could afford to own steam engines, and many could not even afford to rent them.\nWith all of steam’s disadvantages, when the internal-combustion engine began to approach practicality, it was soon adapted to farm use. The first gasoline-powered traction engine (a phrase shortened by 1900 to “tractor”) was built by John Froelich in 1892, four years before Henry Ford built his first automobile. Froelich’s engine, like most early prototypes, didn’t work very well in the real world, and Froelich soon disappeared from history.\nBut the major farm-equipment companies such as John Deere and International Harvester, along with a host of smaller companies, were soon experimenting and turning out internal-combustion tractors. The early gasoline tractors were largely modeled on the steam-powered ones they were beginning to replace. Thus they were large, heavy, clumsy, and expensive. Few farmers wanted or could afford them. Then World War I changed everything.\nAmerican farm prices soared as European grain production plunged and Russia’s huge grain exports were cut off. The demand for horses and mules, meanwhile, also increased vastly. The belligerents needed the animals to haul wagons and guns on the front lines, where they were slaughtered, like the soldiers, by the hundreds of thousands. With the price of horseflesh rising out of sight and money in their pockets from grain sales, more and more American farmers decided to try tractors. Henry Ford decided the time had come to produce one.\nIn 1915 he announced his plans for the Fordson tractor, saying he would sell it for $250. His new tractor was smaller than most then on the market, structurally much simpler, and specifically designed to be mass-produced—a tractor version of the Model T, in other words. It took Ford two years to get into production, and the initial price ended up at $750. Still, from the beginning sales were brisk. By March 1918 he was making eighty a day, and production hit three hundred a day by year’s end. In 1920 Ford boasted that he had sold one hundred thousand tractors, twice the total number that had been in use on American farms when the Fordson was introduced.\nAlthough already the largest manufacturer of tractors, Ford then decided to go after market share. In January 1922 he slashed prices, selling the Fordson for only $395, less than the price of a good team of horses. The other tractor manufacturers were stunned and had no choice but to meet his price. Many, Ford included, were losing $300 on every tractor they sold, and a good number vanished from the marketplace. International Harvester, however, did not.\n“Ford was backed,” Cyrus H. McCormick, grandson of the founder, explained, “by the most popular commercial name of the time and by the uncounted millions earned for him by his epoch-making car [but] he was trying to capture a business with which he had no previous contact. International Harvester had on its side many years of training gained from contact with farmers, less capital by far, and utter inexperience with defeat.” That contact with farmers and farm equipment was to prove crucial in the contest between Ford and McCormick for dominance in the tractor market.\nYou see, the old horse-drawn farm equipment had been powered by a bull wheel. That was a large wheel sticking out from the side of the equipment, armed with cleats that dug into the ground. As the horses pulled the equipment, the bull wheel, at least in theory, turned the machinery. When the soil was wet, however, the bull wheel often just slithered along without turning and had to be helped manually, an exhausting and sometimes dangerous job. Farmers hated bull wheels.\nBut the bull wheel, for all its inadequacy, was the best way there was for powering horsedrawn equipment. As one engineer of the time explained, “horses are obliged to transmit their power through the ground to the machinery they operate because of their inherent and unchangeable construction.”\nThe construction of tractors is not unchangeable, however, and in the cutthroat tractor market of the early 1920s International Harvester soon offered a vast improvement on the bull wheel, the power takeoff. The PTO is a rotating shaft powered by the tractor’s engine that can be connected to the equipment being pulled by the tractor. It has since been a feature of every successful tractor model.\nBeing a farm-equipment manufacturer, International Harvester soon had a line of equipment designed to work with the new, much more efficient, and far more reliable power source offered by its tractors. Farmers loved the PTO, and International Harvester quickly pulled ahead of Ford in tractor sales.\nWhen Ford produced his Model T, he produced a practical, affordable horseless carriage, all that the traveler needed to travel and just what the marketplace was looking for. It was one of the great capitalist bull’s eyes of history. His Fordson tractor, however, was no more than a practical, affordable horseless horse. That, in the end, was not quite enough.\nEven geniuses don’t hit the bull’s eye every time.']"	['<urn:uuid:441bf3b2-235b-4741-a6a5-792a4d4b5052>', '<urn:uuid:80db5a92-f7ff-4b38-8806-0ec2c911b32d>']	factoid	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T19:58:41.790432	10	80	2979
24	What safety features does the P38 have and how to keep it working well?	The P-38 incorporated advanced safety features including a de-cocker, firing pin block, and loaded chamber indicator, making it safe to carry with a chambered round and hammer down. To maintain optimal performance, the firearm requires regular cleaning after use, including field stripping and thorough cleaning of all components with appropriate solvents. Critical moving parts should be properly lubricated, but not over-lubricated as this can attract dirt. Regular inspection for wear and damage is essential, and any worn parts should be replaced promptly. The firearm should be stored in controlled conditions and test-fired regularly to ensure reliable operation.	"['By LTC Joel Johnston (Ret), US Army Ordnance Corps\nP-38 BYF 44 (Mauser 1944).\nMatching numbers, Original finish, Non-import.\nHistory: The P-38 is considered the first modern combat handgun. The design came immediately before WWII as a replacement for the aging Luger. The pistol was the first double action combat handgun and was produced during the war by Walther, Mauser, and Spreewerk. It had a de-cocker, firing pin block, and a loaded chamber indicator so it was safe to carry the pistol with a chambered round and the hammer down. British Special Forces often used captured P-38s due to their reliability.\nManufactures and Varieties: Wartime P-38s were made by 3 manufacturers; Walther, Mauser, and Spreewerk of Spandau Berlin. Walthers are marked on the slide with an ""AC"" and then the date. The same is true with Mausers which are marked with ""BYF"" and the date. Spreewerk was a bit more illusive and only marked their guns with ""CYQ"" and no date. The Spreewerks are the roughest finished of the lot. They often have rough machining marks and poor finish. As with most German war materiel, the quality products declined towards the end of the war. Walther made a post war pistol known as the P-1 starting in 1957 and they can be recognized by the Walther Banner and an aluminum frame vs the all steel frame of the WWII pistol.\n|AC over 42 marking on slide||BYF marking on slide.||CYQ marking on slide||JVD mag, Czech made||Walther mag mark||Military guns have 3 proofs.|\nSpecifics on Markings\nThese values are approximate and only for matching, original finish and non-import guns at 90%. I think collector and gun show prices are significantly higher than Blue Book. This is true with many martial arms on the market. There are other factors such as matching magazines and holsters that may drive a price higher.\nBuying Tips: Frankly, WWII mid to late war German P-38s are plentiful. if you are going to shoot the rifling out of the gun or install a blank adapter, then by all means buy a mis-matching import or a P-1. Otherwise, I advise not buy a mis-matched, re-finished, or import marked gun. There are just too many nice ones out there to settle for second best. Besides, a quality gun always appreciates; shooters on the other hand remain shooters.\nNon-matching refers to the serial numbers. On most wartime German guns, the last two digits of the receiver serial number is stamped on each and every part. On a P-38, the entire number is stamped on the frame, slide, and the front of the barrel. The locking lug often is numbered with the last two or three digits of the serial number. Unlike a Luger that has every single part numbered, getting a matching P-38 is not that difficult since there are only 3 major and 1 minor numbered parts. Incidentally, while the Germans were furiously stamping the numbers on each and every gun part, the Americans were turning out 65,000 M-1 carbines per day.\nCurrently, there are many ""Russian Capture"" P-38s on the market. Many of these were re-finished and have an import company stamp on the frame. Look for these marks! They may be on the side or on the backstrap. Also look at the slide rails! This is another area they stamp import marks and it will certainly hurt the value. Russian captures can be had in all varieties for $500-$600. P1s are $250-$350.\nClick to enlarge and print the Exploded Disassembly Diagram\n(Make Sure Your Weapon is Unloaded first!)\n|1||Put weapon on safe. Lock the slide to the rear, remove your magazine.|\n|2||Inspect the chamber to ensure the pistol is unloaded.|\n|3||Rotate the barrel retaining latch to the down position.|\n|4||Release the slide lock and move the slide/barrel assembly off of the frame.|\n|5||Push the locking block operating pin, pushing the locking block up, and releases the barrel from the slide.|\nPistol is field stripped for cleaning.\n|7||Push the locking block operating pin, pushing the locking block up, allows barrel to fit back into the slide|\n|8||Fit rails of slide onto the pistol frame. Push down locking block (extended operating pin) to clear the frame.|\n|9||Slide barrel slide assembly completely onto frame.|\n|10||Make sure the firing pin lock lifter and other dog legs are pushed down to clear slide.|\n|11||Lock slide to rear by pushing up on slide lock.|\n|12||Rotate the barrel retaining latch to the up position.|\nDisclaimer: Ol\' Army Joel accepts no responsibility for accidents involving improper handling of firearms. Virtual Arms Room is no substitute for expertise and gun competence.', 'Ensuring that your firearm is in top shape is essential for maintaining optimal performance and, most importantly, ensuring safety? Regular maintenance is key to the longevity and reliability of your firearm. One of the first steps in keeping your firearm in peak condition is to thoroughly familiarize yourself with its manual. The manufacturer’s instructions provide valuable insights into the specific needs and recommendations for your firearm model. Cleaning your firearm is a fundamental aspect of maintenance. Residue from firing can accumulate in various parts, affecting both the functionality and accuracy of the weapon. After each use, it is crucial to field strip your firearm and clean it thoroughly. Start by removing the slide or bolt, and the barrel, and carefully clean each component with appropriate solvent and cleaning tools. Pay close attention to the bore, ensuring it is free from any debris that may have accumulated during use. Neglecting to clean your firearm can lead to corrosion, which can compromise the integrity of the weapon.\nIn addition to regular cleaning, inspecting your firearm for wear and tear is vital. Check for any signs of damage, rust, or excessive wear on the barrel, slide, and other critical components. A damaged or worn-out part can jeopardize the firearm’s reliability and accuracy. Replace any damaged parts promptly, and keep spare parts on hand to address issues quickly. Regularly inspect the magazine for any cracks or deformities that could hinder its proper functioning. Proper lubrication is another key element in maintaining your firearm’s optimal performance. Apply a high-quality firearm lubricant to critical moving parts such as the slide rails, barrel hood, and trigger mechanism. This not only reduces friction but also helps protect against corrosion. However, it is important not to over-lubricate, as excess oil can attract dirt and debris, leading to potential malfunctions. Storage conditions play a crucial role in preserving the integrity of your firearm. Store your firearm in a cool, dry place, away from extreme temperatures and humidity.\nInvesting in a quality gun safe provides an added layer of protection against environmental factors and unauthorized access. Regularly check the condition of your firearm while in storage to ensure it remains in optimal condition. Regularly test-fire your firearm to ensure it functions as intended. This practice not only helps you stay familiar with the weapon but also allows you to identify any potential issues before they become critical. If you notice any changes in accuracy or performance, address them promptly to maintain the firearm’s reliability. Lastly, stay informed about any recalls or updates from the manufacturer and Contact us. Manufacturers occasionally discover issues or improvements that may require attention. Staying proactive and addressing these matters promptly ensures that your firearm remains safe and performs at its best. Keeping your firearm in top shape for optimal performance requires a combination of regular cleaning, inspection, lubrication, proper storage, and proactive awareness of manufacturer updates. By investing time and effort into these maintenance practices, you not only extend the life of your firearm but also ensure its reliability when it matters most.']"	['<urn:uuid:c1db657f-d35d-4aa3-9ee3-1a596262ad95>', '<urn:uuid:306445cf-492a-4308-b181-e9aee9819c4f>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:58:41.790432	14	97	1271
25	How do therapeutic massage techniques differ between general practice and cancer care?	In general practice, massage techniques like tapotement are used for muscle energizing and re-invigoration, applied rhythmically at 4-10 strikes per second for 1-10 minutes, but must avoid bony areas and compromised tissues. In cancer care settings, massage is specifically integrated as a complementary therapy, not an alternative treatment, with techniques like Swedish massage and reflexology proven to reduce pain when used as part of a comprehensive pain management plan. These oncology treatments require certified professionals with specific cancer patient experience and must be approved by the treatment team.	"['Tapotement is a massage technique with a wide range of applications. At the end of a relaxing Swedish massage, it can be used to re-invigorate and ground the client before they leave. It is also the most frequently used technique to energise the muscles and the nervous system in pre-event Sports massage. In Chinese medicine and other medical settings, it is often used to loosen up mucus in the lungs. The name comes from the French word tapoter, which means ‘to tap’ or ‘to drum’.\nFingers, cupped hands or loosely held fists or the edge of the hand are used to apply rhythmical percussion strokes. Tapotement is usually performed alternating hands and maintaining a fast pace of between four to ten strikes per second. Sixty seconds tends to be the minimum threshold where tapotement’s effects kick in. Therefore, the aspiring therapist must moderate their pace so that she can maintain the strokes for at least a minute. Over time, this can be increased to four to ten minutes spread over two, three sessions or delivered at once.\nIt is worth emphasising that the therapist applies tapotement from their hands and wrists while keeping them relaxed. Engaging the elbow or the shoulder is not correct, causes unnecessary stress on the therapist’s body and can lead to exhaustion and injuries. Tapotement can be exhausting if not performed with proper body mechanics. Even when applied correctly, it is usually only used for short periods to avoid muscle fatigue.\nSuccessful tapotement requires a degree of surrendering on the side of the practitioner. As they let go of their thinking mind, the body naturally finds the rhythm for an enjoyable massage experience.\nAccording to the way it is applied, tapotement can be categorised as follows:\nTapotement is an effective and an established massage technique that can be easily overlooked if its benefits are not clearly understood.\nIt is advisable to avoid tapotement over bony areas and only use it over fleshy parts of the body. The gastrocnemius and soleus muscles in the posterior lower leg are good candidates, while the anterior tibia (the shin on the opposite side) is not.\nPercussion movements directly over the kidneys are usually not recommended. As a counter argument, it may be conceived that tapotement in that area can help loosen or even trigger the release of a kidney stone. Although this can be very uncomfortable in the short-term can enhance the client’s well-being in the long-run.\nHypersensitive areas or compromised muscle tissues should be avoided. The same precaution applies to varicose veins and abnormally contracted, strained or inflamed tissues.\nFractured areas are also to be avoided at all costs. Finally, popliteal space (back of the knee), the head, the neck and the spine should not be treated with tapotement as well.\nTapotement is an exceptionally versatile tool and a great complement to effleurage, petrissage, and friction in any therapist’s arsenal. It increases the benefits for the client but also helps them get energised and grounded towards the end of the session if this is desirable. Finally, it helps add variety to the work of the therapist making it more engaging.\nMassage techniques: effleurage and petrissage\nMassage techniques: friction\nMartin Stefanov Petkov\nMaster your Super Power', 'Integrative oncology approaches\nIntegrative oncology addresses symptom control with therapies other than pain medications or procedures. These therapies historically have not been part of mainstream Western medicine. They belong to a group of therapies called complementary therapies, which are used by as many as 40 percent of people with cancer. However, it is essential to understand the difference between “complementary” and “alternative.” Complementary therapies are used to control symptoms and are used in combination with traditional cancer treatments; in contrast, alternative therapies are used instead of traditional cancer treatment, with the hope that they will cure the disease. Integrative oncology includes only complementary therapies that studies have shown to be beneficial and safe. It does not include alternative therapies.\nThe most commonly used integrative therapies for cancer-related pain include mind-body therapies, acupuncture, and manipulative and body-based therapies.\nThe mind-body connection is real and can be powerful. With mind-body therapies, the brain, mind and body interact to promote health and well-being. These therapies have been found to be beneficial for people with cancer by improving quality of life, which can help reduce feelings of pain. Mind-body therapies include mindfulness meditation, guided imagery, yoga, tai chi and qigong.\nAcupuncture is the Chinese art of placing special needles in certain parts of the body; sometimes heat or electric pulses are applied to the needles. Studies have shown that acupuncture is effective in reducing pain in some people with cancer, and the therapy is generally safe.\nManipulative and body-based therapies\nManipulative and body-based therapies include massage therapies of various techniques, such as Swedish massage, shiatsu, tui na, reflexology, Ayurvedic massage, lymphatic drainage and myofascial release (see Table 1). These therapies, especially Swedish massage and reflexology, have reduced pain in people with cancer when used as one part of a pain management plan.\nTable 1. Types of massages\n|Ayurdevic massage||The entire body is vigorously massaged with large amounts of warm oil and herbs.|\n|Lymph drainage therapy||An extremity with lymphedema is massaged to move excess fluid toward alternative pathways for drainage.|\n|Myofascial release||Pressure is applied to areas in the foot, hands and ears that correspond to other parts of the body.|\n|Shiatsu||Pressure is applied with the finger, thumb, palm, elbow or knee to specific areas of the body.|\n|Swedish massage||Massage given with a combination of kneading, rolling, vibrational, percussive and tapping movements, with the application of oil.|\n|Tui na||Massage given with pressing, rubbing, waving, shaking, percussion or manipulation; can be applied to be light and soothing or strong and invigorating.|\nChoose therapies wisely\nComplementary therapies are safe when they are provided by certified and/or licensed professionals who have experience working with people who have cancer. Talk to your doctor or another member of your treatment team for a referral to a reputable professional.\nBeware of advertisements for diets or supplements that claim to help cure your cancer or relieve your cancer-related pain. These products have not been subjected to the rigorous scientific study that is necessary to ensure they are safe and effective for people with cancer. Talk to your doctor or other member of your treatment team before starting any special diets or taking any vitamin or herbal supplements.\nChoosing complementary therapies can help you take a more active role in your care. Many of them are also effective at reducing stress and anxiety, which can help you cope with pain. But remember the importance of choosing safe therapies (see below). In addition, remember that these therapies are best used in combination with an established treatment plan, not instead of traditional treatment.\nChoosing complementary therapies carefully\n- Find out what scientific studies have been done on the safety and effectiveness of the therapy.\n- Discuss the therapy with your doctor or other member of your treatment team before making a decision.\n- Choose a complementary therapy practitioner as carefully as you would choose a doctor. Use only licensed and/or certified practitioners who have experience with people with cancer.\n- Research dietary supplements or herbal products to make sure they do not interact with medications you take.\n- Beware of terms like ""scientific breakthrough,"" ""miracle cure,"" ""secret ingredient,"" or ""ancient remedy.""\n- Consult only reputable sources of information.\n- Tell your doctor about all the therapies you use.']"	['<urn:uuid:0718c566-3e89-4baf-8d71-579d45ae164b>', '<urn:uuid:be6a1dae-2fe0-4ef3-9df4-3093b8ea098c>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:58:41.790432	12	88	1238
26	shoe health trees vs high heels effects	Shoe trees and high heels have contrasting effects on foot health. Wooden shoe trees help maintain shoe shape and allow leather to breathe while drying, preventing deformation and maintaining the original form of footwear. In contrast, high heels can negatively impact foot health by inflaming the arch, causing toe stress leading to corns and calluses, and increasing fall risk by altering the line of gravity. The effects of high heels can extend beyond the feet to affect ankle joints, the Achilles tendon, balls of the feet, knees, and back.	['When it comes to maximizing the life of your shoes, there are three cornerstones: shoe care, shoe trees and rest. In this post I go through lots of info regarding one of these: the shoe trees.\nAs most people know, shoe trees are used primarily to help the shoes to retain their shape, and to contribute to let the shoes dry. The latter mainly if the blocks are made of wood, which they certainly should be. But it’s not as important that they are of cedar wood, as many seem to believe, any wood has decent absorbency and other common types of wood to the shoe trees are birch and maple. The main advantage of cedar is the pleasant aroma it spreads. What wood does is to absorb some moist, but the main thing really is that it let’s the leather breath while having the correct shape, most moist goes out on the outside, so to speak. Plastic trees of course don’t absorb anything, but what’s worse is that it closes the pores in the lining leather which is quite bad in the long run. Just picture yourself putting a plastic bag against your skin and keeping it there for hours. However, what is important is that the wood isn’t coated, then its absorption capacity is taken away, if they are stained however, it’s no problem, and sometimes it can be hard to tell the difference. Ask the sales person if you have any doubts.\nWhen you use the trees, make sure to put them in straight away, since it’s when the shoes are warm and very moist that the leather will shrink as most. Let the trees stay in the resting shoes for at least 24 hours. After the shoes are completely dry, the leather won’t change shape in the same way if you remove the trees, it’s when the leather is drying that it wants to contract itself. It’s of course always good to have shoe trees in all shoes, but it’s not necessary, if you only have one pair of trees and move them around to the last shoes used, that makes a very big difference compared to not using trees at all.\nIf you don’t use shoe trees, no matter how good the quality of the shoes are, in time they will look awful anyway. Below a picture which is a striking evidence of this:\nDifferent types of shoe trees\nThere are many different types of trees, that work in slightly different ways and in many cases are different good for your shoes. Here are a few examples.\nFrom the left:\n1. Spiral trees in wood. Possibly better than not using trees at all, but since this type of tree makes the pressure on the shoe vertical instead of horizontal they risk to damage and deform the shoe.\n2. Shoe tree where the toe is divided into two equal parts, but with a smaller heel. Main disadvantage of this model is that the heel does not get help to retain the shape.\n3. Shoe tree where the toe is divided into two equal parts, and with a large heel. A disadvantage is that this type of tree can not follow the shoe form further up towards the end of the tongue.\n4. Shoe tree where the toe part is divided into a large solid part and one smaller. Fill up large parts of the shoe well, a con being that the small part sometimes can miss-shape the upper especially at it’s back part.\n5. Lasted shoe trees. Trees that are shaped exactly the same as the lasts the shoes are built on, which has the advantage that the shoe will retain its original shape much better than with trees that are not adapted to the shoe.\n6. Non-lasted full shoe trees. Trees that are similar to nr. 4 but with a full piece of wood for the front part. Works very well especially if they are similar in shape to the last of the shoes. Picture: A Fine Pair of Shoes\n7. Hinged shoe trees. Here a hinge is used instead of the more common smaller spring parts. Typically only used for lasted shoe trees, since the fit needs to be very good for them to work well. Picture: Herring Shoes\n8. Tree-piece shoe trees. This is the Rolls Royce of the shoe trees, which is common among bespoke manufacturers, and basically does not exist in other than lasted versions. The advantage of these is that they fill out the entire shoe, all the way to the top of the tongue. Picture: Claymoor’s List\n9. Hollow shoe trees. If the ones above are the Rolls Royces, these might be called the Ferraris. Can only be hinged, so normally only lasted ones, and they probably require the highest amount of craftmanship. Pros is that they are good in helping the shoes to dry, since the moist can go through the thin amount of wood out in the air.\n10. Boot trees. Here the back piece is larger and higher, to work better with boots. To be frank these are rarely especially good, since the ankle part often doesn’t fill out the boot’s ankles at all anyway, and the most important part to retain the shape will always be the lower parts of the shoes and to flatten the sole.\nStraighten out the sole\nThe shoe trees task when it comes to restoring the shape of the shoe is partly to directly push out creasesby reasonably well by filling the shoe’s front parts, but most important is that the trees should straighten out the sole of the shoe. Both for the sole to not be bent upwards, something that is common with time, and that when the sole is stretched it also fold out the creases indirectly, so to speak, and the shoes regains its original shape. It’s therefore important to check how the shoe trees you are buying are shaped in front so that they have a good shape that flatten the shoes you will use them for(of course it is relative, some shoes have a higher toe spring from the start and is not similarly in need of “flat” shoe trees).\nShape of the heel\nAnother element that is important is that the heel has a good shape. As mentioned above, it is preferable that the block will fill the heel as well, but it should not be too large so that it miss-shapes it. It can eventually make the shoes loose in the heel.', 'How High Heels Can Negatively Affect The Feet\nThere are many women who enjoy wearing high heels despite knowing foot pain may develop from wearing these specific type of shoes. Research has shown that when the heel is high, the arch in the foot may become inflamed and this may typically cause pain and discomfort. If high heels are worn on a frequent basis, the toes may undergo stress, and this may possibly cause corns and calluses to develop. The likelihood of incurring a fall may increase as a result of the line of gravity being altered by the height of the heel. There are several ways to enjoy wearing high heels while keeping your feet as comfortable as possible. These may include slipping off your shoes at an appropriate time and performing gentle stretching exercises, including pointing, flexing and rolling your ankles. Additionally, circulation in the feet may be increased by wiggling the toes. Your feet may benefit by wearing shoes that are more comfortable the following day, which may allow the foot muscles to rest. Please speak to a podiatrist if you would like additional information concerning the dangers of wearing high heels.\nHigh heels have a history of causing foot and ankle problems. If you have any concerns about your feet or ankles, contact one of our podiatrists from Foot & Ankle Specialists of Nevada. Our Doctors can provide the care you need to keep you pain-free and on your feet.\nEffects of High Heels on the Feet\nHigh heels are popular shoes among women because of their many styles and societal appeal. Despite this, high heels can still cause many health problems if worn too frequently.\nWhich Parts of My Body Will Be Affected by High Heels?\n- Ankle Joints\n- Achilles Tendon – May shorten and stiffen with prolonged wear\n- Balls of the Feet\n- Knees – Heels cause the knees to bend constantly, creating stress on them\n- Back – They decrease the spine’s ability to absorb shock, which may lead to back pain. The vertebrae of the lower back may compress.\nWhat Kinds of Foot Problems Can Develop from Wearing High Heels?\n- Morton’s Neuroma\n- Plantar Fasciitis\nHow Can I Still Wear High Heels and Maintain Foot Health?\nIf you want to wear high heeled shoes, make sure that you are not wearing them every day, as this will help prevent long term physical problems. Try wearing thicker heels as opposed to stilettos to distribute weight more evenly across the feet. Always make sure you are wearing the proper shoes for the right occasion, such as sneakers for exercising. If you walk to work, try carrying your heels with you and changing into them once you arrive at work. Adding inserts to your heels can help cushion your feet and absorb shock. Full foot inserts or metatarsal pads are available.\nIf you have any questions please feel free to contact our offices located in Las Vegas and Henderson, Nevada . We offer the newest diagnostic and treatment technologies for all your foot and ankle needs.']	['<urn:uuid:ce9beb5c-84f6-4eda-84b9-f2a26ec4876f>', '<urn:uuid:2ecc6406-638b-48e8-a92c-468dcddf951e>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T19:58:41.790432	7	89	1603
27	What was unique about how Native Americans used their voices when singing sustained musical notes?	Indian singing had a distinctive feature that was almost universal - a rhythmic pulsation of the voice on sustained notes, similar to the effect produced on a violin when the same note is slightly sounded several times during one stroke of the bow.	['Like mothers everywhere they sang lullabies and they sang to ward off evil. The women sang as they scraped the skins, pounded the pemmican or prepared medicine. When the men were away hunting or at war they chanted and sang prayers for their safety and success. They mourned their dead, often for years. There were songs of greeting, or going, and of loneliness, and of joy.\nMen sang too – sang to the Great Spirit. Alone on a high place to greet the dawn or to give thanks for animals that had allowed themselves to be slain for food. There was the holy song sung privately, the song that had been given by the sacred animal – guardian, to be the “death song”, sung publicly only when death was at hand. There were the songs around the fire in the tipi at night when the elders taught the history and folklore of their tribe to the children, and kept history alive. For the Indian, truth, tradition, history and thought was preserved in the ritual of poetry and song. The songs record the teachings of wise men, the great deeds of heroes and the counsel of peers.\nComposers of great songs were honoured, for their words were poetry. The Indians spoke in poetry. To learn the very language, such as Cree, was “like learning to sing”, for the words were full of symbolism. They cannot easily be translated into English because many thought-patterns have no counterpart in English. Even those who know the language may not understand the songs unless they know what meaning lies behind the symbolic words. As well, the listener needs a knowledge of the event which called the song into being, the legend with which it is connected, or the ceremony of which it is a part. There were songs so ancient that even the Indians did not know the meanings of the words. Some, like war songs, were wordless or ended in meaningless but unchanging syllables like “we ho he ye ye he ye yo”, just as many white man’s songs end in syllables like “fa la la la la la lah”. There was social singing at the feasts of rejoicing over the birth of a child and the attaining of manhood and womanhood. Songs were sung at the great summer gatherings for the hand-games and gambling, and for the dances that renewed the unity of the clan and of the people with the Great Spirit. It is said, “To the Indian, song is the breath of the spirit that consecrates the acts of life. Not all songs are religious but there is scarcely a task, light or grave, scarcely an event great or small but has its fitting song.”\nThe Indian was very sensitive to natural things he saw about him. Seeing spectacular northern lights, a new waterfall, a view from a high place, or any other natural experience might move a man to compose a new song. Usually these personal songs were short, and impressionistic – rather like a Japanese haiku, but less structured – at least in translation, but with much repetition of lines.\nNothing that we have found to date in Canada does for Canadian Indian music what the late Natalie Curtis Burin’s The Indian Book (reprinted by Dover Publications, New York, 1963 from Harper Bros. 1923) does for the United States. In 1906, when she began the work that preserved nearly one hundred and fifty of the old songs, she needed the personal written permission of President Teddy Roosevelt to go onto American Reserves to record them. The Department of Indian Affairs had absolutely forbidden the singing of traditional music in Indian Schools. The older Indians were afraid to sing for her for fear of angering the authorities, and a scientist working on one reserve told her that if the government official heard her recording tribal music on her phonograph, (no tape recorders then) she would be expelled from the reserve.\n“The wealth of indigenous music, poetry, and legend was not only neglected, but was being rapidly obliterated by the steady pressure of the government’s effort to crush the Indian as rapidly as possible into the white man’s mold.”\nIn Canada, there was less active repression. However, since much Indian education took place in church-run schools, there was much attention to Christian hymns and chants. Several Indians have told this writer that “I know the whole church service in Latin but I don’t know a word of what it means.” However, until the advent of the radio and TV our northern Indian elders, having very little contact with the white people until after 1899, kept the old songs alive. The sociable Crees have mixed more with the white people than the Athapaskan speakers and have adopted more “white” songs, but they still remember their own.\nThe Beavers, Sikanni, Slaveys, Chipewyans and Dogribs are said to have more of the primitive style of singing but that, too, is passing. The Ridingtons, Professor Robin and his wife Tonia, have preserved on tape some of the Beaver music, and the voice of the last prophet, Charlie Yahey. Also, American researchers Lurie, Dorothy Durath, June Helm and Nancy Oestreich have made a study of the Dogrib tea dance and stick-game music.\nThis writer will attempt only to compile, as carefully as possible, general observations on Indian singing from published material that is not likely to be available to students in local libraries.\nIndian singing has melody which ranges between “a sort of falsetto tenor to a bass so low as to scarcely sound the pitch of any given note.”\nNatalie Cutis records that Indian tribes differ as widely in their music and in their manner of singing as in their life and customs. But there is one peculiarity that is almost universal. “This is a rhythmic pulsation of the voice on sustained notes somewhat like the effect produced on the violin when the same note is slightly sounded several times during one stroke of the bow.”\nAmong the Dogribs, only the men sang, but the professors noted “an extreme fondness for pulsation”, which they said, “is produced by a rhythmic propulsion of short breaths on one note in the manner of gasping. Dogrib men use three kinds – a melodic reiteration of “he he. . . ” in moderate tempo at the same speed as the drum, a melodic pulsation twice as fast as the drum, and an atonal low, rapid pulsation which is very soft and sounds like a chuckle. Some songs consist of pulsation and sound “like a flock of geese in flight.” Combine all of the above with “vocal embellishments” – strange throaty sounds slurs, staccato accents and, on some long notes, a tightening of the throat that produces a sort of quaver. Then one can understand why Natalie Curtis says that Indian music cannot be performed on a percussion instrument like a piano, but can be played best on a stringed instrument like a violin.\nIndian music does not employ harmony – there is no “part singing”, – all in unison. However the Metis children’s school choir from Kelly Lake had learned to sing eight-part harmony unaccompanied. The unusual and indescribably “different” quality of their rendering of English folk songs undoubtedly had something to do with their native singing style.\nIndian singing has rhythm. The Indian lived with rhythm — the pulsing of his own blood, the lapping of water and the running of the animals. The swing of the paddle, the pounding of the pemmican, the beat of wings, the rocking of a baby — all are part of the natural rhythm of their lives.\nNatalie Curtis, herself a musician, says “No … music has such complex and changing rhythm as has the music of the American Indian”. The Indian does not need a drum for singing and not even for dancing. It is said that “The Indian can dance without a drum, but no Indian can dance without a song.” The singers set the rhythm – the drummers follow, sometimes in 1/4 time but also in 1/8 – 3/8 or 3/4 time, as shown in the notation of Natalie Curtis. In 3/4 time, when it is counted “one (omit two) three”, it is called “thunder drumming”. Generally it is monotonous to our ears, but three or four times in each song the drummers accent the beat to keep the singers “on time”. On these accented notes the singers and dancers bow low to honour the drum.\nAmong our northern tribe the drum might be a hollow log with skin stretched over it and bound with rawhide. Generally it was a hoop with skin stretched taut over one side, but Charlie Yahey’s ceremonial drum is unusual in that is has two heads. Some of the northern Indians had strings of sinew stretched across the head to give the effect of a snare drum. The instrument could be struck with the hand, or a drumstick the end of which was a piece of buckskin stuffed and sewn like a ball. From time to time the drum had to be heated to restore its tone.\nNot all of the songs were accompanied by drums, – the tea dance, for instance, which was a feature of the big traditional summer meetings, was not accompanied by drums. The Dogribs had a repertoire of twenty-five songs, sung in any order chosen by the song leader, who had an important role in the tribe. Professors Lurie and Helm recorded the “their voices rise and fall in melodies and cadences of a compelling beauty once one’s ears become accustomed to them.” Dances seem to accompany all singing.\nWhenever there was drumming there was action, either in the motions of the body and arms when the men were seated at a hand-game, or, in the more usual way, in “steps” and postures. The Indian men being forbidden by tradition to show their emotions even in expression of the face, threw themselves into dancing as an emotional release that was as natural as breathing.\nJohn Frederic Gibson, in A Small and Charming World, records an incident to illustrate this. He had been recording the singing of an elderly man who remembered the old songs, although the people had adopted modern music for their dances own use. Gibson was testing the tape.\nAt the time, he wrote, “the children were coming home from school, wandering up the grass path in the summer heat. We could look out from the doorway and see the small figures [drawing] near. I switched on the tape recorder and listened again to the drum beat and the strong voice of the old man. The effect was electric; it was immediate. Everyone started to smile, and the children threw down their books and began to dance. They danced into the house so that the dust flew up from the wooden floor. Their bodies moved back and forth and their feet pounded in time with the drum. People came running over from the other houses and I felt as if the whole community was coming around after being anaesthetized. When I switched off the tape recorder the laughter continued like an echo and the children stared at one another as if wondering what had happened.”\nAnother incident of the almost hypnotic effect of drumming occurred at the opening of the Metis housing development at Chetwynd. A touring group of champion native dancers were performing. Near us an aged local Indian and his wife were standing. Only a tapping of their feet like a musician keeping time showed their response to the music. Then a solo-dancer began to perform the “Eagle Dance”. The rhythm of the drums, while still monotonously regular, took on a new urgency. Suddenly the old man’s face lit up and his whole body began to dance “in place”. It was not that he was following the same stylized steps and postures of the Eagle dancer. It was as if he was responding like a harmonic to a vibrating string. With rapt face, and eyes turned upwards he was performing as impromptu ballet which we watched with fascination, wishing with all our hearts that we had a movie camera, which of course, would have been an invasion of an old man’s private emotional world. Suddenly he became aware of what he was doing and stopped. The prize-winning performance on the platform, wild as it was when it approached the climax of the upswept “wings”, did not have the genuinely emotional pull of the old man’s spontaneous performance. As he stopped we realized that if “white” conditioning had not held us back, we would have been joining him!\nWe have found nothing describing the solo dances of the Peace River Indians. It is reasonably certain that the otherwise universal custom prevailed here. The shamans, at least, would have performed them, especially while trying to effect a cure. The custom of wearing caps made of the heads of animals, masks, beads, amulets, feathers and ornaments seems to have been universal. Skins of small animals, such as ermine, fastened to the clothing, was not just for whim or ornament, but symbolized events in which a knowledgeable person could read a story\nOther wild dances were the so-called “war-dances”. Before going out to battle, braves dressed themselves in their best clothing, painted their faces, and brandished their bows and arrows, but not to scare their enemies, as the white man assumed. The performance was essentially a prayer that they might die well and bring honour to the tribe. The Indian warrior did not aspire to long life. The greatest honour was to die in battle and have his body hacked in pieces; the greatest disgrace was to have one’s enemy show mercy. Further, the belief that one went through the hereafter attired as he left this world made him adorn himself with the symbols of his exploits.\nThe steps of the war-dance were “wild” and the actions spontaneous because the warriors were working themselves up to a high pitch of emotion much as the early Christians did by hymns and prayers before being thrown to the lions, for martyrdom was sure entrance to heaven. While the men were away, the women danced their own prayers for safety and victory.\nAfter the successful war party returned the real war dance or victory dance took place. Stories of courage were reenacted and thanks given to the Great Spirit. These dances were said to be more individual and innovative.\nOur country’s Indians never adopted the Sun Dance of the Plains, although the Sarcees did. The Kiowa Apaches (kindred Athapaskan speakers) are said to be the best dancers in the U.S.A. where Indian dancing is being revived and even taught in schools where dance competitions are held yearly. The old people deplore the fact that the dances have lost their religious feeling.\nThose who are researching the traditional Indian dances says that they are anything but the shuffle that most white people think of, and that is shown in movies and TV. To dance properly, the posture should be correct – “back flat, rump in, no hunching of the back, although a dancer may lean forward, – hands always relaxed, fingers half or completely closed, but never rigid. Every movement is controlled, not wild, and, while often fast, is always smooth except in war dance.”\nMary Weekes writes in Indians if the Plains that the dances of men were sometimes rapid and violent, while those of the women were less forceful. In general the men lifted the heel and ball of the foot, then slapped them down with great force and swiftness. The dance of the women, a peculiar rocking or swaying motion, has been likened to the waving of a wind-rocked stalk of corn.\nAs well as war dances, there were hoop dances, “sneak-up” dances, the “chicken dance”, “rain dance”, “ghost dance”, “pipe dance”, and many others.\nIn the Peace country the two most often mentioned are the drum dance, preceding a feast or gambling session, and a “tea dance” after the gambling.\nThe “drum dance” is done in a circle. The dancers face clockwise, and dance tight up against one another making it almost impossible to maintain any semblance of a step and even to keep the beat. There are no partners; people crowd in wherever they can get in. The drummers stand at one side, decide on the songs and lead the singing. The drum dance was not as popular as the tea dances.\nIn the tea dance the men and women dance in any order, elbow to elbow, facing the centre of the circle. The commonest steps were:\n- A simple flat side step, with no emphasis in either foot or raising of heel or toe.\n- A step with some emphasis on the lead foot.\n- A step, by some men, throwing the heels outward.\n- A short hop with feet together, done only by old ladies\nThe knees were relaxed, shoulders drooping, – “a relaxed looseness”. Many danced with arms folded at waist level, hand holding a wrist. No one among the Dogribs broke out to do any fancy dancing, – the accompaniment was by voice only. Professors Helm and Lurie remarked that although the dancers were in close bodily contact, and the dances went on for five or more hours, there was no suggestions of any sexual connotations. The very old women often out-danced the men.\nThe absence of sexual overtones is not surprising considering that Indian dancing was a reaffirmation of close tribal unity, and of unity with the Earth Mother. Therefore they were always performed on the bare earth, even after a rain. They were frequently prayers – prayers for rain, prayers for game when food was scarce, prayers of thanksgiving.\nIn any case, among a people rigidly trained in self-control and suppression of any show of emotion, singing and dancing was an emotional release.']	['<urn:uuid:0a424218-b23d-4640-b78f-cd6cd4da6d9c>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	15	43	2991
28	medical science explain process brain development permanent lazy eye condition	Lazy eye (amblyopia) typically develops during the critical period of vision development between birth and age 7, when the brain is actively organizing and interpreting visual input from the eyes. If left untreated beyond age 7-8 years, the condition usually becomes permanent. This occurs because during this critical period, if the eyes are not properly aligned or one eye has significantly different vision, the brain begins to favor one eye over the other. After this developmental window closes, no therapy or treatment can effectively reverse the vision impairment.	['What is lazy eye/Amblyopia?\nHow to Fix a Lazy Eye in Adults| Lazy eye is a condition in which one or both eyes have decreased vision caused by visual deprivation or abnormal binocular fusion.\nThe lazy eye develops in early childhood; hence it is a disease of childhood. Once a lazy eye has developed it becomes almost impossible to reverse the condition in some cases, while timely recognition of the lazy eye by the parents of the child can give a good prognosis for the vision.\nAs no organic cause can be determined by the examination, that is why lazy eye disease remains undetected until late. Some therapeutic procedures followed in children can result in the best visual results. However, compliance is a keep factor for the best results.\nAs the process of normalization is prolonged, many patients do not keep compliance which is a key factor in a poor outcome. The education of the parents is very important who is having a child with lazy eye or amblyopia.\nAccording to research, lazy eye affects up to 1 in 33 of the population this means up to 10 million people in the USA may have a lazy eye\nClassification of Lazy eye:\nThe causes of amblyopia can be divided as followed\nStimulus deprivation amblyopia:\nOne of the most common causes of stimulus deprivation is congenital cataract, an especially unilateral cataract that is highly amblyogenic. Other common cause includes drooping of the upper lid up to the eye’s visual axis.\nConstant occlusion of one eye (> 1 week per year of age) due to any cause is very likely to develop amblyopia.\nThis condition arises when the refractive power between the two eyes is >1D. This is a highly amblyogenic stimulus.\nWhen the symmetrical refractive error is >+5.0 DS or > -10.0 DS unilateral amblyopia may occur if correction is not done.\nAstigmatic/ meridional amblyopia:\nThe relative risk of astigmatic amblyopic is increased if cylinder power is > 0.75 DC. Risk is more when there is a difference in axis or if the magnitude between two eyes is different.\nStrabismic amblyopia is caused when one eye is preferred for fixation. If the process is alternating between the two eyes, then the risk is low.\nPathophysiology of Lazy Eye:\nThere is a reduction in the spatial resolving power of the retinal cells.\nLateral geniculate nucleus:\nIn the lazy eye, there is a reduced number of cells in all six layers of the lateral geniculate nucleus.\nThere is a reduction in the number of cortical cells.\nClinical Features of Lazy eye:\n- Decreased visual acuity of two more lines on the Snellen chart.\n- Crowding phenomenon.\nThere is an abnormality of contour interaction between the point of fixation and the adjacent objects. Visual acuity is better for single optotypes than for multiple optotypes.\n- Normal ocular examination.\n- Decreased contrast sensitivity.\n- Binocular suppression of amblyopic eye.\nManagement of Lazy Eye:\n- Patching: Covering the stronger eye with a patch can force the brain to use the weaker eye. This is a common treatment for lazy eye in children and can also be effective in adults.\n- Glasses or contact lenses: Remove refractive error and perform cataract surgery for cataracts\n- Vision therapy: Vision therapy is a form of rehabilitation that can help to improve the visual skills of the affected eye. This may include exercises to improve eye coordination and strengthen the eye muscles.\n- Atropine eye drops: Atropine eye drops can be used to temporarily blur the vision in the stronger eye, encouraging the brain to use the weaker eye.\n- Surgery: In some cases, surgery may be necessary to correct a misalignment of the eyes (strabismus), which can be a contributing factor to lazy eye.\nExclude other causes:\nOther causes of decreased vision such as refractive error, cataract, and tumors should be excluded.\nsurgery for cataracts.\nHow Long Does it Take to Fix a Lazy Eye With an Eye Patch\nOcclusion therapy (GOLD STANDARD):\nThe amount of occlusion therapy for lazy eye depends on the age, severity, and cause of lazy eye.\n- Patching should be started as soon lazy eye is detected.\n- For part-time occlusion, it depends on the age of the patient. 1 hour/day for each year of age.\n- Full-time occlusion should not exceed 1 week per year of age.\n- Patching should be continued for 3-6 months.\n- If there is no progress for consecutive 3 months, patching should be considered a failure.\n- Patching should be maintained up to the age of 9, which is labeled as the age of the matured visual system.\nThis method is preserved for non-compliant or patients who show a failure to patching therapy or lazy eye\n1% atropine drop is placed in the better eye to blur the vision for near.\nImage degradation is made in the better eye so that amblyopic has a competitive advantage.\nIn this method under correction, a better eye is made with optical lenses.\nAdditional Management Principles:\n- Strabismic amblyopia:\n- Occlusion therapy should be started before strabismus surgery. It is done because the fixation behavior will be harder to determine once the strabismic correction is made.\n- Parent motivation towards patching is increased by the visual reminder of strabismus.\n- Amblyopia due to refractive error.\n- Refractive correction is made before patching therapy.\n- Part-time occlusion is preferable if binocular interaction is present, amblyopia is mild and the child is in school.\n- Stimulus deprivation amblyopia.\n- Remove any barrier within the first 6 weeks of life.\nThere are several steps that can be taken to prevent the development of the lazy eye or amblyopia:\n- Early detection: Regular eye exams are important for detecting any potential vision problems, including lazy eye, as early as possible. It is recommended that children have their first eye exam at 6 months of age and then at 3 years old and before starting school.\n- Correction of refractive errors: If a child is found to have a significant difference in the refractive error between the two eyes, or one eye is more farsighted, nearsighted, or has a greater amount of astigmatism than the other, correcting the error with glasses or contact lenses can help prevent the development of lazy eye.\n- Treatment of strabismus: Strabismus, or a misalignment of the eyes, can lead to the development of lazy eye. If a child is found to have strabismus, prompt treatment with glasses, eye patches, or surgery can help prevent the development of lazy eye.\n- Regular follow-up: Children with a high risk of developing amblyopia should have regular follow-up visits with an ophthalmologist or optometrist to monitor their vision and make sure that their treatment plan is working.\n- Awareness and education of the primary care physician.\n- Vision screening programs in all communities.\n- The red reflex of every baby should be checked at birth.\nCan a Lazy Eye Be Fixed\nAmblyopia can be reversed up to the age of 7-8 years, after this age it is usually permanent. It is the average age for the development of normal vision in the human eye, after this, no therapy or treatment can reverse the vision.\nSigns of Lazy Eye\nThe signs of lazy eye, or amblyopia, can vary, but may include:\n- Poor vision in one eye: The affected eye may have a lower visual acuity than the other eye.\n- A tendency for the eye to turn in or out: This is called strabismus and it is a common cause of lazy eye.\n- Lack of depth perception: People with the lazy eye may have difficulty judging distances or perceiving the three-dimensional nature of objects.\n- Crossed or uncrossed eye: The affected eye may appear to be turned in or out.\n- The difference in the size of the pupils: The affected eye may appear smaller than the other eye.\n- Sensitivity to light in one eye: The affected eye may squint or close when exposed to bright light.\nIt’s important to note that not all people with the lazy eye will have all these signs and symptoms, and some people may not have any obvious symptoms. It’s important to have regular eye exams, particularly if there is a family history of lazy eye or other vision problems. This can help to detect the condition early and increase the chances of a successful outcome.\nAt what age does a lazy eye develop\nLazy eye, or amblyopia, typically develops during childhood. The most critical period for the development of vision is between birth and age 7, during this time the brain is actively organizing and interpreting visual input from the eyes. If the eyes are not properly aligned or one eye is significantly more farsighted, nearsighted, or has a greater amount of astigmatism than the other, the brain may begin to favor one eye over the other. This can lead to the development of a lazy eye. However, it is important to note that amblyopia can also occur in adults as a result of injury, disease, or other conditions that affect visual acuity in one eye.\nWhat if there is no response after 3 months of Patching therapy for children\nIf there is no improvement after 3 months, then consider the following\n- Wrong diagnosis\n- Uncorrected refractive error\n- Failure to prescribe specific treatment\n- Irreversible amblyopia\nHow to fix a lazy eye | Treatment for lazy eye\nIn children below the age of 9 years, patching therapy is considered a gold standard. It will force the lazy eye to do it function more effectively while the patch is placed on the better eye.\nLazy Eye Training\nGames/activities designed to challenge the weak eye have proven to be very beneficial, but they are not enough as a single treatment to cure vision.\nLazy eye training tools include certain types of computer or iPad games and activities such as jigsaw puzzles and drawing pictures.\nTraining with computer games and videos to be effective in several small studies, one in 2016 and one in 2018. However, before it can be considered effective enough to be used without other forms of treatment, more research is needed, such as wearing an eyepatch.\nCan lazy eye worse\nIf left untreated the lazy eye may get worse over time. The brain gradually stops picking signals from the lazy eye and it can render a blind eye.\nHow to fix a lazy eye in adults at home |amblyopia in adults\nUntil now there is no treatment option for the fixing of lazy eye in adults. The only possibility that can be provided is the optical correction to the best of its level.\nWhat is Vision Therapy\nThrough a series of progressive therapeutic eye exercises, patients develop visual skills. Visual enhancement is achieved by improving the communication between your brain and eyes.\nVision therapy aims to improve a person’s visual abilities. Vision therapy is performed once per week under the supervision of an eye doctor in sessions lasting 30-45 minutes. uses a variety of ways – such as testing, eye exercises, occlusion (patching) prisms, and lenses to treat visual problems.\nHome exercises are given in the office to reinforce the exercises learned during the therapy session. For best results, commitment to weekly sessions and therapy homework assignments is essential. Through vision therapy, both eyes will be trained to work together to achieve clear and comfortable vision.\nSome vision therapy programs to treat lazy eye may include:\n- Accommodation (focusing)\n- Fixation (visual gaze)\n- Saccades (switching eye focus, “eye jumps”)\n- Pursuits (eye tracking)\n- Spatial skills (eye-hand coordination)\n- Stereopsis (3-D vision)\nLazy eye / Amblyopia affects 3 out of every 100 children. The condition is treatable and usually responds well to strategies such as eye patching and corrective lens wear.\nA lazy eye is something that can be avoided only by having a check-up with a pediatric ophthalmologist at an early age. The best results for a lazy eye are usually seen when the condition is treated early, in children 7 years of age or younger.\nCan lazy eye be cured in adults?\nStudies funded by the National Eye Institute (NEI) found that lazy eye can be successfully treated even in adults. Vision therapy is a customizable, individualized treatment program performed under the supervision of an eye doctor\nHow do adults get rid of lazy eye?\nVision therapy is an effective treatment method for amblyopia in adults.\nWhen is it too late to fix a lazy eye?\nIt’s never too late to get treated for a lazy eye. some optometrists say It is possible to treat amblyopia in adults via vision therapy\nCan glasses fix a lazy eye?\nGlasses are just a tool when treating your lazy eye. The main method is by patching the good eye.']	['<urn:uuid:dc741fce-f5df-47bc-aa0b-88ea83447228>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	10	88	2136
29	resolution support differences vc6 prores video	Both VC-6 and ProRes support multiple resolutions but handle them differently. ProRes preserves SD, HD, 2K, 4K and even 5K frame sizes at full resolution. VC-6 goes further by supporting 8K and offering unique advantages - it allows users to capture in 8K from RAW and never have to cross-convert to other resolutions, instead providing proxies automatically and enabling editing at multiple resolutions all from a single file. VC-6 requires only 12.5% of the full resolution to be demosaiced for 8K processing, while ProRes requires maintaining full resolution data.	"['StreamingMedia.com Industry Announcements\nView Press Releases --- Add Your Press Release\nStreamingMedia.com provides this section as a service to its readers and customers.\nPress releases are subject to approval by the editorial staff of StreamingMedia.com and may be edited or altered for length and clarity, or to remove unsubstantiated and unverifiable claims.\nAll content presented within the press release section is that of the submitter. StreamingMedia.com does not necessarily endorse such content and bears no responsibility or liability for its accuracy.\nNext-generation SMPTE VC-6 video production codec standard published\n• SMPTE VC-6 (ST-2117) is the new mezzanine codec to optimize and accelerate a wide variety of production workflows\n• VC-6 provides vital advantages over technologies like Apple ProRes and DNxHD including multi-resolution and region-of-interest decoding\nLondon, UK(05 Oct 2020)\nV-Nova, a leading provider of video compression solutions, has announced that after a comprehensive peer review process, the VC-6 video codec has now been fully ratified and published by the Society of Motion Picture and Television Engineers (SMPTE).\nPublished as SMPTE ST-2117, VC-6 is the first video codec to leverage AI as part of its innovative approach to compression, which is not based on transforms such as DCT or wavelet. VC-6 employs a hierarchical series of S-Trees and achieves its compression by reusing the same structures at different resolutions – a capability that makes the codec extremely fast when implemented in software.\nThe novel AI-powered hierarchical approach means that full-resolution full-quality proxies are intrinsically available as part of the format as is region of interest decoding. This reduces the demands placed upon data transfer and decode processing, while VC-6’s compatibility with all major industry container formats like MXF and QuickTime means it can be seamlessly incorporated into any workflow.\nThe codec’s efficiency means that it is ideally suited to 4K and 8K production. In particular, the emerging 8K format brings with it significant production challenges, which can be overcome with the new codec. VC-6 allows the user to capture in 8K from RAW and never have to cross-convert to other resolutions. Instead, VC-6 can provide proxies automatically and enable editing at multiple resolutions all from a single file.\nInitial use cases for the technology show dramatic reductions in 8K processing by requiring a mere 12.5% of the full resolution to be demosaiced, up to 60% reduction of disc access for 4K files, and 2-4x faster decoding than existing ProRes-based solutions.\nVC-6 is already shipping as part of V-Nova’s P.Link for contribution and remote production application, whilst the company is currently working on a multi-platform SDK and cloud service deployment of VC-6 expected to be made available via an Early Access Program later this year.\nGuido Meardi, CEO and co-founder of V-Nova, said, “With 4K and 8K production becoming the norm, the demand for intelligent and scalable video compression technology has never been more acute. Standardization by SMPTE confirms that VC-6 is the most innovative solution to these new production challenges. We look forward to introducing the capabilities of VC-6 to this important market.”', ""As of this writing, there are six flavors of ProRes for you to choose from for editing your projects. They are as follows:\n- ProRes 422 HQ\n- ProRes 422\n- ProRes 422 LT\n- ProRes 422 Proxy\n- ProRes 4444\n- ProRes 4444 XQ\nI know when I first looked at that I asked myself “Why they hell are there so many?” It turns out they all do serve a unique purpose and aren’t just there to make your import/export process a nightmare. Before we get into how they’re different, let’s take a peek at how they’re the same. They all share these characteristics:\n- Variable bit rate (VBR) encoding — This basically means that scenes with less pixel information (due to low-light or little motion, for instance) will have a lower bitrate than their vivid, high-motion counterparts. You may be familiar with similar techniques used on MP3 or AAC audio tracks.\n- 10-bit color/pixel depth — Your images will display colors accurately and with super high fidelity\n- 4:2:2 chroma subsampling — I won’t get too much into the specifics of this one, but just understand it’s a compression method that focuses on reducing the amount of bandwidth allotted to the chroma (color) channel. JPEG image compression uses a very similar technique.\n- Intra-frame encoding — Each frame of your video is encoded and decoded independently of the ones around it\n- SD, HD, 2K, 4K and even 5K frame sizes are preserved at full resolution\n- One of the only codecs optimized for multiple processors\nThe only one that differs is the one with the extra digit: ProRes 4444. With this version, you get all of the great stuff listed above AND the following:\n- Preserved color spaces for both RGB and YCbCr source material\n- 12-bit color/pixel depth\n- Alpha Channel support\nThen we have the brand spanking new ProRes 4444 XQ, which has all of the above with this gem:\n- Target data rate of approximately 500 Mbps for 4:4:4 sources at 1920x1080 and 29.97 fps\nPrior to ProRes 4444, if you wanted Alpha Channel Transparency, you had to use Apple’s Animation codec. The Animation codec is great for compositing or motion graphics, but when you add full-frame video the file sizes become astronomical. ProRes 4444 maintains transparency and keeps file sizes slightly more manageable. The best of both worlds.\nSo, after reading all of the above, you may be tempted to just select ProRes 4444 for all of your projects going forward and bask in the comfort of knowing you’re using one of the most superior editing codecs on the planet. That would probably work out great for you up until you’re on your 189th external hard drive purchase to store all of your footage. Like the title suggests, my goal is to get you to select the right version of ProRes given your circumstances, so here we go.\nKnow Your Gear. Know Your Project.\nIt’s pretty easy for me to know what version of ProRes to select based on these two factors alone. Was your footage shot on a DSLR with no need for alpha transparency? Well you can immediately take ProRes 4444 (XQ) and ProRes 422 HQ off the list, then whittle down from there. Was your footage shot on a RED, Blackmagic, ARRI, or Sony cinema camera? You’re most probably looking at ProRes 422 HQ or higher to maintain high bitrates and color fidelity.\nApple has posted an excellent white paper (PDF link) which is what I referred back to when writing this blog post. It’ll act as an excellent reference for viewing the target data rates and determining which flavor to choose.\nIt's worth noting that nearly every external recorder on the market currently allows you to choose not only ProRes as your codec, but the specific type to use for recording. I believe the ARRI ALEXA is the only camera to currently record directly to ProRes 4444, though I'd very much appreciate any clarification on this.\nDue to the fact that there are so many combinations of frame size, resolution, color depth, and more, these are by no means definitive answers to the question of what codec to choose, but I’ll try to help and give examples based on my own experience working with certain types of footage and equipment.\nProRes 4444 XQ — If you're working with the ARRI ALEXA, which shoots ProRes 4444 XQ natively with an additional module, or anything that captures 12-bit (or higher) RAW like a RED, a Blackmagic camera or a Canon 5D3 running Magic Lantern that requires the highest possible quality for digital compositing, special effects, or heavy color grading, choose this option. I doubt anyone working outside of very large theatrical or television productions would need this.\nProRes 4444 — If maintaining 12-bit color depth is something you require or know you’ll be keying out certain colors throughout the edit, choose this one. The most obvious types of footage to use in this circumstance would be anything shot against a green screen.\nProRes 422 HQ — Same as above basically without the alpha channel. In my experience, you’d need a hell of an eye to tell the difference between ProRes 4444 and ProRes 422 HQ.\nProRes 422 — Here’s where I believe most people will end up. If you’re shooting on a modern DSLR or Mirrorless camera that provides an uncompressed signal over HDMI (hopefully with a higher bit-depth straight from the sensor), you'll have an appreciably better picture by choosing this codec.\nProRes 422 LT — I easily recommend this codec for anyone shooting on DSLRs, especially if you're shooting 4K with something like the Atomos Shogun. Your camera's internal codec will have a compressed data rate of anywhere from 20Mbps to 70mbps (usually variable). ProRes 422 LT aims for 102Mbps, which still looks great if you're getting a sensor readout over HDMI and will serve you much better in post, especially while color grading. You’ll also save a pretty hefty amount of disk space going this route.\nProRes 422 Proxy — This format is great for cutting dailies or doing offline editing. It’s also great for editing footage on a low-power laptop. Most training materials from places like Lynda or Total Training provide the footage in ProRes 422 Proxy format. Definitely not recommended as a start-to-finish solution.\nThat's How You Do It\nIt’s been really interesting to see ProRes mature and gain traction in the professional field over the past few years. It’s become more and more of a de-facto standard as it makes the process of moving right into post-production that much easier.""]"	['<urn:uuid:74e4637e-be38-4441-af68-c0a9ecb105a9>', '<urn:uuid:07ea4382-62db-49dc-b11f-35b906aff803>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T19:58:41.790432	6	89	1607
30	when original bagdad cemetery leander texas first grave burial history	The Bagdad Cemetery opened in 1857 when Charles Babcock donated one acre of land as burial ground for his three year old son John L. Babcock, who became the first person buried there.	"['The Texas Chainsaw Massacre was released into theaters on October 1st, 1974. Filming locations include Bastrop, Hutto, Leander and Round Rock, Tx. Filming started on July 15th, 1973 and lasted for thirty-two days. There\'s not a whole lot to say regarding this masterpiece that hasn\'t already been said or covered somewhere else prior. There are the documentaries The Shocking Truth and Flesh Wounds as well as numerous books that include The Texas Chainsaw Massacre Companion, Devil\'s Advocates The Texas Chainsaw Massacre, Chain Saw Confidential and The Texas Chainsaw Massacre and It\'s Terrifying Times.\nThe Texas Chainsaw Massacre has always been my favorite horror movie 1-A, with The Exorcist being 1-B. The very first horror convention I ever attended was the 2004 Cinema Wasteland in Strongsville, Ohio. That Texas Chainsaw Massacre 30th anniversary reunion was quite possibly the largest gathering of Chainsaw alum ever assembled. In attendance was Roger Bartlett, Bob Burns, Marilyn Burns, Allen Danziger, Gunnar Hansen, Ed Neal, Paul Partain and Lou Perryman. Needless to say, I was starstruck. Sadly however, on May 31st of that same year, only two months later, Bob Burns would pass away. And roughly seven months after Bob\'s death, Paul Partain passed away as well, on January 28th, 2005.\nThe chainsaw model wielded by Leatherface was a Poulan 306a and it weighed roughly thirteen pounds. The generator (air cooled engine) that Pam and Kirk are drawn to was a Wisconsin brand, which was the state Ed Gein, the influence for the Leatherface character was from. Probably a complete coincidence but interesting nonetheless. The films distressing score was composed by Tobe Hooper and Wayne Bell. Among instruments used were a stand-up bass, a five-string Kay upright double bass, a Fender lap steel guitar, an African instrument with attached tambourines, a pitchfork and children\'s musical toys like cymbals, xylophones and shakers. The total score length is fifteen minutes. Songs used in the film include Fool for a Blonde by Roger Bartlett, Daddy\'s Sick Again & Misty Hours of Daylight by Arkey Blue, Waco & Glad Hand by Timberline Rose and Feria de las Flores & Poco a Poco No by Los Cyclones. The films first DVD release was on October 6th, 1998.\nThe cemetery - 400 N. Bagdad Rd. Leander, Tx. 78641\nThe cemetery opened in 1857. When faced with the burial of his three year old son John L. Babcock, Charles Babcock donated one acre of land as burial ground. Eight years later Col. Charles C. Mason, the first postmaster of Bagdad died and was buried in close proximity to John. Not only is Mason\'s monument among the tallest in the cemetery, but it would later go on to serve as the opening for one of the most famous and influential horror films ever made. It\'s the rear of the monument that\'s shown in the opening scene of the film, sitting about ten feet behind the false monument made for the movie in which the rotted corpse was postured atop of. In the scene where the van first pulls up to the cemetery, there are three men slowly approaching it. The man in the middle, wearing the white wife beater and cowboy hat, is none other than John Dugan, the 20 year old actor who also played the 113 year old grandpa in the film. There have been more than 3,500 burials at the cemetery since it first opened.\nAll ""Now"" pictures taken in 2018.\nThe gate to the main entrance of Bagdad Cemetery.\nThe reverse (front) side of the large monument shown in the opening scene and 2nd ever burial at the cemetery.\nThe grave stone of the first ever burial at Bagdad Cemetery in 1857.\nHitchhiker gets picked up - Across the street from* 101 Farm to Market Rd. 685 Hutto, Tx. 78634\nThere were two things that helped me find this location. The first was a tip from Paul Partain to fellow Chainsaw fan Tim Harden where he pointed out that most of the driving scenes near the beginning of the film were shot along highway 685. The second was a deleted scene that first appeared on the 40th anniversary collector\'s edition blu-ray. The scene showed an angle of the location that hadn\'t been shown prior, where you can see a small distinct building down the road in the background. I was able to match the building in question in a vintage aerial image. The small building was removed at some point during the early 90\'s. Hutto High School, which was built in 1999, now sits across the street from where the van picked up the hitchhiker.\nAll ""Now"" pictures taken in 2018.\nGas station - 1073 Texas 304 Bastrop, Tx. 78602\nSince it\'s appearance in the film, the gas station has had a few different names over the years that include Hills Prairie Grocery and Bilbo\'s Texas Landmark. In 2016 the name was again changed, this time to The Gas Station. It now sells horror movie memorabilia and BBQ, as well as rents cabins that are located just behind the building. The original Gulf sign that used to be there, as seen in the film, is now located at Dick\'s Classic Car Museum located in San Marcos, Tx. Aside from the Gulf sign, they have a plethora of antique automobile\'s that date all the way back to 1911 and I recommend checking it out if you find yourself in the area.\nAll ""Now"" pictures taken in 2018.\nAn honorary memorial that was first unveiled at the Cult Classic Convention in Bastrop in September 2018.\nThe interior of The Gas Station.\nGrandparents house - Hester\'s Crossing Rd. and Country Road 172 Round Rock, Tx. 78681\nAlthough it was made to appear quite a distance from the family house in the film, in actuality they were almost directly across the street from one another. The mostly limestone house was erected in the late 1800\'s. Built by William S. Thompson who lived there until his death in 1903. In 1909 the house and property were purchased by William Quick. He married Sally Stark and they had five children (Emma, Marie, Edith, Edward and Erik) while living there. Erik would later go on to have a daughter named Norma. It was Norma\'s former room in the house that was shown with the animal print wallpaper when Sally talks about her fascination with the zebra\'s. The house sat unoccupied for sometime before burning down in the last 70\'s. The pile of limestone as well as the porch foundation were still there when I visited the site in January of 2001. In 2002 construction began on Texas State Highway 45 and today the exact spot where the house once stood at is now where TSH-45 now lies.\nThe house in it\'s earlier days.\nAs seen in the film.\nFamily house - 1010 King Ct. Kingsland, Tx. 78639\n(Grand Central Cafe)\nThe house was designed by architect George Franklin Barber. The rooms feature twelve foot ceilings, ornate wood moldings and a curved entry hall along the central staircase. Barber published a monthly magazine in the late 1800\'s called American Homes, which also promoted his original house plans that he sold through his publication. He had hundreds of house plans available which ranged from one bedroom homes to more elaborate three story houses. There were actually three of these nearly identical houses built in Round Rock at the beginning of the 20th century. One is the house seen in the film, the second remains a mystery and the third was located just down the road from the house in the film and was also known as the Burkland-Frisk house, as it was built by Leonard Frisk and later owned by Tony Burkland. The house still stands today and is currently a dentist office, located just up the interstate in Georgetown. Back to the Texas Chainsaw house, it was built in 1909 for the Thompson family, the previous occupants of the grandparents house used in the film that was located just across the street. It was then purchased in the early 40\'s by Robert and Nina Sellstrom. In 1949, their daughter Betty even had her wedding reception at the house. In 1971 they sold the house and the surrounding one-hundred plus acres to Celia Nueman who rented the property out. The first tenants were Stuart and Rebecca Isgur. During the filming of the movie in 1973, Stuart\'s brother Ron was the only resident staying in the house. The filmmakers found the house through the softball team Ron was playing on because Bob Burns, the art director of the film, had a commercial art company that sponsored the team that season. The house would continue to be leased out off and on through the years. In 1998 the house was purchased by Dennis and Barbara Thomas, cut into seven pieces and moved 61 miles west to Kingsland, Texas where it became part of the Antlers Inn resort. It was then reassembled and restored to it\'s original condition by carpenter Anthony Mayfield. Since then it\'s been called The Kingsland Old Town Grill and The Four Bears Restaurant but is now known as The Grand Central Cafe. Not only does the place have outstanding food but it completely embraces it\'s Chainsaw past. While exploring the house you\'ll come across pictures, posters, shirts, new paper articles and other Texas Chainsaw Massacre artifacts. Also, the staff couldn\'t possibly be more accommodating to fans of the movie. I highly recommend The Grand Central Cafe, not only to Chainsaw buffs but anyone else who may be looking for somewhere with great food and the atmosphere to match.\nAll ""Now"" pictures taken in 2018.\nThe entrance to The Grand Central Cafe.\nMyself having breakfast in the chicken and bones room from the film.\nA comparison picture of the family house (top) and the Burkland-Frisk house (bottom) as they appear today.\nOriginal family house location and ending - Hester\'s Crossing Rd. and Country Rd. 172 Round Rock Tx. 78681\nKnown as Quick Hill for former resident and land owner William Quick. Country Road 172 once ran directly over the hill, right in between the family house and the grandparents house from the film. Then in the early 90\'s the new Country Road 172 was built just to the west of the hill turning the original into Old Country Road 172. Not long after, Old Country Road 172 was gated off and inaccessible to the public. And finally in 2002, construction for Texas State Highway 45 started and it cut directly through the center of Old Country Road 172, dangerously close the the 89 year standing spot of the Texas Chainsaw Massacre family house...which had been moved four year prior. The portion of road shown at the end of the movie where Sally escapes and Leatherface does his infamous chainsaw dance is still there. The very first movie location I ever visited was Quick Hill, on January 1st, 2001. I was moving cross country from Kentucky to California and I had everything I owned stuffed into my car but I decided to roll the dice and venture about 300 miles off course anyway. This was before GPS was widely available and even though I\'d never been to the Austin area before I had very little trouble locating the spot, thanks to the reliable directions provided by www.texaschainsawmassacre.net. I remember it vividly. I was a sunny day, unusually warm for early January. As I made my way up the gated off road I examined the surroundings and with each step what I was seeing resembled more and more what I had seen on the screen so many times. This was only a couple years after the family house had been moved and almost all of the other structures were still there, albeit they\'d certainly seen better days. One of the sheds was filled to it\'s ceiling with what looked to be mostly junk. I was pretty sure that most of that ""junk"" were things removed from the house prior to it being moved just a couple year earlier and if I had ANY room in my car I probably would\'ve taken some if it as mementos. Also still there during my visit were the remains of the grandparents house, just across the street. While it had burned down in the late 70\'s, all of the limestone from it was still there, just sitting in a big pile. The porch foundation was also still visible. I was able to take two souvenirs that day, although very small one\'s. One was a chunk of the limestone from the grandparents house and the other was a small piece of cement that had broken off from the steps of the family house. I still have both of them to this day. I must\'ve stayed on that hill for about two hours that day exploring and just taking it all in and I didn\'t see another person up there the whole time. It was so worth the near 300 mile detour and a day I\'ll never forget. Since then I\'ve visited the location another half dozen times, each time seeing it get smaller and smaller. In 2013, the Camden La Frontera apartments were built near the bottom of the north side of Quick Hill, intruding upon the hill even more. For years there has been a large billboard on the south side of the property facing Texas State Hwy. 45 reading ""For Lease"" and to be honest I\'m not quite sure why the remaining land on Quick Hill still hasn\'t been developed. All Chainsaw fans can do is cross their fingers and hope it stays that way.\nAn aerial shot of Quick Hill taken with my drone (2018).']"	['<urn:uuid:1cc9574a-b922-47bb-8f46-7e87dd6fe665>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	10	33	2282
31	What causes lab equipment contamination, and how do cleanrooms prevent it?	Lab equipment becomes contaminated through sample residues left on improperly cleaned tools like pipettes, syringes, and containers, as well as through contact with contaminated surfaces. Solvents and reagents used in sample preparation can also introduce contaminants. To prevent contamination in cleanrooms, several measures are implemented: installing proper filtration systems with regular maintenance (replacing pre-filters six times yearly and HEPA filters every three years), using pass-through chambers to reduce cross-contamination during material transfer, implementing thorough cleaning protocols with specialized products like distilled water and non-shedding cloths, and utilizing anti-static components including specialized flooring and wall panels.	['It is often said that the best place to catch a disease is in a hospital; likewise, the most common place for sample contamination to occur is in the analytical laboratory. Sample contamination may be defined as the inadvertent addition of target or detectable analytes to samples during the sample collection, transportation, preparation, or analysis processes. Consequently, virtually all laboratories have contamination problems and these problems are, to a large extent, unavoidable and insurmountable without strict attention to procedural validation, monitoring, and routine instrument maintenance.\nCommon sources of contamination include organic or inorganic analytes in atmospheric dust and sample residues. Sample residues are left behind on improperly cleaned tools such as pipettes, syringes, spatulas, as well as storage or volumetric containers. A clean sample in contact with a contaminated surface is by definition a contaminated sample. In some cases, careless or improper sampling techniques which expose the sample to air, fluid, or surface borne analytes are also a source of contamination. In these instances, proper training of personnel and the implementation of validated cleaning, handling, and storage procedures can help reduce contamination-related errors while saving money, effort, and time.\nSample preparation or pretreatment procedures are another common source of contamination. Solvents used in extraction or purification procedures or the addition of diluents, buffers, or analytical reagents may introduce foreign or unintended analytes that may interfere with the final analysis or lead to incorrect or misleading conclusions. Proper raw material and lab supply intake procedures can help ensure that contamination issues were not the result of a laboratory equipment supplier.\nFar less obvious sources of contamination are the analytical standards and blanks which are an integral and indispensable component of virtually all analytical procedures. Many analysts automatically assume that analytical standards or reagents prepared as blanks are accurate and free of contaminants, leading to a potentially false conclusion that the source of any analytes detected is the sample. Here again, validation, procedural clarity, monitoring, and training are the only reliable defense.\nIn many cases, the removal of excess extraction or reaction solvent is a necessary precursor to the analytical process. There are a number of accepted methods for solvent removal and sample concentration including rotary evaporation, distillation, oven drying, freeze drying, and gas assisted evaporation, i.e., “nitrogen blow down”. Each of these methods is a potential source of contamination so care must be exercised in the proper application of these techniques.\nWhen gas assisted evaporation systems such as Organomation’s N-EVAPs, MULTIVAPs or MICROVAPs are utilized for sample preparation and the removal of excess solvent, it is always advisable to:\n1. Make sure that sample vials are properly and verifiably cleaned prior to the introduction of the sample or sample containing solution. Disposable, single use sample vials are the preferred choice but if reusable sample vials are used they should be cleaned and stored following a validated cleaning and storage procedure.\n2. Make sure that gas delivery needles are clear and free of any residues. Ideally needles should be cleaned before each application and examined before use.\n3. Make sure the gas source is clean and dry. If bottled nitrogen or other inert gas is being used the gas should be high purity (99.99%) or ultra high purity (99.999%) in situations where higher precision is desired. In cases where the sample is moisture sensitive or susceptible to microbial attack, the gas should be passed through a dryer and a micro filter. If atmospheric air (usually from a compressor) is used, the gas must be passed through a dryer and a micro filter as well.\n4. Take care when mounting sample vials in the nitrogen evaporator. Do not allow the sample or sample containing solution to come into contact with the needle. Before mounting the sample vial raise the needle high enough to allow the sample vial to be seated in the holder without tipping and then lower the needle into position after the sample is in position.\n5. Avoid creating excessive sample turbulence inadvertently. Set the gas flow rate at a lower setting initially and then increase the flow using the needle valve. If a series of samples are being processed this is only necessary for the initial series and the setting may be used for subsequent samples provided the same sample vials and solution volumes are maintained. Splashing or spattering may result in sample loss, cross contamination, and needle contamination or soiling.\n6. Make sure that the bath temperature is sufficiently high to produce a satisfactory evaporation rate. However, too high of a temperature will create condensation on the exposed surfaces of the instrument. Condensed moisture or solvent may drip into the sample or leave residues on the needle or surfaces above the sample.\n7. Take care when removing sample vials from the instrument; raise the needles as high as possible to ensure that the sample vial can be removed without contact with the needle. The needle must not come into contact with the sample vial walls or the sample in the vial. It is also advisable to discontinue the gas flow due to the possibility that by lifting the tube vertically out of the vial holder the sample could be brought into near contact with the needle which could result in turbulent splashing or spattering.\nAgain, all laboratories have contamination problems that can only be controlled by the implementation of an active program designed to continuously monitor the many different types and sources of contamination. Reviewing, validating, and training personnel on these preventive measures will ultimately save the laboratory both time and money.\nINFORMATION RELATED TO cGLP, TRAINING & REGULATIONS\n1. Barge, Maureen S. and Ussar, James P., Good Laboratory Practice Standards: Applications for Field and Laboratory Studies (ACS Professional Reference Books), American Chemical Society, 1992\n2. Seller, Jurg P., Good Laboratory Practice: The Why and The How, 2005\n3. Allport – Settle, Mindy J., Good Laboratory Practice: Nonclinical Laboratory Studies Concise Reference\n4. Huber, Ludwig, Validation and Qualification in Analytical Laboratories, 2nd Edition, 2007\n5. Huber, Ludwig, Good Laboratory Practice and Current Good Manufacturing Practice, Agilent Technologies Deutschland Gmbh, 2002\n6. Compliance and Safety Training, Preventing Contamination in the Laboratory, DVD Program (http://complianceandsafety.com)\nAdditional sources of information regarding cGLP, training and regulations applicable to specific industries or field are available from:\n-Federal Registry: 21 CFR 211, Subpart I (4)\n-American Chemical Society\n-ASQ (American Society for Quality)\n-American Association for Laboratory Accreditation\n-Compliance and Safety Trainining (http://complianceandsafety.com)', '5 Biggest Threats to Medical Cleanrooms (and How to Prepare for Them)\nIn the medical industry, there’s no room for error. The slightest amount of contamination can not only result in reduced efficiency and missed development deadlines, but it can also put human lives at risk.\nKeeping your medical cleanroom airtight, fully controllable, and customized to meet your application’s ISO classification is essential in conducting safe, effective, and efficient operations. You can accomplish this by understanding and planning ahead for any risks your cleanroom may encounter. Below, we’ll outline five of the biggest threats to medical cleanrooms, as well as how you can prepare for them.\n5 Biggest Threats to Medical Cleanrooms\nWhile some threats to medical cleanrooms are fairly common sense, others may take you by surprise. Either way, it’s important to take all into consideration throughout the design and installation processes. Here are five of the most common threats medical cleanrooms regularly face:\n1. Personnel Within the Medical Cleanrooms\nStaff members are known to pose the greatest risk for cleanrooms — and medical cleanrooms are no exception. Improper or disregarded handwashing and gowning protocols can result in thousands of contaminant particles being introduced — from skin particles to hair strands, to perfumes and cosmetics. In fact, some cleanroom inspections have attributed personnel as the cause for 80% of identified particles.\n2. Medical Cleanroom Materials\nWork samples and materials within medical cleanrooms can also pose a health threat to personnel. In many medical cleanroom applications, cleanroom operators work with microbiological substances that can cause serious harm if they aren’t contained and controlled properly and if operators aren’t wearing correct protective gear. From studying infectious diseases to dealing with bodily fluid samples, it’s important to be aware of potential biosecurity risks.\n3. Equipment in Your Medical Cleanroom\nThe machines and tools within your cleanroom pose varying levels of contamination risk. Machines often emit gas, lubricants, emissions, and other airborne particles while in use. Also, everyday tools like pens, beakers, trays — even cleaning items — can all introduce particles if not sanitized properly before use or handled appropriately during use.\n4. Dysfunctional Filtration Systems\nIf your medical cleanroom’s filtration system isn’t working properly, your cleanroom likely isn’t all that clean. Filtration systems — consisting of fans, pre-filters, and HEPA or ULPA filters — are responsible for treating contaminated air supplies and maintaining consistent air change rates in order to ensure good air quality. However, over time, their components can grow old and function at lower capacity, increasing the chance of higher particle counts and dead zones.\n5. Static in Your Medical Cleanrooms\nStatic electricity in medical cleanrooms isn’t always a risk that comes first to mind — but it’s definitely one to take into account. Its “cling” effect increases the chance of attracting airborne particles to surfaces that need to remain sterile. Also, in applications involving medical device development, static can cause particles to be drawn in and permanently damage sensitive electronic components.\nHow to Prepare Your Medical Cleanroom for Safe Operations\nFortunately, there are ways to prepare for and prevent each of the medical cleanroom threats listed. Although complete sterilization is virtually impossible, implementing the right tools, technologies, and procedures can help you reduce risk as much as possible. Here are a few preparation tips to consider during the design and installation processes:\nInstall and Maintain a Fully Functioning Filtration System\nIn order for your cleanroom to meet its ISO classification requirements, you need to install a quality filtration system. Make sure that your filtration system provides the required percentage of ceiling coverage and maintains consistent air exchange rates.\nAnd, once it’s designed and installed properly, make sure you stick to a regular filtration system inspection and maintenance schedule. Your pre-filters should be replaced at least six times per year, and your HEPA or ULPA filters should be replaced at least once every three years. If not maintained properly, your filtration system could fail and threaten control over your cleanroom environment.\nApply the Right Type of Pressurization for Your Medical Cleanroom’s Application\nBoth positive and negative pressure cleanrooms have their place in the medical industry — it just depends on your specific application. Most medical cleanrooms operate with positive pressure, using HEPA filters and an external airflow pattern. However, some applications involving hazardous substances need negative pressure to prevent contaminants from escaping and causing harm to the surrounding environment.\nInclude Pass Throughs in Your Medical Cleanroom Design\nPass-throughs are chambers for moving products in and out of your cleanroom. Adding one or more pass-throughs to your medical cleanroom can make transferring tools and materials quicker and more efficient. At the same time, pass-throughs reduce cleanroom traffic cross-contamination, helping to provide a less compromised work environment.\nAdopt Thorough Cleanroom Cleaning Protocols\nMedical cleanrooms require some of the most stringent ISO standards — and while your cleanroom may be designed with features and technologies to uphold that, you still need to do some cleaning as well. By establishing a daily, weekly, and as-needed set of cleaning tasks, you’ll ensure your cleanroom is operating at its maximum possible sterilization levels.\nWhile you’re cleaning, make sure to use specified cleaning products for medical cleanrooms like distilled water, neutral chemical solvents, and non-shedding cloths or wipes. How you clean is just as important as how frequently you clean.\nImplement Anti-Static Cleanroom Components\nControlling static isn’t always easy, but it is possible. There are many specialized products made for controlling static within a cleanroom, including anti-static flooring and wall panels. On top of this, a great way to control static is to adopt clothing and gowning guidelines that limit it. These could include anti-static garments, ESD cuffs, low-static shoes, or basic lab coats.\nNo matter the specific environmental threats your facility and application may face, Angstrom Technology can design and install a medical cleanroom that controls and prevents them all. Contact our team to get started today.']	['<urn:uuid:2e73025b-8ba3-4222-ad0f-b0bfd1b0f889>', '<urn:uuid:13008f74-d565-4352-b36a-2023ad05fc8c>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:58:41.790432	11	95	2050
32	genetics researchers attempts create super crops global hunger solutions progress	Scientists are working to create super-rice varieties through the sequencing of the African rice genome. Led by the University of Arizona, researchers can now identify beneficial traits more easily and transfer genes through conventional breeding or genetic modification techniques. Additionally, efforts are being made to establish super-crop science and technology centers worldwide through collaboration between researchers, such as work between Rod A. Wing and Quifa Zhang from Huazhong Agricultural University in China. These coordinated efforts aim to address global hunger challenges within the next 25 years through focused international cooperation.	['A rice genome to feed the world\nJuly 31, 2014\nAn international team of researchers led by the University of Arizona (UA) has sequenced the complete genome of African rice.\nThe genetic information will enhance scientists’ and agriculturalists’ understanding of the growing patterns of African rice, and help development of new rice varieties that are better able to cope with increasing environmental stressors to help solve global hunger challenges, the researchers say.\nThe research paper was published in Nature Genetics (open access).\nThe 9 billion-people question\n“Rice feeds half the world, making it the most important food crop,” said Rod A. Wing, director of the Arizona Genomics Institute at the UA . “Rice will play a key role in helping to solve what we call the 9-billion-people question.”\nThe 9 billion-people question refers to predictions that the world’s population will increase to more than 9 billion people — many of whom will live in areas where access to food is extremely scarce — by the year 2050. The question lies in how to grow enough food to feed the world’s population and prevent the host of health, economic and social problems associated with hunger and malnutrition.\nNow, with the completely sequenced African rice genome, scientists and agriculturalists can search for ways to cross Asian and African species to develop new varieties of rice with the high-yield traits of Asian rice and the hardiness of African rice.\n“African rice is once more at the forefront of cultivation strategies that aim to confront climate change and food availability challenges,” said Judith Carney, a professor in the Department of Geography and the Institute of the Environment and Sustainability at the University of California, Los Angeles, and author of “Black Rice.” The book describes the historical importance of African rice, which was brought to the United States during the period of transatlantic slavery.\nCarney is also a co-author on the Nature Genetics paper, and her book served as one of the inspirations behind sequencing the African rice genome.\nAlthough it is currently cultivated in only a handful of locations around the world, African rice is hardier and more resistant to environmental stress in West African environments than Asian varieties, Wing said.\nAfrican rice already has been crossed with Asian rice to produce new varieties under a group known as NERICA, which stands for New Rice for Africa.\nThe African rice genome is especially important because many of the genes code for traits that make African rice resistant to environmental stress, such as long periods of drought, high salinity in the soils and flooding.\n“Now that we have a precise knowledge of the genome we can identify these traits more easily and move genes more rapidly through conventional breeding methods, or through genetic modification techniques,” noted Wing, who is also a member of the UA’s BIO5 Institute and holds the Axa Endowed Chair of Genome Biology and Evolutionary Genomics at the International Rice Research Institute. “The idea is to create a super-rice that will be higher yielding but will have less of an environmental impact — such as varieties that require less water, fertilizer and pesticides.”\nWing is also working with Quifa Zhang from Huazhong Agricultural University in Wuhan, China, to create a set of super-crop science and technology centers around the world, where focused and coordinated efforts could help solve the 9 billion-people question. “We really only have about 25 years to solve this problem, and if we’re always competing with each other it’s not going to work,” he said.\nIn November, Wing and his collaborators will celebrate the 10th anniversary of the completion of the Asian rice genome and the new completion of the African rice genome at the 12th International Symposium on Rice Functional Genomics, a conference that will be held in Tucson, Arizona.\nAbstract of Nature Genetics paper\nThe cultivation of rice in Africa dates back more than 3,000 years. Interestingly, African rice is not of the same origin as Asian rice (Oryza sativa L.) but rather is an entirely different species (i.e., Oryza glaberrima Steud.). Here we present a high-quality assembly and annotation of the O. glaberrima genome and detailed analyses of its evolutionary history of domestication and selection. Population genomics analyses of 20 O. glaberrima and 94 Oryza barthii accessions support the hypothesis that O. glaberrima was domesticated in a single region along the Niger river as opposed to noncentric domestication events across Africa. We detected evidence for artificial selection at a genome-wide scale, as well as with a set of O. glaberrima genes orthologous to O. sativa genes that are known to be associated with domestication, thus indicating convergent yet independent selection of a common set of genes during two geographically and culturally distinct domestication processes.']	['<urn:uuid:d8bb7bff-56fa-40d8-8832-a5e87d299540>']	open-ended	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	10	90	788
33	As a network engineer, I wonder how does BBR compare to EMV in handling congestion control?	These are quite different technologies - EMV is for payment card systems while BBR is specifically for network congestion control. BBR is a TCP congestion control algorithm that estimates bottleneck bandwidth and round-trip propagation time to optimize network performance, showing 2-25x throughput improvements over previous algorithms like CUBIC. EMV, on the other hand, is a specification for integrated circuit payment cards focused on ensuring worldwide interoperability and secure transactions, with no direct role in network congestion management.	"['EMV Integrated Circuit Card SpecificationsEMVCo\'s primary role is to manage, maintain and enhance the EMV™ Integrated Circuit Card Specifications to ensure interoperability and acceptance of payment system integrated circuit cards on a worldwide basis. EMVCo also maintains specifications and testing procedures for terminal compliance testing and card type approval testing to help ensure cross payment system interoperability through compliance with the EMV specifications.\nThe testing and approval process for Visa Smart Debit/Credit (VSDC) contact-only acceptance products is managed by EMVCo. Specifications, administration documentation, test requirements and test cases may be obtained at the EMVCo site.\nA Guide to EMV\nVISA Related SpecificationsVISA Technology Specifications\n- Contact Chip\n- Visa Integrated Circuit Chip Specifications\n- Certificate Authority\n- Visa GlobalPlatform\n- Contactless Chip\n- Visa Contactless Payment Specifications (Visa payWave)\nNear Field Communication (NFC) is a standards-based short-range wireless connectivity technology that makes life easier and more convenient for consumers around the world by making it simpler to make transactions, exchange digital content, and connect electronic devices with a touch. NFC is compatible with hundreds of millions of contactless cards and readers already deployed worldwide.\nInteroperability Specification for ICCs and Personal Computer Systems\nThe PCSC specifications can be found at the PC/SC Workgroup site\nGlobalPlatform is tasked with aligning the smart card infrastructure across existing technologies and multiple operating systems. The primary objective of the organization is the development and publication of standards and specifications that can be used by companies implementing multiple application smart card programs. The GlobalPlatform card specification and terminal framework is owned, managed and amended by GlobalPlatform.\nThe GlobalPlatform specifications can be found at: http://www.globalplatform.org/\nPublic-Key Cryptography Standards\nThe Public-Key Cryptography Standards are specifications produced by RSA Laboratories in cooperation with secure systems developers worldwide for the purpose of accelerating the deployment of public-key cryptography. First published in 1991 as a result of meetings with a small group of early adopters of public-key technology, the PKCS documents have become widely referenced and implemented. Contributions from the PKCS series have become part of many formal and de facto standards, including ANSI X9 documents, PKIX, SET, S/MIME, and SSL.\nThe Public-Key Cryptography Standards can be found at the RSA LABORATORIES\n3G Mobile Communication Specifications\nThe 3rd Generation Partnership Project (3GPP) is a collaboration agreement that was established in December 1998. The collaboration agreement brings together a number of telecommunications standards bodies which are known as ""Organizational Partners"". The current Organizational Partners are ARIB, CWTS, ETSI, T1, TTA, and TTC.\nThe original scope of 3GPP was to produce globally applicable Technical Specifications and Technical Reports for a 3rd Generation Mobile System based on evolved GSM core networks and the radio access technologies that they support (i.e., Universal Terrestrial Radio Access (UTRA) both Frequency Division Duplex (FDD) and Time Division Duplex (TDD) modes). The scope was subsequently amended to include the maintenance and development of the Global System for Mobile communication (GSM) Technical Specifications and Technical Reports including evolved radio access technologies (e.g. General Packet Radio Service (GPRS) and Enhanced Data rates for GSM Evolution (EDGE)).\nThe term ""3GPP specification"" covers all GSM (including GPRS and EDGE) and 3G specifications.\nThe 3G Mobile Communication Specifications can be found at the 3GPP site\nSmart Card Documents\nWhat\'s So Smart about SmartCards [152KB PDF, 12 pages]\nGovernment Smart Card Handbook [3.7MB PDF, 262 pages]\nIBM Red book - Smart Cards: A Case Study [1.38MB PDF, 234 pages]\nSmart Card Handbook, Fourth Edition by W. Rankl, W. Effing\nRFID Handbook, Third Edition by Klaus Finkenzelle', 'BBR: Congestion-based congestion control Cardwell et al., ACM Queue Sep-Oct 2016\nWith thanks to Hossein Ghodse (@hossg) for recommending today’s paper selection.\nThis is the story of how members of Google’s make-tcp-fast project developed and deployed a new congestion control algorithm for TCP called BBR (for Bandwidth Bottleneck and Round-trip propagation time), leading to 2-25x throughput improvement over the previous loss-based congestion control CUBIC algorithm. In fact, the improvements would have been even more significant but for the fact that throughput became limited by the deployed TCP receive buffer size. Increasing this buffer size led to a huge 133x relative improvement with BBR (2Gbps), while CUBIC remained at 15Mbps. BBR is also being deployed on YouTube servers, with a small percentage of users being assigned BBR playback.\nPlaybacks using BBBR show significant improvement in all of YouTube’s quality-of-experience metrics, possibly because BBR’s behavior is more consistent and predictable… BBR reduces median RTT by 53 percent on average globally, and by more than 80 percent in the developing world.\nTCP congestion and bottlenecks\nThe Internet isn’t working as well as it should, and many of the problems relate to TCP’s loss-based congestion control, even with the current best-of-breed CUBIC algorithm. This ties back to design decisions taken in the 1980’s when packet loss and congestion were synonymous due to technology limitations. That correspondence no longer holds so directly.\nWhen bottleneck buffers are large, loss-based congestion control keeps them full, causing bufferbloat. When bottleneck buffers are small, loss-based congestion control misinterprets loss as a signal of congestion, leading to low throughput. Fixing these problems requires finding an alternative to loss-based congestion control.\nFrom the perspective of TCP, the performance of an arbitrarily complex path is bound by two constraints: round-trip propagation time (RTprop), and bottleneck bandwidth, BtlBw (the bandwidth at the slowest link in each direction).\nHere’s a picture to help make this clearer:\nThe RTprop time is the minimum time for round-trip propagation if there are no queuing delays and no processing delays at the receiver. The more familiar RTT (round-trip time) is formed of RTprop + these additional sources of noise and delay.\nBandwidth Delay Product (BDP) is the maximum possible amount of data in transit in a network, and is obtained by multiplying the bottleneck bandwidth and round-trip propagation time.\nBDP is central to understanding network performance. Consider what happens to delivery rate as we gradually increase the amount of data inflight. When the amount of inflight data is less than BDP, then delivery rate increases as we send more data – delivery rate is limited by the application. Once the bandwidth at the bottleneck is saturated though, the delivery rate cannot go up anymore – we’re pushing data through that pipe just as fast as it can go. The buffer will fill up, eventually we’ll start dropping packets, but we still won’t increase delivery rate.\nThe optimum operating point is right on the BDP threshold (blue dot above), but loss-based congestion control operates at the BDP + Bottleneck Buffer Size point (green dot above).\nNow let’s look at what happens to RTT as we increase the amount of data inflight. It can never be better than RTprop, so until we reach BDP, RTT ~= RTprop. Beyond BDP, as buffers start to fill, RTT goes up until buffers are completely full and we start dropping packets.\nOnce more, the optimum operating point would be right on the BDP threshold. This was proved by Leonard Kleinrock in 1979, unfortunately about the same time Jeffrey M. Jaffe proved that it was impossible to create a distributed algorithm that converged to this operation point. Jaffe’s result rests on fundamental measurement ambiguities.\nAlthough it is impossible to disambiguate any single measurement, a connection’s behavior over time tells a clearer story, suggesting the possibility of measurement strategies designed to resolve ambiguity.\nBBR is a congestion control algorithm based on these two parameters that fundamentally characterise a path: bottleneck bandwidth and round-trip propagation time. It makes continuous estimates of these values, resulting in a distributed congestion control algorithm that reacts to actual congestion, not packet loss or transient queue delay, and converges with high probability to Kleinrock’s optimal operating point.\n(BBR is a simple instance of a Max-plus control system, a new approach to control based on nonstandard algebra. This approach allows the adaptation rate [controlled by the max gain] to be independent of the queue growth [controlled by the average gain]. Applied to this problem, it results in a simple, implicit control loop where the adaptation to physical constraint changes is automatically handled by the filters representing those constraints. A conventional control system would require multiple loops connected by a complex state machine to accomplish the same result.)\nSince RTT can never be less than RTprop, tracking the minimum RTT provides an unbiased and efficient estimator of the round-trip propagation time. The existing TCP acks provide enough information for us to calculate RTT.\nUnlike RTT, nothing in the TCP spec requires implementations to track bottleneck bandwidth, but a good estimate results from tracking delivery rate.\nThe average delivery rate between a send and an ack is simply the amount of data delivered divided by the time taken. We know that this must be less than the true bottleneck delivery rate, so we can use the highest recorded delivery rate as our running estimate of bandwidth bottleneck.\nPutting this altogether leads to a core BBR algorithm with two parts: a protocol to follow on receiving an ack, and a protocol to following when sending. You’ll find the pseudocode for these on pages 28 and 29-30. From my reading, there are a couple of small mistakes in the pseudocode (but I could be mistaken!), so I’ve recreated clean versions below. Please do check against those in the original article if you’re digging deeper…\nHere’s the ack protocol:\napp_limited_until is set on the sending side, when the app is not sending enough data to reach BDP). This is what the sending protocol looks like:\nThe pacing_gain controls how fast packets are sent relative to BtlBw and is key to BBR’s ability to learn. A pacing_gain greater than 1 increases inflight and decreases packet inter-arrival time, moving the connection to the right on the performance charts. A pacing_gain less than 1 has the opposite effect, moving the connection to the left. BBR uses this pacing_gain to implement a simple sequential probing state machine that alternates between testing for higher bandwidths and then testing for lower round-trip times.\nThe frequency, magnitude, duration and structure of these experiments differ depending on what’s already known (start-up or steady state) and the sending app’s behaviour (intermittent or continuous). Most time is spent in the ProbeBW state probing bandwidth. BBR cycles through a sequence of gains for pacing_gain, using an eight-phase cycle with values 5/4, 3/4, 1, 1, 1, 1, 1, 1. Each phase lasts for the estimated round-trip propagation time.\nThis design allows the gain cycle first to probe for more bandwidth with a pacing_gain above 1.0, then drain any resulting queue with a pacing_gain an equal distance below 1.0, and then cruise with a short queue using a pacing_gain of 1.0.\nThe result is a control loop that looks like this plot below showing the RTT (blue), inflight (green) and delivery rate (red) from 700ms of a 10Mbps, 40-ms flow.\nHere’s how BBR compares to CUBIC during the first second of a 10 Mbps, 40-ms flow. (BBR in green, CUBIC in red).\nWe talked about the BBR benefits in Google’s high-speed WAN network (B4) and in YouTube in the introduction. It also has massive benefits for low bandwidth mobile subscriptions.\nMore than half of the world’s 7 billion mobile Internet subscriptions connect via 8-to 114-kbps 2.5 G systems, which suffer well-documented problems because of loss-based congestion control’s buffer-filling propensities. The bottleneck link for these systems is usually between the SGSN (serving GPRS support node)18 and mobile device. SGSN software runs on a standard PC platform with ample memory, so there are frequently megabytes of buffer between the Internet and mobile device. Figure 10 [ below] compares (emulated) SGSN Internet-to-mobile delay for BBR and CUBIC.']"	['<urn:uuid:2bfbcacb-548f-455a-a97a-36c3f17e5e6c>', '<urn:uuid:db78bbbe-8f77-4975-9d4f-de19c8041064>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T19:58:41.790432	16	77	1934
34	compare thorne zytkow object formation eclipsing binary formation	The formation processes of these stellar systems are quite different. Eclipsing binary stars form as pairs where each star gravitationally attracts the other and orbits around their common center of mass, with their orbital plane aligned edge-on toward Earth. In contrast, a Thorne-Żytkow object forms when a neutron star collides with a red giant or supergiant star. This collision can occur either in crowded globular clusters or in binary systems where one star goes supernova and the resulting neutron star's orbit intersects with its companion. The collision process in TŻOs involves the neutron star spiraling into the red giant's core over hundreds of years, potentially resulting in either a single neutron star or a black hole if their combined mass exceeds certain limits.	"[""Our editors will review what you’ve submitted and determine whether to revise the article.Join Britannica's Publishing Partner Program and our community of experts to gain a global audience for your work!\n- Phenomena observed during eclipses\n- The geometry of eclipses, occultations, and transits\n- The frequency of solar and lunar eclipses\n- Eclipse research activities\n- Solar research\n- Transits of Mercury and Venus\n- Eclipsing binary stars\n- Eclipses in history\n- Literary and historical references\nEclipsing binary stars\nAstronomers have estimated that more than half of all stars in the Milky Way Galaxy are members of a double or a more complex multiple star system. Most of these are too far from Earth for the individual stars to be resolved. In a double star, or binary, system (see binary star), each star attracts the other gravitationally and orbits about a unique point, the centre of mass of the pair. If the plane of their orbits lies edge-on toward Earth, each star will be seen to eclipse the other once each orbital period. Such a system is known as an eclipsing binary.\nIn an eclipsing binary system, the total amount of light varies periodically; for this reason it is alternatively called an eclipsing variable star. The light curve of an eclipsing binary—i.e., a plot of its changes in brightness over time—has a deep minimum when the brighter star is eclipsed and a shallower minimum when the dimmer star is eclipsed. The variable star Algol, or Beta Persei, was the first eclipsing binary to be recognized as such.\nEclipsing binaries are the principal sources of information on the masses and radii of stars. A complete analysis of the light curve can yield the radii of the stars (in units of their separation); orbital characteristics such as eccentricity, orientation in space, and tilt with respect to Earth; and even the surface temperatures of the stars. Kepler’s third law relates the orbital period, the separation of the stars, and the sum of their masses. From observations of the periodic shifts of each star’s spectral lines due to motion of the star toward or away from Earth (the Doppler effect), astronomers can determine the velocity along the line of sight of each star in its orbit. The ratio of the stellar masses then follows from their velocities. With the sum and ratio of the masses in hand, both masses can be determined.\nStudies of eclipsing binaries have revealed unexpected structural details and time-related changes in the component stars. Some stars turn out to have dark starspots, for example, similar to but much larger than sunspots on the Sun. Other stars flare in brightness as mass is exchanged from one component to the other. Rapid rotation of some stars flattens their shapes into ellipsoids. Even the long-known solar phenomenon of limb darkening, the gradual decrease in brightness from the centre to the edge of the Sun’s disk, has been detected in the component stars of eclipsing binaries.\nZeta Aurigae is the prototype of a class of eclipsing binaries composed of a cool supergiant star and a hot blue star. Although the supergiant’s atmosphere is large enough to reach to the orbit of Venus were the star to replace the Sun in the solar system, it is very rarefied. When the blue star first passes behind the supergiant, its light is not fully extinguished but travels through the supergiant’s cool atmosphere, which modifies the light’s characteristics. Thus, the blue star acts as a probe of the supergiant’s atmosphere. By analyzing the combined spectra of the two stars, astronomers can determine the temperature, density, and composition of the supergiant’s atmosphere.\nBeta Lyrae is the prototype of another class of eclipsing binaries, in which one star is embedded in a ring or disk of material that it has pulled off the other star. One star has twice the mass of the Sun; the companion star is much dimmer, though it has a mass of about 12 Suns. This binary is highly variable, and it shows signs that mass is spiraling from one star to the other at a rate of about five Earth masses per year. This exchange of mass has apparently caused an increase in the orbital period, from 12.89 days in 1784, when it was discovered, to 12.94 days in 1978.Jack B. Zirker Jakob Houtgast\nEclipses in history\nEclipses of the Sun and Moon are often quite spectacular, and in ancient and medieval times they were frequently recorded as portents—usually of disaster. Hence, it is not surprising that many of these events are mentioned in history and literature as well as in astronomical writings.\nWell over 1,000 individual eclipse records are extant from various parts of the ancient and medieval world. Most known ancient observations of these phenomena originate from only three countries: China, Babylonia, and Greece. No eclipse records appear to have survived from ancient Egypt or India, for example. Whereas virtually all Babylonian accounts are confined to astronomical treatises, those from China and Greece are found in historical and literary works as well. However, the earliest reliable observation is from Ugarit of a total solar eclipse that happened on March 3, 1223 bce. The first Assyrian record dates from much later, June 15, 763 bce. From then on, numerous Babylonian and Chinese observations are preserved. Eclipses are occasionally noted in surviving European writings from the Dark Ages (for instance, in the works of the 5th-century bishop Hydatius and the 8th-century theologian and historian St. Bede the Venerable). However, during this period only the Chinese continued to observe and report such events on a regular basis. Chinese records in the traditional style continued almost uninterrupted to modern times.\nMany eclipses were carefully recorded by the astronomers of Baghdad and Cairo between about 800 and 1000 ce. Also after about 800, both European and Arabic annalists began to include in their chronicles accounts of eclipses and other remarkable celestial phenomena. Some of these chronicles continued until the 16th century and even later, although the peak period was between about 1100 and 1400. About 1450, European astronomers commenced making fairly accurate measurements of the time of day or night when eclipses occurred, and this pursuit spread rapidly following the invention of the telescope. This discussion is confined to eclipse observations made in the pretelescopic period.\nThe present-day value of ancient and medieval records of eclipses falls into two main categories: (1) chronological, depending mainly on the connection between an eclipse and a significant historical event, and (2) astronomical, especially the study of long-term variations in the length of the mean solar day.\nThe Sun is usually so brilliant that the casual observer is liable to overlook those eclipses in which less than about 80 percent of the solar disk is obscured. Only when a substantial proportion of the Sun is covered by the Moon does the loss of daylight become noticeable. Hence, it is rare to find references to small partial eclipses in literary and historical works. At various times, astronomers in Babylonia, China, and the Arab lands systematically reported eclipses of small magnitude, but their vigilance was assisted by their ability to make approximate predictions. They thus knew roughly when to scrutinize the Sun. Arab astronomers sometimes viewed the Sun by reflection in water to diminish its brightness when watching for eclipses. The Roman philosopher and writer Seneca (c. 4 bce–65 ce), on the other hand, recounts that, in his time, pitch was employed for this purpose. It is not known, however, whether such artificial aids were used regularly.\nWhen the Moon covers a large proportion of the Sun, the sky becomes appreciably darker, and stars may appear. On those rare occasions when the whole of the Sun is obscured, the sudden occurrence of intense darkness, accompanied by a pronounced fall in temperature, may leave a profound impression on eyewitnesses. Total or near-total eclipses of the Sun are of special chronological importance. On average, they occur so infrequently at any particular location that if the date of such an event can be established by historical means to within a decade or two, it may well prove possible to fix an exact date by astronomical calculation.\nThe Moon even when full is much dimmer than the Sun, and lunar eclipses of quite small magnitude are thus fairly readily visible to the unaided eye. Both partial and total obscurations are recorded in history with roughly comparable frequency. As total eclipses of the Moon occur rather often (every two or three years on average at a given place), they are of less chronological importance than their solar counterparts. There are, however, several notable exceptions, as is discussed below.\nLiterary and historical references\nAccording to long-established tradition, the history of astronomy in ancient China can be traced back before 2000 bce. The earliest relics that are of astronomical significance date from nearly a millennium later, however. The Anyang oracle bones (inscribed turtle shells, ox bones, and so forth) of the latter part of the Shang dynasty (c. 1600–1046 bce), which were uncovered near Anyang in northeastern China, record several eclipses of both the Sun and the Moon. The following report is an example:\nOn day guiyou [the 10th day of the 60-day cycle], it was inquired [by divination]: “The Sun was eclipsed in the evening; is it good?” On day guiyou it was inquired: “The Sun was eclipsed in the evening; is it bad?”\nThe above text provides clear evidence that eclipses were regarded as omens at this early period (as is true of other celestial phenomena). Such a belief was extremely prevalent in China during later centuries. The term translated here as “eclipse” (shi) is the same as the word for “eat.” Evidently the Shang people thought that a monster actually devoured the Sun or Moon during an eclipse. Not until many centuries later was the true explanation known, but by then the use of the term shi was firmly established to describe eclipses, and so it remained throughout Chinese history. The oracle-bone text, translated above, twice gives the day of the sexagenary cycle; this cycle, which was independent of any astronomical parameter, continued in use (seemingly without interruption) until modern times. Nevertheless, as the year in which an eclipse occurred is never mentioned on the preserved oracle bones (many of which are mere fragments), dating of these observations by astronomical calculation has proved extremely difficult. In general, Shang chronology is still very uncertain.\nThe Shijing (“Classic of Poetry”) contains a lamentation occasioned by an eclipse of the Moon followed by an eclipse of the Sun. The text, dating from the 8th century bce, may be translated:\nThe Sun was eclipsed, we found it greatly ominous…that this Moon is eclipsed is but an ordinary matter; but that this Sun is eclipsed—wherein lies the evil?\nThe different attitudes toward solar and lunar eclipses at this time is interesting. Throughout the subsequent 1,000 years or so, lunar eclipses were hardly ever reported in China—in marked contrast to solar obscurations, which were systematically observed during much of this period. The earliest of these observations are recorded in a chronicle of the Chinese state of Lu (now in Shandong province), the birthplace of Confucius. This work, known as the Chunqiu (“Spring and Autumn [Annals]”), lists many solar eclipses between 722 and 481 bce. On three occasions the Chunqiu describes eclipse ceremonies in which drums were beaten and oxen were sacrificed. Further, three eclipses (occurring in 709, 601, and 549 bce) were described as total. The earliest of these, that of July 17, 709 bce, is recorded as follows:\nThird year of Duke Huan, 7th month, day renchen [the 29th day of the cycle], the first day of the month. The Sun was eclipsed and it was total.\nComputation shows that this eclipse was indeed total at Qufu, the Lu capital.\nFrom about 200 bce (following the unification of China into a single empire), a wide variety of celestial phenomena began to be noted on a regular basis. Summaries of these records are found in astronomical treatises contained in the official histories. In many instances, a report is accompanied by a detailed astrological prognostication. For example, the Houhanshu (“History of the Later Han Dynasty”) contains the following account under a year corresponding to 119–120 ce:\nOn the day wuwu [the 55th cyclical day], the 1st day of the 12th lunar month, the Sun was eclipsed; it was almost complete. On the Earth it became like evening. It was 11 deg in the constellation of the Maid. The woman ruler [i.e., the empress dowager] showed aversion to it. Two years and three months later, Deng, the empress dowager, died.\nThe date of this eclipse is equivalent to January 18, 120. On this exact day there occurred an eclipse of the Sun that was very large in China. The above-cited text is particularly interesting because it clearly describes an obscuration of the Sun, which, though causing dusk conditions, was not quite total where it was seen. With regard to the accompanying prognostication, it should be pointed out that a delay of two or three years between the occurrence of a celestial omen and its presumed fulfillment is quite typical of Chinese astrology.\nSystematic observation of lunar eclipses in China began about 400 ce, and from this period onward the official astronomers often timed the various phases of both solar and lunar eclipses with the aid of clepsydras (water clocks). Chinese astronomical techniques spread to Korea and Japan, and, especially after 1000 ce, eclipses were regularly observed independently in all three countries. However, the Chinese records are usually the most detailed.\nThe following account from the Yuanshi (“History of the Yuan Dynasty”) of the total lunar eclipse of May 19, 1277, follows the customary practice of quoting timings in double hours (12 to a combined day and night) and marks (each equal to 1/100 of a day and night, or 0.24 hour):\n14th year of the Zhiyuan reign period, 4th month, day guiyou [the 10th cyclical day], full Moon. The Moon was eclipsed. Beginning of loss at 6 marks in the hour of zi; the eclipse was total at 3 marks in the hour of chou; maximum at 5 marks in the hour of chou; reappearance of light at 7 marks in the hour of chou; restoration to fullness at 4 marks in the hour of yin.\nThe three consecutive double hours zi, chou, and yin correspond, respectively, to 11 pm to 1 am, 1 am to 3 am, and 3 am to 5 am. The measured times are equivalent to 12:34 am (start of eclipse), 1:50 am (beginning of totality), 2:19 am (mid-eclipse), 2:48 am (end of totality), and 4:05 am (end of eclipse).\nFrom the 3rd century ce onward, there is evidence of attempts at predicting eclipses by Chinese astronomers. Crude at first, these predictions reached their peak accuracy near the end of the 13th century, with typical timing errors of about one-fourth of an hour."", 'A Thorne–Żytkow object (TŻO or TZO) is a type of star wherein a red giant or supergiant contains a neutron star at its core, formed from the collision of the giant with the neutron star. Such objects were hypothesized by Kip Thorne and Anna Żytkow in 1977. In 2014, it was discovered that the star HV 2112 was a strong candidate.\nA Thorne–Żytkow object is formed when a neutron star collides with a star, typically a red giant or supergiant. The colliding objects can simply be wandering stars. This is only likely to occur in extremely crowded globular clusters. Alternatively, the neutron star could form in a binary system after one of the two stars went supernova. Because no supernova is perfectly symmetric, and because the binding energy of the binary changes with the mass lost in the supernova, the neutron star will be left with some velocity relative to its original orbit. This kick may cause its new orbit to intersect with its companion, or, if its companion is a main-sequence star, it may be engulfed when its companion evolves into a red giant.\nOnce the neutron star enters the red giant, drag between the neutron star and the outer, diffuse layers of the red giant causes the binary star system\'s orbit to decay, and the neutron star and core of the red giant spiral inward toward one another. Depending on their initial separation, this process may take hundreds of years. When the two finally collide, the neutron star and red giant core will merge. If their combined mass exceeds the Tolman-Oppenheimer-Volkoff limit then the two will collapse into a black hole, resulting in a supernova that disperses the outer layers of the star. Otherwise, the two will coalesce into a single neutron star.\nThe surface of the neutron star is very hot, with temperatures exceeding 109 K: hotter than the cores of all but the most massive stars. This heat is dominated either by nuclear fusion in the accreting gas or by compression of the gas by the neutron star\'s gravity. Because of the high temperature, unusual nuclear processes may take place as the envelope of the red giant falls onto the neutron star\'s surface. Hydrogen may fuse to produce a different mixture of isotopes than it does in ordinary stellar nucleosynthesis, and some astronomers have proposed that the rapid proton nucleosynthesis that occurs in X-ray bursts also takes place inside Thorne–Żytkow objects.\nIt has been theorized that the evolution of TŻOs will result in neutron stars with massive accretion discs, as mass loss will end the TŻO stage, and the remaining envelope converts to being a disc. These neutron stars may form the population of isolated pulsars with accretion discs. The massive accretion disc may also result in the collapse of a star, becoming a stellar companion to the neutron star. The neutron star may also accrete sufficient material to collapse into a black hole.\nAs of 2014, the most recent candidate, star HV 2112, has been observed to have some unusual properties that suggest that it may be a Thorne–Żytkow object. The discovering team have noted that HV 2112 displays some chemical characteristics that don\'t quite match theoretical models, but emphasize that the theoretical predictions for Thorne–Żytkow object are quite old and theoretical improvements have been made since it was originally conceptualized.\nList of candidate TŻOs\n|HV 2112||01h 10m 03.87s||−72° 36′ 52.6″||Small Magellanic Cloud||2014||This star was previously catalogued as an asymptotic-giant-branch star, but observationally is a better fit for red supergiant status.|||\n|U Aquarii||22h 03m 19.69s||−16° 37′ 35.2″||Aquarius||1999||This star was catalogued as a R Coronae Borealis variable.|||\n|VZ Sagittarii||18h 15m 08.58s||−29° 42′ 29.6″||Sagittarius||1999||This star was catalogued as a R Coronae Borealis variable.|||\nList of candidate former TŻOs\n|Candidate former TŻO||Right Ascension||Declination||Location||Discovery||Notes||Refs|\n|GRO J1655-40||16h 54m 00.14s||−39° 50′ 44.9″||Scorpius||1995||The progenitor for both the companion star and the black hole in this system is hypothesized to have been a TŻO.|||\n- Thorne, Kip S.; Żytkow, Anna N. (15 March 1977). ""Stars with degenerate neutron cores. I - Structure of equilibrium models"". The Astrophysical Journal 212 (1): 832–858. Bibcode:1977ApJ...212..832T. doi:10.1086/155109.\n- Levesque, Emily M.; Massey, Philip; Zytkow, Anna N.; Morrell, Nidia (2014). ""Discovery of a Thorne–Żytkow object candidate in the Small Magellanic Cloud"". Monthly Notices of the Royal Astronomical Society: Letters 443: L94. arXiv:1406.0001. Bibcode:2014MNRAS.443L..94L. doi:10.1093/mnrasl/slu080. Lay summary – PhysOrg (4 June 2014).\n- Brandt, W. Niel; Podsiadlowski, Philipp (May 1995). ""The effects of high-velocity supernova kicks on the orbital properties and sky distributions of neutron-star binaries"". Monthly Notices of the Royal Astronomical Society 274 (2): 461–484. Bibcode:1995MNRAS.274..461B. doi:10.1093/mnras/274.2.461.\n- Vanture, Andrew; Zucker, Daniel; Wallerstein, George (April 1999). ""U Aquarii a Thorne–Żytkow Object?"". The Astrophysical Journal 514 (2): 932–938. Bibcode:1999ApJ...514..932V. doi:10.1086/306956.\n- Eich, Chris; Zimmerman, Mark; Thorne, Kip; Żytkow, Anna N. (November 1989). ""Giant and supergiant stars with degenerate neutron cores"". The Astrophysical Journal 346 (1): 277–283. Bibcode:1989ApJ...346..277E. doi:10.1086/168008.\n- Cannon, Robert; Eggleton, Peter; Żytkow, Anna N.; Podsialowsky, Philip (February 1992). ""The structure and evolution of Thorne-Zytkow objects"". The Astrophysical Journal 386 (1): 206–214. Bibcode:1992ApJ...386..206C. doi:10.1086/171006.\n- Cannon, Robert (August 1993). ""Massive Thorne–Żytkow Objects – Structure and Nucleosynthesis"". Monthly Notices of the Royal Astronomical Society 263 (4): 817. Bibcode:1993MNRAS.263..817C. doi:10.1093/mnras/263.4.817.\n- Levesque, Emily; Massey, Philip; Żytkow, Anna; Morrell, Nidia (30 May 2014). ""Discovery of a Thorne-Zytkow object candidate in the Small Magellanic Cloud"". Monthly Notices of the Royal Astronomical Society Letters 1406: 1. arXiv:1406.0001. Bibcode:2014MNRAS.443L..94L. doi:10.1093/mnrasl/slu080.\n- Foellmi, C.; Moffat, A.F.J. (2002). ""Are Peculiar Wolf-Rayet Stars of Type WN8 Thorne-Zytkow Objects?"". In Shara, Michael M. Stellar Collisions, Mergers and their Consequences. ASP Conference Proceedings. arXiv:astro-ph/0607217. Bibcode:2002ASPC..263..123F. ISBN 1-58381-103-6.\n- Mereghetti, Sandro (1995). ""A Spin-down Variation in the 6 Second X-Ray Pulsar 1E 1048.1-5937"". Astrophysical Journal (December 1995) 455: 598. Bibcode:1995ApJ...455..598M. doi:10.1086/176607.\n- Brandt, W. Niel; Podsiadlowski, Philipp; Sigurðsson, Steinn (1995). ""On the high space velocity of X-ray Nova SCO 1994: implications for the formation of its black hole"". Monthly Notices of the Royal Astronomical Society (15 November 1995) 277 (2): L35–L40. Bibcode:1995MNRAS.277L..35B.']"	['<urn:uuid:ff20048d-7bb5-4ea4-802e-ef477534c0ce>', '<urn:uuid:9563d9a9-8ef7-41c4-817d-e89b610a5039>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T19:58:41.790432	8	123	3485
35	sewer system rain capacity modern challenges	Sewer systems face dual challenges with rainfall. Locally, wastewater backs up into houses during heavy rains due to partial blockages in sewer lines. On a broader scale, climate change is causing more severe weather events, making storm sewers increasingly undersized for current rainfall patterns. Systems built for historical '100-year storm' levels may be inadequate as these storms become more frequent and intense.	"[""- What is the Athens sewer moratorium?\n- Why is my sewer bill higher than my water bill?\n- What is the availability charge?\n- What causes wastewater to back up into my house when it rains?\n- How do you treat wastewater and return it to the environment?\n- Can I tour a wastewater treatment plant?\n- If your home has a Grinder Pump\nWhat is the Athens sewer moratorium?\nBy definition, a moratorium is an authorized suspension of some\nactivity. In the case of the sewer moratorium in Athens, at the direction of\nthe State of Tennessee, AUB must comply with very stringent guidelines\nregarding any new sewer connections. The State mandate, called an Agreed\nOrder, is aimed at minimizing the amount of untreated water that must\nbypass our collection and treatment process during, for instance, a heavy\nrainfall. The sewer lines, especially individual service lines going to\nhomes, in the AUB area are fairly old. Many have cracks due to age and\ntree roots that have wrapped around the pipes. These cracks allow storm\nwater and groundwater to inundate the sewer system, which overloads our\npipes and our treatment plant. If the overload is heavy enough, not all\nof the water can be handled, so some must bypass the treatment process.\nThis is what the moratorium is intended to minimize. By and large, unless\na septic tank at a location is failing, AUB cannot connect any new sewer\nservices under the moratorium's conditions. However, some exceptions do\napply. AUB is continually working to fix old pipes and sewer lines. As\nwe fix and replace old pipes, we are able to add new services per the\nWhy is my sewer bill higher than my\nSimply put, treating wastewater is much more expensive than\ntreating drinking water. Your wastewater bill amount is determined by\nthe amount of water that you use. The reasoning is that water you use\nat home generally goes to an AUB wastewater treatment plant. Exceptions\nwould be watering a garden, washing a car, and so forth. But the vast\nmajority of residential water used ends up at a treatment plant.\nCalculating wastewater billing based on water usage is standard to the\nwater utilities industry. To meter wastewater is even more expensive.\nThe solids in wastewater will not pass through a meter unless pretreatment\nof the wastewater (such as grinding and liquefying) is performed before\nit goes through the meter.\nWhat is the availability charge?\nAUB has a cost associated with building and maintaining the\ninfrastructure of our utility systems. Regardless of the amount of\nelectricity, natural gas or water sold, there are fixed operating\ncosts incurred by AUB each month. The availability charge helps\nrecover a portion of these fixed costs. For instance, your mortgage,\nrent, insurance, etc., are costs that remain fixed month to month\neven if your income changes. Some of AUBs fixed costs include:\n- Maintaining the wastewater collection system, pump system and treatment facilites\n- Maintenance of lines and poles, pipes and facilities\n- Interstate natural gas pipeline demand charges\n- Natural gas storage facility reservation charges\n- Safety and Inspection Programs for Customers\nWhat causes wastewater to back\nup into my house when it rains?\nThis is usually caused by a partial blockage in the sewer line.\nPaper towels, for instance, are the worst culprits when it comes to blocked\nlines, especially on or near the customer's connection to the AUB wastewater\nHow do you treat wastewater and return it\nto the environment?\nfor a movie on how AUB treats wastewater. For more info on\nwater and the environment, check out the\nWater Environment Federation.\nAfter traveling through the collection lines and reaching the\ntreatment plant, the wastewater runs through a bar screen-a large metal\nrack with rods placed every few inches-to remove large items such as trash,\nplastic, rags, sticks, etc. Next, the water flows very slowly through\nsettling tanks where floating material such as oil and grease are skimmed\noff and where solids settle to the bottom. These solids form what is\ncalled primary sludge, which, along with the oil and grease, is pumped\nto a solids treatment process. At this point, the water still contains some\nsolids that must be removed. It then is sent through a secondary, aerated\ntreatment process involving microorganisms. The microorganisms use the\nremaining solids as their food supply. These microorganisms eventually\nsettle to the bottom as well, as they move through large basins called\nclarifiers. Some of these are circulated back through the process to\ncontinue their cleaning work, while others are simply removed as the\ncolony of organisms grows. (Solids that are removed after the primary\nand secondary treatment phases go through a solids digestion process,\nwhere they are heated in the presence of still other biological organisms.\nAfter digestion takes place, the remaining material can be reused beneficially,\nsuch as to condition and fertilize agricultural soil.) As for the clarified\nwater, it travels from secondary treatment through tanks or channels where\neither chlorine, ultraviolet light, or a combination of both, are used for\ndisinfection. The clean, disinfected water can now be returned to the\nCan I tour a wastewater treatment plant?\nYes. AUB is happy to provide tours to individuals and\ngroups. To arrange a tour, just call us at 745-4501 and tell the\nperson you speak with that you would like to learn more about our\nprocesses by taking a tour."", 'What do you think is the most likely reason for a water grid shutdown? It isn’t terrorism and it isn’t pollution. The answer is, the age of the infrastructure used to deliver the water or collect the sewage. Think about the city you live in. Chances are it has existed for hundreds of years, if not longer. Most cities water systems grow in phases. They keep adding to the network every year as the population grows. The end result is most cities have infrastructure that range from less than a year to hundreds of years and with many different materials. I have seen water mains made of wood in service as late as 2011. Like any piece of equipment it all has a useful lifespan, beyond that lifespan failures become increasingly more likely to be catastrophic. The result is a large volume and dollar amount of material and equipment that needs repairs or to be replaced.\nNormally the stress on water systems comes from population growth. A water main that was ok in the 1980’s may not be large enough for today’s population. As cities infill and build higher density buildings they frequently overburden the water systems. Either causing contamination or total failure of the system.\nOlder equipment is also more susceptible to natural disasters, terrorism and human accidents. These three things can break a new system too, however they don`t have to try as hard with the older systems.\nClimate change is having an affect too. As severe weather events are on the rise, storm sewers might be found lacking, as was the case in Calgary, Alberta and Toronto, Ontario recently. The system was grossly undersized for the amount of rain that fell. They said things like “it was a month’s worth of rain in one day” on the news. When the fact is, it once was a months worth of rain, and is now something more frequent, lets say a weeks worth of rain. I’m not suggesting we build our systems to meet a 1000 year storm, but I am suggesting that our current idea of a 100 year storm may be an underestimation and that the error is getting worse. To bring it back to infrastructure, if we are built to the current 100 year storm levels, what happens if the 100 year storms are getting worse? We will find out in the not too distant future.\nWhat are the options for people to take? The first and most important thing to do is to plan ahead and replace older parts of the system before they fail. A $50,000 job to replace an old section of pipe at a time you choose is a lot cheaper than waiting for it to fail at the time you are least prepared. If you are connected to a public utility, ask them about their equipment replacement plan. If they are not looking 25 years into the future or longer then ask them why not? If you have private systems, you need to ask the same questions. Can you afford to replace the septic system when it fails? Or can you afford to dig/drill a new well when the casing cracks?\nAs you can probably surmise the addition of more people + more rain + more water and more sewage means system failures will become more frequent and probably for longer periods of time. What does this mean to the average person? Plan for system failure. Have a backup system ready to go when it does. Know the age of your equipment and it`s expected lifetime. That way you wont be caught off guard.']"	['<urn:uuid:7cc4b372-8398-4d0e-b561-2af6c6694d49>', '<urn:uuid:ec1f5fba-4bf9-4bc9-b1ec-9c5b74891271>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T19:58:41.790432	6	62	1505
36	peruvian saddle padding versus us dragoon saddle padding	The Peruvian saddle uses leather padding called pellonera as a seat cushion for work saddles without skirts, while US Dragoon saddles were required to use issue gray wool bed blankets or shelter halves for extra padding, with additional saddle pads being strongly discouraged.	"['Peruvian horse-riding tack, as well as that used by horsemen throughout the American continent, has its origin in fifteenth century Europe. Equestrian portraits by European classical painters like Vasquez, Titian and Van Dyck show saddles bearing close resemblance to the modern Peruvian saddle. When the Spanish Conquistadors arrived in America, they brought with them their European riding equipment. This was modified over the years in accordance with the uses to which the horses were put, and the availability of certain materials. In Peru the need was to essentially maintain the features of a comfortable and secure saddle, as horses were mainly used for transportation. This gave rise to the Peruvian montura de cajón or box saddle, thus named because the rider sits ""boxed in"" between the pommel and cantle.\nThe saddle consists of a wooden frame (saddle tree) with a moderately high pommel and cantle. The tree is covered with tight-fitting pieces of rawhide, with the cinch, stirrups, crupper, breeching buckles and straps attached to the frame. To make the saddle more comfortable and protect the rider’s legs from rubbing against the buckles and straps, leather skirts are usually placed over the saddle tree and around the pommel and cantle. These skirts are often embossed with the beautiful designs for which Peruvian leather artisans have become famous.\nWork saddles that do not have skirts use a leather pad, called pellonera, as a seat cushion. The pellonera can also be used for added comfort over saddles with skirts.\nTo give saddles a better appearance, the pommel and cantle are sometimes covered with fine leather. More ornate saddles have rivets of nickel or silver on the borders of the pommel and cantle and along the edges of the skirts.\nThe carona is a thick leather pad that goes under the saddle and over the blanket, and is decorated with the same motifs as the saddle skirt. Besides enhancing the appearance of the saddle, it protects the back of the horse from the weight of the rider and also shields the saddle from the horse’s sweat.\nTo compliment Peruvian show tack, and as a sign of wealth and good taste, a pellón is sometimes used. The pellón, or ‘Pellón San Pedrano’, is a type of tapestry used as a pad over the saddle, and is described by Verne R. Albright in “The Peruvian Paso and His Classic Equitation” as being ""composed of thousands of hand tied spit braids made from black dyed wool and inserted into a rug type backing. The underside is lined with fine kid leather and usually contains pockets.""\nThe pockets were used to keep valuable belongings in bygone times when horses were the principal method of transportation. The pellón itself could also be used as bedding when long journeys required the rider to dismount and rest.\nOne of the peculiarities of Peruvian tack is the use of the breechings called the guarnición. Much has been written about the origin and purpose of the guarnición, but it most likely derived from a harness first used to prevent the saddle from slipping forward when riding over rough terrain. Over time, the utilitarian purpose of this harness gave way to an ornamental use and the guarnición became a traditional part of Peruvian tack. It consists of long leather straps (retrancas) that encircle the rear of the horse and are attached to buckles on each side of the saddle. They are further secured by two lateral straps (caidas) attached to the base of the tail cover. The florón or tail cover is an elongated piece of leather fixed to the back of the saddle by a large, ornate buckle. The term florón, meaning “big flower” in Spanish, is probably derived from the round shape of the middle section of the tailpiece, traditionally embossed with floral designs. More recently however, the creativity of leather artisans has given rise to a variety of designs that include linear motifs, horses, seal of arms and other fanciful leather work. As a general rule, the guarnición should have the same embossed patterns as the rest of the tack.\nFinally, a crupper is always used in conjunction with the Peruvian saddle. The crupper is attached to the same buckle that holds the tailpiece, and both crupper and tailpiece are held together by a short leather strap called a cruzeta.\nPerol Chico uses working saddles with comfortable leather seat padding for its rides. The padding is thick enough to provide a comfortable seat during long rides, and thin enough to maintain contact with the horse through the seat, which is essential in our riding style. Too much padding, like the thick sheepskins used in some countries, might be very comfortable for the rider but it diminish his contact with the horse. A horse can feel a fly on his skin so he can also feel and respond, if well trained, to subtle aids given by the riders’ seat.', 'Equipment list for the aspiring\n2nd Dragoon/Cavalry Reenactor\nA sad tale...\nListen my friends and you shall hear, of the shopping tale of a\nOne pistol would do but two were more fun, now not enough money\nleft for the carbine, just one.\nThe coat was too heavy kinda like a blanket, and the dye turned\nall purple when the sun it had baked it.\nThe saddle you know had a fiberglass tree, and the ""authentics""\nmade mirth and laughed at it with glee.\nHis heart it was golden and desired to do right, but the rush to\nbuy stuff created a blight.\nThe next time he shops will be with more wisdom, some friends\nwith the knowledge will help the decision.\nYes it\'s happened to more of us than we wish to admit.\nWe look on those early mistakes and wish we could Hang \'Em High.\nIt\'s better to go slow and get it right than waste time and a Fist Full of\nDollars rushing to get all that great stuff you thought you saw the other\nguys using, when For a Few Dollars More you can do much better in the\nauthenticity of your gear and clothing. It\'s all out there, The Good,\nthe Bad, and the Ugly, and we\'re here to help you sort it out. That\nway, you won\'t be branded a FARB, or Josey Wales type.\nNow let\'s not get too tied up in knots either. We are not\na hard-core authentic unit. We strive more for functional excellence\nthan material perfection. We typically get labeled by hobbyists as a ""Mainstream"" unit. While this label works for\nit leaves much unclear. We intend to work toward a good to better material\nimpression using well made clothing and equipment. We encourage all\nmembers to start at good mid-quality level and work up from there.\nDON\'T BUY ANYTHING WITHOUT\nTALKING TO US FIRST. Expect to spend from $3,000 to $4,000 to get started,\nassuming you already have horse and transport for it.\n- The 2nd United States Dragoon / Cavalry, Company\nH which, after initial assignment to the Defenses of Washington and\nProvost Martial (till September 1862), was part of the Army of the Potomac\'s Reserve Cavalry Brigade.\n- Pre-war and early war items are preferred to cover a greater time span.\n- Please review the following target impression standards for participants.\nWe expect it to take time.\n- Participation beyond the first two years will require meeting those guidelines listed below.\n- All participants are welcomed and encouraged to exceed basic guidelines at\nComplete Uniform and Equipment definitions and requirements\nPart I. Uniform and\n- 1858 Fatigue Cap (required item)\n- Also called the Forage or Bummer cap.\n- Finely woven dark or royal blue wool\n(not navy). A painted black leather brim and chinstrap are preferred.\n- US regulation, small eagle buttons.\nBlack / brown polished cotton liner.\n- The brim should stay flat, rather than\nthe sides curling as a modern cap\n- This is NOT the volunteer Kepi, which\nhas a shorter crown and closely resembles the 1872 Garrison Cap.\n- 1850’s Enlisted Uniform Hat (optional\nitem for later)\n- ¼ inch ribbon at base of crown.\n- 2 rows of stitching around the brim\n- Shellacked outside with label inside\n- Regulation brass insignia, plume and\ncord (required for this hat)\n- Optional leather chin strap\n- Period civilian pattern hats (optional,\nto be used very infrequently)\n- Fine (non-fuzzy) wool felt of medium to\ndark gray, brown, or black color.\n- Sewn-on silk ribbon brim edge binding\n- Leather or cotton duck sweat band\n- Chin strings not typical (get one that\n- Hats had round crown sides without\nfront “pinch”, with a round, flat, furrowed, bowl, or “beehive” shaped crown\ntops. No modern styles!\n- “Hat Brass” (optional items)\n- Designed originally for the Uniform\nhat, and not the Fatigue Cap until after General Order #53 of 1863. We see\nmany Fatigue Caps so adorned in the picture of Company I from spring 1863.\nYou may want a second, unadorned cap for living history purposes.\n- Crossed Sabers with the 1” Company\nLetter below, and ¾” Regimental number above, all generally centered on the\nfront of the Uniform Hat crown, or on the top of the Fatigue Cap crown.\n- US Eagle hatpin, to attach right side\nof the uniform hat brim up to the crown.\n- NONE of these to be worn on civilian\nstyle hats or caps.\n- Hat Cord, with tassels (optional item)\n- Color ORANGE through 1863, Yellow\n- Worn around the base of the hat crown,\ntassels aligned front and rear along the right side.\n- Worn only on the Uniform Hat, not\n- Plume (optional item)\n- Black ostrich feather, about 12” to 14”\n- Attached to the left side of the\nUniform Hat crown, aligned front to back\n- Only worn on the Uniform Hat, not\n- 1858 Fatigue blouse (required item) Also\ncalled the ""Saque Coat"" or Shell Jacket\n- Medium to dark blue lightweight wool\nflannel with a clearly visible diagonal weave. NOT blackish “navy” blue\nwhich often fades to purple.\n- Short fold over collar, faced lapels\n- Cuffs with a small scalloped vent in\n- Four large US eagle buttons.\n- Hand-sewn buttonholes. You can easily\nredo them by hand.\n- Unlined versions have flat-felled\n- Linings are of one-piece wool or\nwool/cotton weave in the body and muslin in the sleeve.\n- 1854 Mounted Service Jacket (optional for\n- Of dark blue or royal blue wool as with\nthe Fatigue blouse\ndyed worsted wool tape trim (we\'re Dragoons, remember?). This color lasted\ntill after Gettysburg. Beyond that and when not portraying the 2nd US, we\ncan just use the Fatigue Blouse noted above, unless you also want to buy a\nyellow trimmed jacket.\n- 12 Small eagle buttons down the front,\n3” tall collar with 2 ornamental buttons each side\n- Hand sewn buttonholes. You can easily\nredo them by hand.\n- Full body/sleeve linings as the Fatigue\nblouse, or polished cotton/cotton.\n- Attachments for shoulder scales\n- Enlisted brass shoulder scales\n- Mounted Pattern Trousers (required)\n- Made of sky-blue kersey wool with a\nclearly visible diagonal weave.\n- Top of the thin, tapering waistband\nshould reach the wearer\'s navel.\n- Reinforced seat and inseam.\n- Narrow, three to five button fly.\n- Raised back with yoke.\n- Side pockets and right-side watch\n- Facings on vented cuffs.\n- Detail work, like buttonholes, done by\nhand. You can redo them yourself.\n- Dark blue Mounted pattern trousers\n(optional for early-war)\n- As above but dyed dark or royal blue.\n- Foot Pattern Trousers (optional mid to\nlate war or if you’ve already got ‘em)\n- As above but without reinforced seat\nOvercoat (optional, cold weather use only)\n- Sky blue wool kersey like the trousers;\n- Mounted Pattern is double breasted with\nwrist length cape: strongly preferred.\n- Foot Pattern had shorter cape and is\nsingle breasted: acceptable if you already have one.\nShirts, under garments, and etc.:\n- Civilian Pattern Shirt (required)\n- 100 percent natural materials in period\ncolors and patterns.\nmetal, bone, wood, shell, or mother-of-pearl buttons.\n- Fall down collar or a banded collar,\nwith or without a detachable collar.\n- One, two or no pockets.\n- Domet or gray flannel US Issue Shirts\n(optional- really nice when it’s cold)\n- Domet flannel shirt has three tin\nbuttons: one at neck and one at each cuff.\n- Domet flannel is a cotton warp and wool\nweft, off-white in color.\n- Gray wool flannel shirt has 4 or 5\nbuttons, with two or three on a placket front and one on each cuff.\n- Suspenders & belts (optional - not issued\n- Civilian patterns of period materials\n- Drawers (optional)\n- Canton, cotton, or wool flannel or wool\nknit, all acceptable.\n- White, unbleached, and colors\n- Socks: Solid-color yarn: off-white, gray,\nbuff, blue, or bluish-gray. No color rings or bands of. Of wool, cotton or a\nwool/cotton blend. No modern pattern socks.\n- Shoes: Issue brogans with pegged or sewn\nsoles. (First choice and early war requirement)\n- Boots: Properly constructed, below the\nknee, military style boot. (Second choice)\n- Single-piece vamp (leg portion) if\n- Pegged or sewn soles.\n- Straight cut top 10"" tall maximum\n(taller or knee flaps discouraged).\n- Wear trousers (outside) in dress\nformations and inspections.\nPart II Accoutrements and Weapons\n- U.S. issue M1858 sword belt – (required)\nOf black buff or bridle leather.\n- 2 piece enlisted eagle buckle with\napplied silver wreath.\n- Shoulder strap is optional as pictures\nshow many did without.\n- Saber straps.\n- Cap pouch (required) Pre-war or early war\n- Pistol Cartridge Box - For pistol\ncartridges - the 2nd US carried these throughout. Cylinder pouches (a\nreenactorism) are discouraged except for some reenactments!\n- Holster - Black leather, butt forward,\nend plug, worn on right side.\n- Carbine Cartridge box (required)\n- ""Sharps box"" with tin insert preferred,\ncarried on saber belt.\n- M1860/Burnside accepted as second\n- Carbine Sling (required) Of black buff or\nbridle leather with iron roller snap hook.\n- Sabre Knot - Wrist strap secures the\nsaber to you when drawn, for living history purposes only.\n- Haversack (required) US issue tarred (not\ndyed black). Slung over shoulder normally, but over saddle will do.\n- Canteen (required)\n- Stopper attached with a loop of hemp,\nlinen twine, or leather shoelace strongly preferred over chain (you can\n- Snap hooks (a reenactorism)\n- 1858 Smooth-side type (preferred)\n- Cotton strap or un-dyed leather strap\nwith iron roller buckle and leather safe.\n- Should be worn on person.\n- Wool covered with brown/gray jean,\ncloth cover preferred over light or dark blue.\n- 1862 Bull’s-eye type (optional)\n- Cotton strap or herringbone webbing\n- Wool covering of brown/gray jeancloth\npreferred over light/dark blue.\n- Blanket: Gray/brown US Issue with black\nstripes properly woven.\n- Shelter half: (required) Light canvas\nwith hand-sewn grommets and tin or bone buttons.\n- Gum blanket and/or poncho (required) with\nsmall brass grommets\n- “Wedge” or “A-Frame” tent: Originally\nmeant for 4 or 5 men. This is your heavy camp shelter.\nOrdnance reports show the Second\nDragoons/Cavalry carried Colt’s .44 ""Army"" pistol, Sharps’ 1859/1863 Carbine,\nand 1840/1860 Cavalry Sabers throughout the war period.\n- Carbine (required - note that this should\nbe your first weapon purchased!!)\n- Model 1859 or 1863 Sharp\'s\n- Other US issue carbines only if you\nalready have it.\n- Saber (required) Made with wire\nwrapped, leather bound grip, and a peened tang (no nut on the end)\n- 1840 Heavy Cavalry (aka Wrist Breaker)\npreferred. We weld them for extra strength.\n- 1860 Light Cavalry also accepted.\nIt\'s easier to wield.\n- Pistol - (Only one allowed for living\nhistory and other sensitive events. Pistols can be loaded using cartridges in\n- Colt model 1860 .44 caliber ""Army""\nRevolver strongly preferred. (Required for serious living history)\n- Colt model 1851 .36 caliber ""Navy""\nRevolver, next choice for early-war.\n- Remington .44 or .36 caliber 1858\n""Army"" or “Navy” revolver last choice.\n- All others strongly discouraged or not\n- BUY YOUR CARBINE FIRST !!! BUY YOUR\nSABER SECOND, AND BUY THE PISTOL LAST!!!\nPart III Personal\n(underlined are of first priority)\n- Cup - issue tin cup - no stainless\n- Small sheet steel fry pan with riveted\nhandle, or canteen half (no speckle ware)\n- Fork and / or spoon\n- Folding pocket knife of a period look\n- Hygiene: tooth brush, small mirror, comb,\nshaving equipment, etc.\n- Cotton or Linen ration ""poke"" bags with\n- Pipe & tobacco pouch and matches\n(cigarettes not period correct)\n- Housewife (needles, thread, thimbles,\n- Writing utensils & paper\n- Gaming paraphernalia\n- Wallet & period repro money\n- Pocket watch with key wind and chain fob.\nPart IV. Horse Equipments\n- Saddle -\nEnlisted Model 1859 McClellan - All iron hardware, including ""jappaned"" or\nblued iron bar buckles. All items required unless noted otherwise.\n- Coat straps should be of proper weight\nwith correct buckles, leather stops recommended.\n- Proper wool web girth and surcingle\nwith iron roller buckles.\n- Crupper: (optional) an issued item, but\nyou may do without.\n- Breast straps: (optional) not an issued\nitem with 1859 equipment. If your horse requires one, we prefer a good\nfield pattern breast strap over the pre-war military pattern.\n- Hooded wooden stirrups, toe straps\ndiscouraged, but allowed.\n- Carbine Socket (optional)\n- Saddle Bags - Blackened leather, smooth\nor pebble grain.\n- Smaller bags with iron buckle\n- Must contain a body brush and hoof\npick. Should also contain a properly reproduced or original currycomb and\n- Halter –\n(required) U.S. issue of black bridle leather and iron hardware.\n- Bridle –\n(required) Blackened bridle leather\n- 3 or 6 buckle.\n- All buckles should be ""jappaned""\nblued iron bar buckles.\n- Rosettes on brow band allowed.\n- Bit - U.S. issued iron curb bit with\n- Link Strap - with iron wire snap hook.\n- Enlisted Reins - sewn in the center and\nto the bit.\n- White Canvas, with flat or rounded\nblack leather bottom\n- Black or un-dyed leather strap with\niron roller buckle.\nand Picket Pin (lariat required for campaign events)\n- 4-strand, left-laid 30\' hemp, whipped\nat one end.\n- Eye spliced to hand forged iron picket\n- Saddle Blanket\n– (required) U.S. issued dark blue wool with\nORANGE woven stripe\n- ""U.S."" hand stitched in center in\n- Saddle pads strongly discouraged. Use\nyour issue gray wool bed blanket or shelter half for extra padding. Pads,\nif used must be completely hidden at all times.\n- Grain Sack Roughly 6 inches by\n2 Feet (optional)\n- Watering bit and reins (optional)\n- Period watering bucket (wood, canvas,\netc.) Or use your Nosebag.\n- Spurs and Straps: (optional)\n- Plain brass spurs\nCorrect black leather straps with iron buckles']"	['<urn:uuid:c9ea2556-aa7a-4bf9-a7ef-9991c8d04fac>', '<urn:uuid:f8f2b6d7-4d8c-48bb-b046-64e19abfbb26>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T19:58:41.790432	8	43	3155
37	I work with spectroscopic techniques and am intrigued by the detection challenges in both molecular spectroscopy and wildlife monitoring. Can you explain how the signal detection problems in Raman spectroscopy relate to the difficulties in detecting bird collisions with wind turbines, and what solutions have been proposed for both?	In Raman spectroscopy, detecting signals from few molecules is extremely challenging because it requires detecting very weak signals - while you're looking for scattered photons against a dark background, it takes numerous photons to generate even a weak Raman spectrum. Similarly, detecting bird collisions with wind turbines faces signal detection challenges - collision rates are difficult to measure accurately without proper methods like scavenger activity controls. To address these detection challenges, different solutions have been proposed. For molecular detection, a hybrid approach combining atomic force microscopy with infrared spectroscopy has been developed, allowing for mechanical motion detection rather than direct light detection. For wind turbines, proposed solutions include careful site selection (avoiding wetlands and mountain ridges), improving turbine configuration (placing them parallel to main bird flight directions), and modifying construction (replacing lattice towers and wire-cables). Both fields continue to face ongoing challenges in accurate detection and measurement.	"[""With all the technology at our disposal, you would think that looking at chemistry at the nanoscale would be a piece of cake. After all, we can see the fluorescence from single atoms, right? We can see supernova in distant galaxies; how can it be so hard to figure out how molecules are sitting next to each other on a surface?\nVery hard, it turns out. This is because molecules are complicated beasts, and figuring who they are and what they are doing with their neighbors can be even more complicated. Normally, we might choose to do this with light. But when we get down to the level of a few molecules, the signals are so weak that they are very hard to detect. Now, a very clever combination of atomic force microscopy and infrared spectroscopy has shown great promise for looking at molecules at very small scales.\nWhen it comes to figuring out molecules through their interaction with light, we basically have two options: direct absorption spectroscopy and Raman spectroscopy. Atoms and molecules absorb certain colors of light, and this can tell us a lot about them. The high energy light (visible and ultraviolet) is absorbed by electrons moving to new energy levels, but this is not very useful for understanding molecules.\nInstead, we look in the infrared, for light frequencies that will set the atoms in molecules vibrating. These vibrational frequencies depend on the atoms that make up the molecules, how they are connected with their neighbors, and even the shape the molecule is bent into. Direct absorption spectroscopy involves shining the light on the sample and looking for missing photons. The colors that molecules absorb tell us the vibrational frequencies of the molecule.\nThis works really well with a population of identical molecules. But when you get down to just a few molecules, you are trying to detect a few missing photons in the background of a very bright light. This is simply not possible.\nRaman spectroscopy takes an alternative approach. We put in comparatively high energy photons, which scatter off the molecule. Every now and again, one is absorbed and excites vibrational motion. The leftover energy is re-emitted as a photon of light with a lower frequency. By looking at the new frequencies of light, we learn the vibrational frequencies of the molecules. The good thing is that you are looking for photons against a dark background. The bad thing is that it takes a lot of photons to generate a very weak Raman spectrum. So, again, it is very difficult to observe Raman scattering from just a few molecules.\nLu and Belkin from University of Austin in Texas have developed a technique that delivers the best of both worlds. It provides efficient excitation of vibrational frequencies while also retaining the best of Raman scattering: detecting the signal against a dark background. How do they do this? Simple, they convert the light absorption into bulk mechanical motion, and then sense that motion through atomic force microscopy.\nWhen a flash of light is absorbed by a molecule, it is absorbed by exciting a specific vibrational motion. As the atoms in the molecule move, they excite the motion of other atoms. In turn, they excite others, possibly including some in neighboring molecules. In the end, that one bit of energy ends up distributed over a whole bunch of atomic motions over many different molecules. In other words, we heat up the sample, and, as it is heated, it expands.\nThe expansion doesn't last for long, of course, because the energy continues to spread out, leaving the area cool again. The trick is to detect this expansion as it's happening. Lu and Belkin do this by placing the cantilever of an atomic force microscope (see side bar) over the sample. They then flash the light at the resonant frequency of the cantilever, so the rhythmic expansion and contraction of the sample excites the cantilever motion, which is then detected.\nIt turns out that this works pretty well, with the researchers demonstrating a spatial resolution of 50nm. That is pretty good, but isn't the few molecule detector that I was looking for. So, what limits this technique? The main limitation results from the diffusion of the heat. Even when you are not over the precise location of the absorption, you will still detect the acoustic waves.\nThe only way to get around this would be to look at the relative phase of the expansion: it takes time for the heat to travel outwards from the center of the absorption. But with a heat diffusion time of 100ns and a resonant frequency of 10kHz, we are talking about detecting phase differences of one part in 105.\nUltimately, this technique is probably not going to be iteratively improved a few nanometers at a time. Instead, it is likely to take a significant change in how the molecules are excited.\nOptics Express, 2011, DOI: 10.1364/OE.19.019942"", 'Impacts on biodiversity of exploitation of renewable energy sources: the example of birds and bats – facts, gaps in knowledge, demands for further research, and ornithological guidelines for the development of renewable energy exploitation.\nThe purpose of this report is to collect and to evaluate the available information on the impacts of exploitation of renewable energy sources on birds and bats. The focus is on wind energy as there is only little information on the impact on birds and bats of other sources of renewable energy. The report aims at better understanding the size of the impact, the potential effects of re-powering (exchanging small old wind turbines by new big turbines), and possible measures to reduce the negative impact on birds by wind turbines. In addition the need for further research is highlighted.\nThe evaluation is based on 127 separate studies (wind farms) in ten countries, most of them in Germany. Most studies were brief (not more than two years) and did not include the preconstruction period. Before-After Control Impact studies that combine data collection before and after, in this case construction of a wind farm, on both the proposed development site and at least one reference site were rare. In only a few cases, would the design of the study and the length of the study period theoretically allow statistically significant effects of wind farms on birds and bats to be found at all. Assessments of impacts, therefore, are usually based on few studies only. This report includes all studies readily available to the authors, irrespective of the length of the study period and the quality of the study design. In order to base the assessments on as many independent samples as possible even rather unsystematic observations were included. The information of the data was reduced to a level that justified the application of sign tests. The compilation of many different individual studies gave the following results:\nThe main potential hazards to birds and bats from wind farms are disturbance leading to displacement or exclusion and collision mortality. Although there is a high degree of agreement among experts that wind farms may have negative impacts on bird populations no statistically significant evidence of negative impacts on populations of breeding birds could be found. There was a tendency waders nesting on open grounds to be displaced by wind farms. Some passerines obviously profited from wind farms. They were probably affected by secondary impacts, e.g. changes in land management or abandonment from agricultural use next to the wind plants.\nThe impact of wind farms on non-breeding birds was stronger. Wind farms had significantly negative effects on local populations of geese, Wigeons, Golden Plovers and Lapwings.\nWith the exceptions of Lapwings, Black-tailed Godwits and Redshanks most bird species used the space close to wind turbines during the breeding season. The minimal distances observed between birds and pylons rarely exceeded 100 m during the breeding season. Some passerines showed a tendency to settle closer to bigger than to smaller wind turbines.\nDuring the non-breeding season many bird species of open landscapes avoided approaching wind parks closer than a few hundred metres. This particularly held true for geese and waders. In accordance with published information disturbance of geese may occur at least up to 500 m from wind turbines. For most species during the non-breeding season, the distances at which disturbance could be noted increased with the size of the wind turbines. For Lapwings this relationship was statistically significant. There was no evidence that birds generally became „habituated“ to wind farms in the years after their construction. The results of the few studies lasting longer than one season revealed about as many cases of birds occurring closer to wind farms (indications for the existence of habituation) over the years as those of birds occurring further away from wind farms (indications for the lack of habituation).\nThe question whether wind farms act as barriers to movement of birds has so far received relatively little systematic scientific attention. Wind farms are thought to be barriers if birds approaching them change their flight direction, both on migration or during other regular flights.\nThere is evidence for the occurrence of a barrier effect in 81 bird species. Geese, Common Cranes, waders and small passerines were affected in particular. However, the extent to which the disturbances due to wind farms of migrating or flying birds influences energy budgets or the timing of migration of birds remains unknown.\nCollision rates (annual number of killed individuals per turbine) have only rarely been studied with appropriate methods (e. g. with controls of scarvenger activities). In particular, such studies are missing for Germany. Collision rates varied between 0 and more than 50 collisions per turbine per year for both birds and bats. Obviously the habitat influenced the number of collisions. Birds were at high risks at wind farms close to wetlands where gulls were the most common victims and at wind farms on mountain ridges (USA, Spain), where many raptors were killed. Wind farms in or close to forests posed high collision risks for bats. For both birds and bats, the collision risk increased with increasing size of the wind turbine. The relationship, however, was not statistically significant.\nGulls and raptors accounted for most of the victims. In Germany the relatively high numbers of White-tailed Eagles (13) and Red Kites (41) killed give cause for concern. Germany hosts about half of the world population of breeding Red Kites and has a particular responsibility for this species. Bird species that were easily disturbed by wind farms (geese, waders) were only rarely found among the victims. Bats were struck by wind turbines mostly in late summer or autumn during the period of migration and dispersal.\nPopulation models created by the software VORTEX revealed that significant decreases in size of bird and bat populations may be caused by relatively small (0,1 %) additive increases in annual mortality rates, provided they are not counter acted by density dependent increases in reproduction rates. Short-lived species with high reproductive rates are more affected than long-lived species with low reproductive rates. The latter, however, are less able to substitute additional mortality by increasing reproductive rates.\nThe effects of „repowering“ (substitution of old, small turbines by new turbines with higher capacity) on birds and bats is assessed by the available data and by simple models. There is no information, however, on the effects of the newest generation of very large wind turbines. According to current knowledge, repowering will reduce negative impacts on birds and bats (disturbance and mortality) if the total capacity of a wind farm is not changed (many small turbines are replaced by few big turbines). In a scenario in which the capacity of a wind farm is increased 1.5 fold, negative impacts start to dominate. In case of a doubling of wind farm capacity, repowering increases the negative impacts of the wind farm. Repowering offers the chance to remove wind farms from sites that are associated with high impacts or risks for birds and bats. New turbines could be constructed on sites that are likely to be less problematic with respect to birds and bats.\nEffective methods of reducing the negative impacts of wind energy use on birds and bats include:\n- choice of the right site for wind farms (avoidance of wetands, woodlands, important sites for sensitive non-breeding birds and mountain ridges with high numbers of raptors and vultures),\n- measures to reduce the attractivness of wind farm sites for potential collision victims,\n- configuration of turbines within wind farms (placement of turbines parallel to and not accross the main migration or flight directions of birds),\n- construction of wind turbines: replacement of lattice towers, wire-cables and overhead power lines.\nMeasures to increase the visibility of wind turbines and to reduce the effects of illumination remain to be studied.\nIn spite of many publications on windfarms and birds there still is a great demand for further research. First of all there is an urgent need for reliable data on collision rates at wind turbines of birds and bats in Germany. This holds true particularly for the new and big turbines which will replace the present generation of wind turbines.\nIt is still unclear whether these big and necessarily illuminated turbines pose a high collision risk to nocturnal migrants which have not yet been greatly affected by smaller turbines. The high collision rates of Red Kites in Germany also merit urgently study. The aim of the research has to be a quick reduction of the collision rates. The sensitivity to wind farms of many other bird species that are of particular nature conservation interest (storks, raptors, Cranes) has not yet been sufficiently studied.\nThere is hardly any information on the impacts of arrays of solar panels on birds and bats. Studies should be initiated as soon as possible.']"	['<urn:uuid:a99f8b86-0379-49af-b544-ab76be02b3e0>', '<urn:uuid:94aae7dd-018c-48dc-b84c-619735491aee>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:58:41.790432	49	147	2292
38	contamination sources e coli salmonella	Both E. coli and Salmonella can contaminate food and water, but they have some distinct sources. E. coli contamination primarily comes from raw milk, improperly cooked meat (especially grain-fed beef), and fruits/vegetables cleaned with contaminated water. Salmonella can be contracted through contaminated food and water, as well as through direct contact with infected animals, their feces, or their environment.	"[""Beef Bacterial Contamination with E. Coli\nThere is no need to immediately panic when hearing the term E. coli. Escherichia coli, a rod-shaped bacteria, is commonly a part of the physiological gut flora of warm-blooded organisms, including humans. E. coli itself is relatively harmless if it stays where it belongs in the lower intestine of our digestive tract. The human body is colonized by E. coli right after birth and these colonies are then part of our digestive system, along with many other bacteria.\nThe trouble comes when this pathogenic bacteria migrates where it doesn't belong. Also, most strains of E. coli do not cause disease, but there are a few virulent strains that are dangerous to human health. These might cause gastroenteritis, urinary tract infection, neonatal meningitis, in some rare cases even bowel necrosis, mastitis, and gram-negative pneumonia. One specific strain of E. coli causes premature destruction of red blood cells which leads to hemolytic-uremic syndrome.\nIntestinal infections caused by E. coli are usually accompanied by diarrhea, in extreme cases resulting in dehydration or even death. But how does this bacteria get to our stomach from the outside? First, via water that has been contaminated with feces. Secondly you can become infected via food. The most likely contaminated food sources are raw milk and meat that has not been cooked properly, even just cross contamination during cooking. Fruit and vegetable that have been watered or cleaned with contaminated water could also make you sick. The dose required to contract the infection for this strain of E. coli is very low, only a miniscule amount of it is necessary to start an infection.\nJust as almost everything has two sides, we also know of E. coli strains that are somehow helpful in modern medicine. For example E. coli nissle, discovered in 1917, is proven to lower the risk of ulcerative colitis. E. coli K-12 was used during many important studies. One type of synthesized insulin was modified from E. coli bacteria and is helping type 1 diabetics who have a damaged pancreas and are not able to absorb insulin that is commonly used. Last but not least, there is a vaccine created with E.coli that is used against Lyme disease in USA.\nThe most important (and dangerous) strain of E.coli is O157:H7. It is present in animals digestive tracts and later found in feces, particularly of cows. Fecal matter can easily get into slaughtered meat in a factory because of careless manipulation. Then, all it needs is not enough cooking time and infection can start in the body of a consumer. Thorough cooking is very important with grain fed beef, as E.coli O157:H7 is mostly found in cows that were fed or finished with grains.\nLearn more about E. coli, the dangers and health consequences, but also find out if its necessary or better for your health to drop beef from your menu for good. Learn what you deserve to know to be able to make healthy and safe food choices for yourself and your family."", 'Questions and Answers\nWhat are Salmonella?\nSalmonella are bacteria that make people sick. They were first discovered by an American scientist named Dr. Daniel E. Salmon in 1885.\nWhat illness do people get from Salmonella infection?\nMost types of Salmonella cause an illness called salmonellosis, which is the focus of this website. Some other types of Salmonella cause typhoid fever or paratyphoid fever.\nWhat are the symptoms of infection?\nMost people with Salmonella infection have diarrhea, fever, and stomach cramps.\nSymptoms usually begin six hours to six days after infection and last four to seven days. However, some people do not develop symptoms for several weeks after infection and others experience symptoms for several weeks.\nSalmonella strains sometimes cause infection in urine, blood, bones, joints, or the nervous system (spinal fluid and brain), and can cause severe disease.\nScientists classify Salmonella into serotypes (types) by identifying structures on the bacteria’s surfaces. Although more than 2,500 serotypes have been described, fewer than 100 are known to cause human infections.\nHow is Salmonella infection diagnosed?\nSalmonella infection is diagnosed when a laboratory test detects Salmonella bacteria in a person’s stool (poop), body tissue, or fluids.\nHow is infection treated?\nMost people recover from Salmonella infection within four to seven days without antibiotics. People who are sick with a Salmonella infection should drink extra fluids as long as diarrhea lasts.\nAntibiotic treatment is recommended for:\n- People with severe illness\n- People with a weakened immune system, such as from HIV infection or chemotherapy treatment\n- Adults older than 50 who have medical problems, such as heart disease\n- Infants (children younger than 12 months).\n- Adults age 65 or older\nCan infection cause long-term health problems?\nMost people with diarrhea caused by Salmonella recover completely, although some people’s bowel habits (frequency and consistency of poop) may not return to normal for a few months.\nSome people with Salmonella infection develop pain in their joints, called reactive arthritis, after the infection has ended. Reactive arthritis can last for months or years and can be difficult to treat. Some people with reactive arthritis develop irritation of the eyes and pain when urinating.\nHow do people get infected?\nSalmonella live in the intestines of people and animals. People can get Salmonella infection from a variety of sources, including\n- Eating contaminated food or drinking contaminated water\n- Touching infected animals, their feces, or their environment\nWho is more likely to get an infection and severe illness?\n- Children under 5 years old are the most likely to get a Salmonella infection.\n- Infants (children younger than 12 months) who are not breast fed are more likely to get a Salmonella infection.\n- Infants, adults aged 65 and older, and people with a weakened immune system are the most likely to have severe infections.\n- People taking certain medicines (for example, stomach acid reducers) are at increased risk of infection.\nWhat should I know about antibiotic resistance and Salmonella?\nResistance to essential antibiotics is increasing in Salmonella, which can limit treatment options for people with severe infections. One way to slow down the development of antibiotic resistance is by appropriate use of antibiotics.\nWhat can be done to prevent antibiotic resistance and resistant bacteria?\nAppropriate use of antibiotics in people and animals (use only when needed and exactly as prescribed) can help prevent antibiotic resistance and the spread of resistant bacteria.\nHow common is Salmonella infection?\nCDC estimates Salmonella cause about 1.35 million illnesses, 26,500 hospitalizations, and 420 deaths in the United States every year.\n- CDC. Antibiotic Resistance Threats in the United States, 2019. Atlanta, GA: U.S. Department of Health and Human Services, CDC; 2019.\n- Scallan E, Hoekstra RM, Angulo FJ, Tauxe RV, Widdowson MA, Roy SL, Jones JL, Griffin PM. Foodborne illness acquired in the United States–major pathogens pdf icon[PDF – 9 pages]. Emerging Infectious Diseases. 2011;17(1):7-15.\n- CDC. Foodborne Diseases Active Surveillance Network (FoodNet): FoodNet Surveillance Report for 2012 (Final Report) pdf icon[PDF 9 – pages]. Atlanta, Georgia: U.S. Department of Health and Human Services, CDC. 2014.\n- CDC. Suspecting Foodborne Illnesses in Special Populations: Quick Facts for Providersexternal icon. Atlanta, Georgia: U.S. Department of Health and Human Services, CDC. 2012.\n- Carter JD, Hudson AP. Reactive arthritis: clinical aspects and medical managementexternal icon. Rheum Dis Clin North Am. 2009 Feb; 35(1): 21-44.']"	['<urn:uuid:2c739922-db45-47c5-ba43-31681a025afd>', '<urn:uuid:228cba84-af4e-4635-bc49-f860292b6969>']	factoid	direct	short-search-query	similar-to-document	comparison	expert	2025-05-12T19:58:41.790432	5	59	1227
39	how calculate net return investment	Net Return can be calculated using the formula: Net Return = Return – (Total Fees + Costs). This is not shown on investment statements but needs to be calculated manually to understand how much money you are really making.	"[""Cape Town - Regardless of who you invest with or what policy you have, chances are you’re unable to make sense of each and every line item on your investment statement.\nThis is not necessarily a failing on your behalf, as investment statements are notoriously tricky to understand - there’s no standard format, and what you can expect to see can differ across providers and policies.\nAccording to Emma Heap, managing director of Retail at 10X Investments, it is essential that consumers learn the ins and outs of their financial statements to avoid having to rely on someone else to monitor their money.\n“Consumers should be able to understand what is happening with their finances from month to month so that they don’t have to rely too heavily on their providers when it comes to making important financial decisions.”\nThe categories you can expect to find in your financial statement:\nGenerally, an investment statement will contain basics like your name, policy numbers, a summary of investments and policies, and the details of your financial advisor.\nAll transactions that have taken place since your last statement will usually be listed. Your statement will reflect the gains and losses you have made, along with any contributions and withdrawals, and this amount will be shown as a percentage or in a rand amount.\nThe retirement income you draw from your living annuity or life annuity is reflected on your statement as monthly income or draw-down.\nSome providers will give you a breakdown of the portfolios you are invested in, and their allocations.\nExamples of some terminology you can expect to see in this category include the following:\n- Asset class: The types of assets you’re invested in, such as equities (or stocks); bonds; property; and cash;\n- Asset mix/allocation: This will show you what percentage of your money is invested in which asset classes.\nThis section details how your money has grown over the duration of the time stated on the statement. Depending on your recent transactions and your return, the beginning and ending balances are usually different.\nFor many investors, this is seen as the most important statement category as it shows the amount by which investments have increased or decreased in value. However, as we explain in the Fees section below, your returns are actually only half the picture.\nYour returns are what providers, brokers, and advisors usually highlight as the most important part of the statement. While returns do matter, they alone do not determine how much money ends up in your pocket. If you are interested in how much money you are really making, you need to look at your net return.\nThe question is, what is a net return and how do you find it? The bad news is that you can't find it on your investment statement.\nThe good news is that you can calculate it yourself, with the following simple formula:\nNet Return = Return – (Total Fees + Costs).\nMost statements unfortunately only display one or two fees, while many omit fees entirely, saying that the information is available on request. And those that do show fees often obscure their impact by showing them as a rand amount, as opposed to a percentage, which can disguise the fees’ impact as they compound over time.\nInvestors can learn what the costs and fees are by knowing the types of fees they can expect to see, including administration fees, performance fees, platform fees, fund manager fees, financial advisor fees, penalty fees, marketing fees, and transaction fees.\nHeap concludes that while a service provider is responsible for providing statements, it is up to the consumer to educate themselves on what these statements are showing.\n“This will make it easier to know what is happening and allow you to ask the right questions, should you see any concern areas on a statement. While you may pay for guidance from a financial advisor, in order to make the most of this guidance, it is always in your best interest to educate yourself on what is being said.”\nSUBSCRIBE FOR FREE UPDATE: Get Fin24's top morning business news and opinions in your inbox.""]"	['<urn:uuid:9154215d-a7ff-4338-b029-e4909c96b938>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T19:58:41.790432	5	39	692
40	I'm curious about what exactly makes expensive vodka taste better than cheap vodka, and why doctors say drinking it with medications is risky?	Expensive vodka tastes better due to several factors in its production. Though vodka is primarily ethanol and water, higher quality vodkas have fewer impurities due to more careful distillation and filtration processes. Cheaper vodkas contain higher levels of impurities like methanol, propanol, and acetaldehyde, which negatively affect flavor and smoothness. Some manufacturers add compounds like citric acid, glycerol, and sugar to improve smoothness, particularly in cheaper vodkas. As for drinking vodka with medications, this is risky because alcohol is a depressant that can cause significant central nervous system dysregulation when combined with medications. It can lead to excessive sedation, dizziness, blood pressure drops, and impaired cognitive functioning. Additionally, alcohol can interfere with medication efficacy and magnify side effects - for instance, with some antidepressants, one alcoholic drink can feel like two.	"['You could be forgiven for thinking there’s not a great deal that’s interesting about the chemistry of vodka. After all, isn’t it essentially just a mix of two compounds, ethanol and water? Though this is pretty much the case, there’s more to vodka than you might expect. Here we take a look at some of its chemical secrets.\nFirst, let’s briefly summarise how vodka is made. The method is similar to that for most fermented spirits. Though the stereotypical image most people have of vodka is that it’s made from potatoes, in fact it’s much more common for it to be made from cereal grains, including corn, rye, and wheat. Fermentation of these grains using yeast produces alcohol (ethanol), but only up to around 16% – too low for vodka.\nFurther steps are necessary to arrive at the finished product, the key step being distillation. Distillation involves the boiling of the mixture; because ethanol boils at 78˚C, it boils off before the water does and it can therefore be concentrated. Unfortunately, a lot of the other compounds produced during fermentation boil off at lower temperatures than water too, so precise control of the distillation process is necessary to ensure that these aren’t present in the final product.\nOften, it’s distilled more than once to ensure a minimal amount of impurities remain. To be even more certain of this, many manufacturers filter the vodka through activated charcoal, which helps pull out more of any impurities still present. More traditional manufacturers rely on precise control of the distillation process, however. After all of this the vodka’s alcohol percentage is around 96%. The final step in the process is diluting it with water to bring the percentage down to around 40%.\nThe final product has little other than ethanol and water present, so in theory all vodkas should be essentially identical in perception and flavour. However, if you’ve ever compared a high quality vodka with the cheapest stuff you can buy in your local supermarket, you’ll know that there’s often a slight but discernible difference. This can be due to a handful of reasons.\nOne suggestion is that differences in perception could be due to differences in they way ethanol and water interact in different vodkas. As well as existing as individual molecules, the water and ethanol molecules can form structures called hydrates. These are cage-like structures, with a number of the water molecules surrounding an ethanol molecule.\nThe most common of these hydrate structures in vodka has around 5 water molecules to each ethanol molecule. Researchers discovered that its concentration varied in different brands of vodka. Though their hypothesis has yet to be conclusively confirmed, they speculate that these structural differences in different vodkas could account for slight differences in taster perceptions.\nAnother factor is impurities. Though most of these are removed during distillation and filtration, small, milligram amounts will still remain. These impurities can include other alcohols, such as methanol and propanol, as well as compounds such as acetaldehyde. Cheaper vodkas contain higher levels of these impurities, which can negatively affect flavour perception, and lead to a vodka that’s less smooth.\nThe final factor is additives. Though we think of vodka as just ethanol and water, it’s actually permitted in a number countries to add small amounts of other additives. Mostly, these are to improve the smoothness of the vodka, so they’re likely to be found in higher amounts in cheaper vodkas containing more impurities. Compounds used for this purpose include citric acid, glycerol, and sugar.\nVodka isn’t always unadulterated of course, and flavoured vodkas are also possible by adding various compounds or extracts after the manufacturing process. One of the most well-known flavoured vodkas is Żubrówka, which is flavoured using bison grass. Interestingly, this was (and still is) banned by the FDA in the USA, as flavouring with bison grass also leads to the presence of the compound coumarin, which in much larger amounts has been shown to exhibit liver toxicity in rats. A form of Żubrówka is now sold in the US, but it’s one in which the coumarin content has been removed.\nSo, there you have it – there’s more to vodka than just ethanol and water after all!\nEnjoyed this post & graphic? Consider supporting Compound Interest on Patreon, and get previews of upcoming posts & more!\nReferences & Further Reading\n- Vodka: analysed, filtered and poured – L Jarvis, C&EN\n- Vodka’s molecular cocktail – M Lalloo, Chemistry World\n- What’s inside? Vodka – P Di Justo, Wired\n- A collective measure of structural differences in vodkas (£) – N Hu and others\n- Ion chromatography to detect adulteration of vodka and rum (£) – D Lachenmeier and others\n- Identification of vodkas from ion and gas chromatography (£) – V Arbuzov & S Savchuk', 'Doctor insights on:\nCytomel And Alcohol\nWhat are the physical and mental side effects of alcohol and prozac (fluoxetine). Beer mixed with tequila and prozac (fluoxetine)?\nAlcohol & Depression: Alcohol in any form -- including beer and tequila -- is a depressant. Combined with prozac, (fluoxetine) you could have much more sedation or intoxication than you expect. Especially when used in large quantities, alcohol disrupts your sleep cycle and neuronal tone. People usually take Prozac (fluoxetine) for depression or anxiety. Disrupting your sleep & neurons can worsen these conditions. ...Read more\nNot recommended: Psychoactive medications, especially ones that tend to be sedating, such as Seroquel, (quetiapine) can cause significant central nervous system dysregulation when alcohol is consumed. Not only would you experience potential side effects such as dizziness, excess sedation and drops in blood pressure, but also impaired cognitive functioning. This includes poor judgement and disinhibition. Hope this is helpful. ...Read more\nRecently taking eliquis, metatoprolol and Tylenol. Alcohol intake minimal. Enzymes raised significantly! Suggestions?\nPlease clarify: Let us know which lab tests were elevated. Can resubmit with clarification. ...Read more\nNot recommended: Psychoactive medications can cause significant central nervous system dysregulation when alcohol is consumed. Not only would you experience potential side effects such as dizziness, excess sedation and drops in blood pressure, but also impaired cognitive functioning. This includes poor judgement and disinhibition. Alcohol can also interfere with full efficacy of these medications. Hope this helps. ...Read more\nNot a good idea: Serotonin reuptake inhibitors like sertraline magnify the effects of alcohol. One drink is like taking two drinks. It is best to not use alcohol while on this medication. If you do drink it is best to limit it to one drink only. One drink is considered one 12 oz beer, one 4 oz glass of wine or one 1.5 oz of hard liquor. ...Read moreSee 2 more doctor answers\nIm taking, levothyroxine, tramadol, labetalol, seroquel, (quetiapine) and sertraline can any of these meds cause high ammonia levels?\nNo and I suspect: that the fact you are asking this of a stranger online, rather than the doctor(s) who are prescribing these medications for you means that you KNOW that the answer is no. Alcohol can increase depression and anxiety in many people, as can marijuana. I will wager that your doctors don\'t know about your drinking or weed use. If you truly want help to get better, tell them and stop drinking/weed. ...Read more\nBest To Avoid: Celexa (citalopram) is an antidepressant and alcohol is a depressant. Common sense tells us to avoid alcohol while treating depression, otherwise one agent will cancel the other. I have always considered alcohol the ""original medication"". It calms you, gives you confidence, stimulates you and puts you to sleep and can even be unused as an antiseptic. If you are unable to stop drinking talk to your doctor. ...Read moreSee 1 more doctor answer\nNot reported: Metformin has not been reported to interfere with the absorption of thyroid hormones. One report suggested it could lower TSH levels in some patients but with no effect on thyroid hormone levels (t4). I recommend you have your levels checked if symptomatic or as recommended by your provider. ...Read more\nBodybuilding?: Dont do either of those. Cytomel (liothyronine) will throw your thyroid panel off and you maybe on it for the rest of your life. Dont mess with your thyroid. Clenbuterol is a beta agonist only made in certain countries like mexico. Way too many side effects including palpitations, hypertension, and sudden death. You can get shredded without that crap! strict diet and cardio. ...Read more\nWhat\'re the effects drinking alcohol while taking these medications: paroxetine, zolpidem, and levetiracetam?\nAdditive: Acohol has an additive sedating effect with all those meds. That you\'re taking that particular pharmacological cocktail implies you have a significant neuropsychiatric disorder. Alcoholmusevin these conditions can be detrimental. Please share your drinking history with prescribing doctor. ...Read moreSee 1 more doctor answer\nCan i still drink alcohol if i\'m taking Depakote and lamictal for epilepsy and celexa (citalopram) for depression?\nAlcohol: Definitely not, especially with your history of seizures. ...Read more']"	['<urn:uuid:5e477a27-5ae6-4da1-a831-dff10a4584a1>', '<urn:uuid:57383d70-3f89-4787-9a30-37ece13fd2f1>']	open-ended	direct	verbose-and-natural	distant-from-document	three-doc	novice	2025-05-12T19:58:41.790432	23	131	1487
41	building wireless antenna what is widest working frequency range possible	The microstrip patch antenna array design achieves an impedance bandwidth of 27.5% for |S11| < -10 dB in simulation, and 22.6% in measured performance, which represents the widest bandwidth among the discussed designs.	['Design of millimeter-wave planar antenna arrays with substrate integrated waveguide feeds\nStudent thesis: Doctoral Thesis\nRelated Research Unit(s)\nThe thesis presents a series of planar antenna arrays for emerging millimeter-wave wireless applications. All proposed antennas, which are excited by coupling apertures etched on the copper-clad surface of the substrate integrated waveguide (SIW), exhibit wide bandwidths and good radiation characteristics. Due to the use of the SIW to compose feed networks, low feed network losses and high-gain performance are obtained forthe designed antenna arrays. Moreover, the feeding scheme of the aperture coupling with a two dimensional geometry is easy to realize at millimeter-wave frequencies, which enables the proposed antenna array to be fabricated conveniently by different manufacturing technologies. First, cavity-backed microstrippatch antenna arrays with full corporate SIW feed networks are demonstrated. A prototype of 16 × 16 radiating elements with a SIW to waveguide transition is fabricated by applying standard printed circuit board (PCB) facilities. A gain up to 30.1 dBi with a 3-dB gain bandwidth of 16.1%, an impedance bandwidth of 15.3% for SWR < 2, and symmetrically broadside radiation patterns with -40 dB cross-polarizations are achieved. The performance of the proposed antenna array is also systematically evaluated. The result serves as a reference for designing large antenna arrays operating at millimeter-wave frequencies. Second, a microstrip patch antenna array with an improved wide bandwidth is studied. A novel wideband SIW-fed dual-aperture-coupled microstrip patch antenna is proposed. A single layered SIW feeding network with wideband T-junctions and the wideband radiating element are employed to achieve high-gain and wideband performance simultaneously. The simulated and measured impedance bandwidths of a 4×4 antenna array are 27.5% and 22.6% for |S11| < -10 dB. A gain up to 19.6 dBi and symmetrical unidirectional radiation patterns with low cross polarization are also achieved. Third, anovel perforated dense dielectric (DD) patch antenna array fed by a single-layered SIW feed network is investigated. The proposed antenna array offers advantages of wide bandwidth and good radiation performance and can be fabricated conveniently by standard PCB technology. A 4 × 4 prototype operated at the 60-GHz band is designed, fabricated and measured to demonstrate the design. An impedance bandwidth of 23% for SWR < 2, a gain up to 17.5 dBi with a 3-dB gain bandwidth of 24%, and symmetrical unidirectional radiation patterns with cross polarizations of less than -20 dB are obtained. Fourth, a series of novel SIW-fed aperture-coupled ME-dipole antennas with linearly, circularly and dual-polarized radiations are developed. Compared with reported designs, the proposed designsmaintain the advantages of the ME-dipole antennas, including wide bandwidth, symmetrical E- and H- plane radiation patterns, low backward radiation and high gain, but are easier to realize at millimeter-wave frequencies. By employing the dual-polarized aperture-coupled ME-dipole antenna, a 2 × 2 wideband antenna array working at the 60-GHz band that can generate two-dimensional multi-beams with dual-polarizations is designed, fabricated and tested. A measured impedance bandwidth wider than 21.9% for |S11| < -10 dB and a gain up to 12.5 dBi are obtained. Owing to the superiority of the ME dipole, the radiation pattern of the array is also stable and nearly identical in two orthogonal planes for both polarizations.\n- Microwave antennas, Antenna feeds, Design and construction, Antenna arrays']	['<urn:uuid:2163be5d-8465-4781-8e70-fd7b108d6fa4>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	10	33	536
42	immune suppression mechanism hiv survival host explain	HIV is able to avoid destruction by the immune system by tricking the body into overproducing regulatory T cells (Tregs). These Tregs suppress the activity of virus-killer cells (CD8+ T cells), preventing them from effectively eliminating the virus. With too many Tregs present, the CD8+ T cells become 'exhausted' and can hardly perform any killing, allowing HIV to establish a permanent residence in the host.	['Human bodies are naturally equipped with elaborate defense mechanisms to squelch intruding microorganisms. But some viruses, like HIV, are able to slip under the immune system radar and set up permanent residence in a human host. Exactly how this feat is accomplished is not known for certain, but a better understanding of virus war tactics is key to developing effective medical treatments.\nSome evidence suggests that viruses like HIV are able to avoid destruction by tricking the body into overproducing a type of cell that suppresses the immune system. A new study in PNASProceedings of the National Academy of Sciences substantiates this theory and offers insight into how medicine can counter this.\nThe immune system is complicated, but you can sort of think of it with a Pac-Man analogy (Disclaimers: I don’t wish to make light of this subject, however I am having trouble thinking of another analogy and the immune system is confusing. Also, though I minored in biology in college, I am an organic chemist, not an immunologist. I also haven’t played much Pac-Man). Here are the participants:\nPac-Man gets the honor of being a virus-killer cell – called a CD8+ T cell. This is one of the cells produced by the body’s immune system. Here, the white circles are virus particles, and the CD8+ T cells are responsible for killing them. Now unfortunately, these virus-killers are also capable of killing normal human cells if allowed to run amuck. To prevent this, there has to be some kind of checks-and-balances system. So, there’s a second type of immune system cell, called a Treg (regulatory T cell). The Tregs are able to suppress the activity of the CD8+ T cells, to make sure they don’t just kill everything in sight. This is what a normal virus infection might look like:\nHowever, it seems that some viruses can trick the body into producing way too many Tregs. With so many Tregs around, the CD8+ T cells can hardly do any killing at all, not even of viruses. Thus, an infection with a virus like HIV might look something like this:\nDuring a chronic virus infection, the CD8+ T cells become “exhausted” (technical term!), presumably from the extra Tregs around. Researchers have been wondering – if you can get rid of some of those Tregs, could the exhausted CD8+ T cells recover and start killing viruses again?\nThe answer is yes, according to the authors of this paper. They infected mice with a type of mouse virus (FV)friend virus that, like HIV, is able to establish a chronic infection.\nThey didn’t use run-of-the mill mice, though. These mice had been genetically modified so that their Tregs had an Achilles heel. The Tregs could be selectively eliminated at will by the researchers via injection with a particular toxin, a toxin that would not normally affect Tregs. This genetic modification allowed the researchers to control Treg levels – something that wouldn’t be easily achievable with regular mice.\nAfter the FV infection had set in, and Treg levels were presumably high due to the virus’s influence, the researchers injected the mice with the Treg-suppressing toxin. In theory, the resulting drop in Treg activity would allow the CD8+ T cells to recover and start killing viruses again.\nAnd this is exactly what seemed to happen. As the mouse’s Treg activity started to decrease, the concentration of virus particles in the mice started to get lower. The researchers found evidence that the CD8+ T cells were doing the virus-killing when they noticed increased levels of certain signalling molecules produced by CD8+ T cells.\nThe results of this study suggest that chronic infections in humans, such as HIV, could potentially be treated by supressing human Treg cells. However, the use of such a therapy would first require a lot more research. Although HIV, like FV, is believed to suppress the immune system by increasing Treg activity, it paradoxically stimulates the immune system in other ways. It is possible that medically depleting Tregs could actually exacerbate disease progression in an HIV-infected individual. Further research is certainly in order.\nDietze, K., Zelinskyy, G., Gibbert, K., Schimmer, S., Francois, S., Myers, L., Sparwasser, T., Hasenkrug, K., & Dittmer, U. (2011). Transient depletion of regulatory T cells in transgenic mice reactivates virus-specific CD8+ T cells and reduces chronic retroviral set points Proceedings of the National Academy of Sciences DOI: 10.1073/pnas.1015148108']	['<urn:uuid:46c2e102-9e79-4368-a545-93c0899b7a7d>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	7	65	729
43	I noticed my new digital devices cost more than the old analog ones. Why are they more expensive if they're supposed to be simpler?	While digital communication systems are generally simpler and cheaper in terms of the overall system, digital instruments themselves have a higher cost and are not as portable as analog instruments. This is despite the fact that digital instruments draw only negligible power compared to analog instruments which draw large power.	"[""This book provides a distinctive, radical way beyond the quarrels between evolutionary science and Christian belief. The gap esferico is a tachyon electron at any of their mutual arrangement. The primary benefit of digital signals is that they can be handled by simple, standardized receivers and transmitters, and the signal can be then dealt with in software which is comparatively cheap to change. The digital signal is formed by the sampling of the analog signal. If their attendance in a week is plotted, it would look like the following figure. In this case, power can be saved but the receiver circuit will become complex because the carrier and two sidebands must be regenerated in the receiver.\nThis signal keeps on varying with respect to time, according to the instantaneous values of the quantity, which represents it. Modulation is very important block in communication system. Modern digital and analog communication systems by B. The crucial difference between Analog and Digital Communication is that Analog communication uses analog signals for transmission and reception of data while digital communication uses digital signals for transmitting and receiving data. These are the error correcting bits. Advantage of Digital Communication: 1 Digital Communication systems are simpler and cheaper. The portion of filling the tank is varied by the varying time.\nA digital signal is generally represented by a binary sequence. Communication systems analysis and design a systems approach, Richard A. In analog communication, the signal can take up voltage level corresponding to any real number, In digital communication signal can take up only one among two voltage levels corresponding to 1 and 0. At the receiver, this modulated si … gnal is received and processed to recover the original message signal. Because of coding techniques it suitable for police military etc. Output Transducer This is the last block which converts the signal into the original physical form, which was at the input of the transmitter. Digital and Analog Communication Systems Leon W.\nBest suited for audio and video transmission. This revision of Couch's authoritative text provides the latest treatment of digital communication systems. In practice, only three of these are widely used. Noise figure is defined as the ratio between Signal to Noise ratio at the output to the Signal to Noise ratio at the input 2. Communication is basically interaction among people or sharing information. In Analog communication, the analog message signal modulates some high carrier frequency inside the transmitter to produce modulated signal. Source Decoder The resultant signal is once again digitized by sampling and quantizing so that the pure digital output is obtained without the loss of information.\nEmphasis is a system process designed to decrease, within a band of frequencies , the magnitude of some usually higher frequencies with respect to the magnitude of other usually lower frequencies in order t … o improve the overall signal-to-noise ratio by minimizing the adverse effects of such phenomena as attenuation differences or saturation of recording media in subsequent parts of the system. Bakshi, Jan 1, 2009, Digital communications, 608 pages. In comparison, digital signals have the capability of carrying limitless amount of data, while the amount of information analog signals can carry is limited. Digital instruments are free from observational errors like parallax and approximation errors. The distortions which might occur during the transmission, are corrected by adding some redundant bits. The digitized signals allow the communication to be more clear and accurate without losses.\nGoetz, 1988, Education, 353 pages. Our job is to design a system to measure the level of water in the tank and send this information to a distant location so that other people may monitor it. Since the bandwidth requirement in digital systems is more thus, they consume less power. Hi there user: Analogue is a comparable mechanism that varies infinitely by the range. In high level modulation system, modulation is done at the higher power level of the carrier signal while in the low level modulation system, it is done at low power level. Imagine if the army transmitted a position coordinate to a missile digitally, and a single bit was received in error. What are the Pros and Cons? Although digital signals suffer the same degradation, as long as the receiver can sort out the high and low levels that make up a digital signal, the picture quality will be identical to the transmission quality.\nComet's Nine Lives , Jan Brett, 2001, Juvenile Fiction, 32 pages. It could be a transmitting station from where the signal is transmitted. System architecture, Radio subsystem, Base station controller, Mobile station. Memory Stored in the form of wave signal Stored in the form of binary bit Power Analog instrument draws large power Digital instrument drawS only negligible power Cost Low cost and portable Cost is high and not easily portable Impedance Low High order of 100 megaohm Errors Analog instruments usually have a scale which is cramped at lower end and give considerable observational errors. Imagine if the army transmitted a position coordinate to a missile digitally, and a single bit was received in error? A priori, the universe is in control of this conflict, however, Sigwart considered a criterion of the truth of the necessity and obstacole for which there is no support in the objective world. Whereabouts Lost , Alan Dunbarker, Jan 1, 2009, Fiction, 284 pages.\nFollowing are the difference between analog communication and digital communication: communication communication is converted original information into equivalent analog signal. Moreover, noise affects the Analog signal more than digital signal because analog signal is a continuous time-varying signal. Analog and Digital communication systems Analog Communication System Analog communication is that types of communication in which the message or information signal i. Samples analog waveforms into a limited set of numbers and records them. The demodulated wave will be original signal. Lyric Stokes lives a charmed life.\nHi, Difference between analog and digital communication? With digital technology this human speech can be saved and stored in a computer. The Act is in 18 parts with 18 schedules and contains provisions for comprehensive reform of Britains gambling laws, with a new regulatory system to govern the provision of all. Digital communication provides various advantages such as it is immune to noise and distortion as it possesses greater signal to noise ratio. Digital and Analog Communication Systems Ray, at first glance, optical stable. This signal helps to establish a communication between the sender and the receiver.\nError Probability is low Hardware Hardware is complicated and less flexible than digital system. A digital communication system offers many advantages to the user, that cannot be achieved with an analog system. Source Encoder The source encoder compresses the data into minimum number of bits. E 1year student,actually i have a habit i. This signal has its pattern continued but the pattern is not repeated.""]"	['<urn:uuid:1f028d3e-1608-4cad-be83-f76b2a73bf25>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	24	50	1136
44	safety features conventional nuclear vs molten salt reactors	Conventional nuclear and molten salt reactors have distinct safety features. While conventional plants require multiple redundant high-pressure systems and strict construction standards for concrete and steel, MSRs have inherently safer design features. MSRs operate without high pressure, generate no explosive materials, and include a simple freeze plug that melts in emergencies to drain fuel to passive cooling tanks. Additionally, MSR fuel temperature stays below material limits, unlike conventional reactors where fuel pellets can become hotter than reactor materials can handle. Advanced nuclear technology, including MSRs, provides greater safety at lower cost through simpler, more reliable safety systems and their neutronics and thermal-fluidic properties.	['Is the atomic age over? For decades, nuclear power seemed like the perfect energy solution, both efficient and carbon neutral. If the problem of properly disposing of nuclear waste could only be managed, the world may have found the solution to its energy needs. As the beehive hairdo fell out of fashion, so did nuclear power, with citizens increasingly worried about the potential for environmental contamination and harms to public health. New advanced nuclear technology may give nuclear power a second chance, however, offering clean nuclear power at a price 40 percent lower than current technology.\nA study by the Energy Innovation Reform project found that advanced nuclear technology could produce electricity at a cost 40 percent lower than the cost of conventional nuclear power. Not only are advanced nuclear plants cheaper, the research suggests that they may be safer than conventional plants as well.\n“Advanced nuclear technologies represent a dramatic evolution from conventional reactors in terms of safety and nonproliferation, and the cost estimates from some advanced reactor companies—if accurate—suggest that these technologies could revolutionize the way we think about the cost, availability, and environmental consequences of energy generation,” the researchers found.\nAdvanced nuclear power plants take advantage of many different technological innovations. One key example is construction, which accounts for about a third of the cost of opening a new nuclear plant. Of these expenses, the lion’s share of the costs go to “indirect services” which include engineering during construction and project management costs, as well as the price of temporary buildings and equipment rental. New technology allows builders of nuclear plants to potentially lower their construction costs without sacrificing safety. The concrete and steel used in construction must meet strict requirements and plant designs are held to strict safety standards.\nWhen companies build advanced nuclear power plants, including molten salt reactors, which use molten salt as a coolant, and high-temperature gas reactors, which use helium for the same purpose, these and other advanced nuclear technologies take advantage of a modular design which both minimizes the number of pumps and systems used in the plant, and allows for sections to be mass-assembled off-site.\n“Advanced nuclear plants require the same high levels of expertise and oversight during their construction and operation as conventional plants, but advanced reactors have simpler and more reliable safety systems and require relatively less safety-related oversight, due to their neutronics and thermal-fluidic properties,” the researchers found. “This enables the possibility of providing a greater level of safety at lower cost.”\nThe average levelized cost of electricity from all participating companies came out to $60/MWh, 39% lower than the $99/MWh expected by the U.S. Energy Information Administration for conventional PWR nuclear plants entering service in the early 2020s.\nTo reach this conclusion, the report analyzed data from eight advanced nuclear reactor companies: Elysium Industries, General Electric, Moltex Energy, NuScale Power, Terrestrial Energy, ThorCon Power, Transatomic Power, and X-energy. It is the first study to compare the costs of conventional nuclear versus advanced nuclear technology between energy companies. Previously, each company had used its own metric to estimate plant construction and operation costs, making comparison between projects difficult.\n“Advanced reactor companies are developing plant designs in ways intended to avoid the nature and scale of risks associated with these highly customized, site-built projects,” they wrote. “This includes high levels of modularization and standardization in plant designs, as well as exploring new manufacturing strategies and business models to limit the risk of cascading schedule delays and the amount of up-front capital needed.”\nDespite the promises of advanced nuclear energy, the researchers caution that the new technology is still years away from wide-spread adoption.\nThe report comes at a particularly significant time, when regulators are in the process of reconsidering how nuclear power should be treated under regulatory policies aimed at both promoting green energy and reducing carbon. Earlier this year, Exelon, a Pennsylvania electric company, announced that it would close the Three Mile Island nuclear power plant unless state legislators provided policy incentives to keep the plant open. Previously, Exelon had been able to negotiate concessions from lawmakers in Illinois which allowed them to keep two nuclear power plants open in the state.\nSimilar debates are taking place across the country. In New York and Illinois, generating companies running nuclear power plants have taken state regulators to court to challenge nuclear’s exemption from zero-emissions generation subsidies. In the suit, the generation companies had argued that the portion of the electric generation market currently handled by state regulators instead falls under the jurisdiction of the Federal Energy Regulatory Commission because of the Federal Power Act. The generation companies hope that under federal authority, they will find it easier to compete with state subsidies for green energy.', 'What is a Molten Salt Reactor? It is a completely different nuclear reactor than we have been using, with molten fuel cooled by stable salts. (We’ve been mainly using the Light Water Reactor, LWR, with solid fuel cooled by high-pressure water.)\nA Liquid Fluoride Thorium Reactor (LFTR) is a type of Molten Salt Reactor (MSR) that can use inexpensive Thorium for fuel (thorium becomes uranium inside the reactor). A slightly different type of MSR can consume the uranium/plutonium waste from solid-fueled reactors as fuel. MSRs make no long-term nuclear waste (over 99% of the fuel is fissioned, not left as waste).\nWe know Molten Salt Reactors work since we built and operated one — decades ago!\nImage “How Does a Fluoride Reactor Use Thorium” from PDF Kirk Sorensen – Thorium Energy Alliance.\n- Molten Salt Reactors have no high pressure to contain (no water coolant), and generate no combustible or chemically explosive materials;\n- A simple Freeze Plug melts in any emergency or for maintenance. The molten fuel then drains to passive cooling tanks where fission is impossible;\n- Reactor materials won’t melt under normal or emergency conditions, so radioactive materials stay contained. The fuel temperature is always much lower than the hottest the materials can handle. (In LWR, the temperature inside fuel pellets is always hotter than the reactor materials can handle, so cooling must remain working.)\n- Even if something (e.g. a bomb or earthquake) broke the reactor vessel, it would make a spill that quickly cools to solid, doesn’t interact with air or water, and would have most fission products chemically bonded to the salt, resulting in a cleanup volume of a few cubic meters;\n- MSRs would passively cool even without electricity (no MSR ever uses water, and no pumping is needed to cool the reactor);\n- Salt coolant can’t boil away (the boiling point of the salt is much higher than the reactor temperature), and the fuel is strongly chemically bound to the coolant, so loss of coolant accidents are physically impossible.\n- The molten fuel expands/contracts with temperature changes. Higher fission rate increases the temperature, which makes the fuel salt less dense, lowering the fission rate — all Molten Salt Reactors are very stable (a “strong negative temperature coefficient of reactivity”).\n- Fission products can be chemically separated from the fuel, while the reactor is operational. Fuel can be added as needed, to keep the fuel density steady (keep fuel concentration just above the minimum to maintain fission).\nMuch More Economical\n- Ambient-pressure operation makes MSRs easier to build while costing less (no high-pressure steam containment building, no high-pressure pipes);\n- Operating cost is less since the inherent safety of MSR means less complex systems than the LWR (every LWR requires multiple-redundant high-pressure systems);\n- Fuel cost is lower since no manufacturing fuel pellets (LWR pellets have to contain fission products under very high pressure) or fuel rods. For a LFTR, thorium is a cheap, plentiful fuel; (other MSR designs could eliminate LWR waste by using it as fuel);\n- For a LFTR, no expensive enrichment is required, simply add solid or molten thorium or plutonium to the molten fuel; for a thermal-spectrum MSR use low-enriched uranium; for a fast-spectrum MSR, un-enriched or depleted uranium can be used.\n- Total to develop LFTR technology and a factory to mass-produce them, will be less than the $10-12 Billion cost of a single new LWR; then a 100MW LFTR would cost about $200 Million. Sites can have as many reactors as needed to supply the city or region.\n- Easy siting, no large water source needed, no large safety zone required (because there is no water and no high pressure). Reactors would commonly be located several meters underground.\nMuch Less Nuclear Waste\nLWR uses ~2% of the fuel, because fission products trapped in the fuel pellets block fission, and the pellets get damaged by radiation and pressure. The rest of the uranium is considered “waste”, to be stored for over 100,000 years. Well, that is waste only if we only use LWR, or similar solid-fueled types of nuclear reactors. There are several types of nuclear reactor possible, that can fission All that uranium, plutonium, and other transuranic elements. (God didn’t make “useful uranium” and “defective uranium”; it’s the reactor design of LWR that only uses ~2% of the fuel, and that is after enrichment.)\nMSR has molten fuel, no fuel pellets, no fuel rods. Some of the fission products, those that block fission the most, are gasses — in LWR they are carefully trapped in the pellets, in MSR they bubble right out of the fuel salt and are collected. Most other fission products are easily chemically separated from the circulating fuel salt. Most MSR designs, including LFTR, use over 99% of the fuel.\nA MSR’s waste is safe (radiation levels below the original uranium ore and below background radiation) within 350 years. To produce 1 gigawatt electricity for a year, takes 800kg to 1000kg of thorium or uranium/plutonium “waste”. 83% of the fission byproducts are safe in 10 years, 17% (135 kg, 300 lbs) within 350 years, with no uranium or plutonium left as waste. After that, radiation is below background radiation levels. (Compare that 1000kg with 135kg for 350 years, to 250,000kg uranium to make 35,000kg enriched uranium for a solid-fueled reactor like LWR, for that same gigawatt-year electricity, all needing storage for 100,000+ years.)\nNo uranium, plutonium, or other long-term elements in LFTR or any MSR waste, since they are simply left in the reactor until they either fission or decay to short-term waste. (Standard industrial processing inefficiency of 0.1% leaves 1kg uranium; we can do better than that, but still much less per gigawatt-year than the 5500 kg uranium left in an open ash pile from an average USA coal plant!)\nMost of the fission products are valuable for industrial use. After a few years, radioactive decay brings them below background radiation, ready for use. For example, several rare earth metals, used for consumer electronics, are fission products. (As a bonus, the rare earth materials we currently mine are almost always found with thorium, which is currently considered a “nuclear waste” though it has one of the lowest levels of radiation of any radioactive material, radiation stopped by a thin layer of plastic or paper; when we use MSR we mine a little less rare earth materials and leave a little less thorium “waste”.)\nCan Consume Nuclear Waste\nInstead of thorium, a Molten Salt Reactor can use uranium-235 or plutonium waste, from LWR and other reactors. (Fast-spectrum molten salt reactors (FS-MSR) can use all isotopes of uranium, not just the 0.7% U-235 in natural uranium — with all the safety and stability of MSR.) 800kg of nuclear waste would work in the same reactor instead of 800kg thorium, with about the same fission byproducts, and the same electrical output. Convert 800kg to be stored for 100,000+ years, into 135kg to store for 350 years and 665kg for 10 years. No “PUREX reprocessing” needed, simply extract the uranium and plutonium (including fission products) from the fuel rod, and put it in a MSR. (In a MSR designed to use a different salt than LFTR would use, the zirconium cladding of a fuel rod could even be used to make the salt coolant.)\nSince no MSR uses water for cooling, there is no storage of water containing radioactive materials, and no concern of stored radioactive water leaking. (MSR can transfer heat to existing equipment such as steam generators, for example replacing the boiler at a coal plant, but doesn’t use water anywhere in the reactor.) Instead of using water, MSR could produce heat to efficiently desalinate water for drinking or farming.\nWithout needing a huge steam containment building (since there is no high pressure and no steam), MSRs such as LFTR use a much smaller site. A LFTR containment building would protect the reactor from outside impacts, and have extra radiation shielding, but would be much smaller and less expensive than a LWR containment building. MSRs can be safely built close to where there is electrical need (10MW to 2GW or more), avoiding transmission line power loss. No water source required.\nLFTRs could even be deployed for military field use or disaster relief. Imagine a few standard “18-wheeler” shipping containers brought in after 2017 Hurricane Harvey and Hurricane Maria, or 2018 Typhoon Mangkhut, providing 100MW electricity and desalinating water.\nCan Desalinate Water and Make Vehicle Fuel\nIn addition to delivering carbon-free electricity, LFTRs high temperature output can desalinate water (which we need in some areas even more than electricity, and we will need more as the world population grows).\nLFTRs also can generate carbon-neutral vehicle fuels, from water and carbon dioxide (from the atmosphere or ocean or large CO2 sources such as coal plants). The high heat of a LFTR (over twice what a LWR can generate) can split CO2 and split water, so making gasoline will be affordable.\nMolten Salt Reactors can be designed to output wide ranges of heat, for different industrial processes. They all automatically follow the load, meaning that if less heat is used there is less fission producing heat.\nReducing CO2 in the Oceans\nCarbon dioxide in the air enters the oceans, making acid. The acid is already killing plankton and other ocean life: the carbonic acid dissolves their “shells”. Researchers are exploring methods of using MSR heat to extract CO2 from solid materials containing a lot of CO2, store the carbon and release or use the oxygen, and then we could put those CO2-absorbing materials into the ocean to remove CO2 from the water. (Storing CO2 in a solid would work; storing compressed CO2 underground has a huge risk of leaks that would suffocate life on the surface.)\nMSRs are less expensive and more environmentally friendly than other sources of base-load power or grid power storage, needed to supplement wind and/or solar power.\nThe total cost of developing MSR technology and building assembly line production (like assembly line production of aircraft or ships, with better safety standards than is achievable with on-site construction, at much lower cost) will be much less than the $10-$12 Billion for a single new solid-fueled water-cooled reactor or single nuclear waste disposal plant. With sufficient R&D funding (around US $1 billion), five years to commercialization is entirely realistic (including construction of factories, less than US $5 Billion), and another five years for a national roll-out is feasible. (Unfortunately, the U.S. Nuclear Regulatory Commission says they will start writing licensing and regulations in 30 years.)\nCompletely Different Reactor\nThere is very little MSRs have in common with the solid fueled, water cooled reactors in use today. (Using thorium in a solid fueled, water cooled reactor, such as India is doing, does not give the safety and waste-reducing benefits of a molten fueled, salt cooled reactor.)\nNeed to communicate your complex information clearly? Contact Me.']	['<urn:uuid:b4c82220-8bbb-4f68-ba98-818baf488040>', '<urn:uuid:0507bb38-e09d-46ae-875c-f98eb32a17a4>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T19:58:41.790432	8	103	2607
45	horse exercise academy speed recommendations	For proper conditioning, it's recommended to start with 4-6 miles every other day for about an hour, mostly walking with very little trotting. After 3-4 weeks, you can increase either speed or distance - but never both simultaneously. For older horses, workload should increase on a 5 to 7 day cycle. The key principle is Long Slow Distance training, focusing on low intensity aerobic work. Over time, you can gradually increase workout time to a few hours, incorporating more trotting and occasional cantering.	"[""Saturday, March 12, 2011\nShape Up! Part I\nSaturday March 12 2011\nSince it's that time of year for many riders – time to get back in the saddle and get your blubbery horses back into shape, I'll have a few articles and links on conditioning horses. The following is a general training article of mine that was originally printed in Trail Blazer magazine in 2007.\nYou’re one of those riders who doesn't enjoy the confinement of an arena. Your horse doesn’t take well to it either. You love the outdoors and can really think of nothing better to do than spend all day in the open spaces with your horse. Maybe you’d like to have the option of adding competition to your riding repertoire. But how do you get there from here?\nWhether your goal is to participate in competitive trail rides, endurance rides, or just pleasurable trail rides, start with the fundamentals of building a solid foundation of physical conditioning and mental training underneath your horse. Getting your horse fit and confident to handle any challenge, physical and psychological, is essential to a thriving partnership and success on the trail.\nCONDITIONING: LET’S GET PHYSICAL\nConditioning is subjecting the horse to the stress of exercise, in gradually increasing workloads over time, allowing the horse’s body systems time to adapt to each increase. This process is known as progressive loading. Not only will it maximize the horse’s performance, but will also help keep the horse sound. Increasing workloads means slow and steady increase in either the duration of exercise, or the speed of exercise, but not both at the same time, approximately every week.\nYou have an advantage if you are working with an older horse who has previously had an athletic career. His body has already been accustomed to the conditioning program, so you won’t be starting from scratch, as you would with a youngster or an older horse that has never had any kind of physical training.\nWhat type of horse do you need for success? “Any breed of horse and any discipline of riding can compete at the North American Trail Ride Conference Novice level as long as the physical and mental preparations are made to compete at 20 to 25 miles,” says Lynn Smothermon, a recreational/competitive trail trainer in Orange, California. The same applies to limited distance endurance rides and pleasure trail rides - any breed can be successful. While Arabian horses have proven best in general for long distance endurance rides, there are of course exceptions; every horse is an individual, and some may naturally do better than others despite their breeding.\nPatience is a key to conditioning; it is tempting to start too soon and do too much too fast. It can take 2 to 3 years to fully condition a horse’s body systems. The cardiovascular system is the first to whip into shape. In approximately 6 months, your horse may stop huffing and puffing so hard after a workout, and he may appear to be in shape. But it’s the other systems that need the most nurturing and that take the longest to come round. According to Dr Nancy Loving, DVM, it can take ligaments and tendons 6 to 12 months to fully develop, and it can take up to 1 to 2 years for the conditioning of bone.\nFAST IS SLOW\nAsk ten different trainers, and they will give you ten very different plans for properly and carefully conditioning your trail horse. While methods always vary, the basic underlying theme and key to getting your horse fit in all these disciplines, that all trainers will likely agree to, is Long Slow Distance training, or low intensity aerobic work\nLynn Smothermon says, “In my opinion the young horse is all about slow and steady. This means spending at least 6 months to a year building hours in the saddle, with a calm confident walk as the foundation and forefront to all the other work involved – uphills, downhills, gullies, creek crossings. It’s just miles and miles of patient body building work based on the horse’s abilities.”\nFor the older horse, start out with taking him 4-6 miles every other day for about an hour. This will be mostly walking, with very little trotting. After 3-4 weeks, it’s time to add stress by increasing his workload. This means adding a little speed, or a little distance – but never both at one time. Either add more trotting over the same distance, or increase your training time by 15 minutes. A good rule of thumb for older horses is to increase workload on a 5 to 7 day cycle.\nOver the weeks and months, you will gradually increase your horse’s workout time up to a few hours, with more time spent trotting and occasionally cantering. This is a good time to begin monitoring your horse’s heart-rate and recovery; it is one of the best ways to determine the progress of his fitness. In well-conditioned horses, the heart rate should be around 60 beats per minute 10 minutes after a reasonably demanding workout.\nAs you work on conditioning your horse, monitor his progress by observing the change in his physical appearance – you should be able to see muscle development and definition in the first month. Monitor his weight by measuring his girth. Watch and feel his legs for any signs of heat or swelling. Observe his attitude: is he enthusiastic and alert during and after his training, or is he dull and tired? Keep a log of your training schedule and progress, and his heart rate and recoveries. Take pictures every week so you can see his physical change over time.\nWhen your horse’s heart rate recovers to 48 beats per minute within ten minutes of completing his exercise, (which can take from several months to a year of training), and your goal is simply pleasure trail riding, you can maintain your horse’s fitness at this level by continuing the same distance or speed of workouts a few times a week.\nIf your goal is competitive trail riding or endurance, now it’s time to add strength training. Add some inclines to your training, or trot your horse through sand. Be very cautious in sand, however, as it’s hard on tendons and ligaments. It’s best to avoid trotting through sand with young horses, and be extremely wary in deep sand with any horse.\nNow is also a good time to take your horse into the arena once a week as part of his workout regimen. Suppling exercises of circles and figure 8’s, leg yielding and sidepassing will increase your horse’s flexibility and range of motion, and therefore help prevent injuries.\nIf your ultimate goal is long distance endurance riding, you should add some anaerobic training to your conditioning program. Endurance rider Dr Nancy Loving gives good insight on aerobic and anaerobic conditioning in her books Go the Distance and All Systems Go.\nIf you are aiming for Trail Riding Competition, your first goal will be the Novice level of 25 miles. Same goes for endurance competition: your first goal should be the shorter limited distance rides of 25 miles. Depending on how your horse comes out of the ride – tiredness, weight loss, heart rate recovery – you may want to do several more limited distance rides – no more than one a month, before you progress to a slow 50-mile endurance ride. When your horse has done several 50-mile rides and handles it well – perhaps in his second season of endurance – he may be ready for his first multi-day ride, or a longer endurance ride.\nThe same goes for ride competitions as it does for training: slow is better. If you want to have a fast top ten horse, spend 2 years of riding slow (especially if you are riding a young horse, a 5 or 6-year-old in his first years of competition). Your horse will stay sounder longer and go many more miles over the years.\nNext: Part II – Training: It's All Mental""]"	['<urn:uuid:d92d71c9-17cb-4ad4-95ac-f155236cfd76>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	5	83	1342
46	I'm interested in Germany's tech history. Where were laser systems made?	Laser systems were produced in regions that had universities with physics or engineering departments and existing laser source producers.	"['Regional Knowledge and the Emergence of an Industry: Laser Systems Production in West Germany, 1975-2005\nWe analyze the emergence and spatial evolution of the German laser systems industry. Regional knowledge in the related field of laser sources, as well as the presence of universities with physics or engineering departments, is conducive to the emergence of laser systems suppliers. The regional presence of source producers is also positively related to entry into laser systems. One important mechanism behind regional entry is the diversification of upstream laser source producers into the downstream systems market. Entry into the materials processing submarket appears to be unrelated to academic knowledge in the region, but the presence of laser source producers and the regional stock of laser knowledge are still highly predictive in this submarket.\n|Date of creation:||15 Nov 2010|\n|Contact details of provider:|| Postal: Carl-Zeiss-Strasse 3, 07743 JENA|\nPhone: +049 3641/ 9 43000\nFax: +049 3641/ 9 43000\nWeb page: http://www.jenecon.de\nMore information through EDIRC\nPlease report citation or reference errors to , or , if you are the registered author of the cited work, log in to your RePEc Author Service profile, click on ""citations"" and make appropriate adjustments.:\n- Guido Buenstorf & Steven Klepper, 2009.\n""Heritage and Agglomeration: The Akron Tyre Cluster Revisited,""\nRoyal Economic Society, vol. 119(537), pages 705-733, 04.\n- Guido Buenstorf & Steven Klepper, 2005. ""Heritage and Agglomeration: The Akron Tire Cluster Revisited,"" Papers on Economics and Evolution 2005-08, Philipps University Marburg, Department of Geography.\n- Björn Alecke & Christoph Alsleben & Frank Scharr & Gerhard Untiedt, 2006. ""Are there really high-tech clusters? The geographic concentration of German manufacturing industries and its determinants,"" The Annals of Regional Science, Springer;Western Regional Science Association, vol. 40(1), pages 19-42, March.\n- Guido Buenstorf, 2007. ""Evolution on the Shoulders of Giants: Entrepreneurship and Firm Survival in the German Laser Industry,"" Review of Industrial Organization, Springer;The Industrial Organization Society, vol. 30(3), pages 179-202, May.\n- Guido Buenstorf, 2006. ""Evolution on the Shoulders of Giants: Entrepreneurship and Firm Survival in the German Laser Industry,"" Papers on Economics and Evolution 2005-20, Philipps University Marburg, Department of Geography.\n- Buenstorf, Guido & Klepper, Steven, 2010. ""Why does entry cluster geographically? Evidence from the US tire industry,"" Journal of Urban Economics, Elsevier, vol. 68(2), pages 103-114, September.\n- Anselin, Luc & Varga, Attila & Acs, Zoltan, 1997. ""Local Geographic Spillovers between University Research and High Technology Innovations,"" Journal of Urban Economics, Elsevier, vol. 42(3), pages 422-448, November.\n- Diego Comin & Bart Hobijn, 2011. ""Technology Diffusion and Postwar Growth,"" NBER Chapters, in: NBER Macroeconomics Annual 2010, Volume 25, pages 209-246 National Bureau of Economic Research, Inc.\n- Diego Comin & Bart Hobijn, 2010. ""Technology diffusion and postwar growth,"" Working Paper Series 2010-16, Federal Reserve Bank of San Francisco.\n- Diego A. Comin & Bart Hobijn, 2010. ""Technology Diffusion and Postwar Growth,"" Harvard Business School Working Papers 11-027, Harvard Business School.\n- Diego A. Comin & Bart Hobijn, 2010. ""Technology Diffusion and Postwar Growth,"" NBER Working Papers 16378, National Bureau of Economic Research, Inc.\n- Diego Comin & Bart Hobijn & Emilie Rovito, 2008. ""Technology usage lags,"" Journal of Economic Growth, Springer, vol. 13(4), pages 237-256, December.\n- Ron A. Boschma & Rik Wenting, 2004. ""The spatial evolution of the British automobile industry,"" Papers in Evolutionary Economic Geography (PEEG) 0504, Utrecht University, Section of Economic Geography, revised Aug 2004.\n- Guido Buenstorf & Matthias Geissler, 2011. ""The origins of entrants and the geography of the German laser industry,"" Papers in Regional Science, Wiley Blackwell, vol. 90(2), pages 251-270, 06.\n- Guido Buenstorf & Matthias Geissler, 2008. ""The Origins of Entrants and the Geography of the German Laser Industry,"" Papers on Economics and Evolution 2008-14, Philipps University Marburg, Department of Geography.\n- Michael S. Dahl & Christian Ø.R. Pedersen & Bent Dalum, 2003. ""Entry by Spinoff in a High-tech Cluster,"" DRUID Working Papers 03-11, DRUID, Copenhagen Business School, Department of Industrial Economics and Strategy/Aalborg University, Department of Business Studies.\n- Mario Cleves & William W. Gould & Roberto G. Gutierrez & Yulia Marchenko, 2010. ""An Introduction to Survival Analysis Using Stata,"" Stata Press books, StataCorp LP, edition 3, number saus3, September.\n- Jurgen Egeln & Sandra Gottschalk & Christian Rammer, 2004. ""Location Decisions of Spin-offs from Public Research Institutions,"" Industry and Innovation, Taylor & Francis Journals, vol. 11(3), pages 207-223.\n- Audretsch, David B & Stephan, Paula E, 1996. ""Company-Scientist Locational Links: The Case of Biotechnology,"" American Economic Review, American Economic Association, vol. 86(3), pages 641-652, June. Full references (including those not matched with items on IDEAS)']"	['<urn:uuid:8615eac9-65d0-4fca-8133-da478a6d81b6>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T19:58:41.790432	11	19	756
47	motor cortex vs intraperietal sulcus brain calculations	The motor cortex and intraperietal sulcus serve different functions in cognitive processing. The motor cortex is involved in understanding action-related words, as demonstrated by experiments where TMS suppression of motor cortex activity interfered with comprehension of hand-movement related words. In contrast, the intraperietal sulcus shows specific activation patterns during mathematical calculations, with electrical activity spikes occurring not only during actual calculations but also when numbers are mentioned or when words like 'many' or 'more than' are used. This demonstrates how different brain regions specialize in distinct cognitive processes while working together in distributed networks.	"['Researchers from HSE, Northumbria University, and Aarhus University have experimentally confirmed the hypothesis, whereby comprehension of a word\'s meaning involves not only the \'classic\' language brain centres but also the cortical regions responsible for the control of body muscles, such as hand movements. The resulting brain representations are, therefore, distributed across a network of locations involving both areas specialised for language processing and those responsible for the control of the associated action. The results have been published in the journal Neuropsychologia.\nOne of the basic issues related to the nature of human cognition is the question about the correspondence between physical experiences and feelings, on one hand, and the nature of the brain representations of words and sentences describing these experiences, on the other.\nTraditional modular views of cognition suggest that, to encode and comprehend the meaning of a word such as \'throw\', the brain\'s ""language module"" does not to involve any structures related to the meaning per se (i.e. the ""motor module"" responsible for the associated movements programs such as the arm and hand movements involved in the act of throwing.\nAn alternative is offered by an embodied or distributed view suggesting that the brain areas encoding the meaning of a word include both the areas specialised for representing linguistic information, such as the word\'s acoustic form, but also those brain areas that are responsible for the control of the corresponding perception or action. On this account, in order to fully comprehend the meaning of the word \'throw\', the brain needs to activate the cortical areas related to hand movement control. The representation of the word\'s meaning is, therefore, \'distributed\' across several brain areas, some of which reflect experiential or physical aspects of its meaning.\nA team of researchers from Denmark, England, and Russia (Nikola Vukovic, Matteo Feurra, Anna Shpektor, Andriy Myachykov, and Yury Shtyrov) investigated the nature and the mechanisms of such distributed word representations. They carried out a series of experiments aiming at finding out how stimulating motor cortex using transcranial magnetic stimulation (TMS) affects word comprehension.\n28 volunteers took part in these experiments. A TMS magnetic pulse was delivered to the areas in motor cortex responsible for hand movements as participants engaged in one of the two computer-based experimental tasks: detecting whether a presented string of letters is a word or not, and choosing whether the presented stimulus relates to an abstract or a concrete action.\n\'We used TMS to inhibit neural activity in the motor cortex as participants tried to distinguish between words related or unrelated to hand movements,\' says Andriy Myachykov, leading Research Fellow at the HSE Centre for Cognition & Decision Making and a Senior Lecturer at Northumbria University, Newcastle-upon-Tyne. He notes: \'The advantage of TMS methodology is that it allows to establish the causal link between the stimulated brain area and the cognitive function or behaviour it\'s hypothesised to support. This distinguishes TMS from many other existing neuroimaging methods. If motor programmes are directly involved during the comprehension of action words, then suppressing neural activity in hand-related motor cortex would interfere with word processing but only if the word also denotes hand movement. Namely, this should lead to increase in task performance errors and longer reaction times. This is exactly what we found\'.\nThese new findings suggest that language-specialised brain areas work in constant interaction with other areas known to support other cognitive processes, such as perception and action. The resulting distributed meaning representations act as dynamic cortical networks rather than a series of specialised modules as suggested by traditional theories.', ""'We are now able to eavesdrop on the brain': Scientists come closer to mind-reading devices following breakthrough brain study\n- Stanford University School of Medicine develop new way to monitor brain technique\n- Called 'intracranial recording', it involves removing a section of the skull and attaching electrodes to exposed brain surface\n- Following tests on three epilepsy patients, researchers found there were spikes in electrical activity whenever the subjects were faced mathematical equations, even just with words like 'more than' or 'many'\n- The results mean potential new medical applications, such as brain chips that grant people who cannot speak a way of communicating\nScience fiction has long speculated what it would be like to peek inside a person's mind and find out what they are thinking.\nNow scientists are one step closer to such technology after forging a new brain monitoring technique that could lead to the development of 'mind-reading' applications.\nThe breakthrough comes from a Stanford University School of Medicine study that was able to 'eavesdrop' on a person's brain activity as they performed normal functions by utilizing a series of electrodes attached to certain portions of the brain.\nThe process, called 'intracranial recording', was tested on three epilepsy seizure patients who had been admitted to the hospital for pre-surgery observation, according to FoxNews.com.\nA new study from the Stanford University School of Medicine has developed a new way to monitor the brain\nDuring visits, the patients had a portion of their skull temporarily removed so that intracranial electrodes can be attached to the exposed brain surface.\nThey are then monitored for up to a week as the electrodes pick up electrical activity in the brain, allowing neurologists to observe the patients’ seizures and pinpoint the exact portion of the brain from which the seizures are originating.\nThe patients were confined to their rooms and were comfortable, pain-free and alert, making them perfect for understanding how the brain operates in normal scenarios.\nThe patients were asked to solve mathematical equations and various true/false questions that appeared on a computer screen.\nSome of the true/false questions required the use of simple mathematical calculation, such as verifying whether or not 2 + 2 = 5.\nAfter analyzing the patients' electrode records from these experiments, the researchers saw a spike in the electrical activity of the brain’s intraperietal sulcus when the patients performed calculations.\nResults from the study identified brain cells that spiked in electricity when faced with mathematical equations. Researchers discovered that 'when the subject is reminiscing, laughing or talking' these cells are not activated\nThey also found that activity in this brain region spiked several other times throughout the day.\nBy watching back video recordings of the experiments, the research team discovered that whenever a number was mentioned - even just through words like 'many' or 'more than' - the same spikes were seen in the intraperietal sulcus, which was a very unexpected finding.\nThe findings, which will be detailed in Nature Communications, provide a new framework for studying for the brain works in normal day-to-day circumstances.\n'We’re now able to eavesdrop on the brain in real life,' lead author Dr. Josef Parvizi, associate\nprofessor of neurology and neurological sciences and director of\nStanford’s Human Intracranial Cognitive Electrophysiology Program\n(SHICEP), told The Amalgest.\n'The beauty of this paper is not just to report another experimental finding, but it is a breakthrough in terms of methodological advancement in terms of being able to record from brain activity in real life, natural conditions.\n'These nerve cells were not firing chaotically.\nThe ways in which doctors and scientists are able to study the brain are limited because of the organ's sensitivity. The new method, called intracranial recording, is being seen as a breakthrough in the area\n'They’re very specialized, active only when the subject starts thinking about numbers.\n'When the subject is reminiscing, laughing or talking, they’re not activated.'\nThus, it was possible to know, simply by consulting the electronic record of participants’ brain activity, whether they were engaged in quantitative thought during nonexperimental conditions.\nParvizi said the success of the study meant that intracranial recordings could change the way researchers observe the brain.\nCurrent brain monitoring techniques, such as magnetic resonance imagining (MRI), do not provide a very accurate picture of the human brain in normal settings.\n'The MRI scanner is several tons, and you can’t actually take an MRI scanner home, but this (apparatus) is something you can walk with – as a patient of course,' Parvizi said.\n'So subjects that are implanted with these spying electrodes, they were walking and talking… We have a new method by which we can study the brain activity in natural environments, so it’s totally different than other experiments.'\nParvizi said this technique has the potential to lead to very beneficial medical applications, especially for patients whose brains or nervous systems have been severely damaged.\nThe future: It is hoped the findings of the new study will one day allow for people who cannot speak to be implanted with brain chips that could allow machines to talk for them\n'If we’re able to decipher the code of brain activity, let’s say beyond mathematics, then patients who are unable to speak, for example (due to) stroke, or are unable to move, we could use this deciphering method to communicate with machines so that machines can do (the talking),' Parvizi said.\n'Or we can somehow try to understand what’s going on in the brain activity without even patients talking.'\nWhile some experts have speculated that Parvizi’s new technique could one day be used in a sinister way to read a patient’s private thoughts, Parvizi said that is still a very fictional concept.\n'This is too far-fetched. We are not there yet. We are light years away from mind-reading,' Parvizi said.\n'I don’t want people to get scared that doctors are mind-reading their patients.'\nThe comments below have not been moderated.\nThe views expressed in the contents above are those of our users and do not necessarily reflect the views of MailOnline.\nWe are no longer accepting comments on this article.""]"	['<urn:uuid:011c95cd-362c-4bd8-b091-f90d9a1a74b1>', '<urn:uuid:a6da87db-0a9e-4e7c-a0c0-4c4d449d3170>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T19:58:41.790432	7	94	1603
48	Will I be responsible for environmental issues when buying assets?	Yes, you can be responsible for environmental liabilities when buying assets, even if they're unrelated to purchased assets. Courts have found that purchasers of substantially all company assets may assume the company's environmental liabilities, regardless of whether they're associated with the purchased assets.	['Virtually all business transactions involve some level of environmental risk. The key is to identify all of the potential risks and collect sufficient information about them early in the due diligence period of a transaction. This proactive approach to environmental due diligence will help the buyer determine whether the risks are acceptable in light of the overall transaction and develop a strategy for managing them, both in the contract negotiations prior to acquisition and after the transaction is complete.\nHow much and to what extent businesses should conduct environmental due diligence typically depends on the nature of the transaction and the anticipated use of the property after purchase.\nBelow are five tips for buyers to consider when determining the appropriate level of environmental due diligence that should be performed in business transactions.\nAlthough the tips are geared toward buyers, sellers should also consider them so that they can better anticipate the types and level of due diligence that a buyer may perform (and understand their reasons for doing so). Identifying and correcting environmental problems in advance of the transaction may help prevent buyer demands for purchase price reductions or substantial escrows to cover potential environmental liabilities.\nThe most common types of environmental liability and costs associated with businesses and their properties include:\nWhen purchasing a property with no operations or structures from an unrelated third party, the environmental concerns may be limited to contamination on or emanating from the property. A Phase I environmental site assessment (Phase I ESA) that does not identify any environmental contamination concerns may be sufficient in that context. This scenario, however, is rare.\nA buyer planning to redevelop a property with existing structures will be concerned about the potential for added costs and delays associated with hazardous materials in the structure and conditions that may require special permits or design modifications. Purchasers of property who will continue the same operations will be concerned about the company’s compliance with environmental laws and permits and whether substantial expenditures may be necessary for the operations to achieve or maintain compliance. Stock purchasers and, as discussed in Tip 4, some asset purchasers will be concerned about all of these potential liabilities, including legacy environmental liabilities.\nA Phase I ESA is a non-intrusive investigation into past and current uses of a property to evaluate the potential existence of hazardous substance or petroleum contamination on or migrating from the property.\nA Phase I ESA generally consists of the following activities by an environmental professional:\n(i) a review of publicly available records;\n(ii) a site visit;\n(iii) interviews of owners, occupants and government officials; and\n(iv) a report describing the results of the investigation and the environmental professional’s opinion as to whether there is specific evidence of, or reason to believe, that the property is contaminated.\nAlthough it may contain general information about the property and facility operations, a Phase I ESA does not specifically address whether the operations comply with environmental laws and permits or whether there are site conditions that could affect the buyer’s development plans. A Phase I ESA also does not address the company’s potential liability for off-site waste disposal or legacy environmental liabilities.\nGiven that a Phase I ESA does not address many of environmental liabilities that a buyer may be concerned about, what other environmental due diligence methods are available? Below are a few of the most common additional due diligence methods.\nPhase II Environmental Site Assessments – testing of soil/groundwater to determine whether the property is, in fact, impacted.\nEnvironmental compliance audits – evaluation of the facility operations to determine whether all required permits have been obtained and the operations comply with the permits and environmental regulations.\nAsbestos surveys – identification and testing of building materials suspected to contain asbestos (Building materials that contain asbestos must be removed by a licensed asbestos contractor prior to any renovation or demolition activity.)\nWetland surveys – identification of wetlands subject to federal or state regulation in connection with construction and development activities\nComputer database searches – third-party database search services may be used to research the company’s former properties and businesses as well as determine whether it has been named a potentially responsible party (PRP) at Superfund sites\nFollowing a stock transaction, the surviving corporation will obtain all of the liabilities and obligations of the merging corporation. That’s why environmental due diligence in a stock deal should be comprehensive and include not only current properties and operations of the company, but also its former properties and operations, as well as any businesses that have been acquired/divested.\nBuyers often assume that if the transaction is structured as an asset deal rather than a stock deal, they need not worry about environmental liabilities that are unrelated to the purchased assets. While the general common law rule is that asset purchasers are not liable for debts and obligations of the seller corporation, there are four exceptions:\n(a) The purchaser expressly or impliedly agrees to assume seller’s liabilities;\n(b) The transaction is a de facto merger;\n(d) The purchaser is a “mere continuation” of the seller; or\n(e) The transaction is a fraudulent effort to escape liability.\nMany federal and state courts have found, applying especially the second and third exceptions, that a purchaser of substantially all of the assets of a company will assume the company’s environmental liabilities, whether or not they are associated with the purchased assets.\nRegardless of the exceptions applied, court decisions on successor liability for asset purchasers are made on a case-by-case basis and tend to involve fact-intensive analyses. The following factors have been relied on in finding that successor liability applies to asset purchasers.\nEach of these factors should serve as potential red flags to the buyer that its asset purchase may make it a successor of the seller — one who is now responsible for all of the seller’s environmental liabilities. If one or more of these factors are present following a transaction, the buyer should investigate the gamut of seller’s potential environmental liabilities, including its compliance history and that of its former properties, businesses and operations.\nLast, but not least, consult with an environmental attorney early in the transaction. Be sure to provide the attorney with all available details about the proposed structure of the transaction and the plans for the property and operations after the closing. The attorney can assist in developing a strategy and timetable for completing the appropriate environmental due diligence. Often, an attorney can also advise on steps that can be taken before and after the transaction to minimize or manage environmental risk.\nAlthough we would like to hear from you, we cannot represent you until we know that doing so will not create a conflict of interest. Also, we cannot treat unsolicited information as confidential. Accordingly, please do not send us any information about any matter that may involve you until you receive a written statement from us that we represent you (an â€˜engagement letterâ€™).\nBy clicking the â€˜ACCEPTâ€™ button, you agree that we may review any information you transmit to us. You recognize that our review of your information, even if you submitted it in a good faith effort to retain us, and, further, even if you consider it confidential, does not preclude us from representing another client directly adverse to you, even in a matter where that information could and will be used against you. Please click the â€˜ACCEPTâ€™ button if you understand and accept the foregoing statement and wish to proceed.']	['<urn:uuid:a358cb66-9381-4558-b730-1c30b51f4331>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T19:58:41.790432	10	43	1245
49	How much protein does a person need when losing weight?	When dieting, it is recommended to consume 1.0-1.2 grams of protein per pound of body weight (2.2-2.6 g/kg) to maximally support strength and muscle growth.	['This article focuses on the extra nutritional considerations for vegan readers looking to minimize any performance and muscle gain compromises. There are three key areas: protein quality and quantity, fat intake, and the supplementation that may be necessary.\nIn late 2019 I wrote an article titled “Debunking ‘The Game Changers’ Netflix Documentary.”\nI’m not usually one to write debunk pieces, preferring instead to focus on giving recommendations on what people should be doing. Still, this documentary gained such widespread attention, and I was receiving so many questions from curious readers, that I deemed it necessary to put out a public stance on it.\nThe documentary made several outrageous health and performance claims that simply aren’t backed by science. The purpose of my article was to point a few of these out because I don’t think anybody should be scared into following a vegan diet for health or performance reasons. — Plants are good, but it’s not necessarily the exclusion of animal products that makes a diet better.\nThere are plenty of non-performance, non-health reasons someone may choose to follow a vegan diet (religious, environmental, moral) and I’m sure a fair few people feel conflicted with their desire to get jacked and lean.\nFortunately, though it does require considerably more effort and thought, you should still be able to do that. This article will focus on those special nutrition considerations that fall outside of my general nutrition setup recommendations.\nProtein Intake Considerations For Vegans\nIs protein quality is an issue for vegan diets that we should consider compensating for?\nProtein is made up of building blocks called amino acids. One of the key determinants of protein quality is the essential amino acid content (EAAs). (Essential nutrients we have to get from the diet; non-essential nutrients we can get from biochemical conversions in our body.)\nMany plant-based sources of protein don’t have all nine EAAs in enough abundance to call them complete. For this reason, you may have heard it said that you should consume a meal containing mixed protein sources. However, given how slow the rate of digestion is, in practice, as long as you have a diet drawing from mixed sources of protein, you’re probably ok.\nSo, how much protein should a physique conscious vegan consume?\nWell, we know from the latest research that somewhere between 0.7–1.0 g per pound (1.5–2.2 g/kg) of body weight is the range that is most likely to maximally support strength and hypertrophy, in people who lift weights, if they are not dieting.\nWe also know that protein needs are higher in those who are dieting, with 1.0–1.2 g per pound (2.2–2.6 g/kg) of body weight recommended.\nAs these numbers are not derived from vegan diet research, it might be smarter to shoot for the higher end of the range, so that the quantity of the protein can potentially make up for any shortfall in EAAs. (Especially given that protein from plants is generally less well absorbed so less ends up in the blood.)\nBut protein quality is not just about the completeness of the EAA profile. We know from research that a subset of the EAAs called the branch-chain amino acids (BCAAs), particularly leucine, is important for muscle repair, maintenance, and growth.\nA protein source can be ‘complete’ without it being ‘high quality’ for muscle building.\nPlant-based sources of protein tend to have fewer BCAAs and leucine. So, it is a good idea to augment your diet with a ‘high quality’ and ‘complete’ protein powder like pea or pea/rice blends. These powders are close to the amino acid profile of whey protein (the highest quality protein source we have, but a milk-derived one).\nNote: I would not recommend soy powder. While it has a complete EAA profile, the BCAA content is low, making it not very high quality for our purposes. A 70:30 mix of pea and rice protein closely mimics the amino acid profile of whey. You can order custom blends from a number of stores, I’ve heard good things about True Nutrition.\nSo in summary, my daily protein intake recommendations for vegan readers are:\n- Aim to eat 1.0 g of protein per pound (2.2 g protein/kg) of body weight when not dieting.\n- Aim to eat 1.2 g of protein per pound (2.6 g protein/kg) of body weight when dieting.\n- Take a 40 g serving of protein powder 30–60 minutes prior to your workout if training fasted. If you have access to the 70:30 mix of pea and rice protein, ~30 g will be sufficient.\n- Try to have a varied diet so that you don’t run into any ‘completeness’ issues.\nFat Intake Considerations For Vegans\nVegan diets tend to be lower in fat. Low fat intake levels are linked with low testosterone levels, which can impact health and performance. So, make sure to eat enough oils, nuts, and seeds. (Half an avocado a day could be your friend also.)\nI’d recommend 15-25% of your daily caloric intake come from fats, with an absolute minimum of 0.25 g/lb (0.5 g/kg).\nSupplement Considerations For Vegans\nDue to lack of bioavailability, or a lack of the nutrient in the diet, there are several things that may be low and worth supplementing. Below is a summary.\nFor a more detailed look each of these considerations, check out this excellent article on my friend’s site: Plant Gains? Advice to the Vegetarian and Vegan Athlete.\nAlso, before going out and supplementing with everything here I would recommend getting some bloodwork done prior so you can see what is needed.\nVitamin B12, because roughly 50% of vegans are deficient, and over time this can lead to anemia and even irreversible degeneration of the nervous system (source). Recommended daily dose: 2.4 up to 6 μg.\nIron, due to a lack of red meat. Recommended daily dose: 14 mg for men, 33 mg for women.\nZinc, due to poor absorption from plants. Recommended daily dose: 16.5 mg for men, 12 mg for women.\nCalcium, due to poorer absorption. Recommended supplement dose 500-1000 mg daily.\n(^ Many of those above will be covered in vegan multivitamin products.)\nOmega-3, due to a lack of fish consumption, through an algae-based supplement. Recommended daily dose: 1–2 g of EPA and DHA in total.\nVitamin D3, which is not exclusively a vegan issue, but I wanted to point out that lichen-based sources of D3 are now available. Recommended supplement dose 1000–3000 IUs depending on body mass and sun exposure.\nCreatine, due to a lack of red meat, fish, and poultry. Performance enhancement benefits aside, it plays a critical role in cognitive function and is worth supplementing. It’s created in a lab and is not from animals. Recommended daily dose: 5 g.\nI hope this was useful. – Andy']	['<urn:uuid:9efaae66-a992-47d1-b760-215f94dc2c9d>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	10	25	1125
50	nox formation process environmental impacts	NOx forms through two main pathways: thermal NOx (when fuel burns at high temperatures) and fuel NOx (when fuel nitrogen reacts with oxygen). Its environmental impacts are significant - it contributes to ground level ozone formation, acid rain, particulate matter, and indirectly to global warming. When NOx and volatile organic compounds (VOCs) react in sunlight, they form photochemical smog. The process involves NOx being photolyzed by UV-A radiation to produce ground-state atomic oxygen, which then reacts with molecular oxygen to form ozone. This pollution comes from various sources including ships, factories, coal-burning plants, and motor vehicles.	['The continuous rise of Nitrogen Oxide (NOx) in the atmosphere is a matter of great concern. Several factors have been stated as the reason for the increase of this element in the air and toxic emission from ships is one of them.\nNitrogen Oxide (NOx) is formed in the atmosphere when fuels such as oil, gas, and coal are burned at a very high temperature. The pollution caused because of NOx is supposed to be known as one of the most dangerous forms, which eventually contributes towards global warming.\nNitrogen Oxide and its effects on the Environment\nA high-level of nitrogen oxide being released into the atmosphere can result in to:\n- Ground Level Ozone\n- Acid Deposition\n- Particulate Matter\n- Indirect Effect to Global Warming\nSignificant contributors to this toxic oxide are factories, coal-burning plants and emissions from motor vehicles and marine diesel engines.\nStudies show that shipping is the source of 18-30% of the world’s nitrogen oxides. Moreover, though several steps have been taken to reduce the emission of NOx from ships, there is still a long way to go to bring down emissions to zero or an acceptable level.\nIMO’s MARPOL Annex VI\nMARPOL (Marine Pollution) is one of International Maritime Organization (IMO)’s regulatory policies focuses on preventing different forms of marine pollution including oil, noxious liquid substances, harmful substances, waste water, garbage and emissions of sulphur oxides and nitrogen oxides at sea.\nMARPOL Annex VI Regulation 13 sets out the mandatory limitations on NOx. The regulation affects not only ships from signatory states but also ships entering MARPOL signatory-member waters.\nMARPOL Annex VI and ECAs\nApart from the mandatory NOx limitations for oceangoing vessels around waters of the member states, IMO also defines Emission Control Areas or ECAs where the MARPOL NOx emission standards will apply.\nThe ECAs includes:\n- Baltic Sea (2006)\n- North Sea (2007)\n- Waters in North American coasts that include waters adjacent to the Pacific coast, the Atlantic/Gulf coast extending to 200 nautical miles from US coast (2010)\n- Waters around Puerto Rico and the US Virgin Islands that are just recently designated by IMO (effective 2014)\n- Norway, Japan and Mediterranean areas are being considered for future ECA proposal.\nIn the past few years, IMO has become stricter in implementing norms to reduce shipping emissions and to minimize the effects of marine pollution.\nIs there a way to reduce NOx emission?\nIn the hope of complying with IMO’s global limits on nitrogen oxides and the enforcement of more stringent standards within the three Emission Controlled Areas marine diesel engine operators, ship owners and seafarers are left with no option but to find the best technology in reducing the amount of NOx from their ship’s exhaust systems and to take steps to make their ship “greener”.\nAmong the various emission control applications Selective Catalytic Reduction (SCR) System comes out to be the most efficient, effectively reducing ship’s NOx emission by 90-95%. By mixing a reagent (SCR 40 – 40% Marine Urea Solution) to the exhaust gas, nitrogen oxides are converted to Nitrogen (N2), water and Carbon Dioxide (CO2).\nGlobal Marine Urea Solution Suppliers\nSome marine SCR System providers offer the supply of urea solution with their application. A known SCR System provider in Japan is Hitachi Zosen while Miratech is from the United States. Both suppliers offer Marine SCR application that complies with Tier III NOx emission standards.\nFor marine urea solution requirements, there are growing urea markets in the US, China and recently in Singapore. Read here for more information regarding marine urea solutions in Singapore.\nSourced by ekomeri.com', 'System to Control Nitrogen Oxides (NOx) in Coal Fired Power Plants\nBy Dr. Prasanna Seshadri\nCommonly referred to as NOx, Nitrogen oxides are one of the primary pollutants emitted by high temperature combustion systems such as pulverized coal (PC) fired boilers. Due to its role in both acid rain and ozone formation, NOx is a regulated pollutant under the EPA’s Clean Air Act. The options to reduce NOx emissions include source reduction and post combustion treatment of the flue gas. To control NOx at source, fuel and air distribution, otherwise referred to as staging, are modified to reduce flame temperatures. This is typically achieved by using low NOx burners.\nPost combustion measures for NOx control include both catalytic (SCR) and non-catalytic (SNCR) where NOx in the flue gas is reduced to molecular Nitrogen by reaction with Ammonia. However, it is quite common for utilities and other industrial combustion systems to employ both source reduction and flue gas treatment to meet required regulations. This paper will particularly focus on the two tail end options, SCR and SNCR, for flue gas NOx control.\nNitrogen oxides (NOx) denote the combined emissions of nitric oxide (NO), nitrogen dioxide (NO2) and other nitrogenous species in combustion derived flue gas. While NOx comprises all oxygenated nitrogen species, NO is the most dominant species in combustion gases accounting for anywhere between 95-99% of the total NOx in the gas stream. Together with SOx and Particulate Matter (PM), NOx is strictly regulated. NOx contributes to the formation of smog and acid rain, as well as increasing the amount of ozone (O3) in the earth’s troposphere. When NOx and volatile organic compounds (VOCs) react in the presence of sunlight, they form photochemical smog, which is a significant form of air pollution. The high concentrations of ozone in the atmosphere typically arise from high NOx emissions together with other reactive hydrocarbons, termed as VOCs. At lower levels, ozone is created by a chemical reaction between nitrogen oxides (NOx) and VOCs in the presence of heat and sunlight. Production of O3 follows the chain reaction mechanism as follows7:\nNOx + VOCs + Sunlight → O3 + other products (1)\nNO2 + hν → NO + O(3P), λ<400 nm (2)\nO(3P) + O2 → O3 (3)\nPeroxy-radicals (HOCO) in the atmosphere, produced as a result of carbon monoxide (CO) oxidation with the hydroxyl radical OH, go on to react with NO to produce NO2, which is then photolysed by UV-A radiation to give a ground-state atomic oxygen. This further reacts with molecular oxygen to form ozone7. While NOx emission limits vary globally, the U.S has one of the strictest limits in the world, especially for existing coal power plants. For new installations, the limits are even more stringent requiring the use of Best Available Control Technology (BACT) in both attainment and non attainment areas. Selective Catalytic Reduction, referred to as SCRs, is the current BACT for NOx control in power plants and also for other industrial applications. NOx is also regulated in combustion engine vehicles, especially vehicles using diesel engines.\n2. Nitrogen Oxides (NOx) From Fuel Combustion\nTypically, NOx formation happens during fuel combustion through two major pathways. In the first case, Nitrogen present in combustion air reacts with oxygen forming Nitrogen Oxide species. This is referred to as Thermal NOx and is a result of high combustion temperatures. The other major source of NOx is fuel NOx which is the result of fuel Nitrogen reacting with the oxygen in combustion air. Unlike thermal NOx, fuel NOx formation can happen at much lower gas temperatures. Fuel NOx is a significant contributor especially in coal power plants and can account for over 50% of the overall amounts of NOx generation. The control of NOx in combustion derived flue gas usually involves a two-phased approach – during fuel combustion and post treatment of flue gas. The primary focus of combustion control is to minimize thermal NOx formation and the latter option is to reduce the overall amount of already formed NOx.\n3. Control Techniques\n3.1. At Source Reduction\nWith the development of advanced burner technologies, NOx control begins with fuel combustion. Low NOx burners are designed to reduce the overall combustion temperature, thereby reducing the total amount of thermal NOx. Low NOx burners control fuel and air mixing to create larger and more branched flames. They accelerate fuel ignition and intensify combustion by achieving fuel rich conditions in the burner zone1. By improving ignitibility in rich fuel flame areas and producing moderate combustion in moderate fuel flame areas, the production of NOx emissions is reduced2.\nFigure 1. Low NOx type NR burner by Mitsubishi Hitachi Power Systems.\nNote. Adapted from https://www.mhps.com/products/boilers/technology/low-nox-burner/index.html\nCombustion reduction and burnout are achieved in three stages within a conventional low NOx burner. In the first stage, combustion occurs in a fuel rich, oxygen deficient zone where NOx formation takes place. In the second stage a reducing atmosphere follows where hydrocarbons react with the already formed NOx and reduce them to molecular Nitrogen. In the third and final stage of combustion an oxygen rich environment finalizes combustion of the fuel. Control of secondary air is important to ascertain full combustion, but not enough oxygen to regenerate NOx2.\nHowever, source control by itself is not enough to meet NOx emission standards. Downstream technologies are also required to further reduce NOx. The two most common downstream measures to control NOx are Selective Catalytic Reduction (SCR) and Selective Non-Catalytic Reduction (SNCR). Both technologies convert NOx in the flue gas to inert molecular Nitrogen by reaction with ammonia. The different reactions for NOx reduction are shown below and is applicable for both SCRs and SNCRs.\n4NO + 4NH3 + O2 → 4N2 + 6H2O (4)\n2NO2 + 4NH3 + O2 → 3N2 + 6H2O (5)\nNO + NO2 + 2NH3 → 2N2 + 3H2O (6)\nThere are process, efficiency and cost differences between the two technologies and due diligence is required to make a proper selection. The selection of NOx control systems needs to be done in coordination with the overseeing environmental agency.\n3.2. Selective Catalytic Reduction (SCR)\nSCRs are state of the art systems used to eliminate or reduce NOx emissions from the flue gas stream. In this process, ammonia (NH3) is injected into the flue gas stream to create a chemical reaction between NOx and NH3 forming inert Nitrogen molecules and water vapor as products. These chemical reactions are shown in equations 4-6. For the chemical reactions to occur, the gas needs to be between the optimum temperatures of 300°C and 400°C; therefore, the system is usually placed before the air preheater, at the exit of the economizer.\nThis technology uses a catalyst which facilitates the breakdown of NOx and increases the overall conversion efficiency. Most catalysts used in coal power plants consist of vanadium making up the active catalyst, and the substrate (or catalyst support) is usually made of titanium3. Active catalyst is finely dispersed across the support media. However, the final composition can consist of many active metals and support materials to meet specific requirements in each SCR installation. Typically, over 90% reduction in NOx can be achieved with the installation of a SCR6. SCRs are also a widely used technology in the off-gas treatment of large gas combustion turbines.\nFigure 2. Schematic of a SCR process.\nNote. Adapted from https://www3.epa.gov/ttn/ecas/docs/SCRCostManualchapter7thEdition_2016.pdf\nSCRs are designed to handle dust loads and do not require dust capture equipment upstream of the process. The system is relatively easy to maintain and capable of stable operation. While the system offers high NOx control, it is capital cost intensive and requires considerable plant outage time for retrofit applications.\nFigure 3. Selective Catalytic Reduction (SCR) System.\nNote. Adapted from https://www.mhps.com/products/aqcs/lineup/flue-gas-denitration/\nSCR capital costs vary by the type of unit controlled, the fuel type, the inlet NOx level, the outlet NOx design level, and reactor arrangement. Data collected on new installations between 2012- 14 indicated SCR costs ranged from $270/kW to $570/kW5. Typical operation and maintenance costs are approximately 0.1 cents per kilowatt-hour (kWh)5. Operating costs for SCR consist mostly of replacement catalyst and ammonia reagent, and while historically the catalyst replacement has been the higher cost, the reagent cost has become the most substantial portion of operating costs for most SCR. Since the gas pressure drops across the SCR system, the plant may also require ID fan modifications to compensate for the increased pressure loss.\n3.3. Selective Non-Catalytic Reduction (SNCR)\nWhile SCR is considered as the BACT, in certain applications, SNCR technology can be deployed at a relatively low capital cost. However, SNCR does not employ a catalyst and NOx reduction is typically a modest 30-50%4 for most systems. This technology utilizes atomizing nozzles to inject ammonia directly into the hot gas to chemically reduce NOx to Nitrogen and water vapor. Unlike the SCR, NH3 injection is at a much higher temperature window between 850-1050 C4. The goal is to maximize NOx control performance while optimizing chemical utilization with low reagent consumption.\nFigure 4. Schematic of SNCR process.Note. Adapted from https://www3.epa.gov/ttnecas1/models/SNCRCostManualchapter_Draftforpubliccomment-6-5- 2015.pdf\nAdditionally, urea can also be used as a reagent for SNCR applications. Urea (NH2CONH2) is easier to handle and store than NH3. Hence, urea-based systems are more common than ammonia based deployments, but operating data reveals higher NOx reductions occur with ammonia reagent. This is mainly due to improper and incomplete gas mixing with urea injection compared to anhydrous ammonia. Urea dissociates into ammonia first and then reacts with NOx as shown in chemical reactions 4-6. The dissociation of urea to ammonia is as follows: NH2CONH2 + H2O -> 2NH3 + CO2 (7)\nFigure 5. Urea based SNCR process schematic.\nNote. Adapted from https://www3.epa.gov/ttnecas1/models/SNCRCostManualchapter_Draftforpubliccomment-6-5- 2015.pdf\nThe effect of temperature is critical for SNCR applications. Not only does temperature affect conversion efficiencies, but it can also have a debilitating effect if ammonia or urea is injected outside the recommended range. At lower temperatures NO and the ammonia do not react. Ammonia that has not reacted is called ammonia slip and can react with other combustion species, such as sulfur trioxide (SO3), to form ammonium salts. These salts can later form hard deposits on downstream equipment thereby reducing operating efficiencies. At higher temperatures ammonia decomposes to form more NOx.\nSNCR is applied in a wide range of industrial processes, including cement and steel production. While not a BACT option for power plants, it is suited for areas under Reasonable Available Control Technology (or RACT) protocol. Retrofits are also much easier and require a small area for installation. Available data for Best Available Retrofit Technology (BART) analyses for 11 cement kilns indicates estimated NOx reductions for SNCR systems between 35 percent and 58 percent with a median reduction of 40 percent6.\nThe mechanical equipment associated with an SNCR system is simple compared to SCR and hence capital costs for SNCR installations are generally low. Based on available data, the installed capital cost of SNCR applications ranged anywhere between $5–20/kWe (kilowatt) for power generation units6. The absence of an expensive catalyst reduces CapEx requirements for both new systems as well as retrofits. Most of the cost of using SNCR is an operating expense. Reagent costs currently account for a large portion of the annual operating expenses associated with this technology. The annual cost of reagent purchase in $/yr is estimated using the reagent volumetric flow rate, the total operating time, and the unit cost of reagent. One of the bigger challenges with SNCRs is the stack emission of ammonia slip and this concentration is typically higher than SCRs due to larger injection rates. The temperature window at which SNCR typically operates plays a role in NH3 slip levels. Distribution of the reagent can be challenging especially in larger coal-fired boilers because of the long injection distance required to cover the relatively large cross-section of the boiler. Multiple layers are required to adjust to constantly changing boiler loads and this makes it challenging to fine-tune. Additionally, when urea is injected, large quantities of water are required, which can result in efficiency losses.\nDepending on the type of process, PEC Consulting can help in the selection of the best control technology to help reduce NOx emissions. We conduct an objective analysis to select the most feasible and efficient option to meet overall process and environmental requirements.\nSTEAM / Its Generation and Use, The Babcock & Wilcox Company, 42nd Edition 2. International Energy Administration, Clean Coal Centre, Clean Coal Technologies, “Low NOX Burners”, https://www.iea-coal.org/low-nox-burners/\nInternational Energy Administration, Clean Coal Centre, Clean Coal Technologies, “Selective Catalytic Reduction (SCR) For NOx Control”, https://www.iea coal.org/selective-catalytic-reduction-scr-for-nox-control/\nInternational Energy Administration, Clean Coal Centre, Clean Coal Technologies, “Selective Non-Catalytic Reduction (SNCR) For NOx Control”, https://www.iea coal.org/selective-non-catalytic-reduction-sncr-for-nox-control/\nChapter 2, Selective Catalytic Reduction,\nhttps://www3.epa.gov/ttn/ecas/docs/SCRCostManualchapter7thEdition_2016.pdf 6. Chapter 1, Selective Noncatalytic Reduction, https://www3.epa.gov/ttnecas1/models/SNCRCostManualchapter_Draftforpubliccomm ent-6-5-2015.pdf\nReeves, Claire E.; Penkett, Stuart A.; Bauguitte, Stephane; Law, Kathy S.; Evans, Mathew J.; Bandy, Brian J.; Monks, Paul S.; Edwards, Gavin D.; Phillips, Gavin (2002-12-11). “Potential for photochemical ozone formation in the troposphere over the North Atlantic as derived from aircraft observations during ACSOE”. Journal of Geophysical Research: Atmospheres. 107 (D23)\nDr. Prasanna Seshadri is an experienced engineer with strong background in process design, project engineering and management, technology and new product development for a wide range of energy conversion and environmental control processes used in power and other heavy manufacturing industries. He is Subject Matter Expert (SME) for solid fuel combustion, acid gas, and heavy metals emission control for thermal conversion systems and wastewater treatment. He has a BS in Chemical Engineering from the University of Madras, India, MS in Environmental Engineering and PhD in Energy Engineering from the University of North Dakota.']	['<urn:uuid:d61eb555-5360-4771-bb06-eedd73ac81f4>', '<urn:uuid:87540a4c-f429-4cc7-9f8c-4a4655c65286>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T19:58:41.790432	5	96	2866
51	I study marine ecosystem interactions and wonder how coral reefs contribute to ocean biodiversity, and what primary disease mechanisms affect their health?	Coral reefs, despite covering only 0.17% of Earth's surface, are among the planet's most biodiverse ecosystems. They serve as ideal environments for fish fry development and support 1-9 million species of vertebrates and invertebrates. About 4,000 species of fish and 800 coral species have been identified in reef systems. The reefs provide both food and shelter in the eternal battle for survival, with a single square kilometer capable of supplying approximately 15 tons of fish and other food annually. Regarding diseases, coral health is compromised through complex interactions between the coral host, pathogens, and reef environment. The most infectious coral diseases are caused by bacteria, with transmission facilitated in areas of high coral density. Disease outbreaks can reduce live coral cover, decrease colony density, and even trigger shifts from coral- to algal-dominated communities. Disease transmission can occur through coral predation, with predators acting as vectors through oral or fecal transmission of pathogens. Multiple factors exacerbate disease occurrence, including climate warming, land-based pollution, sedimentation, overfishing, and physical damage from recreational activities.	"['published on 10 February 2020 in ecosystems\nCoral turns pale!\nThe delicate balance of the coral reef\n“When looking at a coral reef, we are fascinated by the enormous variety of forms of animal and plant life that inhabit it. However, paradoxically for this life that is apparently inexhaustible, it is difficult to understand that everything depends on a fragile superficial layer of microscopic organisms, the madrepore polyps that multiply constantly, never stopping, indefatigably building their limy skeletons on which other madrepore polyps will grow, or where other living creatures will find food and shelter, in the eternal battle for survival”.\nAngelo Mojetta, marine biologist\nPrecious builders of biodiversity\nThe coral reef is one of the ecosystems with the richest biodiversity in the entire planet, even though it accounts for only 0.17% of the Earth’s surface. It is a widespread and impressive limestone formation of animal origin, characterized by thousands of colours and a number of different shapes. The organisms that are responsible for this complex biome, are the anthozoa madrepore, known as “building corals”.\nThe corals or madrepores, consist of small polyps of variable sizes (from few millimetres to a few centimetres), surrounded by a calcium carbonate cup, which is characterized by different shapes in the different species. Single cell dinoflagellate algae called “zooxanthellae” live inside each polyp, giving them a brownish-green colouring.\nThis particular association is called “mutualistic symbiosis”, which means that both species draw an advantage from living together. The algae in fact, due to chlorophyll photosynthesis, provide the polyp with energy in the form of sugars, they produce oxygen and eliminate carbon dioxide (which could form carbonic acid and damage the limey skeleton of the polyps). In exchange, the host polyp gives protection to the microscopic and numerous algae that live in it. Generally corals depend greatly on this symbiotic relationship, and receive up to 90% of their energy requirement through this process. Each square centimetre of madrepore can contain up to about one million zooxanthellae algae. Coral reefs are made of calcium carbonate (CaCO3) which the coral polyps use to build their supporting structure; the polyps absorb this substance from the sea and fix it in their outer skeleton. Coral formations develop mainly between the water surface and a depth of thirty metres.\nThree environmental conditions are required for their development:\n- mean temperature of the water in winter must always be above 20°C;\n- water salinity must remain constant;\n- the presence of a lot of light must be guaranteed. Only in these precise conditions, coral can grow and reproduce. Some species (e.g. brain coral) grow 5 to 25 millimetres a year, others (e.g. antler coral) grow more rapidly, up to 10-20 centimetres a year.\nThe coral reef is an ecosystem that is constantly growing, because new polyps grow on the old polyps that have died, so that the part on the surface is made up of live corals.\nA pale colouring? It’s because of the stress!\nBleaching is the term that is now commonly used to define the whitening of corals. In case of environmental stress (as for example a rise in temperature), the coral polyps expel the algae that live in symbiosis with the coral, the zooxanthellae, which are responsible for the colours of the corals, due to their photosynthetic pigmentation. Symbiont zooxanthellae are strongly pigmented, therefore when the algae are expelled the coral polyps appear transparent and only their white calcium carbonate skeleton is visible. The consequence of this phenomenon is a loss of colour, up to total bleaching of the coral colonies. At times, in some corals, the whitening has a bluish, yellow or pink colouring instead of bright white. This is due to the proteins produced in some species of corals, which colour the tissues, and become the dominant pigment when the zooxanthellae are not present.\nIf the cause of the stress lasts only few days, the coral will rapidly return to its normal state, if instead the condition continues for a prolonged period of time, the coral will die. It may also occur that once the whitening process begins, the polyps continue to expel the zooxanthellae even if the causes of the stress have been removed. In fact even if the coral becomes bleached, it may not be dead. If the conditions that determined the phenomenon cease, the algae will recolonize the polyps and the situation will go back to normal. If the stress lasts for many consecutive days, it will take a long time for the concentration of the symbionts to return to a normal condition, and the coral may even die. Coral that is unable to survive, starts being destroyed, specially by the action of the sea, and also by the sea creatures, as for example parrot fish that feed on corals. In a few weeks the coral gets crumbled in this way.\nFurthermore the number of coral colonies decreases significantly if bleaching causes the death of the coral before it has reached a reproductive age, for example acrophores need 4-5 years to reach maturity. Changes in the reproductive conditions are probably the effects that mostly condition the distribution and abundance of coral formations. The best known cause of coral bleaching is apparently the increase in sea temperature. An increase in the temperature, of even 1-2°C for 4-8 weeks, can lead to bleaching because corals already live close to the maximum thermal threshold. The increase in temperature has been the most evident cause, up to date, of bleaching, on a global scale. However, there are other causes that influence coral bleaching. In fact this phenomenon has been noted also in the presence of other factors: increased solar radiation that increases the temperature more than normal, strong UV radiations, changes in the chemical composition of the water (in particular the salinity level following, for example, heavy rainfall), or the water opacity, changes in the currents which, as a consequence of the accumulation of sediments, can deviate warmer water from the lagoon towards the reef, coral diseases, prolonged periods of coral exposure in the case of exceptional low tides, high sedimentation rate and tropical cyclones.\nWhy do high temperatures make corals whiten?\nFrom recent studies, it has been noted that an excessively high temperature leads to a collapse in the photosynthetic system of the zooxanthellae, which leads to an increase in the production of oxygen that damages the coral cell structures. Therefore, in order to avoid high concentrations of oxygen and consequently to avoid damaging the tissues, the coral polyps are forced to expel the zooxanthellae. Genetic variations in the various types of zooxanthellae influence the corals’ resistance to stress caused by the temperature, and consequently can influence coral bleaching.\nCoral bleaching in 1998\nIn 1998, coral bleaching reached catastrophic proportions due to the impact of El Niño (a phenomenon characterized by anomalous movements of water in the oceans) which caused an average increase of 2°C in the temperature, in some areas of the Indian Ocean, in particular in the Maldives. Over 90% of the creatures that were symbiotic with the algae, including the corals, was bleached, specially at depths from 8 to 20 metres and in particular in some areas of the North Malé Atoll in the Maldives.\nThe coral bleaching phenomenon was so evident that year, that people swimming near the coral reef in the Maldives would say it was like “walking on the snow clad Alps”. During that period, the surface temperature of the water was approximately 32°C, and this temperature could be recorded up to a depth of 15 metres. The concomitant lack of wind worsened the situation because there was no cooling effect on the water surface. In particular, the data that were collected pointed out that the coral branches in the first 10 metres below the surface recorded losses around 90% in 1998. The total coral cover which was initially about 40% dropped to 2-3%. It was if suddenly a forest fire had broken out in a millennial forest, the damage was not only ecological and biological with a decrease in the biodiversity, but also socio-economic, for all those populations whose survival is tied to the coral reef.\nThe Maldives and the consequences of coral bleaching\nThe sea bottom in the Maldives has the richest coral structures in the entire Indian Ocean, and these are known as atolls, i.e. coral formations that surround a central circle-shaped lagoon (the word “atoll” derives from the Maldivian word “atholu” which means “a ring of islands”). These madrepore structures are usually found in deep ocean waters near ancient submerged volcanic islands. And here we can find over 66 kinds and over 100 different species of madrepores, and here about one third of the coral fish of the entire Indian Ocean live. The reason for such abundance depends on the variety of environments in the Maldives. Near the reefs which rise 2000-3000 metres up to the surface, we find peaceful lagoons surrounded by pure white beaches. The reefs are broken by fissures which transform into tunnels and canyons and provide the ideal habitats which satisfy the requirements of all the species in the communities that live there.\nThe 1998 bleaching became very important for the Maldives. In fact, the population of the Maldives, which amounts to about 270,000 inhabitants, and the economy of the Country live mainly on tourism. The coral reef of the Maldives is one of the most beautiful in the world, and therefore the greatest tourist attraction of the Country. Because of this, the number of visitors in the last thirty years has increased almost exponentially and “coral tourism” contributes to 30% of the Gross National Product. In particular tourists from Italy account for 21.2% of the total, with about 130,000-140.000 tourists per year. Fortunately immediately after the effect of El Niño in 1998, recolonization of the reef started very soon and starting from the following year coral colonies could be observed, initially the encrusting corals and later the branched corals. After 1998, however, some species that were common in the Maldives have become rare.\nMany corals have different types of zooxanthellae in them, and can change the percentage of a species rather than another. The pigment of a coral, among its various functions, also acts as a protection for the zooxanthellae from excessive light. In many corals in the Western Pacific, there are fluorescent pigments which favour the environment for the zooxanthellae, as they concentrate light where it is lacking and protect the algae when there is excessive light. As a consequence, the corals with a high concentration of fluorescent pigment granules, are less vulnerable to bleaching in case of high temperatures. The capacity to survive the bleaching phenomenon varies among the different species of corals.\nSome massive corals such as the Porites lobata, can survive in the presence of high temperatures and many of the consequent effects, other corals are more sensitive, such as the Acropora spp. and do not survive the bleaching phenomenon. Recent studies have shown that the species that are subjected continuously to small stresses are able to develop a resistance to the bleaching phenomenon.\nCoral transplantation around the world\nIn order to save the coral reefs many scientists are carrying out coral transplantation procedures, in various areas around the world.\nIn Japan, for example, in the Sekisei lagoon an attempt is being made to make corals grow again by putting coral larvae on special ceramic discs. Scuba divers place the ceramic discs near coral formations that have survived, where the coral larvae will settle. The discs are then shifted to a protected area of the lagoon where the colonies can grow. This is a real nursery for the cultivation of coral, used specifically for transplantation, in order to prevent taking coral from the reefs, which would damage them. In this way scientists are trying to facilitate the natural reproduction of corals, offering an alternative shelter for the larvae to develop, which in nature takes place in the fissures in the rocks. When the organisms begin to grow, they are transplanted on the reef.\nIn Israel, instead, corals are collected from other coral barriers, they are then broken into pieces of about half a centimetre in diameter, and are then glued onto any sub-layer, where they grow for about a year, protected by an underwater net. Only at that stage the corals are transplanted onto the barrier which is lifeless.\nThe “baby corals” nursery in Israel has over 10,000 corals and various stages of growth and about 3,000 colonies have already been transplanted in the Red Sea and in other seas in Asia and in America.\nIn Australia, researchers collect corals that are ready for reproduction and take them to a laboratory, where the corals, at the opportune time, release their eggs and spermatozoa and the marine biologists cross the gametes in order to increase the genetic diversity of the larvae. The larvae are initially made to grow in large tanks, and subsequently on an artificial reef, and finally are transplanted onto the Australian coral reef.\nWhere, instead, the money for submarine research is scarce, as in the Philippines, unemployed fishermen led by marine biologists, detach pieces of healthy coral and shift them where the reef shows signs of suffering. Notwithstanding these different approaches in order to try to repopulate the coral barriers of the world, if the causes of the deterioration and death of the corals are not removed, the transplantation procedure will not be successful in bringing a final solution.\nThe wealth of the coral reefs\nThe “coral reef” biome has many functions. In fact, the reefs are the ideal environment for the birth and growth of fry, (young fish, before they become adults) that will form the populations of adult fish in the oceans all over the world. 20-25% of the fish fished in the developing countries (approximately 10 million tons a year) lives in the coral reefs. The populations of the Pacific fish 90% of their protein requirement in the reefs.\nIn Asia, the life of a billion people depends on the fish of the coral reefs. It has been calculated that, if correctly managed, a square kilometre of reef can supply approximately 15 tons of fish and other food per year. Furthermore, corals could be useful also in the field of medicine.\nThe first studies on corals in fact have shown that half of the new anti-tumour drugs could come from these sea organisms. Another important function of corals is to protect the coasts. The structure of the coral reef, in fact, decreases the violence of the waves and of the tropical hurricanes. Without this protection the coasts would be damaged and also the fish and prawn farms that are spreading in the tropical countries, would be destroyed. The true wealth of the coral reef, however, is biodiversity. Up to date, about 4000 species of fish and 800 corals have been classified, and it is calculated that 1 to 9 million species of vertebrates and invertebrates live and in some way exploit the reef. Today it is not yet possible to make an economic estimate of the naturalistic value of this ecosystem, however researchers are sure that the loss of species, which for the barrier have been calculated as a million species in the next 40 years, will have repercussions on the stability of the ecosystems and consequently on human life.\nEdited by Tiziana Bosco', ""Beyond threats associated with climate and ocean change, coral reefs are also affected by various local and regional threats. These threats may occur alone or synergistically with climate change adding to the risks to coral reef systems.\nOverfishing and Destructive Fishing\nUnsustainable fishing has been identified as the most pervasive of all local threats to coral reefs. ref Over 55% of the world’s reefs are threatened by overfishing and/or destructive fishing. Overfishing (i.e., catching more fish than the system can support) leads to declines in fish populations, ecosystem-wide impacts, and impacts on dependent human communities. Destructive fishing is associated with some types of fishing methods including dynamite, gill nets, and beach seines. These harm coral reefs not just through physical impacts but also through by-catch and mortality of non-target species including juveniles. Read more about threats and management strategies in the Reef Fisheries Toolkit.\nTraditionally, impacts from wastewater pollution have been associated with human health, but the detrimental effects of wastewater pollution on marine life – and the indirect impacts they have on people – cannot be overlooked. Wastewater transports pathogens, nutrients, contaminants, and solids into the ocean that can cause coral bleaching and disease and mortality for coral, fish, and shellfish. Wastewater pollution can also alter ocean temperature, pH, salinity, and oxygen levels disrupting biological processes and physical environments essential to marine life.\nOther sources of pollution to coral reef waters include land-based pollution associated with human activities such as agriculture, mining and coastal development leading to the discharge or leaching of harmful sediments, pollutants, and nutrients. Marine-based pollution associated with commercial, recreational, and passenger vessels can also threaten reefs by discharging contaminated bilge water, fuel, raw sewage, and solid waste, and by spreading invasive species. Learn more in the Wastewater Pollution Toolkit or in the Wastewater Pollution Online Course.\nMore than 2.5 billion people (40% of the world’s population) live within 100 km of the coast, ref adding increased pressure to coastal ecosystems. Coastal development linked to human settlements, industry, aquaculture, and infrastructure can cause severe impacts on nearshore ecosystems, particularly coral reefs. Coastal development impacts may be direct (e.g., land filling, dredging, and coral and sand mining for construction) or indirect (e.g., increased runoff of sediment, sewage, and pollutants).\nTourism and Recreational Impacts\nRecreational activities can harm coral reefs through:\n- Breakage of coral colonies and tissue damage with direct contact such as walking, touching, kicking, standing, or gear contact that often happen with SCUBA, snorkelling, and trampling\n- Breakage or overturning of coral colonies and tissue damage from negligent boat anchoring\n- Changes in marine life behavior from feeding or harassment by humans\n- Water pollution by tour boats through the discharge of fuel, human waste, and grey water\n- Invasive species which can be spread through transportation of ballast water, hull fouling of cruise ships, and fouling from recreational boating\n- Trash and debris deposited in the marine environment\nCoral disease is a naturally occurring process on reefs, but certain factors can exacerbate disease and cause outbreaks. Coral disease outbreaks can lead to an overall reduction in live coral cover and reduced colony density. In extreme cases, disease outbreaks can initiate community phase-shifts from coral- to algal-dominated communities. Coral diseases can also result in a restructuring of coral populations.\nDisease involves an interaction between the coral host, a pathogen, and the reef environment. Scientists are learning more about the causes of coral disease, especially in terms of identifying the pathogens involved. To date, the most infectious coral diseases are caused by bacteria. Transmission of coral diseases can be facilitated in areas of high coral cover ref as well as through coral predation, as predators can act as vectors by oral or fecal transmission of pathogens. ref\nThe causes of coral disease outbreaks are complex and not well understood, although research suggests that important drivers of coral disease include climate warming, land-based pollution, sedimentation, overfishing, and physical damage from recreational activities. ref\nOn coral reefs, marine invasive species include some algae, invertebrates, and fishes. Invasive species are species that are not native to a region. However, not all non-native species are invasive. Species become invasive if they cause ecological and/or economic harm by colonizing and becoming dominant in an ecosystem, due to the loss of natural controls on their populations (e.g., predators).\nPathways of introduction of marine invasive species include:\n- Ship traffic, such as ballast water and hull fouling\n- Aquaculture operations (shellfish aquaculture is responsible for the spread of marine invasive species through global transport of oyster shells or other shellfish for consumption)\n- Fishing gear and SCUBA gear (through transport when moving from place to place)\n- Accidental discharge from aquaria through pipes or intentional release\nSargassum are a type of brown, fleshy macroalgae that can have detrimental ecological and economic impacts on coral reefs when overabundant.\nIn the Indo-Pacific, high percent cover of Sargassum is common on degraded coral reefs and often represents a phase-shift from a coral to algae-dominated reef system. ref Their reproductive biology and morphology make them excellent colonizers of free space and particularly resilient to disturbances such as tropical storms. ref When overabundant, they can negatively impact the reef by shading, limiting space available for coral larvae to recruit, and transmitting pathogens. ref\nIn the Atlantic, two species of floating sargassum, S. natans and S. fluitans, are responsible for causing large mats of algae blooms which are particularly harmful and prevalent on the Caribbean and West African coastlines. ref Floating algae mats are naturally prevalent in the Northern Atlantic and provide many ecological benefits such as habitat, food, and nursery grounds to many species of fish, crustaceans and even sea turtles. ref However, in the last ten years, a shift in oceanic currents has led to an algae invasion in coral reef areas, causing reduced sunlight required by corals and anoxic and hypoxic conditions on reefs, as well as poor conditions on beaches that are detrimental to the tourism industry. ref\nCoral predators (or 'corallivores') are naturally occurring organisms that feed on corals for their polyps, tissue, mucus, or a combination of the above. Such predators typically include echinoderms (starfish, sea urchins), mollusks (snails), and some fish.\nCorallivory is a common process that, under normal conditions, allows for natural turnover in the ecosystem. However, when these predators are overly abundant (e.g., outbreak conditions), they can cause significant declines in coral cover.\nCommon coral predators include:\n- Crown-of-Thorns starfish (COTS), which are found throughout the Indo-Pacific region, occurring from the Red Sea and coast of East Africa, across the Pacific and Indian Oceans, to the west coast of Central America. COTS can be a major driver of coral loss in the Indo-Pacific, particularly under outbreak conditions.\n- Drupella snails, which are commonly found living on corals in reefs throughout the Indo-Pacific and Western Indian Ocean.\n- Coralliophila snails, which are often more problematic for Caribbean reefs, although some species are prevalent in the Pacific.""]"	['<urn:uuid:b92526b8-c207-4287-b266-bf39c0fdf961>', '<urn:uuid:6dd87468-19aa-450b-b9cc-46d264a1ee33>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T19:58:41.790432	22	170	3716
52	local government support minority business development achievements	Local governments act as first-responders to community needs by implementing social equity services and working closely with communities. In practical application, organizations like The Enterprise Center in Philadelphia are helping create opportunities for minority-owned businesses, recently enabling one such business to secure university contracts and double their revenue. Additionally, local governments are improving planning processes to include historically marginalized groups' perspectives and enhance accountability through performance measures.	"['Smart Ideas: Revisited\nFeb 13, 2023 // By:aebi // No Comment\nSocial equity solutions are developed to resolve fairness in public contexts and to provide support to low-income communities and individuals. They can consist of programs that use business resources and also support for entrepreneurs, as well as programs that concentrate on substance abuse, assimilation, and recovery. In order to resolve social equity, local governments require to enhance preparation and also decision-making procedures. This needs an intersectionality perspective, that includes the views of historically marginalized teams. An area’s efforts to build equity should also consider the neighborhood landscape. Including regional data can assist to make sure that the options and projects established work. The Pew Research Center’s survey of 10,000 grownups discovered that 34% of Americans think that more can be done to accomplish equity. It additionally found that several groups are frequently left out from the political procedure. Although systems are biased, adjustment is possible within existing frameworks. Nonetheless, the challenge continues to be to much better advertise liability through performance measures. City governments have an unique role in dealing with social equity due to the fact that they are generally the very first -responders to neighborhood needs. By functioning closely with the area, they can build partnerships that are withstanding. Public managers should comprehend that they have a duty to offer every one of the neighborhood, regardless of what external elements may be limiting their capability to do so. Raising access to social equity services will enhance the quality of life in the community as well as allow all citizens to gain from boosted financial growth. Social equity solutions should be integrated right into all aspects of civil service shipment. This indicates that community leaders should ensure equitable interaction when drafting laws, regulations, as well as various other plans. These policies must likewise resolve the voices of robbed communities and also underserved teams. Purchasing these approaches will make certain that the neighborhood’s goals are fulfilled. Panelists that participated in the “Social Equity in the Cities” panel discussion shared their experiences in working toward more equitable plans and techniques. While some panelists talked about the relevance of integrating neighborhood viewpoints in new initiatives, others concentrated on the worth of utilizing historical information to educate choices. No matter the method, they all agreed that social equity is very important as well as need to belong of neighborhood preparation. One means to do this is to apply Expungement Campaigns, which assist people who have actually committed crimes via a structured process. Another strategy is to deal with organizations that concentrate on minority-owned organizations. Giving financial aid as well as economic resources to little companies will certainly enhance opportunities for individuals in disadvantaged areas. For city governments, enhancing their ability to supply social equity services is a leading priority. Los Angeles Mayor Cory Booker is functioning to boost the city’s Social Equity Program and also has passed regulations to do so. His goal is to make Los Angeles a leader in this area. He is working with nonprofits as well as organizations to increase funding for this program. There are a number of resources readily available to support social equity campaigns, consisting of the Federal government Alliance on Race and Equity, a nationwide network of federal governments. Additionally, city government managers must be aware of resources offered to them at the state and government levels.', ""Entrepreneurship is hard. Minority entrepreneurship is harder. The 9th annual Regional Affinity Incubation Network (RAIN) Conference tackled the quest to build a more diverse and inclusive entrepreneurial community.\n“From startups, to growing companies and industry leaders, there is an opportunity for all of us to build access and inclusion into the foundations of our region’s startup community infrastructure,” said University City Science Center President and CEO Stephen S. Tang as he opened the conference in Quorum, the Science Center’s clubhouse for entrepreneurs. “Let’s seize that opportunity and help Greater Philadelphia and its citizens compete even more effectively on a national and global scale.”\nRAIN convenes the region’s innovation and entrepreneurship community, and explores ways to make the startup community more vibrant, connected and accessible. The Science Center hosts the event each July.\nA highlight of this year’s incarnation was a lively panel discussion on “Leveling the Playing Field” featuring Dan Rhoton, executive director of Hopeworks ‘N Camden; Gabrielle Wanamaker, vice president of Business Programs at The Enterprise Center in West Philadelphia; Tariq Hook of Zipcode Wilmington; and Alessandra Brown of the Roxbury Innovation Center in Massachusetts. The session was moderated by Lori Reiner of EisnerAmper, the conference’s presenting sponsor.\nWe caught up with Rhoton and Wanamaker to dive deeper into what it takes to foster a diverse, dynamic business environment.\nKeystone Edge: Can you elaborate on the theme of the conference? How big of an issue is “leveling the playing field” to promote inclusion among entrepreneurs and startups?\nGabrielle Wanamaker: Small businesses play a very important role in the local and national economy — they account for more than half of all jobs and sales in the U.S. Slightly less than half of businesses in Philadelphia are minority owned. However, revenues for many minority businesses are proportionally lower. For instance, average sales for an African-American business in Philadelphia is $62,000. There certainly are minority-owned business that have millions in sales, but it is a small percentage.\nNationally, African Americans, Asians and Latinos have shared buying power of approximately $3.4 trillion, which is larger than the GDP of many countries. In the African-American community, a dollar turns one time, meaning that a dollar is spent once in the black community before it leaves and is spent elsewhere. Often those dollars go to make purchases that support the revenue growth of larger businesses. When those organizations look for suppliers, many overlook minority businesses to supply goods and services. Others may want to buy locally from diverse suppliers, but are not sure where to look for them.\nThe market is there and the need is there. Hopeworks has found that employers are quite willing to employ young people, if we prepare those young people for work.Dan Rhoton, Hopeworks 'N Camden\nDan Rhoton: Leveling the playing field is no longer an option. Not only are there not enough “traditional” candidates to meet the demand, but there are larger issues. The youth Hopeworks works with – often struggling with issues of trauma, poverty and adversity – have the potential to either dramatically increase society’s bottom line with their expertise and knowledge, or cost our society a tremendous amount of money through social services, incarceration and other costs.\nCan you tell me more about the specifics you observe in Philadelphia and Camden? What are the available resources/opportunities/challenges in each of those cities?\nGW: In a study of the best places for black-owned business in the U.S., Philadelphia and Camden combined ranked 56th. Many of our businesses are sole proprietorships. The Enterprise Center is working to help businesses create solid infrastructures that are prepared for growth. We work hard to help create new opportunities for high-potential minority-owned business. Philadelphia has several organizations that support small businesses. The landscape for entrepreneurs in Camden is changing with the addition of Waterfront Labs [an innovation hub and community center].\nGetting capital is a challenge for minority-owned businesses. Finanta [a Philadelphia nonprofit lending institution] and The Enterprise Center are focused on [that]. We find that new ventures require “patient” capital to grow and few of those patient dollars go to women and minorities. We now have the ability to provide patient capital in the form of equity to a select number of businesses. A local venture capital group reached out and shared that they are interested in broadening their portfolio to include more women and people of color. That was a pleasant surprise.\nRecently, we helped a women/minority-owned business land a contract to supply several universities with promotional items. That contract will nearly double her revenues to $500,000 this year; The Enterprise Center’s Capital Corporation lent her $100,000 to provide the capital she needed to fulfill the new contract.\nThe Enterprise Center is working to help businesses create solid infrastructures that are prepared for growth. We work hard to help create new opportunities for high-potential minority-owned business.Gabrielle Wanamaker, The Enterprise Center\nDR: The market is there and the need is there. Hopeworks has found that employers are quite willing to employ young people, if we prepare those young people for work. Our youth need the technical skills, but that is the least important part of what we do. The truly important part of what we do is the trauma-informed emotional and social skill-building that allows our youth to not just survive, but thrive in any environment.\nWhat do economic development officials, incubators, educators, investors, etc. need to do to insure a more inclusive entrepreneurial ecosystem?\nGW: It helps to step out of your bubble to see what others are doing and to be intentional about being inclusive in your exploration. In San Francisco and Cincinnati, large organizations provide prime space to Minority Business Enterprises (MBE) to get their goods out to a larger audience. We have a local developer who is providing retail space to an MBE in his latest new apartment [building] to support the entrepreneur and the local community.\nDR: The best way to create diversity is to get rid of unpaid internships. The moment you set up an unpaid opportunity, you immediately exclude most of the population that has responsibilities, kids or other obligations. If you truly want diversity in tech, pay your interns.\nWhat do you think the coming years will bring?\nGW: I am seeing more interest from major organizations in the Philadelphia region who want to buy local and make an investment in the entrepreneurial ecosystem. They are broadening their scope to include diverse entrepreneurs as their suppliers.\nDR: With new technology, innovation and transformation are no longer reserved to those with capital. This is an exciting change for many underserved communities to create technologies around their needs.\nELISE VIDER is news editor of Keystone Edge.\nWRITER IN RESIDENCE is a partnership between the University City Science Center, Keystone Edge and Flying Kite Media that embeds a reporter on-site at Quorum, the Science Center’s clubhouse for entrepreneurs at 3711 Market Street. The resulting coverage will provide an inside look at the most intriguing companies, discoveries and technological innovations coming out of this essential Philadelphia institution.""]"	['<urn:uuid:877e9e23-63d4-4de6-ba86-2e8f3409386b>', '<urn:uuid:9d15eb11-51cf-4e0e-a381-b1b3ddd29361>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T19:58:41.790432	7	67	1733
53	davis wing design performance military applications	The Davis wing design demonstrated superior performance in wind tunnel tests at GALCIT, showing very low friction even at high angles of attack and outperforming the standard NACA model. This wing design was implemented in the B-24 Liberator bomber and its naval variant, the PB4Y-2 Privateer. The 110-foot long, twin spar Davis High Lift wing with massive Fowler flaps was key to the aircraft's long range and relatively high speed, allowing it to stay in squadron service longer than rival aircraft. The wing design improved long distance flying by approximately 10% and achieved equal fuel savings on regular runs. The success of this design led to the production of over 18,500 B-24 Liberators and 739 Privateers, which saw extensive service in both European and Pacific theaters during WWII.	"['In 1937 the well known first licensed pilot of America, Walter Brookins, introduced our inventor David Davis to Reuben Fleet, the founder/owner of the Consolidated Aircraft Corporation in San Diego, California.\nConsolidated was a well established manufacturer of long range flying boats for the American Navy, such as the PBY-Catalina and PB2Y-Coronado. The engineering department under Isaac ‘Mac’ Laddon was working at that time on a successor for the Catalina, which would be called (until the intended procurement by the Navy), the Model 31 Corregidor. There were also negotiations going on regarding production licenses for the Boeing B-17, the heavy four-engined Army Air Corps bomber.\nEnter Davis, introduced by Brookins to the top of the firm as a technical wizard with special interest in long range flight. For the last several years he had been working on wing design, with special attention to the cross sectional shapes of wings. He claimed to have developed better wings than NACA, the official federal aviation research authority. To prove it he presented his hosts with his Patent 1,942,688 “FLUID FOIL” of Jan.9.1934.\nThe Consolidated executives received their guests politely and, after the introduction, proceeded to look somewhat bewildered at the presented patent. It showed numerous unwieldy pairs of formulas apparently describing the form of the upper surface of a wing section (suction side) and of the under surface of the same wing section (pressure side). It should be noted that the formulas contained multiple appearances of two parameters, A and B, whose actual values were left unspecified. Written in this way, the paired formulas did not describe a single wing section, but a whole family of sections.\nOn page 2 of the patent a single example of calculation was given, where A had received the value: A=0.717257 and B=0.208228. When these values were substituted in the formulas a smooth looking wing shape resulted as shown in Fig.1 on page 0 of the patent and reproduced here at the top of this article.\nPage 3 of the patent (see picture below) left the technical staff of Consolidated even more bewildered.\nLaddon: Where on earth did you find these formulae?\nDavis: I derived them from theory.\nLaddon: Theory? Aerodynamic?\nDavis: Yes, hydrodynamic, the Magnus force on a rotor moving in an air stream. But the derivation is not part of the patent.\nLaddon: I see.\nThey then had a long discussion about the required values for the A and B parameters. The Davis’ patent stated that A influenced the lift force delivered by the wing at a certain angle of attack and B the fluid resistance (drag). Consolidated required, for long range cruise flight, as low an air drag as possible at a relatively high angle of attack. What values of A and B would be best?\nDavis promised to disclose the desired parameters if Consolidated agreed to use his wing section for actual production aircraft and pay him a certain royalty per airplane sold.\nI am not informed how much time these discussions took, but the evidence is there that the negotiations were intense. Davis must have mentioned that he could show certain results from model experiments. However, I am not sure he told the other party how he had been able to obtain these test results.\nTo settle the uncertainties, the two parties decided to send one model of a Davis airfoil in addition to a standard NACA model of Consolidated’s choice to the so-called ’10 foot wind tunnel’ of the Guggenheim Aeronautical Lab at Caltech in Pasadena to let an unbiased party make the comparison. Both wind tunnel models were for a wing of the same planform en depth. Davis personally fabricated his sample to a high degree of surface perfection (with shape parameters A=1 and B=-1). The Consolidated entry followed the standard specs for a NACA 21-series wing section.\nThe test results confounded GALCIT professor Clark Milligan to a high degree. The Davis section showed very low friction even at high angles of attack. Even after re-finishing Consolidated’s NACA model to the same surface specs, the Davis section turned out to be superior. A total of three trials gave near identical results.\nEverybody finally agreed that the Davis design was not a bad choice at all. The engineers caught the spirit of things and said: why not use it for the wing of the Corrigedor that we are building now? So they did and although the big boat looked according to some like a ‘pregnant guppy’ it flew like a lark (see my previous blog on this subject).\nMeanwhile, Major Fleet communicated with his network at the Air Corps and he got the approval to build a prototype heavy bomber with the new wing. The wing was amazingly slender and prolonged long distance flying by some 10% and saved fuel on regular runs to an equal amount.\nThe prototype was designed and built in nine months and flew one day before the contract stipulated, on December 29, 1939.\nThe B-24 Liberator and its many variants turned out to be a big success for Consolidated Aircraft and for David R. Davis. According to Wiki a total of 18,500 were constructed. Their war record in Europe and in the Pacific theater was most impressive.\nDavid Davis received his royalties, but his name was never mentioned, not by Galcit, not by academia in general, except for Dr. Vincenti, who wrote the very well researched definitive account on which this blog is based.\nNACA did not waste time in developing its own competing wing profile that was used in the North American P-51 long distance fighter.\nSources of Illustrations:', '| Consolidated PB4Y-2 Privateer\nby Dennis Baer\nHaving belatedly, come to the conclusion that sea plane hulls were not compatible with efficient airframe aerodynamics, the U.S. Navy added land based patrol aircraft to their air arm in early WWII. Before the war, the Army Air Corp had set Navy brassís teeth on edge by using their early Boeing B-17 bombers to demonstrate long range interception of inbound surface ships. The Navy had countered this threat to their monopoly on defending America\'s coasts by getting the War Department to ban any maritime patrols by Army bombers that were more than 100 miles from shore. This bit of foolishness was instantly forgotten when Nazi submarines began sinking Allied trans-Atlantic shipping in record numbers.\nBeing late to the party, the Navy had to accept Consolidated B-24 Liberators as patrol planes. The sleeker, easier to fly B-17s were all reserved for Army Air Corp use. Eager to reduce crew fatigue on long patrols, Consolidated was instructed to allocate three B-24s for conversion to Navy requirements. To ease control problems, Consolidated had already attempted to change the B-24\'s twin rudders to a single vertical fin. The first conversion, the XB-24K, done at the Consolidated San Diego plant used the tail from a Douglas B-23 Dragon. The results were unsatisfactory.\n|The Ford Motor Company, which operated a huge B-24 license-production plant called Willow Run in Michigan, then transplanted a vertical fin from a C-54 transport onto one of their B-24s, creating the XB-24N. They also added a new custom built horizontal fin and replacing the nose turret with an ERCO 250 SH ball turret, which was more streamlined than the standard nose turret of production B-24s. It was superior enough to the standard B-24J that the Army ordered seven more pre-production YB-24Ns and inked an order for 5,168 B-24N bombers. Horrified that a mere car manufacturer might have upstaged them, Consolidated pulled strings to have this order canceled. Equally horrified that they might be forced to accept some of these planes instead of getting their own design, the Navy helped to kill the N-model.|\nChanges from the B-24 included a single vertical fin and replacing the nose turret which was more streamlined than standard models. The forward fuselage was lengthened and there were changes in armament. The Privateer used the same wing and landing gear as the B-24.\nMeanwhile, new fuselages were constructed for the Navy PB4Y-2s in San Diego. For the second time, the basic Liberator fuselage was lengthened (legendary founder of Consolidated, Ruben Fleet personally ordered the first B-24C and all subsequent Liberators to have the nose section lengthened 2 feet, 7 inches at company expense ""to improve the appearance."" He could have saved the effort; it was still a homely plane). The new fuselage was now a full 74 ft, 7 inches, 10 feet 5 inches longer than the first B-24A. The space was devoted to new electronics and a flight engineerís station. The first three Privateers (the initial name ""Sea-Liberator"" had been quickly dropped) flew with standard twin tails. When the new single tail was ready, it caused the Privateer to tower a hanger-roof-scraping 29 feet 1 5/8 inches off the floor.\nA second dorsal turret was fitted with twin .50 caliber machine guns, as well as a pair of ERCO 250 THE turrets which replaced the two flexible, hand held guns of the Liberator\'s waist gunners. The Navy, in it\'s infinite wisdom, noted that the teardrop shaped waist turrets could be depressed downward to such a degree that the fields of fire from these guns converged at a point 30 feet below the belly of the plane. For this reason, no belly turret was fitted. How the waist gunners were suppose to stand in their turret blister and see to aim their guns under their feet without falling over was not explained.\nThe trademark of the Liberator family, the 110 foot long, twin spar Davis High Lift wing with it\'s massive Fowler flaps was unchanged. This was the secret to the long range and relatively high speed of the Liberator/Privateer family, and the reason the Privateer stayed on in squadron service long after the rival B-17 had been retired.\nFour Pratt & Whitney R-1830-94 engines were retained although they were fitted with only a single stage supercharger. As low altitude patrol planes, the two stage supercharger was considered unnecessary. The oil cooler air inlets were moved to above and below the engine, eliminating the characteristic horizontal oval appearance of the engine cowl. Military and T.O. power was rated at 1,350 hp (1,000 kW) each. Later, when some survivors of Navy and Coast Guard service were modified for use as fire bombers, they were fitted with 1,700 hp (1,270 kW) Wright R-2600 engines.\nThe two distinguishing features between the B-24 and the Privateer were the tail configurations and the engine cowlings. Cowling configurations for the oil coolers differed depending on the engine installation.\nTwelve .50 caliber Colt-Browning M2 machine guns were carried, all in powered turrets. This was sometimes augmented with one or two 20 mm nose cannons for strafing. Normal bomb load for a 1,300 mile radius patrol was 4,000 pounds of bombs, depth charges or mines. Some planes were also armed with one or two ASM-N-2 Bat anti-shipping glide bombs which was first used successfully during a raid against Japanese shipping in Valikpapan Harbor, Borneo in April of 1945.\nOperationally, the classic use of patrol bombers is to hunt down and sink enemy ships. The Privateer stepped easily into this role, the way having been paved by years of anti-submarine and anti-ship operations in Navy PB4Y-1 Liberators and USAAF B-24s equipped with a series of radar sets collectively known as ""Low Altitude Bombing"" sets. By WWII standards, the Privateer was lavishly equipped with an electronic suite that could be customized on a mix and match basis so that Privateers could be airborne communication platforms, radar and radio station hunter/killers, anti shipping search and destroy units, weather reconnaissance planes or search and rescue units to locate downed airmen with their radio direction finders. If the situation demanded, they could even act as their own standoff anti radar jamming unit.\nPatrol craft are not glamorous, like fighter planes, or vital to the troops on the ground like bombers, close support attack planes, or the cargo planes that keep them supplied. What the Privateer lacked in pizzazz, it more than made up for in versatility and practicality. The Navy wanted the seas swept clear of enemy transport, enemy radars, enemy radio navigation aids, and enemy scouting vessels. It wanted mines planted, submarines harassed or destroyed, communications augmented and weather information for 1,300 miles around the Privateer\'s base. No other aircraft was as capable of this as the Privateer .\nThe last Privateer was delivered in October of 1945. Of 739 airframes built, there may be two in flying condition today.\nAfter WWII, Privateers were used as hurricane hunters and played a large role in Reserve squadrons, helping to keep up training for thousands of Naval Reservists. In 1950, numerous mothballed Privateers were recalled for service in Korea, where their air-to-surface radar was used to hunt down and destroy North Korean infiltrators along the coasts. They also flew dangerous nighttime Firefly missions to drop flares over embattled United Nations troops so that air support could continue around the clock.\nThe US Coast Guard removed the side and nose turrets and rebuilt the nose of the plane with a huge glazed observation dome.\nOther Cold War missions included the theoretical deliver of nuclear weapons by Naval aircraft and an unspecified number of PB4Y-2s were modified to deliver second generation atomic bombs. Other airframes were optimized for Ferret missions to gather electronic intelligence. These are high value/high risk missions, often employing a long series of uncomfortable, tension filled runs aimed at approaching or penetrating enemy territory to eavesdrop on enemy radar signals or radio traffic. Most valuable for intelligence analysts is the air-to-ground and air-to-air communication between interceptor controllers and interceptor pilots. The easiest way to get this chatter is to provoke the enemy air defenses into launching an interception of your own aircraft. The hard part is getting away alive after they do. On April 8th, 1950 a Privateer from VP-26 was caught by Soviet MiGs over the Baltic Sea and destroyed with all crewmembers either killed in the crash or strafed to death. Apparently a number of the 38 Privateers seconded to the Nationalist Chinese Air Force suffered similar fates at the hands of the Red Chinese People\'s Army Air Force.\nOther post war service included 22 Privateers provided to Aeronautique Navale for service with the French colonial forces in Vietnam. They were used as bombers until after Dien Bien Phu, with four lost in combat. Six were returned to U.S. service, the remaining twelve were flown to North Africa where they fought in Algeria and later during the Suez Incident. In 1961, the survivors were scrapped in favor of new Lockheed P2V Neptunes, a fate shared by most other Privateers.\nThe longest serving Navy Privateers were expended as a radio control target drones off Point Magu, California in the early 1960\'s. The last, flying under the call sign Opposite 31 and carrying the ironic nickname Lucky Pierre was shot down by an Bullpup missile with an experimental proximity fuse that turned the Bullpup into an air-to-air weapon. The relatively large warhead detonated over the back of the Privateer\'s wing, sparking a spectacular fireball of burning fuel and marking the end of the Privateer\'s military career.\nA small number of ex-US Coast Guard search and rescue Privateers survived to become anti-forest fire water bombers. The Coasties had removed the side and nose turrets and rebuilt the nose of the plane with a huge glazed observation dome. After being sold off as surplus, civilian operators gave them new Wright R-2600 engines along with the installation of their borate/water slurry tanks. These were Super Privateers"" and these planes soldiered on into the 1990\'s.\nA modified Consolidated PB4Y was donated to the Yankee Air Force Museum in Ypsilanti, Michigan. The starboard turret is missing but Yankee Air Force Museum hopes to replicate the port side. |\n|One of these planes (call sign ""Tanker 125"") was donated to the Yankee Air Force Museum in Ypsilanti, Michigan. You will note from the rear view that only one ERCO 250 THE turret is installed. Unfortunately, only this portside turret was available, the starboard one having been scrapped. Eventually, the museum hopes to build a ""mirror image"" starboard turret and fit it to this static display plane.|\n|Consolidated PB4Y-2 Privateer|\n|Wing span:||110 ft 0 in (33.53 m)|\n|Length:||74 ft 7 in (22.73 m)|\n|Height:||30 ft 1 in (9.17 m)|\n|Empty:||37,485 lb (17,003 kg)|\n|Max Gross:||65,000 lb (29,484 kg)|\n|Maximum Speed:||237 mph (381 km/h) at 13,750 ft (4,190 m)|\n|Cruise:||140 mph (225 km/h)|\n|Service Ceiling:||20,700 ft (6,310 m)|\n|Range:||2,800 miles (4,506 km)|\nFour, Pratt & Whitney R-1830-94 Twin Wasp |\n1,350 hp (1,007 kW), 14-cylinder radial, air cooled engines.\nTwelve 0.50 machine guns (nose, turrets, waist and tail positions)|\nBomb load 12,800 lb (5,806 kg).\nReturn To Aircraft Index.\n©Dennis Baer The Aviation History Online Museum.\nAll rights reserved.\nCreated March 3, 2008. Updated January 14, 2014.']"	['<urn:uuid:07a2bfc9-7c35-477d-b195-98c32c8615e6>', '<urn:uuid:d932787e-5a89-4e6a-a086-d0b242100061>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T19:58:41.790432	6	128	2809
54	What aircraft will first carry the Navy's new MS-177A sensor?	The MS-177A long-range multi-spectral imaging sensor will be installed and tested on a Navy flight test P-3C Orion aircraft, marking the first time this sensor has been flown by the Navy.	['- Sensor to be installed onboard flight test P-3C Orion aircraft under ONR contract\n- Experiment will mark the first time Collins Aerospace’s MS-177A has been flown by the Navy\n- Sensor provides better spectral and spatial image resolution at longer ranges and greater coverage area per hour than other airborne ISR sensors in the U.S. Navy inventory, expanding ASW, ASuW and MIW mission capabilities.\nWASHINGTON, D.C. (Sept. 14, 2020) – Collins Aerospace Systems, a unit of Raytheon Technologies Corp. (NYSE: RTX), has been awarded a $19.9 million contract by the U.S. Navy’s Office of Naval Research (ONR) to conduct a maritime experiment of the company’s MS-177A long-range multi-spectral imaging sensor on a Navy flight test P-3C Orion aircraft. The experiment will mark the first time Collins Aerospace’s MS-177A has been flown by the Navy. The previous version of the sensor, the MS-177, has completed flight testing on the U.S. Air Force (USAF) Global Hawk and will soon go operational.\nOver the course of the 30-month contract period, Collins Aerospace will fabricate an MS-177A sensor and supporting flight test hardware from its existing USAF production line and install it on the P-3C aircraft. The follow-on phase will encompass experimental flights in a maritime threat environment. Results of the experiment will demonstrate the MS-177A’s ability to expand the Navy’s maritime ISR capabilities in the Anti-Submarine Warfare (ASW), Anti-Surface Warfare (ASuW) and Mine Warfare (MIW) mission domains using a mature USAF sensor system.\n“When used in conjunction with other spectrum sensors, our proven MS-177 family of systems can greatly increase the probability of detection for threats, while operating in both permissive and contested environments,” said Kevin Raftery, vice president and general manager, ISR and Space Solutions for Collins Aerospace.\nThe MS-177A system employs design elements of Collins Aerospace’s fielded Senior Year Electro-optical Reconnaissance System (SYERS) sensor flown on the U-2 to deliver high geo-location accuracy, collecting imagery in the Visible, Near-IR, SWIR and MWIR spectral channels resulting in advanced terrestrial and maritime mission capabilities. The MS-177A sensor’s Field of View (FoV) and spectral and spatial resolution offer unmatched high-resolution, multi-spectral, high coverage rate airborne intelligence, surveillance and reconnaissance (ISR) capability to the U.S. military. Its long-range performance allows the host platform to operate in contested as well as permissive environments on both land and sea, day or night.\nAbout Collins Aerospace\nCollins Aerospace Systems is a leader in technologically advanced and intelligent solutions for the global aerospace and defense industry. Collins Aerospace has the capabilities, comprehensive portfolio and expertise to solve customers’ toughest challenges and to meet the demands of a rapidly evolving global market. With 2019 net sales of approximately $26 billion, the business has 78,000 employees across more than 300 locations globally. It is one of the four businesses that form Raytheon Technologies.\nAbout Raytheon Technologies\nRaytheon Technologies Corporation is an aerospace and defense company that provides advanced systems and services for commercial, military and government customers worldwide. With 195,000 employees and four industry-leading businesses ― Collins Aerospace Systems, Pratt & Whitney, Raytheon Intelligence & Space and Raytheon Missiles & Defense ― the company delivers solutions that push the boundaries in avionics, cybersecurity, directed energy, electric propulsion, hypersonics, and quantum physics. The company, formed in 2020 through the combination of Raytheon Company and the United Technologies Corporation aerospace businesses, is headquartered in Waltham, Massachusetts.']	['<urn:uuid:8e2c4e14-9ce7-49a2-95e2-d22b7d5be4ce>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	10	31	554
55	As an audiologist, I'm particularly interested in understanding how tinnitus manifests sonically and what treatment options exist - could you explain the different types of sounds patients experience and the main approaches to managing them?	Tinnitus can manifest through various sounds including static, high-pitch whistles, buzzing (similar to insects), ringing (high-pitched whine), whooshing (from blood circulation), screeching (like grinding metal), electric motor sounds, and roaring (like ocean waves). These sounds can change over time, with patients potentially experiencing multiple different sounds. As for treatment approaches, there are two main strategies: masking the noise and helping the brain dismiss it. Treatment options include sound therapy through various means like table-top devices, CD-based systems, hearing aids, or combination devices. Additionally, counseling is crucial for management, as it helps address the anxiety and fear associated with tinnitus. While there's no specific cure or universally effective treatment, other options include biofeedback (with 70-90% success rate), medications (though limited by side effects), and various sound-based interventions like neuromonics or cochlear implants.	['Most individuals refer to tinnitus as a buzzing or ringing sound. But that classification, though useful, is dismally insufficient. Tinnitus doesn’t always manifest in one of those two ways. In fact, a large range of sounds can be heard as a result of this condition. And that’s a significant fact.\nBecause, as useful as that “ringing and buzzing” shorthand may be, such a limited definition could make it challenging for some people to recognize their tinnitus symptoms. If Barb from down the street hears only whooshing or crashing in her ears, it might not even occur to her that tinnitus is responsible. So everyone, including Barb, will profit from having a better idea of what tinnitus can sound like.\nTinnitus Might Cause You to Hear These Noises\nTinnitus is, generally, the sense of noises in your ears. Sometimes, this noise really exists (this is called objective tinnitus). And at other times, it can be phantom sounds in your ears (which means that the noises can’t be heard by others and don’t really exist – that’s called subjective tinnitus). The type of tinnitus you’re dealing with will most likely (but not always) have an impact on the noise you hear. And you could potentially hear a lot of different noises:\n- Static: The sound of static is another kind of tinnitus noise. Whether that’s high energy or low energy static varies from person to person.\n- High-pitch whistle: Think about that sound your tea kettle makes when it starts boiling? That specific high pitched squealing is sometimes heard by those who have tinnitus. This one is undoubtedly quite unpleasant.\n- Buzzing: In some cases, it’s not ringing you hear, but a buzzing noise. This buzzing sometimes even sounds like an insect or cicada.\n- Ringing: A ringing in the ears is the most common of the tinnitus noises. Frequently, this is a high pitched whine or ring. The ringing is frequently called a “tone”. Ringing is probably what most people think about when they contemplate tinnitus.\n- Whooshing: Some people hear a whooshing noise caused by blood circulation in and around the ears which is a form of “objective tinnitus”. With this kind of tinnitus, you’re basically hearing your own heartbeat.\n- Screeching: You know that sound of grinding metal? Maybe you hear it when someone who lives near you is working on a building project in their back yard. But it’s the type of sound that often manifests when someone is experiencing tinnitus.\n- Electric motor: Your vacuum has a very distinct sound, mostly due to its electric motor. Some individuals with tinnitus hear a similar noise when their tinnitus flares up.\n- Roaring: The noise of roaring ocean waves is another typical tinnitus sound. It may sound calming at first, but the truth is that the noise is much more overpowering than the gently rolling waves you might imagine.\nSomeone who is suffering from tinnitus might hear many possible noises and this list is hardly complete.\nChange Over Time\nSomeone with tinnitus can also experience more than one sound. Brandon, for instance, spent the majority of last week hearing a ringing sound. Now, after going out to a loud restaurant with friends, he hears a static sound. Tinnitus sounds can and do change, sometimes frequently.\nIt’s not well known why this happens (that’s because we still don’t really know what the underlying causes of tinnitus are).\nCanceling Out Tinnitus\nTinnitus treatments will typically take two possible strategies: masking the noise or helping your brain figure out how to dismiss the noise. And in either case, that means helping you identify and familiarize yourself with the sounds of your tinnitus, whatever they may be.', 'There is no cure and no specific treatment is efficacious for noise/ringing in the ears/head; however person can opt from many options available. In some cases it can be quieted with treatment of underlying cause, thus thorough otologic evaluation and investigation is to be done. In majority of sufferers there is no serious pathology and reassurance is all what is needed.\nThere is no specific drug designed specifically for tinnitus. There is no active research going on because of high cost and high rate of failure involved in creating a new drug.\nAnxiolytics, Carbamazepine, Lidocaine, and intravenous Barbiturates show encouraging results but potentially serious side effects limit usefulness. Relatively low doses may bee effective in tinnitus management.\nPatient and clinician together determine right drug considering side effects if it is worth.\nPerson can be habituated to sound in the environment, so aim is to train the brain to adapt to sound.\nTRT has 2 elements- Direct counseling and sound therapy\nSound therapy: –\nSound therapy is to be combined with other therapies like counseling to improve effectiveness.\nTinnitus is more noticeable and disturbing when environment is quiet. Person with tinnitus thinks that sound is because of or can cause some serious damage to brain or inner ear. This negative emotional response from our subconscious brain (limbic system) travels to conscious brain-cortex, so person reacts.\nIn sound therapy instead of treating patient surrounding sound is manipulated so that it becomes less noticeable, here are many ways to achieve the goal such as: –\n- Table top devices-environmental sound machine, water fountain, clocks, fan, radio\n- CD based system\n- Hearing aids\n- Combination of hearing aid and masker\n6. Cochlear implant/electric stimulation-\nTable top devices;\nDevices kept on side of bed such as ticking clock, adjusting radio in between two stations, or fan, may divert attention and make tinnitus less noticeable.\nIf a patient has a hearing loss in the frequency range of the tinnitus, hearing aids may be helpful in covering the tinnitus.\nThe use of masking in the treatment of the tinnitus has mixed success, Patient’s perception of the pitch and loudness and the overall intensity of the masking signal should be well understood.\nTinnitus maskers introduce an external pleasant masking sound into affected ear , thereby minimizing or eliminating the perception of the tinnitus.\nCochlear implant may mask tinnitus by ambient sound or may suppress tinnitus by the electrical stimulation sent through the auditory nerve. Some forms of electrical stimulation to the ear can stop tinnitus briefly.\nEffective counseling is most critical aspect of tinnitus management. Tinnitus sufferers are anxious and frightened by the presence of tinnitus and need a careful and clear explanation of the nature of the disorder.\nIt can be quite disturbing leading to sleep deprivation, depression & decreased work efficiency. Most important aspect of tinnitus is distress, irritation and distraction caused by the noise. Person often feels that such noise is a symptom of brain tumor or stroke. Reassurance should be given by E.N.T. specialist, neurologist and audiologist.\nDistress caused by tinnitus is in correlation to degree of attention paid to tinnitus and shifting attention of person to a different signal is bio feedback. It is relaxation technique that teaches to improve coping ability and has 70-90% success rate.\nTinnitus patients have high levels of anxiety, tension, or other symptoms of chronic stress. Biofeedback is quite effective relaxation technique; it teaches person to control certain autonomic body functions such as pulse, muscle tension and skin temperature. Goal is to help person manage stress not by reducing stress but by changing the body reaction to it.\nControversies exist about role of alternate therapies: —\nResearch results have not conclusively identified these treatments as helpful for tinnitus.\nControl of inhalational and food allergy\nZinc, magnesium and vit-B12\nHerbal ginkgo biloba have mixed result\nMusic- music is soothing to soul, relieves pain and anxiety, and promotes relaxation and positive impulse to limbic system.\nBut it emphasizes the low frequencies and has little power in high frequency.\nTinnitus patient mostly has less hearing in high frequency and hear better at lower frequency. To correct this mismatch neuromonics are created (creator Dr Paul Davis and Peter Hankey, Australia). Modified music is delivered through high fidelity ear phones that match hearing and tinnitus level.\n*Research is going on for drugs Acamprosate, Vestipitant+Piroxitine, Neramaxane\n*Trans Cranial Magnetic Stimulation- SHAM stimulation-safe and effective.\n*Chronic electrical stimulation of auditory cortex- Rehabilitation by specially trained clinician -daily acoustic neural stimulus customized to audiometric profile is presented to desensitize. Approximately 90% success rate is observed in selected patients with improved life\n*Transtympanic perfusion inner ear:\nGenta- in menieres disease with intractable vertigo. Successful and less invasive than surgical procedures such as vestibular nerve section or labirynthectomy.\nDexa- in cochlear meneires may improve hearing, tinnitus, pressure feeling.']	['<urn:uuid:1c3b749c-4e42-46ff-8830-bfc31f52387a>', '<urn:uuid:f6e69a35-e306-4af0-bf8a-9c4aa42560b5>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:58:41.790432	35	131	1415
56	What does 'orient' mean and what drawing techniques are used for observing?	'Orient' comes from the Latin word 'oriens' meaning 'east' or 'rising,' and was used to describe properly positioning things eastward, like ancient temples. For observational drawing techniques, artists need to practice seeing past preconceived ideas, understand positive and negative shapes, and work with light and shadow to reveal form. Drawing should be done light and large initially, before building value into the work.	"['The term ""Orient"" derives from the Latin word oriens meaning ""east"" (lit. ""rising"" < orior "" rise""). The use of the word for ""rising"" to refer to the east (where the sun rises) has analogs from many languages: compare the terms ""Levant"" (< French levant ""rising""), ""Vostok"" Russian:Восток (< Russian voskhodRussian:восход ""sunrise""), ""Anatolia"" (< Greek anatole), ""mizrahi"" in Hebrew (""zriha"" meaning sunrise), ""sharq"" Arabic:شرق (< Arabic yashriqيشرق ""rise"", shurūqArabic:شروق ""rising""), ""shygys"" Kazakh:шығыс (< Kazakh shyguKazakh:шығу ""come out""), Turkish:doğu (< Turkish doğmak to be born; to rise), Chinese:東 (pinyin:dōng, a pictograph of the sun rising behind a tree) and ""The Land of the Rising Sun"" to refer to Japan. Also, many ancient temples, including pagan temples and the Jewish Temple in Jerusalem, were built with their main entrances facing the East. This tradition was carried on in Christian churches. To situate them in such a manner was to ""orient"" them in the proper direction. When something was facing the correct direction, it was said to be in the proper orientation.\nOrienta is an album by The Markko Polo Adventurers released in 1959. The album was produced by Simon Rady, arranged and conducted by Gerald Fried and recorded in stereo in Hollywood, California. The album uses a combination of sound effects and Asian-inspired music to tell humorous vignettes. Its suggestive cover art features a photograph by Murray Laden.\nThe person who would later become known as El Oriental was born on December 6, 1972 son of Mexican luchador and wrestling promoter Alfonso Moreno. Later on Moreno along with Dr. Wagner and Blue Panther trained El Oriental for his professional wrestling career. He made his debut on October 23, 1992 in Arena Azteca Budokan, the arena his father operated at the time. El Oriental adopted the ring name upon making his debut and has competed exclusively under that name since then. Early in his career he worked mainly in or around his home state of Ciudad Nezahualcóyotl, Mexico State, often working with his sisters Cynthia, Rossy and Esther Moreno.\nThe first Sabre was a former knife thrower named Paul Richarde until he was selected by Modred to oppose Black Knight. Paul Richarde was given an armor, an animated gargoyle. and Mordred\'s Ebony Dagger (the weapon with which Mordred had killed the first Black Knight). He was defeated by Black Knight after his horse Aragorn kicked the dagger from Le Sabre\'s hand.\nThe second Sabre is a mutantsuper villain. His first appearance was in X-Men #106. Young and reckless, Sabre was chosen by Mystique to join her new Brotherhood of Mutants, though never actually participated in any missions. He had the mutant ability of super speed, and took the name of the deceased Super Sabre. It is unknown if he continues to serve Mystique behind the scenes, or if he even retains his powers after Decimation. Hyper-accelerated metabolism augments his natural speed, reflexes, coordination, endurance, and the healing properties of his body.\nA sketch (ultimately from Greek σχέδιος – schedios, ""done extempore"") is a rapidly executed freehand drawing that is not usually intended as a finished work. A sketch may serve a number of purposes: it might record something that the artist sees, it might record or develop an idea for later use or it might be used as a quick way of graphically demonstrating an image, idea or principle.\nSketches can be made in any drawing medium. The term is most often applied to graphic work executed in a dry medium such as silverpoint, graphite, pencil, charcoal or pastel. But it may also apply to drawings executed in pen and ink, ballpoint pen, water colour and oil paint. The latter two are generally referred to as ""water colour sketches"" and ""oil sketches"". A sculptor might model three-dimensional sketches in clay, plasticine or wax.\nApplications of sketching\nSketching is generally a prescribed part of the studies of art students. This generally includes making sketches (croquis) from a live model whose pose changes every few minutes. A ""sketch"" usually implies a quick and loosely drawn work, while related terms such as study, modello and ""preparatory drawing"" usually refer to more finished and careful works to be used as a basis for a final work, often in a different medium, but the distinction is imprecise. Underdrawing is drawing underneath the final work, which may sometimes still be visible, or can be viewed by modern scientific methods such as X-rays.', 'Observational Drawing is fundamental to learning how to see and how this impacts your drawing skills.\nPerhaps you are a student that enjoys drawing varied subjects but now desire to explore deeper. Learn techniques from Betti Pettinati-Longinotti for drawing varied subject matter from observation.\nAre you looking for new possibilities to strengthen and challenge your drawing ability? This online drawing course will provide skills and insights to help you create drawings, which apply working directly from life.\nDesigned as an intermediate course for artists who would like to build on their basic understanding of drawing.\nCourse Materials (included in tuition):\n- How to See, How to Draw (PDF download) Retail: $29.99\n- Start Sketching and Drawing Now by Grant Fuller(PDF download) Retail: $22.99\n- Drawing for the Absolute Beginner by Mark and Mary Willenbrink (eBook) Retail: $19.99\n-  Instructional videos from Betti Pettinati-Longinotti\nWhat You’ll Learn:\n- Using essential tools and materials\n- Tips for using and understanding drawing from observation\n- Learning to see as it applies to drawing varied subject matter\n- Varied techniques for drawing from life\nWho should take this course:\n- Artists with a basic understanding and skill with drawing, that would like to increase their understanding and expertise in drawing from observation.\n- Experienced artists that would like to broaden their technique in drawing still life, landscape and portraits.\nART SUPPLIES YOU’LL NEED:\n- A range of drawing pencils of the B family\n- Color pencils, pack of a variety of colors\n- White plastic eraser\n- Kneaded eraser\n- Workable fixative\n- Hand-held pencil sharpener\n- Sketchbook or hardbound journal\n- Drawing paper- 16 x 20”, white drawing paper- Suggested, Strathmore; Canson, Rives, Bienfang or another good printmaking paper\n- Canson paper (for drawing with colored pencil)\n- Stumps, blenders, tortillions are optional\n- 18 x 24” or larger Drawing Board\nIn order to be successful with this course, read assigned pages from:\nDrawing for the Absolute Beginner by Mark and Mary Willenbrink, How to See, How to Draw: Keys to Realistic Drawing by Claudia Nice, Start Sketching and Drawing Now: Simple techniques for drawing landscapes, people and objects by Grant Fuller.\nAll assignments should be completed on 16 x 20” or larger 80 lb. white drawing paper or a good printmaking paper like Rives or Bienfang; Assignment with color pencil should be completed on Canson paper; digital photo taken at 72 dpi, jpeg. Sketchbook exercises may be scanned jpegs, 72 dpi.\nAll assignments must be submitted through the Blackboard System for evaluation. To expedite responses to questions, contact the instructor via e-mail.\nLesson 1: Getting Started\n- As a way of introduction, lets review good practices and understanding of successful drawing. Read Drawing for the Absolute Beginner, Chapter 1, “Sketching and Drawing” pages 11- 25.\n- Overview the three texts and post-it tab the areas that are most compelling to you regarding drawing from observation: objects, landscapes and portraits.\n- Do some visual research on the Internet, and communicate to me your favorite artists that work from direct observation. Share with me and other students in your group on the ANU Bulletin Board a few urls, so we can get a sense of what kind of ‘drawing aesthetic’ most interests you. Join in the conversation in response to the artists that inspire yourself and others.\nSession 1: Value Scale/ Sketchbook Studies\n- Create a 10 point Value scale in pencil. See my video- “Creating a Value Scale”.\nHomework assignment (3 Parts): Provide a visual understanding and foundation of value and contour. Turn these assignments in:\n- Do some visual research on the Internet, and communicate to me your favorite artists that work from direct observation (still life, landscape &/or portraits). Share a few urls, on our ANU Bulletin Board so the others in your group and I can get a sense of what kind of ‘drawing aesthetic’ most interests you. I will also share some of my favorites!\n- Spend about 30 minutes to create a value scale, using your drawing pencils, from dark to light, using the white of the paper as your lightest value. Use the pressure of the pencil to create luscious dark shades. Create a 10 point value scale 2” x 10 inches with 1-inch increments of value. This will help you to have a value scale as a tool and reference, to look for the quality and range of value in your drawing. Please watch the video demo on “Creating a Value Scale” that is linked on the ANU site for this course. Turn in a scanned copy of the Value Scale you create.\n- Lets do some preliminary exercises in your sketchbook to get started with drawing. Complete several observational sketches or studies of objects, practicing with your drawing pencils and explorations of show an understanding of contour and value applied in drawing. Spend a few hours practicing these.\nTurn in a few scanned pages from your sketchbook of your observational sketches or studies.\nSession 2: Still Life Drawing\nRead How to See, How to Draw, Chapter 2, within “Seeing Past and Preconceived Ideas”, read “Observational Skills”, pages 22-25; “Positive and Negative Shapes”, pages 34-35; Chapter 7- “Revealing Form through Light and Shadow” pages, 121-129.\nHomework assignment (2 Parts):\n- Always practice first in your sketchbook. Any exercise requires some warm-up, drawing also takes some warm up practice. Turn in scanned copies of your warm-up exercises in your sketchbook.\n- Using the information on building contour, shapes, positive and negative space begin a still life drawing on large paper. You will need to assemble objects to draw directly from life into a composition that appeals to you. Think about how the shapes relate to each other, and the light and shadow within the objects and environment of the space. Spend 6 hours on this assignment using your drawing pencils. Take photos of your progress intermittedly, so I can see the composition as it develops.\nTurn in a completed photograph of your 6 hour still life drawing in pencil, with a few photos of it in progress; in addition submit a photo of the actual still life that inspires your drawing.\nTips for good practice- Draw light and draw large, filling the space of your page. Once you feel sure about your composition you may begin to build value and\nSession 3: Drawing the Landscape from Observation\nHomework assignment (3 Parts):\n- After choosing a landscape to draw from directly outdoors, warm up by spending some time, about 30 minutes, in your sketchbook or journal, drawing the composition. When choosing a place to draw, make sure to choose one that is safe, and that provides good possibilities for the composition of your drawings. (View my video- “Working with your Sketchbook/Journal”)\n- Spend about 3 hours with a 16 x 20” pencil drawing on white drawing paper of your chosen landscape from direct observation. Build your landscape composition to include a foreground, middleground and background with a focal point within one part of that composition. Take a photograph of the landscape a view you chose to draw for a comparison from your drawing to the photograph of the composition. If this is a first experience drawing outdoors, it can seem intimidating. If you choose a space accessible to the public, you can probably expect both spectators and commentaries. Be friendly but focused on your work.\n- Spend an additional 3 hours on a color pencil drawing on 16 x 20” colored Canson paper, of the same landscape composition as your pencil drawing. Spend some time in your sketchbook or journal noting some differences of the working in pencil, versus working in color. With color pencil, start with one color and build your value and texture by using more opaque color and shading layers of varied colors. Build dark and lights also by shading with both white and black color pencils.\nTurn in photographs of your progress as well as your 2, 3 hour completed drawings, and a photograph of the actual landscape from your perspective\nSession 4: Drawing a Portrait from Observation\nRead Chapter 6 “Faces and Figures” Start Sketching and Drawing Now pages 78-91.\nSee my video- “Facial Features”.\nHomework assignment (2 Parts):\n- Warm-up by drawing 30 minutes with studies in your sketchbook of your portrait subject, and some facial studies of the features of your subject.\nChoosing your subject- Make some good decisions on the model you will draw. This may be a self-portrait or a drawing of someone else that is willing to sit for you for 6 hours. It will be tempting to work from a photograph, but challenge yourself to work directly from life. I would like a photograph for some comparison to your drawing, but again, try to be persistent to work directly from life. You may use yourself as a model and draw a self-portrait by looking in a mirror.\n2. Using the information on facial proportions, draw a full portrait on large paper, from direct observation. Spend 6 hours on this assignment using your drawing pencils. Use good proportion and begin to shade once the layout of your portrait composition is in place. Remember to draw light and large, before building value into your drawing. Take photos of your progress, intermittently so I can see the process you utilized.\nTurn in completed photographs of your sketchbook studies and of your 6 hour portrait drawing in pencil, with a few photos of it in progress.']"	['<urn:uuid:79b6eb67-65e5-47b3-8b65-d0b91c79a5c4>', '<urn:uuid:99edffd8-6472-49d2-82e4-0adcc5c2776c>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T19:58:41.790432	12	63	2308
57	What special features and technology helped find that valuable 500-year-old gold coin in the field near Ashbourne?	The discovery was made possible through metal detector technology that uses electromagnetic coils and frequency detection. Metal detectors contain a transmitter coil that creates a magnetic field and a receiver coil that detects frequency changes when metal is present. The detector was able to pick up a deep signal about 6 inches into the ground, leading to the discovery of the Henry VII gold Angel coin from 1485-1509, featuring a ship with Royal Arms and Latin inscription, now valued at over £4,000.	"['Treasure hunter who unearthed a 500-year-old gold coin featuring the Royal Arms could earn more than £4,000 - but he refuses to part with his rare find\n- Gareth Millward, 35, unearthed the rare find in a field in Derbyshire last summer\n- Latin inscription on its rim reads \'By the cross save us, Oh Christ our Redeemer\'\n- The hammered gold coin would have been used during the Tudor era experts say\n- An ancient ship with the Royal Arms on its sail is depicted on its front face\nA treasure hunter has struck it rich after digging up a 500-year-old gold coin that could be worth more than £4,000 ($5,200), but refuses to part with his rare find.\nGareth Millward unearthed the coin in a field near Ashbourne, Derbyshire, and says it is the highlight of his four-year hobby.\nThe 35-year-old says he had a hunch the piece of land he was inspecting was about to come up trumps as he headed out on the hottest day of the year last summer.\nHe had been scanning the area for six weeks and his friends had found rare sovereign coins - but his gold find was something a metal detectorist dreams of digging up.\nA treasure hunter has struck it rich after digging up a 500-year-old gold coin that could be worth more than £4,000 ($5,200)\nExperts say the artefact, which has now been logged by Derby Museum, could fetch between £2,000 and £4,000 ($2,600 and $5,2000) at auction and Mr Millward\'s example is in the best possible condition, which means it could be worth more.\nThe hammered gold coin would have been used during the Tudor era and features an ancient ship with the Royal Arms on its sail.\nInscribed around the coin, in Latin, a passage reads: \'By the cross save us, Oh Christ our Redeemer.\'\nAbout the find, he said: \'After only 20 minutes in the field I got a deep, iffy signal with my metal detector.\n\'I dug down around six inches of rock hard pasture, the signal was still in the bottom of the hole so took another two inches out expecting it to be deep lead.\n\'As the bone dry dirt hit the ground next to the hole it crumbled to pieces revealing a flash of gold.\n\'I knew instantly what it was, I picked it up and looked at it in amazement. It was the most beautiful thing I had ever seen in my life.\n\'A solid 68 on the surface - I had unearthed a Henry VII full gold Angel, 1485-1509.\'\nThe hammered gold coin would have been used during the Tudor era and features an ancient ship with the Royal Arms on its sail\nGareth Millward, 35 (pictured), unearthed the rare find on the hottest day of the year last summer\nMr Millward, of Middleton-by-Wirksworth, Derbyshire, described his excitement when he pulled the coin from the earth.\nHe added: \'I shouted out loud ""Oh my God, gold hammered! Gold hammered!""\n\'I sat there for near on an hour in absolute disbelief trying to comprehend what had just happened.\n\'This was only my fifth-ever hammered coin find. I spent a further two hours in that field and found almost nothing.\n\'I asked myself how on earth did I walk straight on to this coin.\n\'I must have been the luckiest man in metal-detecting on that day.\'\nThe invention of the metal detector cannot be truly claimed by one person.\nIt is a combination and amalgamation of several different pieces of technology.\nAlexander Graham Bell did fashion a device that was an electromagnetic, metal locating machine.\nThis was based on a device invented by physicist Heinrich Wilhelm Dove.\nSometime later, an engineer Gerhard Fischer, filed a patent regarding a design.\nA metal detector consists of a stabiliser, control box, shaft, and search coil.\nIt is the two coils that are actually responsible for the detection of metal.\nThe outer coil is the transmitter coil while the inner coil is the receiver coil.\nThis works to detect and amplify frequencies. This type of technology is known as Very Low Frequency or VLF technology.\nWhen electricity is provided to this transmitter coil, there is a magnetic field created around the coil.\nThis is the same science behind electromagnets.\nWhen the machine wafts over metal the electrons in the metal - due to its metallic bonding and sea of electrons surrounding a fixed positively charged mass - are affected by the magnetic field.\nThe change in the electrons triggers a tiny electrical field in the metal object which alters the frequency of the metal detector.\nThis indicates metal is present.\nMore advanced metal detectors are also able of differentiating between different types of metal ad the frequency change is different and therefore the pitch of the note is altered.\nSource: The Detectorist\nThe coin has been shown to the owner of the land Mr Millward was detecting in and the landowner let him keep it.\nMr Millward said: \'He and his family were stunned. He said his father would have loved to see it but he had unfortunately passed away only a few week earlier.\n\'We spoke for a while, he appreciated my honesty and said that the coin was mine to keep and also granted me permission to search the rest of his land.\n\'This beautiful piece of history had been lost many centuries ago, laying undisturbed and dormant until I came plodding along with my detector.\n\'I believe it was my destiny to find that coin on that day, but also the reward for my dedication and hard work in this great hobby.\n\'My mind wanders at the story behind this coin, how it got there, who dropped it and what the landscape was like back then.\n\'It fascinates me as well as frustrates me, knowing I\'ll never quite know the truth.\n\'But what I do know is this was the best day of my life.\'\nThe rare coin, which has the passage \'By the cross save us, Oh Christ our Redeemer\' inscribed in Latin around its rim, was unearthed in a field near Ashbourne, Derbyshire\nMost watched News videos\n- French Iranian knifeman\'s chilling video disguised in a mask\n- Fedex plane nearly crashes into a Southwest Airlines plane in Texas\n- Moment Russian soldiers execute surrendering Ukrainian troops\n- No snowflakes here! Lion-hearted passengers move bus stuck in snow\n- Harry and Meghan had \'to find their own voice\' says Ndileka Mandela\n- Jealous builder smashes Transit van into former lover\'s car\n- Migrant brazenly sprays £20 notes in taxpayer-paid hotel room\n- Senior Hamas Official says war of liberation \'is coming soon\'\n- Israeli military shows Hamas tunnels during ongoing ground operations\n- Viral nativity boy put to the task on This Morning ahead of play\n- Hero mother is killed saving her five-year-old daughter from a shark\n- Mothers and grandmothers beg for remaining hostages to be released', 'What does frequency mean when metal detecting?\nA common definition of frequency is as follows: The number of waves per unit of time measured in Khz or kilohertz. In a metal detector this is the number of electronic waves sent into the ground to detect metal.\nExample: 10 Khz means your detector will send and receive 10 000 times per second.\nWhy is Frequency important?\n- Both Pulse induction and beat frequency oscillator type metal detectors use frequency of electronic fields or pulses sent into the ground.\n- Different frequencies have different advantages and disadvantages when metal detecting.\n- Metal detector frequencies range between 3 – 100 Khz as a general rule.\n- Have longer wavelength.\n- Gets greater depth as long waves penetrate the ground more easily.\n- Better for detecting high conductivity targets like silver.\n- Not good for finding smaller targets.\n- Not good for low conductivity targets like gold or Iron.\n- Have shorter wavelengths.\n- Great for detecting small objects like tiny gold nuggets.\n- Better for low conductivity targets like gold and Iron.\n- Less depth is achieved than low frequency.\n- Higher accuracy, closer to the surface.\n- More sensitive to ground mineralisation interference.\nMost hobby detectors use a middle ground to try and get the best of both frequencies in the Goldilocks zone around 6 – 8 Khz for the best depth and sensitivity trade off. Some detectors even let you manually set the frequency on your detector. Others even use multiple frequencies at one time.\n2 types of Frequencies:\nSingle Frequency operation: Also called continuous wave has only one frequency selection. This is usually found in the Beat Frequency Oscilators like the Garrett Ace 250. This type is most often found in Entry level machines.\nMultiple or Dual Frequency operation: Some more advanced metal detectors make use of more than one frequency at the same time. Examples of this can be found in the Minelab Excalibur II, Etrac and CTX 3030. This is often called full band spectrum frequency technology allowing the user to get the best depth and accuracy at the same time. This is more often found on Pulse induction machines.\nMetal Detector Frequency Comparison Chart:\n|Garrett||Ace 150||Single||6.5 Khz||Beat frequency.|\n|Garrett||Ace 250||Single||6.5 Khz||Beat frequency.|\n|Garrett||Ace 350||Single||8.25 Khz||Beat frequency.|\n|Garrett||Sea Hunter||Multiple (manually adjustable)||7.5 Khz||Pulse induction.|\n|Garrett||AT Pro||Single||15 Khz||Beat frequency.|\n|Garrett||Infinium||Multiple (manually adjustable)||7.3 Khz||Pulse induction.|\n|Garrett||GTI 2500||Multiple (manually adjustable)||7.2 khz||Pulse induction and Beat frequency.|\n|Garrett||AT Gold||Single||18 Khz||Beat frequency.|\n|Minelab||GPX||Multiple (manually adjustable)||Multiple specially calibrated to conditions selected.||Pulse induction.|\n|Minelab||Eureka Gold||Triple||6.4, 20, 60 Khz||Pulse induction.|\n|Minelab||Xterra 305||Single||7 Khz or 18.5 Khz with a different coil.||Pulse induction.|\n|Minelab||Xterra 505||Single||7 Khz or 18.5 Khz with a different coil.||Pulse induction.|\n|Minelab||Xterra 705||Single||7 Khz or 18.5 Khz with a different coil.||Pulse induction.|\n|Minelab||Xcalibur 2||Multiple (17 concurrent)||1,5 – 25,5 Khz||BBS Broad band spectrum Pulse induction.|\n|Minelab||Etrac||Multiple concurrent||1,5 – 100 Khz||FBS Full band spectrum Pulse induction.|\n|Minelab||CTX 3030||Multiple concurrent||1,5 – 100 Khz||FBS Full band spectrum Pulse induction.|\nFrequency and Conductivity.\nMetal objects in the ground all conduct electricity better or worse. This may be plotted on a scale from Ferrous iron on the low side to Non Ferrous silver on the high side with gold and foil somewhere in the middle. This is often the basis for the LCD screen layout as you can see on the Garrett Ace.\nHobby metal detectors can tell the type of metal by how well it conducts electricity of the frequency pulses it sends into the ground (usually the speed that the signal sent back decays over time.) This allows the modern hobby metal detector to discriminate between metals and not only indicate the type of metal it thinks it’s found but also allows you to screen out certain metals like iron if you want to stop the detector from picking them up.\nWhether it is a simple single frequency machine like the Ace 350 or a complex machine like the Etrac using 1 – 100 Khz at the same time frequency is an important consideration when making your purchase.\nWe hope this helps your choice and understanding of a hobby metal detector and frequency!\nFind this page useful? Hit the g+ button bellow 🙂']"	['<urn:uuid:36185b40-a7b8-4b98-881e-5a73e779cc1e>', '<urn:uuid:5b7e9660-2b30-4969-be4f-d4bd3dfdf7cc>']	factoid	direct	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T19:58:41.790432	17	82	1851
58	How does parasite distribution influence treatment timing and efficacy?	Parasite distribution analysis shows complex patterns within farms, with some locations showing multiple genotypes indicating various infection sources that require targeted management. The vast majority of parasites (95%) exist on pastures rather than in cattle (5%), which significantly impacts treatment strategies. For effective control, treatments must be timed strategically - fall treatments combined with freezing conditions help reduce parasite populations, but spring treatments are also crucial. Treatment efficacy varies by product type: white dewormers are only effective for one day, while endectocides work for 14-28 days depending on the product and parasite type. This timing must account for the fact that parasites can survive extended periods on pastures, making continuous management necessary for optimal control.	"['Leo van Iersel has recently been trying to convince me that rooted networks might also be useful as exploratory data analysis (EDA), in addition to the unrooted networks that I have championed in print (Morrison 2010) and in this blog. I have tried to find a dataset that will support his case, and the one discussed here is the best that I have been able to find.\nIn infection biology we are interested in the transmission of pathogens from one host to another, possibly in geographically distant locations. It is usually assumed that pathogens (viruses, bacteria, protists, microfungi, helminths) with the same genotype found in different locations represent transmission from a single source location. Conversely, a mixture of genotypes at a single location is assumed to represent multiple sources of infection, possibly at different times. This type of analysis is a combination of population genetics and phylogenetics.\nSuch transmission studies can produce quite complex results, even to the extent of having different pathogen genotypes simultaneously in the same host. Data analysis is usually based on either a rooted tree or an unrooted haplotype network, but it can also conveniently be studied using a rooted reticulation network. I will illustrate the latter with a simple example.\n|Click to enlarge|\nThe figure shows a rooted network for 1,544 aligned nucleotides from 72 samples of the nematode Dictyocaulus viviparus, which is the parasitic lungworm of domestic cattle. The data are concatenated mitochondrial protein (2 genes), rRNA and tRNA gene sequences, from Höglund et al. (2006). The analysis shows the inferred historical relationships among 64 farm samples from Sweden (8 worms from each of Farms 29, 34, 36, 38, 49, 65, 68 and 76) and 8 samples from a isolate that had been maintained in the laboratory (L, used as the outgroup to root the network).\nThe data have been analyzed using the reticulation network method of Huson et al. (2007), based on splits generated by the Median network. Since the character data are essentially binary (with two exceptions), this produces exactly the same result as for a recombination network.\nIn the network, most of the samples from within each farm seem to be closely related in a simple divergent fashion through time, as would also be conveniently displayed by a standard tree-based analysis. There are apparently two major clades of genotypes, with 6-7 subclades. We can conclude from the tree-like relationships that four farms show evidence of only a single source of infection (Farms 34, 36, 38 and 76 each have a single genotype), while two farms appear to have at least two genotypes and thus probably two sources of infection (Farms 49 and 68).\nHowever, two of the farms show more complex patterns than these, which would not be revealed by a simple tree analysis. These two farms have groups of samples that descend from reticulation nodes (indicated by the arrows), thus suggesting the pooling of two distinct sources of genetic material. Note that there is no suggestion that these reticulations represent either recombination or hybridization, given that the data are from mitochondrial genes. This analysis is best treated as exploratory (EDA), highlighting genotypic complexity that warrants further biological investigation, rather then providing an explicit hypothesis of evolutionary history.\nFarm 29 is shown as having one unique genotype (5 individuals) plus another genotype (3 individuals) that has elements possibly related to both of the major clades of genotypes. Perhaps these latter 3 individuals represent an earlier infection, given their apparent association with the basal branches of the two clades.\nFarm 65 appears to be even more noteworthy. There are 3 individuals that are apparently related to those on Farm 36, plus 3 individuals of somewhat uncertain relationship. Then there are 2 individuals with elements possibly related to the genotypes on Farms 76 and 49. This is clearly a very interesting farm, from the point of view of lungworm infection and transmission, with at least three possible infection sources. This is important information that needs to be taken into account for possible management strategies.\nThis use of a rooted network analysis for exploratory data analysis seems not to have been considered before. However, it seems to me that it adds considerably to the practical information that can be gleaned from a study of the transmission of pathogens.\nHöglund J., Morrison D.A., Mattsson J.G., Engström A. (2006) Population genetics of the bovine/cattle lungworm (Dictyocaulus viviparus) based on mtDNA and AFLP marker techniques. Parasitology 133: 89-99.\nHuson D.H., Klöpper T.H. (2007) Beyond galled trees — decomposition and computation of galled networks. Lecture Notes in Bioinformatics 4453: 211-225.\nMorrison D.A. (2010) Using data-display networks for exploratory data analysis in phylogenetic studies. Molecular Biology and Evolution 27: 1044-1057.', ""Tips to clean up parasites in pasture grasses.\nOnly 5 percent of parasites live in cattle, which means 95 percent of parasites are on pastures. That is why Dr. Bert Stromberg, parasitologist and professor, College of Veterinary Medicine, University of Minnesota, says a parasite control program also can help reduce parasite burdens on pastures.\nCattle are only one stop on the parasite life cycle; therefore, parasite control programs should focus on cattle and the resulting burden on pastures.\n“We know that a large number of parasites live on the pastures and they can survive there for an extended period of time, even during Minnesota winters,” Dr. Stromberg says. “Therefore, producers should keep in mind that a strategic deworming program should focus on taking care of parasites in the host before they contaminate pastures.”\nDr. Frank Hurtig, director, Merial Veterinary Services, says a fall parasite control treatment, combined with freezing conditions will help, but producers should not stop there.\n“Freezing conditions will help kill some of the parasites on pastures,” Hurtig says. “Producers should consult their veterinarians about the best time to treat for parasites, but cleaning up cattle in the spring also can help reduce the overall parasite load that can affect cattle's performance.”\nHe adds that in Southern climates, it is even more important that producers consider parasites on pastures - they cannot count on a freeze to do the work for them.\n“In climates in between North and South, where temperatures may be moderate one week and freezing the next, producers need to remember that as long as cattle are grazing, they can pick up parasites,” Hurtig says.\nTo demonstrate this, a study was conducted in Oregon - a place where temperatures can fluctuate - during two weeks in the winter. During this time, temperatures never got above freezing. Results varied; however, parasite-naïve calves turned out on contaminated pastures on one operation picked up as many as 200,000 nematodes.\n“This study shows that if cattle are exposed to pastures they continue to ingest parasites during the winter months,” Hurtig says. “It is essential to break the life cycle of the parasites when they are in the cattle to help reduce pasture re-contamination.”\nHe adds that producers should discuss a strategy with their veterinarian to help ensure cattle are protected from parasites all year.\n“Producers may not realize it, but parasite control treatments are only effective against worms for the day you treat in the case of some white dewormers, and 14 to 28 days for endectocides, depending on the product used and the parasite,” he says.\nHe adds that it is equally important that producers are using products that are effective against the economically important parasites in their area. For example, not all products control liver flukes, so Hurtig says producers need to make sure they use a product labeled for liver fluke control.\nParasite control has been identified as the most economically important practice to beef production. However, Hurtig says, for a parasite program to be most effective, producers should use products they can trust.\n“Fall is an excellent time to start; however, following that with a spring treatment is important to help ensure parasites are not robbing cattle of performance throughout the year,” he says.""]"	['<urn:uuid:0801c6c2-3e1f-4d8e-a797-a56e8b5bf71d>', '<urn:uuid:c85cb9d3-8019-4596-b912-1b66d3caf4bf>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T19:58:41.790432	9	115	1320
59	How does the burden of proof compare between family court proceedings and criminal cases in Michigan's legal system, particularly regarding custody and criminal charges?	In Michigan's legal system, different standards of proof apply to various proceedings. For child custody matters, decisions are made under the 'best interest of the child' standard, considering 12 specific factors outlined in MCL 722.23. For criminal cases, the burden of proof is 'beyond a reasonable doubt,' which is the highest standard requiring no reasonable doubt about any factual elements of the charged crime. In comparison, civil cases use a 'preponderance of evidence' standard (more than 50% probability), while certain proceedings like termination of parental rights require 'clear and convincing' evidence, which falls between preponderance and beyond reasonable doubt. These different standards reflect the varying gravity and consequences of different legal proceedings.	"['How long after I file before I am divorced?\nIn Michigan, the law provides that a divorce without children has to be filed for 63 days before a divorce Judgment is granted. So, without children, it takes at least 64 days for a divorce to occur. It may take up to 6 months if the you and your spouse can not decide how to divide the assets and debt.\nA divorce with children takes longer. The law provides that when a divorce is filed and the parties have minor children, the parties have to wait 6 months before a divorce Judgment can be entered with the court. However, in most courts in Michigan, Judges will enter a Judgment prior to the expiration of the 6 months, if both parties tell the Court that it is in the best interest of the children that the Judgment is entered sooner than the 6 months. Usual reasons given are the children stability, or to reduce the children’s stress by “things” becoming final.\nHow is child support calculated?\nMichigan law provides that child support is calculated using the child support prognosticator. The following factors are considered: 1.) both parties income, including W-2 and 1099 income, business income, cash income, wages and tips, 2.) the number of the parties’ children, 3.) the number of overnights excised by both parents, 4.) health care premiums, 5.) child care, and 6.) tax filing status and number of exemptions. If either party has additional children they are supporting, the number of additional children is included. This information is included in a computer program and the child support is calculated.\nCan child support be agreed upon at a lower amount than the prognosticator supports?\nYes, however, the court does not have to adopt your agreement. All courts have an obligation to assure that the established support is sufficient to support the child in the same manner as the payer of support lives. If the support prognosticator calculates support to be $1,000 per month, the court most like will not enter a support order for $440 per month. If the parties agree to support of $850 per month, and the payer of support supports the child in another manner, e.g. pays for private schooling or all extracurricular activities, the Court will most like be convinced that the agreed upon support amount is sufficient.\nWhat are the 12 custodial factors the Court reviews prior to making a custodial decision?\nMichigan statute, in MCL 722.23, sets forth the 12 custodial factors the court must consider when it makes a custodial decision. The standard is the “best interest of the child” standard.\n- The love, affection, and other emotional ties existing between the parties involved.\n- The capacity and disposition of the parties to give the child love, and guidance, and to continue the education and raising of the child in his/her religion or creed.\n- The capacity and disposition of the parties involved to provide the child with food, clothing, medical care or other remedial care or other remedial care recognized and permitted under the laws of this state in place of medical care, and other material needs.\n- The length of time the child has lived in a stable, satisfactory environment and the desirability of maintaining continuity.\n- The permanence, as a family unit, of the existing or proposed custodial home or homes. Plaintiff is married and lives in an apartment.\n- The moral fitness of the parties involved.\n- The mental and physical health of the parties. Both parties are young and in relatively good physical health. Defendant is in good mental health.\n- The home, school, and community record of the child. The child’s custodial environment is with both parties.\n- The reasonable preference of the child.\n- The willingness and ability of each of the parties to facilitate and encourage a close and continuing parent-child relationship between the child and the other patent or the child and the parents.\n- Domestic violence.\n- Any other factor considered by the court to be relevant.\nI heard that joint custody of the children is always awarded. Is this true?\nMichigan Statute MCL 722.26a provides that when parents cannot agree to custody of the children, one party can ask the court to award joint custody of the children. When asked, the court shall than “consider an award of joint custody.” When making its consideration, the Court shall determine “in the child’s best interest” whether the “parents will be able to cooperate and generally agree concerning important decision affecting the welfare of the children.” Child support may still be ordered, to assure the “needs of the children” are being met based on “the actual resources of the parent.”\nIn most cases, a custody and parenting time order is put in place that assures the most consistent and stable contact between the parents and the children.\nWhat happens at a Friend of the Court hearing?\nA Friend of the Court hearing is before a Referee. At the hearing, you should present evidence supporting your case, including testimony of witnesses. The Referee will make a decision based on what was presented at the hearing. The Referee’s recommendation will become an order, unless you object to the recommendation. When you object to the Referee’s recommendation, you will schedule your objections in front of the Judge. The Judge will then decide whether the Referee made the correct decision based upon the facts presented and the law. If the Judge determines the Referee’s decision is correct, then the Judge will adopt the Referee’s recommendation.\nThe Friend of the Court schedules Referee hearings for child support and changes to custody and parenting time orders. Most times, a Referee hearing is scheduled because it can be scheduled sooner than a Judge hearing and will cost you less, you believe the Referee has a “good grasp” of your families’ case and believe the Referee will make a “good” decision, or the Judge orders you to have a Referee hearing instead of a Judge hearing.\nConfusing? That’s because it can be, and that is why an attorney should be present with you at any Friend of the Court hearing.', 'The terms and definitions on this page are relevant to criminal cases in the State of Michigan, United States of America, unless noted otherwise. Criminal laws and procedures in other states and countries may be very different.\nDo not take legal action solely in reliance on the information posted on this page!\nThis page provides general information that is intended, but not guaranteed, to be correct, complete and up-to-date. Do not rely, for legal advice, on information given on this page or any externally referenced Internet sites. If you need legal advice upon which you intend to rely in the course of your legal affairs, consult a competent attorney in your area.\nThe legal system can be filled with confusing phrases and terms. This list should help you to understand that system a little better.\n- Bond money paid to a court, by or on behalf of a criminal defendant, as security that, when released from jail, the defendant will appear at future hearings. If another person posts the bail money, then that third party vouches that the defendant will appear at future court dates. Bail can be forfeited if the defendant fails to appear or violates release conditions.\n- A court employee who assists the judge in maintaining order in the courthouse, and who is responsible for the custody of a jury.\n- An intentional, unwanted and forceful/violent touching of another person, or something closely connected with that person.\n- A trial held before a judge and without a jury.\n- A court order commanding the defendant\'s (or a missing witness\') arrest and appearance in court after the person had previously failed to appear for a hearing. A bench warrant could also be issued against a defendant for violating a court order, such as conditions of release or probation.\n- A finding at a preliminary examination that sufficient evidence exists to require a trial at the Circuit Court level on the charges made against the defendant.\nBond / Bail Bond\n- A promise or contract to do or perform a specified act, or pay a penalty for failure to perform. This is usually guaranteed by a \'surety\', who promises to pay if the \'principal\' defaults, or by paying a cash bond. In criminal cases, \'bond\' means the same thing as \'bail\': a financial obligation signed by the accused or a surety intended to guarantee the defendant\'s future appearances in court. The amount of the bond is set by a judge or magistrate. The bond can include conditions of release (i.e., no contact with the victim, no alcohol consumption, etc.) Factors influencing the amount of bond set include the seriousness of the charge, the defendant\'s criminal history, and the defendant\'s ties to the community.\n- There are four types of bonds:\n- Personal recognizance bonds (a.k.a. ""PR"" bonds, or ""signature bonds"") do not require the defendant or a third party to pay money to the court, unless the defendant later fails to appear.\n- Percent bonds require the defendant to post a percentage of the full bond (generally as low as 10%) to get out of jail, and the remaining percentage is due only if the defendant later fails to appear.\n- Cash bonds require the full amount of the bond to be paid in cash before the defendant can be released. If the defendant appears at all future court dates, most of the monies are returned to the person posting the bond.\n- Surety bonds are posted by a professional bondsman after being paid a non-refundable percentage of the full amount by the defendant.\n- As in ""breaking and entering""... using some force to enter a building (opening a door, raising a window, taking screen off, etc.); damage need not result.\n- A written statement submitted by the lawyer for each side in a case that explains to the judges why they should decide the case or a particular part of a case in favor of that lawyer\'s client.\nBurden of Proof\n- The duty to establish by evidence a requisite degree of belief concerning a fact in the mind of a trier of fact. The duty to establish facts in an adversary proceeding. Different burdens of proof exist in the law:\n- Prima facie - evidence which is good and sufficient ""on its face"" to establish a given fact when unrebutted or not contradicted.\n- Probable cause - facts and circumstance sufficient to convince a person of reasonable caution that an offense has been committed.\n- Preponderance - the burden of proof in civil cases. Evidence which, as a whole, shows that the fact sought to be proved is more probable than not. Evidence which is more credible and convincing to the mind. It is generally visualized as that side of the dispute toward which the scales tip when the credible evidence is weighed by the trier of fact. Something more than 50% of the credible evidence.\n- Clear and convincing - the burden of proof in selected proceedings, such as termination of parental rights. A measure of proof which produces a firm belief as to the allegations. It is difficult to quantify, but is more than a ""preponderance"" and less than ""beyond a reasonable doubt"".\n- Beyond a reasonable doubt - the degree of belief a criminal juror (or the judge in a bench trial) must have regarding all factual elements of a charged crime. no doubt, based on reason and common sense, can exist as to any fact needed to be proved.']"	['<urn:uuid:36f08fdb-0894-4e09-ac30-7865ee9e5aee>', '<urn:uuid:16c1c82e-ced5-444a-9411-de6bf80bc178>']	open-ended	direct	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-12T19:58:41.790432	24	112	1943
60	eastman school music first purpose	The Eastman School was founded in 1921 as the first music conservatory in the United States to serve simultaneously as a center for academic study of music while serving the community through musical education and appreciation.	['It was almost 20 years ago that chamber musicians David ’92E (DMA), Janet ’92E, Philip ’91E, ’92E (MM), and Timothy ’91E (DMA) Ying arrived in the rural town of Jesup, Iowa, population 2,000. Fresh out of the Eastman School, the siblings who formed the Ying Quartet were in Jesup as part of a chamber music outreach program funded by the National Endowment for the Arts. For two years, the townspeople of Jesup had in the Yings their very own “resident string quartet.”\n“It was a huge opportunity for us,” says Philip Ying, now chair of the Eastman School’s chamber music department. “We were placed in an environment where we had to articulate for ourselves and to anyone we wanted to be an audience, what was so great about this music that we play? We had to think about what connects our art to people—people in Iowa who had little exposure to live string quartet music. We had to ask ourselves, what does it have to do with them? What does it have to do with community? What does it have to do with being a human being?”\nToday, a slightly reconstituted quartet (violinist Ayano Ninomiya has replaced Timothy Ying) resides at the Eastman School, where its members have yet to run out of answers to these questions. They’ve pushed the creative boundaries of what two violins, a cello, and a viola can accomplish together—by mixing musical genres, for example, and incorporating other art forms, such as poetry and dancing, into their performances.\nAnd they’re a symbol of the spirit of innovation and optimism that Douglas Lowry, the dean of the Eastman School since 2007, evokes as he describes what he’s come to call “the new Eastman evolution.”\n“Quite simply,” he says, “we’re completing George’s original vision.”\nFounded by George Eastman in 1921, the Eastman School was the first music conservatory in the United States to serve simultaneously as a center for the academic study of music. But it was never to be an ivory tower. Eastman intended for the school to serve the community through musical education and appreciation. The words “For the enrichment of community life” were etched into the Eastman Theatre façade.\nEastman also had planned for the school and theater to occupy the entire stretch along East Main Street from Gibbs to Swan Street, but he was unable to purchase a final lot along Swan at a reasonable price. Thus, part of the new Eastman evolution includes a major expansion of the school’s performance and rehearsal spaces—in exactly the location Eastman had intended.\nIn December, students, faculty, and guest artists will present a series of concerts to celebrate the completion of the nearly 40,000-square-foot addition abutting Eastman Theatre. Within that space will be the 222-seat Hatch Recital Hall, outfitted with live Web streaming capabilities, and a design that permits acoustics to be modified to suit different types of instruments and performances; a new and acoustically updated rehearsal hall spacious enough for large ensembles; a recording control room; teaching studios; and the multistory Wolk Atrium, housing a box office and a gift shop, lighting up East Main Street through a glass façade.\nBut an even larger aspect of the school’s evolution is a series of changes that have been taking place in its approach to professional music education—changes that Lowry says reflect George Eastman’s pioneering spirit and his vision that musicians engage and connect with broad audiences.\nIt’s not only an exciting time, but also a challenging one, for Eastman and for music schools generally. The core of the faculty and student body are educated as classical musicians, yet symphony audiences are aging, and many American orchestras exist on the financial brink. Advances in computer technology, digital recording and listening devices, and the rise of the Internet have made it possible for new artists to record and deliver their music more widely, and with less overhead, than ever before—although those very same technologies may often make it more difficult for musicians to earn a living through their craft.\nAnd the mixing of musical genres, although not new, has spurred an extraordinarily creative era in music, while providing musicians with the potential to attract fresh audiences.\nAmong the most widely cited points of pride for the school is Break of Reality, a self-described “cello rock” band. Formed at Eastman in 2003, the group today consists of three cellists, including Patrick Laird ’07E, as well as percussionist Ivan Trevino ’06E. Earning national attention and praise, the group has offered an opportunity in which, in the musicians’ own words, “the fans of Led Zeppelin, Radiohead, and Yo-Yo Ma are finally getting acquainted.”\nAlarm Will Sound, a 20-member ensemble formed at Eastman in 2001, offers musically and visually rich performances of contemporary music that have also been widely applauded. Allan Kozinn, a classical music critic for the New York Times, praised the musicians of Alarm Will Sound as “young, virtuosic, and full of lively ideas,” making the group “one of the most vital and original ensembles on the American music scene.”\nAcross the school’s 13 departments, faculty members point to such adventurous groups as evidence that Eastman graduates are well-equipped to lead in an uncertain musical environment. The\nEastman School began looking forward nearly 20 years ago, when then director Robert Freeman began a formal dialogue on how the curriculum might be revised to better serve students. Ramon Ricker, senior associate dean for professional studies and professor of saxophone, was part of that effort. He now directs the Institute for Music Leadership, founded in 2001 to carry out curricular reforms and inspire students to take an entrepreneurial approach to developing their professional careers.\n“We were looking at the music school curriculum, which had been the same for the past 100 years,” says Ricker. “You took a one-on-one lesson on your instrument, you played in the orchestra or band or you sung in the chorus, you learned about music theory and music history, and you took a few humanities courses.”\nThe first center of its kind in the nation—and a model that schools such as the New England Conservatory, Oberlin, and others have since adopted—the institute established new courses, workshops, and other initiatives to “bridge the ivory tower with the real world”—not only to meet that world on its own terms, but to help shape it as well, by training musicians to think more broadly about their music and their potential roles in society.\nToday, the institute’s Catherine Filene Shouse Arts Leadership Program offers students half-semester and semester courses on digital portfolio creation, intellectual property protection, recording, outreach, and other practical and philosophical aspects of building a musical career. Above all, says Ricker, it encourages musicians to think of themselves as a small business—as the “sole proprietors” that he says is what most musicians, in effect, are.\n“I’ve known for 50 years that you’ve got to create your own situation to create a career in music,” says Ricker. “I’ve always played a lot of jobs. I’ve done a lot of commercial work.” And that’s on top of his roles as an Eastman professor and a member of the Rochester Philharmonic Orchestra for nearly 40 years.\nPercussion chair Michael Burritt ’84E, ’86E (MM) advances a similar view. Seated across from his five-octave marimba—a form of the instrument he notes didn’t yet exist when he was a student at Eastman—he says, “I feel like Eastman’s always been a school that’s tried to look forward. I think it’s set a lot of trends in higher music education, whether it’s the Institute for Music Leadership, or the jazz program, or the wind ensemble, which was the first pre-eminent ensemble of its type.”\nAnd Christopher Azzara ’92E (PhD), a pianist, arranger, and chair of the music education department, says, “There’s a lot about Eastman that’s unique that will help us to be a leader.” Azzara, who is also an affiliate faculty member of the jazz studies and contemporary media department and an expert on improvisation, says that he has been able to develop courses in improvisation—a longtime requirement for jazz musicians—for classical musicians as well.\nTheir optimism is a response to a widely felt concern at music schools around the United States that classical music, the core interest of many music students, and perhaps the majority at Eastman, is facing a “crisis.” But Lowry draws a clear distinction between the music, which he says still flourishes, and the business model which has sustained it in the past. If there’s a crisis, it’s the business model, and not the music itself, that’s the source of it.\nThat business model is one that confined classical music performance to fixed contexts like the concert hall, and centered it on the permanent symphony orchestra. A century ago, symphony orchestras and large concert halls were financed by wealthy benefactors who believed in the notion that the music of 17th-, 18th-, and 19th-century Western Europe, commonly understood as “classical music,” could both edify and elevate humanity.\nBut ornate concert halls and commonly accepted performance strictures alienated potential audiences, says Phillip Ying. “Sometimes when you keep music in the concert hall exclusively, people come in and they think they’re supposed to behave a certain way, to dress a certain way, they worry about whether should they clap between movements.” In the unexpected settings such as hospital waiting rooms, bank lobbies, or prisons where the Ying Quartet performed in Iowa (and occasionally still performs), “people react in more spontaneous ways to the music,” says Ying. “It deepens our appreciation for how music touches our humanity and expresses our humanity.”\nLowry is confident that classical musicians will continue to find audiences both inside and outside the concert hall, and thrive in doing so. He turns to history for perspective, noting that the concert hall has not been the only arena for music.\n“If you were to step back into the 1600s, most of the music that we consider as part of the Western European canon, has its roots in the music of the Church,” he says. “Then, as we move through history, we see vocal music and choral music transform itself into opera, a staged medium, and instrumental music move out into the concert hall.”\nToday, says Lowry, one of the most common stages for musical performance is the computer screen. And it can be a powerful tool for musicians who want to expand their reach.\nLast spring, for example, bassoonists Kara LaMoure ’10E, Eryn Bauer ’10E, Brittany Harrington ’10E, and Lauren Yu ’10E, became a YouTube sensation with a video performance not of a classical piece, but a medley LaMoure had arranged of hits by the pop icon Lady Gaga. Featuring the four bassoonists in Gaga-style platinum blond wigs, short skirts, and heels, the video, which the musicians placed on YouTube primarily for the amusement of their friends, received more than 200,000 hits in a matter of days, showing up on Facebook pages, in band classrooms, and even on a Baltimore Sun blog.\nTo be sure, the bassoonists had been known beforehand for their humor. They are, after all, a “bassoon quartet,” and the bassoon, says Harrington, with its low-pitched buzz, “definitely has a reputation for being goofy.” And they call themselves the Breaking Winds Bassoon Quartet.\nBut LaMoure says she took some lessons from the experience. It reinforced her view that genre is not as useful a concept as it once was. “People should be more fearless about the way that they’re playing music and the kinds of pieces that they’re playing,” she says. “I feel like music should be adapted to any instrumentation.”\nThe Eastman School will celebrate the completion of its new wing with a series of concerts from Dec. 6 through Dec. 12. The celebration will feature student, faculty, and guest artists and will take place in all of the school’s venues, including Hatch Recital Hall, Kilbourn Hall, and Kodak Hall at Eastman Theatre. For more information, visit Eastman’s Web site at www.esm.rochester.edu.\n“I know that we have been reaching some people who really don’t know anything about classical music,” she adds. “So we’re doing some serious stuff, to see if we can get people exposed to that now that we’ve lured them in with our Lady Gaga.”\nBut it’s also true that transcribing and arranging popular music for nontraditional instruments can provide musicians a novel kind of challenge. Harrington, who is playing this year with the Rochester Philharmonic and Syracuse Symphony Orchestras, says “we have a lot more arrangements now that we’ve all done, more than we can get through in one rehearsal.” Those include songs from the iconic 1970s rock group Queen, some show tunes, and even some hip-hop.\nAll of the bassoonists are pursuing graduate study in performance. But beyond that, at a time of artistic, cultural, and technological transformation, their career paths aren’t clear-cut. One thing that will not change, says Ricker, is the need for the drive and initiative that have always been necessary for musicians. Musicians have to be, in a word, entrepreneurial. But Ricker stresses that this has, to some extent, always been true. “Someone once asked me who the first entrepreneur in music was. And I thought: Mozart. Because Mozart was one of the first to not have a regular appointment in the court, working for royalty, or in the Church.” Contrast Mozart to Bach. Bach, Ricker says, “had a church job and he cranked out work every week for that.”\nHardly the vision of Bach cultivated in the concert hall. But a long-standing reality of the business of music, to which the Eastman School—faculty, leadership, and students alike—continues to respond.']	['<urn:uuid:29f2d260-cfda-4ba1-bc78-cf831008b4da>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	5	36	2267
61	effectiveness timing herbicide applications tree heaven colonization capabilities urban rural	Tree-of-heaven herbicide applications must be made in mid- to late summer (July to September) when the tree is moving carbohydrates to the roots, as applications outside this window will only injure aboveground growth. The tree has invaded both urban and rural forests, rapidly colonizing disturbed areas and displacing thousands of acres of native vegetation. It can quickly spread in woodland edges, roadsides, railways, fencerows, and forest openings, particularly taking advantage of areas defoliated by insects or impacted by disturbances.	['Tree of heaven leaves. Credit: Bigstock\nTree-of-heaven, commonly referred to as ailanthus, is a rapidly growing deciduous tree native to a region extending from China south to Australia. It was first introduced into the United States in the Philadelphia area in 1784. Immigrants later introduced tree-of-heaven to the West Coast in the 1850s. It was initially valued as an urban street tree and was widely planted in the Baltimore and Washington, D.C., area. From these areas, tree-of-heaven has spread and become a common invasive plant in urban, agricultural, and forested areas.\nSize: Tree-of-heaven has rapid growth and can grow into a very large tree, reaching heights of 80 to 100 feet and up to 6 feet in diameter.\nBark: The bark of tree-of-heaven is smooth and green when young, eventually turning light brown to gray, resembling the skin of a cantaloupe.\nBark. Photo: Dave Jackson\nLeaves: Tree-of-heaven leaves are pinnately compound, meaning they have a central stem in which leaflets are attached on each side. One leaf can range in length from 1 to 4 feet with anywhere from 10 to 40 leaflets. The leaflets are “lance” shaped with smooth or “entire” margins. At the base of each leaflet are one to two protruding bumps called glandular teeth. When crushed, the leaves and all plant parts give off a strong, offensive odor.\nLeaf. Photo Matt Yancey\nLeaf Margin. Photo: Eric Burkhart\nTwigs: The twigs of tree-of-heaven are alternate on the tree, stout, greenish to brown in color, and lack a terminal bud. They have large V- or heart-shaped leaf scars. The twigs easily break to expose the large, spongy, brown center, or pith.\nBrown Spongy Pith. Photo: Dave Jackson\nLeaf scar. Photo: Dave Jackson\nSeeds: Seeds on female trees are a 1-to-2-inch-long twisted samara, or wing. There is one seed per samara. The samaras are found in clusters, which often hang on the tree through winter.\nSeeds (samaras). Photo: Dave Jackson\nTree-of-heaven is dioecious, meaning a tree is either male or female, and typically grows in dense colonies, or “clones.” All trees in a single clone are the same sex. Female trees are prolific seeders with the potential to produce more than 300,000 seeds annually. The single-seeded samaras are wind dispersed. Established trees continually spread by sending up root suckers that may emerge as far as 50 feet from the parent tree. A cut or injured ailanthus tree may send up dozens of root sprouts. Sprouts as young as two years are capable of producing seed. Tree-of-heaven produces allelopathic chemicals in its leaves, roots, and bark that can limit or prevent the establishment of other plants.\nTree-of-heaven grows almost anywhere, from mine spoil in full sun to fertile, partly shaded, alluvial soils along rivers and streams. Besides urban areas, tree-of-heaven is now found growing along woodland edges, roadsides, railways, fencerows, and in forest openings. Tree-of-heaven is intolerant of shade and cannot compete under a closed forest canopy but will quickly colonize disturbed areas, taking advantage of forests defoliated by insects or impacted by wind and other disturbances.\nThis species is easily confused with some of our native species that have compound leaves and numerous leaflets, such as staghorn sumac, black walnut, and hickory. The leaf edges of these native trees all have teeth, called serrations, while those of tree-of-heaven are smooth. The foul odor produced by the crushed foliage and broken twigs is also unique to tree-of-heaven.\nDue to its extensive root system and resprouting ability, tree-of-heaven is difficult to control. Treatment timing and following up the second year are critical to success. Mechanical methods, such as cutting or mowing, are ineffective, as the tree responds by producing large numbers of stump sprouts and root suckers. When cutting tree-of-heaven is necessary to remove potentially hazardous trees, it is best to treat with an herbicide first, allow 30 days for it to take effect, and then cut.\nHand pulling young seedlings is effective when the soil is moist and the entire root system is removed. Small root fragments are capable of generating new shoots. Seedlings can be easily confused with root suckers, which are nearly impossible to pull by hand.\nTo control tree-of-heaven, target the roots with systemic herbicides applied in mid- to late summer (July to September) when the tree is moving carbohydrates to the roots. Herbicide applications made outside this late growing season window will only injure aboveground growth. Following treatment, repeated site monitoring for signs of regrowth is critical to prevent reinfestation.\nHerbicides applied to foliage, bark, or frill cuts on the stem are effective at controlling tree-of-heaven. Cut stump herbicide applications encourage root suckering and should not be utilized. Apply all treatments no earlier than July 1 up until the tree begins to show fall colors. There are many effective herbicides available for use on tree-of-heaven, including dicamba, glyphosate, imazapyr, metsulfuron methyl, and triclopyr. For most treatments we recommend using herbicides containing the active ingredients glyphosate or triclopyr.\nFoliar herbicide sprays are used where tree height and distribution allow effective coverage without unacceptable contact with nearby desirable plants. Treatments are applied in mid- to late growing season with equipment ranging from high-volume truck-mounted sprayers to low-volume backpack sprayers.\nFor dense or extensive infestations, treat initially with a foliar application to eliminate the small, low growth. Then follow up with a bark or frill application on the remaining larger stems. The initial foliar application will control most of the stems, while the follow-up stem treatment controls missed stems or those too tall for adequate coverage.\nBasal bark applications provide a target-specific method for treating tree-of-heaven that in general is less than 6 inches in diameter. Using a low-volume backpack sprayer, a concentrated mixture of herbicide containing the ester formulation of triclopyr in oil is applied from the ground line to a height of 12 to 18 inches, completely around the stem. To maximize translocation to the roots, apply herbicides from mid- to late summer.\nFrill herbicide applications, called hack-and-squirt, are highly selective with a concentrated herbicide solution applied directly into the stem. For effective hack-and-squirt applications, apply the herbicide solution to spaced cuts around the circumference of the stem. Leaving uncut living tissue between the frill cuts allows the herbicide to move to the roots. Again, make applications in mid- to late summer.\nWell-established tree-of-heaven stands are only eliminated through repeated efforts and monitoring. Initial treatments often only reduce the root systems, making follow-up measures necessary. Persistence is the key to success.\nPrepared by David R. Jackson, forest resources educator.', 'Invasive Tree: Ailanthus altissima\n21 September 2004\nVirginia Cooperative Extension, Charlottesville/Albemarle County Office\n460 Stagecoach Road, Charlottesville, Virginia 22902\nphone: 434.872.4580 fax: 434.872.4578\nMaybe you call it tree-of-heaven, tree-of-paradise, or stink-tree, but its scientific name is Ailanthus altissima. Those who regularly battle Ailanthus have been overheard using some other names as well. The Ailanthus problem is well known among natural resource managers who classify Ailanthus as an invasive exotic: invasive because the tree spreads rapidly and has few insect or disease pathogens and exotic because this species is not native to the U.S. Among invasive exotics, Ailanthus is especially notorious due to its ability to quickly invade disturbed areas and resist control efforts.\nIntroduced from Asia to the U.S. in 1784, Ailanthus was originally used as an urban landscape tree and later to stabilize construction sites. However, like many introduced species, Ailanthus quickly escaped from its intended uses and has since spread to every region of the U.S. Ailanthus is able to withstand pollution and stabilize erosion prone road cuts but that is where the benefits end. Ailanthus has invaded urban and rural forests alike, displacing thousands of acres of native vegetation and offering little or no economic or wildlife benefits in return. And, it is spreading.\nAilanthus can be easily confused with sumac (Rhus typhina) and young black walnut (Juglans nigra). Ailanthus leaves are pinnately compound (i.e., they have multiple leaflets on a single leafstalk). Leaves are typically 12 to 24 inches long and unlike walnut have an odd number of leaflets (between 11 and 25). One to four lobes distinguish the bottom of each leaflet with gland-dots at the tip of each lobe. Twigs and stems are light brown and covered with tan bumps or lenticels. The pith or inner core of the twig is a spongy brown color (compared to the dark brown chambered pith of walnut) and produces the odor from which Ailanthus earns the name stink-tree. Twigs and branches break easily which aids in identifying pith color and odor.\nLike eastern redcedar (Juniperus virginiana) and Virginia pine (Pinus virginiana), Ailanthus is considered a pioneer species, a plant that is generally shade intolerant and grows rapidly on disturbed sites such as fallow agricultural lands, field and forest borders, recently harvested forest stands, road cuts, and right-of-ways. Ailanthus spreads prolifically from seeds, stumps, and roots. Mature female trees produce up to 300,000 windborne seeds each fall. Both male and female trees propagate quickly from root suckers and stump sprouts and can reach 9 to 12 feet in their first year of growth. Seedlings and sprouts rapidly form pure stands by outgrowing surrounding vegetation and by producing an allelopathic compound that suppresses many native woody and herbaceous species.\nCurrent control methods for Ailanthus can be broadly characterized as physical and chemical and may be approached differently depending on the size of the Ailanthus population. Before control measures are undertaken it is crucial to consider how the reclaimed site will be used and that the desired tree species be established soon after Ailanthus is initially controlled. Landowners must always remain vigilant for reemerging populations of Ailanthus.\nPhysical control methods include cutting, removal of roots, burning, and mowing. Cutting alone will not kill Ailanthus. In fact, the tree will vigorously resprout from the roots and stumps and will result in many more stems than were initially cut. Digging roots is practical only for very small patches, as all traces of root must be removed. Similar to cutting, burning and mowing kill only the above ground stems, and trees will resprout from the roots. Because root removal is extremely labor intensive, and burning and mowing are non-selective, cutting is usually the preferred method of physical control. However, cutting is only effective when coupled with chemical control.\nThe best control will result from a late growing season combination of physical and chemical control that begins by cutting trees before they have produced and scattered their seed. For the homeowner with one or a few trees to control, the easiest method is to cut the tree down and apply full strength glyphosate to the top of the cut stump. Glyphosate is sold under the trade name Roundup and others now that the patent has expired. For the landowner with a larger area to manage, two types of herbicide, imazapyr and triclopyr have proven very effective in controlling Ailanthus. Imazapyr is sold under the trade names Arsenal (water-soluble salt) and Stalker (oil base) and triclopyr under the trade names Garlon 3A (water-soluble salt) and Garlon 4 (oil base). Both water and oil based mixtures have benefits and drawbacks. In general, water-soluble mixtures are easier to mix and clean, and break down more quickly in the environment. On the other hand, while more difficult to clean up and longer lasting in the environment, oil mixtures are usually more effective in penetrating bark and leaves.\nTo be safe and effective, herbicide use requires careful knowledge of the chemicals, appropriate concentrations, and the effective method and timing of their application. For more details, read the herbicide label.\nDespite effective cutting, burning, and use of herbicides, Ailanthus can quickly reestablish itself by airborne seed from neighboring properties. You might share this information with your neighbors and encourage them to eradicate their Ailanthus populations as well. You might also consider sharing the costs of equipment, herbicides, and application with your neighbors.\nFor more information on gardening, contact your local Virginia Cooperative Extension office. Many county Extension offices have a help desk that is staffed by Master Gardener volunteers. These volunteers are trained to answer questions about garden and landscape topics. The local Virginia Cooperative Extension office numbers are Albemarle 984-0727, Fluvanna 591-1950, Greene 985-5236, Louisa 540-967-3422, and Nelson 263-4035.']	['<urn:uuid:ec97c53a-6b2b-42c5-b1f2-0f87391240fb>', '<urn:uuid:9b375c7d-f861-4f21-a88b-af260f293f1d>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T19:58:41.790432	10	79	2032
62	when was responsible care initiative launched	Responsible Care was launched in 1985 by the Canadian Chemical Producer's Association	['?Responsible Care? is the chemical industry’s unique global initiative that drives continual improvement in health, safety and environmental (HSE) performance, together with open and transparent communication with stakeholders. Responsible Care was launched in 1985 by the Canadian Chemical Producer’s Association and is now implemented globally through international, national and regional associations such as the International Council of Chemical Associations, American Chemistry Council, European Chemical Industry Council (CEFIC), Association of International Chemical Manufacturers (AICM) and\nEastman adheres to the Responsible Care? Gl?obal Charter, which goes beyond the original elements of Responsible Care?. The Charter focuses on new and important challenges facing the chemical industry and global society. It includes the growing public dialogue over sustainable development and public health issues related to the use of chemical products. The Charter also addresses the need for greater industry transparency and the opportunity to achieve greater harmonization and consistency among the national Responsible Care? programs currently implemented.\nEastman was a leader in the development of the American Chemistry Council’s Responsible Care Codes of Management Practices. We built our Responsible Care Management System on the Codes of Management Practices and our HSES management system. Our management system is comprised of our Responsible Care Pledge, Guiding Principles, procedures, policies and documents that govern our activities, operations and the way we conduct business. Core elements of our system:\nEastman maintains a rigorous product safety review process, including a dedicated Global Product Stewardship and Regulatory Affairs team, which leads intensive product safety reviews to ensure that our products are among the safest and most effective materials on the market.\nEastman is committed to ensuring the highest sustainability and safety standards possible throughout our global operations and supply chain, including transportation safety and logistics optimization across all sites.\nHealth and Safety\nWe continually strive to improve our workplace safety, with an ultimate goal of zero injuries and incidents.\nOur commitment to protecting the environment and our communities start with operational safety. Eastman has extensive documented processes and procedures to prevent potential incidents from occurring and it they do occur, to reduce their impact.\nEastman has an unparalleled commitment to ensure safety both on and off our plant facilities. Through careful planning and diligent practice, our trained emergency responders employ specialized knowledge and equipment to help prevent incidents.\nEastman maintains performance-oriented security programs designed to protect Company facilities, products, intellectual property, information, people on Company property and the public. These programs are consistent with applicable regulations and with sound, industry-accepted security practices.\nEastman’s Global Environmental Affairs team leads the charge for Eastman’s environmental focus. The men and women of Eastman take environmental stewardship to heart.\nWe regularly seek input and openly communicate with citizens and community leaders. In 1990, we developed Community Advisory Panels (CAPs) as part of our Responsible Care? initiative.\nCommunity Advisory Panels\nFor nearly three decades, Eastman has organized Community Advisory Panels in its plant locations. These are committees made up of local citizens and leaders who meet several times annually with Eastman representatives. Our Community Advisory Panel (CAP) members help us ensure we’re keeping the community’s best interests in mind. Currently, we have 12 panels in place globally, representing about 22 percent of Eastman’s total manufacturing locations.\nA key part of HSES management systems is certification by an independent, accredited auditor. As part of our Corporate RCMS certificate, our Corporate Headquarters is annually audited by an accredited, 3rd party auditors.']	['<urn:uuid:2de7af33-77e4-4499-8311-7333c6265f22>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T19:58:41.790432	6	12	564
63	what essential components standards required comprehensive infant sleep study compared limited monitoring	Polysomnography for infants is more comprehensive than a limited sleep study or pneumogram. The process requires careful preparation including caregiver education, appropriate recording environment setup, and laboratory preparation before recording. The study covers infants from 35 weeks conceptional age to 6 months post term, and includes monitoring of sleep states and stages, electroencephalographic arousal, and cardiorespiratory patterns and events. This standardized approach is particularly important for studying SIDS, sleep and breathing disorders, brain development, and early cardiorespiratory functioning.	"[""By David H. Crowell\nChild polysomnography (IPSG) holds nice promise for the examine of SIDS and different sleep and respiring issues, the practical integrity of the constructing mind, and early cardiorespiratory functioning. even though instructions and criteria were constructed for polysomnography, there was no standardized procedural unmarried resource or protocol for IPSG as utilized to babies through the years, beginning with preterm and carrying on with prior 6 months submit time period. until eventually now.An Atlas of little one Polysomnography offers detailed insurance of IPSG for this age diversity with a intensity of picture illustrations you'll find in no different source. The contents conceal the basics of polysomnography corresponding to caregiver schooling, the recording atmosphere, and training of the laboratory sooner than recording. The ebook includes:В·A dialogue of polysomnography unlike a extra constrained sleep examine or pneumogramВ·Directions on find out how to in attaining optimum PSG leads to very younger infantsВ·Reliable strategies for scoring sleep and eventsВ·Waveform examplesUsing examples from the Collaborative domestic toddler computer screen evaluate (CHIME) event, the writer provides suggestions for info acquisition, recording concerns, tracking, sleep kingdom and level definitions, reputation and smoothing, electroencephalographic arousal, and cardiorespiratory styles and occasions. Elucidating approaches and criteria for recording and scoring sleep and sleep-related occasions, An Atlas of boy or girl Polysomnography is a key source for sleep researchers and clinicians who paintings with babies from 35 weeks conceptional age to six months submit time period.\nRead or Download An Atlas of Infant Polysomnography PDF\nSimilar pediatrics books\nNetter's Pediatrics, edited through Drs. Todd Florin and Stephen Ludwig, is a wealthy visible relief with greater than 500 photographs via Dr. Frank Netter and different artists operating in his variety to help you diagnose and take care of young ones with universal scientific stipulations. this can be the 1st time that Netter's drawings of pediatric sickness are introduced jointly in one quantity.\nMany fogeys locate getting their baby to varsity within the morning to be rather a problem. in case your baby continually pleads with you to allow him remain domestic from university, if he skips institution, if his morning regimen is fraught with misbehaviors, or if he indicates indicators of misery and nervousness on the topic of attending university, this booklet can assist.\nRegardless of little ones making up round 1 / 4 of the inhabitants, the 1st variation of this booklet used to be the 1st to target a public future health method of the overall healthiness and illness of kids and adolescents. It mixed medical and educational views to discover the present nation of health and wellbeing of our youngsters, the old roots of the speciality and the connection among early toddler and baby wellbeing and fitness on later grownup healthiness.\nThis accomplished quantity exhibits how one can use either diagnostic and behavioral review knowledgeably and successfully through the strategy of therapy. the 2 traditions have built alongside separate paths--each with its personal conceptual underpinnings and psychometric strengths. Used jointly, they could produce an entire photograph of a kid's or adolescent's wishes and strengths.\n- Lung Surfactant: Cellular and Molecular Processing\n- Food Allergy and Your Child\n- The Big Book of Symptoms: A-Z Guide to Your Child’s Health\n- Molecular biology and pathology of paediatric cancer\n- OB Peds Women's Health Notes\n- Evidence-based otitis media\nExtra resources for An Atlas of Infant Polysomnography\nMiniature antimony pH electrodes for measuring gastro-esophageal reflux. Ann Thoracic Surg 1983;33:491–5 21. Vandenplas Y, Goyvaerts H, Helven R, Sacre L. Gastroesophageal reflux, as measured by 24-hour pH monitoring, in 509 healthy infants screened for risk of sudden infant death syndrome. 1 Example of a continuous signal and a sampled version of that signal. 2 A square wave signal represented in the time and frequency domains. 5 Sensors for detecting infant movement. 6 Example of amplification of a biomedical signal.\nIn some cases in infants this signal is very small and difficult to measure and so alternative methods have been developed. One of these involves monitoring the reflection of a very low intensity infrared illumination of the eye, with the lid either open or closed18. When the eye is stationary the reflected light intensity remains constant, but when the eye is moving there is a time-varying component of the reflected light that is sensed by a photodetector. The infrared light source and photodetector can be housed in a silicone elastomer cap that is placed over one eye and secured by non-allergenic tape.\nAmplification is a method of processing a signal to increase the amplitude by a factor greater than one and this factor is known as the gain of the amplifier. Amplifiers can increase signals to many thousand times their original amplitude to allow them to be observed and/or recorded. If a biomedical signal in the frequency domain is considered, an amplifier should have the same gain for all frequency components of that signal. If this is not the case, the amplified signal will be distorted and will have a waveform different from that of the original signal.""]"	['<urn:uuid:4487b10a-c1ef-4fc4-a22b-5f654d1d3ce2>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	12	78	836
64	What happened to Heraclea during major historical conflicts?	Heraclea surrendered under compulsion to Hannibal in 212 B.C., and later during the Social War its public records were destroyed by fire.	"['He was regarded as the most careful writer on the war with Hannibal, and one who did not allow himself to be blinded by partiality in considering the evidence of other writers (Cicero, De Oratore, ii.\nIt was destroyed by Hannibal in 216 B.C., but restored in 210; in 90 B.C. it served as the Roman headquarters in the Social war, and was successfully held against the insurgents.\nHanoi is 1 For others of the name see Carthage; Hannibal; Punic Wars.\n20) speaks of its wealth and of the to, and an overwhelming force (the Siceliot cities delaying too much in coming to the rescue) under Hannibal took and destroyed the city in 409 B.C.; the walls were razed to the ground; 6000 inhabitants were killed, 5000 taken prisoners, and only 2600 escaped to Agrigentum (Acragas).\nFinally in 218 Hannibal took it and passed on into Italy.\nIn the war with Hannibal, they were among the first to declare in his favour after the battle of Cannae, and it was in their country that Hannibal held his ground during the last stage of the war (at Castrum Hannibalis on the gulf of Scylacium).\n1 It was an important base in the war against Hannibal, and at last refused further contributions for the war.\nThe evacuation of Greece by the Romans gave Antiochus his opportunity, and he now had the fugitive Hannibal at his court to urge him on.\nCornelius Scipio at Magnesia ad Sipylum (190), following on the defeat of Hannibal at sea off Side, gave Asia Minor into their hands.\nThe Allobroges first occur in history as taking part with Hannibal in the invasion of Italy.\nHis name occurs as an element in Carthaginian proper names (Hannibal, Hasdrubal, &c.), and a tablet found at Marseilles still survives to inform us of the charges made by the priests of the temple of Baal for offering sacrifices.\n155b; Hannibal and the Aetolian Thoas take part in the councils of Antiochus III).\nWe hear that Heraclea surrendered under compulsion to Hannibal in 212 B.C. and that in the Social war the public records were destroyed by fire.\nIn the Second Punic War it thrice bade defiance to Hannibal; but in the Social War it was betrayed into the hands of the Samnites, who kept possession till Marius, with whom they had sided, was defeated by Sulla, who in 80 B.C. subjected it with the rest of Samnium.\nOn the 6th of July he beat off a British attack, capturing the "" Hannibal,"" 74.\nOften mentioned during the Punic Wars, it was captured by Agathocles in 310, and was the refuge of Hannibal and the remnants of his army after the battle of Zama in 202.\nIt was a member of the Campanian confederation, and shared the fortunes of Capua, but remained faithful to Hannibal for a longer time; the great part of the inhabitants, when they could no longer resist the Romans, were transferred by him to Thurii, and the town was reoccupied in 211 by the Romans, who settled the exiled inhabitants of Nuceria there.\nAfter the battle of Cannae Crotona revolted from Rome, and Hannibal made it his winter quarters for three years.\nIt successfully resisted the attacks of Hannibal; and it is noteworthy that it continued to strike copper coins even under Augustus and Tiberius.\nThenceforward its position was dependent, and in the Second Punic War, after several vicissitudes, it was depopulated and plundered by Hannibal.\nHANNIBAL, a city of Marion county, Missouri, U.S.A., on the Mississippi river, about 120 m.\nIt is served by the Wabash, the Missouri, Kansas & Texas, the Chicago, Burlington & Quincy, and the St Louis & Hannibal railways, and by boat lines to Saint Louis, Saint Paul and intermediate points.\nMark Twain\'s boyhood was spent at Hannibal, which is the setting of Life on the Mississippi, Huckleberry Finn and Tom Sawyer; Hannibal Cave, described in Tom Sawyer, extends for miles beneath the river and its bluffs.\nHannibal has a good public library (1889; the first in Missouri); other prominent buildings are the Federal building, the court house, a city hospital and the high school.\nThe river is here spanned by a long iron and steel bridge connecting with East Hannibal, Ill.\nHannibal is the trade centre of a rich agricultural region, and has an important lumber trade, railway shops, and manufactories of lumber, shoes, stoves, flour, cigars, lime, Portland cement and pearl buttons (made from mussel shells); the value of the city\'s factory products increased from $2,698,720 in 1900 to $4,442,099 in 1905, or 64.6%.\nHannibal was laid out as a town in 1819 (its origin going back to Spanish land grants, which gave rise to much litigation) and was first chartered as a city in 1839.\nThe town of South Hannibal was annexed to it in 1843.\nClaudius Marcellus defeated the Gauls and won the spolia opima; in 218 Hannibal took it and its stores of corn by treachery.\nOn the strength of a monument bearing his name, it has been surmised that Hannibal was born in Malta, while his father was governor-general of Sicily; he certainly did not die in Malta.\nSolon Hannibal Borglum >>\nNorth-east of Rolla, Phelps county, Hannibal Cave (in Ralls county, about 1 m.\nMONT GENEVRE, a very easy and remarkable pass (6083 ft.) between France and Italy, which is now considered by high authorities to have been crossed by Hannibal, as it certainly was by Julius Caesar, Charles VIII., and in the war of 1859.\nIn 280 B.C. Pyrrhus unsuccessfully attacked its walls; and in the Second Punic War Hannibal was deterred by their strength from attempting to make himself master of the town.\nHannibal Hamlin Joseph H.\nIn 218 B.C. they were attacked by Hannibal, with whose friends the Insubres they had a long-standing feud, and their chief town (Taurasia) was captured after a three days\' siege (Polybius iii.\nThe successors of Strato in the headship of the Lyceum were Lyco, Aristo of Ceos, Critolaus, Diodorus of Tyre, and Erymneus, who brings the philosophic succession down to about z oo B.C. Other Peripatetics belonging to this period are Hieronymus of Rhodes, Prytanis and Phormio of Ephesus, the delirus senex who attempted to instruct Hannibal in the art of war (Cic. De orat.\nC. before Hannibal; but there is some doubt as to the correctness of Livy\'s statement, for, though there were continual wars with the Ligurians, after this time, it is not mentioned again until we are told that in 177 B.C. a Latin colony was founded there in territory offered by the Pisans for the purpose.\'\nRomulus after his ascension declares it to be the will of heaven that Rome should be mistress of the world; and Hannibal marches into Italy, that he may ""set free the world"" from Roman rule.\nThus, Hasdrubal\'s devotion and valour at the battle on the Metaurus are described in terms of eloquent praise; and even in Hannibal, the lifelong enemy of Rome, he frankly recognizes the great qualities that balanced his faults.\nIn the Second Punic War it was occupied by Fabius Cunctator in 217 B.C., taken by Hannibal after a gallant defence by troops from Praeneste and Perusia in the winter of 216-215, but recaptured in the following year, serving the Romans as their base of operations against Capua.\nHannibal is said to have embarked here on his exile from Carthage.']"	['<urn:uuid:a1254f34-b0dd-4b9a-b0ce-9103601689db>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	8	22	1234
65	I'm renovating my house. What angles do I need for crown molding corners?	For inside corners with the molding backside flat against the miter table and top edge against fence: left side needs 31.6° right miter with 33.9° left bevel, while right side needs 31.6° left miter with 33.9° right bevel. For outside corners: left side needs 31.6° left miter with 33.9° right bevel, while right side needs 31.6° right miter with 33.9° left bevel.	['Crown molding is like, well, a crown for a room. It’s one of the first things I notice about a home because it’s such a beautiful accent, and it adds a lot of visual interest. Installing crown molding is a project that I’ve wanted to share with you for some time, and when a friend told me that he was putting crown up at his parents house, I immediately offered to help.\nFor some reason it seems that homeowners shy away from installing crown molding themselves. I think the combination of miter and bevel cuts intimidates the average DIYer. Well this article will put all those fears to rest. I’ll cover how to cut those tricky corners, scarf joints, nailing the crown molding and a few tricks to help along the way. The result is a beautiful, seamless installation that you can brag about to all your friends.\nInstalling Crown Molding\nLet’s start this project by discussing the tools you’ll need to install crown molding. We used:\n- Porter Cable finish nailer\n- 16 ga. finish nails\n- Porter Cable compressor (review)\n- Ridgid compound miter saw (MS1290LZA)\n- Stud finder\n- Ladder / mini scaffold\n- Tape measure\nTip: You can use a coping saw and sandpaper to cut inside corners but we chose to use the miter saw instead.\nBy far the toughest part of installing crown molding is translating the mental image of a corner into accurate cuts. Ridgid recognizes this and included a handy reference on the side of their miter saw (and in the manual) that gives you the correct miter and bevel angles. If you need to work faster, you can achieve just about every cut without making multiple adjustments to the saw, however you’ll need to rotate your workpiece into the correct orientation. For the first time installer, I’d suggest following the basic guide else you may end up with a lot of wasted molding.\nThe easier setup starts with the backside of the molding flat against the miter table and the top edge against the fence:\n- Inside corner, left side: miter 31.6° right, bevel 33.9° left\n- Inside corner, right side: miter 31.6° left, bevel 33.9° right\n- Outside corner, left side: miter 31.6° left, bevel 33.9° right\n- Outside corner, right side: miter 31.6° right, bevel 33.9° left\nHere are settings for the more adventurous installer. With a constant bevel of 33.9° left:\n- Inside corner, left side: Set top edge of molding against fence, miter 31.6° right, save left piece\n- Inside corner, right side: Set bottom edge of molding against fence, miter 31.6° left, save left piece\n- Outside corner, left side: Set bottom edge of molding against fence, miter 31.6° left, save right piece\n- Outside corner, right side: Set top edge of molding against fence, miter 31.62° right, save right piece\nTip: Especially with bevel angles, it’s tough to see exactly where the blade will cut the workpiece. Make your cuts a little longer, and then carefully trim to size.\nTip: Some installers prefer not to miter inside corners. Instead, they’ll run the first piece square against the wall and cope cut the second piece to fit the contours of the molding.\nHere’s a picture of one side of an outside corner.\nCutting Scarf Joints\nScarf joints have nothing to do with keeping you warm in the winter, but they will help your molding look nice. For long straight runs, two pieces of molding overlap at a scarf joint. By overlapping the two pieces and not simply butting the ends together, the transition is almost undetectable. Furthermore, if the molding shifts or shrinks, a scarf joint will remain hidden. We chose to create a 45° overlap scarf joint although many people will opt for a compound angle cut.\nTip: Scarf joints should occur on a stud and it’s not a bad idea to use wood glue on the ends.\nYou can see how the two piece overlap,\nand how they look after we nailed them in place.\nNailing the Crown Molding\nWe secured all the pieces with 16 gauge finish nails through the flat part of the molding. Use the first couple nails to adjust your compressor pressure so that each nail is properly countersunk below the surface of the molding (roughly 80 psi). Nail into a stud whenever possible, and use a stud finder to locate them. If you’ve ever done any home improvement work, you know that walls and corners are never perfect. As we nailed everything in place, we found it necessary to bend the molding to fit tightly against the ceiling and wall. This is a dangerous game as that can make corners and scarf joints even more difficult. To ensure that everything fit together snug, we would only drive a few nails and then dry fit the next piece.\nTip: If you install a backer board that hides behind the crown molding, you won’t need to find studs as you work. You nail the molding to the backboard wherever it’s convenient.\nPictures of Crown Molding Installed\nHere’s how everything looked before we started.\nYou can see how nice it looks even before caulk and touch-up paint.\nTiny gaps like these occur when the walls / ceiling aren’t straight or square. Fortunately, caulk heals many wounds.\nAnd after caulk (without touch-up paint)…\nI think the crown molding looks pretty good. Have you ever installed crown molding? What tips can you offer? Did you go the extra mile and cope cut the inside corners?']	['<urn:uuid:e5794e07-fc44-41d2-92eb-13db2ad66c5a>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	13	62	921
66	How do changes in ocean temperature affect global climate?	Ocean temperatures significantly influence global climate through multiple mechanisms. When surface waters warm, they heat the atmosphere above, allowing it to hold more moisture. This process can act like a 'snow gun,' with prevailing winds carrying moisture to cold continents where it falls as snow. Ocean currents also play a crucial role - when they come in contact with air, they influence heat absorption and release. For example, warm Atlantic currents combined with westerly winds give northwestern Europe higher average temperatures than expected for its latitude. Additionally, phenomena like El Niño, where surface-water temperatures become unusually warm in the eastern Pacific, can alter global weather patterns, causing floods, typhoons, or droughts.	"[""This is a hotly (or perhaps that should be coldly) disputed question. Over the past 50 million years, the Earth's climate has cooled. Large ice sheets formed on Antarctica 35 million years ago, but did not form in the Northern Hemisphere until about 2.7 million years ago. Earth scientists mostly agree that global climatic cooling is associated with decreasing levels of carbon dioxide in the atmosphere. They also generally agree that ice sheets grow only if sufficient moisture is available to produce winter snow, which in is then able to survive the summer heat. So what triggered the onset of the Quaternary ice ages 2.7 million years ago? A trigger mechanism for the onset of glacial conditions seems essential because orbital forcing has always occurred but it has not always induced a strong reaction in the climate system. Why, for example, did glacial stages occur in the Pleistocene but not in the Palaeogene or Neogene? As Walter Wundt (1944) observed, it would seem that only under certain geographical and geological circumstances do orbital variations permit ice ages to make an appearance.\nA precondition of glaciation appears to be a cold climatic background. Now, many factors could cause the atmosphere to become cooler (see Raymo 1994). Key factors seem to be the carbon dioxide (and possibly methane) content of the atmosphere, the arrangement of continents and oceans, and the presence of large mountain ranges, the last two of which affect continental temperatures and potential moisture sources. It is possible that the redistribution of continents and oceans and the growth of mountain ranges through the Cenozoic era led gradually to a cooling of the atmosphere that, by Pleistocene times, was cold enough to permit ice sheets to form during insolation minima. However, early experiments with a general circulation model, run with orbital configurations corresponding to times of rapid ice sheet growth, raised doubts about the ability of Croll-Milankovitch forcings to trigger the growth of ice sheets (Rind et al. 1989). Under none of the orbital configurations was the model able to maintain snow cover through the summer at locations suspected of being sites where major ice sheets began forming, despite reduced insolation during the summer and autumn. The model also failed to preserve a layer of ice, 10 m thick, placed in localities where ice existed during the Last Glacial Maximum. Only by adjusting ocean surface temperatures to their values at the height of the ice age could the model manage to preserve a smallish patch of ice in northern Baffin Island. To David Rind and his colleagues, the experiments brought out a wide discrepancy between the response of a general circulation model to orbital forcing and geophysical evidence of ice sheet initiation, and indicated that the growth of ice occurred in an extremely ablative environment. To explain how ice sheets might grow in an ablative environment requires a more complicated model or else a climatic forcing other than orbital perturbation - a reduction in carbon dioxide content is a possibility.\nStudies of marine plankton (diatoms, coccolithophores, and foraminifera) from the floor of the subarctic Pacific Ocean help clarity the role of moisture supply in initiating Northern Hemisphere ice sheets (Haug et al. 2005). Alkekone unsaturation ratios and diatom oxygen-isotope ratios suggest that, 2.7 million years ago, summers warmed and winters cooled, so inflating the seasonal temperature contrast of the subarctic Pacific Ocean. The warmer summer seas heated the atmosphere, enabling it to hold more moisture. The upshot was that, like 'a snow gun blasting away at ski slopes', prevailing westerly winds blew the moisture onto the cold North American continent where it fell as snow and accumulated as ice (Billups 2005). The cause of the sudden increase in late summer temperatures seems to relate to a change in the mixing between surface and deep ocean waters. Surface waters will not warm if they mix efficiently with cooler and deeper water, a situation that seems to have prevailed up to 2.7 million years ago, as indicated by diatom abundance. After that time, diatom abundance nose-dived, probably because the nutrient supply fed by mixing with deeper waters stopped, at least on a seasonal basis, as a halocline developed. The reduction in vertical mixing of the ocean waters allowed the surface sea to warm during late summer and early autumn, which led to the loading of the 'snow gun' that triggered the onset of the Northern Hemisphere glaciation. Had not the ocean mixing stopped, the obliquity minimum that occurred 2.7 million years ago would have reduced water vapour transport and starved ice-sheets of a snow supply (Haug et al. 2005).\nGeorge Kukla and Joyce Gavin (2004) make a radical proposal to explain the onset of the last glaciation. They contend that the main impact of past orbital changes on climate was in changing the strength of the solar beam in early spring and autumn, and was not, as is the customary view, in varying summer insolation at high latitudes. At the last interglacial-glacial transition, this shift led to a warming of low latitude oceans and cooling of the northern lands. The increased equator-to-pole and ocean-to-land temperature gradients facilitated the poleward transfer of water onto land-based ice. The earlier ocean warming, combined with decreased water vapour greenhouse forcing over land in spring and earlier establishment of snowfields in autumn, led to the growth of ice sheets and to intermittent episodes of accelerated calving. This model only addresses the first 20,000 years of a glacial cycle. It does not explain the full development of a glaciation and the processes precipitating the collapse of the ice sheets some 100,000 years later, although the relative impact of orbital variations, as opposed to the dynamics of ice and ocean currents, might well decrease during the course of an interglacial-glacial succession. In detail, the Kukla-Gavin (2004, 44) hypothesis runs as follows:\n1 During the last interglacial, the climate was quasi-stable and roughly similar to the climate of the current interglacial, with the dominant strength of the solar beam in boreal autumn magnified by high obliquity.\n2 Some 116,000 years ago, the equinoctial seesaw (ESS) - the difference between the strength of the solar beam at the top of the atmosphere at spring and autumn equinoxes - shifted from an autumn mode to a spring mode (Figure 4.3). This led to an increase of the El Niño frequency and intensity at the expense of the La Niña frequency and intensity, and a warming of tropical oceans. At the same time, the northern lands cooled owing to a decreased greenhouse forcing and the earlier growth of seasonal snowfields. Additional moisture for the build-up of high latitude ice came from water bodies that warmed earlier in spring and remained warm later into the autumn. The weaker autumnal insolation intensified the temperature difference between relatively warm waters and cooler lands.\n3 The meridional circulation strengthened in boreal autumn and winter in response to steeper insolation and temperature gradients between colder high latitudes and warmer tropics. Moreover, the large temperature contrast between the oceans and the land caused intensification in the transfer of ocean water onto the land-based ice.\n4 Accumulation of snow in nivation zones increased, favouring the growth of glaciers in high latitudes. The changed distribution of the ice mass accelerated the outflow of ice into the forelands and open ocean. The sea-level dropped.\n5 Episodic surges from glacier margins into the ocean lowered sea-surface temperatures and salinity, which in turn extended the duration of seasonal pack ice and enabled more intense and farther reaching outbreaks of Arctic air (Leroux 1993). These changes affected the thermohaline circulation (Broecker 1991), and might have led to a worldwide alteration of oceanic circulation and encouraged the flip-flop behaviour of local climates (Bond and Lotti 1995).\n6 Eventually, a major outbreak of polar ice into the oceans cooled the subtropical and tropical oceans to such a degree that the meridional temperature and precipitation gradients decreased (Bush and Philander 1998; Broecker 1991), starving the glaciers.\nIn short, this new mechanism for starting up the global refrigeration system has the oceans in low latitudes as the key recipient of the insolation signal. Conventional mechanisms, in contrast, regard the increased albedo and the concomitant drop of land temperature as the only trigger of glacial conditions. Under the new mechanism, the warming of low latitude oceans and the increased temperature gradient between the warmer oceans and cooler land combine with decreased water-vapour greenhouse forcing in autumn and with the earlier establishment of snowfields, leading to the accumulation of polar ice and drop of sea-level. The hypothesis rests upon the correlation of radiometrically dated palaeoclimatic evidence with computed past orbital variations. No other climate model, with one exception (see Clement et al. 1999), yet supports it. Even so, the existing data show that the warming of tropical oceans, the probable increase of global mean temperature, and the growth of polar ice accompanied the past orbital shift, which is qualitatively similar to, but stronger than, current orbital shift. Thus, the current global warming may be a product of both humanmade and natural causes.\nWas this article helpful?"", '2 Temperature & Precipitation Climate: the average weather conditions in an area over a long period of time.Climates are described using average temperature and precipitation.Another way scientists describe climate is by using the yearly temperature range, or the difference between the highest and lowest monthly averages.The factors that have the greatest influence on both temperature and precipitation are latitude, heat absorption and release, and topography.\n3 LatitudeSolar EnergyThe higher the latitude of an area is, the smaller the angle at which the sun’s rays hit Earth is and the smaller the amount of solar energy received by the area is.Because Earth’s axis is tilted, the angle at which the sun’s rays hit an area changes as Earth orbits the sun.\n4 Global Wind PatternsBecause Earth receives different amounts of solar energy at different latitudes, belts of cool, dense air form at latitudes near the poles, while belts of warm, less dense air form near the equator.Winds affect many weather conditions, such as precipitation, temperature, and cloud cover.Thus, regions that have different global wind belts often have different climates.\n5 Land heats faster than water and thus can reach higher temperatures in the same amount of time. Waves, currents, and other movements continuously replace warm surface water with cooler water from the ocean depths.In turn, the temperature of the land or ocean influences the amount of heat that the air above the land or ocean absorbs or releases.The temperature of the air then affects the climate of the area.\n6 Specific Heat and Evaporation specific heat: the amount of energy needed to change the temperature of 1 gram of a substance to 1 degree C.Water also releases heat energy more slowly than land does.\n7 Ocean CurrentsThe temperature of ocean currents that come in contact with the air influences the amount of heat absorbed or released by the air.If winds consistently blow toward shore, ocean currents have a strong effect on air masses over land.For example, the combination of a warm Atlantic current and steady westerly winds gives northwestern Europe a high average temperature for its latitude.\n8 El Niño Southern–Oscillation (ENSO) El Niño: the warm-water phase of the ENSO; a periodic occurrence in the eastern Pacific Ocean in which the surface-water temperature becomes unusually warm (ENSO).The event changes the interaction of the ocean and the atmosphere, which can change global weather patterns -> causes floods, typhoons, or droughts.The ENSO also has a cool-water phase called La Niña, which also affects weather patterns.\n9 Seasonal WindsMonsoon: a seasonal wind that blows toward the land in the summer, bringing heavy rains, and that blows away from the land in the winter, bringing dry weather.Temperature differences between the land and the oceans sometimes cause winds to shift seasonally in some regions.\n10 TopographyElevationThe surface features of the land, or topography, also influences climate.The elevation, or height of landforms above sea level, produces distinct temperature changes.Temperature generally decreases as elevation increases.\n11 Rain ShadowsWhen a moving air mass encounters a mountain range, the air mass rises, cools, and loses most of its moisture through precipitation.As a result, the air that flows down the other side of the range is usually warm and dry. This effect is called a rain shadow.One type of warm, dry wind that forms in this way is a the foehn (FAYN), a dry wind that flows down the slopes of the Alps.\n13 Climate ZonesEarth has three major types of climate zones: tropical, middle-latitude, and polar.Each zone has distinct temperature characteristics, including a specific range of temperatures.Each of these zones has several types of climates because the amount of precipitation within each zone varies.\n14 Tropical Climatetropical climate: a climate characterized by high temperatures and heavy precipitation during at least part of the year; typical of equatorial regions.These climates have an average monthly temperature of at least 18°C (65◦F), even during the coldest month of the year.Within the tropical zone, there are three types of tropical climates: tropical rain forest, tropical desert, and savanna.\n16 Middle-Latitude Climate Middle-latitude climate: a climate that has a maximum average temperature of 8°C (46F) in the coldest month and a minimum average temperature of 10°C (50F) in the warmest month.There are five middle-latitude climates: marine west coast, steppe, humid continental, humid subtropical, and Mediterranean.\n18 Polar Climatepolar climate: a climate that is characterized by average temperature that are near or below freezing; typical of polar regions.There are three types of polar climates: the sub arctic climate, the tundra climate, and the polar icecap climate.\n20 Study Climate ChangeClimatologist: a scientist who gathers data to study and compare past and present climates and to predict future climate change.Climatologists use a variety of techniques to reconstruct changes in climate.\n21 Potential Causes of Climate Change Factors that might cause climate change include the movement of tectonic plates, changes in the Earth’s orbit, human activity, and atmospheric changes.\n22 Plate TectonicsThe movement of continents over millions of years caused by tectonic plate motion may affect climate change.The changing position of the continents changes wind flow and ocean currents around the globe.These changes affect the temperature and precipitation patterns of the continents and oceans.\n23 Orbital ChangesChanges in the shape of Earth’s orbit, changes in Earth’s tile, and the wobble of Earth on its axis can lead to climate changes.The combination of these factors is described by the Milankovitch theory.Each change of motion has a different effect on climate.\n25 Human ActivityPollution from transportation and industry releases carbon dioxide, CO2, into the atmosphere.Increases in CO2 concentration may lead to global warming, an increase in temperatures around the EarthBecause vegetation uses CO2 to make food, deforestation also affects one of the natural ways of removing CO2 from the atmosphere.\n26 Volcanic ActivityLarge volcanic eruptions can influence climates around the world.Sulfur and ash from eruptions can decrease temperatures by reflecting sunlight back into space.These changes last from a few weeks to several years and depend on the strength and duration of the eruption.\n27 Global WarmingGlobal warming: a gradual increase in the average global temperature that is due to a higher concentration of gases such as carbon dioxide in the atmosphere.Global temperatures have increased approximately 1°C over the last 100 year.Researchers are trying to determine if this increase is a natural variation or the result of human activities, such as deforestation and pollution.']"	['<urn:uuid:901f1d38-61cd-4dd5-928e-ba1fc36ab650>', '<urn:uuid:99bac074-d46d-4e10-9985-5226c0ec119f>']	open-ended	direct	concise-and-natural	distant-from-document	three-doc	novice	2025-05-12T19:58:41.790432	9	111	2567
67	eco building materials construction waste effects	Green buildings address both construction materials' environmental impact and waste management. During construction, materials like recycled stone, renewable bamboo, and locally sourced products are preferred to reduce greenhouse gas emissions, which account for 20% of a building's lifetime emissions. The Longwood Center recycled over 98% of construction waste and used over 28% recycled materials. Materials are selected to be non-toxic, reusable, and renewable where possible, with preference for local manufacturing to reduce transport energy. New innovations like green cement that sequesters CO2 and hemp-based wood alternatives are being developed to further reduce environmental impact.	"[""Moseley Architects - Virginia Beach; Hastings+Chivetta Architects\nGreen Judges' Choice Winner 2008 Green Education Design Showcase\n||Facility Use: College/University 4-Year Institution\nProject Type: Addition\nCategory: Green Design\nLocation: Farmville, VA\nDistrict/Inst.: Longwood University\nCompletion Date: August, 2007\nDesign Capacity: 2,211 students\nEnrollment: 4,800 students\nGross Area: 74,683 sq.ft.\nSpace per pupil: 34 sq.ft.\nSite size: 3 acres\nCost per student: $7,100\nCost per sq.ft.: $210.18\nTotal project cost: $15,697,000\nBuilding construction cost: $9,523,580\nSite development cost: $4,137,420\nFurniture & equipment cost: $286,000\nFees and other: $1,500,000\nLongwood University Health & Fitness Center\nThe new Health & Fitness Center at Longwood University, now in its second year of operation, provides a green recreational experience for the University’s faculty, staff, and students. Intended to consolidate recreational spaces that were once dispersed across the campus and to expand the University’s health and wellness programs, the Center not only provides recreational opportunities for the University community, but it also serves to lessen the environmental impacts of development and construction.\nPresenting a contemporary take on the campus’ traditional architecture, the 74,650 square foot building features a combination of brick and stone with a glass curtain wall and ample windows that suffuse the building’s fitness areas with daylight. Efforts to improve indoor air quality in the building create healthier surroundings where students can breathe easy as they use the facility’s multi-purpose and two-court gymnasiums, fitness center with climbing wall, exercise rooms, racquetball courts, and indoor track.\nDuring planning, the project team referred to the LEED Green Building Rating System to guide design decisions. Energy efficiency, water use reduction, indoor environmental quality, and waste minimization were given top priority. Once expected to earn basic LEED certification, the new facility surpassed the University’s and the project team’s expectations to earn LEED Gold. Features that contributed to the facility’s LEED Gold certification include: mechanical system and building envelope improvements to maximize energy efficiency; renewable energy; CO2 monitoring; efficient plumbing fixtures to reduce water use; recycled and regionally manufactured materials to conserve natural resources; and low-emitting materials and IAQ testing for optimum indoor air quality.\nIn a building with large, open interior spaces, energy efficiency can be a challenge, but the design team recognized and met the challenge through clever design strategies. The energy model completed for the project showed that the facility is expected to use 43 percent less energy (by cost) than a standard facility of the same type and size. One of the features that contribute to this savings is energy recovery through the use of enthalpy wheels, which recover sensible and latent energy from building exhaust air. The building envelope has been enhanced to include a larger amount of roof insulation, more efficient wall insulation, and high-efficiency glazing. Carbon dioxide sensors allow for demand-controlled ventilation, signaling the HVAC system to provide a greater amount of ventilation in occupied areas and helping occupants to received adequate air, while ensuring that unoccupied spaces don’t receive more ventilation than is necessary. Carbon dioxide sensors have been installed in spaces with highly variable occupancy, such as gyms and exercise rooms for the most efficient operation of building systems.\nIn addition to the Health & Fitness Center’s many energy-saving features, a portion of the Center’s energy is provided by the University’s central steam plan, and approximately 11 percent of this energy (by cost) comes from the burning of sawdust, a renewable resource that would otherwise go to waste.\nIn order to conserve water within the facility and reduce water use by at least 40 percent, water-efficient plumbing fixtures were installed. These fixtures include waterless urinals, low-flow showers, and ultra low-flow lavatories. By reducing water use, these fixtures also contribute to energy conservation; with efficient plumbing fixtures, less water leaves the building and requires treatment at a treatment facility.\nDuring construction of the facility, the project team was determined to use resources efficiently by recycling construction waste and incorporating recycled and regionally manufactured materials. By developing and implementing a Construction Waste Management Plan, the contractor was able to recycle over 98 percent of the waste generated during construction. Products made from recycled materials contributed to over 28 percent of the total materials used on the project, and over 54 percent of the materials used were manufactured within 500 miles of the project site. Recycling waste, using recycled materials, and obtaining materials that were manufactured regionally further contributes to energy conservation by reducing or eliminating the need to harvest virgin materials for manufacturing and to transport materials long distances.\nAnother way in which the project helps to conserve and utilize energy efficiently is through improvements to the project site. The facility’s location provided important advantages for making it green. The site chosen for the project was an existing parking lot on the University’s campus. The lot was demolished, the asphalt and concrete were recycled, and the new facility was constructed. Furthermore, the parking was not and will not be replaced, resulting in a net reduction in parking on the site. The location of the building also provides opportunities for utilizing alternative means of transportation, as it is located within a convenient walking distance of two bus stops on the Farmville Area Bus blue and red routes. By encouraging the use of alternative forms of transportation, the facility helps to reduce the energy use and pollution associated with driving. Another important aspect of the site is the use of highly reflective concrete to reduce the localized, artificial rise in temperatures known as the Heat Island Effect. The Heat Island Effect can lead to increased dependence on HVAC systems, so steps to reduce the effect can contribute to decreased cooling loads.\nA topic of great importance in a recreational facility is indoor environmental quality, which encompasses air quality and occupant comfort. Efforts to protect indoor air quality inside the building began during construction with the development and implementation of a Construction Indoor Air Quality Management Plan. In accordance with this plan, materials such as drywall, insulation, and ceiling tile were protected from dirt and moisture while being stored on site, and the open ends of installed ductwork were covered to prevent contamination. All adhesives, sealants, paints, carpet and composite wood materials were selected to decrease the amount of volatile organic compounds (VOCs) emitted and circulated through the building’s air supply. Prior to occupancy, testing was conducted to ensure levels of VOCs and other particulates in the air were suitable for a building with better-than-average indoor air quality. As the University’s first LEED project, the Longwood Health & Fitness Center has only increased the enthusiasm for sustainability among the Longwood University community. The facility serves as the keystone for the University’s efforts to integrate principles of sustainability throughout the campus, and the University is using the facility as a model for other projects.\n|1) Control of Institution: Public|\n2) Type of Institution: Traditional\nMethodology & Standards:\n|Primary Source: Primary Source: State Appropriations|\nProject Delivery Method(s):\n|Principles Followed: LEED|\nCertifications Obtained: LEED Gold\nSite Selection and Development: Site Selection; Heat Island Reduction\nWater Conservation: Water Conservation\nEnergy Efficiency and Conservation: Energy Efficiency; Building Automation/Energy Management Systems; Alternative Energy Source\nMaterials Use: Recycling/Reuse; Sustainable Materials Selection\nIndoor Environmental Quality: Indoor Air Quality\nCommissioning: Building/systems have been commissioned\nAssociated Firms and Consultants:\n|Landscape Architecture: Draper Aden Associates|\nStructural Engineer: Dunbar Milby Williams Pittman & Vaughan, PLLC\nCivil Engineer: Draper Aden Associates"", 'Green Buildings are Eco Friendly Structures\nWhen the pre historic man constructed a hut for the first time using bamboo trees and coconut leaves to protect himself from sun and rain, he was starting to exploit nature for his humble needs. Apart from killing the trees he also disturbed the natural habitat of the insects and birds in those trees and interfered in the cycles of nature. That was a beginning.\nNow, it is beyond imagination, how much damage has been inflicted on earth by the construction of various types of buildings using sand and water from the rivers, stones from the mountains, cement manufactured from the ingredients dug from the land. In addition, carbon emission from the buildings and manufacture of construction materials warm up the air and space.\nBut, after getting conscious about the environment and after feeling the environmental responsibilities, the way our people try to address this problem is wonderful. One of the intelligent initiative is the concept of “Green Buildings”.\nThe concept of Green Buildings envision a new approach to save water, energy and material resources in the construction and maintenance of the buildings and can reduce or eliminate the adverse impact of buildings on the environment and occupants.\nBy preferring Green Building over a conventional building we help this planet earth and the people to retain nature to a maximum extent possible in three ways with reference to the location of the buildings.\n1. Retain the external environment at the location of the building.\n2. Improve internal environment for the occupants\n3. Preserve the environment at places far away from the building\nGreen Buildings Retain the Environment at the location of the Building.\nSuppose we propose a multi-storied office complex to accommodate thousands of officers and staff, it requires a vast area. Therefore selection of a site for such a building complex should consider retention of local vegetation, wild life, natural water courses etc. Either a site with bio diversity should be avoided or the building should be planned to reduce site disturbance.\nLand :The landscaping and the exterior design in a green building shall be in such a way that there is more shaded area, the light trespass is eliminated and local species of plants are grown.\nWater : The green building by its design and shape shall not disrupt the natural water flows, it should orient and stand just like a tree. Rain falling over the whole area of the complex shall be harvested in full either to replenish the ground water table in and around the building or to be utilized in the services of the building. The toilets shall be fitted with low flesh fixtures. The plumbing system should have separate lines for drinking and flushing. Grey water from kitchenette, bath and laundry shall be treated and reused for gardening or in cooling towers of air conditioning.\nEnergy: The solar energy at the top of a green building is harvested to supplement the conventional energy,. The natural light is harvested in the intermediate floors to minimize the usage of electricity. Sunlight is restricted by the high grown trees outside the lower floors of the building. High efficiency light fixtures make a pleasant lighting apart from saving the energy. High-efficiency windows and insulation in walls, ceilings, and floors are used for the benefit of better temperature control.\nGreen buildings improve internal environment for the occupants\nLight: In a designed green building the occupants shall feel as if they are in outdoor location. The interior and exterior designs shall go hand in hand by blending the natural and artificial lighting and presenting transparent views wherever possible.\nAir: In the air conditioned environment, a green building shall be specially equipped to ensure the Indoor Air Quality for a healthy atmosphere. Even the nasal feelings shall be pleasant free from the odour of paints and furnishings.\nA comfortable atmosphere at work stations improve the attendance of the staff and increase the productivity.\nGreen buildings preserve the environment at places far away from the buildings.\nWe all know that a building is constructed using cement, sand, steel, stones, bricks, and a lot of finishing materials. These materials are quarried or procured from far way from the location of the buildings. Building materials are responsible for about 20 percent of the greenhouse gasses emitted by a building during its lifetime,.\nGreen buildings shall use the products that are non-toxic, reusable, renewable, and/or recyclable wherever possible. Locally manufactured products are prefered so that the collective material environment of the locality remains a constant and moreover the fuel for the transport of materials is saved.\nAs we see, our food and domestic products are tagged with green as a fashion of eco-friendly practices, building materials are also going green. The futuristic green buildings are to use green materials which are in research stage now.\nGreen wood : A Stanford team has done a research for wood alternate. Hemp fibers and biodegradable plastic when pressed together and heated form layers and this material is as strong as wood. When buried in land fill, it degrades faster. This wood creates more raw materials when it breaks down. Microbes produce methane gas when they decompose this wood substitute and other debris thrown into landfills. Another type of bacteria absorbs this gas and turns it into plastic that can be used to create a new wooden plank. By this cycle, there is a continuous source of raw material for this wood. When this material at research comes to market, it may help to control deforestation and promote the rainfall.\nGreen Cement: Bruce Constantz at Calera, based in Los Gatos, has developed a green method to produce both cement and aggregate, another component of Concrete. Their method sequesters Carbon Di Oxide from power plant flues and mixes the gas with sea water to produce the mineral raw materials of concrete. For every ton of green cement Calera manufactures half a ton of fly ash from coal plants is used apart from preventing production and emission of Corbon Di Oxide.\nOther Green Building materials: Renewable plant materials like bamboo (because bamboo grows quickly) and straw, lumber from forests ecology blocks, dimension stone.recycled stone, recycled metal are some of the other materials used in a Green Building.\nLEED GREEN BUILDINGS RATING SYSTEM\nThe Buildings constructed based on the Green concepts should confirm to the prescribed standards. There should be continuous assessment and monitoring from the planning/design stage up to the completion of construction, for declaring a building as a Green Building.\nFor this, LEED ( Leadership in Energy and Environmental Design ) Green Building Rating system is followed. In this system points are awarded for adopting Green concepts in various categories and the Buildings are certified Green at levels such as Silver, Gold or Platinam based on the total number of points they get in LEED Rating.\nMeasurement / Audit for Green Concept in Buildings\nCategories for Rating LEED points\nSustainable site 12\nWater efficiency 8\nEnergy and atmosphere 17\nMaterial and resources 13\nIndoor Environment Quality 15\nDesign process and innovation 4\nEmploying LEED designer 1\nTotal points 70\nCertification for Green Buildings\nLevel Points required\nLEED Certified 26 to 32\nSilver Level 33 to 38\nGold Level 40 to 52\nPlatinam Level 53 and above\nThere are only few Certified Green Buildings in India. But it is good to know that the awareness is gathering momentum and we look forward for a greener future.\nUnpolluted Water, Soil, Mountains and thick Forests\nare the Forts defending a Land\n( Thiruvalluvar 742)\nWritten by Malarthamil on the eve of observing World Environment Day on June 5\nMalarthamil is a civil engineer and writer-poet inspired by Thirukkural – a classical Tamil poetry that expounds various aspects of life.']"	['<urn:uuid:40623361-1432-4177-8f10-b7fabb4636cf>', '<urn:uuid:a775abb6-67ac-4548-9bb0-9d98a3f6edf8>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T19:58:41.790432	6	94	2514
68	climate change education winchester vs deloitte australia approach compare	While both organizations address climate change, their approaches differ. The University of Winchester's Centre for Climate Action focuses on education, awareness, and behavior change through research, teaching, and public engagement. In contrast, Deloitte Australia takes a business-oriented approach, focusing on reducing their carbon footprint with specific emission reduction targets and working with clients and industries to manage climate transition.	"[""Centre for Climate Action (Climate Change Education and Communication)\nWorking towards a better world by contributing to the advancement of climate change awarenessView content\nLaunched in April 2018, the Centre for Climate Action (Climate Change Education and Communication, or CCEC) is an ambitious interdisciplinary and collaborative centre aiming to be a home and catalyst for action on CCE) at the University of Winchester. We undertake research and teaching in the area of CCEC and organise public engagement events.\nThe University of Winchester is a values-driven institution; a key part of the University's values and Strategic Plan is a focus on raising climate change awareness and we aim to become an international Centre of Excellence for CCEC. Find out more about our values.\nOur membership is drawn from across the University, harnessing our expertise in climate change, education and responsible management. We work closely with a range of external partners and we support PhD students. To meet the Centre team and our partners, see below.\nAims and objectives\n- To explore the integration of Climate Change Education and Communication\n- To raise awareness of the urgency of climate change\n- To create the desire to change behaviour\n- To create the environment for personal and collective action to adapt and mitigate for the risks of climate change\nThe centre will do this by acting as a hub for CCEC research, stimulating conversations to set the agenda and drive change, and by developing projects to embed the centre within the University's programmes and culture.\nWhole Earth Exhibition\nTo mark the launch of the Centre on 20 April 2018, the University hosted a hard-hitting outdoor exhibition showcasing the sustainability issues facing the earth and how scientists are trying to fix them. The launch was part of the Whole Earth Festival, which included a welcome from Vice-Chancellor Prof. Joy Carter (an environmental scientist herself), a talk by photographer Mark Edwards, the founder of Hard Rain, and a panel discussion.\nThe exhibition 'Whole Earth? Aligning human systems and natural systems' presents the earlier 'Hard Rain: Our Headlong Collision with Nature' exhibition along with new material to create a new tool for educators to develop understanding of sustainable development. It hopes to encourage young people, students and universities to consider their role in making society more sustainable.\nYou can read about the background to Whole Earth? and the Hard Rain Project here.\nMeet the team\n- Dr Tammi Sinha, Centre Director and Senior Lecturer in Project & Operations Management\n- Prof. Carole Parkes, Professor of Responsible Management\n- Tim Burgess, Senior Lecturer in Education\n- Dr Darren Jeffers, Post-Doctoral Fellow in Physical/Environmental Geography\n- Steve Hallett, Project Coordinator, Energy and Environment\n- Anthony Courtney, Education for Sustainable Development Coordinator\n- Dr Savithri Bartlett, Senior Fellow of Knowledge Exchange\n- Dr Martina Hutton, Senior Lecturer in Marketing\n- Prof. Denise Hewlett, Professor of Knowledge Exchange\n- Dr Simon Boxley, Senior Lecturer in Education\n- Claire Lorrain, Head of Continuous Improvement Unit\n- Francesco Caputo, Lean Consultanct, Continuous Improvement Unit\n- Charmaine Martin, Student Fellow\n- Susanne Clarke, PhD Student\n- Claire Lorrain, PhD Student\n- Quinn Runkle, NUS Improvement Community of Practice Member\n- Nigel Ward, Improvement Community of Practice, Southampton Solent University\nThe Centre works closely with the following organisations and campaigns:\nWinACC can lay claim to being one of the most active local climate action groups in the UK. The organisation is hosted and supported by the University.\nHampshire Climate Action Network\nLed by WinACC, Hampshire Climate Action Network is an informal way for local groups to share ideas and collaborate. Meetings take place every six months; 'Hampshire' includes Southampton and Portsmouth.\nThe University of Winchester Business School is a PriME signatory and Champion.\nWe are a member of the University's Sustainable Futures Steering Group\nDriving sustainability across campuses, curriculums and communities."", 'Deloitte Australia believes climate change is one of the biggest shared challenges facing humanity. We believe in the science of climate change and the economic and commercial risks inaction poses to our society, economy, communities, and businesses.\nAs business leaders, policy influencers and individual citizens, we recognise the need for comprehensive action. Doing nothing is not an option, and it is not without cost. There will be tangible long-term benefits by acting now.\nOur commitment to change\nTaking tangible climate action is an absolute priority for us. We’re acting on this through reducing our carbon footprint, supporting our clients and using our voice.\nDeloitte’s near-term (2030) greenhouse gas (GHG) reduction goals have been validated by the Science Based Targets initiative (SBTi) as 1.5°C-aligned, science-based targets. Deloitte has also committed to set long-term emissions reduction targets using the SBTi’s Net Zero Standard.\nOur near-term goals are to:\n- Reduce absolute Scope 1 and 2 GHG emissions 70% by 2030 from a 2019 base year.\n- Reduce Scope 3 GHG emissions from business travel 50% per FTE by 2030 from a 2019 base year.\n- Engage with our major suppliers with the goal of having 67% (by emissions) set science-based targets by 2025.\n- Invest in meaningful market solutions for emissions we cannot eliminate.\nOur additional 2030 goals to reduce emissions include:\n- Sourcing 100% renewable energy for our buildings\n- Converting 100% of our fleet to hybrid and electric vehicles\nDeloitte’s near-term goals (2030) were validated in advance of the issuance of the SBTi Net-Zero Standard. Deloitte’s next step will be to set long-term emissions reduction targets following the SBTi standard to continue playing our part in achieving a net-zero world.\nAs a professional services firm, we work with our clients through the process to achieve sustainable operations and responsible climate choices.\nIn working with organisations, we take the following positions:\n- We will actively work with new and emerging low emissions intensive industries and businesses to help them establish themselves in crafting a new system of production in Australia\n- We will actively advise and work with high emissions intensive businesses to effectively manage their way through this major transition\n- We will work with regulators and governments to ensure that best practice regulation and policy is in place to support Australia’s economic growth, jobs, and as a leader in decoupling the economy from emissions\n- We will actively work with all sectors to help them understand the physical impacts of climate change and develop adaptation and response pathways, and\n- We will actively work with other businesses, communities, and governments to ensure that our economy makes a responsible and equitable transition to a new future.\nWe will participate in the discussions on government policies and actions on climate change that are in the best interests of a sustainable and prosperous future. Policies aimed at strengthening economic growth can support low-emission pathways; and actions to stimulate investment in low-emission investments can strengthen economic growth and job creation.\nAs a global firm, we will draw on our vast experience with business, communities, governments and regulators, alongside our research and analytical expertise to contribute a balanced debate on climate change.']"	['<urn:uuid:daf50450-65bf-4437-bc26-c0837a6045ea>', '<urn:uuid:d63cf17d-1e7f-4566-8cc4-b19d16392cd4>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T19:58:41.790432	9	59	1165
69	Being interested in both historical mapping and marine biology, I'd like to know how government institutions like NOAA handle both historical preservation and modern environmental challenges - what are some examples of their work in preserving historical maps and monitoring coral reef health?	NOAA works to preserve important historical artifacts like the 1861 'slave density map' which they presented to Lincoln's Cottage for educational programs. In terms of modern environmental monitoring, they study critical marine ecosystems including coral reefs, where they track bleaching events that now occur nearly every year and research survival mechanisms like colorful bleaching, which involves corals producing protective pigments in response to stress.	['The “slave density map,” created by the men of U.S. Coast Survey in 1861, is one of Coast Survey’s most treasured historical maps. Artist Francis Bicknell Carpenter included it in his painting, “First Reading of the Emancipation Proclamation of President Lincoln,” because Lincoln consulted it so often in devising his military strategy. According to Carpenter, President Lincoln used the map in his decisions to send his armies to free blacks in some of the highest density areas in order to destabilize Southern order.\nFrancis Bicknell Carpenter placed the “slave density map” in the lower right corner of his painting of the Emancipation Proclamation.\nPresident Lincoln’s Cottage, now maintained by the National Trust for Historic Preservation, is where President Lincoln developed the Emancipation Proclamation. So it was fitting that, on Lincoln’s birthday this year, NOAA’s Office of Coast Survey presented a copy of the map to Cottage officials, to assist with their vital educational programs.\nIn the very library where Lincoln may have studied the map, Coast Survey’s Dawn Forsythe (left) and NOAA’s Ben Sherman (right) presented the map to Erin Carlson Mast, the Cottage’s executive director, and Callie Hawkins, associate director for programs.\nDawn Forsythe (Coast Survey), Erin Carlson Mast and Callie Hawkins (Lincoln’s Cottage), and Ben Sherman (NOAA) with a copy of the slave density map in the Lincoln Cottage library.\nThe Cottage plans to use the map in their educational programs. To learn more about the map, see Mapping Slavery in the Nineteenth Century.\nThe men of Coast Survey created the map to help the public understand the secession crisis, by providing a visual link between secession and slavery.\nExplore your world and learn how NOAA — the National Oceanic and Atmospheric Administration — takes the pulse of the planet every day and protects and manages ocean and coastal resources.\nJoin us on NOAA’s Silver Spring, Maryland, campus for a day of discovery. Listen to engaging talks by NOAA experts, explore interactive exhibits, take special tours, and have fun with hands-on activities for ages 5 and up. Meet and talk with scientists, weather forecasters, hurricane hunters, cartographers, and others who work to understand our environment, protect life and property, and conserve and protect natural resources.\nThe Silver Spring campus is at 1315 East-West Highway, next to the Silver Spring Metro Stop (Red Line). Public parking is available.\nNOTE: A government-issued photo ID is required for adults. Check NOAA Open House for a list of acceptable forms of identification.\nVisit www.noaa.gov/openhouse for details or call 301-713-7258 for more information.\nCold Bay Elementary School students visit the NOAA Ship Rainier\nOn September 13, NOAA Ship Rainier began surveying Cold Bay, its fourth project of the summer. Cold Bay is a small town on the Aleutian Peninsula approximately 540 miles southwest of Anchorage, Alaska. The town currently has approximately 88 full-time residents and boasts an airport with one of the longest runways in Alaska.\nOn September 19, after deploying her launches for the day, officers and crew welcomed aboard the entire Cold Bay Elementary School – all eight students, teaching assistant Mrs. Lyons, and their teacher, Mrs. Burkhardt. The students are currently between fourth and seventh grade and go to school in a state-of-the-art, two-room school-house.\nDuring the tour, the students learned about driving the ship and making nautical charts. They saw how sonars work, and they even used a sediment sampler to determine the seafloor composition.\nThe students were full of questions and enjoyed learning about life on a ship. They also captured the admiration of Rainier‘s commanding officer. “When Cold Bay residents describe their town, they can also boast of wonderful elementary school students who have a desire to explore new things,” explained Cmdr. Rick Brennan. “One of the great things about working on a NOAA ship is the opportunity to meet students like this. Combining our love of the sea with their enthusiasm for learning — that’s where America’s future hydrography starts.”\nThis student is ready to work!\nThe group examines bottom samples collected by the Rainier.\nCmdr. Rick Brennan explains how davits work.\n- Cmdr. Brennan with friends — and potential future hydrographers.', 'Cecilia D’Angelo, a molecular coral biology lecturer at the University of Southampton. Jörg Wiedenmann, Elena Bollati & Cecilia D’Angelo/University of Southampton, Palawan colourful bleaching image by Ryan Goehrung/University of Washington\nBleaching events used to be few and far between, but they now occur nearly every year. After the coral is exposed, it often breaks down and dies, altering the ecosystem for the diverse array of life that relies on it. Corals stand little chance of bouncing back from these events — but a new study suggests they have an unusual survival method: taking on a vibrant neon color.\nWhen bleaching events occur, extended heat spikes cause corals to turn a ghostly white, often leading to their death. Even slight increases in annual ocean temperatures can wreak havoc on this relationship, expelling the algae from the coral’s tissue and exposing its white skeleton. These corals can still undergo some of their normal functions for a short period of time as they hope their algae come back — whereas drastic changes in ocean temperature almost always lead to coral death. Reports of colorful bleaching during the most recent mass bleaching event in the Great Barrier Reef in March and April gave scientists hope that patches of the system have a chance to recover. “Bleaching is not always a death sentence for corals, the coral animal can still be alive,” said Dr. They found that colorful bleaching events occur when corals produce “what is effectively a sunscreen layer” on their surface to protect against harmful rays and create a glowing display that researchers believe encourages algae to return. The Ocean Agency describes the process as a “chilling, beautiful and heartbreaking” final cry for help as the coral attempts to grab the algae’s attention. “Our research shows colorful bleaching involves a self-regulating mechanism, a so-called optical feedback loop, which involves both partners of the symbiosis,” lead researcher Professor Jörg Wiedenmann of the University of Southampton said in a press release. Climate Change\nDying coral reefs turn vibrant neon in apparent survival effort\nScientists say climate change is turning coastal Antarctica green\nStudy: Climate change makes a Dust Bowl heat wave more likely\nAir pollution is already spiking in China with virus lockdown lifted\nIndia’s carbon emissions fall for the first time in four decades\nMore in Climate Change\nResearchers at the University of Southampton’s Coral Reef Laboratory studied 15 colorful bleaching events worldwide between 2010 and 2019 — including one in the Great Barrier Reef, the world’s largest coral reef system — and recreated those ocean temperatures in a lab. But “colorful bleaching” has the opposite effect: the dying corals gain more pigment, and glow in shades of bright pink, purple and orange. Scientists first spotted the mysterious neon coral a decade ago, but they had been unable to figure out why it occurred. As the recovering algal population starts taking up the light for their photosynthesis again, the light levels inside the coral will drop and the coral cells will lower the production of the colorful pigments to their normal level.”\nColorful bleaching Acropora corals in the Phillipines. Dying coral reefs turn vibrant neon colors in apparent last-ditch effort to survive\nBy Sophie Lewis\nMay 22, 2020 / 4:48 PM\n/ CBS News\nScientists use mini-satellites to save coral reefs\nFor years, coral reefs around the world have been devastated by mass bleaching events as the oceans continue to warm due to climate change. “If the stress event is mild enough, corals can re-establish the symbiosis with their algal partner.”\nThe internal changes that allow colorful bleaching to occur. In 2017 alone, nearly half the corals on the Great Barrier Reef died — and experts say we are running out of time to save them. “Unfortunately, recent episodes of global bleaching caused by unusually warm water have resulted in high coral mortality, leaving the world’s coral reefs struggling for survival,” D’Angelo said. Scientists emphasized that while colorful bleaching is a good sign, only a significant reduction of greenhouse gases globally — in addition to improvement in local water quality — can save coral reefs beyond this century.\n“Now that we know that nutrient levels can affect colorful bleaching too, we can more easily pinpoint cases where heat stress might have been aggravated by poor water quality,” researchers said. Ryan Goehrung, University of Washington\nCoral reefs support more species per unit area than any other marine environment, including about 4,000 species of fish, 800 species of hard corals and potentially millions of other undiscovered species, according to the National Oceanic and Atmospheric Administration. Colorful bleaching Acropora corals in New Caledonia.\nRichard Vever/The Ocean Agency/XL Catlin\nCoral animals symbiotically coexist with tiny algae, providing them with shelter, nutrients and carbon dioxide in exchange for their photosynthetic powers. Together, these actions can secure a future for coral reefs.”\nFirst published on May 22, 2020 / 4:48 PM “The resulting sunscreen layer will subsequently promote the return of the symbionts. This study, published Thursday in the journal Current Biology, suggests the corals change color as a last-ditch effort to survive. “This can be managed locally, whereas the ocean heat waves caused by climate change will need global leadership. Disruptions to coral reefs have far-reaching implications for ocean ecosystems.\nIt’s not just warming oceans that cause colorful bleaching. Researchers say changes in nutrient levels within coral reefs due to fertilizer run-off from farms also lead to bleaching events — a problem that can be fixed at the local level. Experts believe only coral that has faced mild or brief disturbances, rather than extreme mass bleaching events, can attempt to save itself using this process.']	['<urn:uuid:9dc7f4fa-09fa-435e-bedb-99380e33e6e1>', '<urn:uuid:369f1095-9589-4d83-9007-35418b336092>']	factoid	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:58:41.790432	43	64	1622
70	company data security threats work from home versus office statistics and barriers	The transition to work-from-home has increased security incidents including data loss and oversharing, with 38% of small businesses being late tech adopters. The main barriers are lack of resources and high implementation costs. When employees work from home, it expands the organization's digital footprint and security risks, especially when using personal computers that are less likely to be patched and kept up-to-date against vulnerabilities. Companies are now considering hybrid models, though these can be risky due to potential mistakes during rule changes.	"['Technology evolves quickly and it\'s not always easy to acclimate – especially for small businesses. However, early adoption could make or break your business.\nAccording to a new survey by AT&T, 75 percent of small business owners are eager to embrace new technology, but 30 percent find it hard to adopt.\nAnne Chow, president of National Business at AT&T, believes that small business owners shouldn\'t make technology their primary focus, but they should have a plan to implement new tech on an ongoing basis.\n""Tech is not the pipeline,"" she told Business News Daily. ""It\'s the lifeline.""\nChow added that, while many small businesses feel they aren\'t in the position to adopt such costly trends, technology has the power to make a small business big.\nStill, the AT&T survey found that 38 percent are late adopters. Here are four reasons small business owners are often late comers to new tech adoption.\nLack of resources\nIt\'s no surprise that small businesses don\'t have the same resources as larger ones. But because of this, many small business owners don\'t think they need to worry about fancy software or preventative services.\n""As a small business owner, time and money are your resources,"" said Mazdak Mohammadi, owner and founder of blueberrycloud. ""You make up for your lack of money by using your available time, and vice versa. New technology should either help save time, [or] increase revenues. If either one of these benefits is not clearly guaranteed, it is hard for the business owner to risk the limited time and money they have to try new technology.""\nHigh cost is especially a concern. Smaller businesses earn lower profits, so it\'s difficult for them to adapt with a budget that doesn\'t support it.\n""Most of small business owner run on a shoe string budget; we do not have extra fund to experiment,"" said Alice Kao, co-owner of Alice\'s Smokehouse. ""Unless the solution is well proven and cost effective, small business owners like me will steer away from it.""\n""There are numerous benefits to adopting new technology, however, most small businesses lag behind for one main reason – cost,"" added Marco Cirillo, CTO and founder of Kibii. ""For me personally, onboarding and implementation require significant upfront investment ... Budget is often very tight for small businesses that are trying to cut down on costs, and implementation of new technology is just the first step.""\nFear of being dependent\nMany small businesses thrive on their independence and their ability to operate without the stress of relying on others. However, when implementing new tech, many businesses become dependent on certain products or services, which can feel uncomfortable for small business owners.\n""We don\'t want to be overly dependent on a system if there\'s a chance it could ever go away or become a financial liability,"" said Nick Haschka, CEO at The Wright Gardner. ""For example, it would be very harmful to our business if Google decided to ten times the price on their GSuite product line.""\nHaschka added that this also creates a single point of failure. When you\'re dependent on a system, even minor issues can negatively impact your entire business.\nRebecca West, owner of small interior design company Seriously Happy Homes said small business owners aren\'t always exposed to new tech. Cirillo agreed, noting that this lack of awareness is a common reason why companies are slow to adopt.\n""The sheer range of innovative products and services in the market is extremely intimidating for small businesses that aren\'t familiar with the tech space,"" he said.\nKao stated that with the learning curve, the concern of disrupting current work flow, and the question of whether new tech is reliable and maintenance-free, many small businesses owners don\'t integrate unless it\'s absolutely necessary.\nJames Pollard, owner of a marketing consultancy that works with financial advisors at TheAdvisorCoach.com, said that he\'s hesitant to adopt new technology for his business because he\'s already on track to meet his business and financial goals without doing so.\n""If there\'s an upward trajectory and everything is going well with the stuff I\'m already doing, why rock the boat?"" he said. ""Is there a chance that I could reach my goals quicker with new technology? Sure, but there\'s also a chance that I could slow don\'t my progress.""\nMany small business owners share this outlook, believing that integrating new tech with a process that\'s already successful might cause more harm than good.', 'Employees who work from their homes may be putting their companies’ systems at risk.\n“Many employees do company work from personally managed and owned systems and these machines are often the ‘Wild, Wild West’ in terms of how they are secured,” said Mike Gentile, the chief executive of San Clemente-based cybersecurity company Cisoshare.\n“The majority of complex attacks, such as ransomware, etc., right now are still often caused by a simple phishing attack or an employee mistake like clicking on a bad link.”\nCisoshare is one of several cybersecurity firms that are emerging in Orange County, which is carving a strong position in internet security due to the proliferation of hackers from Russia, China and North Korea who demand eye-popping sums in ransomware.\nCrowdStrike Holdings Inc. (Nasdaq: CRWD), a Sunnyvale-based firm that now has a $55 billion market cap, started in Orange County where it still has a large local presence. Irvine’s Cylance sold for about $1.4 billion to BlackBerry in 2019 and also counts a base of operations here.\nIn Newport Beach, the ioXt Alliance started by Mobilitie founder Gary Jabara, wants to make sure the interconnections among the various devices used each day—such as cellphones, smart home lighting controls and automotive technology—are also secure.\nUC Irvine’s Cybersecurity Policy & Research Institute studies ways to make the internet and networks safer, including running mock attack drills. Cybercriminals\nIrvine-based Netwrix Corp. is expanding so quickly that it’s made four acquisitions since January.\n“Most organizations did not have time to prepare a transition plan and provide security training to the employees” when they started working from home last year, said Ilia Sotnikov, security strategist and vice president at Netwrix.\n“Hence the increase in reported incidents that included data loss or oversharing.”\nAttackers know that ransomware is arguably the quickest way to get money from a company without breaking into its system, he said.\n“The cybercriminals took advantage of the global pandemic and highly divisive political scene in the U.S. last year,” Sotnikov said. “We’ve seen considerable changes in how the threat landscape evolved over the last couple years with ransomware as a service, more specialized groups.”\nThe Coronavirus Chaos\n“There was so much chaos during the first few months of the lockdown that every CISO will need to go back and review all of the access and changes that happened,” said Bil Harmer, who is the chief information security officer (CISO) and chief evangelist at computer identity security software maker SecureAuth.\n“When there is chaos and change, the threat actors will be there looking for ways in.”\nHe predicted that companies “will begin putting more and more focus on digital identities and a continuous authentication methodology that will allow them to adjust access on the fly as the landscape or the user behavior changes.”\nCisoshare, founded by Gentile, placed No. 21 on this year’s Business Journal list of Best Places to Work in Orange County and No. 2 on last year’s list of fastest-growing companies, both in the small-firm category.\nCompanies who let employees work from their homes face an increasing threat level similar to that of an apartment owner who adds more apartments to a portfolio: the more units there are, the greater the business risks, Gentile said.\n“When employees are working from home, it expands the digital footprint and perimeter of the organization,” Gentile said during a recent interview.\nProviding security also requires using precious resources and talent—both of which are in short supply at plenty of companies, Gentile said.\n“The majority of security risk lives in the cracks when people don’t effectively collaborate and ‘cover all the bases’ when building something,” he said.\nTraining on workstations from which a network is accessed can reduce the risks when employees work from home in a decentralized environment, Gentile said.\nCompanies that opt for a “hybrid” model combining both work-from-home and the office should be wary.\n“Hybrid can be risky due to any time rules change, there is a higher likelihood of mistakes,” Kevin McDonald, the chief operating officer and chief information security officer of Alvaka Networks in Irvine lists 16 points of vulnerability. They include use of bootlegged software, browsing illicit sites, opening infected files that would otherwise be blocked, communicating with unverified individuals and illegal sharing of various contraband such as movies, images, and games.\n“Gambling, pornography, sports, gaming sites, alternative bulletin boards, messengers, even terrorism and extremist sites lead to infections of the host that then connects to the company,” he said.\n“We all suffer from a bit of that-won’t-happen-to-me syndrome. We’re not a target, we don’t have anything they want, we’re not that rich of a company.”\nHe says ransomware attackers are well aware of the potential payoffs: “One hit and you can retire.”\nCompanies are starting to nudge employees into coming to their offices though the daily back-and-forth from the COVID-19 Delta variant makes it difficult to set firm guidelines.\nFor example, data analytics software maker Alteryx Inc. has “voluntarily opened a number of our offices, including our Irvine location for those who are comfortable coming in,” Chief Financial Officer Kevin Rubin said on Aug. 5. “There’s no mandate that they do.”\n“We will more officially begin asking associates to start coming back no sooner than January,” he added.\nSotnikov sees some bright spots.\n“I think many of the WFH (work-from-home) specific dangers were mitigated over the last 12 months, as organizations had a chance to catch their breath, get new budgets in 2021, catch up on trainings for both admins and employees,” he said.\nThe Senate included more than $1.9 billion in cybersecurity funds as part of the roughly $1 trillion bipartisan infrastructure package, The Hill website said on Aug. 10.\nThe funds will go toward securing critical infrastructure against attacks, helping vulnerable organizations defend themselves and providing funding for a key federal cyber office, among other initiatives.\nExperts point to the targeting of Colonial Pipeline and JBS meat packers earlier this year as examples of the dangers of ransomware demands.\nThe picture is acute on the international front, with both Sotnikov and McDonald noting President Joe Biden’s warning last month that a significant cyber-attack on the U.S. could lead to “a real shooting war” with a major power, highlighting the growing threats posed by Russia and China.\n“That is a very aggressive and provocative statement,” McDonald said. He points to China in particular as he surveys global cybersecurity threats to the U.S.\n“I should be worried about China finally deciding it’s time to become the sole world power and using its understanding of our weak infrastructure to show us how much we don’t really have control of the world anymore,” McDonald said.\nAnd the ultimate piece of bad news?\n“Replacements are made in China,” he said.\nDangers, Positive Signs: OC Cybersecurity Experts Look at Internet Risks\nOC Cybersecurity Experts Give the Business Journal These Tips:\nKEVIN MCDONALD, chief operating officer/chief information security officer, Alvaka Networks in Irvine\nSome dangers are “removed or reduced” if the equipment is owned by the employer.\n“Employee-owned computers are far less likely to be patched and kept up-to-date against vulnerability. This includes the operating systems, office applications, third party applications such as Adobe, Internet browsers, etc.\n“Having a system shared with non-employees (of unknown behavior tendency, character, education, intent) means that there is a high potential for risky behaviors that can result in a compromised local computer.\n“Big time execs and powerful people are targets and they’re the most reticent to participate in this whole process.\n“Cryptocurrency is the “primary reason” for the rise in ransomware in which hackers hijack a computer system and demand payment to release it.”\nMIKE GENTILE, founder/chief executive, Cisoshare in San Clemente\nThe biggest risk to work from home or hybrid for security “is that collaboration and effective small team dynamics are hindered when people can’t work together in person.”\n“The good news is that some of the strongest safeguards when a workforce is decentralized is a strong security training and awareness program, as well as a communication system so employees know how to get in touch with the security team and vice versa. Both of these items are highly effective, but also much more inexpensive than almost all technical safeguards.”\nBIL HARMER, chief information security officer/chief evangelist, SecureAuth in Irvine\n“The hybrid model will not go away, there is far too much upside for companies in it. From 48 extra minutes per day per employee in productivity to reduced footprints in the office (desks, power, coffee, etc), this is a model that will continue.\n“Companies will begin moving to Secure Identity as the first line of defense. They will begin putting more and more focus on digital identities and a continuous authentication methodology that will allow them to adjust access on the fly as the landscape or the user behavior changes.\n“This will allow the user to move around the physical world and have their authentication and authorization adjust as they do to keep them within the acceptable risk profile.”']"	['<urn:uuid:53b310ce-456f-4843-ae60-72605d2b93a7>', '<urn:uuid:ad45c4f6-8fbd-4d17-b17d-404d81708d47>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T19:58:41.790432	12	82	2234
71	What makes wine from the Ambonnay region unique compared to other wine-making areas?	Ambonnay's distinctiveness comes from its unique soil composition. While the region has chalk-laden soils, it also contains a higher degree of siliceous clay. This soil combination results in Champagnes that typically have a more formidable structure compared to wines from other Grand Cru sites.	['In this month’s club selection, we are offering two different interpretations on ‘Blanc de Blancs’ Champagnes. We have selected two 100% Chardonnay champagnes from different villages, different soil composition, and different approach on the winemaking.\nGuiborat is located south of Reims and the Regional Wild Park of Montaigne de Reims, close to Epernay and Le Meusnil-sur-Oger. Here is the land of the best Chardonnay (Salon and Krug both get grapes from here). The famous white, chalky soil imprints the unique DNA to those wines.\nSoutiran is located in Ambonnay, east of the Grand Montaigne de Reims. Here you’ll find Pinot Noir as the main grape, but they grow a small grand cru quality Chardonnay. The soil is still chalky but rich with magnesium which brings salinity and iodized notes to their wines. Two small producers (Negotiant/Manipolant) that are crafting their champagne with artisanal precision like RM (Recoltant/Manipolant).\nChampagne Guiborat, Pur Prism, Blanc de Blancs, Brut Nature\nThe Fouquet family has farmed small parcels in Cramant, Champagne since 1885. At the helm now are 5th generation Richard Fouquet and his wife Karine (though if you were to visit you may find his grandmother keeping a watchful eye – in her mid 90’s and still sharp as a tack!).\nThe Fouquets farm eight hectares, most of which are grand cru holdings from the deep chalk of Cramant and Chouilly, as well as vineyards in Mardeuil, on the other side of Epernay. The Domaines overall production is tiny, as grapes from five of these eight hectares are regularly sold to the Champagne house of Laurent Perrier. Viticulture is focused on producing top quality Chardonnay from the two former villages with the rest of their holdings being Pinot Meunier from the latter. Farming is all done by hand, with aggressive pruning and low yields.\nVinification takes place in their modest home on the edge of Cramant. Mostly fermented and aged in steel, small lots are also aged in neutral cooperage and laid down in their chilly caves bored out of deep Cramant chalk.\nGuiborar Pur Prisme is made by 100% Chardonnay fruit Grand Cru from three villages: Cramant, Chouilly and Oiry in Côte des Blancs. Mostly comes from the 2011 harvest with a small percentage from 2010 reserve wine. Only the first juice of the press is used from an average of 30 years old vines. Partial malolactic fermentation occurs in stainless steel tanks, and it ages 18 months on the lees. After spending 30 months sur latte (meaning the bottles are laid down in the cellar with the crown cap still on), there is the disgorgement. At this point, the crow cap is discarded and the cork in inserted. Usually this is the point when the dosage is added, but not in this case, since this champagne is a brut nature. It is aged for six more months in the cellar, and then the champagne is ready for release.\nPur Prisme is all about elegance, starting from the clarity of its color to the finesse of its tiny bubbles. Minerality and hearty notes of musk and fresh mushrooms interlace with under-ripe stone fruits such white peach and apples. Light and harmonious to the taste, it leaves the palate clean with a long citrus freshness. Great for a lunch party with light hors d’oeuvres. Perfect on its own.\nChampagne Soutiran, Blanc de Blancs Grand Cru\nChampagne Soutiran is a family owned domaine based in the Grand Cru village of Ambonnay. Patrick and Valerie Renaux, who represent the domaine’s third generation, cultivate 6 hectares of vineyards dedicated to Pinot Noir and Chardonnay. In Ambonnay, the chalk laden soils of the region are complemented by a higher degree of siliceous clay, which result in Champagnes that are often equipped with a more formidable structure than wines from other Grand Cru sites.\nIn addition to their Grand Cru vineyards, the Renaux’s also purchase grapes from neighbor growers in Premier and Grand Cru holdings within the Montagne de Reims. Total production at this family owned domaine top out at 120,000 bottles / 10,000 cases.\nThis bottle is 100% Chardonnay from the solely village of Ambonnay. The base wine is mostly 2007 vintage with 30% from reserve wines of different years. The vines here are 45 years old. The fermentation on this champagne is partly in stainless steel tanks, about 80%, and the rest in neutral oak barrels. The wine goes through full malolactic fermentation. Once is bottled, the champagne spend 36 months in the cellar. The dosage here is 10.6 grams/liter. The result is a wine very rich and structured. The tasting profile offers white flowers, citrus, and toasted biscuit nuances.\nFantastic with grilled scallops, Dungeness crab risotto, or Hamachi.\nA votre santé!\nLeave a comment\nComments will be approved before showing up.']	['<urn:uuid:d7290433-70d2-4403-85ce-582929655a44>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	13	44	795
72	difference between aux sends routing and insert effects sound quality comparison	The key difference is that aux sends allow the original (dry) sound to run parallel with the processed (wet) sound, while insert effects process the entire signal. With aux sends, you can control exactly how much of a signal gets sent to be processed using the send knob, while keeping the original sound intact. Insert effects, on the other hand, process the entire signal directly on the track. Aux sends are particularly useful for time-based effects like reverb and delay, as you can EQ just the effect return to reduce muddiness without affecting the dry signal. They also enable more flexible routing options, like sending multiple tracks to the same effect or creating parallel compression setups.	"['Auxes, busses, sends, returns. Music talk can be pretty cryptic.\nIn the world of signal routing, it’s easy to get tangled. Let’s save you the hassle, and get you started on your road to successful signal routing by answering the question “What are auxes, sends, and returns?”\nOpen up your DAW of choice, and get ready to explore!\nDefining Signal Routing\nThe three terms we will be decoding today (aux, sends, returns) all have to do with signal routing. Signal or audio routing is the process of mapping signals to separate tracks in order to add desired external effects.\nFor example, one might route a vocal track to a separate auxiliary track containing reverb. This allows the producer more control when mixing. Since the sound is routed to a different track as opposed to being directly on the vocal, the engineer is able to dial in the precise amount of reverb he/she desires, allowing for more options, and different sounds.\nSignal routing also allows for certain sounds to be grouped together. Oftentimes, engineers like to process their drums in similar ways, so instead of applying the same effects on every individual track, they can save time by routing the tracks all on one channel. In every DAW, audio routing is essential to efficient and precise results.\nAux, Sends, Returns Defined\nBefore applications, let’s gain a firm understanding of each term at its core. These integral parts of signal routing existed long before DAWs, and are essential to the vocabulary of every serious producer.\nIn order to understand Aux, Sends, and Returns, we must first define bus. Bus is a general term used to signify the routing of tracks to a singular channel. The most common example of a bus is the “Master” channel, which consists of all the tracks routed to one signal, otherwise known as a bus.\nAnother common example of a bus would be a drum bus, where all percussion sounds are routed to one channel so that the percussion elements can all be processed in a similar fashion. Busses can be routed to even larger busses, making signal routing limitless.\nAux, short for auxiliary, is a bus used to create external, precise mixing. Aux channels do not have instruments/sounds on them. Sometimes they are referred to as effect channels, as they are merely there to be routed to.\nAux channels can be broken down into two parts: Sends and Returns. Auxes are useful because they allow for the original sound to run parallel with the processed sound. A mixer chooses how much of a particular signal he/she wants to send to the Aux channel. In Ableton, two Aux channels (Titled A and B) are created by default. Aux channels are great for filtering in effects such as reverb and delay.\nThe send is the input part(s) of an Aux channel. It typically rests on the track of the actual sound(s), and is used to dictate how much of the sound should be sent to the Aux channel to then be processed.\nThe return is the fader that determines the levels of the Aux channel, or the amplitude of the sounds after passing through the Aux bus. It is usually on the Aux channel itself. You can think of return as the output of the Aux track.\nUltimately, signal routing is merely a tool used to unleash an unlimited amount of melodic possibilities. However, there are a couple of common uses that come up in everyday mixing, so it’s important to be aware of them.\nThe most obvious and common use of signal routing is creating groups. Groups not only allow for better organization within a project, but make it easier for an engineer to avoid clashing frequencies. For example, it is not uncommon for a mix to be routed into low, mid, and high-frequency groupings before the final mastering process. With a simple EQ on a group track, the producer can single out which frequencies need to be reduced or amplified.\nParallel compression is created by routing signals to an Aux track and adjusting the sends to your liking. The idea behind it is that it allows the mixer to have more control by simultaneously running the dry signal with the processed audio coming from the return track of the aux or bus. Parallel compression is great at preserving dynamics while still keeping tracks cohesive. It is commonly used with vocals and drum groups.\nSignal routing allows for DJs, singers, and producers alike to have dynamic and captivating performances. Since auxiliary tracks allow for effects to be separate from an initial signal, a performer can quickly turn a send or return knob to produce a new sound for a precise duration. Sends and returns streamline the performing process, and allow the user to focus more on the moment, rather than technical difficulties.\nHow to Setup Signal Routing\nThere are infinite ways to implement signal routing. To get us started, here’s a general guide on the setup of 3 of the most common applications.\nDetermine what you’ll be grouping. Common groups include vocals, drums, and tracks grouped by frequencies (Low, Mids, Highs).\n- Select all of the tracks. In Ableton, this can be done by selecting/clicking on the tracks while holding down the shift key.\n- Right-click to select “Group Tracks”, or control G in Ableton.\n- The heading track or the track above all of the grouped tracks, is the new bus. All of the tracks are routed to this bus, and will be affected accordingly.\n- Use the bus to apply effects, EQs, and variants of your choosing.\nParallel Compression setup\nInsert an auxiliary track. In Ableton, these are here by default under tracks “A” or “B”. Make sure the auxiliary track is completely clean without any effects. Defaults often come with either reverb or delay, two effects seen often with parallel compression. They can be removed by selecting the effect within the auxiliary track and pressing delete.\nPlace the desired effect onto the aux track. If there is a dry/wet knob, turn the knob to 100% wet, since you will be toggling the amount of the effect through the send.\nGo to the signal track that you would like to use parallel compression with.\nSlowly adjust the send knob with the audio signal playing to determine how much of the processed signal you would like in proportion to the dry track.\nAdjust the return knob on the auxiliary track to increase/decrease amplitude.\nDecide which effects you would like to have for performance preemptively. Consider the common options of reverb, delay, chorus, and vocoder effects.\nCreate auxiliary tracks, placing one or more of the desired effect(s) on each.\nMap the sends and/or returns to your external controller. In Ableton, this mapping can be done by pressing control M, turning the desired knob, and selecting the send/return you are trying to map the knob to. To save your mappings, simply press M again, and then test to make sure the map has been routed.\nExperiment with live automation. Live signal routing with an external controller allows us to gain a physical understanding of how the different parts of a DAW are connected. The more intricate the mappings, the more you’ll learn!\nAgain, the possibilities created with auxes, sends, and returns are endless. Here are some more uncommon uses that might not be the first thing to come to mind.\nCreating a fuller mix\nAn aux bus paired with reverb/chorus/flanger effects and some creative panning can instantly widen your mix and dry signals!\n- To do this, take the sound of your choosing and pan it all the way to the left or right.\n- Route this track to an aux track.\n- Place the desired effect onto the aux track, making sure the wet is set to 100% if applicable.\n- Finally, pan the aux track all the way to the left or right, making sure it is in the opposite direction of the dry signal (For example, the dry signal is fully panned to the left while the aux bus is fully panned to the right).\nAdjust sends/returns accordingly. This fun effect will make your mixes sound larger and fuller. Try experimenting with different panning/effect ratios!\nChain effect control\nAnother benefit of using aux busses as opposed to putting the effect directly on the signal track is that you can layer multiple effects and/or have more control over the effect on the aux bus. For example, if you notice that a certain frequency is producing an abrasive hum when using chorus, but not while using delay, you can set up two separate aux tracks with each effect and cater them appropriately. The chorus aux can be equalized while the delay aux remains untouched.\nSignal routing allows for limitless control and possibilities with your projects. These common yet essential techniques are great for polishing your sound, creating inventive sounds, or simply organizing your tracks! Make proper signal routing a part of your mixing ritual.', 'We offer a second set of tips to enhance your corrective and creative mixing skills.\nLast month, I offered some tips and ideas to help with mixing in Logic Pro, focusing predominantly on setup and preparation for a mix, and then on ways of manipulating the bass and drums. In the concluding part of this series, having laid our foundation, we\'ll examine how we might further enhance our mix using various techniques and covering different mixing scenarios.\nThis is a common mixing complaint. Time‑based effects inherently take up large amounts of space in a mix. To minimise their clouding effect, you could try experimenting with panning, but it may be more useful to reduce the spectral range of the effects themselves. Try EQ\'ing the aux returns for the effects or, if you prefer, the sends to the effects. Either way, the object is to thin out the sound of the reverb or delay, thus reducing the \'mud\' around the direct sound. If, for example, a lead vocal is very sibilant, adding lots of delay will only create further problems with \'s\' sounds. Narrowing the frequency range of the delay can make the effect complement rather than detracting from the vocal. In Logic Pro, the Tape Delay plug‑in makes this easy, as it has a simple high/low‑cut filter, which you can see in the screen above. Reverb can also create \'mud\' when added to a track. Try cutting in the 250‑500Hz area and using a low‑pass filter set at around 7kHz to thin out the sound. Like Tape Delay, the Space Designer reverb has its own EQ built in.\nHow can I make a vocal build throughout a track?\nBy changing effects or adding different effects in different sections of a song, you can give the impression of the vocal evolving. For example, the verse might just have a short delay, during the bridge you might then add a reverb, and in the chorus a doubler effect, giving each section a distinctive sound. This could be taken to the extreme by also changing the EQ for each section.\nIn Logic, you could implement this technique by placing all the effects on inserts and then bringing each one in by automating the bypass buttons on the effects. Alternatively — and possibly more flexibly — you could put the vocal processors on aux tracks and use sends from your vocal track, employing automation to not only control the send bypass but also the send amount. The screenshot at top right shows a vocal track being sent to a delay, doubler and reverb. As an aside, notice how easy it is to see the signal flow in the mixer using Single view rather than Arrange view! It\'s also easy to change the order of the plug‑ins or copy them in Logic Pro. Command‑drag to swap the plug‑ins around and use Option‑Command to copy a plug‑in.\nIs there an ideal order for a lead-vocal processing chain?\nThere are no hard and fast rules for the order of processes in a vocal chain, but each one may affect the others. I like to use EQ/Limiter/EQ/Compressor/De‑esser, but this is just a guide. The first EQ is for surgical changes, notching out unwanted artifacts or simply removing muddiness or sibilance. The limiter evens out any major level fluctuations. The second EQ is used to improve character and usually has a fairly gentle, shelf‑like design (a Neve/API type); I typically use it to add \'air\' around 12kHz or warmth around 100Hz. The compressor is used to tighten the sound and possibly add some extra excitement, and will generally have a ratio of around 4:1 but may need to go as high as 20:1. Finally, a De‑esser is used, if necessary, to tame any excessive sibilance.\nMy individual tracks sound great in isolation, but some sounds seem to disappear once they are in the mix.\nThough this might simply be an EQ problem with certain sounds in your mix (some sounds don\'t need to actually be that \'big\' to cut through!), it could also be a phase‑cancellation problem. In adjusting the phase, you are effectively changing the time relationship between sounds, and you could experiment by literally inserting very short delays, but as a starting point it\'s far easier to simply reverse the phase between tracks (Logic\'s Gain plug‑in has a phase‑reverse button and also has the ability to swap the left and right channels). For example, we could try \'flipping\' the phase between a DI\'d bass and the same bass recorded via a miked‑up amplifier, and between the hi‑hat and the snare mics. Typically, some sounds may become \'thin\' or, in extreme cases, seem to be coming from behind your head! This phenomenon can sometimes be used to your advantage, to make backing vocals or synth pads appear to come from beyond the speakers.\nWhat\'s parallel compression, and how can I use it in Logic Pro?\nParallel compression, sometimes known as New York compression, is a process whereby the dry signal is mixed with a heavily compressed version of an identical signal — like using a compressor with a Wet/ Dry knob! The effect becomes extremely useful when you use buses to control the amount sent from an audio or instrument track to an aux return object. While still maintaining the dynamic range and imaging of the original, unprocessed signal, you can use very extreme compression settings on the compressed version to add excitement to your track. (You\'re not restricted to using compression, and could just as easily add parallel EQ.)\nThe technique is easily set up in Logic by placing a compressor on an aux track fed from a bus, as shown in the screenshot above. The compressor is set up with very extreme settings, making it easy to hear the effect of adding the parallel compression.\n• Insert a send on an audio or software instrument track, and route the signal to an unused bus.\n• On the aux track that Logic has created, insert a compressor plug‑in.\n• Adjust the settings of the compressor to give quite an extreme, aggressive effect (fast attack and release, high ratio and plenty of gain reduction).\n• Make sure the aux send on the source track is set to pre‑fade so that any level adjustments on the direct signal will not affect the level of the processed signal.\n• Adjust the level of the compressed aux track to add it to the unprocessed signal.\nWhen it comes to mixing, Logic can seem like a daunting piece of software for less experienced users. However, keeping the screen uncluttered and carefully controlling what you are displaying, as explained last month, makes the process, certainly visually, far more intuitive. Adopting a method and evolving a set of \'go to\' techniques also greatly adds to the enjoyment of the job. Mixing is meant to be like painting a picture, not running a marathon, so the more we keep the technology under control, the less arduous the task will be. It\'s worth noting that even experienced mixers hit barriers; here\'s a final thought from one of the world\'s greats, George Massenburg: ""A mix is never finished, you just have to abandon it!”.\nI want to use my analogue outboard gear during mixing in Logic. How do I do this?\nLogic has an I/O plug‑in, found in the Utility folder, which enables you to send a signal out of the software and back through the audio interface you\'re using. As long as PDC (Plug‑in Delay Compensation) is set to \'All\', the delay incurred by the process of going out and coming back in will be compensated for by Logic. But what Logic cannot do is measure the exact delay incurred by a specific audio interface. It can only make a generic setting for delay compensation. This is obviously not a problem if you\'re inserting an effect across your stereo bus, as all audio is travelling down the same path and will be delayed by the same amount, but if you\'re inserting across individual tracks this could be a problem.\nTo deal with this issue, I use a plug‑in called Latency Fixer, made by Expert Sleepers. The plug‑in, when used in conjunction with Logic\'s own I/O utility, enables me to manually enter a delay setting, in samples, which Logic can then use to exactly compensate for the delay created by the sound travelling out of the audio interface and back again. The sample delay measurement might be made by sending out a rimshot sound or similar percussive sound that you\'ve recorded, through the outputs of the audio interface and recording it back into Logic. If we zoom in on the waveforms it is then easy to see and measure the delay between the two. Artificial Audio\'s Latency Bundle is also very effective at getting round this problem, and does all the hard work for you without the need to record audio back into Logic.\nWhy do my mixes never sound as wide as my favourite records?\nI believe it was Bob Clearmountain who once said that if everything in a mix is in stereo, what you wind up with is two lots of mono!\nWhat he is getting at is if all the source material is recorded in stereo, with a left and a right side panned to the extreme sides of a mix, you wind up with very little direct weight in the centre and, more importantly, no real sense of individual sounds placed in a stereo field. It\'s far more effective, and often produces much more impact, to place the kick, snare, bass and vocal in the middle and the guitar coming from just the left side, for instance, with maybe piano just from the right. This is over‑simplified, but to create an interesting stereo balance it\'s best to have things occupying their own place in the stereo field rather than competing for a space with other instruments. In Logic, the Direction Mixer, found in the Imaging plug‑in submenu, can be used to great effect to pan stereo source material to either side of the stereo spectrum.']"	['<urn:uuid:09cf484f-9628-4e21-9655-f628c5d1a378>', '<urn:uuid:88fde1f9-dd8e-4e7d-97a5-50e3511e7504>']	open-ended	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T19:58:41.790432	11	116	3179
73	carbon capture storage financial viability investor attitudes challenges	Carbon capture and storage faces both financial and technical viability challenges. On the technical side, the process requires extensive infrastructure with capture plants being as large as power stations themselves, consuming 30% of the station's energy output, and facing significant challenges in finding suitable geological storage sites. From an investment perspective, while there are substantial opportunities (with US$588 billion potential in South Africa alone by 2030), investors are increasingly focused on environmental commitments. Major investors like BlackRock are urging companies to commit to net zero targets and may divest from companies that fail to do so. The COVID-19 pandemic has intensified this focus, despite temporarily derailing some companies' environmental plans, making it crucial for companies to align with climate objectives in their recovery strategies.	"[""BY Lloyd Christie AND Zinzi Lawrence\nCompanies must reach net zero emissions to stay relevant to investors\nThe race to achieve net zero emissions has played a significant role in pushing forward the fight against climate change. Key stakeholders are becoming alive to the opportunity presented by a decarbonised economy and those who do not get on board now risk being left behind.\nWhat is net zero?\nSimply put, net zero is premised on attaining a balance between greenhouse gas emissions produced and the amount removed from the atmosphere.\nThe concept of net zero gained momentum following a 2018 report by the Intergovernmental Panel on Climate Change which concluded that in order to reach the 1.5ºC target committed by various countries in the Paris Agreement, global emissions must be reduced by half by 2030 and need to reach net zero by 2050. Various countries, cities and companies have made commitments to achieve net zero. Most notably in 2020, the EU, Japan and China announced their commitments to achieve net zero.\nFor its part, South Africa, a country still heavily reliant on fossil fuels for the generation of electricity, has committed to achieving net zero by mid-century in its low emission development strategy published in 2020.\nWhat does net zero mean for investment?\nRealising their role as catalysts in the global fight against climate change, investors are increasingly focusing on companies’ integration of environmental, social and governance (“ESG”) in their business model and commitment towards achieving the net zero target.\nThis is given credence by Larry Fink, Chief Executive of BlackRock, the world's largest asset manager, in his annual letter to CEOs where he urges companies to commit to net zero and alludes to a possible divestment in companies that fail to do so.\nFrom an investor’s perspective, companies that are best placed to receive ESG investment are those that voluntarily adopt ESG principles and commit to achieving net zero.\nIn 2020, the Financial Sector Conduct Authority, in collaboration with the International Finance Corporation, conducted an industry wide survey which was published in March 2021. This survey looks into the opportunities available for the South African retirement industry to leverage on green and climate finance. This is a clear demonstration that key stakeholders are recognising their role and taking action towards facilitating the transition to a low carbon economy. In its survey, the “IFC estimates there is US$588 billion in climate mitigation investment potential in selected sectors in South Africa up to 2030, and US$29 trillion investment potential across 21 emerging markets representing 48 percent of global emissions. This does not include investments needed to support adaptation and resilience”.\nWhat impact has COVID-19 had on net zero and ESG?\nThe COVID-19 pandemic has intensified the sense of urgency around climate change. The International Energy Agency highlighted in a report this year that global carbon dioxide emissions have returned to pre-pandemic levels. It has been argued that the COVID-19 pandemic has derailed some companies plans to “green” their companies and integrated ESG.\nHowever, the pandemic, undoubtedly presents an opportunity for companies to align more closely with climate change objectives, including the commitment to achieve net zero as part of their post-crisis recovery programmes.\nInvestor sentiment highlights that companies that do not adapt will be left behind, while those that embrace change will see greater opportunities. It should not be understated that to attain the net zero target and achieve success in attracting ESG investment will be costly and success will be dependent on a company’s ability to transition to clean sources of energy, reducing emissions from appliances by making use of available technology, amongst other efforts.\nWhat remains clear is that investor focus on ESG will continue to grow in the coming years. For South African based companies, the robust environmental and regulatory framework provides a good basis for such companies to position themselves favourably and to attract ESG investment.\nExecutive | Head of Natural Resources and Environment\nmobile +27 82 210 2159\nAssociate | Natural Resources and Environment\nmobile +27 66 476 0776"", 'Challenges of carbon capture and storage technology\nThe Australian Coal Association is putting a lot of hope, and money into carbon capture and storage. Carbon, in the form of gas, C02, is taken out at the source of burning at the power plant, but its an expensive process. A capture and storage plant would be much the same size as the power plant itself and it would require 30 per cent of the power plant\'s energy output to operate. James McGregor, the project manager at the CSIRO\'s National Energy Centre in Newcastle spoke to AM.\nSource: AM | Duration: 2min 13sec\nTONY EASTLEY: While politicians thrash out policy in Canberra, other big players are in the mix.\nThe Australian Coal Association is putting up a lot of money into carbon capture and storage. It\'s a process which has been termed ""clean coal"".\nTechnically it\'s not all that complex and the techniques have been around for years. Carbon in the form of gas, C02, is taken out at the source of burning at the power plant.\nA capture and storage plant would be much the same size as the power station itself though and it would require 30 per cent of the station\'s energy output to operate.\nAs well there are large regulatory issues about transport and storage.\nJames McGregor is the project manager at the CSIRO\'s National Energy Centre in Newcastle.\nThe capture of CO2 itself, it isn\'t cheap.\nJAMES MCGREGOR: No, and that\'s one of the big challenges for carbon capture and storage technologies at the moment. I guess it\'s in its infancy so it\'s in its early stage of technological development and we think that there can be significant cost reductions in the near future.\nTONY EASTLEY: So you\'re confident that with your type of pilot plant that we\'ve seen here, that CO2 can be captured, it can be then easily transported even across large distances?\nJAMES MCGREGOR: Yes. What we do is when we compress the carbon dioxide at the back end of the capture plant we compress to what\'s known as a supercritical liquid. So a supercritical liquid is at room temperature and it has the density of a solid but it behaves like a liquid, so we can use standard pumping systems to move carbon dioxide around.\nTONY EASTLEY: So theoretically at a power station in the Hunter Valley in New South Wales the CO2 is captured but the storage of that CO2 may be hundreds if not thousands of miles away?\nJAMES MCGREGOR: Possibly, and that depends on the I guess a lot of the geology and where the best sites are suited for CO2 storage.\nTONY EASTLEY: So the capture is easy; the storage may be more difficult.\nJAMES MCGREGOR: Yes I think that\'s the highest risk and the hardest part of all the CCS is the geological storage. The capture and transport whilst they represent the highest cost components, the identifying and finding the sites is going to be I guess a major challenge for large-scale deployment.\nTONY EASTLEY: James McGregor, thanks for joining us on AM.\nJAMES MCGREGOR: My pleasure.']"	['<urn:uuid:04aeb563-4336-4252-a97d-0744818dcb44>', '<urn:uuid:5b489964-9284-4c1a-8b91-2dd9153521ab>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T19:58:41.790432	8	124	1191
74	hazmat worker storage areas oxygen limits dangers	When dealing with hazardous materials storage areas, there are two main oxygen-related dangers: oxygen deficiency which poses an asphyxiation risk, and oxygen enrichment which creates a toxicity risk. These conditions are particularly dangerous in confined spaces due to limited ventilation. For hazmat workers, proper training is essential - most complete 40 hours of OSHA-mandated training to safely handle such environments.	['The first line of defense between a safe project and an accident waiting to happen is preparation.\nHazardous waste management knows this and takes every step possible to make sure all the materials moved in and out of America’s transportation network are kept far from human or animal contact. Jobs that require entering confined spaces, transporting dangerous liquids or manufacturing certain materials have even more safety requirements than average. It’s not enough to simply pull on a pair of gloves and a gas mask. RCRA training online is your best resource toward learning the safety skills necessary to keep yourself, and everyone else, safe.\nPrepare for your new career with a DOT hazmat training course. Here’s a basic overview to get you started.\nThe Definition Of Hazardous Materials\nIt’s important to know what, exactly, you’re up against when you clock onto an average shift. Some materials are able to be safely interacted with by hand, while others require you be covered head-to-toe. Hazardous materials are defined as such due to their increased or guaranteed probability of poisoning, burning or killing an animal or a human. They can be physical, chemical or radioactive. Hazardous materials rely on everyday workers such as yourself, needing to be transported safely and kept free from the environment to avoid a harmful ripple effect.\nAmerican Transportation Of Hazardous Products\nThe United States today boasts some of the largest production industries worldwide. Electronics are the highest valued commodity shipped accounting for over $1,600 billion. Natural gas, asphalt and Coca Cola are the highest weight of commodities, as well. Texas is the state with the biggest oil production, with 2016 producing over 1,175 million barrels or 45% of all oil production in the country. Hazardous waste certification training online is becoming more widespread as American success continues to reach all corners of the globe.\nThe Most Expensive Industries To Date\nThe American transportation network is vast. Millions of trucks and train systems regularly transport everything from food to hazardous materials on a daily basis. As explored above electronics remain one of the United States’ most consistent money makers, with gas and concrete following close behind. Flammable liquids, including gasoline, are the most transported hazardous material in the country. They total 85% by value, 85% by weight and over 65% by ton-miles. RCRA training remains one of your best resources toward learning more about your field and being prepared for whatever comes your way.\nUpdated OSHA Protocol\nTaking a DOT training course is necessary for both new workers and old workers alike. Safety standards aren’t static and are required to update regularly to keep worst case scenarios as far as possible. The majority of hazmat workers complete up to 40 hours of training mandated by OSHA, though some are required to have state specific licenses, as well. The Department Of Transportation has gradually broken down hazardous cargo into nine classes to maintain better safety while increasing efficiency.\nSigning Up For RCRA Training Online\nOver three billion tons of hazardous materials are shipped every year. Thanks to the efforts of hard workers and certified training courses countless industries are provided the products and materials they need to run without a hitch. Your new career field is not only in high demand, it has plenty of room for growth. The year 2016 saw hazardous material removal workers making an average annual salary of $40,000. Over 90% of daily hazmat shipments are done by truck, as well, and can see you on the open road or working in a manufacturing hold.\nIt’s better to be safe than sorry. Your RCRA training online will prepare you for a bright and reliable future with today’s hazardous materials.', 'Confined Space Entry\nConfined Space Entry Training - Online Course\nMany worksites have areas that are classified as restricted spaces or confined spaces. When working in these environments, additional precautions are necessary to prevent accidents. This Confined Space Entry online course provides workers with the required knowledge to stay safe when accessing a confined space.\nA restricted space is an area that is fully or partially enclosed with limited means of access, and not designed for continuous occupancy. The main hazard in a restricted space is difficulty entering or leaving, but the environment itself is controlled and kept safe. Building attics are an example of restricted spaces.\nA confined space has the same characteristics as a restricted space, with additional risk factors like hazardous atmospheres or high potential for injury. An area is also considered a confined space if any activity conducted inside or outside creates hazardous conditions. Restricted spaces can become confined spaces when interior conditions change.\nThe hazardous atmospheres that may be encountered in confined spaces include the following conditions:\n- Oxygen deficiency (asphyxiation risk)\n- Oxygen enrichment (toxicity risk)\nHazardous atmospheres can develop faster in confined spaces, due to the lack of natural ventilation and low air quality.\nConfined spaces may involve conditions that can cause injury or illness, such as unstable walls inside a mine tunnel. Confined spaces can also become dangerous as a result of the activities conducted inside. For example, when a tank interior is being welded, fumes must be removed immediately with an extractor. These fumes irritate the respiratory system with brief exposure, and extended exposure causes severe health issues like organ damage.\nSome examples of confined spaces are tunnels, wells, manholes, cold storage, ship holds, subcellars, tanks, culverts, silos, vaults and open ditches. These environments are normally entered for construction activities, inspections or maintenance.\nIn both restricted spaces and confined spaces, escape and rescue are more difficult due to the limited means of access. Adequate equipment and safety protocols are critical during a rescue; according to the Canadian Centre for Occupational Health and Safety (CCOHS), 60% of workplace fatalities related to confined spaces occur during rescue operations.\nEntering a Confined Space\nGenerally, occupational health and safety codes consider that workers are in a confined space when their breathing zone is inside. Since limited means of access restrict natural ventilation, confined spaces often have low air quality. Many confined spaces also have entry hazards, such as steep ladders that are long or narrow.\nBefore entering a confined space, workers must identify the potential hazards and plan accordingly. Some hazards can be eliminated completely, while others can only be mitigated. Permits are normally required before entering confined spaces, and workers must wear all the personal protective equipment (PPE) that is necessary, depending on the location and task. Being well familiarized with emergency and rescue protocols is also very important.\nWorkers must also use adequate monitoring equipment in confined spaces, since some dangerous gases are colorless and odorless. Mechanical ventilation is often necessary, to compensate for the poor air quality in confined spaces. All the hazards found in regular workplaces can be encountered in confined spaces, but escape routes for workers are much more limited.\nLike with many other workplace hazards, the risk of accidents in confined spaces is reduced when workers have all the necessary knowledge and equipment. Companies should be familiarized with all the code requirements and regulations for confined space entry and confined space training, which may change by jurisdiction.\nCONFINED SPACE ENTRY ONLINE COURSE TOPICS\n- Confined space basics\n- Recognizing confined spaces\n- Existing and potential confined space hazards\n- Eliminating and controlling hazards\n- Classification of confined spaces\n- Entry authorization\n- Confined space entry planning\n- Personal protective equipment (PPE)\n- Emergency and rescue procedures\nThis course was created using standards that will allow playback on most internet capable devices with standard web browsing capabilities including Apple’s iTouch, iPad, and iPhone, as well as most other smart phones and tablets including those with Android and Windows operating systems.\nAverage Completion Time\nCompletion times vary depending on the number of times the information is viewed prior to finishing the course. The average completion time is 2.5 hours.\nTesting is conducted in this online course to reinforce the information presented. You are provided three opportunities to achieve a passing mark of 80% or greater.\nCertificate of Completion\nUpon successful completion of this course, a certificate will be available to download and print. You can access your certificate through your online account.']	['<urn:uuid:30764899-ecb9-4cff-9fb1-661de9287d6c>', '<urn:uuid:6a1e4795-a1b4-496a-acd7-078e1c1b2140>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T19:58:41.790432	7	60	1364
75	minimum optimal temperature for electric bicycle battery charging process	You should never charge the battery below 32 degrees Fahrenheit. The battery needs to be at room temperature before charging.	['Like regular bicycles, electric bikes also need to be maintained to ensure they run smoothly. With a mix of mechanical and electric parts, maintaining an e-bike may sound like a complex task.\nHowever, it is not such a hassle if you know the right way to do it. Here’s how to take care of an electric bike.\nHow to Take Care of an E-Bike\nYou can take care of an electric bike through the following steps.\n- Wash It Regularly: Dirt, dust, grease, and other unwanted substances accumulate on different parts of an electric bike with each ride. They need to be washed regularly for optimal performance.\n- Maintain the Components: The wear and tear of an e-bike’s different components can be controlled to make them last longer. The components that need your attention are the brakes, batteries, tires, and drivetrains.\n- Schedule for Service: There are some aspects of maintenance that only expert technicians can handle. Hence, you need to get your e-bike checked and serviced by a professional.\nRead How Old to Ride an Electric Bike?\nNow, let’s look at each factor of maintenance in detail.\n1. Washing It Regularly\nLike every vehicle, e-bikes also get dirty every time you take them out on the road. Hence, you need to clean them to avoid the accumulation of dirt.\nBut how often should you clean your e-bike? Well, there is no fixed rule for that. It mostly depends on how much you use it, the type of terrain you ride on, and the weather conditions where you live.\nJust remember that the dirt might become ingrained if you let it sit for too long. Wash it after every ride (or as often as possible) if you are a regular user or ride in areas that can cause corrosion, like in the sand or near the beach.\nElectric components, like displays or remote keys, can get damaged upon contact with water. Although e-bikes don’t fail completely if splashed with a little water, the amount of water they can resist varies from model to model.\nHence, it’s best to protect the components from water – as much as possible.\nRemove the batteries, display, and other electric parts that are easily detachable. Cover the permanently mounted components with a cloth or a poly-film.\nDo not use a jet wash or a high-powered hose to wash your e-bike. Water applied at high pressure can completely remove lubrication from different parts and accumulate in the e-bike’s inner components, causing corrosion.\nInstead, you can use a normal garden hose, a wet rag, a gentle brush, or a soft sponge to clean the excess dirt.\nCleaning stubborn dirt and grime will also require a gentle soap, detergent, or any other cleaning solution.\nSome e-bike owners also prefer to use dedicated bike shampoos and care products that don’t damage the plastic and seals.\nJust remember that your cleaning solution should not be strongly degreasing, and it should not touch the brakes.\nThe next step is to clean the drivetrain. Because it’s oily, the chain picks up more dirt than other parts. You will need a degreaser to remove the dirt and grime from such lubricated parts.\nNow that your e-bike is clean, you have to dry it off completely. Get rid of the excess water by bouncing it off on tires. Then wipe all the parts dry with a soft towel, especially the bolts and the drivetrain.\nYou can also keep it under the sun to let the water evaporate. Once the bike is dry, lubricate the components again and wipe off any excess lubrication.\nSome people also use a shining solution to maintain the paintwork and keep their e-bike looking brand new.\nBut applying such products requires caution because they can cause the brakes to malfunction.\nSo, make sure you keep such products away from the brakes.\nRemember to check your e-bike every time you wash it. See if the batteries, drivetrain, and display are working properly.\nRead How to Charge an Electric Bike in Remote Areas?\n2. Maintaining the Components\nThe main components of an e-bike that require regular care and maintenance are the batteries, brakes, tires, and drivetrain. Let’s see what you can do to keep them in optimal condition.\nHow to Take Care of E-Bike Batteries\nBatteries are the power source of electric bikes. They determine the range of your e-bike, i.e. how far it can go.\nThere are three common kinds of e-bike batteries available: lead-acid, nickel, and lithium-ion batteries.\nMost e-bikes today are powered through lithium-ion batteries. They can last around three to five years, depending on their type and quality, the level of your usage, and how well you maintain them.\nThe better you maintain them, the longer they last. But how do you keep your e-bike battery in top-notch condition?\nThe tips mentioned below will help you make the most of your battery.\nTips to Charge Effectively\nBatteries do not charge quickly if they are cold. Therefore, you should never charge the battery below 32 degrees Fahrenheit.\nBring the battery up to room temperature before connecting it to a charger.\nCharge the battery in a dry place and avoid flammable materials, humidity, or heat sources. Moreover, you should only use the charger provided by the manufacturer.\nOther chargers may also work with your e-bike, but they don’t perform the same way and may damage your battery in the long run.\nNever let your e-bike battery drain completely; charge your battery after every long ride. Make sure your battery and charger are not covered during the charging process.\nYou should also not leave your battery discharged for long periods. It’s recommended to charge your e-bike at least once a month, even if the bike stays unused for a long time.\nYour battery should at least be 30 to 60 percent charged before leaving it unattended for long. Avoid storing your battery in extreme hot or cold conditions.\nThe recommended storage temperature is between 59 °F (15 °C) and 77 °F (25 °C). Keep it in a cool, dry location when storing it for a long time.\nHere is a chart sourced from Battery University that shows the estimated recoverable battery when stored at different temperatures for a year.\nIt proves how storing a battery in high temperatures accelerates capacity loss.\n|Temperature (Degree Celsius)||40% Charge||100% Charge|\n|60||75%||60% (after three months)|\nRead Can I Overcharge An E-Bike Battery?\nTips for Battery-Efficient Rides\nRemember to check a few things before every ride. Check if the battery is locked and loaded firmly; it should not move without the key.\nPlus, see if it has enough charge for your planned trip. Finally, make sure all the mechanical parts of the e-bike are working properly.\nIt is possible to increase the range of an e-bike battery through efficient riding habits. Firstly, many e-bikes provide the option of riding in eco-mode.\nEco-mode allows the motor to provide you with the lowest level of assistance, which means a lower burden on your battery.\nNote the wind resistance when riding an e-bike. When the wind resistance is high, riding at high speeds requires more energy output from the battery.\nHence, you should go slower in these conditions to increase your range.\nAnother way to ride efficiently is to maintain a higher cadence, which means putting more effort into pedaling. A battery-efficient cadence is around 50 revolutions per minute.\nYou can also reduce the amount you stop or brake to protect the battery from wear and tear.\nTake extra care when attaching and detaching the battery from an e-bike. It is heavy and expensive; you might end up with a damaged battery if you don’t follow the instructions provided by the manufacturer in the e-bike manual.\nRead How Do You Charge an Electric Bike\nHow to Take Care of Your E-Bike’s Tires\nE-bikes generally have a solid and heavy frame to support the weight of the electric components, mainly the battery and motor.\nHence, make sure your e-bike has a sturdy set of tires underneath that can handle all this weight.\nMaintaining tire pressure is vital to prevent your e-bike from being damaged. Underinflated tires don’t work or support your bike efficiently, making your rides dangerous.\nIt’s important to keep the tires inflated at all times. Tires face a lot of wear and tear from road friction. Hence, they need to be replaced every one or two years on average.\nYou can buy replacement tires from renowned brands with qualities like firm sidewalls, puncture resistance, and good traction.\nRead How Much Does an Electric Bike Battery Cost?\nHow to Take Care of the Drivetrain\nThe drivetrain of an e-bike consists of numerous metallic components; pedals, cranks, chain, cassette, chainrings, and derailleur.\nThey grind against each other, creating loads of friction. Friction can cause the metal to become abrasive and wear out over time.\nUnusual sounds from your bike are an indication that your drivetrain needs service. Lubricating the drivetrain regularly will make it last longer.\nThe most vulnerable part of your drivetrain is the chain because the main movement occurs there. This is why it needs the most attention when it comes to maintenance.\nMake sure it stays clean by wiping the dirt and grime off with a rag.\nYou don’t need to lubricate every part of the drivetrain separately; this can lead to excess grease around the brake discs or cassettes.\nJust lubricate the center of the chain, and it will spread the grease to other parts as it cycles through.\nRead What Are the Different Types of Electric Bikes?\nHow to Take Care of the Brakes\nMaintenance of the brakes varies according to their types. They are a bit trickier than other components, so it’s best to let the professionals handle your brake maintenance.\nMost e-bikes have mechanical or hydraulic disc brakes. Both types of brakes can malfunction if they come in contact with oil. You can clean them using a cloth soaked in isopropyl alcohol.\nIf you have mechanical brakes, you will need to replace the cable inside them. In hydraulic brakes, you have to change the fluid from time to time.\nHydraulic brakes should be replaced every 6 months, while mechanical cables should be replaced every 6-12 months.\nYou should only attempt to service your brakes if you know how to handle them and your brake levers are responsive.\nRead Can You Use a Car Battery for an Electric Bike?\n3. Scheduling a Professional E-Bike Service\nElectric bikes are a complex combination of electrical and mechanical systems.\nIf you cannot handle the care and maintenance required, have it regularly serviced by professionals. They will inspect every component, and repair or replace it as needed.\nEven if you know all about e-bikes and feel confident enough to handle everything alone, do not dive deep into some technical aspects of e-bike maintenance, especially the motors.\nYou will end up with a voided warranty and possibly an issue with your e-bike.\nAn ideal interval to get your e-bike serviced is around six months to one year, depending upon the frequency and intensity of your use.\nYou can trust your instincts and book an appointment whenever you feel your bike needs service. But who should you trust with your e-bike?\nYour first priority should be going straight to the manufacturer. Their service technicians know the brand’s products.\nThey are trained to diagnose the problem areas within your e-bike. Plus, they can also update your bike’s software to ensure that everything runs smoothly.\nChoose a service provider for your e-bike very carefully because not every shop provides a high-quality service.\nYou should go to a shop that specializes in e-bike service, has positive reviews from its customers, and provides reasonable warranties for their work.\nRead How to Choose a 750W Electric Bike?\nE-bikes are no less than a blessing when it comes to pollution-free traveling. However, they contain many mechanical and electric parts that need regular care and maintenance.\nWashing your e-bike regularly helps you get rid of all the dirt and grime they pick up from the road. However, only cleaning them is not enough to keep them performing at their best.\nYou also need to take care of all the components individually, like the brakes, tires, drivetrain, and batteries.\nYou should book a service appointment for the technical parts of your e-bike that should not be handled by anyone but professionals. They know how to inspect every nook and cranny of your e-bike and diagnose all possible maintenance issues.\nYou may like the following electric bike articles:\n- How to remove speed limiter on an electric bike\n- Can I Ride My Electric Bike in the Rain?\n- 500-Watt Electric Bike or 750-Watt Electric Bike?\n- Can an Electric Bike Run Without Pedaling?\n- How Fast is a 2000-watt Electric Bike?\n- Do You Need Insurance For An Electric Bike?\n- How Far Can a 500-Watt Electric Bike Go on One Charge?\nMy name is Matthew, staying in Seattle, Washington. Electric Vehicles (Electric Cars & Electric bikes) caught my attention for the last few years and my love for electric cars and bikes is everlasting. I spend many of my weekends traveling to various places all over various cities with my electric vehicle (e-bike and electric car). Here I am sharing my expertise, experience, and invaluable information about electric cars and electric bikes. Check out more.']	['<urn:uuid:dacc8a5b-5fd5-4881-ab6f-02f98e2ed332>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	9	20	2227
76	How do worms help make better garden soil?	In worm composting, red wiggler worms break down material much quicker. You need 1lb of worms for every 1/2 lb of daily kitchen waste. The worms process the waste in a bin with damp brown material, creating nutrient-rich compost. This method is especially good for composting small amounts in limited space.	['Holler if you’re a baller.\nComposting is the process of allowing your kitchen and yard waste to decompose, and then adding the resulting material to your garden, to supply nutrients and create a richer growing environment for your plants. It is one of the essentials to improving soil quality.\nTHE BENEFITS OF COMPOSTING\nIf you have a garden of any kind, having your own compost heap is a no-brainer. It will supply you with good fertilizer for your plants, so there will be no need to spend money on that. It’s also very good for the environment, since it reduces landfill waste and reduces the water and electricity needed for waste treatment. This article is an introduction to compost, how the stuff works and how to make it work for you.\nThe main nutrients supplied by compost are nitrogen, potassium and phosphorous. These are essential for a healthy soil, and a consistent supply of compost material will supply these in abundance, and in an ideal ratio of the three. There are other nutrients supplied by compost, but these are the big ones.\nCompost will also improve the water retention of the soil. This will help plants grow as water will be available whenever they need it.\nGENERAL TIPS FOR COMPOSTING\nHeat helps to break down your material quicker, so ideal compost container is a dark color that faces the sun.\nThe time of year also makes a big difference- material may take a few months to break down if added at the start of the summer, or most of the year if added at the end of it.\nYour compost needs room for air to circulate, so incorporate layers (ie 2 inches of kitchen waste followed by 2 inches of crumpled newspaper).\nIt’s a good idea to have more than one compost container, as you can feel one up, let it sit and do its thing, and still have space for new material.\nMATERIAL FOR COMPOSTING\nFirst we need to differentiate between the types of compost material (as well as what you SHOULDN’T put in your compost). There are two categories of compost material: green and brown.\nGreen material is high in nitrogen, and includes grass clippings, manure, egg shells and vegetable scraps.\nBrown materials are rich in carbon and phosphate, and include leaves, wood shavings, sawdust, and cardboard or any paper without colored ink.\nWhat not to compost: meat (it will smell and attract scavengers), dairy products (the same), any part of a diseased plant (this can spread), Feces (can breed bacteria) and any paper with colored ink.\nMETHODS OF COMPOSTING\nThis is the easiest and most common form of composting, the method used in most backyard compost heaps. You need a container for your material (dark plastic containers work great). Start with a layer of browns (for example, leaves in the fall) then add a layer of greens. When the green layer gets about 2 inches high, add another 2 inch layer of browns. This should create a balanced fertilizer at the end of the process, which will take about a year.\nHot composting is a method that is a little more interactive. Start with a layer of brown, then add equal parts brown and green. Once you’ve done this, don’t add any more, and turn your mixture once every week. Various bacteria will build up and speed up the decomposition process, and your material should be ready in 2 to 3 months.\nThis method of composting uses worms to break down your material much quicker. Red wiggler worms are ideal if the majority of your material comes from kitchen waste. This method is great for composting small amounts if your space is limited.\nPut a damp layer of brown material in your bin, to about 3/4 full. You then drop your red worms onto this layer, allowing them to wiggle in. Plan for 1lb of worms for every 1/2 lb of daily kitchen waste your produce. Add scraps every day, and bury them in the bedding. You’ll need some holes on the bottom for moisture to escape, and an area underneath to collect it.\nWhen your compost is ready, move it to one side, and add a new layer of damp brown material. Over a few days your worms will naturally gravitate towards it, and you can remove your compost without losing your worms.\nGood luck and stay prepared!']	['<urn:uuid:51055cf0-49bd-4448-bdf8-c377c22e4d4b>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	8	51	732
77	What wireless solutions exist for non-invasive medical diagnostics and monitoring?	Several wireless solutions have been developed for non-invasive medical diagnostics and monitoring. One notable example is the capsule endoscope, which is a small pill-sized device (20mm length, 10mm width) that transmits real-time images for up to ten hours using Wireless Body Area Network (WBAN) technology. This device is particularly valuable for accessing the small intestine non-invasively. Remote patient monitoring systems use portable instruments for tracking vital signs and transmitting data through cellular networks. Wireless sensors are also used for monitoring brain electrical activity in neurodegenerative disorders and muscle activity in stroke recovery. The implementation of these technologies requires expertise in circuit design, signal processing, and radio systems, particularly for medical applications. While these solutions offer significant advantages in terms of patient comfort and diagnostic capabilities, factors such as cost, infrastructure requirements, and the need for reliable power sources can limit their widespread adoption.	['The department´s educational and research programmes covers vast areas of the IT field: from telecommunication via antennas to artificial intelligence, from basic theories via algorithms to implementations on silicone and in biomedical systems. Digital information is currently used in nearly any context; digital broadcast of radio and television, internet and mobile telephony are just some examples. Future devices and systems will increasingly be based on a complicated collaboration between different techniques. It is in this light the department´s breadth should be seen.\nIn Electroscience, about 60 employees are working in four research groups. The groups work together focusing on applications in, for example, wireless communication and medical engineering.\nResearch covers most of the aspects of analogue and digital technology, from a theoretical and comprehensive system level, to specific algorithms and circuits. Courses are given for a number of programmes, but mostly for the programme in Electrical Engineering. Research is divided into the four main areas given below:\nIn Electronic Design, integrated circuits (microchips) with specific functions are designed and developed, mainly for wireless communication, image processing and medical applications, e.g. pacemakers. The demands vary depending on the area of application, but high computational capacity and low energy consumption are central. Research is carried out in both analogue and digital circuits, as well as conversion between these kinds of signals.\nKeywords: circuit design, analogue, digital, integrated circuits, FPGA, field-programmable gate array\nResearchers in Radio Systems are working on the basic issues of radio communication. They are studying the propagation of radio waves from transmitters to receivers, and developing technical solutions as well as complete systems. Their main aims are to develop quick, efficient and robust systems for future wireless communication.\nKeywords: radio systems, radio channels, propagation of electromagnetic waves, antennae, wireless networks, mobile telephony, systems with multiple input and output (MIMO), multi-carrier technology (OFDM), ultra-wideband (UWB), communication with medical implants\nWithin the field of Signal Processing, the electrical activity of the human body is being investigated with the aim of developing methods and algorithms for medical diagnostics and treatment. Special emphasis is placed on methods for the analysis of heart signals resulting from arrhythmia, such as atrial fibrillation, or in connection with dialysis; and also the modelling and analysis of biological rhythms, mainly heart frequency variability.\nKeywords: medical signal processing, mathematical modelling, cardiology, arrhythmia, pacemakers, audiology, oto-acoustic emissions, epilepsy\nTheoretical Electrical Engineering is a basic scientific subject, with many applications. Methods are being developed in electromagnetic wave propagation and the investigation of materials (i.e. how electromagnetic waves, such as radio waves, are reflected from or propagate in different materials) and efficient antennae. Apart from mobile telephony, applications include surveillance systems and non-intrusive sample analysis.\nKeywords: electromagnetic field theory, antennae, wave propagation, mobile telephony, scattering theory, optical fibres, non-intrusive sample analysis\nThe activities in Information Technology cover large areas of what is generally termed IT: from telecommunication, via image compression, to artificial intelligence: from basic theory, via algorithms, to implementation on chips.\nDigital information is used in almost everything these days: digital radio and TV, the Internet and mobile telephones are just a few examples. In the future, equipment and systems will be based more and more on the complicated interaction between various techniques. This involves everything from household goods, computer networks at home and multimedia systems, to huge communication networks and information systems.\nCurrent research projects include data and image compression, coding theory, cryptology, modulation and access methods, signal processing, model-based diagnostics (artificial intelligence, AI) and computer systems for search engines and other information systems.\nSome thirty-five employees work at the division. Courses are given in the programmes for Computer Science, Electrical Engineering, Industrial Management and Engineering, Engineering Mathematics and Information & Communication Engineering.\nKeywords: information technology, communication systems, coding theory, ASDL technology, data security, cryptology, data compression, image compression, computer systems, search engines, data transmission, model-based diagnostics and alarn clean-up and maintenance\nCommunication Systems applies an all-embracing systems perspective to digital communication systems that include a substantial amount of software. Research follows two main directions, telecommunication systems and software systems.\nAbout 30 people are employed at the division. Courses are given mainly in the programmes for Computer Science, Electrical Engineering and Information and Communication Engineering, and some in Industrial Management and Engineering.\nTelecommunication Systems saknas i listan focuses mainly on issues of capacity in the design of various systems, e.g. fixed telephony, data transmission, the Internet, mobile telephony, wireless networks, broadband, etc.\nKeywords: mobile systems, GSM, 3G, 4G, data transmission, broadband, the Internet, fixed telephony, IP telephony, communication protocols, Bluetooth, wireless networks', 'Wireless technology in healthcare\nThe technology boom of the last few decades has left no sector untouched. As use of technology has gained wider acceptance, it is being used both by the doctors and the patients in the healthcare industry. Wireless technology has a number of advantages over wired alternatives. Recently, some forms of wireless technology such as WLAN, Bluetooth, RFID, WBAN, 3G and 4G, have made possible several advances in wireless technology. Miniaturization of medical devices to scales previously unimaginable has also made it possible for integration of wireless technology in day to day practice. Mobile medicine is another innovative use of wireless technology. An important reason for this is easy availability of high speed internet.\nUses of wireless technology in healthcare system\nThis is one of the more popular and more used forms of wireless technology in India. Capsule endoscope resembles a small pill, approximately 20mm in length and 10mm in width. The patient is instructed to swallow the capsule endoscope, which then transmits real time images for a period of up to ten hours. The advantage of a capsule endoscope against other forms of scopes like colonoscope or oesophagogastroduodenoscope lies in the fact that it can easily access the small intestine. It is also a non-invasive procedure, which causes almost no discomfort to the patient.\nThe capsule contains extremely low power utilizing circuits and physiological sensors which transmit images to sensors on the patient’s body, which are then transmitted to an external processing unit. This is known as the WBAN or the Wireless Body Area Network technology. Multiple images and videos are obtained over a span of hours, which help in making the diagnosis easier. Sometimes, a capsule endoscope is the only means of definitively and non-invasively diagnosing small-bowel ailments.\nFactors which limit the use of a capsule endoscope are its cost and its diagnosis-only capabilities. Even though there has been a significant reduction in the cost of capsule endoscopy since its introduction, (a capsule endoscope cost approximately Rs 1 Lakh 3years ago, but today it costs approximately Rs 35,000) it is still out of reach for majority of the Indian population. Though an accurate diagnosis can be made, no corrective action can be taken by the capsule endoscope, as opposed to a conventional scope, which can be used for therapeutic purposes as well.\nRemote patient monitoring system\nIn the last couple of years, there has been a lot of advancement in the remote patient monitoring systems both in the inpatient as well as outpatient settings.\nApproximately 3.5 lakh unnatural deaths were reported in India in the year 2009 (Garg, 2012). Many of these deaths occur due to the lack of prompt medical services. Remote patient monitoring is a practical tool which can bridge this gap. A3 remote monitoring technology, an initiative of IIT- Bombay, a one of kind technology today in India, is an excellent example of successful application of remote monitoring of patients. This technology makes use of a portable instrument which has a 12 lead ECG and also monitors other vital parameters like blood pressure, heart rate, respiratory rate and oxygen saturation levels. The results are then sent via any of the cellular networks to the doctors. Emergency treatment can be started in the ambulance, even before the patient has reached the hospital. Patient’s data is also stored confidentially on a cloud server for future use.\nAdministering prompt emergency medical service is just one of the applications of remote patient monitoring. It can be used in ambulances during transit, in health camp settings, in patients’ homes, at public places like malls and to bridge the gap between primary health centres and referral hospitals.\nAnother UK based start-up, Isansys Lifecare systems, has also been operating in India for the last few years. The company provides web-based vital signs monitoring platform and a cardiac monitoring device that can be stuck on to the chest. The device continuously monitors the ECG rhythm of the patient’s heart and the data is transmitted to the doctor. This has been found ideal to monitor non-critical patients who need continuous monitoring. It has also been used in smaller hospitals that cannot afford expensive equipment.\nAnother area where wireless monitoring is becoming popular is the operating rooms. Most of the equipment required to monitor the patient’s vitals during and after anaesthesia can be replaced by wireless devices. These devices provide a surgeon better access to the patient and are also cost effective.\nUse of wireless sensors for patient rehabilitation\nWSN: Wireless Sensor Network has played a key role in facilitating patient rehabilitation. Many a times, even after the patient is cured of his illness, he remains bed-bound, only for the sake of monitoring as most of the monitors are wired. With WSN technology becoming more popular, the mobility of the patient improves tremendously. This translates into earlier recovery and less number of inpatient days.\nAnother important area for the use of WSN is facilitating recovery after surgery, traumatic injury, and stroke and neurodegenerative disorders like Parkinson’s disease. In case of stroke and traumatic injury, there are wireless devices that measure muscle activity and also provide stimulation to the muscles. For neurodegenerative disorders, the electrical activity of the brain is monitored. Several software programmes are available which can be used to store and analyze the images.\nWith the easy availability of high speed internet, mobile medicine has become accessible to a lot of doctors and patients.\nDiagnosis of difficult cases, advice on treatment, interpretation of laboratory reports and radiographic images has become very easy. Mobile medicine has also been used for remote consultation and obtaining second opinion. Guidance during a surgical procedure is yet another application of mobile medicine.\nAnother new dimension of mobile medicine has been the ability of the patient to instantly communicate with the doctor via instant messaging services. Several start-ups in India offer a platform for the patients to directly interact with the doctors. With virtually all the people owning smart phones, these platforms are widely accessed. Patients can easily post their queries and get a diagnosis over the phone! These platforms are immensely popular with the patients as they offer instant solutions to their problems and the patient is saved the hassle of waiting in the crowded waiting rooms. He also gets the consultation free of cost!\nMobile medicine has been integrated into many of the Hospital Information Systems. The clinicians received automated SMS on their mobile phones if certain critical parameters of the patient are deranged. They also receive the patients’ laboratory reports, which help them to decide prompt treatment plan.\nOther commonly used technologies\nRFID has been used for tracking and monitoring the patients. Patient details can also be uploaded on to the chip for easy access. Though it is not yet widely used in India, there are numerous benefits of using RFID. RFID operates on radio frequency. The earlier versions had an obvious drawback that they could be used only in a limited area and the patient had to be close to the detectors to be tracked. This drawback has been overcome and now tracking and monitoring of patients over long distances is possible.\nOne benefit of RFID technology is during mass casualties. The patients can be tagged during such scenarios for easy tracking. Triaging can also be done, which helps in deciding immediate treatment.\nAnother unique use of RFID technology is in tagging the medical equipment like wheel chairs and trolleys which can be easily tracked. This limits the loss of hospital equipment.\nThe RFID technology is widely used today in hospitals across India for restricting access to certain areas.\nAdvantages of wireless technology\nWireless technology has several advantages over wired technology. It is safer, cost effective, reduces the risk of infection in many cases and is patient friendly.\nOnly certain technologies like the capsule endoscopy are out of reach of the common man today. The use of wireless technology, in most cases, translates into significant monetary savings for the hospitals. Quality of patient also improves with the use of wireless technology, as monitoring becomes easy and the discomfort caused to the patient is reduced. There is better communication between the patients and the clinicians and also the other staff members like nurses. The risk of infection which is always present with invasive procedures is significantly reduced by using wireless technology. Since most of these devices are light-weight, portable and easy to use, they afford maximum comfort to the patient as compared to bulky, wired devices. They reduce the burden on health care providers and also reduce informational errors which can lead to patient mismanagement.\nChallenges in using wireless technology\nIt goes without saying that wireless technology needs sound infrastructure. Strong network communication, continuous power supply and data integrity are just some of the factors that need to be addressed before a hospital successfully implements wireless technology. It is of immense importance that wireless technology be totally reliable, especially in critical care units and operating rooms. Consequences of technical glitches in these scenarios, for example an exhausted battery, can be disastrous. Since most of these devices are battery operated, care should be taken to see that the devices are always charged. When a change of battery or recharging a battery is required, alternatives should be in place so that continuous monitoring can be carried out.\nOne of the most important things to be taken care of while uploading patients’ data is security. Patient data has to be totally confidential and access needs to be restricted, yet authorized personnel should easily be able to access the data.\nIn India, wireless technology is being used only in larger cities as the infrastructure that is required is found only in these cities, that too in larger hospitals. Many rural locations in India are deprived of a sound mobile network and WiFi networks are only available widely in larger cities. Many areas struggle with power-cuts and irregular power supply. We still have a long way to go before the benefits of this technology are appreciated widely everywhere. Existing technology itself is not fully utilized till date. Devices which are used for rehabilitation of neurodegenerative disorders need to be designed with special care, so that the patient co-operates in using them.\nSome knowledge of technology is essential in operating these devices. Clinicians who are comfortable with using wired devices and patients with lower education levels might not be convinced to use these devices.\nAir Ambulance Apps\nHi Flying - Air Ambulance International the First Air Ambulance service in India which was fully operational in 1990 - embarked on International patient transfers in 1996 plans an app for easy access to its services in times of emergency patient transportation for state to state transfers for tertiary care centers and International transfers across different countries by both Commercial and Charter flights.\nThe App is the first of its kind and has received very positive response from doctors across the country.\nSummary and conclusion\nWireless technology has become an important part of the healthcare system today. In some form or the other, majority of the hospitals are using wireless technology. Remote monitoring of patients, continuous monitoring during surgeries and during mass disasters, triaging and patient rehabilitation are just some of the many uses of wireless technology. Capsule endoscopy is a widely used diagnostic modality which uses wireless technology. Instant access to physicians via mobiles is possible today because of wireless technology.\nIn most of the cases, wireless technology has many advantages over wired technology. It is patient friendly and reduces costs to the hospitals. However, wireless technology is still in its infancy in India. There is still a long way to go before the required infrastructure becomes commonplace.\nAir Ambulance India']	['<urn:uuid:2265a6b8-b49d-4dd8-9fcd-d07aab235d00>', '<urn:uuid:504dce02-180d-40bd-8434-700192f1d17a>']	open-ended	with-premise	concise-and-natural	distant-from-document	three-doc	expert	2025-05-12T19:58:41.790432	10	143	2701
78	drum reel abu garcia maintenance compare	The two reels have different maintenance requirements. Drum reels are very simple with nothing to break and can last a long time with minimal maintenance. In contrast, Abu Garcia reels require more detailed maintenance including cleaning with Simple Green/Oxy Clean, lubricating various parts with oil and grease, and careful disassembly/reassembly of multiple components like the faceplate, spool, line guide, and gears. However, with proper cleaning and lubrication, Abu Garcia reels can provide decades of reliable performance.	['There are 3 types of reels typically used in bowfishing. The drum or shoot thru hand wrap reel, the spincast reel and the AMS Retriever reel. All work well for bowfishing while having their plusses and minuses. Here are the reels.....\nThis first reel is a simple hand wrap drum reel. While this one is attached to the bow by tape, others have a bolt running out its back that is threaded into the stabilizer hole on the bow. This reel is mounted under the arrow rest.\nThis reel is a shoot thru hand wrap reel. While it is similar to the drum reel in ways, it is mounted differently. The reel is taped again to the bow but higher where the arrow is shot thru the center of the reel.\nThe big plusses for these reel are their simplicity and low price. One can buy one of these for around $10. There is nothing to break on them and they will last a long time. The big minus for these reels are their speed in shooting and wrapping the line. The large diameter of the reels create alot of friction on the line as the bow is shot slowing down the arrow. This reduces the effective distance an arrow can be shot and the depth they can reach under water. They also are harder to wind the line onto and take more time to do so, making second shots on missed fish rare.\nThe next reel is the spincast reel. It is simply an oversized spincast reel that bowfishing line can be wound into. Several of these reels have been manufactured specifically for bowfishing.\nThe plus for these reels are their speed. You can take extremely long shots and have your arrow back and loaded quickly for a second shot at a missed fish. The minus for these reels is that button! If you forget to hit it or accidentally engage the reel you will more likely loose the arrow as the line snaps after the shoot. Worse yet, the arrow might snap back and hit you. Duribility is also an issue. You can expect to change these out every season or two as the shock of shooting really wears them quickly. They are attached to the stabilizer hole of the bow with a reel seat or on the sight mounting holes with a side mount seat. Some even use a short rod as this one does.\nThe final reel is the Retriever manufactured by AMS Bowfishing. The Retriever is the only reel manufactured specifically for bowfishing.\nInstead of being reeled onto a spool, the line is stacked into the bottle, and this leaves no drag on the line when it is shot. Larger diameter line is used with this reel making hand fighting a fish easier. The plusses, it is ver durable and will last many years. Simple to use and no button to push. They are also very lite which is important when holding the bow for hours at a timeThe only minus we can think of is it is a little slower to reel than the spincast. This reel is also the best for big game bowfishing as a float can be used with the slotted model. (shown here)\nHope this gives those considering bowfishing a little insight into reels. As always, call or PM any BAI member for more info on the club or bowfishing!', 'The Abu Garcia Ambassadeur fishing reels can cast a mile, as well as haul in monster catfish and they’ll stand up to decades of use and abuse and keep on performing.\nKeeping your fishing reels clean and well lubricated assures optimum performance. If you don’t have the time or the desire to take your catfish reels apart, your local reel repair shop will clean reels for around 20–30 bucks each. If you like to tinker, taking apart an Abu reel is relatively easy, if you keep some key points in mind.\nHere’s a guide to cleaning your fishing reels. As always, one may find some easier methods, but here’s my method.\nThese basic steps for cleaning and lubricating a fishing reel will work for all baitcast fishing reels. Some of the parts may have slight variations but the overall process will be the same.\nHere’s how to clean and lubricate an Abu Garcia Ambassadeur fishing reel.\nBig thanks to Steven Gonzales for the assistance with this post!\nTools and Supplies Needed\nHere’s what you’ll need to get started cleaning your fishing reels.\n- 10mm open ended wrench\n- Flat tip screwdriver\n- Simple Green or Oxy Clean\n- Clean towel or rag\n- Something to put parts in\n- A toothbrush or cleaning brush\n- Cal’s Drag Grease\n- Ultrasonic Cleaner (Optional)\n- Replacement Parts (As Needed)\nIf you find parts need to be replaced during the cleaning process you can find parts readily available on eBay or even through your local bait and tackle shop.\nIf you’re not sure what you’re looking for or need a reel schematic most of the Abu Garcia Schematics are available online through Abu Garcia.\nSouthwestern Parts and Service is also a great resource for schematics and parts.\nYou can use any reel lube and oil that you prefer. I use a variety of lubricants but the Abu Garcia brand products work well because they do their job well and can be found at most stores.\nFrom my screwdriver kit, I only use the 1.4mm flat tip and the #1 Phillips screwdrivers. The extra flat tip screwdriver is available in the event that the 3 thumbscrews are too tight on the Abu faceplate, but most importantly, for the pawl cover.\nTo clean the parts you can use Simple Green or Oxy Clean. Be careful not to get any over spray on to your line, unless your cleaning agent is biodegradable. This reduces the chances of affecting the properties of your line.\nIf you’re really going to be cleaning a lot of reels you can use an ultrasonic cleaner to clean the parts. We’ve gone to this system because of the volume of fishing reels to clean it really helps expedite the process and the investment is minimal if you buy one like this. We use a mixture of water and a small amount of Simple Green and it makes the cleaning process a breeze.\nBefore you begin, you don’t have remove your fishing line, just put a piece of electrical tape on the line. This comes in handy when putting the reel back together, because it keeps your loose line from getting caught between the spool and the frame. If it’s time to change out your fishing line then go ahead and remove it. Here’s an awesome DIY fishing line stripper you can build for pennies.\nDon’t forget to work in a well-controlled area. There are a few tiny parts, and they can easily be flung off….never to be seen again.\nWe’ll break the reel down into 4 major parts:\nAll parts are referenced as if you are holding the reel in the casting position, with the level wind pointed away from you.\nHow To Clean Abu Garcia Ambassadeur Fishing Reels\nAlthough we’ll begin by removing the right faceplate first, we’ll go over this part of the reel last. This section has the most moving parts and consumes the majority of the percentage of your time when cleaning the reel.\nStart by removing the 3 thumb screws on the right faceplate. Use a flat tip screwdriver, if the screws are too tight. The right faceplate assembly will come off the frame and the spool will come off the spool axle.\nNext, remove the 3 screws on the left faceplate with a #1 Philips screwdriver.\nClean out the faceplate of any dirt and debris. Check the teeth on the cog wheel to ensure that the gear isn’t stripped. Apply the grease/lube along both sets of teeth on the cog wheel. I put one drop of oil in the axle seat (hole in the faceplate) and one on the cog wheel spindle. You can set this faceplate aside, because that’s all there is to that piece.\nNow, we will work with the level wind. Turn the worm gear, so that line guide moves all the way over to one side. You’ll need to do this in order to clear the reel foot with a screwdriver. Remove the pawl cover with the screwdriver. Pull the pawl out from the line guide assembly, taking care not to lose it (small part).\nSlide the worm gear clip off of the worm gear and set this piece aside. Pull out the worm gear and the entire assembly will come apart.\nAgain, take care, here. The worm gear sleeve at the other end of the worm gear cover could come loose (small part). If you have a C4 reel, you will have a bearing in place of the sleeve.\nClean off any dirt or old grease. Run a cotton swab through the worm gear cover and line guide to get out any dirt or debris. Clean the pawl tip as well, since this is what runs through the grooves on the worm gear.\nAs far as the frame is concerned, I usually clean it for cosmetic purposes, but you really don’t want any dirt in and around the ingress points like where the spool makes contact and where the worm gear cover sits. The thumb guard snaps on, and you can remove it to give it a more thorough cleaning.\nLet’s put this assembly back together:\nOn the worm gear itself, put one drop of oil on the tip and one drop on the white collar just below the gear. Slide the worm gear into the worm gear cover. When sliding the assembly back into the frame, remember that the worm gear is tabbed, so it will flush into the frame when seated correctly. If the plastic sleeve fell out of the worm gear cover, make sure it’s oriented correctly. If you have problems seating the assembly, usually, it’s because the sleeve is backwards.\nHere’s where having a third hand comes in handy. Slide the complete worm gear assembly back into the frame. While it may take a little patience and some finger fumbling, you want to remember 3 key points:\nEnsure the top tip of the line guide seats inside the slot of the top frame post.\nEnsure the bearing doesn’t fall out and onto the floor…..never to be seen again (yup…learned that one the hard way).\nMake sure the tabs of the worm gear cover fit snugly into the reel frame on the sleeve/bearing side.\nWhen you reinstall the worm gear clip, make sure it seats between the white collar and the gear. Flip the frame over and move your line guide back over to one side. Pop in the pawl and move your line guide slightly, until your pawl is almost flush and is inside the grooves on the worm gear. Screw your pawl cover back on and turn the worm gear to verify function.\nMove the line guide to the center and place a drop of oil on either side of the line guide onto the grooves of the worm gear (here is one place that you can use two drops). Move your line guide across back and forth to spread the oil around. I also place a drop of oil in the groove on the thumb guard post, where the top potion of the line guide slides across.\nNow, you are done with this section. Place this aside, and grab your spool.\nPull off the plastic spool pinion gear. Place a drop of oil on the spool bearing and pop the spool pinion gear back on.\nNote: Older Abu’s may have a brass bushing instead of the bearing. I recommend replacing the bushing with a spool bearing for a much smoother reel. If you wish to do this:\nSpool bearing – part number 13472\nThe part number is the same for the 4000 – 6000 series.\nNow, flip the spool over and place a drop of oil on this bearing as well. There is no need to remove the spool brake. That’s it for the spool. Now for the good stuff…..\nThe spool axle is held in by a little black clip inside the spool tension knob. Loosen the spool tension knob and pull the axle out.\nUsing a #1 Philips screwdrivers, remove the handle cap.\nHere, you will see a small C-clip. Be VERY careful with this clip. I’ve lost way too many, so I keep spares handy. This C-clip not only keeps your handle nut from getting loose, but it keeps your handle gear shaft tight and down to align your gears inside the faceplate (just some FYI). If you lose this, it isn’t the end of the world, but you could notice some skipping issues with your reel. Not a fun time.\nWith your 1.4mm flat tip screwdriver, carefully wedge the tip in between the C-clip and the handle post. Once again, be careful not to fling this clip off into the infinite reaches of space.\nRemove the handle nut with the 10mm open ended wrench and remove the handle. There will be a handle spacer plate right underneath the handle, so take care when removing the handle.\nNow unscrew your star drag. Turn your assembly over and two metal tension plates will come out off of the shaft.\nTurn your assembly right side up again and remove the two inner faceplate screws. Lift the faceplate off of your brake plate assembly.\nGrab the handle shaft gear and remove that entire assembly off of the brake plate post.\nRemove the excess grease from the brake plate assembly. There is a little copper washer that sits on the brake plate post.\nRemove any excess grease and wipe it clean. Place a small drop of oil at the base of the post and place the washer back on top of the oil.\nNext, remove the drive gear from the handle drive shaft, including all the metal and felt drag washers, plus the anti-reverse bearing sleeve.\nClean off all the grease from the handle drive shaft.\nTurn the drive gear upside down and you should see a black felt washer. Carefully, clean any and all excess grease and oil. Grease and oil will seep into this assembly and the washers and will cause slippage with your drag. Place that black felt washer back onto your handle drive shaft.\nWipe off any and all oil and grease from the remaining felt and metal washers.\nPlace your drive gear back onto the handle drive shaft.\nNow, place all the washers back onto the drive gear in the order that you see in this picture from left to right.\nA felt washer goes on the drive gear first, then the round washer.\nNext, add another felt washer, then the round tabbed metal washer.\nFinally, add the last felt washer, then the round washer with the raised collar.\nOn the brake plate assembly, add one more drop of oil on top of the copper washer that sits on the post, and slide the drive gear assembly back onto the post.\nApply a light amount of grease between the brake plate pinion gear and the drive gear.\nClean the anti-reverse bearing sleeve and place it back on the drive gear assembly.\nClean out the faceplate. Place one drop of oil in the axle seat and two drops inside the anti-reverse bearing.\nGo back and second check the washers on your drive gear. Make sure the washer with the tabbed ends is sitting correctly in its grooves.\nSlide your faceplate back onto the brake plate assembly, minding the alignment of the push button release, so that you get a proper seat. Screw down the faceplate with the two inner screws.\nNow you will be reinstalling the two metal tension plates. Orientation is important. The easiest way to remember, is that the tension plates should mimic the open and closed parenthesis shapes as they fit onto the handle drive shaft like so: ( ) The plate curving upwards should go on first, then the plate curving down.\nAt this point, I tighten my spool tension knob almost all the way down, because it’s easier to get to.\nScrew your star drag back on.\nTurn your assembly over and place a drop of oil in the hole of your metal spool pinion gear.\nSlide the axle (brass collar side in) back into the spool pinion gear until you hear it click. It should do a slight lock into the black clip inside the spool tension knob. Once it’s in, you can back off on the spool tension knob. On the inside of the metal collar that surrounds the gear/axle, place two drops oil. You can put the drops anywhere, but I usually put them at opposite ends of each other for even distribution. This will help to lube the brake blocks on the spool for smoother casting.\nSet the assembly aside for now. Grab the frame and screw on the left faceplate.\nBack to the right faceplate assembly, slide your spool onto the spool axle, ensuring that the brake blocks are not protruding out of the collar and that the brake block assembly is aligned with the gear for a perfect seat.\nWith both assemblies oriented this way, I like to pop the frame assembly on from the top. This way, I know my spool is still seated correctly. Take care and don’t force the pieces together. Ensure your alignment is correct. Once the assemblies have snapped into place, tighten down the 3 thumbscrews.\nReinstall the handle spacer plate, with the two outer ends flat against the star drag.\nApply a drop of oil at the base of each paddle on the handle and turn the paddle a few times to spread it out.\nPlace the handle back on the shaft and tighten the nut with the wrench. Before re-attaching the C-clip, check the alignment of the handle nut cap.\nOnce again, very carefully, pop the C-clip back onto the post.\nScrew down the handle nut cover and you are done!\nThis article is part of an ongoing tutorial on rods and reels for catfish, check out the Ultimate Guide To Catfish Reels and the Ultimate Guide To Catfish Rods for even more in depth information on choosing and using rods and reels for catfish.']	['<urn:uuid:21db2741-15ea-4098-8917-6dabc3c30442>', '<urn:uuid:98264bf6-d432-4bba-b5e7-aee10372c326>']	open-ended	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T19:58:41.790432	6	76	3081
79	inflation protection options retirement income annuities	A Cost of Living Adjustment (COLA) rider can be added to fixed index annuities to protect against inflation. This rider allows the monthly income amount to increase based on inflation measures chosen by the issuing company. Some annuities offer a fixed annual percentage increase, while others adjust income based on the annuity's interest earnings. However, initial income payouts with a COLA rider may be lower than standard annuity payouts, potentially taking years to catch up with level-income options.	['Fixed Index Annuity Riders — What You Should Know About Them\nWhen using an annuity for retirement income security, there are many questions that need to be considered. Annuities can pay you a guaranteed income for life, but they aren’t for everyone. They need to have a defined purpose in your retirement plan that solves a specific problem.\nThe annuity owner can determine when the annuity begins to pay out, and how the payouts occur. The payouts can occur for a fixed period of time, or they can be set up to pay out for the remainder of the contract holder’s life.\nDone? Not yet. The financial professional offering you the annuity might suggest a series of additional benefits, called “riders,” which can be attached to your annuity. A rider can offer add-on benefits to your base contract. It can make the decision-making process even more involved.\nHere are some of the types of riders you might find on a fixed index annuity. We will also answer some of the questions that can arise when you explore these riders.\nGuaranteed Withdrawal Benefit\nOne key consideration to an annuity is how you might receive an income stream from the annuity contract, but still retain some liquidity. While annuitization, or converting your annuity money into an irreversible stream of payments, secures a guaranteed income source, you give up all access to your money.\nOne alternative, an annuity income rider can offer a balance between income and access. An income rider is often known as a guaranteed withdrawal benefit. Life insurance companies use names such as “guaranteed lifetime withdrawal benefit” or “guaranteed minimum withdrawal benefit” in their annuity products.\nA guaranteed withdrawal benefit guarantees that someone will receive payments until death, often for a rider cost (but not always). These payments will keep going even if the annuity money goes to zero. What’s more, the annuity can continue to earn interest.\nThe annuity owner can stop and restart payments at any time, thus offering more freedom of choice than a pension. If the contract holder also needs some liquidity, they can take free withdrawals of up to a certain percentage from their annuity money.\nOn the other hand, free withdrawals can affect the payments that you receive with the guaranteed withdrawal benefit, as a trade-off. It’s good to keep that in mind if you choose to tap into your fixed index annuity for liquidity.\nLifetime Income Benefit Rider\nA lifetime income benefit rider is another name for an income rider. This is similar to how an income rider can be called a guaranteed withdrawal benefit.\nThis rider guarantees a set regular income payment from the annuity, which can be designated to start at some time in the future, and can be paid out monthly, quarterly, or annually. The guarantee here is that the payments will continue until death, even if the principal is exhausted before that happens.\nThis can help as a strong defense against the danger of running out of money in retirement. Why might someone not want an income rider attached to their annuity? There are often rider fees, which usually cost around one percent.\nIt may also be a negative if you are looking to your annuity primarily for growth. In exchange for the lifetime income benefit, your annuity may have lower rates on it, limiting your money’s growth potential compared to what other annuities can offer. This enables the insurance company to be able to pay for the extended period payouts.\nDeath Benefit Rider\nYou might also consider certain add-on benefits for estate planning purposes. Yes, it’s possible for an annuity to pay a guaranteed income for the life of the owner. Additionally, a death benefit rider can be attached to the contract so that a final payment is made when the annuity owner passes away.\nA death benefit rider allows for the owner’s heirs to receive at least the amount of principal premium paid for the annuity – a kind of return of premium. There is also a death benefit rider that allows for a multiplier on the contract value in case the owner passes away early. Some contracts with this rider benefit require for some years to have passed initially before it might apply.\nIt’s also possible to purchase a living benefit to heirs, transferring the annuity assets to a surviving family member.\nAs stated above, there are many choices. The cost is usually a percentage of the principal, paid on an annual basis.\nLong-Term Care Rider\nThink hard about this opportunity. Long-term care insurance is difficult to purchase these days, especially with underwriting, and is very expensive.\nBut people are living longer in the 21st century, although not always in a condition where they can care for themselves. Long-term care for a loved one can put a family into severe financial straits.\nA long-term care rider allows for an annuity payout to be adjusted so that the annual payout can help cover the cost of long-term care. This rider usually covers a short period of time, such as three or five years. But it allows you and your family to have the funds they need to pay for the long-term care.\nThis rider is a good idea for someone who has a family history of living a long time, even in the case of advanced mental or physical disability.\nHowever, riders differ based on contract details. Some contracts return to the original payout amount after the maximum allowable period for the long-term care withdrawals has passed (the suggested three or five years stated above). On the other hand, some contracts stop payments altogether if the original contract value has reached zero.\nThese details are often complex and need to be discussed with a financial professional before agreeing to any terms.\nCost of Living (COLA) Rider\nCost of living adjustment (COLA) riders aren’t exactly the most popular subject in town.\nNevertheless, with shocks to the economy by supply-chain and geopolitical changes, inflation has once again raised its ugly head. The cost of living is increasing, and thus, the need for a COLA rider.\nAn annuity contract calls for a guaranteed and specified monthly income. For a fixed index annuity, that income amount is typically the same every month. A COLA rider allows that amount to be increased based on a particular measure of inflation.\nA caution: the company issuing the annuity gets to determine the inflation measure used. It might not be the best measure of current monetary value.\nSome annuities allow for a fixed, annual percentage COLA increase rather than an annual determination of current inflation rates. Others allow for adjusting income depending on the fixed index annuity’s interest earnings, which vary and aren’t guaranteed.\nIn other cases, the income payout may be lower, in early years, on a COLA rider than if you went with a plain-vanilla annuity payout that doesn’t have increasing income. If that is so, it may be a number of years before the increasing income option catches up with this plain-vanilla level income option. Ask your financial professional to explain this in depth.\nIf you are looking at this option, ask your financial professional about the fine details of any rider benefits that you might tap to combat inflation. Don’t be afraid to ask any questions about something you don’t understand, and don’t feel pressured to move forward without understanding your options. This is your life savings, after all.\nSome Final Food for Thought\nThis article only touches on some of the finer points of attaching a rider to a fixed index annuity contract. There are several other choices to be made.\nYour options regarding an annuity should be discussed with your financial professional, who should be experienced and understand your personal situation well. If tax questions or other questions about your situation arise, bring in your tax advisor and other relevant professionals for personal guidance.\nAs you consider different rider options with any fixed index annuity you are exploring, it helps to keep these questions in mind:\n- How is this annuity rider benefit solving a problem in my financial situation?\n- Do I understand this fixed index annuity rider as well as its pros and cons?\n- What am I giving up elsewhere in my annuity by opting for this rider?\n- What does the rider cost all-in?\n- What will be the cost of choosing not to go with that fixed index annuity rider benefit?\n- Am I counting on this one annuity to cover too many potential issues in my retirement plan where it might fall short? (ex: lifetime income, long-term care, so on)\nLike with any financial decision, you are putting money toward something that you hope will provide financial benefit to you for years to come. You want to make certain you are making the correct decision for your needs.\nExpert, Independent Guidance Can Make a Difference\nIt could be unwise to attempt to make those decisions yourself, unless you have a financial advisor or agent who understands these products and the issues they solve well.\nIf you are looking for a financial professional to help with these what-ifs, or give a second opinion, then no sweat. Many independent and experienced financial professionals are available at SafeMoney.com to assist you.\nYou can get started by visiting our “Find a Financial Professional” section, where you can connect with someone directly. Please feel free to request a phone meeting or initial appointment to discuss your goals, situation, and explore a potential working relationship. Should you need a personal referral, please call us at 877.476.9723.']	['<urn:uuid:a95f28a9-ff61-4ba8-88c3-e4d70e7b692a>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	6	78	1597
80	How do both green manure crops and animal manure contribute to nitrogen enrichment in soil, and what are the potential drawbacks of each method?	Green manure crops, particularly legumes like field peas or vetch, contribute nitrogen through their nitrogen-fixing ability when grown and then worked into the soil. For animal manure, it naturally contains nitrogen along with phosphorus and potassium. However, both methods have drawbacks. For green manure crops, you must wait 2-3 weeks after incorporating them into soil before planting vegetables to allow for decomposition. Animal manure presents several challenges: fresh manure can contain dangerous pathogens, may include weed seeds that passed through the animal's digestive system, can contain high levels of soluble salt (5-10% of dry weight), and repeated applications can sometimes cause zinc deficiency, particularly in sandy soils. Additionally, fresh animal manure contains volatile nitrogen forms like ammonia that can burn plants or inhibit seed germination.	"['We’re all familiar with that wonderful brown substance we call manure, or, as I prefer to call it, “fruit of the butt.” Manure is a rich source of nitrogen for our growing plants, and it can also serve as a mulch. Yes, without manure, life would be pretty sh…, well, you get the idea.\nNow, I’d like to introduce you to a different kind of manure. This kind does not come from a bovine or equine backside, but it grows right out of the soil. These are various plants known collectively as cover crops or green manure. Green manure plants are grown for the sole purpose of being killed by cold weather, chopped up, and worked into the soil. Like the other kind of manure, it provides nitrogen, but it does a whole lot more. Green manure crops can crowd out weeds, reduce soil erosion, and improve the overall condition of the soil.\nGreen manure crops are generally planted in late summer or early fall, then chopped up and worked into the soil in the early spring. Alternatively, they can also be planted in spring or summer, and then tilled into the soil before planting a vegetable crop. They can also be planted in place of a vegetable crop and then worked into the soil to condition it for the following year’s crops.\nGreen manure crops consist of both legume and non-legume plants. Legumes such as field peas or vetch are planted for their nitrogen-fixing ability while non-legumes – grain crops like rye or wheat are planted for their ability to crowd out weeds. A wide variety of plants can be used as green manure crops. The table below lists some common ones.\nThe University of Wisconsin Horticulture, Division of Extension gives these instructions for planting. To plant a cover crop or green manure, first clear the planting area of any large stones and other debris. Rake the area smooth and broadcast seed according to the seeding rate given in Table 1 or as recommended by the seed provider. Rake the area again to incorporate the seeds into the soil, and lightly water the area. To prevent the cover crop from self-seeding in other areas of your garden, and to utilize the cover crop to its fullest potential, cut down plants when, or just before, they start to flower. You can cut plants by hand, or by using a trimmer, brush cutter, or mower. Cutting before flowering not only prevents the cover crop from going to seed, but also stops the plant from taking up nutrients from the soil to store in its seed. Once plants have been cut, incorporate the plants into the soil (using a shovel, pitch fork or rototiller) where they can more readily decompose. Allow approximately two to three weeks for the cover crop to decompose before planting your vegetables into the soil. (Source: University of Wisconsin Horticulture, Division of Extension)\nSo if you are looking for a way to improve the condition of your soil, and the brown manure isn’t doing the job, then why not try the green?', ""Using manure as fertilizer can be great for your garden. Whether you’re using cow manure fertilizer, chicken manure fertilizer or horse manure fertilizer you can make your plants very happy with manure fertilizer.\nBut before you start using manure in your garden, there are a few things you should know about using manure fertilizer.\nUsing Manure as Fertilizer Offers Lots of Pluses – And a Few Potential Negatives\nManure fertilizer can do lots of wonderful things for your garden soil and for your plants.\nAll manures contain the big 3 nutrients – nitrogen, phosphorus and potassium. But they also contain an abundance of many trace minerals and nutrients.\nIncorporating manure into your garden soil will also add lots of organic matter to your soil, and that’s a very good thing.\nIt will help to improve the tilth (soil structure) of your soil, which will help your plant’s roots to breathe easier (yes, roots breathe!). The organic matter that manure fertilizer adds to your soil will also improve your soil’s water-holding capacity.\nWhat are the negatives to using manure as fertilizer? Here are some of the potential negatives that you could encounter when using manure fertilizer:\nIf the animal that so cooperatively produced your manure for you was chowing down on weeds that contained mature seeds, then that manure probably contains lots of weed seeds just raring to sprout. The seeds can pass through the animal’s digestive tract unharmed, exiting the nether region of the animal in a pile of poop that’s a perfect incubator for weed seed germination.\nUsing that manure as fertilizer could place you on the losing side in your eternal battle against weeds.\nIf you’re certain that the manure is weed-seed-free, then no worries. Otherwise, you can greatly reduce this risk by thoroughly composting manure before you apply it to your garden. Or you can simply purchase manure that has already been composted. The composting process will destroy the weed seeds.\nManures often contain a fair amount of soluble salt – often as much as 5% to 10% of the dry weight of the manure. This isn’t usually a problem, as rainfall and irrigation will normally leach the salts out of your soil. But if your soil drains poorly, or you already know that you have a problem with salinity in your soil, you might want to be cautious about using manure.\nRepeated applications of large quantities of manure can sometimes cause a deficiency of zinc. This is more likely to happen with sandy soils. A potential zinc deficiency isn’t a reason to avoid using manure, but it is something to be aware of and monitor with soil tests.\nIt's important to remember that we’re only talking about using manure from herbivores – non meat-eating critters. NEVER use manure from dogs, cats, pigs – or humans. These types of manures can potentially infect humans with a number of disease pathogens and parasites.\nCow manure fertilizer, chicken manure fertilizer and horse manure fertilizer are some of the most commonly used manures. I’ve also used turkey manure fertilizer with great success (I bought it composted and bagged).\nTips for Using Manure as Fertilizer\nThe first rule about using manure fertilizer is to NEVER apply fresh manure to a garden that has already been planted. Rather, incorporate fresh manure into your garden soil a few months before planting. There are several reasons for this:\nIt’s quite possible for fresh manure to contain dangerous pathogens such as salmonella and e. coli. If you apply fresh manure to your growing garden, you run the risk of some of your produce becoming contaminated with these pathogens.\nOnce the manure has been incorporated into the soil, any pathogens in the manure will eventually die out, posing no risk whatsoever. Composting manure before using it will also eliminate this risk – but the manure must be composted thoroughly and completely.\nFresh manure contains nitrogen in a volatile form such as ammonia, which will be likely to burn your plants or inhibit seed germination. Aging or composting the manure will eliminate this risk.\nUncertainty About Fertility Levels\nUnlike labeled fertilizer blends, you won’t know exactly what you’re getting with manure. So how much to apply to your garden will be guesswork, at least to a certain degree.\nOne way around this is to buy packaged, composted manure. Packaged manure compost is usually labeled for the N-P-K and trace nutrients, so you’ll be able to calculate how much to use as easily as with any synthetic blend.\nThis type of manure compost is much more expensive than buying bulk, fresh manure, but the pricing is still usually more-or-less competitive with synthetic fertilizers. (See some examples below.)\nManure is a slow-release fertilizer. It takes time for the nutrients in manure to become available to your garden plants. As the manure decomposes, the nitrogen and other elements are converted into a form that is useable for plants. In fact, some of the nutrients in manure become available in plant-friendly form over a period of years.\nClick on the product captions below to purchase or for more info:\nJust to be clear, composted manure, like the products above or that you compost yourself, requires no waiting time. You can incorporate composted manure into your soil at the time of planting.\nBut if you incorporate fresh manure into your garden soil, the minimum wait time before planting is 2 months. Longer is better.\nWhen adding fresh manure to your garden soil, work it into the top several inches of your soil. Never leave it lying on top of the soil - always work it in. Manure can lose much of its nitrogen through exposure to the air.\nMother Nature’s Fertilizer\nAs long as you have knowledge of how to use it safely, using manure as fertilizer can provide a wonderful boost to your garden. And if you want to grow your garden using organic standards, manure fertilizer will likely be a mainstay of your fertilization program.\nManure is, after all, Mother Nature’s own fertilizer.\nDo You Have Comments or Experiences to Share About This Topic?\nDo you have thoughts, comments, experiences with this topic? We'd love for you to share!""]"	['<urn:uuid:ea14a19e-fb5f-4786-ae79-32e8efe53540>', '<urn:uuid:a77ea7ba-de3e-4e59-809c-566ec8f2f3e3>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:58:41.790432	24	125	1543
81	As someone interested in both religious practice and crowd psychology - what motivates people to join mass gatherings like Hajj, and how does being in large groups affect decision-making?	Hajj is a fundamental pillar of Islam that every able-bodied Muslim who can afford it must perform once in their lifetime. The motivation comes from its deep spiritual significance - it erases past sins if performed sincerely and teaches unity, equality and brotherhood among Muslims regardless of race or nationality. When it comes to group decision-making, research has shown that in large groups, the presence of undecided individuals can be important - they can help swing collective decisions toward the majority view. However, copying others' behavior isn't always automatic - studies of crowd behavior show people maintain some independence, with only about 40% following group actions even in large crowds. This prevents people from getting 'trapped' in meaningless group behaviors and allows them to respond to genuinely important stimuli.	"['Significance of Hajj\nThe foundations to the religion of Islam are the five pillars on which it is built upon and the fifth of those pillars is Hajj. Hajj is considered to be one of the most significant deeds for all the Muslims around the world. Every able-bodied Muslim who can afford the trip of Hajj is obliged to make this divine and spiritual journey at least once in his lifetime. Allah almighty has clearly mentioned in the Holy Quran about the obligation of Hajj.\n“In it are signs manifest; (for example), the Station of Abraham; whoever enters it attains security; pilgrimage thereto is a duty men owe to Allah, those who can afford the journey; but if any deny faith, Allah stands not in need of any of his creatures.” (3-97).\nIn this Ayah, Muslims are also warned that if those who have the essential means to perform the pilgrimage still fail to carry out the duty through sheer ingratitude then Allah is neither in need of their pilgrimage nor of the people.\nThere is much enlightenment behind performing Hajj and one of those is that Almighty Allah prepares his men to have the perfect form of obedience to Him. The significance of many acts of Hajj cannot be comprehended by the limited thinking of ordinary man. However, a Muslim is obliged to accept these acts and to fulfill them out of His obedience to Allah.\nThe significance of Hajj can be valued by the fact that the Hajj works as an expiation of a man’s sins and erases the past sins of the pilgrim if is performed whole heartedly without deliberate violation of the rituals that make up the entire ibadah of Hajj. Abu Hurairah (may Allah be pleased with him) said:\n“I heard the Prophet (PBUH) say: ‘Whoever does Hajj for the sake of Allah and does not have sexual relations (with his wife), commit sin, or dispute unjustly (during the Hajj), will come back like the day his mother gave birth to him.’” (Bukhaari: 1449, Muslim: 1350)\nHajj is considered as a Jihad of the weak and women and its retribution is nothing but paradise. Allah Almighty forgives the pilgrim and those too for whom he prays. Moreover it removes the poverty just as the blacksmith’s furnace separates dross from iron. As narrated by Abdullah Ibn Mas’ood :\n“The Messenger of Allah (PBUH) said: ‘Keep on doing Hajj and Umrah, for they eliminate poverty and sin just as the bellows eliminate impurities from iron and gold and silver.’” (Tirmidhi: 810, Nasa’i: 2631)\nOne of the most inspiring consequences of Hajj is that the one who leaves his home for Hajj and dies in the way will keep gaining the reward of every year’s Hajj till the doomsday. He will not be made to account for his doings on the Judgement Day and will be entered into heavens without reckoning.\nThe gathering of Muslims all around the world reminds us that we are all Muslims and that we are all brothers and sisters in Islam. It teaches us that the Muslims are not joined together on the basis of color or race but on the root of belief in Allah (SWT) and his Apostle (PBUH). The pilgrimage brings Muslims of all countries, colors, and races to one place and helps in strengthening the true spirit of unity, equality and brotherhood. These are the values on which Islam builds the human society and it will continue this way till the Day of Resurrection.', ""Copyright © 2012 All rights reserved.\nCurrent Biology, Volume 22, Issue 12, R467-R470, 19 June 2012\nFeatureAdd/View Comments (0)\nIs it wise to join the crowd?\n- Recent research has provided new insights into how crowds of people and herds of animals share information and make decisions. Does this knowledge help to keep people safe, and does it extend to online swarming behaviour? Michael Gross investigates.\nImagine you're walking down a busy shopping street and come across an individual or a small group of people staring intently upwards at a third-floor window. Will you stop and look up as well? Will you be more likely to follow their gaze if there are more people looking up already? Would you be more likely to risk a peep if you were behind the backs of those looking up, so they can't see you're copying their action?\nScientists have studied gaze-following mainly in laboratory settings since the 1960s, but only recently has it become possible to track the movements and responses of thousands of people in naturalistic settings, such as a crowded street or a railway station. The recent studies of large numbers of humans ‘in the wild’ have come to different conclusions from the earlier more restricted ones.\nThe group of Iain Couzin at Princeton, together with David Sumpter at Uppsala and colleagues at Oxford, has recently reported several studies using gaze-following experiments in natural settings. In one group of experiments, the researchers employed a single person or stimulus groups of up to 15 people to walk into the middle of an area they were filming from above, stop, and look upwards at the camera for 60 seconds. From the video footage covering an area of 10 metres width and eight metres depth, the researchers could analyse precisely how passersby responded to the stimulus, including not only length and location of gaze-following, but also whether they changed direction or slowed down. They used statistical analysis to link the behavioural output variables to parameters such as the size of the stimulus group and the relative location of passersby in relation to the stimulus group (Proc. Natl. Acad. Sci. USA (2012), 109, 7245–7250).\nTheir analysis shows that the proportion of passersby following the gaze of the stimulus group increases gradually with the size of that group. A single person doing the stimulus routine only inspired less than 5% of passersby to look up. There is a significantly stronger response to group sizes from 5 to 10 people, with around one third of passersby becoming curious. Beyond that, the increase seems to level off at around 40%.\nThis profile differs significantly from a study reported in the 1960s, where a steeper rise at smaller group sizes led to a higher saturation. Psychologists have commonly interpreted that result in terms of a quorum response, which is often observed in animals. According to this model, as soon as a critical number of individuals engages in a behaviour, virtually all others will copy it. The crowd or herd ‘crystallises’ from a state of randomly distributed gaze orientations to one of uniformly aligned behaviour.\nThe new work from Couzin and colleagues, however, suggests that pedestrians in the street are not quite as easily aligned as the quorum model suggests, and that it can be described as a proportional saturating response instead. Mathematically, this model can also accommodate the earlier data, but leads to a different and more complex interpretation of the behaviour. “We find that our studies are mutually supportive of the view that gaze-copying is not as strong as previously assumed,” Couzin summarises. “This turns out to be a useful thing in that if it were much stronger it could lead to ‘crystallisation’ of gaze-copying behaviour in crowds whereby people get ‘trapped’ into meaningless gaze-copying, as opposed to directing gaze to stimuli that are likely to be more pertinent.”\nAdditional complications come from the paths that people follow. Those whose original trajectories would lead them to pass behind the backs of the stimulus person or group were more likely to look up than those who would pass in front of them. The researchers conclude that the gaze-following is not due to some kind of social pressure. Instead, looking without being seen to be looking appeared the more attractive option.\nSimilarly, in a separate study, Couzin's group observed that moving pedestrians are more likely to follow the gaze of those walking in front of them than of those facing them and moving in the opposite direction (Biol. Lett. (2012), doi:10.1098/rsbl.2012.0160). “It is possible that this response serves an important adaptive function,” Couzin explains; “since those ahead, and moving in the same direction, are likely to be experiencing the world that we will shortly experience, it pays for us to be tuned to pay attention to their gaze behaviour.”\nAnother factor is the distance from the stimulus. In a further type of experiments, where the stimulus group acted suspiciously to attract attention from passersby, the authors could show that the shared attention of passersby remained confined to distances of less than two metres.\nJolyon Faria from the University of Leeds, together with colleagues in Germany, noticed similar distance constraints when they observed pedestrians crossing a road (Behav. Ecol. (2010) 21, 1236–1242). “The functional reason for this localised behaviour is probably that only the behaviour of people very close to you is really relevant because many of the important stimuli (e.g. an oncoming car) only apply in this localised spatial setting,” explains co-author Jens Krause from the Leibniz Institute of Freshwater Ecology and Inland Fisheries at Berlin.\nSafety in numbers\nPedestrians crossing the road may also be seeking safety in numbers, assuming that an oncoming car is more likely to slow down for a group of them than for a single person. Similarly, research has shown that schooling fish are safer from predators in larger groups. This holds true in spite of the fact that the large school of prey fish is more visible and predators may find them more easily.\nIn a Current Biology paper published online (Curr. Biol. (2012), doi:10.1016/j.cub.2012.04.050), the groups of Iain Couzin at Princeton and Nils Olav Handegard at the Institute of Marine Research in Bergen, Norway, have for the first time studied the dynamic interaction between a schooling fish prey and its predator to show in which ways the schooling behaviour can offer protection, and how the predator tries to counter this effect.\nSpecifically, the researchers used high-resolution sonar imaging to observe schools of juvenile Gulf menhaden (Brevoortia patronus) and their predator, the spotted sea trout (Cynoscion nebulosus), in the coastal areas of the Gulf of Mexico. The faster swimming predator typically approaches the school from behind, but the more manoeuvrable prey school swiftly parts to evacuate the predator's path.\nThe researchers deliberately chose a system where they can rule out the interpretation that all the individuals see the predator coming. The natural environment of these fish species is so turbid that the visibility is restricted to the nearest neighbours in the school. Thus, all collective responses of the prey school are likely to be caused by social information exchange, most likely by responding to the behaviour of the nearest neighbours.\n“In this environment the low visibility also likely contributes to the tendency for predators to group together during hunting, forming lines with as many as five individuals in a fast-moving winding attack. We find evidence that predators hunting in this way ‘cut’ groups up and prevent them from reforming as quickly,” Couzin explains. “The collective information transfer among prey is so effective that this strategy appears to be a way to create, and then hunt, small groups where individuals are more vulnerable since they cannot benefit from fast and long-range transfer of cues regarding the location of predators as can large prey schools.”\nSchooling fish have also served as a model system for studies of decision-making in large groups. In a pair of papers published last December and January, Couzin and his Princeton colleague Naomi Leonard, with collaborators in Germany and the UK, used theoretical and experimental methods to determine how groups of animals make collective decisions when there is a disagreement between a larger part of the group (majority) and a smaller but more opinionated group (minority). Models showed that the presence of a large group of undecided individuals makes it more likely that a collective decision is made (Proc. Natl. Acad. Sci. USA (2012), 109, 227–232). The models and experiments with schooling fish also demonstrated that the presence of undecided individuals can swing the collective decision in favour of the majority (Science (2011), 334, 1578–1580).\nAlthough cynical comparisons with herding behaviour of human voters would be easy to make, Couzin warns that the models don't cover the complexities of the democratic process. The research could help to understand smaller groups of people, however. Couzin says that “it may — although I stress we don't know yet — relate to decision-making among some human groups, such as committees or juries.”\nUnderstanding the information flow and behaviour of human groups and crowds is particularly important where there are large numbers on the move and a collective movement may lead to fatal results, as in the Loveparade disaster in Duisburg, Germany, where 21 people died in 2010.\n“One problem with our data for human crowds is that it (necessarily) comes from non-emergency situations, and the heuristics that people use walking down a busy pedestrian street might go out the window when they trying to escape from a fire or disaster situation,” warns Andrew King, who studies decision-making and information flow in human and animal groups at the Royal Veterinary College, University of London. “So we should be careful about using these data to make inferences about what people do in emergency situations.”\nNevertheless, scientific analysis of crowd dynamics and installation of automatic cameras seems to have improved the safety situation at Mecca, where around three million pilgrims congregate each year. Communication difficulties between the visitors arriving from many different countries have contributed to a number of disasters in which hundreds of people died.\nAt such large events, the smooth and unhindered flow of the crowds is crucial. Detailed investigations from Mehdi Moussaid at Toulouse, France, have recently added to our understanding of the flow dynamic of human crowds (PLoS Comput. Biol. (2012) 8: e1002442).\nCopying others or going with the flow can lead to escalating danger, for example in panic situations or in riots (see Curr. Biol. (2011) 21, R673–R676). Even crowds linked by means of electronic communications can develop herd behaviour and panic, the most notorious current example being the financial markets (see Curr. Biol. (2011) 21, R795–R798).\nHerd behaviour has also been invoked to describe escalating collective responses seen in fast-moving online media such as twitter. However, Jens Krause cautions: “I haven't seen any really good data sets on comparing online processes to real-life behaviour and would be careful to generalise because some factors may not be directly comparable.”\nDuncan Watts from Yahoo! has conducted some research on ‘social contagion’ in online communities and argues that medical analogies such as the references to ‘viral’ memes are exaggerating the efficiency of information spread online. “Unlike for influenza, to which you're either exposed or not exposed, even the ideas you do encounter have to compete for attention with everything else that you're exposed to,” Watts concludes (http://poptech.org/e1_duncan_watts).\nA recent modelling study finds that in the situation of a large information overload, randomness has a large role to play (Sci. Rep. (2012), 2, article number 335, doi:10.1038/srep00335). The authors conclude: “Surprisingly, we can explain the massive heterogeneity in the popularity and persistence of memes as deriving from a combination of the competition for our limited attention and the structure of the social network, without the need to assume different intrinsic values among ideas.”\nThe influx of many conflicting signals may also help to explain the relatively low response in the gaze-following experiment. David Sumpter, one of the co-authors of the study, comments: “I would say that humans have different responses depending upon how important the information is to them. In gaze-following, we found a weak response to the gazes of others, while in fish and ants we found strong quorum-like responses. But if humans had to make their minds up about something important I would imagine that they would use quorum responses because these allow for more accuracy and speedier decisions.”\nMost of the time — as long as we are not panicking — we humans are quite good at deciding which cues from the crowd to take and which to ignore. Personally, as a keen photographer, I would of course follow the gaze of someone looking up, as there might be a snap in it for me. As animals endowed with consciousness and self-awareness, we humans have the freedom to decide whether we want to do our own thing or go with the herd.""]"	['<urn:uuid:adeb4a27-b6ee-4e57-87f8-8bc9ca0104b6>', '<urn:uuid:13954bf8-7633-4c89-bb3c-9b8dae229dc3>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:58:41.790432	29	129	2734
82	physical therapist looking for difference dead bug exercise partial crunches ab strength	Both Dead Bug and partial crunches are core-strengthening exercises, but they differ in execution. The Dead Bug is performed by lying on your back, raising arms toward ceiling and feet off floor, then alternating opposite arm and leg movements while keeping abs tight. Partial crunches involve lying with knees bent, feet flat, raising shoulders off floor with crossed arms, while keeping feet, tailbone, and lower back in contact with the mat throughout the movement.	"[""When that lower-back pain hits, the last thing you want to do is work out. But exercise, chosen carefully and done properly, can actually alleviate lower-back pain and help build strength that can keep those twinges at bay. Whether the pain is from an injury, poor posture or hours on end sitting on your end, check with your doctor first to get her go-ahead to do exercises like the Dead Bug.\nImportance of Core Strengthening\nPerforming exercises like the Dead Bug exercise will strengthen your core muscles, specifically your abdominals and obliques. It's important to focus on building strength in your midsection because it's at the center of every activity you perform. Whether your core is transporting kinetic energy from one end of your body to the other, or simply stabilizing your frame, it's always the hub. Strengthening your core makes everyday actions you never even think of possible and will support your back, an essential detail for those who suffer from lower-back pain.\nBe the Bug\nThe Dead Bug exercise is performed lying on the floor on your back. Your knees should be bent with your feet flat on the floor and your arms at your sides. Contract your abdominal muscles and keep them taut as you raise your arms up toward the ceiling and lift your feet from the floor, keeping your knees bent. Hold the position for one count then bring your left arm down to the floor over your head. Keep your right arm extended upward, while at the same time you straighten and lower your right leg to the floor. Hold for one count then bring your left arm and right leg back to their previous positions and perform the exercise on the right arm and left leg. Although you'll keep your abs tight, remember to breathe throughout the exercise. Repeat the movements to execute 10 to 12 repetitions on each side.\nWhen you're first starting out with the Dead Bug exercise, you can do a variation that doesn't require you to keep one arm extended upward at all times. The beginner's variation involves the same opposite arm and leg movements, but has you bend your arms at the elbows and rest your fists on the floor just above your head. Instead of extending your arm over your head when you move the opposite leg, you'll bring it up so you're holding your fist above your face, arm still bent.\nAdditional Recommended Exercises\nDr. Harry Herkowitz wrote the book on the lumbar spine. He noted that when treating athletes for lower-back injuries often he could get them back up and running (or jumping or whatever it is they needed to do) without having to put them in a brace if they participated in a lumbar stabilization program. His program was made up of eight types of exercise, starting with the Dead Bug exercise right in the number one spot. The other seven exercises included in the program to strengthen the core and support the lower back are partial situps, bridging, prone, quadrupedal, wall slides, ball exercises and aerobics.\n- Comstock/Comstock/Getty Images\n- How to Stretch the Trapezius Dynamically\n- Does Ballet Strengthen Your Whole Body?\n- What Exercises Put Stress on Your Lower Back?\n- Advanced Abdominal Exercises\n- How to Strengthen the Legs & Ankles for Ballet\n- Strengthening Exercises for Strained Thigh Muscles\n- Pilates Moves for Obliques\n- Exercises That Do Not Stress the Core"", 'Lower Back Pain: How Exercise Helps\nYou may feel like resting, but moving is good for your back. Exercises for lower back pain can strengthen back, stomach, and leg muscles. They help support your spine, relieving back pain. Always ask your doctor before doing any exercise for back pain. Depending on the cause and intensity of your pain, some exercises may not be recommended and can be harmful.\nAvoid: Toe Touches\nExercise is good for low back pain — but not all exercises are beneficial. Any mild discomfort felt at the start of these exercises should disappear as muscles become stronger. But if pain is more than mild and lasts more than 15 minutes during exercise, patients should stop exercising and contact a doctor. Some exercises may aggravate pain. Standing toe touches, for example, put greater stress on the disks and ligaments in your spine. They can also overstretch lower back muscles and hamstrings.\nTry: Partial Crunches\nSome exercises can aggravate back pain and should be avoided when you have acute low back pain. Partial crunches can help strengthen your back and stomach muscles. Lie with knees bent and feet flat on the floor. Cross arms over your chest or put hands behind your neck. Tighten stomach muscles and raise your shoulders off the floor. Breathe out as you raise your shoulders. Don’t lead with your elbows or use arms to pull your neck off the floor. Hold for a second, then slowly lower back down. Repeat 8 to 12 times. Proper form prevents excessive stress on your low back. Your feet, tailbone, and lower back should remain in contact with the mat at all times.\nAlthough you might think sit-ups can strengthen your core or abdominal muscles, most people tend to use muscles in the hips when doing sit-ups. Sit-ups may also put a lot of pressure on the discs in your spine.\nTry: Hamstring Stretches\nLie on your back and bend one knee. Loop a towel under the ball of your foot. Straighten your knee and slowly pull back on the towel. You should feel a gentle stretch down the back of your leg. Hold for at least 15 to 30 seconds. Do 2 to 4 times for each leg.\nAvoid: Leg Lifts\nLeg lifts are sometimes suggested as an exercise to “strengthen your core” or abdominal muscles. Exercising to restore strength to your lower back can be very helpful in relieving pain yet lifting both legs together while lying on your back can make back pain worse. Instead, try lying on your back with one leg straight and the other leg bent at the knee. Slowly lift the straight leg up about 6 inches and hold briefly. Lower leg slowly. Repeat 10 times, then switch legs.\nTry: Wall Sits\nStand 10 to 12 inches from the wall, then lean back until your back is flat against the wall. Slowly slide down until your knees are slightly bent, pressing your lower back into the wall. Hold for a count of 10, then carefully slide back up the wall. Repeat 8 to 12 times.\nTry: Press-up Back Extensions\nLie on your stomach with your hands under your shoulders. Push with your hands so your shoulders begin to lift off the floor. If it’s comfortable for you, put your elbows on the floor directly under your shoulders and hold this position for several seconds.\nTry: Bird Dog\nStart on your hands and knees, and tighten your stomach muscles. Lift and extend one leg behind you. Keep hips level. Hold for 5 seconds, and then switch to the other leg. Repeat 8 to 12 times for each leg, and try to lengthen the time you hold each lift. Try lifting and extending your opposite arm for each repetition. This exercise is a great way to learn how to stabilize the low back during movement of the arms and legs. While doing this exercise don’t let the lower back muscles sag. Only raise the limbs to heights where the low back position can be maintained.\nTry: Knee to Chest\nLie on your back with knees bent and feet flat on the floor. Bring one knee to your chest, keeping the other foot flat on the floor. Keep your lower back pressed to the floor, and hold for 15 to 30 seconds. Then lower your knee and repeat with the other leg. Do this 2 to 4 times for each leg.\nTry: Pelvic Tilts\nLie on your back with knees bent, feet flat on floor. Tighten your stomach by pulling in and imagining your belly button moving toward your spine. You’ll feel your back pressing into the floor, and your hips and pelvis rocking back. Hold for 10 seconds while breathing in and out smoothly. Repeat 8 to 12 times.\nLie on your back with knees bent and just your heels on the floor. Push your heels into the floor, squeeze your buttocks, and lift your hips off the floor until shoulders, hips, and knees are in a straight line. Hold about 6 seconds, and then slowly lower hips to the floor and rest for 10 seconds. Repeat 8 to 12 times. Avoid arching your lower back as your hips move upward. Avoid overarching by tightening your abdominal muscles prior and throughout the lift.\nLifting Weights May Help\nDone properly, lifting weights doesn’t usually hurt your back. In fact, it may help relieve chronic back pain. But when you have acute (sudden) back pain, putting extra stress on back muscles and ligaments could raise risk of further injury. Ask your doctor whether you should lift weights, and which exercises to avoid.\nTry: Aerobic Exercise\nAerobic exercise strengthens your lungs, heart, and blood vessels and can help you lose weight. Walking, swimming, and biking may all help reduce back pain. Start with short sessions and build up over time. If your back is hurting, try swimming, where the water supports your body. Avoid any strokes that twist your body.\nTry: Some Pilates Moves\nPilates combines stretching, strengthening, and core abdominal exercises. Under the instruction of an experienced teacher, it may help some people with back pain. Be sure to tell your teacher about your back pain, because you may need to skip some moves.\nMake Appointment or Enquiry:\nCall: (65) 64712744 / Whatsapp or SMS: (65) 92357641 / Email: firstname.lastname@example.org – 24 Hours Hotline']"	['<urn:uuid:f9fa6d8e-f68b-4782-baa1-d0ac5efc0a88>', '<urn:uuid:d7c6af85-7654-47e2-af63-11693e039c75>']	factoid	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-12T19:58:41.790432	12	74	1633
83	How does fair treatment by police benefit law enforcement?	When people view the police as legitimate due to fair treatment, they are more willing to defer to police authority and less likely to resist or defy police. Studies show that complaints against police decrease, and people become more willing to comply with the law and cooperate with police efforts to stop crimes and identify criminals. Fair treatment encourages community members to participate in maintaining order through activities like attending community meetings or joining neighborhood watch programs.	['by Tom Tyler\nRecent events in New York City make it clear that there is widespread and continuing anger over the street stop policies of the NYPD. This ongoing discontent reflects a broader paradox in American policing: the police have become more effective in reducing the rate of violent crime to historically low levels but their success has not led to higher levels of public trust in the police. There are lessons from this police experience not only for police commanders but also for judges, court administrators, and others working in the criminal justice system.\nBased upon my own research and that of other social scientists we know why this performance without legitimacy paradox is occurring. Public anger continues because the police have not addressed what actually matters to the public. My research shows that police legitimacy is based upon how fairly the public thinks the police exercise their authority. Until the police change their policies and practices to address public concerns over procedural justice, controversies over police practices such as racial profiling, police street stops and the surveillance of Muslim Americans will not end.\nMany cities, including New York City are experiencing dramatically lower levels of violent crime. Despite these gains public trust and confidence in the police is not increasing, nor is the large racial gap in trust and confidence between White and minority Americans closing. Better performance has not lead to greater public legitimacy. Why? Police leaders are failing to take account of public concerns. Evaluations of policies and practices by police leaders ask if they are effective in preventing crimes. The question of whether these policies and practices are viewed as legitimate by people in the community is not addressed. My research findings however tell us that effectiveness is not the key factor that the public considers when reacting to police policies and practices.\nWhat does the public care about? Research shows that the key issue to members of the public is their evaluation of the fairness of the way the police exercise their authority: i.e. to issues of procedural justice. More than anything else people are concerned about whether they feel that the police officers to whom they give the authority to maintain order in their communities act using fair procedures.\nWhat does the public mean by fair procedures? They mean first that when creating policies the police work with the community to identify problems and the strategies that should be used to address them. When dealing with particular citizens they allow those people to tell their side of the story, to explain their situation, before making decisions. When implementing the law the police explain their policies and how they are being applied in particular cases in ways that the public can see are neutral and unbiased.\nThe public also means that the police should treat people fairly. Fair treatment is respectful and courteous. It acknowledges people’s rights not to be demeaned, ridiculed or insulted by the police. And, the police are trustworthy. They act with integrity, accounting for their actions in ways that show good faith responsiveness to people and their problems.\nWhy should the police care about this public perspective on policing? Insensitivity to public concerns has led police departments like the NYPD to turn victory into defeat. Instead of being congratulated for lowering the rate of violence in New York City, or reducing the rate of unlawful shootings, the police are reviled by an angry population for mistreating people in the community.\nAnd the police lose the benefits of public cooperation. Studies show that when people view the police as more legitimate they are more willing to defer to police authority; less likely to resist and defy the police; and that complaints against the police go down. Legitimacy further encourages willing compliance with the law and cooperation with police efforts in their efforts to stop crimes and identify criminals. When fairly treated people are more willing to work with the police in efforts that join the police and the people in the community in efforts to maintain order by attending community meetings or joining a neighborhood watch.\nIf officers dealt with people seeking to communicate respect and deliver fairness they would be working not only to prevent crime but to build public support for the police. And, more broadly legal authorities need to recognize the value of considering their policies and practices from the perspective of public concerns. That perspective emphasizes that people are “seekers of justice” and evaluate their experiences with the police and courts by evaluating how fairly they experience the actions of the authorities they deal with.\nTom Tyler, Professor of Law and Psychology, Yale University']	['<urn:uuid:a0f0f0b6-3114-4a02-96ed-0604bc3d5c4e>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	9	77	777
84	What makes Aqaba both accessible and environmentally vulnerable?	Aqaba is highly accessible with its international airport just 20 minutes from the town center and borders with Israel, Egypt, and Saudi Arabia within 30-minute drives, plus regular flights from Amman taking 45 minutes. However, this accessibility and development comes with environmental challenges, as coral reefs in the region face threats from climate change and local factors like nutrient level changes from farm fertilizer run-off. These environmental stressors can trigger coral bleaching events, though some corals show resilience through colorful bleaching as a survival mechanism.	['From as far back as five and half thousand years ago Aqaba has played an important role in the economy of the region. It was a prime junction for land and sea routes from Asia, Africa and Europe, a role it still plays today as a port. Because of this vital function, there are many historic sites to be explored within the area, including what is believed to be the oldest purpose-built church in the world and a 13th century fort. Some stories in the famous Arabian Nights Tales also refer to adventures Sinbad had that started by leaving from the port city of Ayla (former name of Aqaba). TE Lawrence (also known as Laurence of Arabia) also had his fair share of exploits here when he fought the Battle of Aqaba (1917). However these sights and the Archeological Museum are not the only sights of interest…\nWith its wealth of other attractions it is surprising that Jordan’s splendid Red Sea Resort is often overlooked by modern-day visitors. The stunning newly built white Mosque sits on the waterfront and is open to visitors (make sure you have long sleeves and trousers/skirt on). There are markets where you can mingle with the locals buying vegetables, fresh fish and souvenirs (see below). The local restaurants serve traditional favourites like mansaf, falaful and knafeh as well as tasty fish straight from the sea. During the summer there are regular live traditional music concerts on the seafront and Aqaba cohosts the Distant Heat Dance Festival with Wadi Rum. There are bars to quench your thirst after a hard days diving.\nThere are many other watersports available here such as waterskiing, kite surfing and windsurfing. Or if you wish to relax when you are not diving the Marine Park has long public beaches for you to enjoy the sun on.\nAqaba International Airport is situated just a 20-minute drive from the town center and services regular flights from Amman as well as from several European cities. From the town center, the borders of Israel, Egypt’s Sinai and Saudi Arabia are no more than a 30-minute drive.\nOne of Jordan’s main priorities is to ensure the local people benefit from the country’s burgeoning tourist industry. With this in mind, they are encouraged to produce ecologically-friendly traditional items that are attractive to visitors.\nAs with all major tourist sites within Jordan, Aqaba not only offers a great selection of hand-crafted souvenirs, such as the traditional Bedouin jewelry, sand bottles, etc., but also excellent modern and traditional jewelry in gold and silver, at exceptionally good prices. The Queen Noor Hussein Foundation, which supports local craftspeople, supplies several outlets in Aqaba with a stunning selection of handmade clothing, carpets, cushion covers, wall-hangings, pottery and glassware.\nAqaba also has many modern boutiques where you can find the very latest in imported jewelry, watches, clothing, accessories and leather goods. Stroll through Aqaba’s largest retail and entertainment complex, the Aqaba Gateway, or take advantage of Aqaba’s Free Zone and shop in style without having to pay any duties on the goods you purchase from certain shops.\nHow to Get Here\nRoyal Wings and Jordan Aviation both offer regular flights from Amman to Aqaba; flying time is about 45 minutes.\nSeveral companies offer charter bus tours and regular tours between Amman & Aqaba, including JETT: tel: +962 (06) 5664141 and Alpha Daily Tours: tel: +962 (06) 585 0430.\nBy car or taxi:\nAqaba via the Dead Sea road is about 3 hours drive from Amman, along the Desert Highway 4 hours and the scenic Kings Highway 5 hours.', 'Cecilia D’Angelo, a molecular coral biology lecturer at the University of Southampton. Jörg Wiedenmann, Elena Bollati & Cecilia D’Angelo/University of Southampton, Palawan colourful bleaching image by Ryan Goehrung/University of Washington\nBleaching events used to be few and far between, but they now occur nearly every year. After the coral is exposed, it often breaks down and dies, altering the ecosystem for the diverse array of life that relies on it. Corals stand little chance of bouncing back from these events — but a new study suggests they have an unusual survival method: taking on a vibrant neon color.\nWhen bleaching events occur, extended heat spikes cause corals to turn a ghostly white, often leading to their death. Even slight increases in annual ocean temperatures can wreak havoc on this relationship, expelling the algae from the coral’s tissue and exposing its white skeleton. These corals can still undergo some of their normal functions for a short period of time as they hope their algae come back — whereas drastic changes in ocean temperature almost always lead to coral death. Reports of colorful bleaching during the most recent mass bleaching event in the Great Barrier Reef in March and April gave scientists hope that patches of the system have a chance to recover. “Bleaching is not always a death sentence for corals, the coral animal can still be alive,” said Dr. They found that colorful bleaching events occur when corals produce “what is effectively a sunscreen layer” on their surface to protect against harmful rays and create a glowing display that researchers believe encourages algae to return. The Ocean Agency describes the process as a “chilling, beautiful and heartbreaking” final cry for help as the coral attempts to grab the algae’s attention. “Our research shows colorful bleaching involves a self-regulating mechanism, a so-called optical feedback loop, which involves both partners of the symbiosis,” lead researcher Professor Jörg Wiedenmann of the University of Southampton said in a press release. Climate Change\nDying coral reefs turn vibrant neon in apparent survival effort\nScientists say climate change is turning coastal Antarctica green\nStudy: Climate change makes a Dust Bowl heat wave more likely\nAir pollution is already spiking in China with virus lockdown lifted\nIndia’s carbon emissions fall for the first time in four decades\nMore in Climate Change\nResearchers at the University of Southampton’s Coral Reef Laboratory studied 15 colorful bleaching events worldwide between 2010 and 2019 — including one in the Great Barrier Reef, the world’s largest coral reef system — and recreated those ocean temperatures in a lab. But “colorful bleaching” has the opposite effect: the dying corals gain more pigment, and glow in shades of bright pink, purple and orange. Scientists first spotted the mysterious neon coral a decade ago, but they had been unable to figure out why it occurred. As the recovering algal population starts taking up the light for their photosynthesis again, the light levels inside the coral will drop and the coral cells will lower the production of the colorful pigments to their normal level.”\nColorful bleaching Acropora corals in the Phillipines. Dying coral reefs turn vibrant neon colors in apparent last-ditch effort to survive\nBy Sophie Lewis\nMay 22, 2020 / 4:48 PM\n/ CBS News\nScientists use mini-satellites to save coral reefs\nFor years, coral reefs around the world have been devastated by mass bleaching events as the oceans continue to warm due to climate change. “If the stress event is mild enough, corals can re-establish the symbiosis with their algal partner.”\nThe internal changes that allow colorful bleaching to occur. In 2017 alone, nearly half the corals on the Great Barrier Reef died — and experts say we are running out of time to save them. “Unfortunately, recent episodes of global bleaching caused by unusually warm water have resulted in high coral mortality, leaving the world’s coral reefs struggling for survival,” D’Angelo said. Scientists emphasized that while colorful bleaching is a good sign, only a significant reduction of greenhouse gases globally — in addition to improvement in local water quality — can save coral reefs beyond this century.\n“Now that we know that nutrient levels can affect colorful bleaching too, we can more easily pinpoint cases where heat stress might have been aggravated by poor water quality,” researchers said. Ryan Goehrung, University of Washington\nCoral reefs support more species per unit area than any other marine environment, including about 4,000 species of fish, 800 species of hard corals and potentially millions of other undiscovered species, according to the National Oceanic and Atmospheric Administration. Colorful bleaching Acropora corals in New Caledonia.\nRichard Vever/The Ocean Agency/XL Catlin\nCoral animals symbiotically coexist with tiny algae, providing them with shelter, nutrients and carbon dioxide in exchange for their photosynthetic powers. Together, these actions can secure a future for coral reefs.”\nFirst published on May 22, 2020 / 4:48 PM “The resulting sunscreen layer will subsequently promote the return of the symbionts. This study, published Thursday in the journal Current Biology, suggests the corals change color as a last-ditch effort to survive. “This can be managed locally, whereas the ocean heat waves caused by climate change will need global leadership. Disruptions to coral reefs have far-reaching implications for ocean ecosystems.\nIt’s not just warming oceans that cause colorful bleaching. Researchers say changes in nutrient levels within coral reefs due to fertilizer run-off from farms also lead to bleaching events — a problem that can be fixed at the local level. Experts believe only coral that has faced mild or brief disturbances, rather than extreme mass bleaching events, can attempt to save itself using this process.']	['<urn:uuid:e732ff28-c5d5-42a5-9902-016dcf9349dd>', '<urn:uuid:369f1095-9589-4d83-9007-35418b336092>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T19:58:41.790432	8	85	1531
85	montreal musician jehin prume wife rosita del vecchio career performances death impact	Rosita Del Vecchio, who married Jehin-Prume in 1866 at age 19, was a talented mezzo-soprano who performed extensively with her husband from 1868. She received training from her husband and in Brussels, and together they enthralled audiences throughout Quebec and beyond. She was also acclaimed as an actress, performing in plays by Fréchette. Tragically, she died of pneumonia on February 11, 1881, and her funeral was described as 'the most important ever given to a woman in Canada,' drawing a crowd of 10,000 people. Her death had a profound impact on Jehin-Prume, who lost musical ambition and inspiration afterward.	['JEHIN-PRUME, FRANTZ (baptized François-Henri), musician and music teacher; b. 18 April 1839 in Spa, Belgium, son of Jules-Antoine Jehin, painter, and Marie-Joséphine-Pétronille Prume; d. 29 May 1899 in Montreal.\nFrançois-Henri Jehin’s paternal grandfather was organist at Spa, and his maternal grandfather filled the same function at Stavelot. After being taught music by his father at age three and the violin by a local musician from age four, young Frantz, as he was already known, gave his first violin concert at six. An uncle and distinguished violinist, composer, and music professor, François-Hubert Prume, then accepted him at the Conservatoire Royal in Liège, where he obtained high marks at the age of nine. His uncle died soon after, and Frantz added Prume to his surname. He continued his studies at the Conservatoire Royal in Brussels, mainly under Hubert Léonard, winning several prizes. The composers Gasparo Luigi Pacifico Spontini and Giacomo Meyerbeer, who both holidayed at Spa, are said to have encouraged the child prodigy. Later he may have had guidance from Henri Vieuxtemps and Henryk Wieniawski.\nAfter playing in Spa, Liège, and Brussels, Jehin-Prume began his first recital tour in late 1855. From then until 1863 he toured widely through central, northern, and western Europe, establishing an enviable reputation for sensitivity and technical brilliance. By 1863 he had been named violinist to the Grand Duchess Catherine of Russia and to Leopold I, King of the Belgians. In late 1864 he toured in Mexico on an invitation from Emperor Maximilian, son-in-law of Leopold, and then left for Cuba (and possibly Brazil) before arriving in New York in May 1865.\nInvited that month to vacation in Lower Canada by Jules Hone*, a former fellow student then residing in Montreal, Jehin-Prume soon appeared in concerts and recitals in that city. Blessed with “an appearance captivating in the extreme and preceded by the lustre of an established reputation,” wrote Louis-Honoré Fréchette*, Jehin-Prume took Montreal by storm. In June and July he went on a tour of Quebec, Ottawa, Kingston, Toronto, Hamilton, London, and Detroit, and then, when it proved to be highly successful, on another that included Quebec, Rimouski, Halifax, and Saint John, N.B. At Quebec in September his host, Joseph-Edouard Cauchon*, induced the legislature to suspend its work and hear a concert by Jehin-Prume in the Legislative Assembly itself. In the winter of 1865–66 he gave concerts in the eastern United States, appearing in December with the New York Philharmonic Orchestra.\nJehin-Prume returned to Montreal in March 1866, and on 10 July he contracted, according to Fréchette, “a brilliant marriage with one of the most sought-after stars of Montreal society,” 19-year-old Rosita (Rosa) Del Vecchio; the huge church of Notre-Dame was nearly filled for the event. Immediately after the marriage Jehin-Prume began a long American tour which included a performance before President Andrew Jackson in the White House in January 1867. Rosita, described by a contemporary as “half French and half Italian,” had displayed exceptional talent in music from childhood, and after further training by her husband and a teacher in Brussels she began performing in public as a mezzo-soprano with Jehin-Prume from November 1868. After giving birth to their only child, Jules, in 1870, she continued her studies in Europe. Although Montreal became the focal point for Jehin-Prume’s activities, he spent much time touring eastern North America, Cuba, and Europe, often with Rosita. In the province of Quebec he and his wife enthralled audiences not only in Montreal and Quebec but also in such places as Joliette, Trois-Rivières, Saint-Hyacinthe, Sherbrooke, Lachute, and Aylmer. His programs, like those of his contemporaries in general, were a mixture of virtuoso show-pieces and arrangements of familiar tunes from opera or more serious pieces by Bach, Beethoven, and Mendelssohn. He also offered many compositions of his own. From January to March 1871, in order to raise the level of music appreciation in Montreal, he organized six classical chamber concerts in which he played concertos, led a large chamber ensemble, and played in a string quartet. Fréchette describes a concert by Jehin-Prume at Quebec during this period, when he was at the height of his powers. “His violin by turns . . . sighed, laughed, sang, cried, [and] screamed deliriously amidst sobs of distraction. The audience [was] subjugated, stirred to the marrow by this overflowing lyricism formed of a tenderness and an ardour transporting to ecstasy, moving to tears.” The writer Arthur Buies* asserted that Jehin-Prume had come to a land “in which there were pedestals everywhere, but he alone is worthy to mount them.”\nJehin-Prume’s ties to Canada were strengthened by his friendship with pianist Calixa Lavallée, especially after Lavallée’s return in 1875 from studies in Paris. The two men appeared together, with Rosita, in numerous concerts, and for a while all three shared a teaching studio. The musician and critic Guillaume Couture* proclaimed the dawning of “a new era in music.” In 1877 Jehin-Prume and Lavallée co-directed an immensely successful performance of Jeanne d’Arc, by Jules Barbier with music by Charles Gounod, in which Rosita played the leading role. From 1877 Jehin-Prume occasionally was concert-master in the orchestra of Couture’s Montreal Philharmonic Society. In 1877–78 he served as president of the Académie de Musique de Québec.\nRosita, who, as a singer, was nearly as acclaimed as her husband, won strong reviews as an actress in two plays by Fréchette, Papineau and Le retour de l’exilé, produced in Montreal in June 1880. On 11 Feb. 1881, however, she died of pneumonia. Her death inspired a poem by Fréchette, “Laissez-moi dormir,” part of which was set to music by Jehin-Prume (Op. 40). Le National (Montreal) noted in its obituary that her funeral “was the most important ever given to a woman in Canada” and that the police had to be called in to disperse a crowd of 10,000.\nWith Rosita, Couture’s “new era” died prematurely. Lavallée went into permanent exile; Jehin-Prume carried on, but, according to Fréchette, who had become an intimate friend, he was a broken man. He lost musical ambition and inspiration. On 24 March 1882, then 42, he married in Montreal Hortense Leduc, a teen-aged performing singer and stepdaughter of the violinist Oscar Martel*. However, after indulging in a number of affairs, Hortense left him for a businessman. Having toured from New Brunswick to Manitoba in 1881, Jehin-Prume performed in Europe from 1882 to 1885. He subsequently settled permanently in Montreal, continuing to play in public on occasion. As he had done previously, he taught, composed, and stimulated artistic activity in the city. He had already trained outstanding pupils in François Boucher and Alfred De Sève, and now Emile Taranto and Béatrice La Palme* benefited from his teaching. He wrote nearly one-third of his works during this period. In addition, in 1891 or 1892 he founded a chamber ensemble, the Association Artistique de Montréal. This group gave 31 concerts until May 1896 when Jehin-Prume, after having suffered severe illness, offered his final public performance; with the close of his career, the association ended.\nJehin-Prume lived out his last years quietly in the comfortable residence of his brother Érasme, himself a violinist. Mired in melancholy according to Fréchette, Jehin-Prume nevertheless fought death to the end, which came on 29 May 1899. By then he was relatively forgotten, but, wrote Fréchette, news of his funeral brought out “an entire population . . . to take a last look and to wave a final goodbye to him whom they had so often applauded.”\nFrantz Jehin-Prume was the first musician of international fame to settle in Canada and, according to Henri Têtu*, he was the best liked; in fact, Têtu added, “he became almost Canadian.” His contribution to the elevation of musical life and artistic appreciation in general in the young country is immeasurable. Not only musicians, such as Lavallée, Couture, Romain-Octave and Frédéric Pelletier, Arthur Letondal, and Jean-Baptiste Dubois benefited from his presence, but artists in other fields, particularly poets, were inspired by his playing. No doubt Fréchette spoke for them too when he declared himself a brother of Jehin-Prume – “brother through art, brother in the soul.”\nFrantz Jehin-Prume is the author of at least 88 compositions, numbered in [Jules] Jehin-Prume, Une vie d’artiste, preface de L.-H. Fréchette (Montréal, ); they include two violin concertos and an oratorio. A few of his works have been published. There are portraits of him in Une vie d’artiste, in the Encyclopedia of music in Canada (Kallmann et al.), and in Le Passe-Temps (Montréal), 10 juin 1899.\nArch. de l’État (Liège, Belgique), Spa, reg. des baptêmes, manages et sépultures, 19 avril 1839. National Library of Canada, Music Division (Ottawa), mss coll., 1974–18, 1985–8. Dwight’s Journal of Music (Boston), 23 Dec. 1865. Gazette (Montreal), 1 March 1871, 12 Feb. 1881. Globe, 15 July 1865. La Minerve, 15 févr. 1871, 22 mai 1877. Le Monde illustré, 9 mars 1901. Montreal Daily Star, 29 May 1899. Montreal Herald, 30 March 1871. L’Opinion publique, 18 nov. 1875. La Presse, 29 mai 1899. Alberto Bachmann, An encyclopedia of the violin, intro. Eugène Ysaye, trans. F. H. Martens, ed. A. E. Weir (New York, 1925; repr. 1966). Catalogue of Canadian composers, ed. Helmut Kallmann (2nd ed., Toronto, 1952; repr. St Clair Shores, Mich., 1972). Dictionnaire biographique des musiciens canadiens (2e éd., Lachine, Qué., 1935). The musical red book of Montreal, ed. B. K. Sandwell (Montreal, 1907). The new Grove dictionary of music and musicians, ed. Stanley Sadie (6th ed., 20v., London, 1980), 9. Riemann Musik Lexikon, ed. Wilibald Gurlitt (12th ed., 3v., Mainz, Federal Republic of Germany, 1959), 1. Helmut Kallmann, A history of music in Canada, 1534–1914 (Toronto and London, 1960). [J.]-E. Lapierre, Calixa Lavallée, musicien national du Canada (3e éd., Montréal, 1966). E. S. J. van der Straeten, The history of the violin (London, 1933). “M. F. Jehin Prume,” Le Canada musical (Montréal), 2 (1875–76): 77; 3 (1876–77): 44–45. Frédéric Pelletier, “Les musiciens du passé: Frantz Jehin-Prume,” Entre-nous (Montréal), 1 (avril 1930): . “Premier concert Prume-Lavallée à Montréal,” Le Canada musical, 2: 134–35. “Premier concert Prume et Lavallée à Québec,” Le Canada musical, 3: 26. J.-B. Rongé, “Correspondance, Spa, le 16 septembre 1873,” Rev. et gazette musicale de Paris, 40 (1873): 301. D.-H. Sénécal, “Quelques mots sur l’album de F. Jehin-Prume, violoniste de sa majesté le roi des Belges,” and “M. F. Jehin-Prume,” Rev. canadienne, 2 (1865): 547–54 and 616–22. Henri Têtu, “Impressions musicales,” L’Action sociale (Québec), 22, 29 mars, 3 avril, 25 mai 1915. Léon Trépanier, “Oscar Martel, violoniste et professeur (1848–1924),” Qui? (Montréal), 4 (1952–53): 47–56.']	['<urn:uuid:6ceadca0-723b-4e2e-90a5-56e2514ae069>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T19:58:41.790432	12	99	1751
86	What are the power specifications for large marine converters?	The bidirectional frequency converter is produced with power range of 1 to 5 MW. For a 2.4 MW model, it has a rated output current of 2010A, operates with input power of 3x690V at 50Hz, and provides output voltage of 0-690V with frequency range of 0-60Hz. The converter offers overload capacity of 120% for 60 seconds and 150% for 3 seconds after 10 minutes. It achieves a power factor (cos φ) of 0.95 at rated speed. Through simultaneous operation of two converters, maximum power of 5 MW can be achieved.	"['Bidirectional frequency converter ""PAPIR""\nBidirectional frequency converter, PAPIR series is produced with power range 1 to 5 MW and widely used in power systems to control vessel’s propulsion system - propulsion/steering unit and thruster. PAPIR ensures smooth start and stop of electric motor, controls its temperature, power and speed, as well as protects the electric drive.\nThe unit functions as a voltage converter with pulse width modulation and active rectifier based on high-power IGBT transistors. Power converter assemblies are used as transistors due to high resistance to harsh environmental conditions and temperature cycling.\nPAPIR microcontrollers are based on up-to-date processor with 180 mln flops performance. High quality control system maintains accuracy of output frequency at 0.1%. The converter supports vector (both with and w/o encoder) and scalar (w/o encoder) types of electric motor control.\nFrequency converter uses RS-422/485 interface to operate together with control systems of higher level. Additional devices such as temperature sensors, encoders, tachometers, etc. can be connected. The converter’s software conducts in-depth diagnostics of its condition; non-volatile error and alarm history ensures operation monitoring.\nThe control panel has 8’’ touch screen with user-friendly interface, as well as duplicate control buttons, rpm and motor load indicators, sound and light signaling units and emergency stop button.\nThe main screen displays key information: power contactor position, cooling system status, rpm and motor load indicators and warning / alarm notifications. All information is displayed as a simple mimic diagram. Additional screens allows for carrying out in-depth diagnostics of the electric drive.\n""Russian Electrotechnical Company Limited"" manufactures frequency converter, PAPIR series according to Technical specifications and type approval certificate issued by Russian Maritime Register of Shipping. Up-to-date production facilities with several parallel assembly lines and trained specialists enable to produce reliable equipment within the shortest time.\nThe unit’s modular design allows for easier transportation and installation on the vessel. The enclosure has a heightened rigidity and mechanical reliability. Simultaneous operation of two converters ensures maximum power of 5 MW.\nMax. weight of converter with power 2.4 MW is 2,800 kg under dimensions 2400х2000х800 (WхHхD). The converter’s power-to-weight ratio is more than 800 W / kg and 625 W / dm3. IP rating – IP44.\nThe converter consists of two sections:\n- power input and control section ensures power input, switching and distribution, control and protection of the internal devices, control from the local panel, interfacing with external ship systems and units. The section contains voltage control relay, automatic circuit breaker, input filters, voltage sensors, controllers and other equipment.\n- conversion section ensures voltage level and frequency conversion, containing controlled rectifier, DC link with water-cooled brake resistor and voltage inverter. The converter is produced in two designs with mirror arrangement of sections for convenient and optimal installation on the vessel.\n- Small dimensions at high power: power-to-weight ratio > 800 W/kg and 625 W/ dm3\n- Converter may be used in complex power distribution systems due to bidirectional inverter\n- Controlled rectifier provides for high quality of harmonic current; no need in input transformer\n- Easy transportation and installation due to modular design\n- Intuitive user-friendly interface with wide functionality\n- Integrated brake resistor (at option)\n- Seawater/fresh water circulation system (at option)\n|Rated output power, MW||1,2||2,4|\n|Max. power consumption (including overload), MW||1,855||3,711|\n|Input power||3х690V, 50Hz|\n|Output voltage, V||0…690|\n|Rated output current, A||1005||2010|\n|Overload capacity, %||120% - 60s., 150% - 3s. (after 10 min.)|\n|Output frequency, Hz||0…60|\n|Power factor cos φ (rated speed)||0,95|\n|Generated thermal power, kW||42||78|\n|Nominal flow rate of cooling fluid, l/min||40||72|\nClosed loop of fresh water circulation is used to cool frequency converter and ensure its highly reliable operation and long life. Water circulation system on seawater – fresh water is at option.\n- Heat transfer using brazed plate heat exchanger, water-water type\n- Flow and fluid temperature sensors\n- Remote control (from the converter or external system) and duplicate controls on the front panel\n- Redundant pump of closed cooling loop\n- Convenient expansion tank to replace cooling fluid in closed loop\nWater circulation system is manufactured as a standalone section; it may be installed immediately adjacent to the converter (from water inlet flanges side) or separately.']"	['<urn:uuid:4ea9d2e5-04a9-42b7-a69b-e4cfbb3263ef>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	9	90	688
87	As a seismologist studying historical mega-earthquakes, I'm curious about how the 2011 Tohoku earthquake compares to other major seismic events in Japan's history and what implications it has for other regions. Can you provide details about its magnitude, impacts, and regional significance?	The 2011 Tohoku earthquake was magnitude 9.0, making it the largest earthquake to strike Japan since seismic recording began 140 years ago. It generated tsunami waves reaching over 30 meters in height and traveled up to 10 km inland along 600 km of coastline. The disaster resulted in 15,894 deaths, 6,152 injuries, and 2,562 missing persons, with economic losses between $14.5-34.6 billion. This event had historical parallels to the 869 Sanriku earthquake in the same region, which also generated devastating tsunamis. The Tohoku earthquake's occurrence has implications for other regions, particularly the U.S. West Coast, which has two subduction zones capable of magnitude 9 earthquakes - Alaska (last ruptured 1964) and the Pacific Northwest/Cascadia (last ruptured 1700).	['Meeting notes are available online.\nFort Mason Center, Building C, Rm 210\n2011 M 9.0 Tohoku, Japan Earthquake\nThe powerful magnitude 9.0 Tohuku earthquake is the largest to have struck Japan since seismic recording began 140 years ago and one of the largest earthquakes ever recorded worldwide. The U.S. west coast has two subduction zones that are capable of magnitude 9 earthquakes offshore. Alaska, which last ruptured in 1964, and the Pacific Northwest (known as Cascadia) last ruptured in 1700. What are the implications for the state of California and the San Francisco Bay Area?\n- Opening Remarks\nDr. David P. Schwartz, USGS & Bay Area Earthquake Alliance Co-Chair\n- What light does the Tohoku super-quake shed on California seismic hazards?\nRoss Stein, USGS Geophysicist\n- A Summary of San Francisco’s Community Action Plan for Seismic Safety (CAPSS)\nJohn Paxton, Principal, Real Estate Advisory Services\n- Planning for Post-Earthquake Recovery: San Francisco’s Resilient City Efforts\nLaurie Johnson, Principal, Laurie Johnson Consulting\nRoss Stein is a geophysicist with the U.S. Geological Survey in Menlo Park, California. He received an Sc.B. Magna cum Laude and with Honors from Brown University in 1975, a Ph.D. from Stanford University in 1980, and was Observatory Post-Doctoral Fellow at Columbia University during 1981. Dr. Stein is a Fellow of the American Geophysical Union (AGU) and the Geological Society of America, was Editor of the Journal of Geophysical Research during 1986-1989, and chaired AGU’s Board of Journal Editors of the in 2004-2006. He was a visiting professor at Institut de Physique du Globe (Paris and Strasbourg) and Ecole Normale Supérieure in 1989, 1993, 1999, and 2008. Stein co-founded and chairs the Scientific Board of the Global Earthquake Model (the GEM Foundation), a public-private partnership building a worldwide seismic risk model in 5 years.\nDuring 1993-2003, the Science Citation Index reported that Stein was the second most cited author in earthquake science. Dr. Stein received the Eugene M. Shoemaker Distinguished Achievement Award of the USGS in 2000, the Excellence in Outreach Award of the Southern California Earthquake Center in 1999, and the Outstanding Contributions and Cooperation in Geoscience Award from NOAA in 1991. He presented the Francis Birch Lecture of the AGU in 1996, the Frontiers of Geophysics Lecture of the AGU in 2001, C. Thomas Crough Memorial Lecture of Purdue University, Andrew C. Lawson Lecture of U.C. Berkeley, and the Condon Public Lecture of Oregon State University in 2004, and gave the Validus Re Distinguished Lecture in 2007. In 2005, he was keynote speaker at the Smithsonian Institution for the Presidential Awards for Excellence in Mathematics and Science Teaching. Stein led non-proprietary seismic hazards investigations for Swiss Re, on Istanbul (2000) and Tokyo (2006).\nStein appeared the Emmy-nominated documentary, ‘Killer Quake’ (NOVA, 1995); the four-part ‘Great Quakes’ series (Discovery Channel, 1997-2001); ‘Earthquake Storms’ (BBC, 2003); and the IMAX film, ‘Forces of Nature’ (National Geographic Society, 2004), which he helped to write and animate. ‘Forces’ was awarded Best feature film of the 2004 Large Format Cinema Association Film Festival; Best film and Best educational film of the 2005 Giant Screen Theater Association, and Grand Prize of the 2005 La Géode International Large Format Film Festival.\nLaurie Johnson is Principal of Laurie Johnson Consulting and Research and a senior science advisor to Lexington and Chartis Insurance companies. She has over 20 years of experience in urban planning, catastrophe risk management, and disaster recovery management, and has researched or helped manage recovery following many major urban disasters, including the 1989 Loma Prieta earthquake, 1994 Northridge earthquake, 1995 Kobe Japan earthquake, and 1997 Grand Forks, ND flood. In 2006, she was a lead author of the recovery plan for the City of New Orleans and coauthored the book, Clear as Mud: Planning for the Rebuilding of New Orleans. She is currently a technical advisor to the City and County of San Francisco on its Recovery and Resilient SF initiative, and a member of the SPUR (San Francisco Planning and Urban Research Association) Resilient City initiative. Laurie completed her Doctorate degree at Kyoto University, Japan in March 2009. Her dissertation focused on developing a management framework for local disaster recovery. She also holds a Master of Urban Planning and a Bachelor degree in Geophysics, both from Texas A&M University. She is on the Board of Directors of SPUR and the Public Entity Risk Institute, on the steering committee for GEER-Geotechnical Extreme Event Reconnaissance organization, and a past Board member of the Earthquake Engineering Research Institute (EERI). She has been a member of the American Institute of Certified Planners since 1990, and a long-standing member of the American Planning Association and EERI.\nAll funds generated from parking revenue are allocated for the preservation and restoration of this National Historic Landmark.\n- 1–2 hours $2\n- 2–4 hours $4\n- 4–6 hours $6\n- SF Main Library, Koret Auditorium June 11, 2014\n- The Women’s Building, Audre Lorde Room May 29, 2013\n- EBMUD Board Room September 27, 2012\n- The Presidio Golden Gate Club April 24, 2012\n- Association of Bay Area Governments, MetroCenter Auditorium June 13, 2011\n- Fort Mason Center, Building C, Rm 210 April 19, 2011\n- Wente Vineyards Visitor Center November 30, 2010\n- EBMUD Board Room July 22, 2010\n- San Francisco Public Library—Koret Auditorium March 8, 2010\n- The Oakland Scottish Rite Center December 3, 2009\n- USGS, Menlo Park September 24, 2009\n- USS Hornet in Alameda August 27, 2009\n- Romberg Tiburon Center (Bay Conference Center) July 29, 2009\n- Doubletree SFO Hotel in Burlingame June 25, 2009\n- San Jose Tech Museum of Innovation May 21, 2009\n- The Oakland Scottish Rite Center March 19, 2009\n- The Presidio Golden Gate Club (Cypress Room) February 19, 2009', 'List of Most Dangerous Earthquakes Ever Recorded in History\nThis list provides information about earthquakes in all parts of the world, which are handed down through historical reports or because of their impact through media reports. The preconceived map shows the concentration of earthquake epicentres along the tectonic plate boundaries.\nEarthquakes that occur under the seabed are often referred to as “seaquakes” or “seaquakes”. If, in this case, raised areas and / or depressions occur over large areas, large volumes of seawater are displaced jerkily. The energy is spread out over water waves and carries the additional danger of tsunamis on coasts.\nHere is the list of the 10 Most Powerful Earthquakes Ever Recorded:-\n10. 1833 Sumatra earthquake\n|Date: November 25, 1833|\n|Location: Sumatra, Indonesia|\n|Duration: 5 minutes|\nThe earthquake in Sumatra occurred on November 25, 1833, at approximately 22:00 local time, with an estimated magnitude of M w = 8.8 to 9.2. This caused a great tsunami that flooded the southwestern coast of the island. There are no reliable records of loss of life, but it is described as numerous victims. The magnitude of this event has been estimated using the elevation records taken from the micro-atoll coral.\n9. 1762 Arakan earthquake\n|Date: April 2, 1762|\n|Location: Chittagong, Bangladesh|\n|Duration: 4 minutes|\nThe 1762 Arakan earthquake occurred at about 17:00 local time on 2 April, with an epicentre somewhere on the coast from Chittagong (modern Bangladesh) to Arakan in modern Burma. It had an estimated magnitude of as high as 8.8 on the moment magnitude scale and a maximum estimated intensity of XI (Extreme) on the Mercalli intensity scale. It triggered a local tsunami in the Bay of Bengal and caused at least 200 deaths. The earthquake was associated with major areas of both uplift and subsidence. It is also associated with a change in course of the Brahmaputra River to from east of Dhaka (Old Brahmaputra River) to 150 km to the west via the Jamuna River.\n8. 869 Sanriku earthquake\n|Date: July 9, 869|\n|Location: Pacific Ocean, Tōhoku region, Japan|\nBasierend auf den beschriebenen Schäden für diesen Ort, die auf eine seismische Intensität von mindestens der Stufe 5 schließen lassen, wird vermutet, dass das Erdbeben eine Magnitude von 8,3 hatte. Eine Simulation durch Minoura et al. von 2001 lokalisierte das Erdbeben zwischen 37° und 39° N, 143° und 144,5° O, wobei die Verwerfung etwa 200 km lang, 85 km breit war und in 1 km Tiefe stattfand. Der Tsunami besaß demnach dann eine Höhe von etwa 8 m. Satake et al. bestimmten 2008 die Verwerfung mit einer Länge von 100 bis 200 km und einer Breite von 100 km bei einer Momenten-Magnitude MW 8,1 bis 8,4. Die Erdbeben-Datenbank des National Geophysical Data Center der US-amerikanischen NOAA gibt eine Oberflächenwellen-Magnitude von MS 8,6 an.\nGeologische Untersuchungen fanden marine Sedimentablagerungen, die auf diesen Tsunami zurückzuführen sind, in der Ebene zwischen dem heutigen Sendai und Sōma mehr als 4–4,5 km landeinwärts. Allerdings lag die Ebene damals etwa einen halben Meter niedriger als heute. Dies bestätigt die beschriebenen großflächigen Überflutungen und die hohe Zahl der Todesopfer (verglichen mit der Tatsache, dass die Gegend damals weit weniger und nur sehr verstreut besiedelt war). So wird für das 8. Jahrhundert für diese zweitbevölkerungsreichste Provinz eine Bevölkerung von 186.000 angenommen.\nZudem wurden Hinweise auf zwei ähnlich verheerende, vorangegangene Tsunamis mit ähnlichen Auswirkungen gefunden: einen zwischen 910 und 670 v. Chr. und einen zwischen 140 v. Chr. und 150 n.Chr. Basierend darauf wird angenommen, dass derartige Tsunamis diese Küstengegend etwa alle 800 bis 1100 Jahre, bzw. unter Hinzunahme des Keichō-Sanriku-Erdbebens 1611 alle 450–800 Jahre treffen. Minoura et al. meinten 2001, dass ein ähnlich starker Tsunami, der etwa 2,5–3 km ins Land eindringe, zu erwarten sei. Diese Vorhersage wurde häufig mit dem Tōhoku-Erdbeben und -Tsunami vom 11. März 2011 identifiziert und dieses wiederum dem Jōgan-Erdbeben 869 gleichgestellt.\n7. 1700 Cascadia earthquake\n|Date: January 26, 1700|\n|Location: Pacific Ocean, USA and Canada|\nThe Cascadia earthquake of 1700 was a mega-shooter between 8.7 and 9.2 on the momentum scale, which occurred in the Cascadia subduction zone on January 26, 1700. In the earthquake were involved the Plate of Juan de Fuca and the Pacific Plate from the island of Vancouver in Canada to the northern coast of California in the United States. The breakage size of the fault was estimated at about 1000 kilometers with a slip of at least 20 meters.\nThe subsequent tsunami struck the east coast of Japan.\nThe earthquake originated at 21:00 on January 26, 1700, although there are no exact records of the time and even of its occurrence. Indigenous texts reveal that a major earthquake occurred, as did Japanese records claiming that large quantities of red cedar were killed by the pre-tsunami sea retreat.\n6. 1868 Arica earthquake\n|Date: August 13, 1868|\n|Location: Arica, Chile (then Peru)|\n|Duration: 5-10 minutes|\n|Max. intensity: XI (Extreme)|\nThe earthquake of Arica of 1868 was an earthquake registered the 13 of August of 1868 near the 16:00 local time. Its epicenter was located at -18,500, -70,350 off the coast of Arica, the present capital of Arica and Parinacota Region, Chile (then capital of the Province of Arica, Department of Moquegua, Peru) and estimated Which released an energy equivalent to an earthquake of 9.0 Mw.\nThe earthquake event devastated much of southern Peru, especially the cities of Arequipa, Moquegua, Tacna, Islay, Arica and Iquique (the latter two currently in Chile). The earthquake was also perceived differently between Lambayeque in the north and Valdivia in the south, and even Cochabamba in Bolivia. Following the main movement, a tsunami swept the Peruvian coasts between Pisco and Iquique and crossed the Pacific Ocean, reaching even California, the Hawaii Islands, the Philippines, Australia, New Zealand and Japan.\nThe estimated death toll would reach 30 people in Chala, 10 in Arequipa, 150 in Moquegua, 3 in Tacna, 300 in Arica and 200 in Iquique.\n5. 1952 Severo-Kurilsk earthquake\n|Date: November 4, 1952|\n|Location: Kamchatka, Russian SFSR, Soviet Union|\n|Casualties: 2,336 dead|\n|Tsunami: 18 m (59 ft)|\nThe main earthquake occurred at 16:58 GMT (4:58 local time) on November 4, 1952. Initially assigned a magnitude of 8.2, the earthquake was revised to 9.0 Mw in recent years. A large tsunami resulted, causing the destruction and loss of life around the Kamchatka peninsula and the Kuril islands. Hawaii was also hit, with estimated damages of up to $ 1 million and livestock losses, but no human casualties were reported. Japan also reported no casualties or damage. The tsunami hit places as far as Alaska, Chile and New Zealand.\nThe hypocenter was located at 52.75 ° N 159.5 ° E, at a depth of 30 km. The length of the fracture subduction zone was 600 km. Replicas were recorded in an area of approximately 247,000 km2 with an epicenter at a depth of km between 40 and 60. A recent analysis of the distribution of the tsunami on the basis of historical and geological records gives some indications about the slip breaking off.\n4. 2011 Tōhoku earthquake and tsunami\n|Date: March 11, 2011|\n|Location: Pacific Ocean, Tōhoku region, Japan|\n|Casualties: 15,894 deaths, 6,152 injured, 2,562 people missing|\n|Duration: 6 minutes|\n|Depth: 29 km (18 mi)|\n|Total damage: US$14.5 to $34.6 billion|\nThe 2011 earthquake of the Pacific coast of Tōhoku in Japan is a magnitude 9.0 earthquake that occurred off the northeast coast of Honshū Island on March 11, 2011. Its epicenter is located at 130 Km east of Sendai, capital of Miyagi Prefecture, in the Tōhoku region, located about 300 km northeast of Tokyo. The maximum seismic intensity is recorded at Kurihara and is 7 on the Shindo scale (highest grade). It spawned a tsunami whose waves reached a height estimated at more than 30 m in places. They have traveled up to 10 km inland, ravaging nearly 600 km of coastline and partially or totally destroying many towns and harbor areas.\nHowever, this earthquake of magnitude 9 is responsible for only a few victims and damage due to the quality of Japanese earthquake resistant construction. The magnitude of the disaster is largely due to the tsunami that followed, which is responsible for more than 90% of the 18,079 deaths and missing persons, destruction and injuries. This tsunami also resulted in the Fukushima nuclear accident placed at level 7, the highest on the international scale of nuclear events (INES) of nuclear and radiological accidents.\nThe reconstruction will take several years and its estimated cost is already the most expensive earthquake in history after that of Kobe in 1995. Estimated economic losses are in the order of $ 210 billion.\n3. 2004 Indian Ocean earthquake and tsunami\n|Date: December 26, 2004|\n|Location: Indian Ocean, Sumatra, Indonesia|\n|Casualties: 230,000–280,000 dead and more missing|\n|Duration: 8.3 and 10 minutes.|\n|Depth: 30 km (19 mi)|\nThe earthquake in the Indian Ocean, also known as the Sumatra Andaman quake, on 26 December 2004 at 00:58 UTC (07:58 UTC time in West Indonesia and Thailand) was a submarine megathrust earthquake with a magnitude of 9, 1 and the epicenter 85 km off the northwest coast of the Indonesian island of Sumatra. It was the third largest ever recorded quake and triggered a series of devastating tsunamis on the coasts of the Indian Ocean. Tourists spent their Christmas holidays on many coastal sections; On the beaches were many people for sunbathing and bathing. In total, the earthquake and its consequences killed some 230,000 people, of whom 165,000 alone were living in Indonesia. Over 110,000 people were injured, over 1.7 million coastal inhabitants around the Indian Ocean were homeless. The event was exceptionally well documented: many tourists had a videocamera to hand or a digital camera, which could also film.\n2. 1964 Alaska earthquake\n|Date: March 27, 1964|\n|Location: Prince William Sound, Alaska, United States|\n|Casualties: 139 killed|\n|Duration: 4-5 minutes.|\n|Depth: 25 kilometres (16 mi)|\n|Total damage: $311 million|\nThe Good Friday quake, also known as the Great Alaska Quake, was the strongest single earthquake in the history of the United States. After the earthquake of Valdivia in 1960 it was the earthquake with the second highest magnitude since the beginning of the regular recording of earthquakes carried out from around 1950 onwards.\nIt occurred on March 27, 1964 at 5:36 pm local time (March 28, 03:36 UTC) and had a moment magnitude of MW 9.2. The epicenter was located in Prince William Sound in southern Central Alaska. Most of the damage occurred in Anchorage, 120 kilometers northwest of the epicenter.\nBy the quake, 125 people were killed, almost all by tsunamis, which reached the fjords of the Prince William Sound and the Kenai Peninsula and reached a maximum height of about 67 meters. Victims were also reported from California and Oregon. The quake lasted nearly three minutes in Anchorage. The biggest destruction in the city was caused by landslides and massive land shifts. Nearly every house near the Turnagain Heights was destroyed by the quake.\nIn the novel The smell of houses of other people of the American-American Bonnie-Sue Hitchcock from 2016, the quake plays an essential role, in that it plays a vital role for some of the youthful protagonists, because of the dying death of their father.\n1. 1960 Valdivia earthquake\n|Date: May 22, 1960|\n|Location: Valdivia, Chile|\n|Duration: 11–13 minutes|\n|Depth: 33 km (21 mi)|\n|Total damage: US$400 million to 800 million|\nThe earthquake of Valdivia on May 22, 1960, also known as the Great Chile Earthquake, was the earthquake with the world’s largest ever recorded magnitude and the heaviest earthquake of the 20th century. At 3:11 pm local time (19:11 UT), the quake on the moment magnitude scale reached a value of Mw 9.5. The topographic shape of large areas of the Little South of Chile was changed, especially the area around the provincial capital Valdivia.\nThe earthquake triggered a tsunami, which caused serious destruction throughout the Pacific. An estimate of the United States Geological Survey (USGS) is about 1,655 deaths, 3,000 injured and two million homeless.']	['<urn:uuid:5eff4d08-5625-4def-8666-a14066873601>', '<urn:uuid:31e78549-e9c7-4f34-b72d-2288144ba48f>']	factoid	with-premise	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-12T19:58:41.790432	42	117	2954
88	What are the main factors that can affect someone's VO2 Max numbers when measuring their fitness level?	The main factors that influence VO2 Max numbers are age, gender, and altitude. Age is significant because your 20s are your peak stage and scores naturally decrease over time. Gender affects scores due to differences in body size, blood volume, and hemoglobin content between men and women. Altitude is important because less air is available to consume at higher altitudes, with an athlete runner's VO2 Max typically decreasing by 5% for every 5,000 feet of increased altitude.	['For athletes, VO2 Max is a popular buzzword.\nThere are tons of discussions about improving numbers scores, calculating them, and what it means to our overall health.\nSo that begs the question, what is VO2 Max, and why is it so important?\nWhat is VO2 Max?\nKnowing your VO2 Max score is an excellent way to gauge your improvements in your physical health. But how?\nVO2 Max is also known as your maximal oxygen uptake. It measures your overall cardio fitness and endurance capacity. For these reasons, athletes are often interested in knowing their scores and improving their numbers, especially if they are considering training.\nYour maximal oxygen consumption is directly related to the ATP that your body produces. ATP is adenosine triphosphate energy and is considered the “molecular unit of currency” of intracellular energy.\nATP is an energy-carrying molecule that’s present in all living things. It takes chemical energy from the breakdown of food molecules and releases it to provide power and fuel to other molecules.\nYour body naturally produces ATP through cellular respiration, a metabolic reaction that breaks down glucose using oxygen. The more oxygen brought into your body by breathing, the more ATP your body naturally creates.\nVO2 Max measures milliliters of oxygen used in one minute per kilogram of your body weight, with the unit mL/kg/min.\nVO2 Max Test\nThe ideal method to determine your VO2 Max score is to take the VO2 Max test in a sports performance lab, where an exercise physiology professional is present. The test takes 10-20 minutes to complete. To get the best results, it’s also crucial that you prepare yourself properly.\nEnsure that you dress in comfortable workout clothes, try not to do any high-level cardio or training beforehand, avoid any food, alcohol, caffeine, or anything that could affect your performance during the test.\nOnce you show up for the test, you’ll be fitted for a face mask to ensure a comfortable, snug fit. A heart rate monitor will also be strapped across your chest to measure your heart rate.\nThe test is in the form of an aerobic fitness activity, usually running on a treadmill or cycling on a stationary bike. The intensity of the activity will be monitored and will gradually increase. Your body will eventually reach aerobic capacity, which is the point where your oxygen consumption levels plateau regardless of the increase in activity intensity.\nOnce you hit the plateau, this is the point your body switches from aerobic metabolism to anaerobic metabolism. You will find that your body will start to get tired.\nWith the maximum oxygen consumption level, the test administrator will calculate and provide you with your VO2 Max score.\nHow to Calculate VO2 Max\nSince it is generally inconvenient and pricey to get your VO2 Max measured at a lab and impractical for the general public, therefore many have found different formulas and algorithms to calculate a VO2 Max estimate.\nThe most accurate one after the direct lab test is the FirstBeat Method. The FirstBeat Method looks at your oxygen consumption in correlation with your speed throughout your workout and analyzes the data along with other background information, such as age.\nOther calculation methods are the Cooper test or the Uth-Sorensen-Overgaard-Pedersen estimation, which looks at your maximum heart rate and resting period.\nAt this point, you probably understand why knowing your VO2 Max score is essential. But what is a good VO2 Max score?\nVO2 Max Range\nLike most aspects of fitness, ranges between men and women are divided—an ideal score for a male in his 20s would be 45 and up. A score of at least 36 is considered adequate.\nAs the age increases, the score decreases because of the natural decline in fitness within the human body.\nFor women in their 20s, 35 and up is considered satisfactory. A score of at least 29 is adequate. Just like the numbers in men, the score will decrease as the person increases in age.\nThe Cooper Test follows a separate range because the calculation is different. These ranges are also essential to know if you used the Cooper Test to receive a VO2 Max score.\nFor men in their 20s, the average score is 2,200. A score of 2,400 means that his performance is above average. 2,800 and over means that he’s in the best possible shape.\nFor women in their 20s, the average score is 1,800. A score of 2,200 is above average, and 2,700 and over means that she’s in the best shape.\nJust like with the other scores, the ranges in the Cooper Test decrease as the age increases. This means that a woman in her 30s can get a 2,000 from the Cooper Test and still be considered above average health.\nWhat Factors Impact VO2 Max Numbers?\nWhen looking at health and VO2 Max ranges, we should consider a few factors to improve accuracy and the correct interpretation.\nThese factors influence the VO2 Max numbers.\nThe main factor in looking at your max number is age. Your 20s are your peak stage regarding your health, and that’s reflected in your VO2 Max score as well. The score will naturally decrease over time.\nDue to the difference in body size, blood volume, and hemoglobin content between men and women, there’s a difference in ranges as well.\nAltitude is vital to know because, at higher altitudes, there is less air to consume. Generally, an athlete runner’s VO2 Max decreases by 5% for every 5,000 feet of increased altitude.\nIf you don’t want to schedule a doctor’s visit or lab test, you can still estimate your VO2 Max at home.\nDr. Kenneth H. Cooper developed the Cooper Test for military use in 1968. It is a 12-minute run to see where your physical fitness lies. Dr. Cooper discovered the distance ran or walked in a 12-minute time span can give us insight into one’s physical fitness level.\nTo calculate your estimated results, follow this formula:\nKilometers: VO2max = (22.351 x kilometers) – 11.288\nMiles: VO2max = (35.97 x miles) – 11.29\nThe unit used to measure the distance will determine which formula you can use to calculate your VO2 Max numbers.\nThis method relies on collecting data like your resting heart rate and your maximal heart rate. To get a measurement using this estimate, you need to follow the formula:\n(VO2max = 15.3 x HRmax/HRrest)\nYou’re taking your maximum heart rate and your resting heart rate and dividing them by each other. Then you’ll take your answer and multiple it by 15.3 to get your VO2 Max estimation.\nWhy is My VO2 Max Score So Important?\nIt should be clear that it can be beneficial for athletes to know their V02 Max, but there are many reasons why the general public should also be aware of their scores.\nPredict Athletic Performance\nFor athletes and runners specifically, knowing your VO2 Max score can help predict your athletic performance. In some cases, it can also help gauge your potential competitiveness compared to your competitors.\nVO2 Max directly contributes to your running economy, which is a great indicator of your physical capacity. Running economy is the measure of how a runner uses their energy. Your body performs exchanges between oxygen and carbon dioxide. People who consume a lower volume of oxygen while running have a better running economy than ones that don’t.\nWith this data, you can predict and compare physical ability and potential success in endurance activities. For example, for two runners exercising at the same intensity, the runner with better running economy is consuming less oxygen, which means they are exerting less effort, and the exercise level is easier for that runner.\nIf your maximum oxygen capacity is high and your energy exertion rate is low, you are more equipped to handle endurance activities.\nAssessment of Physical Health\nRegardless of whether you are an athlete, VO2 Max is a great indicator of your cardiorespiratory fitness and overall health.\nA low VO2 Max score indicates poor physical fitness, which is associated with many health issues, including cardiovascular diseases, obesity, pulmonary diseases, cancer, and more. Knowing your VO2 Max score can give you a more definite idea of your fitness level, which can motivate you to make the necessary adjustments to improve.\nAerobic fitness levels are directly related to your heart health, which can extend your life’s longevity, giving you the means to live a longer, happier, and healthier life.\nBaseline for Training Programs\nYour VO2 Max score is useful when determining what types of physical activities are suitable for your physical capabilities. Whether you are an athlete in training or tackling obesity, your score can help choose an exercise intensity suited to your body’s current limitations. This can prevent injuries and increase the rate of success.\nVO2 Max can help determine your vVO2max (velocity at maximal oxygen intake), which is the intense running pace for a person to reach maximal oxygen uptake. Training at this pace can help you increase your VO2 Max score and improve your overall health.']	['<urn:uuid:509d5d25-ff6f-42bd-9126-5af619d8ce4d>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T19:58:41.790432	17	77	1505
89	hi im doing research what types pollution soil faces and what gases landfills release that affect environment	Soil faces multiple types of pollution including contamination from photographic chemicals, heavy metals, agricultural runoff, and industrial effluents. As for landfill gases, they primarily release methane, which is a potent greenhouse gas that is 20 times more effective at trapping heat in the atmosphere than carbon dioxide. A single ton of municipal waste in landfills produces 123 pounds of methane.	['Soil research is an international journal of soil science publishing high quality research on: soil genesis, soil morphology and classification soil physics and hydrology soil chemistry and mineralogy soil fertility and plant nutrition soil biology and biochemistry soil and water management and conservation soil pollution and waste disposal. Essays - largest database of quality sample essays and research papers on land pollution thesis studymode - premium and free what is soil pollution. Sssaj publishes basic and applied soil research in agricultural papers are grouped by subject matter and cover and soil science society of america . Research paper open access water and soil pollution in punjab with special reference to mandi gobindgarh to measure the impact of soil and water pollution on . Issues pulp and paper mills contribute to air, water and land pollution and discarded paper and paperboard make up roughly 26% of solid municipal solid waste in landfill sites.\nOpen document below is an essay on land pollution from anti essays, your source for research papers, essays, and term paper examples. Essay on soil pollution research papers call for children and studies indoor name: essay about words, it is a science involving chemistry, essays on essays24. Full length research paper topsoil a soil pollution assessment becomes very complex when different sources of contamination are present . Environmental impact due to agricultural runoff research papers pertaining to the heavy metal contents in photographic paper, photo chemicals etc, pollution of.\nToday pollution is very high in both inland and marine waters [tags: environmental pollution essays] research papers 2490 words this soil erosion, . Paragraph, soil pollution essay in english pollution a research paper powerpoint high soil, how to pollution a english without personal pronouns. Soil and water research original papers, ph and conductivity of forest soils in two neighbouring mountains with different pollution in slovakia from 1989 to . Causes and effects of land pollution environmental the method of research adopted is soil pollution slowly and steadily causes environment and .\nWater contamination research papers examine how the pollution of water affects all plants and organisms that live within bodies of water. Land pollution research paper land soil pollution comprises the toxic waste of soils with resources, . Impact of industrial effluents on soil health and agriculture indian agricultural research institute, the high pollution in a small paper mill . Please contact the editor-in-chief, dr jack trevors, at [email protected] to discuss your review proposal news effective 2013, water, air, & soil pollution changed its publication structure to a new publication model: continuous article publishing this means that papers will be published .\nPollution research is one of the leading enviromental journals in world and is widely subscribed in india and abroad by institutions and individuals in industry, research and govt departments . Below is an essay on land pollution from anti essays, your source for research papers, essays, when land pollution is bad enough, it damages the soil. Here is your sample essay on soil sample essay on noise pollution here you can publish your research papers, essays, letters, . Strategically drafted environmental studies and ecology research paper water or soil pollution, writing research papers on environmental studies and ecology.\nSoil pollution research papers - benefit from our affordable custom term paper writing service and get the most from great quality discover key tips how to get a plagiarism free themed research paper from a expert provider use this service to get your sophisticated thesis handled on time. Research paper evaluating pollution potential of irrigation by domestic soil pollution , 169, pp101-123 central soil salinity research institute, p 68. Environmental science and pollution research she has published 98 research papers in jcr in plants and phytoremediation of heavy metal contaminated soil.', 'Environmental Impacts Of Solid Waste Landfilling\nInevitable consequences of the practice of solid waste disposal in landfills are gas and leachate generation due primarily to microbial decomposition, climatic conditions, refuse characteristics and landfilling operations. The migration of gas and leachate away from the landfill boundaries and their release into the surrounding environment present serious environmental concerns at both existing and new facilities. Besides potential health hazards, these concerns include, and are not limited to, fires and explosions, vegetation damage, unpleasant odors, landfill settlement, ground water pollution, air pollution and global warming. This paper presents an overview of gas and leachate formation mechanisms in landfills and their adverse environmental impacts, and describes control methods to eliminate or minimize these impacts.\nTable of Contents\n- Title Page\n- Table of Content\n- 1.1 Background of the Study\n- 1.2 Statement of the Problem\n- 1.3 Objective of the Study\n- 1.4 Research Hypotheses\n- 1.5 Significance of the Study\n- 1.6 Scope and Limitation of the Study\n- 1.7 Definition of Terms\n- 1.8 Organization of the Study\nReview of Related Literature\n- 2.1 Introduction\n- 2.2 Types of Solid Waste\n- 2.3 Methods of Managing Waste\n- 2.4 Theoretical Review: Integrated Sustainable Waste Management\n- 2.5 Stakeholders and Participation\n- 2.6 Conceptual Reviews; Wastes and Solid Wastes\n- 2.7 Management and Solid Waste Management\n- 2.8 Obstacle to Waste Management\n- 2.9 Landfill Development, Operations and Management in Lagos\n- 2.10 Municipal Waste Disposal and Landfill\n- 3.1 Introduction\n- 3.2 Sources of Data Collection\n- 3.3 Population of the Study\n- 3.4 Sample and Sampling Procedure\n- 3.5 Instrument for Data Collection\n- 3.6 Validation of the Research Instrument\n- 3.7 Method of Data Analysis\nPresentation Analysis Interpretation of Data\nSummary Conclusion and Recommendation\n- 5.1 Introduction\n- 5.2 Summary\n- 5.3 Conclusion\n- 5.4 Recommendation\n1.1 Background of the study\nThe Solid wastes comprise all the wastes arising from human and animal activities that are normally solid, discarded as useless or unwanted. Also included are by- products of process lines or materials that may be required by law to be disposed of (Okecha 2000). Solid waste can be classified in a number of ways, on the basis of sources, environmental risks, utility and physical property.\nOn the basis of source, solid wastes are again classified as: Municipal Solid Wastes, Industrial Solid Wastes and Agricultural Solid Wastes. Nigeria’s major urban centres are today fighting to clear mounting heaps of solid waste from their environments. These strategic centres of beauty, peace and security are being overtaken by the messy nature of over flowing dumps unattended heaps of solid wastes emanating from household or domestic or kitchen sources, markets, shopping and business centres. Solid Waste Landfills.\nCity officials appear unable to combat unlawful and haphazard dumping of hazardous commercial and industrial wastes which are a clear violation of the clean Air and Health Edicts in our environmental sanitation laws, rules and regulation. Refuse generation and its likely effects on the health, quality of environment and the urban landscape have become burning national issues in Nigeria today. All stakeholders concern with the safety and the beautification of our environment have come to realise the negative consequences of uncleared solid human wastes found in residential neighbourhoods, markets, schools, and central business districts in our cities. These solid wastes have become recurring features in our urban environment. It is no longer in doubt that our cities are inundated with the challenges of uncleared solid wastes. As a result, urban residents are often confronted with the hazardous impact to their collective health and safety. The hue and cry over the health consequences of exposed and fermenting rubbish have not been quantified, although their impact is noticeable, especially in times of epidemic in congested activity nuclei civic centres, CBDS, neigbhourhoods, etc.\nA United Nations Report (August 2004) noted with regret that while developing countries are improving access to clean drinking water they are falling behind on sanitation goals. At one of its summit in 2000 (Uwaegbelun 2004) revealed that The World Health Organization- (WHO 2004) and United Nations International Children Education Fund- (UNICEF 2004) joint report in August 2004 that: “about 2.4 billion people will likely face the risk of needless disease and death by the target of 2015 because of bad sanitation”. The report also noted that bad sanitation – decaying or non-existent sewage system and toilets- fuels the spread of diseases like cholera and basic illness like diarrhea, which kills a child every 21 seconds. The hardest hit by bad sanitation is rural poor and residents of slum areas in fast-growing cities, mostly in Africa and Asia . Solid Waste Landfills.\nAlthough there is no immediate danger from the methane emitted in atmosphere from landfills, over time it could accumulate inside the landfill mass, thus increasing its concentration with attendant potential to modify the Earth’s climate. 36 percent of human caused methane releases come from our municipal solid waste landfills (USEPA, 1999). A ton of municipal solid waste land-filled produces 123 pounds of methane- a potent greenhouse gas, 20 times more effective at trapping heat in the atmosphere than carbon dioxide (EA, 2008). Hulme et al. (1995) list the adverse impacts of the increased concentrations of greenhouse gases in the atmosphere to include: a threat to disrupt the diversity of habitats and the life dependent on them. In particular, our health, agriculture, water resources, forests, wildlife, and coastal areas are vulnerable to the changes that global warming may bring. It further state that a rise of only a few degrees in the Earth’s average temperature could result in more frequent and intense storms, flooding of beaches, bay marshes, and other low-lying coastal areas; more precipitation in some areas and not enough in others and wider distribution of certain infectious diseases. Such significant changes, note NEST (1991), Hulme et al. (1995) and Nicholson (2001) could damage communities and national economies as well as alter the natural world.\nDeveloping countries like Nigeria are particularly at risk because of her bad waste management system and unhealthy disposal practices. The problem of solid waste disposal is one of the most serious environmental problems facing many cities in Nigeria. Waste management plays an integral role in human activities. Various ways of managing solid waste includes disposal by either burying or burning, reduce or reusing, recycling and energy generation. Solid waste management differs in developing countries like Nigeria and in industrialized countries of the world like Germany. Several factors are responsible for the differences, a good example of these are the types of waste generated in developing countries. Contreau (1982) submitted that, in developing countries, there is much high proportion of organic and considerably less plastic waste such that the large amount of organic material makes the waste denser with greater moisture and smaller particles.\n1.2 Statement of the Problem\nDeveloping country like Nigeria is particularly at risk, because of her bad waste management system and unhealthy disposal practices. Global temperature will continue to increase causing further disruption to climate patterns. Ultimately, all this can only be brought under control by engaging in sustainable waste management practices, and stabilizing greenhouse gases concentrations in the atmosphere. It is on this backdrop that the researcher intends to investigate the impact of solid waste on land fill in Nigeria.\n1.3 Objective of the Study\n- To investigate the effect of solid waste management practice on the environment.\n- To determine the effect of solid waste land fill on the environment\n- To appraise the strategy for effective solid waste land fill management practice\n- To determine the management practices of Nigeria for effective solid waste management land fill\n1.4 Research Hypotheses\nFor the successful completion of the study, the following hypotheses were formulated:\n- H0: solid waste land fill has no significant impact on the environment\nH1: solid waste land fill has a significant impact on the environment\n- H0: there is no significant relationship between solid waste land fill emission and environmental pollution\nH2: there is a significant relationship between solid waste land fill emission and environmental pollution.\n1.5 Significance of the Study\nThe study shall provide a structural study on solid waste\nIt shall investigate the effect of solid waste on the environment\nThe study shall analyze strategies for effective waste management practice. It shall provide a reference source of information for environmental experts. It is believed that at the completion of the study, the findings will be of great importance to the ministry of environment in ensuring that the environment is devoid of pollution. The findings will also be of great benefit to the waste management agency in ensuring that the waste are properly disposed to ensure that it does not constitute a major air pollution in the society. The study will also be of importance to researcher who wishes to carry out investigation in similar topic. Finally, the study will be of importance to lecturers, students, teachers and academia’s as the findings will add to the pool of knowledge.\n1.6 Scope and Limitation of the Study\nThe scope of the study covers the impact of solid waste land fill in Nigeria but in the cause of the study, the researcher encounters some constrain which limited the scope of the study;\n(a) Availability of Research Material:\nThe research material available to the researcher is insufficient, thereby limiting the study.\nThe time frame allocated to the study does not enhance wider coverage as the researcher has to combine other academic activities and examinations with the study.\nThe finance available for the research work does not allow for wider coverage as resources are very limited as the researcher has other academic bills to cover.\n1.7 Definition of Terms\nA landfill site (also known as a tip, dump, rubbish dump, garbage dump or dumping ground and historically as a midden) is a site for the disposal of waste materials by burial and the oldest form of waste treatment (although the burial part is modern; historically, refuse was just left in piles or thrown into pits). Historically, landfills have been the most common method of organized waste disposal and remain so in many places around the world.\nLand, sometimes referred to as dry land, is the solid surface of the Earth that is not permanently covered by water. The vast majority of human activity throughout history has occurred in land areas that support agriculture, habitat, and various natural resources.\nWaste is any substance which is discarded after primary use, or it is worthless, defective and of no use. Examples include municipal solid waste(household trash/refuse), hazardous waste, wastewater (such as sewage, which contains bodily wastes (feces and urine) and surface runoff), radioactive waste, and others\nThe American Public Liquid Association in 1975 defined solid waste as unwanted and useless material with insufficient liquid content to be free flowing, because of its sticky nature, solid waste has the ability to accumulating and physically insulting and degrading the environment if not well managed.\n1.8 Organization of the Study\nThis research work is organized in five chapters, for easy understanding, as follows.\n- Chapter one is concern with the introduction, which consist of the (background of the study), statement of the problem, objectives of the study, research questions, research hypotheses, significance of the study, scope of the study etc.\n- Chapter two being the review of the related literature presents the theoretical framework, conceptual framework and other areas concerning the subject matter.\n- Chapter three is a research methodology covers deals on the research design and methods adopted in the study.\n- Chapter four concentrate on the data collection and analysis and presentation of finding.\n- Chapter five gives summary, conclusion, and recommendations made of the study.\nSummary Conclusion and Recommendation\nIt is important to ascertain that the objective of this study was to ascertain the land use policy and tourism development in Nigeria.\nIn the preceding chapter, the relevant data collected for this study were presented, critically analyzed and appropriate interpretation given. In this chapter, certain recommendations made which in the opinion of the researcher will be of benefits in addressing the challenges of land use policy and how to develop tourism in Nigeria.\nWaste management plays an integral role in human activity. The overall view of solid waste management is to collect, treat and dispose solid waste by urban dwellers in an environmentally and socially satisfactory manner. Until recently, Nigerians have not been particularly concerned about proper waste management, open dumping and open burning in unapproved locations has been the norms. The constraints to effective solid waste management are not limited to lack of policy or laws, but poor infrastructure, education, social awareness of problems and solutions, and lack of institution promoting sustainable environmental actions.\nFire incidents on landfills and dumps in Nigeria, especially those involving burning tyres will continue to pose a serious risk to the health of nearby residents through prolonged or repeated exposure to the toxic chemicals they emit and the contamination of groundwater. Some of the hazardous chemical compounds emitted by burning tyres are capable of causing severe health conditions such as reproductive and developmental disorders, and cancers in humans. Effective landfill management by the operators is therefore necessary to prevent the occurrence of these fires in order to protect the environment and human health. The disposal of waste tyres should be prohibited at all landfills by thoroughly inspecting and controlling incoming waste. Buried waste should also be compacted on a regular basis to prevent hot spots from forming. Since methane is highly flammable and can pose a fire hazard, gas collection and control systems should be installed at the sites to collect landfill gas which can be flared to convert methane to gases less harmful to the environment or converted to energy. This research examines landfill emission and their impact on the environment in Nigeria and therefore calls for the need to improve on waste management practices and construct well engineered sanitary landfills to ensure the protection of human health and the environment.\nHaven successfully completed the study, the following recommendations are put forward by the researcher:\n- Strategic environmental planning of waste management practices should be put in place\n- There is need to ensure strict adherence to guidance and cost analysis of solid waste options in the area.\n- Community participation in collection, selection of sites and design of facilities is inherently essential for sustainability.\n- There is need to strengthen the work force, by recruiting more personnel in the Waste Management Authority.\n- Government should provide adequate funds for waste management personnel for the purchase of more evacuating vehicles and waste disposal containers.\n- There is need for environmental and public health education on the danger of indiscriminate waste disposal in the study area.\nEnvironmental Impacts Of Solid Waste Landfilling\nThe complete material will be sent to you in just 2 steps.\nQuick & Simple…\nMake payment of ₦3,000: through USSD Transfer, Bank Mobile App, ATM Transfer, or POS Transfer to:\n|Account No.: 0811003731|\n|Name: Samphina Academy|\n|Account Type: Current|\nOr Click Here to pay with Debit Card\n|FOR CLIENTS OUTSIDE NIGERIA:|\n|Click Here to pay with Debit Card ($15)|\n|GHANA – Make Payment of 60 GHS to MTN MoMo, 0553978005, Douglas Osabutey|\nSend the following details through Text Message or WhatsApp Messenger | +234-8143831497\n- Payment Details\n- Email Address\n- Environmental Impacts Of Solid Waste Landfilling\nThe complete material will be sent to your email address after receiving your payment information | T & C Apply\nYou may also like:\nThis research material “Environmental Impacts Of Solid Waste Landfilling” is for research purposes and should be used as a guide in developing your research project / seminar work. For no reason should you copy word for word (verbatim) as samphina.com.ng will not be liable for any who copied the material.\nThe aim of providing this material is to reduce the stress of moving from one school library to another all in the name of searching for research materials. This service is legal because, all institutions permit their students to read previous projects, books, articles or papers while developing their own works. According to Austin Kleon “All creative work builds on what came before”.\nsamphina.com.ng is only providing this material “Environmental Impacts Of Solid Waste Landfilling” as a reference for your research. The paper should be used as a guide or framework for your own paper. The contents of this paper should be able to help you in generating new ideas and thoughts for your own research. Use it as a guidance purpose only.']	['<urn:uuid:61ed0236-b96b-4fc9-b06c-e3ced159db13>', '<urn:uuid:14ba08aa-1895-4560-a25d-5faea81d5828>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T19:58:41.790432	17	60	3379
90	What role do bees play in growing our food, and what measures are being taken to stop their declining numbers?	Over 70 percent of the world's crops are at least partially pollinated by bees, including zucchini, pumpkins, summer squash, apples, berries, and melons. Some crops like carrots and onions rely on bees for seed production. To address declining bee populations, which has been noticed since 1998, the European Council has banned neonicotinoid pesticides, and conservation efforts include promoting native plant growth and establishing pesticide-free areas.	['The Pollinator Connection: Be safe, bee home\nby Kristina Lefever for the Tidings April 3, 2020 ~\nHaving a home, no matter the species, is so important — a home you know is always there, fits your size and your lifestyle, and keeps you and your family safe.\nSafe homes are vitally important for pollinators too, and there are many reasons their homes are disappearing. With decades of pesticide use in gardens, yards, farm fields, forests, roadsides and waterways, compounded by unending urban and rural development with human-made structures, lawns, sports fields, pavement, 24-hour lighting and more, plus invasive species (plants and insects), and a changing climate affecting which plants grow where and when, it’s hard for insects, birds and other mammals to find and keep a safe home.\nWhat if in our desire to provide homes, we make it worse? Did you know that mason bees and leaf cutter bees need nests of at least 6 inches in length? Otherwise, the female (mom) will not lay eggs of both genders. It’s hard to continue a species without females. Because not only can the mom control the gender of her offspring, she lays in order by gender. In a 6-inch-long tube, mom will first lay three or four female (fertilized) eggs, followed by two or three male (unfertilized) eggs. If the length is much shorter than 6 inches, she will lay only males. And the boys will not be happy campers when they emerge the following spring.\nThe purpose behind this incredible pattern is two-fold: the males serve as protection — hopefully pests and birds will spare the females after taking out the males at the front end. And the males (laid last, remember), emerge first, so they can be ready for their girlfriends. They will mate, the females will lay her eggs, die, and the eggs will transform into adult bees throughout the year, emerging the following spring to start the cycle all over again.\nSo do us all a favor — when you see those cute little bee houses for sale, check the length. Size does matter. Please advise the store manager why you aren’t purchasing them, and ask that they be returned. Here’s a shout out to Marshalls in Medford. After being informed by our friend Michael that their cute little houses were barely 3 inches deep, and why that mattered, the houses were pulled.\nMost other bee species, including bumblebees, nest in the ground, often under bushes or in untended areas. Please garden, especially now, but be aware that creating that beautiful vision for your landscape could actually be uprooting the next generation of bees and other pollinators who nest or overwinter in the soil, under rocks and leaves, on and in plant stalks, or on tree branches. Remember that 97% of insects are beneficial, and “good” or “bad,” they are an important food source for other creatures.\nHere is a small but critical example. Butterflies, also rapidly disappearing, come from caterpillars. Fewer caterpillars mean fewer birds. One clutch of chickadees requires about 9,000 caterpillars to fledge. How many yards do you know with 9,000 caterpillars? Not mine.\nFor more about how native plants support native insects, I highly recommend googling University of Delaware professor Doug Tallamy. I guarantee you will never look at your landscapes the same again.\nWant to help provide homes for the tiniest among us? It’s easy. Plant native plants, don’t use pesticides (herbicides, insecticides or fungicides), and maintain untended areas that go through their annual cycle without much interference, providing habitat and food for insects and other creatures. The last crop of flowers that aren’t deadheaded become seeds to feed the birds in late winter. (Finches love lettuce and echinacea seed.) Native grasses provide places for bumblebees to nest, and for beetles and spiders to hide (both are great predators). Dry and sunny unmulched bare spots welcome the many species of solitary ground-nesting bees. No worries — most solitary bees don’t even have stingers.\nMore and more people are planting native plants for our native pollinators. A list of local native plant sources we know about can be found at www.pollinatorprojectroguevalley.org/resources.\nPollinator Project Rogue Valley and Beyond Toxics have transformed the frontscape at our office in Phoenix from 100% non-native plants to 75% or more native. Tiny plants are growing, and we eagerly await the flowers of coyote mint, Oregon sunshine, phacelia, oceanspray, penstemon, buckwheat and more. Even when the only plants blooming were non-native heathers, we were amazed to discover queens of three species of bumblebees gathering pollen and nectar to take back to their nests for their babies. Safe homes, we hope.\nKristina Lefever is a member of Pollinator Project Rogue Valley and Bee City USA Ashland, and a board member of Beyond Toxics. She can be reached at email@example.com. The Pollinator Connection appears quarterly.', 'Listen To The Article\nSince 1998, scientists, conservationists, and farmers have noticed an alarming trend. European honeybee populations are declining at rapid rates. Researchers believe several factors are at play here: viruses spread through the colonies, loss of habitat, migratory habits, and increased pesticide use.\nOne class of pesticides, in particular, has been implicated in the decline of bee populations. The European Council recently voted to ban this group of pesticides, neonicotinoids, in an effort to restore bee colonies.\nOver 70 percent of the world’s crops are at least partially pollinated by bees. Without these pollinators, we would face a substantial loss of crop diversity. In your own garden, a loss of pollinators means no more zucchini, pumpkins, summer squash, apples, berries, or melons. Some crops, such as carrots and onions, rely on bees to produce seed, rather than edible plant parts.\nThe need for safer pest control measures is apparent, but you might be wondering how to control pests without harming bees. Read on for a roundup of ideas.\nOrganic Pest Control\nMany pesticides are labeled as safe for organic use. These products are usually derived from plants or other natural materials and they might break down faster in the soil than synthetic products. However, just because a product is labeled “organic,” doesn’t mean it won’t harm bees. Below is a list of common organic pesticides, fungicides, and herbicides, and their toxicity to bees, according to the Xerces Society. When using a pesticide, always opt for the least toxic products.\n- Bacillus thuringiensis\n- Kaolin clay\n- Corn gluten\n- Gibberellic acid\n- Boric acid\n- Horticultural vinegar\n- Lime sulfur and sulfur\n- Diatomaceous earth\n- Insecticidal soap and oil\n- Copper sulfate\nHow you apply pesticides can also make a difference in their toxicity to bees. Pesticides kill bees in several ways. First, bees absorb the chemicals through their exoskeletons when they’re exposed to pesticides in the air. Pesticides can contaminate pollen, nectar, and even dew. When bees come in contact with these substances, they can be killed. Finally, bees sometimes take contaminated pollen and nectar back to the hive, where it harms the other bees.\nIf you must apply pesticides, apply them only to the affected plants, and preferably when the plants aren’t in bloom. Apply them late in the evening when bees aren’t active. Spray pesticides during dry conditions, if possible, since dew can retain the toxins.\nWhenever possible, focus your efforts on prevention strategies, rather than resorting to pesticides to eliminate problems. Organic gardeners take a holistic approach, knowing that no one strategy can combat every pest. When used in combination, though, the following techniques can minimize the pests in your garden.\nFloating Row Covers. Floating row covers are lightweight agricultural fabrics that allow water and sun to permeate while keeping out the bad guys. Install them at planting time and secure them with rocks or pins. Allow enough slack for the fabric to expand as the plants grow. Floating row covers won’t eliminate pests that live in the soil, but they will keep out beetles, flea beetles, and many other pests. Additionally, floating row covers warm the soil and keep it moist so seeds germinate faster. They also encourage faster growth in young seedlings. Remove row covers when summer heat arrives or when plants start to produce flowers so bees can pollinate them.\nHand Picking. This strategy might not appeal to squeamish types, but if you were the sort of kid who loved bugs, this might be right up your alley. Simply stroll through your garden every morning or evening and pick off caterpillars, beetles, snails, and slugs. Drop the pests in a bucket of soapy water. Obviously, this technique won’t work for flea beetles or small pests, but it’s a fast and simple way to control most of the pests in your garden.\nTraps. Veteran gardeners seem to collect creative methods for trapping insects. Wrap a paper collar around tender young seedlings. Push the collar down into the soil at least 3 inches, allowing another 3 inches to remain above ground. Collars are effective at deterring cutworms and other soil-dwelling pests. Some gardeners lay crumpled aluminum foil under plants to keep cutworms, slugs, and snails out. Others place shallow bowls of beer or soapy water in the garden to attract and drown insects. Traps work with varying effectiveness, but are worth a try. Don’t leave anything toxic, such as mothballs, in the garden where children and animals might get it.\nCrop Rotation. One of the simplest ways to control pests and diseases is by rotating your crops from year to year. Some insects are generalists, nibbling their way through the garden like it’s a buffet. Most, though, prefer some plants over others. Cucumber beetles prefer cucurbits, tomato hornworms love tomatoes. Move the crops around and you’ll confuse the pests a bit. Combine crop rotation with inter-planting and you double your odds of success. Inter-planting is the technique of planting two crops together to repel, confuse or deter pests. For example, onions and garlic seem to repel many pests. Plant them among your more vulnerable plants for added protection.\nInvite Beneficial Insects To The Garden. Not every insect is a pest. In fact, some insects can help your garden by feeding on the bad guys. Ladybugs, lacewings, and praying mantises are three insects worth keeping around. Plant flowering herbs in your garden, such as dill, angelica, cilantro, and basil. Beneficial insects often lay their eggs on these plants and their larva feed on them.\nTidy Up. Keeping a tidy garden has more than aesthetic benefits. A clean garden harbors fewer pests. Weeds and grasses growing near your garden can harbor aphids and other pests that not only feed on plants, but carry diseases, such as aster yellows. Overripe produce is sure to attract pests as it rots. Always harvest produce as it ripens and freeze, compost, or give away what you can’t use immediately.\nFinally, sometimes the simplest solution is to take a live and let live approach. A few damaged leaves are rarely fatal to plants and hardly worth the risk of using pesticides. If a specific crop is particularly troublesome, perhaps it’s time to drop it from your vegetable garden inventory. A few simple changes this summer can mean a healthier garden for both you and the bees.']	['<urn:uuid:4ab5931c-ea28-421e-b3de-2b4bce8e2ef0>', '<urn:uuid:f853b51c-7089-4e7f-99e0-3b3f4818ae02>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:58:41.790432	20	65	1863
91	musical entertainment havana cuba nightlife versus political repression artists	Havana offers rich nightlife entertainment with venues like the Tropicana nightclub staging nightly cabarets, and locations like Casa de la Musica hosting traditional Cuban music. However, this contrasts sharply with the government's control over artists, as demonstrated by the repression following the protest song 'Patria y Vida.' Artists face severe consequences for opposing the regime, including imprisonment in maximum security facilities, as happened to several members of the Movimiento San Isidro group.	"['Havana is renowned for its after-dark entertainment scene, and only the tip of the iceberg is visible to tourists on a short stay. Even medium-sized bars usually have a house band playing Cuban classics. The Casa de la Musica in Havana attracts tourists and locals alike, the latter distinguished by their accomplished salsa dance moves and ability to consume whole bottles of Varadero rum. The famous Tropicana nightclub stages nightly open-air cabarets that are a throwback to the decadent days before the Revolution. The Cabaret Parisien at the Hotel Nacional is similar, and both attract tour groups on ‘day and night’ packages from the coastal beach resorts.\nTheater, opera and ballet are staged all year round in Havana and seats are very cheap. Cinemas show films in Spanish, but some have subtitles. Santiago de Cuba is the spiritual home of Son – the music that gave birth to salsa, and regular live sessions are on offer at the Casa de la Trova. In beach resorts, nightlife tends to mimic what is on offer in Havana, with varying degrees of success. Varadero has a thriving scene, but in the smaller resorts (many of which are all-inclusive) most entertainment is planned and formulaic.\nFrom salsa to son, timba to rumba, follows the irresistible sound of Cuba’s hottest music.\nThere is music everywhere in Cuba, people say, and everybody dances in the street. Well, yes and no. There is music everywhere: the jangle of traditional son bursting from cafes on a hot afternoon; the thunk-thunk-thunk of reggaeton blaring out of cars; the bop-bop-bop of congas being played in a living room; saccharine Latin pop blasting from a balcony as a woman does her laundry. There is so much music, in fact, that sometimes one wishes for just a moment’s silence.\nA trip to Havana allows you to immerse yourself in a rich variety of sounds and performing arts and, should the mood take you, to dance, too. The mood has taken me many times and I have been drawn to Havana again and again. It’s a long way to go for a dance but the omnipresent tropical rhythms, along with the city’s timeless ambience and warm but complicated people, have an irresistible allure.\nSince its settlement by the Spanish in 1512, Cuba has become one of the most musically prolific nations in the world. The ensuing centuries saw African rhythms combine with European musical traditions (primarily Spanish and French) to create more than a dozen new forms.\nIn the ’40s and ’50s three of these – mambo, conga and cha-cha-cha – burst out of Cuba and swept the world.\nIn the ’90s it was the turn of son, a provincial music developed in the late 19th century that hit its stride in Cuba the 1920s. It became a worldwide phenomenon when American musician Ry Cooder went to Havana and recorded an album with forgotten stars of the genre. Buena Vista Social Club sold more than 6 million copies and won a Grammy. The film by Wim Wenders, documenting the album’s creation, was nominated for an Oscar.\nIt had been many decades since son was widely heard in Cuba but the government, no slouch when it comes to tourism, realised quickly that the sudden boom in holidaymakers was a direct result of the success of Cooder’s album.\nYou don’t have to go far to find son in Havana any more; it spills out of cafes and bars in the city’s two main tourist areas. Songs from the album are played with verve and feeling by small conjuntos (salsa bands), who follow Compay Segundo’s Chan Chan with My Way or Yesterday. The music may be authentic but the experience is not: you’ll find few Cubans here and it’s possible you won’t be allowed to dance. “No se permite bailar”, dancing is not permitted, reads a sign outside one such restaurant.\nTo hear son in a more lively environment, head for Salon Rosado Benny More, known as La Tropical, in suburban Havana. The open-air venue has been a favorite playground for Habaneros since it was opened by an adjacent brewery in 1904. All the top bands have played there during the past century and it continues to hold an important place in Cuban culture.\nOn Sunday afternoons a band plays traditional music, usually son, to an audience keen to keep alive the accompanying form of dance, which is more refined than the raunchier “casino” style of dancing that accompanies timba, Cuba’s fiery, funky form of salsa that is son’s hyperactive grandchild.\nYou can find timba at La Tropical at night\nTimba groups dominate the other live venues but they also host R&B, son, hip-hop and reggaeton (a Puerto Rican hybrid of hip-hop and dancehall). These clubs have two sessions a day: in the afternoon (5-9pm) and at night (11pm-3am). The matinees are a bargain, never more than $CUC10 ($13) admittance, and they’re an unforgettable experience.\nManolito y su Trabuco, one of the country’s top five salsa groups, play every Thursday afternoon at Cafe Cantante, when they are not on one of their frequent tours to Europe, Canada or South America. There is an open invitation at this gig, which is regularly crammed with locals and tourists, for musicians from other groups, and other genres, to drop in and play.\nEvery year, the Cubadisco week ends with a free outdoor concert, held this year at the Anti-Imperialist Plaza (the space also holds political events). The program had announced that the five nominees in the awards’ most prestigious category, musica bailable (salsa-timba), would play for free.\nAs with many events in Havana, this didn’t come off quite as promised, though the evening wound up offering a more diverse taste of Cuba’s musical delights: Rumbata, an Afro-Cuban folkloric group of drummers and singers, was followed by Orquesta Maravilla de Florida, a charanga group (containing flute and strings instead of brass) formed in 1948 but now comprising young musicians who put a modern twist on the classic sound. Manolito y su Trabuco, the winner in the category, closed the show.\nNeither blazing incandescent lighting (designed to discourage fights) nor intermittent showers could dampen the spirits of the crowd.\nYou don’t always get what you expect in Havana but you almost always get something great. Last year, a week celebrating poetry ended in a remarkable performance in Habana Vieja (Old Havana). At the sea end of Prado, a stately old avenue with a wide marble promenade in its centre, folkloric dancers and their band put on an electrifying performance as the sun set for the assembled, a mixture of locals and foreigners, and those watching from the balconies of nearby apartments.\nThe athletic dancers, dressed in a series of colourful costumes, performed dances of the orishas (gods), as well as a stunning choreography of maypole dancing at breakneck speed, never once getting tangled up in the poles’ colourful ribbons. As darkness fell, the show drew to a close with a spectacular fire dance.\nAfterwards, as the roadies packed up the gear, recorded salsa was played and those who weren’t quite ready to go home paired off and started dancing – yep, we were dancing in the street.\nWhat to hear\nA general rule for going out in Havana is: be relaxed. It can be difficult to find out what’s on and things often change at the last minute. Be flexible and you will see some great performances, even if they are not the ones you had anticipated. Outdoor shows are often cancelled if it rains or if it looks like raining but sometimes they go ahead regardless. If it’s possible, call the venue to check what’s on (you will need to understand Spanish and be persistent: phones aren’t always answered). Otherwise, go anyway.\nSaturdays, 3pm: Conjunto Folklorico Nacional perform on the patio at El Gran Palenque, 103 Calle 4, between Calzada and Quinta, Playa, 836 9075.\nSaturdays, 6pm: Yoruba Andabo perform at the tiny Club Las Vegas, 104 Infanta, between Calles 25 and 27, Vedado, 870 7939.\nSundays, from 1pm: Clave y Guaguanco perform in the closed-off alley Callejon de Hamel, between Aramburo and Hospital, in Centro Habana.\nSundays from 1pm: La Tropical (Salon Rosado Benny More), corner of 41st and 48th, Playa, 209 0985.\nIn tourist bars and cafes in Habana Vieja and Vedado; follow the sounds.\nCasa de la Musica, corner of Galiano and Neptuno, Centro Habana, 862 4165.\nCasa de la Musica, Playa, corner of 25th and 20th, Miramar, 204 0447. Matinees 5-9pm (winter; 4-8pm); nights 11pm-3am. Mostly salsa, also Afro-Cuban, pop, R&B, reggaeton and hip-hop.\nCafe Cantante, corner of Paseo and 39th, Vedado, 879 0710. Matinees and nights. Mostly salsa, also son, Afro-Cuban, pop, R&B, reggaeton and hip-hop.\nSalon Rosado Benny More (La Tropical): see above.\nFlamenco, opera, ballet\nGran Teatro de la Habana, 458 Paseo de Marti, between San Rafael and San Martin, 861 3077.\nTeatro Amadeo Roldan, home of the National Symphony, corner of Calzada and D, Vedado, 830 1011.\nTeatro Nacional, corner Paseo and 39, Vedado, 879 6011.\nThese are held all over Havana. Check TV, papers, radio and the grapevine for details.', ""Documentaries at Miami Film Festival delve into Cuba's voices heard through music\nIn the Cuba of the Revolution, culture has been an often brutally disputed battlefield. Creators make for unruly subjects and present autocrats and dictators with profound challenges and elusive targets. Guns and torture particularly don’t do well against music.\nThree films at the 40th Annual Miami Film Festival, opening Friday, March 3 and continuing through Sunday, March 12 at various venues throughout the city, offer a view of the costs of those battles for some artists — irreparable might-have-beens, never-will-be’s, and exiles — and, implicitly, for the country — but also hope.\n“AfroCuba ’78” (5:45 p.m., Saturday, March 4, at Silverspot Cinema, 300 SE 3rd Ave., Miami) tells the story of a once-promising jazz group, a never-released album, the breaking up of the original band, and the resulting scattering of some of its members.\n“Bebo” (7 p.m., Tuesday, March 7, at Silverspot Cinema, 300 SE 3rd Ave., Miami; also streaming beginning noon, Monday, March 6) offers a view of the life in exile in Sweden of the pianist, arranger, and bandleader Bebo Valdés, a towering figure of the Golden Age of Cuban music. In disagreement with the path of the Revolution, he left the country in 1960. He died in Sweden in 2013 without ever returning to Cuba.\n“Patria y Vida: The Power of Music” (5 p.m., Sunday, March 5, 6:45 p.m. and 9:45 p.m., Tuesday, March 7, at Silverspot Cinema, 300 SE 3rd Ave., Miami) documents the creation, impact, and consequences for those behind a song that exploded in Cuba through social media and became an anthem in the historic protests inside and outside the island on July of 2021.\nFor the Cuban American director and producer of “AfroCuba ’78,” making his film wasn’t just about documenting the music of a band.\n“This film is also about how political manipulation and the games the system in Cuba creates destroyed a project that had the potential to be wonderful, extraordinary,” says Emilio Oscar Alcalde.\nGovernment suspicions about some members wanting to defect ended the chances to travel, killed the release, and created tensions within the band. The original AfroCuba, a group of young virtuosos who blended Afro-Cuban musical traditions and post-bop, came apart. Some members regrouped under the same name but became the band accompanying singer and songwriter Silvio Rodriguez. Others indeed chose exile. “It finished them,” says Alcalde.\nBebo Valdés, a hub in the music life of the Havana of the 1940s and 50s, left for Mexico in 1960 under the pretext of fulfilling a non-existent contract. While on a tour of Europe, he fell in love with a Swedish woman, married, and settled in Stockholm. He made do for years playing modest piano bars and hotel lounges. His story seemed destined to fade to oblivion — until he was rediscovered with the release of “Bebo Rides Again,” an album produced by fellow Cuban exile Paquito D’Rivera in 1994. He debuted in the United States in 1996. He was 78. By the time he retired, Valdés had won four Grammys and five Latin Grammys.\n“Bebo” is a glimpse of his story in Sweden. It includes footage from an interview he granted Swedish journalist Stina Dabrowski in 2005, interviews with Valdés’s Swedish sons, Raymond and Rickard, and footage from a tribute concert organized by Bebo’s grandson, Emilio Valdés, in Union City, New Jersey, in 2019.\nUltimately, however, “Bebo” is as much a celebration of Bebo Valdés as a meditation on resistence and life in exile.\n“I’m concerned with documenting the Cuban diaspora. It’s a mission for me,” says Cuban American filmmaker Ricardo Bacallao, director of “Bebo,” from his home in Berlin, Germany.\n“Nobody asked me to do it, but I see all of us Cubans living outside of Cuba, around the world, doing things, and it’s not being documented.”\nHe notes that the European press still covers the Cuban Revolution with sympathy.\n“Their view seems fixated 60 years ago . . . and it’s so far from reality now, and that’s a problem. You might be a tremendous musician, but if you are a Cuban living in exile, you won’t get the kind of support that other exiles might get. There is suspicion.”\nThat might explain, in part, why a Cuban master musician lived for decades in Sweden, surviving at times doing menial non-music jobs, unrecognized. “How Bebo lived for so long in Sweden and never got the attention he deserved is a question that is still open,” says Bacallao. “I’d like somebody to answer it.”\nThere is a powerful moment in Valdés’s interview when he reflects that “there are two words that must be erased: Hatred and rancor. And the other one, worse yet, vengeance. That is the vilest word there is.” For the filmmaker, the message is clear.\n“This documentary is about us, Cubans living outside the island, and how hard it is to live in exile and the resistance,” he says. “I have interviews with people who knew Bebo from the 1950s, the 60s, and the 70s, and they told me about different Bebos. He was angry. This is a human being, not a superhero. But this (the Bebo Valdés in the interview) is the wise Bebo. You need a certain level of thinking and experience to get to the point of saying: forget about resentment, frustration, and vengeance. We need to have this healing process for the good of the Cuban family.”\nBut before getting to that moment, some battles remain.\nIn “Patria y Vida: The Power of Music,” Beatriz Luengo, a Spanish actress making her debut as director and scriptwriter, offers a privileged view of the creation of the song “Patria y Vida” (“Fatherland and Life”), how the collective that produced it came together, and the impact of the music on both its creators and the people inside and outside Cuba.\nThe very title of the song turns an old Revolution slogan, “Fatherland or Death,” on its head, and the lyrics include lines such as “No more lies, my people ask for liberty/ No more doctrines, let’s no longer shout Fatherland or Death, but Fatherland and Life,” or “May no more blood flow for wanting to think differently. Who told you that Cuba is yours? My Cuba belongs to all my people.”\nThe song also rails against the government’s attempts at censorship, such as Decree 349, which establishes that all artistic activity had to be authorized in advance by the Cuban culture ministry. “Patria y Vida” appeared painted on walls and became the slogan of the opposition in Cuba and cities around the world, including Miami. The video was viewed millions of times on YouTube.\nSuccess comes with costs. The scenes of the repression in Cuba following the people’s embrace of the song’s message are chilling and infuriating. Rapper Maykel Osorbo and visual artist Luis Manuel Otero Alcantara, who participated in the project while still living in Cuba, are now serving nine and five years sentences in maximum security prisons. Both are also members of the Movimiento San Isidro, a group of artists, journalists, and academics formed in 2018 to protest censorship.\nThe Patria y Vida collective also includes Yotuel Romero, a founder of the Paris-based hip-hop group Orishas; Alexander Delgado and Randy Malcom, who comprise the reggaeton duo Gente de Zona, composer Descemer Bueno, and rapper Eliécer Márquez “El Funky.”\nLuengo is the non-Cuban insider. An artist herself, she contributed to the birth and development of the song. She’s also Romero’s wife. The global phenomenon started in the couple’s house. Romero had been thinking for a while about “how to turn the government’s symbols around,” says Luengo, and a conversation about flipping “Patria o Muerte,” which she had seen splashed nearly everywhere while on a visit to Havana “led us to the piano in our living room, and we started.”\nFor Luengo, “this is a story of a song — and the internet. They thought it would all stay between four walls because they knocked down the internet — and as soon as they turned it on again, those images were everywhere. Now if you put the hashtag Patria y Vida on the platforms, you immediately get police abuse, the pain, real people making social denunciations.”\nShe chuckles as she recalls questions about the grand plan of “Patria y Vida.”\n“There was no playlist, no marketing budget, calculation on followers, algorithms. There just wasn’t. It really began, as I believe great stories begin without a pretense. You watch the documentary and realize that they had no pretenses,” says Luengo. “Their big ambition was to help get the voice of Cuba heard.”\nIF YOU GO:\n40th Annual Miami Film Festival\nFriday, March 3 through Sunday, March 12. Complete schedule at miamifilmfestival2023.eventive.org.\nSilverspot Cinema, 300 SE 3rd Ave., Miami, Coral Gables Art Cinema, 260 Aragon Ave, Coral Gables, and Cosford Cinema, 5030 Brunson Dr, Coral Gables\n$13 for general admission; $12 for seniors; and $10 for Miami Film Society Members, students and veterans.\nINFORMATION: 305-237-FILM (3456) or miamifilmfestival.com\nArtburstMiami.com is a nonprofit source of theater, dance, visual arts, music and performing arts news.""]"	['<urn:uuid:ae1fba10-4eff-44f8-8beb-d677f2ba6e28>', '<urn:uuid:f418641b-73e3-40fb-baed-b01e461f8e91>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T19:58:41.790432	9	72	3038
92	What are the key warning signs I should monitor for potential complications after getting surgical stitches, and at what point should I reach out to medical professionals?	You should call your doctor if you notice any signs of infection, including: a yellow or green discharge that is increasing, a change in the odor of the discharge, a change in the incision size, redness or hardening of the surrounding area, if the incision is hot to touch, increasing or unusual pain, or excessive bleeding that has soaked through the dressing. While some soreness, tenderness, tingling, numbness, itching, mild oozing, bruising, and small lump formation around the incision are normal, any of the above infection signs require medical attention.	"[""PATIENT EDUCATION: WOUND CARE\nAfter surgery, you will need to take care of the incision as it heals. Doing so may limit scarring, may help you avoid pain or discomfort, and may help lower the risk of problems like infection.\nYour doctor used stitches, staples, tissue glue, or tape strips to close the incision. And you will need to keep the area clean, change the dressing according to your doctor's instructions, and watch for signs of infection.\nTips for reducing the risk of infection\nTo reduce the risk of infection:\n• Ask your doctor how long you need to keep the area dry. Follow your doctor's instructions exactly.\n• Look at the incision every day, checking for signs of infection (see below).\n• Change the dressing as your doctor recommends.\n• Scrub or rub incisions.\n• Remove the tape strips (such as Steri-Strips) from incisions unless your doctor tells you to.\n• Use lotion or powder on incisions.\n• Expose incisions to sunlight.\n• Take a bath unless you can keep the incision dry. Instead, take showers or sponge baths until your doctor says it's okay to take baths. Before you shower, cover the dressing with a plastic bag or use another method of keeping it dry.\nYou may notice some soreness, tenderness, tingling, numbness, and itching around the incision. There may also be mild oozing and bruising, and a small lump may form. This is normal and no cause for concern.\nSigns of infection\nCall your doctor if you notice signs of an infection, such as:\n• A yellow or green discharge that is increasing.\n• A change in the odor of the discharge.\n• A change in the size of the incision.\n• Redness or hardening of the surrounding area.\n• The incision is hot to the touch.\n• Increasing or unusual pain.\n•Excessive bleeding that has soaked through the dressing.\nChanging a dressing\nBefore you start, make sure you have gauze pads, a box of medical gloves, surgical tape, a plastic bag, and scissors. Then:\n1. Prepare supplies by opening the gauze packages and cutting new tape strips.\n2. Put on medical gloves.\n3. Loosen the tape around the old dressing.\n4. Remove the old dressing.\n5. Remove the gloves. At this point, clean the incision if your doctor told you to do so. (See instructions below.)\n6. Wash your hands, and put on another pair of medical gloves.\n7. Inspect the incision for signs of infection.\n8. Hold a clean, sterile gauze pad by the corner and place over the incision.\n9. Tape all four sides of the gauze pad.\n10. Put all trash, including gloves, in a plastic bag.\n11. Seal plastic bag and throw it away.\n12. Wash your hands.\nCleaning an incision\nTo clean the incision:\n• Gently wash it with soap and water to remove the crust.\n• Do not scrub or soak the wound.\n• Do not use rubbing alcohol, hydrogen peroxide, or iodine, which can harm the tissue and slow wound healing.\n• Air-dry the incision or pat it dry with a clean, fresh towel before reapplying the dressing.\nCaring for stitches, staples, tissue glue, or adhesive strips\nStitches or staples normally cause some redness and swelling where the stitch enters the skin, along with mild irritation and itching. Some drainage from the incision may be expected for the first few days after surgery. But if the discharge does not decrease after a few days, becomes bright red with blood, or contains pus, contact your doctor.\nThe incisions may be protected with tissue glue or small adhesive strips (such as Steri-Strips) instead of a dressing or bandage. If glue was used, be sure to dry the incision area right away if it gets wet. The glue will fall off on its own after a bit of time. If adhesive strips were used, leave them in place until they become loose or fall off on their own.\nOther incision care tips\nAfter some surgeries, you may be given special instructions other than these for taking care of the incision. Be sure to follow those instructions carefully. If you are confused by the instructions or you have a question, call your doctor's office. If the office is closed, leave a message with the answering service. If your pain has increased or you suspect you may have an infection, call your doctor as soon as possible.\nDon't expose your incision to direct sun for 3 to 9 months after surgery. As an incision heals, the new skin that is formed over the cut is very sensitive to sunlight and will burn more easily than normal skin. Bad scarring could occur if the new skin gets sunburned.""]"	['<urn:uuid:50f08f54-e5f5-46ee-b721-586fffb6f60f>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	27	90	783
93	what does original meaning scratch ancient greek graffiti art	In Ancient Greek, 'graphein' meant 'scratch, draw, paint' before it came to mean 'to write'. Similarly, 'graffito' in old Italian slang meant 'a little mark'.	['Experience 11: Scratch – Jun 8 until Sept 21, 2014\nMany of LA’s most influential graffiti and tattoo artists will cover the walls and floors of ESMoA to publicly launch their 21st century encounter with an artistic tradition: The Getty Graffiti Black Book.\nIn 2013 more than 150 of LA’s leading graffiti artists responded to a 16th century manuscript from the vaults of the Getty Research Institute called a liber amicorum (book of friends) by contributing works on paper to be bound into a single book and created the Getty Graffiti Black Book. Street artists have used black books for decades to create a visual memory of drafts and to serve as a vehicle for the exchange of ideas. The extraordinary competition that occasionally arises among such artists can also lead to respect as rivals invite each other to “hit” their black books with original works. The contributing artists decided to give the Getty Black Book the title, LA Liber Amicorum, to capture the spirit of its transformation of rival ‘writing-crews’ into a Los Angeles Book of Friends.\nNow, ESMoA and the Getty Research Institute have invited Getty Black Book artists Axis, Cre8, Defer, Eyeone, Fishe, and Miner to co-curate those crews of creative friends from the LA graffiti art community and turn the art laboratory of ESMoA into an open black book. Graffiti and tattoo artists will transform the space into a cathedral of urban art for the first presentation of the LA Liber Amicorum to the public with SCRATCH.\nGraffito is old Italian slang for “a little mark,” and graphein in Ancient Greek meant “scratch, draw, paint” long before it meant “to write.” Graffiti artists craft letterforms, draft perspective, and merge line, color, and form with the same techniques employed by Renaissance masters like Albrecht Dürer.\nThe first edition of Dürer’s landmark book on perspective was just one of the many rare books that the artists viewed at the Getty Research Institute in the process of creating the LA Liber Amicorum. Some of those jewels in the history of calligraphy, engraving, and emblematic symbolism from the Renaissance to the Enlightenment, as well as sixteenth-century painted friendship-books that inspired the project, will be installed in the space surrounded by the graffiti-writers’ art. iPads will be mounted, so visitors can ‘e-flip’ through the books and not only share the artists’ own creative experience and response to the impact of viewing these rare books, but also continually co-curate the space by choosing which page-openings will be in dialogue with the art on the walls and floor.\nThe SCRATCH art experience is curated by GRI Rare Books Curator, David Brafman.\nSCRATCH Artists AiseBorn | Alex “Defer” Kizu | Arbe | Atlas | Augustine Kofie | Axis | Big Sleeps | Bob Roberts | Charlie Roberts | Cryptik | David Cowan (Miner) | Demer One | Design 9 | Devin Flynn (Relm) | Dr.Eye | Earn One | Eder | Enkone | Eric “Cre8” Walker | Eyeone | Fearo | Fishe | Gajin Fujita | Gorgs | Heaven | Hert | Jeffrey Saunders(Dye5) | Jose “Prime” Reza | Kasl | Keo One | Kopey | Korea | Kozem | Krush | Kyle Kyote | Lokus | Mark 7 | Patrick Martinez | Nuke | Pedro Balugo (PJ) | Plek | Pyer | Raymond “Gramps ”Green | Roder 169 | Saber | Sano | Steven Tishkoff (Mist) | Swank | Tanner | Tempt | Thanks | The Phantom Street Artist | Versus 269 | Vyal | Wram One\n. . . And all of the other artists who left their mark on the walls and in the Getty Black Book’s LA Liber Amicorum Acme | Adict One | Angst | Asylm | Augor | Bash | Betoe | Blosm and Petal | Cab | Cache | Cale One | Care | Chaz | Chris | Chubbs | Crae & K4P Crew | Craola | Craze | Crime (Rick One) | Czer One | Duem and Crae | Elika | Else | ESK31 | Estevan Oriol | Gabe88 | Gasoline | Gkae | Graff One | Green | Haste | Hex (LOD) | Hex (TGO) | Hyde | Jack | Jack Rudy (Mr. Huero) | Jake One | Jero and Each | Joker One | Junior (“Kost One”) | K4P (Collaboration of the K4P crew-Pranxs Method | Biser | Crae | Dsrup) | K4P (Chelo | Noek | Notik) | Kaos | Krenz (Yem) | Kwite One | Ler Keen | Look | Mach Five | Mandoe | Man One | Mike Miller | Mister Cartoon | OG Abel | Owen | Panic One | P. Chuck | Phever | Playboy Eddie | Pletk P17 | Precise | Punk | Push | Pyro | Rayo | Relic | Retna | Rev | Rich One | Rick Ordoñez | Risk | Rival | Sacred 194 | Sel | Ser | Shandu One | Sherm |Skan One | Skez | Skill | Slick | SomeOne | Soon One | Spade | Spurn | Syte One | Teler | Test | Thel | Trigz | Tyke Witnes | Useck | Vox One | Werc | Wise | Wisk | Woier | Zes\nLA Liber Amicorum\nThe Getty Black Book Project was the brainchild of donor and art collector Ed Sweeney, who approached the GRI with the idea and brought the graffiti artists together, ultimately donating the book to the Getty Research Institute.\nImage: Work by Axis ©Axis 2012']	['<urn:uuid:5562f9a6-f24a-47a8-a95b-e9f0d98d16f7>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	9	25	910
94	As a gastroenterologist studying procedural diagnostics, I'd like to understand what medical examination would be most appropriate when we need to visually inspect both the esophagus and parts of the small intestine in a single procedure?	A gastroscopy is a procedure that enables a doctor to look inside your oesophagus, your stomach and part of your small intestine.	['Every day I actually live with the fear that I will aspirate one last time and die. The newest problems relating in order to this disease is wheezing, coughing and body pains and chills.\nA gastroscopy is a procedure that enables a doctor to appearance inside your oesophagus, your own stomach and part of your small intestine. You may even have indigestion because your developing baby in your uterus (womb) presses against your stomach.\nA hiatus hernia takes place when part of your own stomach pushes up in to your diaphragm (the page of muscle under your current lungs). It may partially block refluxed stomach acid cleaning from your oesophagus, leading to heartburn. The improved pressure, particularly after a new large meal, can result in acidity reflux into the oesophagus. It is because these symptoms may end up being a sign of a more serious underlying medical condition, this kind of as a stomach ulcer or stomach cancer. An individual may need to be referred for an endoscopy to rule out any significant cause. The main indicator of indigestion is pain or a feeling regarding discomfort in your upper abdomen (dyspepsia). Severe upset stomach can cause long-term difficulties with parts of your own digestive tract, such as scarring of the oesophagus or even the passage from your own stomach.\nWhen used regularly, nonsteroidal anti-inflammatory drugs (NSAIDs) such as ibuprofen or naproxen can trigger heartburn. text: Since if anyone needed one more to quit: Smoking can make GERD worse. “For typically the first System.Drawing.Bitmap I merely hated you, ” the girl told me, “and and then for the next 2 months — I used to be getting some trouble swallowing — I figured I was proceeding to die of esophageal cancer. ” Then the lady nudged me and extra, “You know, we’re the main reason that it’s not therefore easy to obtain 6 p. m. Her reflux was serious, and am explained that changes were needed. Even though these conditions may not necessarily seem linked, postnasal drip and a cough are typical reflux symptoms that could easily be mistaken with regard to something else.\n9. Enjoy chewing gum and liquorice.\nIndigestion has many causes including medical conditions, medications, diet, in addition to lifestyle. The definition regarding Indigestion is an unpleasant a sense of fullness, pain, or perhaps burning in your upper abdomen. He or she is a Clinical Professor (retired) in the Division of Emergency Treatments, UT Health Science Middle at San Antonio, and it has been the Chief associated with Emergency Medicine at LACE Medical Branch and at UTHSCSA with over two hundred fifity publications.\n8. Don’t wear overly-tight clothing.\nYou don’t notice this particular happening as it’s in tiny amounts each time. They might be caused by something else, such as a heart attack. You may have got a peptic ulcer, which often is a break in the lining of your respective abdomen or duodenum (part regarding your small bowel). pylori bacterial infection is leading to your indigestion (see Analysis of indigestion), your GP may recommend a combination of medicines. It breaks down bubbles of gas inside your digestive system to relieve wind and bloating.\nDefinition in addition to Facts about Upset stomach\nIt is very hard and I feel that people who point out “it’s just heartburn” are usually very ignorant. In mil novecentos e noventa e seis my new doctor realized that I did not have an esophageal sphincter and in early 1997 I had fundoplication surgery to help manage the acid of which kept going up the throat. I have but to go back to my doctor but wondered if anybody out there has received similar absence of success with medicine for severe and almost regular GERD. I have recently been diagnosed with GERD following suffering for a although with almost constant acid reflux and a feeling associated with a lump in the throat. Just recently I returned to the doctor as I was experiencing a rise in heartburn.\nI have never had heartburn in my lifestyle and I only started out feeling burning in our throat when I started taking medicine for GERD. I am now 43 and had not had any symptoms or issues with heartburn or any additional symptom associated with GERD again until 9 a few months ago.\nYour GP might also recommend altering other medications that may possibly be causing your upset stomach. Taking certain medicines with regard to indigestion can hide a number of the problems that could normally be spotted during an endoscopy. An endoscopy is not often needed in order to diagnose indigestion, but your GP may suggest that you have one if:\nCandidates for lap band surgical procedure are often individuals with the body mass index over 40 kg/m2, or usually are more than 45 kilograms over their ideal body mass. Lap Band Surgery (Gastric Banding)Lap band (gastric banding) surgery, also known as laparoscopic flexible gastric banding (LAGB) is a surgical procedure through which an adjustable belt is usually placed around the upper section of the stomach. Therapy of gallbladder depends on the cause, which may possibly include surgery. Causes of gallbladder pain include intermittent obstruction of ducts by gall stones or gallstone inflammation and/or sludge that also may require irritation or infection associated with surrounding tissues, or whenever a bile duct is totally blocked. Gallbladder Pain (Gall Bladder Pain)Gallbladder pain (often misspelled “gall bladder”) is usually produced by of several problems, biliary colic, cholecystitis, gallstones, and pancreatitis.\nStomach ulcers take place once the thick layer associated with mucus that protects… READ MORE Stomach ulcers are painful sores in the lining of the abdomen.\nBut if you’re finding that hard to swallow, or perhaps if you’re over fifty-five and are losing weight without a reason, your GP may possibly refer you for a good urgent endoscopy. An endoscopy uses a long flexible tube with a camera at the end to see inside your oesophagus and stomach.']	['<urn:uuid:1eb73231-6725-42bd-9fbe-c1567c1205f7>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	36	22	989
95	Hey, I'm planning to become a doctor and I'm curious about how long it takes after medical school - can you tell me what training is needed to practice medicine?	After medical school, a candidate typically needs to complete three years of residency and a fellowship. Medical residents generally work about sixty hours per week. It's also common for residents to spend two additional years doing research. The training process is time-consuming, challenging, and competitive.	['The practice of medicine is a science that manages the care of individuals. This practice includes medical diagnosis, therapy, palliation, as well as prevention of diseases. The goal of medication is to advertise the individual’s general health. In addition to this, physician likewise work to promote the wellness of their individuals. In this regard, doctors specialize in different fields such as inner medicine, pediatric medicines, and emergency medicine.\nThe technique of medicine has actually been around for numerous centuries. Historically, it was a technique that was both an art as well as a science. As a matter of fact, several of the earliest types of medication were methods that were tied to faiths. For instance, old philosophers used bloodletting based upon humorism concepts. Today’s clinical area is a mix of science and art. In fact, suturing techniques are learned through practice; science is made use of to comprehend the mobile and also molecular levels of cells.\nIn most countries, physicians are called for to be registered by the federal government. This procedure generally calls for a clinical degree from a college as well as accreditation from the clinical board. In addition, the candidate might need to pass a number of examinations. These requirements are made to help make sure the quality of healthcare provided. Furthermore, they aim to protect the general public from deceitful medical professionals.\nMedical history go back to ancient Greece. Hippocrates is taken into consideration the “dad of modern medicine”. He introduced the Hippocratic Oath, which medical professionals must take. He was also one of the first to classify ailments as well as to use terms such as “worsening,” “relapse,” and “situation.” He likewise did some of the most adventurous surgical procedures of his day.\nVarious other areas of medical research study consist of genes and also lifestyle medicine. Histology, for instance, is the study of cells and also organs under a microscopic lense. Biochemistry and biology, on the other hand, examines the chemical processes that occur within living organisms. Microbiology, on the other hand, focuses on bacteria as well as their communications. Other areas consist of biostatistics, physics, as well as chemistry. In addition, neuroscientists research study conditions of the nervous system. Furthermore, biostatistics is necessary to the planning and examination of clinical research study. Better, it is the basis for evidence-based medication and also epidemiology.\nProfessionals in particular fields are likewise important for the healthcare system. Some professionals, such as gynecologists, specialize in treating problems that impact ladies. As a whole, medical professionals in family medicine technique non-emergency circumstances. Furthermore, obstetricians and also gynecologists likewise focus on reproductive medication. They focus on identifying ailments, as well as some of them are even able to execute treatments.\nGeneral doctors, on the other hand, deal with illness of the body’s cells. Their expertises vary extensively, but they include whatever from dealing with infections to detecting cancer cells. These medical professionals likewise deal with the senior, and also most of them operate in doctors’ workplaces, taking care of residences, and also assisted-living facilities. They additionally aid patients with chronic medical problems and provide flu shots.\nThe training required to practice medicine varies substantially across nations. Typically, a candidate has to finish three years of residency as well as a fellowship. Clinical schools vary considerably across countries, yet in general, medical homeowners benefit regarding sixty hours a week. It is additionally typical for homeowners to invest two extra years doing research. This training is time-consuming, challenging, and also affordable, yet the benefits are significant. If you have an interest in ending up being a doctor, you need to understand that you may be working for years after clinical institution.\nA medical history includes details regarding the person’s existing health and wellness and also previous diseases. It is additionally essential to supply details regarding the person’s family. This may assist the physician determine certain conditions in the family. Some physicians also maintain a document of the client’s allergies. The information you supply will aid determine the most appropriate therapy. Furthermore, a medical professional can assist the individual understand their own body better. This process is called medical diagnosis. If the physician believes that an individual has a significant ailment, the physician might recommend a drug to deal with the trouble.\nAlong with submitting claims directly to payers, clinical billers additionally prepare as well as send appeal letters. A clinical biller has to be familiar with types and understand exactly how each area is utilized. Most of these fields are set into the technique management system. If you select to use digital filing, it is essential to follow HIPAA insurance claim criteria.\nThe header of a medical case must include the patient’s complete lawful name, date of birth, sex, and address. The client’s NPI (National Service Provider Identifier) ought to also be included in the header. Without an NPI, your practice will certainly not be able to receive compensations.\nWhen you’re making use of a health insurance plan, you’ll require to know what to anticipate from the doctor. Some types of procedures are exempt from insurance coverage while others require an HCPC code. A lot of clients will require to pay the deductible prior to their strategy covers the remainder. In many cases, a deductible uses per calendar year. Some health insurance plan might also forgo the deductible for office check outs. You’ll require to recognize your deductible if you wish to stay clear of paying an excessive quantity.\nClinical payment as well as coding services are an indispensable part of the profits cycle for medical care companies. A smooth clinical billing and coding procedure assists to ensure that suppliers are spent for the services they supply. This procedure guarantees that provider organizations stay open and can remain to offer treatment. It likewise reduces the amount of time required for a provider to make money.\nAs soon as a clinical claim gets to the payer, the insurance company will certainly examine the service provider’s insurance claim as well as make a decision the amount of money to pay. If the service provider is denied repayment, the payer will certainly provide an ERA declaration discussing the reasons for being rejected. When this takes place, the biller can either fix the insurance claim or resubmit it.\nWhile Medicare covers numerous services and also materials given by medical providers, it doesn’t cover every one of them. For that reason, it’s necessary to know how to determine which ones are covered before you obtain your health center therapy. In some cases, Medicare will just cover particular solutions when they are offered by a company in its network. In this instance, it’s best to speak to that medical professional team to identify whether they belong of your health insurance. website\nDoctor methods can conserve a substantial amount of cash by using digital claims monitoring. The majority of them currently submit their medical insurance claims with a clearinghouse. This clearinghouse will certainly examine the cases and also ensure they comply with government regulations as well as payer policies. A clearinghouse also gives a series of services to lower errors.']	['<urn:uuid:3eb47614-12bc-4562-8e9c-8222bfe38667>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T19:58:41.790432	30	45	1182
96	crm implementation benefits security risks	CRM implementation offers benefits like better customer understanding, improved interactions across sales/marketing/service functions, and data-driven decision making. However, it also presents significant security risks including malware attacks, data leakage, account hijacking, and insider threats. To mitigate these risks while maintaining CRM benefits, organizations should implement security measures like multi-factor authentication, role-based access controls, encryption, and regular security audits.	"['For those new to customer relationship management, MyCustomer.com has compiled a series of articles and downloads from its extensive archive to create a comprehensive overview. Whether your question is \'what is CRM?\' or \'what CRM software should I use?\', the answers to your questions are here...\nCustomer relationship management has been known to confound even those experts who have worked in the industry for years. So what hope is there for those approaching the field for the first time?\nThere is no agreed definition, but CRM is a blend of operational processes, methodologies and technologies that helps organisations create two-way interactions with customers, thereby enabling them to better understand and anticipate the behaviours, needs and wants of existing and potential customers.\nSales, marketing and customer service are just three of the functions that operate within the CRM framework, simultaneously drawing information from the customer database to facilitate interactions and also feeding new details gleaned from the contact back into the system - whether this be feedback, buying behaviour, etc.\nMyCustomer.com has compiled a definitive list of articles and downloads from its extensive archives to revisit the most important facets of CRM - and ensure that customer relationship management can be understood by everybody.\nWhat is CRM?\nWhat is CRM and what does it consist of?\n""There appears to be much confusion in the marketplace as to the exact nature of CRM, primarily because so many different developments are included within its scope. This has then been aggravated by software vendors trying to include their own products within the scope of CRM. This PowerPoint presentation by Richard Forsyth provides a simple introduction to the nature of CRM.""\nWhat is CRM?\n""In the short time that the concept of CRM has been around, a great deal of confusion has arisen as to its precise meaning. Most of this stems from the activities of management consultants and software vendors who seek to apply their own stamp to the concept. In reality, CRM is a straightforward concept...""\nThe path to CRM success\n""Successful companies build their business around the customer. They know who their most valuable customers are and understand their needs and buying habits. They target and tailor their offerings and personalise their interactions with their customers. They strive to become fully customer driven, delivering superior customer value and consistently providing exceptional customer experience across all customer touch points. Essentially, this is what CRM is about: it’s a customer-centric business strategy – not a technology.""\nHow to get started with CRM\nOften, the hardest part of implementing a CRM system is getting the project started. \'How to get started with CRM\' discusses the key factors to consider when undertaking a CRM project; from setting objectives, developing KPI’s and performing a business assessment, to planning the implementation, potential pitfalls and much more.\nDeveloping a CRM strategy\n""The aim of the CRM strategy is to be able to find ways to deliver greater value to customers in more cost efficient ways that employees find satisfying. A CRM strategy has to be good for the customer, the employee and the organisation.""\nDeveloping a CRM vision and strategy\n""What is a CRM strategy? This is common question and it does not help when answers differ. There is a lot of confusion between a CRM business strategy and a CRM implementation plan. A CRM strategy is the blueprint for how an enterprise is going to turn its customers into an asset by building up their value.""\nA strategic framework for CRM\n""Power and choice are moving to the customer as never before and leading to the commoditisation of products and services in most situations. In this environment, product quality and features are a given, and in many industries are now so undifferentiated as to provide no significant competitive advantage. As a company, you must choose whether to compete on the basis of price in a cutthroat commodity market, or on the basis of customer relationships created through a superior value proposition.""\nEvolving your customer strategy to meet changing needs\n""It is time to dust down the leadership device of strategy, build it around customers and add a strong flavouring of adaptiveness to suit the modern market environment. Combine this with good programme management and you have the tools for success.""\nThe eight step survival guide to buying/implementing CRM software\nRichard Boardman provides a step-by-step guide to avoiding the pitfalls of buying and implementing CRM software.\nCRM buyers guide - How to choose the right CRM software for your business\n""Where do you start? What do you want that you haven\'t got today? What customer information do you struggle with as a company and from department to department? CRM technology is now so developed it’s likely anything you’ll come up with will be possible, with any combination of requirements.""\nOn-demand vs on-premise\n""Software as a service (SaaS) has come a long way since Tom Siebel dismissed it as a \'doomed\' model. Stuart Lauchlan traces the development of SaaS CRM - from the emergence of the next generation of applications firms such as Salesforce.com and NetSuite, to the hybrid models of Microsoft and SAP - and speaks to some of the most significant figures involved in the sector to find out what the future holds for the battle between on-demand and on-premise CRM.""\nThe CRM guide to Cloud Computing\nThe term Cloud Computing is becoming more familiar but what is it exactly and why is it so important to CRM? Stuart Lauchlan offers his guide and explains how CIOs are approaching doing business in the Cloud.\nCRM market trends: what buyers need to know\n""Companies are needing to invest in their customer relationship management capabilities as part of their focus on growth, and in many areas software can help drive this growth. However, there are several CRM market trends occurring that buyers should be aware of.""\nCRM: Hosted or on-premise?\n""You are the IT director of your organisation and you have decided you need customer relationship management software so your organisation can learn more about your customers. However, you now face the dilemma of whether to go for a hosted or on-premise solution. Choosing between the two seems harder than you initially thought, so here are a few pointers to help make your decision a little easier.""\nThe six hidden traps of CRM system implementation\n""While the instances of outright failure are few and far between these days, CRM technology implementation projects continue to be a source of pain and frustration. Recent research relating to IT projects in general indicated that the average project came in 56% over budget and 84% later than expected.""\n10 ways to implement CRM on a tight budget\n""With the economic environment getting tougher, the need for effective CRM systems increases. But with budgets often getting tighter, the scope to introduce much needed technology decreases.""\nAvoid nasty surprises: What you should cost into a CRM programme\n""The chastening early experiences of CRM investment are slowly fading from memory and firms are once again starting to loosen their wallets when it comes to customer relationship management. Nonetheless, there are plenty of nasty surprises lying in wait for firms if the cost process isn’t grasped.""\nHow to make software as a service (SaaS) CRM a success\n""Make sure you have a business case in place. Do you know why you\'re choosing to go down the SaaS route? Weigh SaaS against on-premises on criteria more comprehensive than just cost tradeoffs alone.""\nIs internal CRM the key to a harmonious organisation?\n""The implementation of I-CRM should aim to see processes and knowledge mature in a variety of areas: IT management should evolve to understand business and then actively encourage IT staff to understand it, with a view to requiring it of all staff; similarly, business managers should improve understanding of IT.""\nCRM - Inside out: The hidden customer\n""One way of thinking about business that is most certainly sensible and which also has the potential to make an enormous impact on the bottom line is the idea of applying customer relationship management thinking within an organisation rather than only to external customers.""\nSix tips for mobile CRM success\n""Ideally, companies should review potential mobile CRM applications before they need them to enable a greater analysis of the most appropriate solution for the business. But realistically, a company needs to implement a system as soon as any of its employees need access to customer and transactional data whilst out of the office.""\nThe business case for mobile CRM\n""Facing the prospects of a global recession, corporations are under increasing pressure to win over new customers and keep existing ones. The most critical personnel in this battle are sale representatives and field service technicians. Mobile CRM solutions provide these front-line customer ambassadors with an invaluable competitive edge.""', ""Salesforce is a cloud-based Customer Relationship Management (CRM) platform that has revolutionized the way organizations manage their customer data. With Salesforce, businesses can access data from anywhere and leverage it to make informed decisions. As an organization's largest container of data, Salesforce is also a target for cybercriminals. In this blog, we will explore the threats to Salesforce data and the steps organizations can take to safeguard their most valuable asset.\nUnderstanding Salesforce Data Threats\nSalesforce data threats come in many forms, including:\n- Malware and Ransomware: Malware can infect Salesforce accounts through email phishing or by downloading malicious software. Ransomware can encrypt files and demand payment in exchange for the decryption key.\n- Data Leakage: Data leakage occurs when sensitive information is disclosed to unauthorized parties. Data breaches can occur due to human error, such as misconfiguration, or malicious attacks.\n- Account Hijacking: Account hijacking occurs when an attacker gains unauthorized access to a Salesforce account. This can be done through password cracking, social engineering, or exploiting vulnerabilities.\n- Insider Threats: Insider threats occur when employees, contractors, or partners with access to Salesforce data misuse or leak data. Insider threats can be accidental or intentional.\nBest Practices for Securing Salesforce Data\nTo protect Salesforce data, organizations should implement the following best practices:\n- Use Multi-Factor Authentication: Multi-factor authentication (MFA) adds an extra layer of security to Salesforce logins. With MFA, users must provide additional authentication factors, such as a one-time password or biometric authentication.\n- Restrict Access: Limit access to Salesforce data to only those who need it. Implement role-based access controls (RBAC) to restrict access based on job function.\n- Encrypt Data: Encryption is the process of encoding data to protect it from unauthorized access. Salesforce offers encryption at rest and in transit, which ensures that data is protected both while it's being stored and when it's being transmitted.\n- Implement Monitoring and Alerting: Monitor Salesforce activity for suspicious behavior, such as failed login attempts, and set up alerts to notify administrators of potential threats.\n- Conduct Regular Audits: Conduct regular audits of Salesforce data to ensure that access is restricted appropriately and that sensitive data is not being leaked.\n2Partnering with a Salesforce Security Expert\nPartnering with a Salesforce security expert can help organizations protect their data by:\n- Conducting Risk Assessments: Salesforce security experts can assess an organization's security posture and identify vulnerabilities that could be exploited by attackers.\n- Providing Guidance on Best Practices: Salesforce security experts can provide guidance on best practices for securing Salesforce data, such as implementing MFA, restricting access, and encrypting data.\n- Implementing Security Controls: Salesforce security experts can help organizations implement security controls, such as RBAC and monitoring and alerting.\n- Providing Incident Response: In the event of a security breach, a Salesforce security expert can provide incident response services, such as containing the breach, investigating the incident, and restoring data.\nAdditional Considerations for Salesforce Data Security\n- Data Backup and Recovery: While preventive measures are essential for protecting Salesforce data, it's also critical to have a data backup and recovery plan in place. This ensures that organizations can quickly recover from a data breach or loss.\n- Regular Software Updates: Regularly updating Salesforce software is crucial for maintaining data security. Software updates often include security patches that address known vulnerabilities and protect against new threats.\n- User Training and Awareness: Educating users on Salesforce security best practices and potential threats is essential. This includes training on password hygiene, recognizing phishing emails, and reporting suspicious activity.\n- Third-Party Integrations: Many organizations integrate third-party applications with Salesforce to enhance functionality. However, these integrations can pose security risks if not adequately vetted. Organizations should ensure that all third-party applications are properly assessed for security risks before integrating them into Salesforce.\n- Compliance with Regulations: Organizations must comply with industry regulations, such as GDPR and HIPAA, that govern the handling and protection of customer data. Compliance requirements vary depending on the industry and location, so organizations must understand the regulations that apply to them and implement necessary security measures.\nIn today's digital landscape, data security is a top concern for organizations of all sizes. Salesforce is an essential tool for managing customer data, but it's also a target for cybercriminals. By understanding the threats to Salesforce data and implementing best practices for securing it, organizations can minimize the risk of a data breach. Additionally, partnering with a Salesforce security expert can provide additional protection and ensure that organizations are fully prepared to respond to a security incident. With a comprehensive security strategy in place, organizations can leverage Salesforce to drive business success while keeping their data safe.\nSalesforce data is an organization's most valuable asset, and protecting it should be a top priority. By understanding the threats to Salesforce data and implementing best practices for securing it, organizations can minimize the risk of a data breach. Partnering with a Salesforce security expert can provide additional protection and ensure that organizations are fully prepared to respond to a security incident.""]"	['<urn:uuid:fad39e8a-4574-4ee6-aa5e-c8e21f5b2bea>', '<urn:uuid:550212e7-78a8-4d95-a363-d153ba636e4c>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T19:58:41.790432	5	58	2303
97	how does debt consolidation loan affect credit utilization ratio and what are upfront fees charges	Debt consolidation loans can lower your credit utilization ratio by paying off existing credit card balances, which positively affects your credit score. However, there are various fees to consider: origination fees can range from 1% to 10% or $25 to $400, late charges can be $5 to $30 or 1% of the payment, and some lenders may charge returned payment fees of $30. Federal law prohibits debt relief companies from charging fees before they settle or reduce your debt.	['Debt Management & Settlement Providers – Texas Office Of …\n99% $600$35,000 6 months to 5 years Not specified Late charge: $29; Returned payment fee: $30 It offers unsecured, fixed-rate loans to consumers with broken credit at lower rates than dangerous payday lenders, which can charge as much as 400 percent interest. Loan amounts are smaller and rates are greater than normal debt combination individual loans, but the lender is still a great option to the high rates of interest and concealed costs that can include payday advance.\nThere is no charge for paying the loan off early. If you do not receive an unsecured personal loan, One, Main may accept your automobile, boat, Recreational Vehicle or bike as security, offered it is insured and assesses at an enough value. One, Main charges an origination charge of 1 percent to 10 percent, or a flat rate of $25 to $400.\nLate costs also differ by state. In addition, borrowers in North Carolina have unsecured loan limitations of $7,500. 99% $1,500$20,000 2 to 5 years Not defined Origination fee: 1% to 10% or $25 to $400; Late charge: $5 to $30 or 1.\nDiscover offers unsecured individual loans for financial obligation consolidation, with the alternative to pay lenders straight. Discover personal loans have no origination costs, closing expenses or prepayment penalties.\nDebt Consolidation Company – Consumer Credit\n8/ 5. 0 660 6. 99%24. 99% $2,500$35,000 3 to 7 years $25,000 Late fee: $39 Rather than simply looking at credit report, Upstart considers a candidate’s education, area of research study, making prospective and task history when identifying loan qualification. Its minimum FICO credit report is 600, which is near the lower end of the reasonable credit band.\nThe initial application creates a soft credit pull that does not injure your score, and you can get your loan money in one service day after approval. Upstart likewise charges origination charges of up to 8 percent, which is high.\n5/ 5. 0 600 6. 95%35. 99% $1,000$50,000 3 years or 5 years Not specified Origination cost: up to 8%; Late charge: the greater of 5% of unpaid quantity or $15; Returned check charge: $15; One-time paper copies charge: $10 The $40,000 loan limit can accommodate borrowers with a lot of debt to consolidate, and they can select to have Marcus pay their financial institutions directly.\nYou can change the due date of your regular monthly costs up to three times during the life of the loan. No co-signers are enabled, and it can take three days to receive your loan funds.\nNavient – Education Loan Management And Business …\n99% (with autopay) $3,500$40,000 3 to 6 years Not defined None How do I select the best financial obligation combination loan? It’s essential to get a financial obligation consolidation loan that fits your budget plan and helps you reach your objective of getting rid of debt.\nWhen you look for a financial obligation combination loan, take a look at the APR and ensure the repayment term is comfy.”You’ll desire to reduce the rate of interest on your financial obligation as much as possible, but don’t fall under the trap of extending out the loan term excessive,” says Greg Mc, Bride-to-be, CFA, primary monetary expert for Bankrate.\nFrequently asked questions about debt combination loans, How do high rate of interest affect my debt? When you pay back a loan, you’re not simply paying back the amount you obtained; you’ll also pay an extra amount each month in the type of interest. If you have a high interest rate, you’ll be charged more on your outstanding balance, so it could take longer for you to pay off your debt.\nIf you have a 5 percent interest rate on that card, roughly $20 of your minimum payment would go towards interest and $80 would go toward your principal in the very first month. If you have an 18 percent interest rate, however, $75 of your payment would go toward interest and just $25 towards the principal in the very first month.\nDebt Relief And Debt Relief Scams – Office Of The Attorney …\nWhat are the threats of a debt consolidation loan? One of the biggest risks of a debt consolidation loan is the prospective to enter into deeper financial obligation. Unless you can rein in the costs that got you into financial obligation in the very first location, a debt consolidation loan will not help you.\nAs soon as you’ve obtained credit, it’s easy to be overwhelmed. You might spend beyond your means, become ill or lose your task, making it hard to keep up with your bills. If you are battling with financial obligation, there are steps you can require to prevent insolvency. who will assist you develop a personalized money-management strategy.\nBelieve carefully prior to sending cash to a credit counseling or repair work program that doesn’t have a workplace in your neighborhood.\nCompare a number of services and get a feel for how they run. The credit therapist need to spend at least 20 to 30 minutes with you in order to get a complete photo of your financial resources. Likewise, understand that even if a company states it is “nonprofit” doesn’t guarantee that its services are totally free or budget friendly.\nSimplify Your Financial Life With Debt Consolidation – Morgan …\nThese programs combine your existing debts into a single loan with a lower rate of interest. You deposit money every month with the credit counseling organization, which uses your deposits to pay your unsecured debts, like your charge card expenses, student loans, and medical costs, according to a payment schedule the therapist develops with you and your lenders.\nFederal law needs that you need to receive credit counseling from a government-approved organization within 6 months before you file for any bankruptcy relief. These business, often called “credit repair work clinics,” frequently charge high costs for doing the very same things customers can do on their own.\nCredit repair work companies must supply you with a copy of “Consumer Credit-File Rights Under State and Federal Law” prior to you sign an agreement. They should also give you a composed agreement that clearly discusses services to be carried out, your responsibilities including all expenses, how long it will require to accomplish outcomes, and any guarantees programs declare they can cut a deal with your creditors to decrease the amount you owe.\nAdditional dangers exist if you are not able to conserve enough cash to satisfy your lenders or are effectively taken legal action against and your creditors garnish your salaries. Sometimes, these programs will need to you transfer money in an unique account set up for the purpose of paying off your debt, as directed by the financial obligation relief company.\nPros And Cons Of Debt Consolidation – Nolo\nRecent modifications to the federal Telemarketing Sales Guideline prohibit companies that sell debt relief services over the phone from charging a cost prior to they settle or minimize your financial obligation. Washington law likewise puts limitations on the charges a for-profit financial obligation relief business can charge: the overall cost for financial obligation adjusting services can’t exceed fifteen percent of the overall debt you note in the contract with the debt relief company.\nOnce you’ve acquired credit, it’s simple to be overwhelmed. You may spend too much, end up being ill or lose your task, making it challenging to keep up with your bills. If you are dealing with financial obligation, there are steps you can require to prevent personal bankruptcy. who will assist you establish an individualized money-management plan.\nDepartment of Justice’s U.S. Trustee Program provides a list of government-approved credit counseling firms on its website. The National Foundation for Consumer Counseling provides a list of member companies online at or call 1-800-388-2227 for a 24-hour automatic message with workplace listings. Believe carefully prior to sending cash to a credit counseling or repair work program that doesn’t have an office in your community.\nCompare a couple of services and get a feel for how they operate. The credit counselor should spend a minimum of 20 to thirty minutes with you in order to get a total photo of your financial resources. Likewise, be mindful that simply since a company says it is “nonprofit” does not guarantee that its services are totally free or affordable.\nMilitary Financial Assistance – Military Onesource\nThese programs integrate your existing financial obligations into a single loan with a lower rates of interest. You deposit cash every month with the credit therapy organization, which uses your deposits to pay your unsecured financial obligations, like your charge card bills, student loans, and medical bills, according to a payment schedule the therapist establishes with you and your creditors.\nPersonal bankruptcy needs to be your last hope for financial healing. Federal law needs that you must receive credit counseling from a government-approved organization within 6 months prior to you apply for any personal bankruptcy relief. Some customers turn to companies which declare they can fix credit issues. These companies, often called “credit repair centers,” typically charge high charges for doing the very same things customers can do on their own.\nCredit repair work organizations need to offer you with a copy of “Consumer Credit-File Rights Under State and Federal Law” before you sign a contract. They should likewise offer you a written agreement that clearly discusses services to be performed, your responsibilities consisting of all expenses, the length of time it will take to attain outcomes, and any assurances programs claim they can cut a deal with your creditors to decrease the quantity you owe.\nExtra risks exist if you are not able to save sufficient cash to satisfy your financial institutions or are successfully sued and your financial institutions garnish your salaries. In some cases, these programs will require to you transfer cash in an unique account established for the purpose of paying off your debt, as directed by the debt relief business.\nNational Debt Relief – Bbb A+ Accredited Business\nRecent changes to the federal Telemarketing Sales Guideline restrict business that offer debt relief services over the phone from charging a fee prior to they settle or lower your financial obligation. Washington law also puts limitations on the charges a for-profit debt relief business can charge: the overall cost for debt adjusting services can’t surpass fifteen percent of the overall financial obligation you list in the contract with the debt relief company.', 'The Effects of a Debt Consolidation Loan on Your Credit Score:\nIf you’re drowning in debt from multiple sources, a debt consolidation loan may seem like a lifeline. This financial tool allows you to combine various debts into a single, more manageable loan with one monthly payment. While it can simplify your financial life, you might be wondering how it will impact your credit score. In this guide, we’ll explore the effects of a debt consolidation loan on your credit score and what you need to consider before consolidating your debts.\nUnderstanding Debt Consolidation:\nBefore delving into its effects on your credit score, let’s grasp the basics of debt consolidation. When you consolidate your debts, you take out a new loan to pay off your existing debts. This leaves you with a single debt to manage, often at a lower interest rate. Debt consolidation can come in various forms, including personal loans, balance transfer credit cards, and home equity loans.\nThe Potential Positive Impact on Your Credit Score :\nReduced Credit Utilization:\nCredit utilization ratio, or the amount of credit you’re using compared to your total credit limit, also plays a vital role in your credit score. Debt consolidation can lower your credit utilization by paying off existing credit card balances, which may positively affect your score.\nImproved Payment History:\nOne of the most significant factors affecting your credit score is your payment history. A debt consolidation loan can help ensure that you make on-time payments consistently, potentially boosting your credit score over time.\nManaging multiple debts can be overwhelming, leading to missed payments or late fees. Debt consolidation simplifies your finances, making it easier to stay on top of your payments.\nThe Potential Negative Impact on Your Credit Score :\nApplying for a debt consolidation loan typically involves a hard credit inquiry, which can temporarily ding your credit score. However, this impact is usually minor and short-lived.\nNew Credit Account:\nOpening a new credit account can slightly reduce the average age of your credit accounts, which may have a minimal negative effect on your credit score.\nRisk of Accumulating More Debt:\nWhile debt consolidation can be a smart move, it can also tempt some individuals to accumulate more debt on their credit cards or other lines of credit. This can negatively impact your credit score if it results in higher credit card balances.\nThe Key to a Positive Outcome:\nTo ensure that a debt consolidation loan has a positive impact on your credit score, follow these essential tips:\n- Make timely payments on your consolidation loan.\n- Avoid accumulating new debt while paying off your consolidated debts.\n- Continue monitoring your credit report for any errors or discrepancies.\nIn conclusion, a debt consolidation loan can have both positive and negative effects on your credit score. However, when managed responsibly, it is more likely to have a beneficial impact in the long run by improving your payment history and reducing credit utilization. As with any financial decision, it’s crucial to weigh the pros and cons and make an informed choice that aligns with your financial goals.']	['<urn:uuid:de9e4d91-d71d-4ae3-bea2-c2213039244d>', '<urn:uuid:7ad7b82c-8b80-4114-a43a-cc22f197a391>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T19:58:41.790432	15	79	2257
98	why catholic church stop opera performances in rome history	During the Baroque era, opera was banned by the Vatican because it was considered immoral, especially the stories being portrayed on stage. Some cardinals who loved opera began writing their own libretti with sacred, biblical or allegorical themes as substitutes.	"['Cecilia Bartoli: Bringing operas back to life\nThe Vatican once banned opera as immoral. Cecilia Bartoli has resurrected some of the lost works. The diva tells Jessica Duchen why\nTuesday 15 November 2005\nFor the launch, in a Baroque church at the edge of the Roman Forum, of Cecilia Bartoli\'s latest CD, Opera Probita (""Banned Opera""), the charismatic Italian mezzo-soprano has been labelled ""la dolce diva"" by her record company. The singer is resplendent in a specially commissioned Vivienne Westwood gown of pewter-hued silk, inspired by the sensuous sculptures of Bernini. As she sings Alessandro Scarlatti\'s ""L\'alta Roma, reina del mondo"" (""Noble Rome, queen of the world""), two massive doors behind the platform swing open into the warm night, revealing a floodlit landscape of pillars more than 2,000 years old. For an hour, accompanied by Les Musiciens du Louvre under the direction of the French conductor Marc Minkowski, Bartoli treats her guests to a programme of arias by Handel, Caldara and Scarlatti.\nOpera Probita is probably Bartoli\'s most original and exciting recording to date. It is, in effect, a multi-layered concept album, mingling the music of Baroque Rome with some deliciously sophisticated ambiguity. It\'s a complex cocktail of forbidden fruit, in which sensual music is masked as sacred, matched in presentation by tongue-in-cheek imagery inspired by Fellini\'s La Dolce Vita. The album has gone straight to No 1 in the US classical charts and is Recording of the Month in Gramophone. Bartoli is bringing its repertoire to London, with a performance at the Barbican on 7 December.\nThe evening in Rome proves unforgettable. Opera is in Bartoli\'s blood. Born in the city, she was virtually raised in the Teatro dell\'Opera, where her parents both sang in the chorus. ""I must have started listening to musicians while I was still in my mother\'s uterus,"" Bartoli declares, gazing out over the Forum. ""I grew up with the great operas of Verdi and Puccini, in which my parents used to sing. But I then had to take a different repertoire, because I had to follow my voice, my instrument.""\nBartoli\'s warm, supple, honeyed voice carried her first to 19th-century Italian bel canto roles that demand precision, focus and beauty, rather than volume of sound. She has always kept Rossini operas at the heart of her work: earlier this year she thrilled audiences at Covent Garden in Il Turco in Italia, a fabulous production by Moshe Leiser and Patrice Caurier that updated the action to the era of the films of Pasolini and Fellini, complete with an authentic Vespa and a well-timed takeaway pizza. ""It was a wonderful experience and the audience enjoyed the production so much that every performance felt like a kind of feast!"" Bartoli exclaims.\nBut while others with a voice as gorgeous as Bartoli\'s might be expected to dive from stage to stage, you\'re more likely to find her in a university library, hunting out rare Baroque gems. Her ambitions extend not forwards to Puccini, but backwards to the ground-breaking Renaissance works of Monteverdi. ""I would love to sing Monteverdi\'s L\'incoronazione di Poppea,"" she says, ""as well as more Handel operatic roles, like Alcina or Agrippina.""\nWhat about every mezzo\'s ultimate operatic prize: Bizet\'s Carmen? ""Interestingly, the Opéra Comique in Paris, where this opera was first staged, is rather small, with a tiny orchestra pit,"" Bartoli says, a gleam in her eye. ""The first production of Carmen must have been created for this kind of atmosphere, with real intimacy. We\'re used to seeing Carmen as a big, spectacular show. So a Carmen back to the source, yes. Back to Bizet! Otherwise, no.""\nBartoli\'s journey into the past began when the conductor Daniel Barenboim suggested she should consider singing Mozart roles such as Dorabella in Così fan tutte and Cherubino in Le nozze di Figaro. ""He opened me to the universe of Mozart and classicism,"" Bartoli recounts. Since then, she\'s also worked extensively with conductors such as Nikolaus Harnoncourt and Christopher Hogwood, accompanied by orchestras of period instruments. She\'s a tremendous enthusiast for ""early music"". ""I think that we singers have an old instrument,"" she explains. While musical instruments changed dramatically over the 19th and 20th centuries, the human voice has remained the human voice. She points to the difference in tone between old-style gut strings and modern metal ones on a violin. ""We didn\'t have that kind of development! The voice still sounds more like gut strings.""\nOpera Probita began with Handel. ""He brought me back to Rome,"" Bartoli enthuses. ""What we see in the Forum today is exactly what Handel saw when he arrived here, aged only about 21. Imagine the effect on him, coming from the north of Germany! I\'m sure he was totally impressed; there\'s a sense of astonishment in his music."" Her mission was initially to explore the music of the young Handel and his contemporaries in Rome, but soon the heat was on. ""We found some incredible music, but also an incredible story: the story of prohibition. This was the era during which opera was banned by the Vatican, because it was considered immoral, especially the stories being portrayed on stage. But some of the cardinals who loved opera began to write their own libretti with sacred, biblical or allegorical themes."" These oratorios therefore became a substitute for opera, with music that is distinctly operatic in nature, ""full of drama, passion and a real sense of theatre"".\nA further prohibition was that women were forbidden to perform on stage. Hence the rise of the ""castrati"", male sopranos whose vocal timbre was famously achieved by a painful operation at puberty. The greatest castrati were revered as performers and many of the astonishingly athletic arias in Bartoli\'s Opera Probita were composed for them. Bartoli is effectively singing arias for male singers portraying female characters. ""Technically, it\'s very demanding,"" she adds. ""You never know if you can manage, as a woman, to sing this music which is so virtuosic. Sometimes you have to sustain a long line to the end of a phrase, but the phrases were extremely long to suit men\'s greater oxygen capacity.""\nWhere does La Dolce Vita come into it? ""When it was released in the Sixties, it was considered immoral and anti-Catholic, so it was a little like what happened at the beginning of the 18th century with theatre and opera. Also, there\'s a big scene with Anita Ekberg having a good time in the Trevi Fountain, and another where she walks through the 17th-century streets of Trastevere with a little cat. These are very Baroque visions! So we have the parallel of prohibition, we have the Baroque vision more in the modern direction but keeping the Baroque element, and Ekberg herself is a Baroque figure - a voluptuous, sensual woman like the sculptures of Bernini.""\nAnd the pictures in Bartoli\'s album, portraying her ecstatic and abandoned against surging Roman fountains, weren\'t snapped in the Fontana di Trevi itself, were they? ""Ah,"" Bartoli jokes, ""if the temperature is still 28 degrees in the afternoon and there\'s no police around..."" The image is striking. Is this the sensual world of Bernini\'s fountain, or a symbolic flood of divine grace? Hear Bartoli sing and they become one and the same.\nCecilia Bartoli performs at the Barbican, London SE1 (020-7638 8891) on 7 December; \'Opera Probita\' is out now on Decca\nArts & Ents blogs\n- 1 Notting Hill Carnival: Woman shares selfie after being ‘punched in face for telling man to stop groping her’\n- 2 Joan Rivers: \'Palestinians deserve to be dead\'\n- 3 Daily Show\'s Jon Stewart destroys Fox News for its Ferguson coverage\n- 4 ALS ice bucket challenge co-founder Corey Griffin drowns, aged 27\n- 5 Botched ice bucket challenge leaves man critically injured after plane drops hundreds of gallons of water\nGreat British Bake Off 2014: Diana Beard quits after falling ill\nNicki Minaj suffers wardrobe malfunction during MTV VMAs performance with Ariana Grande and Jessie J\nHomer Simpson takes the ALS ice bucket challenge because of course he does\nFriends reunion: Jennifer Aniston, Lisa Kudrow and Courteney Cox perform mini sketch on Jimmy Kimmel Live\nGreat British Bake Off embroiled in Baked Alaska \'sabotage\' scandal\nExclusive: We share blame for creating \'jihad generation\', says Muslim strategist\nRobin Williams Emmys tribute led by Billy Crystal criticised for including \'racist\' joke about Muslim woman\nThe Rotherham child abuse scandal is a tale of apologists, misogyny and double standards\nScottish independence TV debate: Pumped-up Alex Salmond bounces back in bruising second round against Alistair Darling\nDo you realise just how foolish the UK looks?\nUkip Douglas Carswell defection: Tory MP jumps ship to join Nigel Farage\n- < Previous\n- Next >']"	['<urn:uuid:c4e1abf7-aeaf-4b4a-b596-f876d7642e4d>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T19:58:41.790432	9	40	1449
99	golf ball distance speed loft relationship explain	The relationship between distance, speed, and loft in golf is complex. Ball speed is created by club speed and impact, where higher club speed equals more potential distance. The launch angle can be increased in two ways: through increased clubhead loft or by hitting the ball on the upswing (positive angle of attack). When the same launch angle is achieved with a lower loft angle through a positive angle of attack, there is greater clubhead-to-ball energy transfer, resulting in more distance. This is because the upswing affects the force vector of the head, which is always perpendicular to the clubhead's loft. Long drive competitors can achieve launch angles of 12-15 degrees with driver heads having just 5-6 degrees of loft by hitting on the upswing.	['What it Tracks\nThe TrackMan numbers are the brain behind it all. All major stakeholders of the game, without exception, use and trust our numbers for two main reasons: precision and reliability. With TrackMan, you will quickly understand why and how they use our data in their daily work. With us, you’ll quickly become an insider of the most revolutionary training tool in the industry. We track the full trajectory of any shot, from 6 foot pitches to 400 yard drives, pinpointing the landing position with an accuracy of less than 1 foot at 100 yards. We also display the shot’s 3D trajectory together with 26 impact and ball flight parameters in real time (data is delivered within 1 second).\nBelow are examples of the key club and ball definitions you can track with TrackMan.\nSmash Factor is ball speed divided by club speed. Smash Factor relates to the amount of energy transferred from the club head to the golf ball. The higher the smash factor the better the energy transfer.\nSpin Rate is the amount of spin on the golf ball immediately after impact. Spin rate has a major influence on the height and distance of a shot. Spin rate is one of the least appreciated numbers, especially in windy conditions.\nLaunch Angle is the angle the ball takes off at relative to the ground. Launch angle is highly correlated to dynamic loft. Launch angle will always be a little less than dynamic loft, but will have a similar value.\nCarry is the distance the ball travels through the air. An important thing to know about carry is that the value is given for a landing area that is the same height as where the ball is hit from. Then the golfer can adjust for uphill and downhill shots on the course.\nBall Speed is the speed of the golf ball immediately after impact. Ball speed is created by club speed and impact. Bad impact such as shots hit on the toe or heel will reduce the potential ball speed.\nClub Speed determines a golfer’s potential distance. Club Speed is the speed the club head is traveling immediately prior to impact. More club speed equals more potential distance.\nDynamic Loft is the amount of loft on the club face at impact. The golfer’s attack angle, how the shaft bends, how the golfer releases the club head, whether the club face is open or closed to the club path, and where the ball makes contact on the club face can all impact the dynamic loft.\nAttack Angle is the direction the club head is moving (up or down) at impact. Shots hit off the ground should have a negative attack angle in order to create “ball first” contact. However, golfers with slower club speeds should be careful not to hit too much down (negative attack angle) with their irons.\nClub Path is the direction the club head is moving (right or left) at impact. Most golfers relate this number to hitting the ball “in-to-out” or “out-to-in”. A positive value means the club is moving to the right of the target at impact (“in-to-out” for a right-handed golfer) and a negative value means it is moving to the left of the target (“out-to-in” for a right-handed golfer).\nFace Angle is the direction the club face is pointed (right or left) at impact. Most golfers refer to this as having an “open” or “closed” club face. A positive value means the club face is pointed to the right of the target at impact (“open” for a right-handed golfer) and a negative value means the club face is pointed to the left of the target (“closed” for a right-handed golfer).\nLast Updated on', 'There are many websites out there that give you stats on optimum trajectory angles for Drives at different club head speeds. Unfortunately, optimum trajectory angles do not correspond directly to optimum loft angles.\nIncreasing launch angle to optimize carry distance can primarily be accomplished in two ways — through an increase in the clubhead’s loft or from hitting the ball on the upswing. Hitting the ball on the upswing is also known as generating a positive angle of attack with the swing, and golfers that strike the ball on the upswing know firsthand how a positive angle of attack maximizes the already tremendous energy generated from extreme swing speeds.\nWhen launch angle is increased by adding loft, the efficiency of the clubhead-to-ball energy transfer is not as great as when (or if) the same launch angle is created through a positive angle of attack. For example, if a golfer would benefit from a higher launch angle as induced by more driver loft, that change should be made because more carry distance can be achieved. But the golfer who can increase their launch angle with a positive angle of attack will always optimize their launch parameters to a greater extent.\nWhen the ball is hit on the upswing, the shot’s launch angle is greater than the driver’s measured loft because the head’s loft is tilted upward by the swing. A golfer with a level or downward angle of attack always needs greater clubhead loft to achieve the same launch angle as a golfer with a positive angle of attack (See Illustration). When the same launch angle is generated with a lower loft angle on the clubhead, there is greater clubhead-to-ball energy transfer, resulting in more distance. This result occurs because the upswing, or positive angle of attack, has a significant effect on a physical phenomenon known as the force vector of the head.\nThe force vector is the direction of movement of the force generated by the clubhead’s mass from the golfer’s swing speed toward the ball and it is always perpendicular to the clubhead’s loft. With a positive angle of attack, the force vector also travels in an upward direction to the ball. When a high launch angle is generated by a positive angle of attack using a lower-lofted driver head, the ball slides up the face less, thus creating less spin and losing less energy at impact because the force vector is perpendicular to that lower loft. Yet, the ball takes off at a higher launch angle and with a greater amount of velocity for any given swing speed because of the upward or positive angle of attack.\nIt is not uncommon for national long drive competitors to generate launch angles of 12 to 15 degrees with driver heads featuring real loft angles of five to six degrees. In these extreme cases, long-drive hitters are accomplishing a high launch angle-to-head loft ratio with changes in their swing, ball position and tee height which allow the driver to contact the ball on the upswing, well after it passes the low point in the swing arc.\nSWING SPEED VS. DISTANCE\nTo obtain actual swing speed, get your own Swing Speed Radar. Below is a chart for approximate swing speeds based on carry distance.']	['<urn:uuid:0b151e34-16fb-42db-a455-007d0f50d504>', '<urn:uuid:9a62173a-dc73-422c-a3be-87f8ed352c2d>']	open-ended	direct	short-search-query	distant-from-document	three-doc	expert	2025-05-12T19:58:41.790432	7	125	1162
100	interested twin pole stars before polaris period time when served this role	Kochab and Pherkad served as twin pole stars from 1500 BC to 500 AD. These stars, sometimes called the Guardians of the Pole, were the closest bright stars to the north celestial pole during that period, though neither was as close to the pole as Polaris is currently.	['Ursa Minor constellation lies in the northern sky. Its name means “the smaller bear,” or “the lesser bear,” in Latin. The Great Bear is represented by the larger neighbor Ursa Major.\nUrsa Minor was first catalogued by the Greek astronomer Ptolemy in the 2nd century. It is the location of the north celestial pole and contains the famous Little Dipper asterism, with Polaris, the North Star, located at the end of the dipper handle.\nThe constellation is believed to have been created by Thales of Miletus, a philosopher and astronomer who lived between 625 and 545 BC and was known as one of the Seven Sages of Greece (early 6th century philosophers known for their wisdom).\nIt is also possible that Thales merely introduced the constellation to the Greeks. He was believed to be descended from a Phoenician family, and Phoenicians frequently used Ursa Minor in navigation because, lying so close to the North Pole, the constellation was an excellent guide to true north. The Greeks sometimes called the constellation the Phoenician.\nBefore it became known as the Little Bear (Μικρὰ Ἄρκτος), Ursa Minor was known as Dog’s Tail, or Cynosura (originally Κυνόσουρα in Greek).\nFACTS, LOCATION & MAPUrsa Minor is the 56th constellation in size, occupying an area of 256 square degrees.\nIt is located in the third quadrant of the northern hemisphere (NQ3) and can be seen at latitudes between +90° and -10°.\nUrsa Minor contains one star with a confirmed planet and has no Messier objects.\nThe brightest star in the constellation is Polaris, the North Star (Alpha Ursae Minoris), with an apparent visual magnitude of 1.97.\nThere is one meteor shower associated with the constellation: the Ursids.\nUrsa Minor is usually associated with two different myths. In one, the constellation represents Ida, the nymph who took care of Zeus on the island of Crete when he was small, along with Adrasteia, the nymph represented by the larger constellation Ursa Major. Zeus’ mother Rhea hid Zeus on the island when he was very young to protect him from his father Cronus. Cronus, fearful of an old prophecy that said that one of his children would overthrow him, swallowed five of his children after they were born. When Zeus was born, Rhea tricked Cronus into swallowing a stone instead, and Zeus eventually fulfilled the prophecy. He freed his brothers Poseidon and Hades and sisters Hera, Hestia and Demeter, and became the supreme god of the Olympians.\nIn a different myth, the constellation represents Arcas, son of Zeus and the nymph Callisto. Callisto had sworn a vow of chastity to Artemis, but was later unable to resist Zeus’ advances and the two had a child, Arcas. When Zeus’ wife Hera found out about the betrayal and the child, she turned the nymph into a bear. Callisto spent the next 15 years wandering in the woods and avoiding hunters. One day, she came face to face with her son. Scared, Arcas drew a spear, ready to kill the bear. Luckily, Zeus saw the scene and intervened before it was too late. He sent a whirlwind that scooped the mother and son up to the heavens, where Callisto became Ursa Major and Arcas, Ursa Minor. Arcas, however, is more frequently associated with the constellation Boötes, the herdsman. In a slightly different version of the myth, it is the goddess Artemis who turns Callisto into a bear for breaking her chastity vow.\nIn an older myth, the seven stars that form the Little Dipper were said to represent the Hesperides, seven daughters of Atlas, who tended to Hera’s orchard (Garden of the Hesperides) where a tree of golden, immortality-giving apples grew.\nThe Little Dipper\nThe stars that form the Little Dipper asterism are Polaris (Alpha Ursae Minoris), Yildun (Delta Ursae Minoris), Epsilon Ursae Minoris, Anwar al Farkadain (Eta Ursae Minoris), Akhfa al Farkadain (Zeta Ursae Minoris), Pherkad (Gamma Ursae Minoris), and Kochab (Beta Ursae Minoris).MAJOR STARS IN URSA MINOR\nPolaris – North Star – α Ursae Minoris (Alpha Ursae Minoris)\nPolaris, the closest bright star to the north celestial pole since the High Middle Ages, is the brightest star in Ursa Minor. It has an apparent magnitude of 1.985 and belongs to the spectral class F7:Ib-II. The star is approximately 434 light years distant from Earth.\nThe easiest way to find Polaris in the night sky is to follow Dubhe and Merak, the two bright stars at the end of the Big Dipper asterism in Ursa Major, upwards and then look for the nearest bright star.\nAlpha Ursae Minoris is actually a multiple star, consisting of the bright giant Alpha Ursae Minoris A, two smaller and dimmer companion stars Alpha UMi B and Alpha UMi Ab, and two more distant stars, Alpha UMi C and Alpha UMi D.\nThe brightest component in the Alpha UMi system is a bright giant (II) or supergiant (Ib) star belonging to the spectral class F8. It has a mass six times that of the Sun. Alpha UMi B, or Polaris B, discovered by William Herschel in 1780, is a main sequence star of the spectral type F3, and Alpha UMi Ab is a dwarf star in a very close orbit.\nPolaris is classified as a Population I Cepheid variable. The star’s variability was confirmed in 1911 by the Danish astronomer Ejnar Hertzsprung.\nWhen Ptolemy observed Polaris, it was a third magnitude star, but today it is 2.5 brighter, having changed to its current second magnitude in the meantime.Because of its brightness and proximity to the pole, Polaris is an important star in celestial navigation and has been known by many different names, including Stella Maris (sea star), Alruccabah, Phoenice, Lodestar (guiding star, derived from the Old Norse leiðarstjarna) , Cynosūra (from the Greek κυνόσουρα, meaning “the dog’s tail”), Angel Stern, Star of Arcady, Yilduz, Mismar (needle or nail), Tramontana, Navigatoria and Pole Star.\nKochab – β Ursae Minoris (Beta Ursae Minoris)\nBeta Ursae Minoris is a giant star belonging to the spectral type K4 III. It has a visual magnitude of 2.08 and is 130.9 light years distant from the solar system. It is the brightest star in the bowl of the Little Dipper.\nKochab and Pherkad, Gamma Ursae Minoris, are sometimes called the Guardians of the Pole because they appear to be rotating around Polaris. From 1500 BC to 500 AD, the two stars served as twin pole stars, being the closest bright stars to the north celestial pole. Neither of the two, however, was as close to the pole as Polaris currently is.\nKochab is 130 times more luminous than the Sun and has about 2.2 solar masses.\nThe star’s traditional name comes from the Arabic al-kawkab, which means “the star” and is short for al-kawkab al-šamāliyy, or “the north star.”\nPherkad – γ Ursae Minoris (Gamma Ursae Minoris)\nGamma Ursae Minoris is an A-type star with an apparent magnitude of 3.05, approximately 487 light years distant. It has the stellar classification A3 lab, which means that it is an intermediate luminosity supergiant. It is a very fast rotating star, with a rotational velocity estimated at 180 kilometres per second. Its radius is 15 times solar and it is 1,100 times more luminous than the Sun.\nGamma Ursae Minoris is classified as a shell star, one that has a disk of gas surrounding its equator which causes variations in the star’s magnitude.\nThe star’s traditional name, Pherkad, is derived from the Arabic farqad, meaning “calf,” short for the phrase aḫfa al farkadayn, which means “the dim one of the two calves.”\nYildun – δ Ursae Minoris (Delta Ursae Minoris)\nDelta Ursae Minoris is a white main sequence dwarf of the spectral type A1V, approximately 183 light years from Earth. It has a visual magnitude of 4.35.\nThe star’s traditional name, Yildun, comes from the Turkish word for “star,” yıldız. The name is sometimes also spelled Vildiur, Jildun, Yilduz and Gildun.\nAkhfa al Farkadain – ζ Ursae Minoris (Zeta Ursae Minoris)\nZeta Ursae Minoris is a main sequence dwarf belonging to the spectral type A3Vn. It is in fact on the verge of becoming a giant star, with 3.4 times the mass of the Sun, 200 times the luminosity, and a surface temperature of 8,700 kelvins. Zeta Ursae Minoris is also classified as a suspected Delta Scuti variable.\nThe star’s traditional name comes from the Arabic aḫfa al-farqadayn, which means “the dimmer of the two calves.”\nZeta Ursae Minoris has a visual magnitude of 4.32 and is 380 light years distant.\nAnwar al Farkadain – η Ursae Minoris (Eta Ursae Minoris)\nEta Ursae Minoris is a yellow-white main sequence dwarf star belonging to the spectral class F5 V. It is 97.3 light years distant from Earth and has a visual magnitude of 4.95. It is visible to the naked eye.\nThe star’s traditional name, Anwar al Farkadain, comes from the Arabic phrase ’anwar al-farqadayn, which means “the brighter of the two calves.”\nε Ursae Minoris (Epsilon Ursae Minoris)\nEpsilon Ursae Minoris is a triple star system consisting of Epsilon Ursae Minoris A, a yellow G-type giant which is also classified as an eclipsing spectroscopic binary star, and another component, the 11th magnitude star Epsilon Ursae Minoris B, which is orbiting the primary binary star system from a distance of 77 arc seconds.\nEpsilon Ursae Minoris A is also classified as an RS Canum Venaticorum type variable star. The luminosity of the binary system changes as a result of the components eclipsing each other, and the overall brightness varies from magnitude 4.19 to 4.23 with a period of 39.48 days.\nEpsilon Ursae Minoris is approximately 347 light years distant from Earth.\nDEEP SKY OBJECTS IN URSA MINOR\nUrsa Minor Dwarf (PGC 54074, UGC 9749)\nThe Ursa Minor Dwarf is a dwarf elliptical galaxy in Ursa Minor. It has an apparent magnitude of 11.9 and is approximately 200,000 light years distant. It is a satellite galaxy to the Milky Way.\nMost stars in the Ursa Minor Dwarf are old and there is little to no star forming activity going on in the galaxy.\nThe Ursa Minor Dwarf galaxy was first discovered by A.G. Wilson at the Lowell Observatory in 1954. Information provided by the Hubble Space Telescope in 1999 confirmed that the galaxy had a two billion years long period of star formation about 11 billion years ago and a straight forward evolution since.']	['<urn:uuid:3d223e14-2b23-4c21-a887-c0666d6cb404>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T19:58:41.790432	12	48	1727
