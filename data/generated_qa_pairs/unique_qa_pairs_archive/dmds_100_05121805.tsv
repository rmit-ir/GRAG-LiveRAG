qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	techniques render many elements display screen fast	There are several techniques to efficiently render many elements on screen. One approach is using GPU particle systems with stack-based allocation, where active particles are kept in a tightly-packed linear array, allowing for drawing only active vertices. Additionally, coalescing allocations per frame can significantly reduce expensive TexSubImage2D calls. For other visual elements like notes in MIDI visualization, instanced drawing can be used where a single rectangle geometry is copied to various positions and scales, requiring only four vertices and six indices to be uploaded to the GPU. This technique is particularly efficient for displaying multiple similar elements like notes or visual effects.	"[""Some recent profiling has made it very clear to me that a CPU-side particle system just isn't going to work out for large numbers of particles. It's more of a bottleneck than I anticipated, just because of the fact that it has to ship a vertex/index array to the GPU each frame.\nAlthough there's no real conceptual difficulty in a particle system on the GPU, the real trick is getting it to run fast (no surprises there). With a vanilla GPU particle system, if you want to support 100K particles, then you have to draw the vertices for 100K particles each and every frame, barring any geometry shader fanciness. Not cool. Naturally, you can check in the vertex shader to see if the particle is active or not, and if it isn't, then you cull it. This saves pixel processing. Still, that's a lot of vertices burning through the GPU. When I got this far in implementing my GPU particle system, it wasn't doing much better than the CPU one.\nWell, wouldn't it be great if we could draw only the active particle vertices? But that would require having all active particles coalesced in a linear region of the texture. Hmmm...defragmenter, perhaps? Wait a minute...there's nothing stopping us from using stack-based allocation! By that, I mean that we always allocate the next free block, and when we go to free a particle, we swap it with the last one in use. This is a classic memory scheme for maintaining a tightly-packed linear array. And it works perfectly in this situation, because the particles do not care about order! Using this strategy, we can specify the exact size of the active vertex buffer, so that no inactive particle vertices are drawn in the draw call.\nTo break up all this text, here's a somewhat cheesy picture of loads of particles! It'll look a lot better once I apply texture to them.\nAnother hidden benefit of stack-based allocation is that we can now save a LOT of time on particle allocation by coalescing allocations per frame. Without stack-based allocation, you'll have to make several calls to TexSubImage2D on particle creation. TexSubImage2D is expensive, and will be a HUGE bottleneck if called for each particle creation (trust me, I've profiled it). In fact, it will quickly become the bottleneck of the whole system. However, if we know that particles are packed tightly in a stack, we can defer the allocation to a once-per-frame coalesced allocation, uploading whole rows at a time to the data textures. This is actually easy and intuitive to implement, but yields massive savings on TexSubImage2D calls! With a 256x256 particle texture, you can now perform (in theory) 256 allocations at once! A little probability theory tells us that, on average, we'll make 128 times less calls to TexSubImage2D. Another big win for stack allocation!\nOne final detail - when freeing a particle and performing the swap, you need to be able to swap texture data. I couldn't find a way to read a subregion of a texture in OGL, so I opted to shadow the GPU data with a CPU copy. One might declare that doing so defeats the purpose of having a GPU PS, but that's not true - the real saving (at least for my application) is the draw call. Having the vertex/index data already on the GPU is a huge saving. To avoid shadowing, you could use a shader and pixel-sized quad (or scissor test) to copy the data. But that's messier and requires double-buffering. So I opted for the easy way. As always, to back my decision, I profiled, and indeed found that keeping a CPU shadow of the data is virtually free. Even with having to perform Euler integration on both the CPU and the GPU, the cost is nowhere close to the cost of drawing the particles, which at this point should be the bottleneck.\nNow, for something totally different, here's a (questionably) pretty picture of my dumb AI friends and I flying around planet Roldoni. Yes, the terrain looks terrible. But now that I've turned my attention back to planetary surfaces, it won't stay that way for long. I've also ported my sky shader over from the XDX engine, and it's still looking nice.\nPS ~ A final note : you might think that you can save even more speed on particle allocation if you detect the case in which multiple rows can be uploaded at a time (i.e., you have a massive number of particles to upload in one frame), so you coalesce them into a single TexSubImage2D call covering a large rectangle. It turns out that this optimization really doesn't save much time at all, so there's no need to go that far unless you regularly expect to upload hundreds of thousands of particles (I was testing with allocations of ~10,000 at a time, and the saving was not noticeable). Any way you slice it, the vertex cache is going to get hit really hard if you allocate a huge number of new particles at once, and that, if anything, is going to cause a stall - not the uploading."", ""MIDI visualization, a case study\nTable of Contents\nRecently, inspired by my partner starting to practice piano again and showing me this video, I challenged myself to build a small MIDI visualizer. I wanted to be able to display a MIDI track as an animated score, with moving notes, similar to the one in the video. The main development steps were:\n- loading a MIDI file, and parsing the notes contained,\n- displaying a scrolling score with these notes,\n- adding visual effects to embellish the visualization.\nAt the beginning of the project, I decided to implement my own MIDI parser rather than to rely on one of many available libraries. This helped me get a better understanding of the MIDI specification, and turned out to be viable as I only needed to parse notes and a small subset of the many additional information contained in the files.\nMIDI was designed at its core as a communication protocol, to allow digital music instruments to communicate and sync with each other, and integrate into the music recording pipeline (mixing tables, computers,...). Thus, a MIDI sequence is a stream of packets, each representing an event: a change in tempo, connection/deconnection of an instrument, the beginning/end of a note (\nnote on and\nnote off events), and so on.\nMIDI files were designed by simply defining a group of tracks, each containing a MIDI stream of time-stamped packets, along with a few metadata blocks. The specification is not complex per se, but exhibits many types of events and specific cases.\nThe parser I implemented has nothing specific or brilliant to it: the file is parsed as a byte stream, all events are extracted along with the metadata, into a series of C++ structures. The parsing is straightforward, the only gotchas I had to face were the fact that to save space, multiple consecutive events of the same type can be concatenated under certain conditions, and the fact that some instruments denote the end of a note as a\nnote on event with a null velocity.\nI assume for conveniency that we want to display all the notes of the record; all tracks are thus merged into one, and filtered to only keep the notes and tempo events . Then, I pair\nnote on and\nnote off events based on their note and timestamps, to create a\nNote structure containing the note value, its start time and its duration in seconds, computed using the timestamps, beat and current tempo. Notes are stored in an array.\nAll the elements displayed on screen are using the same geometry: a unique rectangle, instanced/copied in various positions and scales for each element to show ; this means only four vertices and six indices are uploaded to the GPU.\nFor each note of the MIDI score, a rectangle is instanciated on screen, aligned with the corresponding key, and whose height is proportional to the note duration. At any given time, notes that have ended or start too far in the future (and are in both case out of screen) are discarded early.\nTo soften their look, the corner of each rectangle are rounded, by discarding the area outside of the super-ellipse defined by the following formula (where x and y are the local coordinates of the current fragment, w and h the size of the note, r the corner radius):\nA base color (user-defined) is used for major keys, and a slightly darker version is used for minor ones. To make the notes stand out more over the other effects, they are given a thin lighter contour.\nThe background is made using a single rectangle covering the screen. All visual features are directly computed in the shader, based on the screen position of each pixel. The lower fourth of the screen is used to represent the keyboard with a series of tests to determine if each pixel is black or white, as being part of a major/minor key, or a separating line. The octave and measure separations are overlaid as 2-pixels-width lines. The measure count is displayed by first computing the current measure based on time, decomposing the number into its digits, and copying the bitmap representation of each of these from a small font atlas (shown below).\nTo improve the look of the visualization, I added effects similar to those shown in the initial video. This was an exercise in re-creation ; I focused on three effects: animated flashes and bursts of particles when notes are played, and a fading residual trail linked to the particles.\nWhile a note is playing, a flashing effect is displayed on the top edge of the keyboard, where the note's rectangle intersects with it. Each key is associated to a rectangle covering this area on screen, and the flashes are activated in real-time based on the current active notes, through a synced data buffer updated each frame. The animation quickly cycles through a few flash sprites ; a gradient is applied to obtain a halo effect around the spark.\nAt the same time as a note flash is displayed, a stream of particles is emitted from the same location. One could choose to run a full real-time physics simulation to animate each particles system. I used a simpler trick: draw many trajectories by hand on separate layers in Photoshop, and extract positions regularly along each trajectory. We make the assumption all particles move in the [-1,1]x[0,1] rectangle. Using a Matlab script, each trajectory is followed, and the successive corresponding 2D positions are converted to colors: the red and green channels hold the x and y coordinates, the blue channel is used to denote that a trajectory is finished. The positions for each trajectory are stacked up ; we obtain a texture where each line corresponds to the position of a particle over time.\nEach rendered particle linearly reads a given line of the texture, attributed at random, and converts the color back to a 2D position. Towards the end of the particle's life, a fading is applied to make it disappear.\nFading trail effect\nFInally, the third effect is designed to enhance the particles visibility and movements. Each particle leaves behind it a colored trail, slowly fading. To achieve this the particles are drawn to a separate framebuffer, over the blurry trails obtained from the previous frame. The result is blurred using a wide radius box blur, and slightly faded out. The result is then used as a background for the main scene, and will be used as an input to the next frame's trails rendering step, as shown below. This way, particles from previous frames are accumulated, giving slowly fading and spreading splats of color.\nThis project was an interesting way to try a few new things: particles using texture for positions, fade-out using accumulation in a separate framebuffer, and obviously MIDI parsing! The finalized executable can load any MIDI file and display it using a user-defined base color. There is no music playing at the same time, as this would be a whole other adventure...\nUpdate: the source is now available on Github.\nmultiple file sub-formats, for instance: one track, multiple tracks, raw recorded stream. ↩\na fact that my initial documentation did not specify... ↩\nas these impact the duration of the following notes. ↩\nOpenGL instanced drawing gives us the ability to display multiple copies of this rectangle with per-instance parameters. ↩\neither on the CPU or the GPU. ↩\na virtual screen that can be drawn onto, and used as a texture later. ↩\nfaster than a gaussian blur of the same radius. ↩""]"	['<urn:uuid:b115985b-6b53-49f1-90c8-6498a31c8967>', '<urn:uuid:8953c668-672a-459f-b288-3e6d7b6fc254>']	open-ended	direct	short-search-query	distant-from-document	three-doc	novice	2025-05-12T18:05:20.075489	7	102	2127
2	I've been tasked with managing information during a new building construction project. What specific types of facility asset data should I be collecting to ensure effective long-term operations?	You should collect warranty documentation, performance specifications, inspection records, permits, critical spare parts information, manufacturer make and model details, year of installation, and expected life cycle data. These attributes are valuable for proactive building asset management through an integrated work management system. It's important to establish a document library early that organizes and tracks information received throughout the construction and commissioning process to identify any documentation gaps at final acceptance.	['Bringing a new facility to life and integrating with an existing asset portfolio, can be a stressful endeavor for the receiving organization expected to operate, manage, and maintain the building with finite resources. Value engineering may have saved considerable construction dollars, but may have also affected life–cycle costs and maintainability. Owners of newly constructed facilities can be faced with a building that underperforms, requires unplanned downtime, additional rework, or provides a lesser occupant or visitor experience.\nFor a new facility to reach its full potential and deliver the maximum value to occupants and owners, it must perform as intended and deliver the expected physical environment for the organization, employees, visitors, and occupants. A predictable opening day that can confidently deliver the intended environment, whether it is focused on production or customer service, is dependent on three primary factors: workforce planning; effective facility transition planning; and transference of relevant facility asset information.\nThe identification of training needs for operating, maintaining, and managing the new facility is essential for preparing the facility operations and maintenance workforce for the transition. Staffing requirements should be determined based on best practices for organizational processes and maintenance procedures, including response times, maintenance levels, reliability goals, preventive maintenance frequencies, and predictive maintenance practices. Staff preparedness requires advanced training for building systems, applicable codes and regulations, building familiarization and wayfinding, scenario planning and testing of emergency procedures, awareness of locations for critical spare parts, and vendor contact information.\nOpening day risks associated with workforce preparation can be limited to chaos finding tools, or can be as impactful as negatively affecting the customer or user experience if issues such as room temperatures, lighting, or access cannot be quickly resolved. A fully staffed response team should be deployed for opening day to quickly respond to any customer or employee initiated requests to adjust facility components for occupant comfort including air quality, noise and potential odors.\nNow that the staff is prepared, is the building prepared?\nFacility Transition Planning to Asset Management\nFor the long–term operational success and viability of a new facility, asset management requires attention and development during the design and construction phase where the focus is predominantly on building the physical assets, driven by the high pressures imposed for schedule and budget compliance. If not planned, the baton pass from construction to occupancy and asset management can be less efficient and less effective.\nTransitioning from a construction zone to an occupied facility requires a series of validations that building systems and equipment are operating as designed and within efficient tolerance levels as specified by the design and commissioning teams. A building monitoring system will provide valuable insight into the overall performance and efficiency of the building, while allowing fine-tuning and adjustments during the system stabilization period. Including engineering and maintenance teams in the performance validations will offer early training and understanding of any potential systems that may require additional monitoring and adjusting to ensure operational efficiency during and after the stabilization and equipment burn-in phase. A smooth baton handoff during the commissioning and stabilization phase will ensure the facility management teams are not stumbling to balance building systems when customers, visitors, or staff are in the building. Ideally, the engineering and maintenance team should be fully trained and capable of operating the facility prior to the relinquishment of the site by the construction and commissioning teams.\nA seamless transition from construction to operations with asset management will reduce startup costs and provide a higher quality occupant and user experience from opening day. A defined plan that addresses organizational needs for asset information and implemented business processes will frame the transition and improve reliability and maintainability. Shifting the focus to long-term maintainability and asset management early in the design and construction phase will enhance cost reductions throughout the life of the facility and ease the transition from construction to asset management.\nEngagement of the operations and support teams, including security, environmental health safety and security, facility management, and engineering and maintenance, throughout the system startup and commissioning phases will have staff better prepared to accept full operations and maintenance responsibility during the transition to a fully operational facility with the ability to support the owning organization in a productive and safe environment.\nTransference of Relevant Facility Asset Information\nThe ability to respond quickly to unplanned events is frequently dependent on accessible, reliable, and accurate information about the building; including as built plans, building systems information, and real time system performance. Additional information, such as warranty documentation, performance specifications, inspection records, permits, critical spare parts, manufacturer make and model, year of installation, and expected life cycle are valuable attributes to proactively manage the building assets through the use of an integrated work management system (IWMS).\nEstablishing a document library early in the process that organizes and tracks information received throughout the construction and commissioning process will help to identify any documentation gaps at final acceptance of the new facility. This process can be streamlined by pre-determining relevant information for the facility and systems that will need to be captured during the design, construction, and commissioning phases. If an enterprise asset management system (EAMS) is being used to manage the building portfolio, then data can be mapped for direct transference from the design and construction software such as building information models, CAD tables, progress photo databases, inspection reports, commissioning data, etc.\n- Operations and Maintenance Best Practices Guide Release 3.0, Chapter 7: Commissioning Existing Buildings']	['<urn:uuid:d578b74d-e235-47af-9f5c-3c4236472193>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T18:05:20.075489	28	70	904
3	food prices stability poor countries solutions	Poor countries face significant risks from food price instability. If food prices double, household spending would increase by over 10% in 37 countries, with five African nations (Benin, Nigeria, Ivory Coast, Senegal, Ghana) facing the highest GDP losses. Solutions include reducing post-harvest losses through improved infrastructure and cold chain management, as demonstrated by WFP's Zero Loss for Zero Hunger program, which focuses on technology investments and training in developing countries to prevent food losses and increase nutrient availability.	"['Food Waste vs. Food Security: A Conversation With Karl Deily From Sealed Air\nHow does food waste relate to food security? We had the chance to speak with Karl Deily, President of Sealed Air Food Care, to discuss the root causes of food loss and how we can develop innovative approaches to achieve Zero Hunger worldwide.\nQ: A new report shows the number of people who go to bed hungry has increased for a third consecutive year. How is food waste reduction a part of the solution?\nA: Believe it or not, we already produce more than enough food to feed the 7.8 billion people living in the world. However, about one-third of the food produced for global consumption is lost or wasted each year. Solving food insecurity is not about producing more food, but rather making the most of the food that we already have. If we could save just one-fourth of the food currently lost or wasted, we could feed the 821 million undernourished people across the globe. Reducing food waste must be a top priority if we are going to help reduce hunger.\nQ: What excites you about the World Food Programme’s (WFP) work to reduce food loss in developing countries?\nA: Projects like WFP’s Zero Loss for Zero Hunger are essential for a sustainable food system. In developing countries, one-third of food produced for human consumption doesn’t even make it to the consumer due to a lack of infrastructure, cold chain management, and education on harvesting techniques. WFP is helping to prevent those losses through investments in technology and training on post-harvest crop management. Preventing post-harvest losses can help increase the availability of vital, life-sustaining nutrients where they are needed most. This inspiring approach helps reduce food loss while increasing global food security at the same time.\nQ: How does our food system need to change to take sustainable action against hunger?\nA: As we know, protecting our food supply is an essential measure in reducing hunger, and everyone has a part to play, from farmer to consumer. As consumers, it is up to us to prevent food waste. In addition, those that care about this issue need to raise awareness among those responsible in the food industry including retailers and food producers.\nAt Sealed Air we take this seriously. As a company supporting many players in the food industry, we have a responsibility to supply sustainable materials and technology that can protect food and reduce food waste and loss along the chain.\nQ: The United Nations recently released a new report saying the number of hungry people worldwide is growing, with climate change partly to blame. How is climate change fueling the rise in malnutrition globally?\nA: Climate change is a major issue related to food security. Weather patterns are becoming increasingly erratic, and extreme weather caused by a changing climate is making it more difficult for farmers around the world to succeed. As these farmers struggle, the people who rely on their harvests also struggle with hunger.\nMany people don’t realize this, but food waste contributes heavily to climate change. The global effect of processing wasted food in landfills equals about 3.3 billion tons of carbon dioxide. So, preventing food waste not only improves global food security, but it can also help us mitigate the devastating impacts of climate change.\nQ: When you compare developing countries to more developed countries, where is food loss or food waste most likely to occur?\nA: Food waste occurs all around the world, but the reasons for that waste differ depending on where you are in the world. In developing nations, 40 percent of food losses occur at post-harvest and processing levels because of a lack of transportation, infrastructure, storage, and refrigeration.\nMeanwhile, nearly 40 percent of food waste in developed nations occurs at the retail and consumer levels. Consumers are increasingly moving away from processed foods to fresh foods that spoil more quickly, and they aren’t necessarily aware that they’re responsible for food waste. In the end, we are all responsible and can take informed action to improve the situation.\nQ: Can you share an example of a company or organization taking a creative approach to food waste and making a real difference?\nA: Kroger’s Zero Hunger Zero Waste initiative aims to end hunger in local communities and eliminate waste within their own operations by 2025. They have committed to accelerating food donations to give 3 billion well-balanced meals, advocating for public policy solutions and establishing a $10 million innovation fund. Just last month, Kroger’s Atlanta division, consisting of Georgia, South Carolina and Eastern Alabama, reported that after only one year, they have already donated more than $6.1 million, provided more than 15.2 million meals to families in need and directed more than 2.8 million pounds of rescued food to donation partners. It’s companies like this that inspire others to take part in the fight against food waste.\nQ: The theme for this year’s World Food Day is “Our Actions Are Our Future.” What actions can we take to ensure a future of ZeroHunger?\nA: Eliminating hunger by 2030 requires immediate and deliberate actions to address the root causes of hunger worldwide, including food waste and poverty. Businesses and consumers alike need to re-imagine the modern food supply chain and their roles in reducing food waste.\nWe need to continue to innovate and develop new technologies and approaches and then educate those who can benefit the most on how to use those advances. By uniting innovative ideas and diverse expertise, we can continue to develop approaches to fight global hunger for a better future.\nKarl Deily is the President of Food Care, Sealed Air Corporation’s food packaging business, and an Executive Officer and Senior Vice President of the company. Since 2006, Sealed Air—the innovators behind Bubble Wrap® brand cushioning and Cryovac® vacuum packaging—has been a proud partner of World Food Program USA and WFP.', 'Climate change is affecting food production, in the context of rising pressure on land and water\nBy Lin Taylor\nLONDON, May 20 (Thomson Reuters Foundation) - Swelling populations and demand for food combined with ever scarcer water and land resources could lead to a doubling of food prices and trigger civil unrest in some developing countries, a new report says.\nDemand for food with a higher environmental impact, such as meat, has surged as emerging countries like China and India grow in size and in wealth, said Martin Halle, policy analyst at Global Footprint Network (GFN).\n""A few things are very clear: the demand for food is going up tremendously because of population growth,"" he told the Thomson Reuters Foundation.\n""[Food production] is becoming more unstable because climate change is affecting production, in the context of growing land and water scarcity. There\'s very little leeway between supply and demand.""\nIn the past, countries were able to meet those demands by growing more food on more land. But this has come at a cost, Halle said, since the planet is now running out of water and arable land.\nThe last time the world saw a severe food crisis was in 2007 and 2008, the report said, when extreme weather events hit major grain producing regions the year earlier, causing spikes in the demand and cost of food.\nThe higher prices led to social and political unrest in North Africa, the Middle East and South East and South Asia.\nThe report published this week by GFN and the United Nations Environment Programme (UNEP) said most of the same countries, namely Morocco, Bangladesh, Tunisia and Indonesia, are again at risk if food prices were to increase in the next few years.\nCLIMATE CHANGE VOLATILITY\nClimate change and extreme weather patterns will further increase volatility in food production, Halle added, meaning food prices will become more unstable in the coming years.\n""The real game-changer comes when you factor in the environmental constraints - climate change, land scarcity and water scarcity, and all of these are linked,"" said Halle.\nDrought is becoming more frequent and severe in places like southern Africa, and that - combined with the recent El Nino phenomenon - is taking a heavy toll on rural lives and economies.\nFor example, maize prices in South Africa, the continent\'s top producer of the staple crop, reached near record highs late 2015, in the face of rolling heat waves and poor rains over key growing areas.\nUsing models from data across 110 countries, the study found that if the cost of food doubled, household spending would increase by more than 10 percent in 37 countries.\nFive African countries - Benin, Nigeria, Ivory Coast, Senegal and Ghana - would be the worst affected in terms of highest percentage loss to GDP.\nThe major emerging economies of China and India are forecast to lose $161 billion and $49 billion in gross domestic product (GDP) respectively with a doubling of food commodity prices.\n""What this provides is a litmus test,"" said Ivo Mulder, economics advisor at UNEP. ""We are overusing what is available for us and we don\'t really know what the magnitude of the risk is.""\nWhile higher income countries, like the United States, could benefit from food price hikes, Mulder said, their high demand for meat-based products is contributing to the problem.\n""It\'s important to be honest about the types of risks that countries face,"" he said. ""Because even if developed countries are less exposed than developing countries, it doesn\'t mean there is no risk at all.""\n(Reporting by Lin Taylor @linnytayls, Editing by Ros Russell; Please credit the Thomson Reuters Foundation, the charitable arm of Thomson Reuters that covers humanitarian news, conflicts, land rights, modern slavery and human trafficking, women\'s rights and climate change. Visit http://news.trust.org to see more stories)\nOur Standards: The Thomson Reuters Trust Principles.']"	['<urn:uuid:e5ea53e7-48c0-4b1c-9e96-0584274158f7>', '<urn:uuid:63ca15df-13fa-483f-9233-ee3a9caebdba>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T18:05:20.075489	6	78	1630
4	sahara conservation fund endangered species work	The Sahara Conservation Fund (SCF), established in 2004, works to combat extinction threats to large bird and mammal fauna in North Africa. The organization specializes in reserve management, humanitarian assistance, and provides regional expertise. SCF is recognized for its technical expertise in conserving and restoring highly threatened species in the Sahelo-Saharan ecosystem. Their flagship project involves the reintroduction of scimitar-horned oryx back into the wild.	"[""Hall Family ZAA Wildlife Conservation Fund\nThrough the Hall Family ZAA Wildlife Conservation Fund together with direct support programs of our accredited members, ZAA directs vital conservation dollars to aid in the survival of many keystone species and a number of other threatened or endangered species on all seven continents. Our NGO partners in this important work are diverse and share our commitment to wildlife conservation, propagation of wildlife, and habitat conservation. On the ground in situ, we see this critical funding making a real difference and producing results that are measurable. For example, ZAA supports projects dealing with human/wildlife and predator/prey conflict avoidance, anti-poaching patrols and snare removal teams, the teaching of conservation agriculture practices, elephant beehive fencing and a number of progressive community outreach and education programs.\nThe Hall family ZAA Wildlife Conservation Fund was created in memory of ZAA member Lynn Hall's wife and two sons. Lynn and his family have contributed significantly for many years to the art of captive animal management and propagation. The Fund may be used for wildlife projects, including wildlife conservation, propagation of wildlife, and habitat improvement.\nAbout Lynn Hall\nLynn Hall was a man with many interests and talents – he was a successful business owner, aviculturist, world traveler and more. His sense of adventure and desire to learn about the world around him led him to wonderful places and to the accomplishment of marvelous things in his life, things that many could only begin to dream about.\nAs far back as he could recall, he had harbored a fascination for the natural world around him - enjoying a love of birds, fishing and hunting. Though having kept birds since the age of five, Lynn’s first foray into breeding came with the acquisition of a pair of White King Pigeons when he was 12. The pigeon family would prove to stay with him as a special focus.\nHe always yearned for those wonderful pigeons and fruit doves he had only read about or seen in travels abroad with his wife Margie. Finally, in 1984 he realized a dream when he traveled to Jakarta, Indonesia, and made arrangements to bring home several species of fruit doves along with a personal favorite of his, the pheasant pigeon. Lynn fondly recalled this as one of the highlights of his life, as he found himself sitting on the floor at an importer’s station, surrounded by not just one, but nine pheasant pigeons! The joy and amazement at the opportunity to finally acquire one of the birds of his dreams had a prominent effect on him. It was then that he started importing birds, and he truly began his efforts to acquire and successfully breed many of the doves and pigeons of Southeast Asia that previously were just pictures in his mind or found only in a book. “You can do anything if you set your heart to it,” Lynn said, and he proved this over the years.\nHis philosophies and practices showed his commitment to breeding for the species themselves, not for any monetary gain. Working closely with both zoos and the private sector, Lynn was generous in sharing his experiences, successes and failures, all for the betterment of captive management.\nUnfettered by the protocols found in a public setting, Lynn, as a private aviculturist, was able to work closely with the birds, making adjustments as logic and observation dictated. “Do your research, study the habitat of the species,” he advised.\nHe was as well known for his success with fennec foxes as he was with the fruit doves. In 1980 he acquired his first fennecs. At the time, fennecs were thought to be delicate and difficult to keep in captivity; they were short-lived, and little breeding success was achieved. Once again, Lynn’s observation skills and dedication to preserving a species led to great success in keeping them in captivity. Working with John Moore, then director of the Albuquerque Zoo, Lynn developed a diet for the fennecs that proved to be the turning point - not just to survive but to breed and thrive in captivity.\nWell over 400 fennec fox kits were born and raised at his facility over the years. These animals have gone to zoos, education programs and other breeding facilities.\nHe actively participated in conservation efforts such as the Mariana Avifauna Conservation program on Saipan. He was awarded the Jean Delacour Avicultural Award at the International Symposium on Breeding Birds in Captivity (ISBBC) in Toronto, Canada, truly a special honor.\nLynn expressed hope that when his three great-grandchildren grow up, they may get to see some of the birds he worked with, and that many of the species will still be around due in some part to his own efforts in conservation.\nLynn and the stories of his many adventures will be greatly missed.\nBeyond NGO support, ZAA members are also heavily involved in reintroduction programs, and rescue and rehabilitation work. Some of the species our members work with include mountain bongo, Attwater’s prairie chicken, Texas horned lizard, Anegada ground iguana, Kemps Ridley sea turtle, and San Joaquin kit fox.\nSadly, modern-day conservation of threatened or endangered species cannot simply be limited to protecting or reintroducing animals in their native ranges. For many, the “wild” is disappearing as humans encroach on their natural habitats. To ensure a species’ long-term survival, managed propagation of wildlife has become an essential part of the conversation and the only insulation against a potential collapse of wild populations.\nZAA's species management program (Animal Management Program, AMP) is coordinated across the family of ZAA accredited members to ensure the greatest genetic variability.\n2019 ZAA Grant Recipients\nSahara Conservation Fund\nThe Sahara Conservation Fund (SCF) was established in 2004 to address the largely-ignored and catastrophic wave of extinction threatening the large bird and mammal fauna in North Africa. SCF focuses on reserve management, humanitarian assistance, and providing regional expertise. SCF, is a leading source of technical expertise in the conservation and restoration of highly threatened species in the Sahelo-Saharan ecosystem. Known for their flagship project, reintroducing scimitar-horned oryx back into the wild.\niNTERNATIONAL RHINO FOUNDATION\nThe International Rhino Foundation (IRF) works to ensure the survival of rhinos through strategic partnerships, targeted protection, and scientifically sound interventions. Programs are operating on-the-ground in all areas of the world where rhinos live in the wild. IRF works with local communities throughout Africa and Asia to protect viable populations for the five remaining rhino species.\nINTERNATIONAL ELEPHANT FOUNDATION\nThe International Elephant Foundation creates a sustainable future for elephants, by generating and effectively investing resources to support elephant conservation, education, research, and management programs worldwide. Through passion, expertise, knowledge, and partnerships they inspire and engage people to ensure a vibrant future with elephants everywhere.\nPanthera's mission is to ensure a future for wild cats and the vast landscapes on which they depend. The Jaguar Corridor Initiative is the most ambitious corridor project in the world, working with 14 countries to connect habitat for jaguars and associated biodiversity. Panthera partners with governments, corporations, and local communities to preserve genetic integrity and the future of wildlife threatened by habitat loss.\nOKAPI CONSERVATION PROJECT\nThe Okapi Conservation Project (OCP) is devoted to protecting okapi and preserving its habitat. The OCP supports the management of the Okapi Wildlife Reserve in the Democratic Republic of the Congo, helping local communities find sustainable ways of living, and provides an extensive refuge for the region's endangered creatures. The reserve contains 101 mammal species, some of the most notable are: okapi, chimpanzee, forest elephant, bongo, and the Schmidt's guenon.\nThe BirdLife International Partnership strives to conserve birds, their habitats and global diversity, working with people towards sustainability in the use of natural resources. In 2017, BirdLife International assisted with the first ever Multi-species Action Plan for African-Eurasian vultures. This ambitious plan outlines steps to conserve all threatened vultures over 128 countries.\nWildlife Alliance is the leader in direct protection to forests and wildlife in the Southeast Asian tropical belt. With the mission to combat deforestation, wildlife extinction, climate change, and poverty by partnering with local communities and governments. One of the most successful programs has achieved zero elephant poaching since 2006, while proving 24/7 ranger patrols across nearly 1.5 million acres of land.\naUSTRALIAN BUSHFIRE EMERGENCY WILDLIFE FUND, ZOOS VICTORIA\nThe 2019, the bushfires in Australia were the most destructive in Australia's history and had devastating impacts on wildlife and the landscape that supports them. Zoos Victoria response teams provided critical care treating everything from burns to smoke inhalation. Providing long-term care and restoring the wild habitat lost to the fires will take years to recover.\nPollinator Partnership's mission is to promote the health of pollinators, critical food, and ecosystems through conservation, education, and research. Programs focus on promoting pollinator health and provide public and private partners with the tools to get involved. Although the Pollinator Partnership has several global initiatives, a majority of its projects are predominantly found in the US, Canada, and Mexico.""]"	['<urn:uuid:9e083849-a34a-4c05-a9c5-6f7622129441>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T18:05:20.075489	6	65	1494
5	ways maximize chances win academic aid	To maximize chances of winning academic aid, you should: apply for multiple scholarships while maintaining high quality applications, have strong grades and test scores, participate in activities and community service. The scholarships can come from five main sources: federal government, state government, individual colleges, local businesses/organizations, and national businesses/organizations. Start early, pay attention to deadlines, and thoroughly complete applications including getting references and transcripts submitted on time.	"['Scholarship & Award Information\nScholarships are a form of financial aid. They are a grant or funds that do not need to be repaid. They differ from other forms of financial aid in that they are based on some type of outstanding ability in one or more areas, such as academics, art, music, athletics, or community service. Scholarships are not necessarily based on financial need, as are all other forms of financial aid. Some, however, do have financial need as a factor.\nScholarships come in many forms and amounts. They can range from as little as $100 to help pay for books, up to tens of thousands of dollars for all four years of college. Most academic scholarships come from one of five sources: 1. Federal government 2. State government 3. Individual colleges 4. Local businesses and organizations 5. National businesses and organizations. The vast majority of scholarships come from the colleges themselves. These may be campus-wide or for a specific major. Montana colleges have several excellent scholarships available for entering freshmen.\nGenerally, the better your grades, the higher your test scores, and the more activities and community service you participate in, the better chance you have of winning a scholarship. However, many scholarships are based on financial need. Often times, there are some scholarships with very narrow restrictions and only a few students would qualify. If in doubt, check with Mrs. Morgan. You have nothing to lose and everything to win when you apply for scholarships.\nScholarships, Criteria, and Deadlines\nBe attentive to the scholarships as they are posted via the counselor’s monthly update, on the bulletin board, and on the district’s website. It is impossible to put every scholarship that colleges, universities, and vocational schools offer out for seniors to apply for. It is your responsibility to be searching out scholarships that fit your needs. The counseling office will pass along scholarships that arrive through the school.\nPlease check the criteria carefully. For example, if the scholarship requires you to attend a Montana school and you are going out-of-state, you will not qualify.\nScholarships have deadlines. Be sure to give yourself enough time to complete applications thoroughly. Do not wait until the last minute to get applications completed, references turned in, or transcripts sent. Many applications must be typed. It takes time and effort to put together a quality application.\nSeniors are strongly encouraged to take the ACT or SAT before December of their senior year. It is best to take both tests by the end of the junior year. Often times, extra-curricular events in the fall will conflict with test dates. If tests are taken during the junior year this can be avoided. All Montana juniors have the opportunity to participate in the ACT during March at no cost. Most colleges will accept results from either test, however it is the responsibility of the student to check with the college to find out which test is required.\nWhen you apply for a scholarship from a college, you must also pay the application fee for admission. Do not expect to receive a scholarship if you have not been admitted to the school. At some colleges the admission’s fee is waived. Be sure to ask someone in the admission’s office if they offer this option.\nApplications for all Montana colleges are available in Mrs. Morgan’s office. Most out-of-state colleges do not furnish Thompson Falls High School with applications for admissions. Students must write, call, or email and request applications from the admissions office. The most efficient way is to apply online. Check the college’s website for information.\nSeveral representatives from colleges and vocational schools around Montana will be visiting our school during the year. Listen to the daily announcements for these scheduled visits.\nUse the following helpful hints as you begin the process of applying for scholarships. The students who are most successful at earning scholarships are students who follow these points:\n1.Remember that applying for scholarships is the responsibility of the student.\n2.Thompson Falls High School is not notified of all scholarships available at a particular college, so students are encouraged to write to the college they hope to attend and ask for information on freshman scholarships.\n3.Don’t procrastinate! Get busy on this part of your college planning early and stay on top of it. Your success will depend on your perseverance, organization, and your ability to meet deadlines.\n4.Be sure to give your teachers, counselor and personal reference adequate time to write recommendations for you. Two weeks is a good rule of thumb. Give them a list of activities and awards that are noteworthy in the recommendation. When given sufficient time, teachers can write a meaningful letter that truly reflects each student’s unique qualities.\nMontana University System Honor Scholarship\nThe Montana University System Honor Scholarship (MUSHS) is a four-year renewable scholarship that waives the tuition and registration fee at the eligible Montana college you decide to attend.\nTo apply for this scholarship you must:\nØ have a cumulative GPA of at least 3.4 GPA\nØ graduate from a state accredited Montana high school\nØ have been enrolled full time and in attendance in an accredited Montana school for at least 3 years prior to graduation, including their senior year\nØ be a U.S. citizen\nØ have taken the ACT or SAT on or before the test dates of December of the senior year.\nØ be on track to complete the MUS rigorous core requirements by high school graduation\nApplying for the MUS Honor Scholarship is a multi-step process. You need to:\nØ take the ACT or SAT by December of the senior year; documented test scores taken by these dates must be included with the MUSHS application\nØ apply for admission and be accepted by one of the eligible colleges in Montana by February of the senior year; a college acceptance letter from one of the eligible campusesmust beincluded with the MUSHS application\nØ complete the MUSHS application\nØ submit the completed packet to Mrs. Morgan by March 15 of the senior year.\nRecipients will be selected based on their class ranking and their ACT/SAT scores. For more information please visit http://www.mus.edu/\nWestern Undergraduate Exchange (WUE)\nWUE is the Western Undergraduate Exchange, a program coordinated by the Western Interstate Commission for Higher Education. Through WUE, students in most western states may enroll in many two-year and four-year college programs at a reduced tuition level: 150 percent of the institution’s regular resident tuition. In all cases, WUE tuition is considerably less than nonresident tuition. Students residing in the following states may be eligible to participate:\nAlaska Colorado Hawaii Idaho\nMontana Nevada New Mexico\nNorth Dakota Oregon South Dakota Utah Washington Wyoming\nFor more information go to:http://wue.wiche.edu/\nThere are many companies which offer scholarship searches and financial aid advice. Some may charge fees for services that are often available free through other companies. The companies that advertise ""free money"" for college, guarantee a scholarship, or say that the information they have is not available elsewhere are the ones to be wary of. Check with Mrs. Morgan before paying any company for services related to scholarships or financial aid.\nDownload a Free Application for Federal Student Aid (FAFSA) athttp://www.fafsa.ed.gov/or call 1-800-4-FED-AID. Submit online or mail in the form as soon as possible AFTER October 1 of the senior year. Keep a copy. Parents save your year-end payroll stubs to show your earnings for the year. You might need them for financial aid eligibility reviews by schools.\nThe Student Assistance Foundation and the Montana Department of Labor and Industry partnered to offer at no cost the Montana Career Information System (MCIS).This powerful tool is the premier source of career related information. Use it to learn more about occupational interests, education options, programs of study, scholarships and financial aid. Go tohttp://www.smartaboutcollege.org/ and under the red heading Get to College you will find a link to MCIS. Click on that link and register to use the MCIS. If you use nothing else, this tool will give you some incredible information.', ""Scholarship Application Process\nBefore you can apply for scholarships you must first put together a list of scholarships you're interested in. Then you must organize them according to application deadline, prioritizing those with the earliest deadlines. The next and final step is to sit down and actually go through the application process for each scholarship. The more scholarships you apply for the greater your chance of receiving a scholarship award. However, don't hurry yourself. The quality of each application you submit is just as important as the number of applications submitted. The tips below will help you maximize the effectiveness of the scholarship application process.\nStart early.A scholarship application requires more information than just a name, citizen status, and birthdate. You'll be required to provide more than just basic contact information. Most scholarship applications require letters of recommendation from teachers or employers, academic transcripts, cover letters, resumes, essays, and multi-page personal statements. And nearly every scholarship has an application submission deadline. To ensure you meet application deadlines, start gathering everything you need, begin brainstorming for personal statements, and request letters of recommendations months ahead of time. Don't begin writing your personal statement or essay the night before the submission deadline. A key to obtaining scholarship awards is starting the application process long before the submission deadline.\nDon't loose focus of the detail.Applying to several scholarships is the best strategy for maximizing your chances of receiving a scholarship award, but be careful. When students are applying to several scholarships, it's not uncommon for them to mix up the details of their scholarship applications. Make sure you know the specific requirements for each scholarship for which you apply. Overlooking critical detail, or neglecting just a few requirements, can disqualify you from a scholarship award. Read the directions for each scholarship carefully. If you have any questions, don't hesitate to reach out to the scholarship provider via email or a phone call. Believe it or not, scholarship providers are eager to answer your questions and assist you.\nBe OriginalImagine that you're asked to review a thousand scholarship applications and to determine which applicant should receive a scholarship award. After reviewing all one thousand scholarship applications, you're about ready to keel over from exhaustion and boredom. All one thousand applications re really good. All the applications included the requisite number of personal statements, letters of recommendation and essays. They all followed the application guidelines perfectly. But they were all pretty much carbon copies of one another. None of the applicants stood out from one another as original or unique.\nWhen preparing your scholarship application, it's important you follow all application rules and submission guidelines. This does not however mean you shouldn't be orginal. Remember, scholarship judges are people just like you and me. The last thing they want to do is review one thousand scholarship applications that are essentially the same. As you prepare your scholarship application, include information and exhibits that will help set you apart as interesting, unique and especially deserving.\nPay attention to presentation.Pay attention to the presentation of your scholarship applications. Sometimes, presentation is just as important as content. If you've written a great essay, and meet all the scholarship requirements, but submit an application that is sloppy, you could jeopordize your chances of receiving the scholarship award. All things being equal, the student who submits a neat and professional looking application is going to have an advantage over other applicants. Even if it's not required, you should type your responses to essay questions. If you must fill out the application itself by hand, use your very best pennmanship. Never submit an application with with smudges, white out or other blemishes.\nHave someone else review your application for errors.Whether you're applying for one or a hundred scholarship awards, it's easy to make mistakes on a scholarship application. However, the likelihood of making errors will increase, if you're applying for several scholarships. Before submitting your application(s) have good friend, maybe your parent, review your application(s) for errors. A second pair of eyes will often spot errors that you missed.\nHave someone review your essay.Just as critical as the application itself, is the application essay. With respect to flow of thought, clarity and grammar, your essay should be flawless. We recommend having your teacher read your essay and provide feedback. If your teacher isn't available, find a school counselor, qualified adult or even a friend who can review your essay for you. A second pair of eyes will be able to identify issues with incohesiveness and grammar that you are not. Ask your reviewer to make sure your essay adequately addresses and proves your thesis. Also, ask them if they like your essay or have any other feedback. Don't forget, the scholarship judges are people just like your teacher, parent and friend. If the person you have review your essay finds it compelling, the judges probably will too.\nFollow all application submission rules.You've written the perfect essay. You've completed the application. Now it's time to submit your application. When submitting your scholarship application, follow submission instructions precisely. Neglecting to do so could cost you your scholarship. Most scholarship committees have very specific application submission guidelines. It's important you follow these guidelines. Some applications can be e-mailed. Others must be submitted via U.S. Mail in a plain white envelope with a stamp. If you're not familiar with the submission guidelines for a scholarship, find out. Call or write the scholarship provider, if necessary. Not following submission guidelines can automatically disqualify you for consideration.\nAlways be professional and courteous when addressing members of scholarship committees. Many people are impressed by professionalism, and even if you're not awarded a particular scholarship, committee members could recommend you to other organizations offering scholarships.""]"	['<urn:uuid:dd276620-1d54-41d1-a933-e9e1ff3d5573>', '<urn:uuid:5a91d2da-6ccc-4f0b-b1b9-9a9e6fd7a411>']	factoid	direct	short-search-query	distant-from-document	three-doc	expert	2025-05-12T18:05:20.075489	6	67	2293
6	online activity increase during covid lockdown percentage change	When workers started video streaming and using online meeting platforms to perform their daily duties during the pandemic, online data use rose nearly 40 per cent in the U.S. from January to March 2020.	['The Canadian Internet Registration Authority (CIRA) has unveiled an environmental sustainability strategy aimed at reducing its operational impact and promoting a greener internet in Canada.\nCIRA is a national not-for-profit which manages .ca domains on behalf of Canadians. The organization also offers registry, Domain Name System (DNS) and cybersecurity services to organizations across Canada and around the world.\nAs SustainableBiz previously reported, when workers started video streaming and using online meeting platforms to perform their daily duties during the pandemic, online data use rose nearly 40 per cent in the U.S. from January to March 2020.\nIn addition to telework, seven per cent of Canadians reported using the Internet more frequently to earn income than they did prior to the pandemic. Furthermore, every online search query emits approximately 1.45 grams of CO2, which adds up to 26 kgs of CO2 per year if 50 searches are performed per day.\nAccording to a 2019 report by The Shift Project, a Paris, France-based carbon transition think tank, internet and IT technology usage create more greenhouse gas emissions than the aviation industry and is predicted to account for eight per cent of emissions by 2025.\nIn the first year of its strategy, CIRA is prioritizing sustainability in its operations, which entails measuring the environmental impact of its global data centres.\nThe initiative is a key element of the company’s five-year plan to attain more sustainable operations, develop a sustainable internet and support Canada in meeting its climate targets.\nCIRA’s environmental sustainability strategy\n“We have seen how the Internet led to fundamental changes in our society during the pandemic; changes that lessened our reliance on fossil fuels, such as fewer cars on the road and less air travel,” said Byron Holland, president and CEO, CIRA, in a release. “The Internet has proven to be a massive positive force for environmental sustainability. Now we must strive to make the Canadian Internet itself green.”\nCIRA plans to reduce its energy, emissions and waste impact on the planet through three guiding pillars that make up its environmental sustainability strategy:\n– green workplace: the company aims to create an environmentally sustainable office where employees can thrive through:\n– its LEED gold-certified office building;\n– waste reduction efforts in the workplace via enhanced waste data, resource efficiency and employee education;\n– encouraging sustainable commuting options for employees.\n– green operations: CIRA strives to enhance energy efficiency and reduce GHG emissions across its operations through:\n– reducing greenhouse gas emissions from data centres through data collection, energy consumption reduction protocols and prioritizing data centre locations with lower grid emissions intensity. CIRA is also part of the Ottawa Green Business Network, a community of businesses which have made commitments to reduce emissions;\n– sustainable procurement as it relates to equipment suppliers;\n– reducing emissions from business travel.\n– green internet: CIRA collaborates with other companies to help create a green and resilient internet by:\n– funding climate action-related internet projects, such as green data centre research;\n– collaborating with suppliers to provide a clear environmental policy and sustainability credentials;\n– reporting on CIRA’s progress towards its sustainability performance.\n“A green internet means efficient operations and a focus on procurement that considers sustainability at its core,” said Jane Fulford, chief operating officer, CIRA, in a release.\n“From the data centres that power our global DNS infrastructure to our global business travel, we’re doing our part to reduce emissions by making the necessary changes in our operations to minimize our environmental impact.”\nCanadian companies curbing the internet’s climate impact\nAs SustainableBiz previously reported, other Canadian companies are also working to curb the environmental impact of rising internet use.\nThe Sustainable IT Pledge is joining Canadian organizations for the first time to cut emissions created by their IT operations.\nParticipants in the program, led by CIO Strategy Council, include Accenture, ATCO, Bell, Canadian Blood Services, Micro Focus, Microsoft Canada, Oracle, CloudOps, Rockport and others.\nThese companies have pledged to address emissions from their IT operations by:\n– optimizing systems to reduce data processing volumes;\n– using Cloud-based services where possible;\n– setting emissions targets for IT operations and publicly disclosing progress;\n– setting sustainability standards for suppliers while supporting Canada’s cleantech industry;\n– adopting sustainable IT practices.']	['<urn:uuid:4cd72d84-dcf3-450a-9916-c64a4eb271de>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T18:05:20.075489	8	34	703
7	How does modern video technology help doctors, and what security risks exist?	A medianet architecture enables remote medical consultations where patients can meet physicians in different cities and share medical images. However, when sharing sensitive medical data, there are security risks if cloud storage providers fail to implement adequate protection measures like encryption and multi-factor authentication, potentially leading to unauthorized data exposure.	['Let’s have a look at medianet first. A medianet is an end-to-end IP architecture that helps to enable pervasive media experiences. The medianet architecture includes a smarter network, smarter endpoints, cloud services, and shared media services. As a result, a user can deliver a more visual, social, personal, and interactive experience to end users – cost-effectively.\nMedianet can transform everyday business processes. Imagine a patient at home meeting with a physician in another city and sharing medical images, or a security employee in a parking lot viewing campus surveillance feeds on a smartphone. A medianet architecture helps IT organizations deliver the best possible user experience, with exceptional efficiency, across a range of use cases.\nComprehensive Video Strategy\nCisco Medianet technologies integrate together to deliver an end-to-end architecture that enables video applications to interact with the underlying infrastructure. This architecture encompasses solutions for businesses and all service provider industries.\nA Cisco Medianet architecture includes a smarter network, smarter endpoints, shared media services, and cloud services. This architecture can empower a user to deliver a more visual, social, personal, and interactive experience to end users. How does a Cisco Medianet empower a user to deliver this experience? It provides a set of features and capabilities that tightly integrate all parts of the architecture into an ecosystem of technologies.\nAs a result, information can be shared between disparate system technologies more transparently. A userr network has the intelligence to distinguish different application types on a more granular level, such as a video conference application or a video streaming application. And it does so quickly and automatically.\nFeatures and Capabilities\nAutoconfiguration• Automates device configuration and registration\n• Simplifies deployment of video endpoints\n• Reduces ongoing operational costs of media applications and endpoints\nMedia Monitoring• Enhances visibility into traffic flows on the network\n• Helps reduce operating costs with faster troubleshooting of video, voice, and data\n• Enables precise assessment of the impact of video, voice, and data on the network\n• Differentiates business-critical applications and determines the importance of a session based on its business value\n• Enables the network to support service assurance and an optimal user experience\n• Extracts actionable application information from the network\nMedia Services Interface\n• Helps reduce costs of deploying and managing video endpoints\n• Allows a user to quickly troubleshoot issues and cut operational costs\n• Can be used to detect network issues while they occur for immediate remediation\nWhy Cisco for Medianet\nA Cisco Medianet architecture facilitates automation and delivers visibility for greater scale and policy management across the network to endpoints anywhere.\nFor businesses of all sizes, Cisco offers a variety of endpoints that work with our media transformation products and core networking technologies. These endpoints can be centrally managed, easily and quickly, for smooth administrative and end-user experiences. The benefits include greater efficiency with faster troubleshooting and cost savings.\nFor service providers, Cisco offers a single end-to-end architecture with exceptional scalability and efficiency. A user can deliver hosted enterprise video deployments — regardless of their complexity as a service, or quickly release superior services to set a userrself apart from the competition.\nCisco Services or a Cisco partner can help a user begin to build a medianet. Get started with a Cisco Medianet Readiness Assessment to better understand a userr current capabilities and what a user need to complete a userr architecture. A user might already have existing Cisco Medianet-enabled endpoints and network technologies.', 'What Is Cloud DLP?\nA Definition of Cloud DLP\nData loss prevention (DLP) is a process for protecting sensitive data at rest, in-transit, and on endpoints to reduce the likelihood of data theft or unauthorized exposure. DLP solutions aim to prevent sensitive data and confidential information from being stored, used, or transferred insecurely.\nCloud DLP solutions specifically protect organizations that have adopted cloud storage by ensuring sensitive data does not make its way into the cloud without first being encrypted and is only sent to authorized cloud applications. Most cloud DLP solutions remove or alter classified or sensitive data before files are shared to the cloud to ensure that the data is protected when in transit and cloud storage.\nBenefits of Cloud DLP\nToday, data is put at risk as organizations move to the cloud and employees work from various locations, accessing corporate files from anywhere and at all hours. Employees also collaborate using the cloud, but they may do so using unapproved services and cloud storage apps – otherwise known as “Shadow IT.” That’s why it’s critical for organizations to protect sensitive data not only on their own networks and devices, but in the cloud as well.\nKey benefits of leading cloud DLP solutions include:\n- Integrate with cloud storage providers to scan servers, identify, and encrypt sensitive data before the file is shared in the cloud\n- Scan data already stored in the cloud and audit it at any time\n- Accurately discover sensitive data in the cloud\n- Continuously audit uploaded files\n- Automatically apply controls (prompt, block, encrypt) to sensitive data in accordance with enterprise policies\n- Instantly alert appropriate administrators and data owners when data is put at risk\n- Maintain the visibility and control needed to comply with privacy and data protection regulations\nChallenges of Cloud Data Protection\nOrganizations that do not implement a cloud DLP solution essentially leave cloud data protection up to their cloud storage providers. Problems can arise, however, when those providers fail to take security measures commensurate with the organizations’ data protection needs – such as not offering cloud encryption, multi-factor authentication, or strict access controls. What’s more, a compromise at a cloud storage provider can lead to a compromise of an organization’s data if the organization hasn’t taken steps to secure it before it was sent to the cloud.\nThere are enough challenges associated with cloud storage providers handling cloud data protection that many organizations choose to be proactive with protecting their data in the cloud by implementing a cloud DLP solution to secure their sensitive and confidential information, rather than placing trust in cloud services providers that their DLP and other security measures are adequate for meeting company security requirements and compliance standards.\nChoosing Cloud DLP Providers\nWhen selecting a cloud DLP solution, organizations should be sure it offers the following key features:\n- Content- and context-aware monitoring and inspection policies\n- Detailed activity logging and reporting\n- Device-level control\n- Auditing, alerting, prompting, blocking, and removing remediation actions\n- Encryption of sensitive data prior to cloud upload\n- API integration with cloud storage providers to extend data security policy enforcement to the cloud\nThe reality is that businesses and their employees need to be able to conduct business in the cloud in order to remain productive. However, cloud adoption can also put data at risk of loss or unauthorized access. This risk has led to the need for cloud DLP solutions, because businesses must be assured their sensitive data is being protected while they benefit from the scalability and efficiency of the cloud. Choosing a cloud DLP solution that is offers exceptional data discovery and visibility in the cloud and delivers the protective controls required for safeguarding cloud data is critical.']	['<urn:uuid:dac9307c-57ff-4901-a6bc-2cd0320b89e4>', '<urn:uuid:70fd4c7b-e3a0-43b4-8153-e2f70cd959a3>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T18:05:20.075489	12	50	1194
8	What makes standardized machine interfaces both efficient and secure?	Standardized machine interfaces like PackML and PackTags enable efficient communication and data exchange between different machines in a production line, simplifying integration of equipment from various vendors. The state model approach allows manufacturers to monitor throughput across entire lines and implement sophisticated process control. However, these interfaces must be properly secured - safety instrumented systems (SIS) require strict access control measures. This includes keeping controller key-switches in RUN mode, maintaining isolation between SIS and basic process control systems, and preventing unauthorized remote access to engineering workstations. The implementation should follow both safety standards like IEC 61508 and cybersecurity standards like IEC 62443.	"[""Modular Approach Streamlines Machine Design\n| By: Kristin Lewotsky, Contributing Editor\nThe ISA88 TR02 state model improves machine integration while Make2Pack defines a standard for developing reusable code blocks.\nFor applications such as packaging and printing, factory floors can feature machine lines hundreds of feet long. In the traditional model, the individual elements—a form-fill-seal machine and a case packer, for example—frequently operate as difficult-to-integrate silos. At the motion layer, machine operation requires equipment-automation programming to cover dozens of activities, some sophisticated and unique, some mundane; yet all require engineering hours to code, test, and validate. End-users, in particular, needed a better way to bring all of this together. Over the past decade, the International Society of Automation standard ISA-88, initially developed for batch processing, has been augmented to encompass packaging challenges. The latest version of the standard now features concepts that apply to packaging and there is a packaging state model and interface model (PackML, PackTags) that simplify integration in modular machine lines, and a section focused on the motion layer (Make2Pack) that opens the way to modular code development and enormous savings in machine design.\nAfter establishing ISA-88 in the late 1980s to create a modular approach for the batch processing industry, the ISA started development of Make2Pack (ISA-88 Part 5) to address equipment and control modules. In the early 2000’s, the Organization for Machine Automation and Control (OMAC), driven in large part by end-users like Procter & Gamble, began work on a standard of its own to address packaging, in particular. In 2008, the two organizations combined efforts to release PackML (ISA TR88.00.02-2008 Machine and Unit States: An Implementation Example of ISA-88). PackML delivered a state model that simplifies integration in modular machine lines (see figure 1).\nA fairly typical packaging line might include a machine that erects a carton, a machine that inserts the product into that carton, another machine that applies the label, one that inserts the cartons into cases, a final machine that loads those cases onto pallets. In a traditional design, each of those machines is a separate element, often from different vendors and with very different software that needs to be integrated. “Each machine in a line has a different procedure that is used for ‘auto/production’ but has the same method to activate that procedure and view the results and control the machine, even though it is very different from the machines around it,” says retired P&G engineer and one-time OMAC committee chair Dave Chappell, now CTO of Complete Manufacturing Automation Associates (Denver, Colorado). Developing the capability to monitor the entire line for process control, or adjust operations over the entire line can be a time-consuming challenge. PackML defines a series of modes (e.g. production, cleanout, etc.) and states (e.g. Stopped,Idle, Starting, Execute, etc.) designed to give a common look and feel to the machine interfaces. PackTags, part of the same ISA-TR88 release, are named data elements used for open architecture, interoperable data exchange in packaging machinery.\nFor manufacturers, remaining competitive means monitoring throughput and metrics like overall equipment effectiveness (OEE) in real time, and that requires data. The state model approach not only simplifies machine-to-machine integration but enables communication from shop floor to top floor. “You can watch the bottles fly by or bags being filled but that’s like trying to drive a car by seeing how fast you go past fence posts versus actually looking at a speedometer,” says Dennis Brandl, Chief Consultant at B&RL Consulting. “You'd like to have quantitative information available across your entire line which means you want to collect information from a disparate group of equipment.” The state model approach provides an easy way to integrate a line and deliver that capablity, which is why end users are increasingly interested in it.\nThe state model approach also simplifies operation at the supervisory control level, giving manufacturers the ability to slow the line down if a problem has cropped up on the back end. It allows them to take control of the entire production line as opposed to each individual machine. It also provides a more sophisticated level of process control. “Just because each machine is running optimally doesn't mean that the entire line is running optimally,” says Brandl. “As you collect information you can use that information to determine where your bottlenecks are and optimize your process.”\nModularity isn’t just about line-level integration, however. Applied to the machine level, it can significantly speed and simplify the design of new equipment and improve maintenance of existing versions.\nThere was a time that operating at 600 parts per minute was fast for a particular packaging machine. These days, the same machine is creeping toward 700. Delivering that level of functionality can involve thousands of lines of code, turning the development of a new platform into a lengthy process. Just as PackML provides the modular reusable code blocks for the PackML start/stop/hold etc., Make2Pack provides the guidelines for reusable automation blocks that seamlessly interact with every start/stop/hold block developed for the PackML model implementation.\nConsider a bagging machine, whose operation can be divided into modes like “producing,” “setup,” etc. (see figure 2). The machine has a series of status lights and warning alarms that correspond to various operations. Each of those operations—turning on green and blue lights and sounding a horn when the machine starts up, for example—must be executed in the control software. At the same type, many of these same operations must be executed in virtually every machine in an OEM’s repertoire. Make2Pack provides guidelines for developing reusable code blocks for such common operations as start, stop, hold, etc., allowing developers to stop squandering time redoing the same task and instead focus on aspects of development the differentiate their product in the marketplace. Because developers no longer write and rewrite the same block in different ways and in different places in the code for each machine, the approach helps eliminate what refers to as spaghetti code.\n“We've developed [reusable code] where one block will include all the basic control for an axis and it doesn't matter whether it's a servo motor, a stepper motor, a frequency drive, whatever,” says John Kowal, Packaging Market Development Manager at B&R Industrial Automation and OMAC board member. “One function block or at most two or three will handle all the motion—we’re vastly simplifying what would take multiple function blocks to create a simple, scalable, easy to reuse code block.”\nThe technique can deliver big savings to machine developers in terms of reduced engineering hours and faster to market, says Chappell. “There are companies that I've worked with that are realizing 80% plus reductions in what it takes to roll out new machines,” he notes.\nFollowing the Make2Pack approach not only speeds development by minimizing time spent on common, repetitive tasks, it provides a modular structure to code organization. The format allows development teams to craft a document that they can show to other engineers on the project or even, eventually, to operators on the floor so that they can understand the various activities and actions of the machine. The standard is as much a design paradigm as a development tool. “The beauty of this approach is that now I can break my handling of the states and modes away from how my equipment modules and control modules are programmed,” says Dan Amundson, Technical Section Head at Procter & Gamble and OMAC board member. “It dramatically simplifies the software.” To see an example of the technique executed in ladder logic, see the sidebar to the left.\nOnce the design is complete, the actual process of building should devolve into an efficient, easily replicated process. That said, most OEMs offer a fair degree of customization. That’s easy to accomplish by tapping the wizards in the engineering department who did the development of the first place. Same thing holds true for maintenance. The problem arises when those wizards aren’t available, whether because of sick days, vacation, retirement, or simply moving to another company. That’s where the structure that Make2Pack adds to the process, as well as the reusable code blocks, convert maintenance into a process rather than a dependence on one specific person. That’s particularly important in the case of multinational firms shipping equipment all around the world.\nCurrently, the standard is still at the level of early adopters. Admittedly, it takes a certain amount of time and effort to adopt, although OMAC is in the process of forming workshops to ease the transition. One of the reasons that some OEMs are reluctant to embrace the standard is because they fear it will limit their ability to innovate. Not so, says Amundson. “There are a lot of mundane things that have to happen on every machine. Every machine that makes products for us has a start and stop button, they all have to have alarms, they all have to handle faults. Things like that are fairly mundane. PackML and Make2Pack allow us to do those mundane things in a repeatable way all the time. Rather than being hung up on how they’re going to program this start and stop, machine builders can focus on [a feature like] a control module for a feed screw that's unique, and make that really be their competitive advantage.”\nThere was a time ISA-88 was new to the batch process industry, as well; today, it is accepted practice. Ultimately, the acceptance of PackML and Make2Pack may just be a question of time. To the end-user, it provides big benefits, and they are going to increasingly demand it. “We get more repeatable solutions that are higher quality, and our experience so far is that it will make it easier for us to support these machines,” says Amundson. “At P&G, most of our business units are including PackML in their specifications and we are systematically expanding it further as it makes more business sense. I think this is the way of the future.”"", 'I have said it before and I will say it again. There are simple steps that must be taken now to make your automation systems more resilient to the inevitable cyberattack.\nAttackers have now breached the next bastion of the safety envelope of a plant environment and influenced the operation of a safety system. It is important to state upfront that in this case the system detected the fault and went to a failsafe state just as it is supposed to do. But it will not be very long until attackers successfully modify the logic in these systems to accomplish their nefarious objectives. When it comes to safety instrumented systems (SIS), the most important part of the cybersecurity puzzle is understanding and securing access to the system, both from a physical and a cyber perspective.\nAsk yourself: Who potentially could gain access to the system? (For good or for evil)\nThe recent attack intended to manipulate the safety system of an unidentified plant, and the attackers leveraged two significant access control weaknesses in the system. These are implementation or design weaknesses, not vulnerabilities in hardware or software components – so don’t expect the vendor to fix these, that is your job and your job alone!\n- The physical key-switch on the SIS controller was left in the PROGRAM mode. I can’t say much more: If you leave the keys in the car someone will steal it. Place controllers in RUN mode as soon as configuration logic is changed, and regularly verify their position via walkdown.\n- The attacker gained remote access to the SIS engineering workstation to deploy the attack tool. This means that the workstation was not only connected to the SIS controller network, but was also able to communicate to the outside world via another network. The SIS environment should be appropriately isolated and operate independently from the basic process control system (BCPS).\nISA Cybersecurity Resources\nISA offers standards-based industrial cybersecurity training, certificate programs, conformity assessment programs, and technical resources. Please visit the following ISA links for more information:\nBy leveraging safety design principles articulated in international safety standards such as IEC 61508/IEC 61511/ISA84, automation engineers can make informed decisions about the appropriate methods to isolate the safety functions from the BCPS functions. They also must ensure that separation exists in all phases of plant design, operation and maintenance. A common engineering system or a SIS engineering workstation that is interconnected to the plant network may violate these fundamental principles.\nThe cybersecurity standards created by ISA99 and now recognized globally as IEC 62443 lay out the process to safely segment and isolate key control system components through methods such as “zones and conduits.” Use defense in depth principles from ICS-CERT and utilize unidirectional gateway devices where required.\nSome vendors will maintain they have proven that their integration of the BCPS and SIS, especially at the engineering workstation, conforms to and is consistent with these safety and cybersecurity standards. I urge you to ask hard questions, such as what if an attacker gains complete control of the engineering environment? How does the system ensure that unauthorized changes to SIS logic cannot be made?']"	['<urn:uuid:6105297f-a4c2-4c59-8cbd-a3041813b555>', '<urn:uuid:95ac443d-f653-40a1-8b54-47fda3f367c2>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T18:05:20.075489	9	102	2175
9	nineteenth century revival styles similarities differences mission revival vs second renaissance revival	Both Mission Revival and Second Renaissance Revival emerged in the late 19th century, but Mission Revival was uniquely Californian and drew from Spanish mission designs, while the Second Renaissance Revival (1890-1920) was based on Italian Renaissance buildings and was characterized by larger scale, greater size, and more elaborate designs with enriched cornices and varied window treatments across floors.	"['The Journal of San Diego History\nSAN DIEGO HISTORICAL SOCIETY QUARTERLY\nSpring 1985, Volume 31, Number 2\nThomas L. Scharf, Editor\nRaymond Starr, Book Review Editor\nCalifornia’s Mission Revival. By Karen J. Weitze. Los Angeles: Hennessey & Ingalls, Inc., 1984. Photographs. Notes. Bibliography. Index. 160 pages. $22.50.\nReviewed by Lucinda Leggett Eddy, graduate student in history at the University of San Diego and former Institute of History award winner (1983).\nDuring the last quarter of the 19th century, a series of occurrences initiated an architectural movement in California known as Mission Revival. Unlike other late Victorian revivals, which borrowed from older European or Classical models, this movement drew inspiration from a form already present on California soil-the early Spanish missions. Initial designs met with success, although architects struggled over the correct definition and usage of Mission Revival. By the turn of the century, however, there emerged a distinct and popular style which not only captured the spirit of a by-gone era, but proved through use of building materials and design features to be a style uniquely suited to the California environment.\nThe author begins her search for the origins of the movement shortly after the mid-19th century when decline and neglect had left most of California’s missions in ruins. About this time, a change in attitudes occurred as Romanticism spread across the continent. Journalists hired by the railroads filled promotional literature with enthusiastic references to the missions. Initially, a “fascination with the concepts of the sublime and the picturesque,” led writers to portray the missions as “noble institutions” or spiritual bastions in the wilderness. Romantic imagery and sentiment filled the guidebooks and periodicals of the day, elevating the missions to legendary status. Such accounts eventually gave way to a movement toward historicism as the public sought a more accurate picture of mission life. A growing historical perspective fostered detailed research into mission records and encouraged early preservation efforts. Finally, the great Southern California land rush of the late 1880’s spurred promoters to use the history and lore of the missions to lure tourists and sell land. Journalist and promoter, Charles Fletcher Lummis, recognized the potential capital worth of the old missions and upon reflection in later years, commented, “Plymouth Rock was a state of mind. So were the California Missions.”\nAgainst this colorful backdrop, the author presents a definitive study of each phase of the Mission Revival period from inception to decline. Considerable attention has been given to the designs for the Stanford University campus and the California Building for the 1893 World’s Fair in Chicago. Both designs received wide acclaim and signaled the acceptance of Mission Revival as a valid architectural style for California.\nWeitze’s book offers a detailed account of the period between 1890 and 1915 during which the movement enjoyed its greatest popularity. She highlights the early years with an analysis of the efforts made by architects to clearly define what should or should not comprise Mission Revival architecture. Out of this debate came some of the most innovative buildings the movement produced. While architects experimented with primarily residential and hotel designs during the 1890’s, by 1900 the style had gained such popularity that schools, libraries, churches, train stations and commercial structures reflected the tremendous impact the movement had on California design.\nIn part, the popular use of this style also resulted from its identification with the Arts and Crafts Movement. Furthermore, the American cement industry showed how compatible Mission Revival designs were with new construction methods, namely reinforced concrete. Weitze clearly demonstrates the important role played by these two progressive trends and concludes her discussion with an insightful interpretation of the broad impact Mission Revival architecture made on the American scene.\nMeticulous and thorough research best characterizes this book. Weitze has made extensive use of primary source material as evidenced in the text and notes. An impressive selection of photographs and drawings documents each phase of the movement and adds illustrative support to the text. Weitze has also been careful to note specific architects who figured prominently throughout the movement and has identified architects for illustrated buildings wherever possible.\nAn occasional over-abundance of quotes and references sometimes obscures the author’s text, weakening the readability in a few places. However, this remains a small criticism of a book that is otherwise a fascinating and informative study to be enjoyed by either the serious student or casual reader.', 'Renaissance Revival FURNITURE.......................Illustrated Architecture Dictionary\nRenaissance / Renaissance Revival Architecture Styles\nTable of contents:\nRenaissance - 15th century\nFirst Renaissance Revival - 1840-1890\nSecond Renaissance Revival - 1890-1920\nCommercial Renaissance Revival\nRenaissance Revival FURNITURE\nRenaissance and Reformation Stained Glass Windows\nRenaissance style - 15th-17th centuries\nThe architectural style developed in early 15th century Italy during the rebirth (rinascimento) of classical art and learning.\nThe Renaissance period in Europe, from the 15th century to about the end of the 17th century, when art, architecture , philosophy and literature had a had a rebirth based on class Greek and Roman models. For example, see Putto.\nRenaissance in Italy: The intellectual movement began in the 14th century with the writings of Dante, Petrarch, and Boccaccio.\nVitruvius\'s Treatise on Architecture, originally written in the time of Augustus, was issued in Rome in Latin, in 1486, and translated into Italian in 1621. This became one of the bibles of Renaissance architecture, and through it, of design.\nInitially characterized by the use of classical orders (e.g., Doric), round arches, and symmetrical composition.\nIt succeeded the Gothic as the style dominant in all of Europe after the mid-16th century, and evolved throughout the Mannerist phase into Baroque.\nFlorence windows: 3 types:\n- Arcade: central column and round arches. Example: Palazzo Medici-Riccardi\n- Architrave: with consoles or cornice; Example, with consoles: Palazzo Medici-Riccardi\n- Order: with columns and entablature\nRenaissance Palaces of Florence\nThe secular nature of the Renaissance - the triumph of Humanism even in the Catholic South - finds a symbol in the villa and the palace, not least the palaces of Florence, The palaces were built in the middle years of the fifteenth century for such princely and mercantile families as the Strozzi, as well as Medici (Medici-Riccardi), the Pitti, and the Pandolfini. They vary in detail but conform to type:\n- Unlike the villas which were set among the fountains and cypresses of the surrounding hills, these palaces arc fundamentally urban.\n- Each fills a city block.\n- Each is built right up to the street frontage, presenting a cliff of masonry to the outer world.\n- Each has an internal courtyard of shaded and colonnaded charm.\n- Each relegates to the ground floor such subordinate things as offices, stables, kitchens and guard rooms.\n- The bottom floor was constructed of rusticated stone to suggest a firm foundation and impenetrable defenses. Higher floors were formed from smooth ashlar blocks, with the joints hardly perceptible, to represent the refinement of the living area. The overall effect emphasizes that the building appears progressively lighter as the eye moves upward.\n- Ground floor rooms often have quite small windows to the street, covered with heavy grilles. The grilles themselves, as in the case of the Palazzo Pitti, were often fine works of art, their metallic quality being a foil to the rusticated stonework.\n- Mullioned windows were popular.\n- Each palace has great suites of state apartments on the first floor - the piano nobile (second story in US) - with coved and painted ceilings. Externally this gives a splendid area of blank wall above each range of windows.\n- Each palace has a crowning cornice; that of the Palazzo Strozzi overhangs the street by more than seven feet, casting a mighty shadow.\n- The façades, while having scale and dignity, were austere.\n- Often the greatest enrichment was the craggy character of the rusticated masonry or, as in the Alberti\'s Palazzo Rucellai, very flat pilasters.\nWhat is mote important than individual façades is the fact that here had been created a new urban type, which was to be found throughout the centuries in the Georgian square, the Pall Mall clubs, the Wall Street bank. The wealthy businessman, now neither a churchman nor a feudal lord, had found his architectural symbol. Moreover, the modern street, the ""corridor"" of stone frontages, had, for better or worse, been invented.\nRenaissance in France: In France, the Renaissance was introduced during the reign of Francis I.\nFrancis I and the French Renaissance\nParis, as the capital of the newly consolidated Kingdom of France and as the center of the brilliant court of Francis I, attained preeminence in art and literature. This resulted in the adoption of one national architectural style which emanated from Paris and the schools in the vicinity; while the valley of the Loire became a highway along which, in response to new social conditions, the famous chateaux of kings and courtiers sprang up and formed models for other parts of the country.\nThis influence was largely augmented by the presence, at the court and in the schools, of such Italian artists as Leonardo da Vinci, Cellini, Serlio, Vignola, Rosso, Primaticcio, and Cortona, and was further spread by Italian craftsmen who, traveling from place to place in the district south of the Loire, there erected many picturesque buildings.\nThe kingly power was gradually becoming absolute, owing largely to the policy of Cardinal Richelieu and his successor, Mazarin,in the reign of Louis XIII (1610-43).\n- A History of Architecture on the Comparative Method, by Sir Banister-Fletcher, New York, 1950\nRenaissance in England: In England, the Renaissance began in the reign of Elizabeth I, during which Inigo Jones introduced the Italian Palladian style into England.\nRenaissance style - 15th-17th centuries - examples:\n- Left illustration above: Banqueting House at Whitehall, London, England (1619)\n- Czech Republic: Sgraffito - Schwarzenberg Palace, Prague, Czech Republic - Sgraffito\n- England: St. Paul\'s Cathedral, London, England\n- France: Place des Vosges, Paris, France\n- France: Fontainebleau Palace, France\n- France: Château de Bourdeilles, France\n- France: Étienne de La Boétie House, Serlat, France\n- Italy: Pitti Palace, Florence, Italy\n- Italy: Palazzo Strozzi, Florence, Italy\n- Italy: Palazzo Medici-Riccardi, Florence, Italy\n- Italy: Palazzo Fenzi, Florence, Italy\n- Italy: Sforza Castle, Milan, Italy\n- Italy: Ceiling - Capitoline Museums, Rome, Italy\n- Italy: Building, Catania, Sicily\n- Netherlands: Oost-Indisch Huis, 4 Oude Hoogstraat, Amsterdam, Netherlands\n- Poland: Wawel Castle, Cracow (Krakow), Poland\n- Poland: 15 Ulica Kanonicza, Cracow (Krakow), Poland\n- Poland: Deanery, Cracow (Krakow), Poland\n- Poland: Cupola - Church of St. Mary (Kosciol Mariacki), Cracow, Poland\n- Spain: Catalonian National Art Museum, Barcelona, Spain\n- Spain: Palau de Generalitat, Barcelona, Spain\nFirst Renaissance Revival style - 1840-1890\nRenaissance Revival - 1840-1890\nBuildings in the Renaissance Revival style show a definite studied formalism. The tightly contained cube is a symmetrical composition of early sixteenth century Italian elements.\nCharacteristics include finely cut ashlar that may be accentuated with rusticated quoins, architrave framed windows, and doors supporting entablatures or pediments.\nEach sash may have several lights or just one.\nA belt or string course may divide the ground or first floor from the upper floors.\nSmaller square windows indicate the top or upper story.\n- Identifying American Architecture, by John J.-G. Blumenson. New York: Norton. 1981, p.\nSecond (Italian) Renaissance Revival 1890-1920\nBeaux Arts substyle\nA later revival of Renaissance-inspired design in American houses occurred from about 1890-1930 and was the purest in its resemblance to the Italian originals. The period benefited from first hand familiarity with original models, improved printing technology for photographic documentation, and perfected masonry veneering techniques after W.W.I\nAt first, this style was relatively rare, found mostly in architect-designed landmark houses. By about 1920, the technique of veneering a single layer of brick or stone onto the outside of wood framed walls had been perfected leading to smaller and less costly Italian Renaissance designs that were popular in suburban neighborhoods.\nOne of the architects who popularized the style was Richard Morris Hunt (1827-1895), the first American to study at the prestigious L\'Ecole des Beaux Arts in Paris. Hunt was one of the architects who designed buildings for the 1893 Columbian Exposition in Chicago where the style received great publicity.\nThe Italian Renaissance style was much less common than the more popular Tudor and Colonial Revival styles of the period. The style declined steadily in popularity through the 1930s, and post-1940 examples are rare.\nIdentifying features of the Italian Renaissance:\n- low-pitched hipped roofs covered with ceramic tiles\n- widely overhanging eaves, often supported by decorative brackets\n- upper-story windows smaller and less elaborate than windows below\n- commonly with arches above doors, first-story windows, or porches\n- symmetrical facade\nSecond Renaissance Revival 1890-1920\nScale and size distinguish the later Revival from the earlier Renaissance Revival.\nThe window trim or surround also usually changes from floor to floor. Additional floors are seen in the small mezzanine or entresol windows.\nArcades and arched openings often are seen in the same building with straight-headed or pedimented openings. Enriched and projecting cornices are supported with large modillions or consoles. The roof often is highlighted with a balustrade.\nIn turning to larger Renaissance buildings for models, architects working in this style opened the door for greater size, textural richness, and variety in form. The style well suited the grandiosity required by a very rich client like Cornelius Vanderbilt, who commissioned The Breakers, in Newport, Rhode Island.\nRenaissance Revival 1895-1920\nLate-nineteenth- century revivals are larger, grander,\nand more elaborate than earlier nineteenth-century style revivals. They tend to be\nstately rather than exciting, ""correct"" rather than daring.\nItalian Renaissance Revival\nThe Italian Renaissance revival, directly inspired by the great Renaissance houses of Italy, was one of the most popular of the Beaux Arts design modes, lasting from the the 1800s until the 1920s.\nWhile the Victorian Italianate was essentially a loose interpretation of Italian architecture, drawn primarily from pattern books, the Italian Renaissance revival took a much more academic approach, with design features often copied directly from actual Renaissance landmarks, Roman, Florentine, and Venetian prototypes - ducal palazzi or county villas - were translated into American ""palaces,"" primarily in cities such as New York and fashionable resorts like Newport, Rhode Island.\nSecond Renaissance Revival examples from Buffalo:\n- Right illustration above: Lafayette High School - French Renaissance Revival.\n- Francis W. Tracy Monument\n- Ellicott Square Building\n- Photo - John Strootman House\n- Harlow C. Curtiss House, 479 Delaware Ave.\n- St Vincent\'s School\n- 70 Niagara Street\n- St Gerard\'s RC Church\n- Erlanger Theater Demolished\n- Mayflower Apartments Second Renaissance Revival\n- Lafayette Hotel - French Renaissance Revival\n- Walbridge Building Second Renaissance Revival\n- S. Douglas Cornell House - French Renaissance Revival\n- Charles W. Goodyear House - French Renaissance Revival\n- Bemis House - Flemish Renaissance Revival\n- YMCA - English Flemish Renaissance Revival\n- Riviera Theatre Italian Renaissance\n- Liberty Bank Building Second Italian Renaissance\n- Thomas J. McKinney House Second Italian Renaissance\n- Fireplace - Thomas J. McKinney House\n- Fireplace - Appleton House/Medaille College President\'s Residence\n- Marble fireplace - Bush/Depew House\nSecond Renaissance Revival other examples:\n- Albright Memorial Library, Scranton, Pa. - French Renaissance Revival\n- Centraal Station, Amsterdam, Netherlands (1899) - Dutch Renaissance Revival\n- Carnegie Hall, New York City - Italian Renaissance Revival\n- Galleria Vittorio Emanuele II, Milan, Italy (1865-1877)\n- Pierpont Morgan Library Complex, New York City - Italian Renaissance Revival\nweb site consulting by ingenious, inc.']"	['<urn:uuid:2f150a60-3a47-48e9-aab1-f9f6309064ea>', '<urn:uuid:86cb884e-8521-426a-bf2a-fda28581556a>']	factoid	with-premise	long-search-query	distant-from-document	comparison	novice	2025-05-12T18:05:20.075489	12	58	2572
10	How many red-cockaded woodpeckers were in South Carolina in 1993?	South Carolina had 681 clusters of red-cockaded woodpeckers in 1993.	['Burning at Brookgreen provides woodpecker habitat\nPresident Donald Trump’s administration, which has pushed to roll back an array of environmental laws, is moving to reduce endangered species protection for a tiny bird that once faced extinction in South Carolina and neighboring states.\nThe red-cockaded woodpecker would no longer be protected under the Endangered Species Act if the U.S. Fish and Wildlife Service proceeds with a plan that asks landowners to voluntarily take care of birds on their property for the next 30 years. The agency is trying to decide whether to limit some protections for red-cockaded woodpeckers or drop protections altogether, according to an agency work plan and a recent letter to property owners.\nWhile red-cockaded woodpeckers have rebounded from the brink of extinction in the past three decades, some experts and environmentalists say populations haven’t grown enough to warrant dropping Endangered Species Act protections. Lawsuits are expected if the Trump Administration seeks to eliminate red-cockaded woodpeckers from all protections under the law.\nThe endangered species law, approved by Congress in 1973 to help protect animals and plants at risk of extinction, often prohibits landowners from using their property in a way that might hurt populations of vulnerable species. Depending on the circumstances, that can include prohibitions on cutting timber or developing land.\n“The red-cockaded woodpecker is going to be one that requires careful evaluation,’’ said Jacob Malcolm, a conservation center director with the environmental group Defenders of Wildlife. “If the species is no longer listed, we have to critically evaluate whether entirely voluntary efforts will be sufficient to maintain the great progress that has been made.’’\nDefenders of Wildlife says the red-cockaded woodpecker is among 25 threatened or endangered species being examined for lesser protections by the federal government. Some species would be dropped from endangered to threatened status, a lesser designation under the law.\nChanging the red-cockaded woodpecker’s status from endangered to threatened is significant because the Trump administration is working on a proposal to loosen protections for newly designated threatened species, said Sierra Weaver, an attorney tracking the issue for the Southern Environmental Law Center. Threatened species would no longer automatically be afforded many of the protections they get today, she said. The proposed rule is expected to be released soon, she said.\nBut another plan discussed this spring by the Fish and Wildlife Service says the agency may go further with red-cockaded woodpeckers, removing them completely from the list of endangered species.\nConsultant Ralph Costa, a biologist who is one of the South’s leading experts on red-cockaded woodpeckers, said a species restoration plan shows the birds aren’t expected to fully recover for at least 50 years.\n“It’s going to be a real big deal,’’ said Costa, a former Fish and Wildlife Service official. “Proposing a delisting, at this point, would be in disagreement with the recovery plan.’’\nEfforts to weaken the Endangered Species Act are among more than 80 laws and regulations the Trump administration has sought to drop or loosen since the president took office in 2017, The New York Times reported recently. Trump has said regulations are hurting business and he has hired agency directors to lift what he says are unneeded restrictions. Environmentalists have said the changes in many cases threaten to hurt human health and wildlife.\nBrian Hires, a spokesman for the Fish and Wildlife Service, had little to say about his agency’s woodpecker plan, which is expected out this year.\n“Since the rule for the red-cockaded woodpecker (Endangered Species Act) action is still being written, it is premature to discuss details of that action, including protections it will have,’’ Hires said in a recent email to The State.\nRed-cockaded woodpeckers are 7-inch-long birds that feast on bugs people typically dislike, including roaches, spiders and ants. The cavities they drill in pine trees often are used by other animals after red-cockaded woodpeckers leave.\nThe little birds once were abundant from New Jersey through the Southeast to Texas and Oklahoma. In the Carolinas and Georgia, they thrived in native long-leaf pine forests. But the loss of long leafs in the South depleted their habitat and caused populations to plummet. Red-cockaded woodpeckers are mostly black and white, but the males have streaks of red on the sides of their heads, which gives them their name.\nThe dwindling populations caused the federal government to declare them endangered in the early 1970s, leading to conflicts between landowners and wildlife agencies. Endangered species status for the woodpeckers limited people’s ability to do what they wanted with their land, enraging property owners and causing them to take measures to keep the birds off the land.\nSome landowners cleared forests before they were ready to harvest timber to avoid the possibility that woodpeckers would start nesting and restrict what could be done with the land. That hurt species populations that already had plummeted dramatically from the lack of habitat — namely the loss of long leaf pine forests since Colonial times.\nIn the 1990s, the government and landowners worked out an agreement that required people to maintain existing populations of woodpeckers, but did not hold them responsible for protecting land where new populations began to nest. When a landowner wanted to use property with the newly nested birds, federal authorities would relocate those woodpeckers to public lands.\nFederal and state biologists drilled cavities in trees to make it easier for the birds to establish nests. In addition, managers of land with red-cockaded woodpeckers began burning pine forests intentionally to keep hardwood and undergrowth from popping up.\nAll of those efforts have helped the species grow in population in the South and in South Carolina. They are now being found in places the birds haven’t been documented in for decades.\nIn an April 10 letter to landowners, Fish and Wildlife Service southern regional director Leopoldo Miranda said the agency wants landowners to say how they would manage and monitor red-cockaded woodpeckers if the species is dropped from listing under the Endangered Species Act. Relocating birds to safe areas, digging artificial cavities in trees and using prescribed fire are among ways landowners can protect the birds, referred to by biologists as “RCWs,’’ the letter said.\n“If we are able to determine that the current level of management commitments for RCWs are reasonably certain to occur for the next 30 or so years, it may be possible that the RCW be considered recovered and suitable for delisting,’’ the letter said, asking property owners to “note how you anticipate managing RCWs for the foreseeable future.’’\nCam Crawford, who heads the S.C. Forestry Association, said agreements with landowners have defused problems and helped woodpeckers. He says voluntary efforts to protect species can work. The Forestry Association represents the state’s $21 billion wood and paper products industry, including private landowners whose property is affected by Endangered Species Act requirements.\n“It seems to have worked well,’’ Crawford said. “We can bring back these types of birds or animals that are facing a threat, and it doesn’t always have to be a hostile regulatory type approach.’’\nOne thing is for certain: the birds are growing in population in the South.\nIn South Carolina, for instance, the state in 1993 had 681 clusters of red-cockaded woodpeckers, or spots occupied by the birds. Today, the state has more than 1,450 clusters of the birds, according to federal data provided by Costa.\nAs an example of the improvement in woodpecker populations, the U.S. Forest Service reported last month that the woodpeckers were breeding in a recreation area of the Sumter National Forest in Edgefield County for the first time in four decades.\n“The current population of South Carolina is expanding into parts of the historic range where they have been absent for many years,’’ according to a Forest Service news release quoting Caroline Causey, a state Department of Natural Resources biologist.. “Management activities in the Lick Fork Lake Recreation Area have created the perfect habitat for turkey and quail and have culminated in suitable habitat for highly selective’’ woodpeckers.\nStill, Causey and Costa told The State they have concerns about plans to lessen protections for red-cockaded woodpeckers. The woodpecker has not reached targeted goals for recovery in all parts of its range, which extends in South Carolina across the coast west to Columbia, Aiken and Edgefield.\nThe species will be recovered once it has reached those numbers in every area of states listed in the federal recovery plan. And challenges remain. Since 1986, the species has lost 32 percent of its potential habitat, according to federal data examined by the Southern Environmental Law Center. Much of the loss has been on private land, the center said in a letter last year to the Fish and Wildlife Service.\n“The science does not back up trying to down-list them,’’ Causey said in an interview.\nDefenders of Wildlife president and chief executive officer Jamie Rappaport Clark, a one-time director of the U.S. Fish and Wildlife Service, urged her former agency to move carefully.\n“At this critical time, we should be doing everything that we can to save imperiled species and their habitat, and fully funding and implementing the Endangered Species Act to defend against extinction,’’ she said in a recent statement. “We call on the Fish and Wildlife Service to make delisting and down-listing decisions based on sound science. The Endangered Species Act is our strongest conservation law, and one of our nation’s most successful laws ever enacted.”']	['<urn:uuid:b8cabfb4-4160-4b62-ae2d-7e5e2a4f18b5>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T18:05:20.075489	10	10	1565
11	employer legal requirements protect workers noise vibration exposure comparison	For both noise and vibration exposure, employers must assess risks, implement control measures, and provide training to employees. For noise, employers must provide hearing protection when exposure exceeds limits. For vibration, employers must measure vibration levels of tools and monitor exposure time. Both require ongoing health surveillance of affected workers.	"['The Ultimate Guide to Managing Hand Arm Vibration in the Workplace\nHand Arm Vibration is the name given to vibration exposure to the hands and arms of workers from power tools and machinery.\nHand Arm Vibration Syndrome is the name given to the injuries caused by vibration exposure, many of which are painful, disabling and irreversible. HAVS, as it’s commonly referred to, affects the blood vessels, nerves and joints and over time can prevent people from being able to carry out certain everyday tasks.\nThe Control of Vibration at Work Regulations 2005 covers Hand-Arm (HAV) and Whole-Body Vibration (WBV) and represents a formalised approach to human vibration risk assessment and control.\nThe regulations are a specific guideline of instructions set out by HSE (Health and Safety Executive), which, when followed, will enable you to manage vibration exposure in the workplace and lead your business to compliance. This guide will review the major points of the regulations, looking at the most common issues that arise as well as lending our industry expertise to different scenarios to try and make your job as easy as possible – but there is no substitute to reading the full regulations.\nHowever, you must be aware that simple compliance with the regulations doesn’t guarantee protection against personal injury claims.\nThe HSE in 2005 introduced Action and Limit values, compulsory health surveillance, weekly averaging and stricter monitoring protocols, this piece of law has more to it than might appear on the surface.\nIf any of your employees operate vibrating tools and equipment then you must assess the vibration exposure risk to your employees. Managing the risk of HAVS is a legal duty under The Control of Vibration at Work Regulations 2005, and neglecting to take action can lead to serious consequences for both the organisation and it’s employees.\nThe most common health effect of HAVS within industry is Vibration White Finger (VWF), a debilitating condition that can often leave pain, tingling, numbness and a whitening of the fingers. As a result, workers are unable to carry out fine tasks and can eventually struggle to perform every day tasks such as buttoning up a shirt.\nThe physical effects of vibration exposure to the hands\nHand Arm Vibration (HAVS) is only ever caused by exposure to vibration and is entirely preventable, but once damage has occurred it is permanent.\nPart of the problem with the term HAVS is that it uses the word syndrome, which immediately makes it sound like an illness. The first thing to understand is that we’re talking about an injury – not a disease.\nHealth surveillance is a must for any employees who are likely be regularly exposed above the Exposure Action Value (100 Points), despite your actions to control the risk.\nIn addition, it’s important that you establish a baseline level with all new employees. This means finding out as soon as possible after employment what the current condition of someone’s health in relation to hand arm vibration is. The HSE break down vibration health surveillance in 5 tiers, do you know all 5?\nDid you know? Tier 3 of the HSE’s guide to Health Surveillance involves a health assessment by a qualified person using a range of health surveillance equipment.\nWe’ve created a Health Surveillance Testing Kit which will provide you with everything you would need to carry out tier 3 of the HSE’s Health Surveillance guide.\n– – – Nearly 2 million people are at risk of HAVS in the UK – – –\n– – – Operator training is a valuable step towards a more responsible HAVS safety culture – – –\n– – – Organisations are at risk of heft fines if there are no control measures in place – – –\nBy law, employers need to measure the levels of vibration that their employees are being exposed to, as well as assessing the risk to their employee’s health from vibration at work. If your workforce operate power tools and machinery and it’s your responsibility to keep them safe, you will benefit greatly from attending our three day training course in managing Hand Arm Vibration.\nIf your employees frequently work with vibrating equipment and/or handle vibrating materials then it’s possible that you have an issue. At this point the most important thing you need to do is to carry out a Hand Arm Vibration Risk Assessment.\nYou will need to:\nThe law requires companies to assess the risk of injury from vibratory tools in the workplace. The two main elements you must monitor to achieve this are vibration exposure level and vibration exposure time.\nVibration Exposure Level is the vibration magnitude level emitted from the the equipment your workers use, and therefore is the level that your workers are exposed to. Read on to learn how to find this number.\nExposure Time is the length of time that workers are being exposed to vibration. Sometimes referred to as “trigger time”, it is not right to assume an eight hour trigger time from an eight hour shift. Read on to learn how to accurately monitor exposure time.\nOnly when you are aware of the two elements above are you able to assess and control the level of vibration exposure to your workers. The vibration regulations implement Action and Limit Values for you to work to. These are:\nYou can learn more about the exposure action and exposure limit values in this short article.\nThere are a number of different methods of finding the vibration levels of the tools that your employee’s use. Manufacturer’s in the EU are required to publish a vibration magnitude level but be wary of taking these as gospel. As you’re probably aware, tool vibration levels differ depending on the application and/or surface on which the tool is being used.\nThe most accurate way to determine the true vibration exposure that your employee’s are subject to is to measure the vibration levels of your tools.\nIf you don’t measure the vibration levels of your own tools, it will be very difficult to defend against any personal injury claims should they go to court. If the vibration levels are not taking directly from the individual tools your employee’s are using in the work processes that they carry out, your data has little use.\nWith the right equipment measurements can be straightforwards, but because of the many variables in the measurement process you must be able to detect if readings are incorrect. This is where formal training can be an invaluable asset to any Health and Safety manager tasked with carrying out Hand Arm Vibration risk assessments.\nCertainly, some organisations office a service called Tool Testing where they can come in to your site for a half day / full day and measure all of your tools being used the in their normal work processes. After which you are provided with a report filled with all of the data you require for your risk assessment.\nOnce again, be wary and do your research. If you’re comparing quotes from consultants and one is priced much lower than the others, try and find out why.\nWhen looking at investing in some vibration monitoring equipment, a vibration meter or vibration dosemeter, there are two distinct different types and it all comes down to the location of the accelerometer (the sensor which measures vibration).\nCastle Vexo Hand Arm Vibration Meter\nVibration Meters with Tool Mounted Accelerometers are the traditional method of measuring tool vibration levels. This type of vibration meter is made up of a hand held instrument where the tri-axial accelerometer resides at the end of a flexible cable, which allows for easy attachments to tools.\nSvantek SV103 Vibration Meter\nVibration Meters with Grip Accelerometers are more recent in the marketplace and have come about due to the advancements in MEMs technology, which has reduced the physical size of the accelerometer. In this type of vibration meter the accelerometer sits around the hand of the operator, and is attached by a cable to a unit which is strapped around the operators upper arm.\nHand held mounts are generally not recommended because measurements that aren’t continuously attached to the tool surface with an equal contact force are unlikely to provide reliable readings.\nThat said, hand held mounts may be used with care taken to assure good and continuous contact with the vibrating surface. However, “There is currently no wrist or glove mounted device which measures vibration suitable for use in a vibration risk assessment.” (HSE “8 Questions about Vibration Exposure Monitoring”, 2017, p. 2).\nAs we’ve already discussed, determine the vibration exposure levels of your tools is only half of your battle. The other key issue is to determine the duration of which your workers are exposed to that vibration.\nMeasuring employee’s exposure time to vibration is an essential part of a vibration risk assessment, but whether you do it manually with a stopwatch or automatically with a vibration exposure timer is a decision you need to make.\nThese are clever devices that provide you with accurate and valuable data to help with your risk assessment, and there are a lot of different options available which are best suited to different types of organisations.\nOne of the UK’s most popular vibration exposure time is the HAVi, a small orange device which sits on the tool and start counting when it senses vibration. It’s important to remember that these devices do not measure vibration, but simply monitor when vibration is present. If you input the vibration magnitude of the tool into the HAVi, it will calculate the number of exposure points being accumulated and warn the operator when they are nearing and exceeding the EAV (100 Points) and ELV (400 Points).\nEach different type of vibration exposure timer has their own strengths and weaknesses, so your decision will be swayed by your budget, the number of tools your workers use and the types of tools your workers use. Read our quick guide to help you choose the right vibration exposure timer for your organisation.\nYou may well have heard of some of the different types of vibration exposure timers, you can view and learn about many of them here.\nOnly with both the Vibration Level and the Timed Exposure can you start to control Vibration Exposure using the HSE’s Exposure Levels.\nJust like with any subject you are new to, there are industry used terms which you may not be aware of. If you’re used to dealing with Noise, Air Sampling, Electromagnetic Fields or any other similar subject you should be at an advantage as some terms have similar uses across these different issues.\nStart learning yourself or brush up your knowledge with our Glossary of Terms explaining the technical terms found in vibration.', ""The Control of Noise at Work Regulations 2005 (Noise Regulations 2005) require employers to prevent or reduce risks to health and safety from exposure to noise at work. The Regulations require you as an employer to:\nAssess the risks to your employees from noise at work;\nAct to reduce the noise exposure that produces those risks;\nProvide your employees with hearing protection if you cannot reduce the noise exposure enough by using other methods;\nMake sure the legal limits on noise exposure are not exceeded;\nProvide your employees with information, instruction and training. Carry out where there is a risk to health\nThe aim of the risk assessment is to help you decide what you need to do to ensure the health and safety of your employees who are exposed to noise. Your risk assessment should:\nIdentify where there may be a risk from noise and who is likely to be affected;\nContain a reliable estimate of your employees' exposures, and compare the exposure with the exposure action values and limit values;\nIdentify what you need to do to comply with the law, e.g. whether noise-control measures or hearing protection are needed, and, if so, where and what type; and\nIdentify any employees who need to be provided with health surveillance and whether any are at risk\nWherever there is noise at work you should be looking for alternative processes, equipment and/or working methods which would make the work quieter or mean people are exposed for shorter times. You should also be keeping up with what is good practice or the standard for noise control within your industry.\nWhere there are reasonably practicable things you can do to reduce risks from noise, that are reasonably practicable, they should be done. However, where noise exposures are below the lower exposure action values, risks are low and so you would only be expected to take actions that are relatively inexpensive and simple to carry out.\nWhere your assessment shows that your employees are likely to be exposed at or above the upper exposure action values, you must put in place a planned programme of noise control\nYou are required to issue hearing protection to employees:\nWhere extra protection is needed above what has been achieved using noise control;\nAs a short-term measure while other methods of controlling noise are being developed.\nProvide your employees with hearing protectors if they ask for it and their noise exposure is between the lower and upper exposure action values;\nProvide your employees with hearing protectors and make sure they use them properly when their noise exposure exceeds the upper exposure action values;\nIdentify hearing protection zones, i.e. areas where the use of hearing protection is compulsory, and mark them with signs if possible;\nProvide your employees with training and information on how to use and care for the hearing protectors;\nensure that the hearing protectors are properly used and maintained.\nThe Noise Regulations require you to take specific action at certain action values. The values are:\nlower exposure action values:\n- daily or weekly exposure of 80 dB;\n- peak sound pressure of 135 dB;\nupper exposure action values:\n- daily or weekly exposure of 85 dB;\n- peak sound pressure of 137 dB\nThere are also levels of noise exposure which must not be exceeded. These are called exposure limit values:\n- daily or weekly exposure of 87 dB;\n- peak sound pressure of 140 dB.\nIt is important that employees understand the risks they may be exposed to. Where they are exposed above the lower exposure action values you should at least tell them:\nThe likely noise exposure and the risk to hearing this noise creates;\nWhat you are doing to control risks and exposures;\nWhere and how people can obtain hearing protection;\nHow to report defects in hearing protection and noise-control equipment\nWhat their duties are under the Noise Regulations 2005;\nWhat they should do to minimise the risk, such as the proper way to use hearing protection, how to look after it, store it and where to use it;\nYour health surveillance systems.\nHealth surveillance for hearing damage usually means:\nRegular hearing checks in controlled conditions;\nTelling employees about the results of their hearing checks;\nKeeping health records;\nEnsuring employees are examined by a doctor where hearing damage is identified.\nHearing loss caused by work is preventable, but once your hearing has gone it will not come back. Some people suffer deafness, ringing in the ears or other ear conditions caused by excessive noise at work. You need to assess, reduce and control the risks from noise at work. For help go to www.anchorhands.co.uk""]"	['<urn:uuid:44c06298-7389-4594-b4ff-26b902928c29>', '<urn:uuid:198d3454-9ef0-4062-8bea-71736355d4cf>']	factoid	direct	long-search-query	similar-to-document	comparison	novice	2025-05-12T18:05:20.075489	9	50	2561
12	How does secondhand smoke affect health and legal rights?	Secondhand smoke poses serious health risks, particularly for children who develop lung problems, breathing issues, and suffer more frequent colds, ear infections, and bronchitis when living in homes with smokers. Pregnant women exposed to secondhand smoke face higher risks of stillbirth. From a legal perspective, forced exposure to passive smoking can be considered a violation of human rights. This was highlighted in the Florea v Romania case, where the European Court suggested that forced exposure to passive smoking in prison conditions violated article 3. The right to health can be addressed through various legal provisions, and there's an argument for considering smoking in front of children as a 'harmful cultural practice' that states should take steps to abolish.	['Does exposure to smoking by other people violate human rights? This is a question that merits serious consideration. One context in which it has been raised is smoking in the presence of children (see the campaign of the Flemish Anti-Cancer League on this subject, with a link to my presentation on the subject). This raises obvious issues with respect to children’s right to health. I have argued that we might even consider smoking in front of children as a ‘harmful cultural practice’ from the perspective of children’s health, obliging states to take steps (for example through awareness raising) towards its abolition.\nThe right to health of course does not figure in the European Convention on Human Rights. Yet health-related issues may be addressed in the context of other provisions, in particular articles 3 and 8. In the recent case of Florea v Romania the European Court suggests that forced exposure to passive smoking violates article 3.\nTo be fair, the Court does not explicitly state this: it concludes that the prison conditions in which the applicant has lived during 3 years violated article 3. These prison conditions included serious overcrowding (including a period of ca. 9 months in a cell with between 110 and 120 detainees for 35 beds). Yet apart from the overcrowding, the main complaint is about smoking by 90% of his fellow detainees. The applicant was confined to his cell 23 hours per day. Moreover, the common areas were not free from smoking either, as smoking was common even in the prison infirmary and in the wards for chronically ill patients. The Court therefore distinguished this case from an earlier case (Aparicio Benito v Spain, nr 36150/03) in which it had not found a violation, as the applicant had an individual cell at his disposal.\nIt is to be regretted that the Court is not clear as to whether serious forced exposure to passive smoking by itself would qualify as a violation of article 3. Obviously, a qualification as ‘inhuman or degrading treatment’ requires a higher threshold than a qualification as a violation of the right to health. Yet there can be no doubt that the right to health has been violated in this case. Passive smoking constitutes a serious health risk, and the applicant had absolutely no possibility to avoid exposure to tobacco smoke. As he was under total control of the state, the state can be held responsible for this exposure. I submit that the Court might have pronounced more clearly on this issue if it had resorted to article 8 instead of article 3, in a similar manner of cases concerning environmental pollution. A right to ‘healthy surroundings’ can arguably be derived from the existing environmental case-law of the ECHR, and could be applied to this situation. That would avoid the awkwardness of bringing a prison cell under the right to protection of the ‘home’. In the light of the fact that smoking in prison violated Romanian law, there can be little doubt as to the finding of a violation of article 8 in this case had this scenario been followed.', 'It is common knowledge by now that cigarette or tobacco smoking is extremely harmful to your health. And yet, many people still find it very hard to stop themselves from lighting up another stick. Tobacco smoking has been around for many decades and it is not just a passing trend. And because the cigarette industry takes in millions of dollars each year, it is unlikely that cigarette will be completely banned from production despite its detrimental effects to people’s physical health and to the environment. Nowadays, the youth begins to smoke at a younger and younger age each generation. The younger you start smoking, the more dangerous the effects will be to your body and health, and the more difficult it will be for you to quit. To protect yourself, the future generations, and the environment from the dangers of cigarette smoking, it would be better to quit now. Of course, quitting is easier said than done but here are five reasons why you should quit smoking to further convince you that it’s better to put the cigarette down for good.\n1. Live longer\nThere is nothing you can regularly do to your body that is as harmful as smoking. In fact, half of all smokers will most likely die of a smoking-related disease. Apart from lung disease, smoking can cause cardiovascular problems and is linked to 15 forms of cancer. In average, male smokers lose 13.2 years of their life while female smokers lose 14.5. No matter how old you are, it’s never too late to quit smoking. In fact, people who quit at the age of 50 cuts their odds of dying during the next 15 years in half. The sooner you quit, the longer your life span will be.\n2. Save money\nYou may not notice it since a single stick of cigarette doesn’t seem to take much out of your wallet, but the total amount you spend on cigarettes each year takes a huge load of money. Moreover, taxes will still most likely go up. This is because many states depend largely on smokers for revenue but the medical costs of smoking can be a huge burden to the state. So increasing the price cause smokers to reduce consumption without cutting the revenue they earn. Once you do quit smoking, you’ll be amazed at how much money you can save.\n3. Think clearly\nSmoking fogs the mind. The longer you smoke, the higher your risk is for developing memory problems and the lesser your reasoning abilities will become. These risks are reduced for those who quit at a younger age. Elderly smokers are also faced with the heightened risk of dementia and cognitive decline compared to lifelong non-smokers. Thus, to keep your head clear and your reasoning ability straight, quit smoking.\n4. Sleep better\nSmokers are four times more likely not to feel well-rested after a night’s sleep. If you’ve been having problems sleeping, it’s probably because of your smoking habit. Unfortunately, nicotine withdrawal, which you’ll experience when you quit smoking, further contributes to sleep disturbances. However, it will only be in the beginning. Once your body gets used to the lack of nicotine, the better your sleep will become.\n5. Protect your family and friends\nWhen you smoke, you’re not only harming yourself, you’re also harming everyone around you. Secondhand smoke can still cause as much damage as firsthand smoke especially if the person lives with the smoker and especially towards children. After all, secondhand smoke still contains all the carcinogens that you’ve inhaled into your lungs. Children who live with smokers will most likely develop lung and breathing problems, and suffer colds, ear infections, and bronchitis more often compared to children who live in smoke-free homes. Moreover, pregnant women exposed to secondhand smoke have a higher risk of stillbirth compared to pregnant women who are not in close contact with smokers.']	['<urn:uuid:6f20383c-d2b3-46e3-b9cd-faf3ed2eb97a>', '<urn:uuid:e2d65537-db02-4ffa-97ee-59493679baf4>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T18:05:20.075489	9	118	1164
13	I'm worried about my elderly mom who's starting to be forgetful. I heard there's a new game that could help - what's that all about?	There is a brain training game app developed by University of Cambridge researchers that could help improve memory in people with early-stage memory difficulties. The game, called 'Game Show', is played on an iPad where players compete to win gold coins. In a study, patients who played the game for eight one-hour sessions over four weeks showed improved episodic memory - the type of memory used for daily activities like remembering where you left your keys or parked your car. The game was specifically designed to be enjoyable and motivating for users, while providing individualized cognitive training.	"['\'Brain training\' app may improve memory\nPublished : 04 Jul 2017, 13:45\nA ""brain training"" game app developed by researchers at the University of Cambridge could help improve the memory of patients in the very earliest stages of dementia, suggests a study.\nIn the study, published in The International Journal of Neuropsychopharmacology, the researchers tested the effects of the game on patients with amnestic mild cognitive impairment (aMCI), which has been described as the transitional stage between healthy ageing and dementia.\nIt is characterised by day-to-day memory difficulties and problems of motivation. At present, there are no approved drug treatments for the cognitive impairments of patients affected by the condition.\nCognitive training has shown some benefits, such as speed of attentional processing, for patients with aMCI, but training packages are typically repetitive and boring, affecting patients\' motivation.\n""Good brain health is as important as good physical health.\nThere\'s increasing evidence that brain training can be beneficial for boosting cognition and brain health, but it needs to be based on sound research and developed with patients,"" said Professor Barbara Sahakian, co-inventor of the game.\n""It also needs to be enjoyable enough to motivate users to keep to their programmes. Our game allowed us to individualise a patient\'s cognitive training programme and make it fun and enjoyable for them to use,"" Sahakian added.\nThe researchers tested the effects of the memory game app, called the ""Game Show"", on cognition and motivation.\nThe researchers randomly assigned a small group of patients with amnestic MCI to either the cognitive training or control group.\nParticipants in the cognitive training group played the memory game for a total of eight one-hour sessions over a four-week period; participants in the control group continued their clinic visits as usual.\nIn the game, which participants played on an iPad, the player takes part in a game show to win gold coins.\nThe results showed that patients who played the game did better in a test of episodic memory.\nEpisodic memory is important for day-to-day activities and is used, for example, when remembering where we left our keys in the house or where we parked our car in a multi-storey car park.\nCompared to the control group, the cognitive training group also retained more complex visual information after training, the study said.\n- Most viewed\n- Sunny Leone is the new mom in B-town!\n- Useful tips for beautiful nails\n- Microcredit programmes turn many women self-employed\n- Muslim Ummah facing harassment for involvement of some people in militancy: PM\n- 2 women surrender as cops cordon off Meherpur ‘militant den’\n- Court orders de-sealing of hotel room where Sunanda died\n- Minneapolis police chief resigns after Australian woman\'s shooting\n- 22 July Deaths\n- 22 July Births\n- Tahsan, Mithila announce their divorce\n- Maryam Mirzakhani, only woman to take math\'s highest award, dies at 40\n- Students clash with cops at Shahbagh demanding exam dates\n- \'50% women face unwanted touching at markets\'\n- Unidentified girl dies at DMCH, 12 days after rescue in senseless state\n- 2,000 girl cyclists say \'No\' to early marriage\n- Case filed against Roopa Ganguly for rape remark\n- Shah Rukh delights himself to traditional thali in Jaipur\n- Bangladeshi gets 11 years’ jail for raping tourist in Singapore']"	['<urn:uuid:623153eb-592f-46b4-997e-500b6d0389b1>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T18:05:20.075489	25	97	547
14	I'm building a scalable game server and need to handle sudden user spikes. What's the strategy Ministry of Games uses to manage this challenge with Elixir?	Ministry of Games tackles the scalability challenge by implementing their backend with distributed Elixir and deploying it to a Kubernetes cluster. This approach helps them handle huge amounts of simultaneous users and manage rapid ramp-ups that can occur when games get featured in stores.	"['This talk discusses the high availability challenge, an important property when building a system that needs to provide the service continuously. Based on my own experience in building an Erlang/Elixir powered system that constantly pushes the fast changing data to a few thousand of connected users, I will provide a high-level overview on how Erlang can help us implement continuously running systems, and back this theory with some real life examples. This is an introductory talk that targets mostly people new to Elixir and Erlang, aiming to explain why and how Elixir/Erlang can be beneficial. The talk is language agnostic with the focus on Erlang virtual machine rather than a particular BEAM oriented language. After the presentation, the audience should have a better insight on how highly available systems can be implemented, and how Elixir/Erlang can help in achieving that goal.\nEvercam connects to thousands of IP cameras, pulling streams and/or jpegs. We first built the project using .NET and at a certain point we decided to rebuild on OSS. The main project was well suited to Ruby but the camera connections could not scale so we rebuilt that part to Node. Right now we\'re just about to embark on re-architecting to make use of Elixir and would be happy to share our experiences and learning with the Elixir community.\n- People considering Elixir or interested in hearing how it compared in a real project to other languages.\nOne of Elixir\'s greatest strength are macros, and everybody knows that. However, macros are just one way to access a powerful and magical land: the compile time™.\nIn the last few months, I\'ve been working on a gettext implementation for Elixir (under the direction of José). This library does most of its work at compile time.\nIn this talk, we\'ll have a look at what crazy things you can do at compile time as well as some examples of taking advantage of compile time in the wild (and in Elixir itself). We\'ll see how moving things from the runtime to the compile time has huge benefits in terms of performance, without sacrificing clarity and expressiveness. We\'ll dive into Gettext\'s source code as the main example of this.\nThis talk will quickly introduce Ecto for unfamiliar audience members. Ecto recently underwent some major changes with improvements from the lessons learnt after it was first created almost two years ago. Eric will talk about some of the design choices made in Ecto and what sets it apart from traditional ORMs.\nElixir developers interested in Ecto\nPhoenix is an Elixir web framework for building productive, reliable applications with the performance to take on the modern computing world. Together, we’ll review what makes Phoenix great and how it uses Elixir to optimize code for performance – without sacrificing programmer productivity. Along the way, we’ll see neat features like live-reload and generators and how Phoenix’s realtime layer takes on the modern web.\nMachine learning is a popular and rapidly growing field these days, but there aren\'t a lot of machine learning tools available in Elixir; some in the programming community have even suggested that BEAM isn\'t suited enough to the necessary number crunching to do serious machine learning work. Not so! In this talk, we\'ll walk through training a home-grown neural network written in Elixir on financial data from the S&P 500, then develop a Phoenix application to use that network to try to predict stock prices.\nThe elixir of life is a mythical potion that supposedly grants the drinker eternal life and eternal youth. This elixir was said to cure all diseases. Alchemists in various ages and cultures sought the means of formulating the elixir. Today, thanks to José we can all use Elixir, and once you have tried, it\'s hard to stop. Why Elixir is so addictive? How is it transforming lives?\nIn this talk we will take a fun peek at the development of Elixir, and remind ourselves how it all started. We’ll join a whistle stop tour through the years from inception, learning how both the language and the community got to where we are today. We’ll take a closer look at some examples of how elixir has transformed the lives of those who have jumped in with both feet. And finally we’ll take a cautious but brave look towards the future, what opportunities might lay in the road ahead.\nStarting with a brief explanation for why Elixir is an excellent choice for embedded systems, we will look at how to building a cool Elixir robot, using [Nerves](http://nerves-project.org). You will learn how to: * get started (often the hardest part) * structure an embedded OTP appplication * customise an embedded linux using Buildroot/Nerves * control your robot using an embedded web server * control your robot with [Slack](https://slack.com) You will leave this presentation with more than just inspiration; you will have an actionable plan to make something fun with embedded Elixir. You will also have a genuine stepping stone towards creating a robust and secure _Internet of Things_ product.\nThe audience should leave with a clear understanding of how to get started with embedded Elixir. It will be a practical, rather than purely inspirational talk.\nThose with some Elixir knowledge, but no experience of using Nerves to embed Elixir.\nPaul is a coder with 20 years experience (only a couple with Elixir, though). Paul has spoken at various conferences and events including RubyConf, The Naked Agilist, and guest lectures at Glasgow Caledonian and Edinburgh Napier Universities. Paul is now CEO of Cultivate Software. He was also part of the team that won the giant banana presented to Jim Freeze at Elixir Conf 2015.\nI would like to present a brief overview of the variety of tools available for visual reasoning about concurrency patterns, ranging from Erlang/OTP (Observer, erlyberly) through Elixir ecosystem (visualixir), to the toolset available for other concurrent platforms like Go (gotrace, streamtools). The second goal is to showcase a proof of concept for a visual tool that extends the feature set of the aforementioned ones, with a strong focus on specific Elixir abstractions (like GenStage + Flow), rather than concurrency primitives.\nSuccessful mobile games have huge amounts of simultaneous users, which requires a scalable server in the backend. Getting featured in a store can also cause a rapid rapid ramp-up, putting even further pressure on the scalability of the backend. In this talk I\'ll present how Ministry of Games is tackling the scalability problem with distributed Elixir. I will cover how our system is implemented in Elixir as well as some practical aspects of deploying the Elixir server to a Kubernetes cluster.\nWe have built backends for three games in Elixir. Going to talk about what we learnedand how we did these. Also share a bit about our journey to using Elixir/Erlang.\nLearn about working production systems. Learn about solutions used to build realtime PvP, singleplayer and a turn based games. How we deploy the servers.\nGame developers, programmers, devops people.\nRoope spent the last 10+ years building services, most of it for mobile games. Currently a Co-Founder and lead server developer at Grand Cru.\nErlang\'s pg2 module provides distributed process groups and is a key technology underlying Phoenix\'s default PubSub adapter, which powers the Channels API that we know and love. Together, we\'ll take a guided tour of pg2\'s capabilities and see how easy it is to add distributed features to your own applications. Along the way, we\'ll dissect pg2\'s internals to see how the surprisingly short Erlang implementation can be accomplished in Elixir, while uncovering other primitives that make distributed Erlang so powerful. It\'s time to learn just how easy it is to tap into everything distributed Elixir has to offer!\nWith Phoenix 1.0 out recently, it\'s time to look ahead! Together, we\'ll outline what\'s next for Phoenix 1.1, then explore some exciting new features that take Phoenix to another level of modern web development.\nEvery new language or framework needs time to prove itself in production, for its early adopter to try, fail, iterate, and document what they have learned. Elixir and Phoenix can leverage the 30 years head-start of the underlying Erlang platform, but for newcomers to the platform it is not always easy to find and apply Erlang best practices. This talk explores some specific security-related aspects of Elixir, Phoenix and the Erlang VM, through practical demonstrations and use-cases. Topics covered include: use of Erlang\'s \'ssl\' module, distributed Erlang, and VM hardening against DoS attacks.\nThe purpose of this talk is to make people familiar with some of the Erlang/Elixir specific security considerations. It is focussed on those things that may surprise people coming to Elixir from other languages, and therefore skims over common attack patterns (XSS, CSRF, SQLI, etc.) and their mitigations.\nAnyone planning to deploy an Elixir application, with or without experience in deployment/security using other languages/platforms.\nBram is a system architect and security advocate at Cisco Systems. His work focusses on massively concurrent back-end systems for IoT/IoE applications, preferably built using Erlang/Elixir, as well as PKI-based security solutions for such environments. He has previously built/designed API servers using Ruby on Rails, as well as VoIP soft switches in Java.\nIt is the story of the birth of a quadcopter built using a 3D printer, a Raspberry Pi 3 and some Elixir magic. I will begin from the genesis of the project up to a flying drone. Expect some code, some hardware and some 3D printing time-lapse.\nCan you have one and only one process consistently in a distributed system using elixir? :global, :pg2, something else? What about network partitions and stuff? Let\'s build and test some examples.\nSetting up Ecto outside of Phoenix is really simple and can be a great way to get started learning about Elixir. In this live-coding presentation, we will set up Ecto in a test project and dive into some of the great things about Ecto and how to use it.\n00:00 Luke Imnoff: Decompiling .Beam Files\n04:54 Fernond GalianaL: GraphQL\n10:52 Dave Thomas: Code Like its 1999\n19:25 Robert Beene: Talking to the Machine\n26:10 Powell Kimmey Elixir Maths\nChris McCord is the creator of the Phoenix framework, and author of Metaprogramming Elixir and Programming Phoenix. He spends his days crafting OSS and web applications at DockYard. His current interests focus on new web technologies, distributed programming, and teaching others the tools of the trade.\nYou may have heard about Phoenix and Elixir. It is a language and framework that give you performance without sacrificing productivity. Learn why Phoenix is a great choice for Rails developers and how you can introduce it into your organization.\nThe best way to get better at code is by reading other\'s code. It just so happens that Elixir is the best language for reading other people\'s code. We\'ll dive into some examples of popular libraries to see how the experts do it.\nHave you always wanted to contribute to an Open Source project, but you\'re not sure how? The Elixir and Phoenix Framework communities are a great place to get started. In this talk you\'ll learn how to navigate the ecosystem, avoid missteps due to unwritten rules, find something to work on, and make your first contribution.\nWe\'ll start with the easiest things you can do, (ask a question! fix a typo in the docs!) move on to creating a bleeding edge project using the master branch of Phoenix so you can try out your changes, and go all the way through to building Elixir from source and patching the language itself.\n""Programs that write programs are called metaprograms. They occupy the space between your problem and its solution. When used haphazardly the results can be confusing and cryptic. But when used wisely, they simplify that space and make it possible to solve hard problems in elegant ways.\nElixir embraces metaprogramming. In fact, it\'s fundamental to the implementation of the language itself.\nThis talk will answer questions for you such as:\nWhen should I write a macro?\nWhat is quote and unquote and when do I use them?\nWhen and where does this macro code execute?\nMacro.prewalk? What does this have to do with macros?\nAlong with some practical examples.\nWhat\'s more astonishing is how approachable (and safe!) Elixir makes this complex subject. With this solid foundation, your code will be writing itself in no time!""\nHands-on workshop for developers with no prior experience in OTP and Elixir, which aims at building a simple client-server architecture using Elixir/OTP and real tools like telnet or curl.\nThe workshop has several steps, each one requiring a short introduction followed by a programming exercise for the audience. If one gets lost, but would like to continue to the next step, they can switch to the next git branch. All branches will be published on github.com.\nAt the end of the workshop attendees will have a fully functional Elixir TCP server that can be tested using telnet or curl.\nParticipants may want to install Erlang runtime and Elixir before attending the workshop so that they can implement exercise themselves.']"	['<urn:uuid:5b920fc2-6710-4299-b8da-72f33560d3c0>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T18:05:20.075489	26	44	2174
15	I'm curious about the history of the Vilas Park pier - what was its condition before the renovation and how did they restore it?	Before the renovation, the pier was an abandoned floating fishing pier sitting in a state of disrepair in one of the City's materials yards. Union volunteers transported it to a local union shop where they made repairs during the Wisconsin winter, including installing new decking. They also designed and constructed a pier abutment, and added a new sidewalk and steps at Lake Wingra. After completing the restoration, they transported and installed the pier at its new location at Vilas Park, adding a new hand railing to complete the project.	"[""Dedication Ceremony Set for the New Accessible Pier at Vilas Park\nThursday, September 13, 2018 - 10:22am\nThe City of Madison, the Building Trades Council of South Central Wisconsin (BTC), the Union Sportsmen’s Alliance (USA) and Madison Parks invite the public to a dedication ceremony at 1:00pm on Friday, September 14 at the new accessible fishing pier at Vilas Park, 1501 Vilas Park Drive.\nUnion volunteers from the BTC teamed up with the USA to renovate a formerly abandoned pier and install the renovated, accessible fishing pier on Lake Wingra.\n“This project was a great opportunity for multiple union trades to come together and benefit our local community,” said project leader and BTC of South Central Wisconsin President/Executive Director Dave Branson. “It's rewarding to know that this revitalized pier will provide safe and easy access for all to participate in the sport of fishing.”\nUtilizing nearly $22,000 in funds raised by the USA’s Madison Area Conservation Dinner, union volunteers partnered with the USA and Madison Parks to take the original floating fishing pier, which was sitting in a state of disrepair in one of the City’s materials yards, and restore it for the public’s use.\nEric Knepp, Madison Parks Superintendent conveyed, “Madison Parks and the City of Madison are grateful for the significant efforts by the Building Trades and Union Sportsmen’s Alliance to make our park system more accessible to all residents and visitors.”\nVolunteers coordinated transportation of the pier to one of the local union shops where over the course of the cold, harsh Wisconsin winter repairs were made, including the installation of new decking. In preparation for installation of the renovated, now handicap accessible fishing pier, volunteers and union contractors also designed and constructed a pier abutment as well as a new sidewalk and steps on the edge of Lake Wingra in Madison's Vilas Park, which has greatly increased accessibility to the fishing pier.\nAfter completing the restoration of the pier, it was transported and installed at its new location at Vilas Park. Volunteers installed a new hand railing on the pier to complete this project.\n“This project is an excellent example of the impact that USA’s skilled union volunteers bring to the future of conservation and preserving our outdoor heritage,” said USA Conservation Manager Robert Stroede. “If it weren’t for their dedication to conservation and their community, there is a very good chance that this pier would have never made its way back to the water for the public’s use.”\nMore than 30 union volunteers from Ironworkers (IW) Local 383, International Union of Bricklayers and Allied Craftworkers (BAC) Local 13, United Brotherhood of Carpenters and Joiners of America (UBC) Local 314, International Association of Heat and Frost Insulators and Allied Workers (IAHFIAW) Local 18, International Union of Elevator Constructors (IUEC) Local 132, International Union of Painters and Allied Trades (IUPAT) District Council 7, Laborers’ International Union of North America (LiUNA) Locals 113 and 330 and Association of Sheet Metal, Air, Rail and Transportation Workers (SMART) Local 18 donated nearly 200 hours to rebuild and install the formerly abandoned pier.\n- Media Contact: Ann Shea, Parks Public Information Officer, (608)266-5949, firstname.lastname@example.org""]"	['<urn:uuid:df99a1a3-72e6-4337-a94d-b11a4d58b184>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T18:05:20.075489	24	89	523
16	desert animals stay cool water methods	Desert animals use multiple strategies to deal with heat and water scarcity. For cooling, they employ behavioral adaptations like being nocturnal and staying in underground burrows, and morphological adaptations including light-colored fur/feathers to reflect sunlight. For water, they obtain it through various means: some get it from prey or plants, some like the kangaroo rat produce water from seeds, and others like camels can drink up to 30 gallons in 10 minutes, storing it in their blood and cells.	['Introduction: Desert areas are rarely devoid of life. Instead, they abound with wonderfully adapted plants and animals that have evolved various mechanisms for tolerating or avoiding the extremes of aridity and temperature that might be encountered in their environment. Deserts develop under four distinct geographic conditions:\n- Under zones of high atmospheric pressure associated with the subtropics and centered near 30° latitude. Air descending from the upper atmosphere at these latitudes causes evaporation to exceed precipitation. Much of the Sahara and the Australian desert can be associated with this phenomenon.\n- West coasts of continents between 20° and 30° latitude. In these latitudes, prevailing winds are easterly and prevent moist air from coming onto the west coast. Cold ocean currents also occur in these locations and moisture in the sea air condenses as fog along the shore. Some of the world’s driest deserts are located right on the coast; they received most of their limited precipitation from fog. Such fog deserts include Baja California in North America, the western Sahara in northern Africa; the Atacama in South America, and the Namib in southern Africa.\n- Rainshadows of high mountain ranges. When air masses are forced over mountains and downslope, they warm and their capacity for holding water vapor increases. Evaporation exceeds precipitation and an arid environment or rainshadow is created on the leeward side. Such conditions account for some of the North American deserts (exemplified in Death Valley, CA), the Patagonian desert in Argentina; and the Peruvian desert.\n- Interiors of continents. Usually in combination with the rainshadow effect, distance from a major source of moist air results in dry climates in the interior of a land mass. The Great Basin desert of the US, the Australian desert, and the Gobi desert of Mongolia can all be explained in large part to their interior positions.\nClimate: Arid climates (BWh and BWk) are those which average less than 10 inches of precipitation a year. Potential evaporation exceeds precipitation in the annual water budget. Furthermore, rainfall is highly localized and relatively unpredictable in terms of when it will occur, although usually there are seasons of highest probability for precipitation. Annual variation in total precipitation may also be great. Temperatures are also variable. They may exceed 100° F on summer afternoons, but dip by 20-30 degrees or more at night. Winters are cool to cold: “hot deserts” rarely experience frost; “cold deserts” may have prolonged periods of below freezing temperatures and snowfall.\nVegetation: Shrubs are the dominant growthform of deserts. They may be evergreen or deciduous; typically have small leaves; and frequently have spines or thorns and/or aromatic oils. Shallow but extensive root systems procure rainwater from well beyond the canopy of the shrub whenever it does rain. These are the true xerophytes adapted to tolerate extreme drought. They form an open canopy and, except after rains when annuals may cover the desert floor, the ground between shrubs is bare of vegetative growth.\nWater is not entirely lacking in the desert environment and several other growthforms represent strategies to reach water or to store water:\n- Phreatophytes are plants with long taproots that may extend downward 20 to 30 feet to tap ground water supplies. Especially along intermittent streams or under dunes, underground water may be readily available. Mesquite is a good example here in North America.\nOne of the world’s most unusual phreatophyes is Welwitschia mirabilis of the Namib.\n- Succulents store water accumulated during rains for use during the intervening dry spells. Different species store water in different parts of the plant; hence we can recognize stem succulents, leaf succulents, root succulents, and fruit succulents. Many plant families have members that evolved succulence. Most prominent among stem succulents in the Americas are the Cactaceae; in Africa succulent euphorbias have evolved shapes and sizes resembling the cacti. The agaves (Liliaceae) are examples of leaf succulents in the Americas; their role is filled by aloes (Liliaceae) in Africa. Most succulents do not tolerate freezing temperatures so they are essentially limited to the hot deserts.\n- Another growthform adapted to desert conditions is the ephemeral. This is an especially short-lived annual forb that completes its life cycle in two-three weeks. The seeds are encased in a waterproof coating that prevents desiccation for years if necessary. These plants essentially avoid drought by occurring as seeds most of the time.\n- Perennial forbs with underground bulbs store nutrients and water in undergound tissues and also remain dormant most of the year. They can sprout rapidly after sufficient rains and replenish their undergound stores.\nSoils: Calcification is the dominant soil-forming process, if indeed soil forming even occurs. There is poor development of horizons, with accumulation of calcium carbonate at or near the surface. Sparse vegetative cover and tiny leaves results in little humus and soils typically have a light gray color. Aridosols are the dominant soil order.\nFauna: Like the plants, the animals of the desert have evolved an array of strategies for dealing with aridity.\n- Behavioral adaptations such as being nocturnal or crepuscular, being fossorial, and staying the shade during the heat of day are common.\n- Morphological adaptations include those noticed by Bergmann, Allen, and Golger. The better to radiate body heat to the environment from warm-blooded animals, body sizes are small and appendages long. Pelage and plumage is light colored to reflect sunlight and help prevent the absorption of heat form the environment.\n- Rarer, but important, are physiological adaptations such as aestivation (dormancy during summer), the absence of sweat glands, the concentration of urine, localized deposits of fat in tails or humps; and salt glands to secrete salt without loosing fluids.\nReptiles with their waterproof skin, production of uric acid instead of urine, hard-shelled eggs, and ability to gain body heat directly from the sun and to retreat to shade or underground to avoid heat are exceptionally well adapted to drylands and, not surprisingly, diverse there.\nMany birds in the North American deserts, so fragmented by mountains offering humid habitats and permanent streams, simply fly to free water and so are not limited by the lack of open water. They maintain breeding seasons like other temperate zone birds synchronized by changing photoperiods. In Australia, where the desert geography is quite different and aridity more pervasive, bird populations synchronize their breeding readiness according to cues of rainfall, however erratic and sporadic that may be.', '1 High winds often whip across a desert, blowing bits of sand and dust. The temperature can\nclimb to over 100ļF during the day, and the desertís clear skies offer little protection from the\nsunís hot rays. Also, little rain falls in a desert; the average amount is only ten inches a year.\nThis rainfall is unpredictable too. It may rain for several days, and then not rain again for a few\nyears! Even though the desert presents such tough challenges, some\ncreatures still call this\nplace home. How have they adapted to life in a hot, dry place?\n2 Desert animals have found several ways to beat the heat. Most animals know the best trick.\nThey sleep during the day and creep out during the evening hours when itís cooler. Tortoises,\nfoxes, snakes, some lizards, and rodents all spend their days napping in underground burrows.\nHere the temperature stays about 86ļF. The kangaroo rat even shuts the door to its burrow. He\nfills it with dirt to keep out the heat and any unwanted\nvisitors. Other animals, like the desert\ntoad, sleep much longer than a day. They bury themselves in a cool burrow and sleep right\nthrough the hottest part of the summer.\n3 Desert animals also have physical traits that help them handle the heat. For example, jack\nrabbits and foxes have large ears with lots of tiny blood veins. Heat escapes from the veins.\nThis helps cool their bodies. Another helpful feature is a thick coat. Instead of keeping it warm,\na camelís thick coat blocks out the sunís hot rays. Finally, many desert animals have light-\ncolored fur, feathers, or scales. The pale colors absorb less heat. They\nhelp the animals hide in\nthe sand too.\n4 One of the biggest challenges of desert living is not the heat but the lack of water. Desert\nanimals must find ways to obtain enough water in their diet. One animal solves this problem by\nmaking its own water. The kangaroo rat eats dry seeds, but its body changes the seeds into\nfood and water. This animal never needs to take one drink during its lifetime!\n5 Other animals, like snakes, get water from the prey they eat. Still others get water when they\neat plants. Leaves and cacti contain lots of liquid. Of course, some animals do need to take a\ndrink, and the camel can take the largest drink of all. It may gulp down thirty gallons in ten\nminutes! Some people assume this water is stored in the camelís hump, but actually the hump\nonly stores fat. The water a camel needs to survive is stored in its\nblood and cells.\n6 Today, many animals live successfully in the deserts around the world. In the Sahara Desert\nalone, there are forty different kinds of rodents, such as mice and gerbils. There are almost one\nhundred kinds of reptiles. These creatures all have one thing in common. They have learned to\nadapt to their hot, dry home.']	['<urn:uuid:76d9f0c6-77c0-4463-9597-7890572a4967>', '<urn:uuid:2fa0dd44-ce0c-4417-9acf-6a8e52d476bb>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T18:05:20.075489	6	79	1562
17	uv light distribution measurement 3d files how measure germicidal uv radiation better than handheld instruments	With goniospectrometry in the UV range (200-400 nm), it's possible to quantify UV radiation by creating 3D light distribution files (.IES or .LDT formats) that can be used in standard light planning software. This method has been demonstrated using commercially available equipment where a standard goniometer is fitted with a spectrometer sensor. This approach is more comprehensive than traditional handheld instruments and simple dosimeter cards that currently dominate practical measurements in the immature GUV business.	['We are ready to assist you\nViso Media Relations work to serve members of the press quickly and professionally with e.g. text, stories and proprietary images.\nPlease contact technical sales director Anne Bay at\nanne.bay(at)visosystems.com or call +45 29 63 87 40\nPress release 23 MAR 2021\nTo kill germs with UV light, what really matters is exposure time. But how is UV light distributed in a real space and how long should UV lamps be on? 3D UV light measurements combined with the free planning software DIALux leads to much safer implementation of germicidal UV.\nWhy settle for point UV measurements when you can get 3D? Germicidal UV radiation (GUV) has become extremely popular due to the COVID-19 pandemic. As stated by the CIE in May 2020 , it is very important that GUV is applied with appropriate attention to dose and safety. However, the GUV business is immature, and practical measurements are dominated by handheld instruments and simple dosimeter cards.\nWith goniospectrometry in the UV range (200-400 nm) it is possible to quantify UV radiation in the same way as visible light. In fact, it is possible to make UV 3D light distribution files (.IES or .LDT formats) and work with them in a standard light planning software such as Relux and DIALux.\nThis has recently been demonstrated with commercially available equipment from Viso Systems in Copenhagen, Denmark. A standard goniometer is fitted with a spectrometer sensor (range 200 to 850 nm). An example is shown in Figure 1. This UV light distribution can be exported to .IES and .LDT formats that usually contain photometric results (visible light only).\nWith a UV light distribution file, UV light can be simulated in real spaces like the office space example In Figure 2. The iso-curve values on the desks represent the irradiance (in mW/cm2) generated by the two UV lamps in the ceiling. Hence, it is possible to work with UV irradiance much in the same manner as illuminance in the free lighting design software, DIALux Evo.\nWith knowledge about inactivation doses for specific bacteria or for viruses such as e.g. COVID-19, this irradiance can easily be converted to exposure time.\n“This method is a big step ahead in making germicidal UV applications safer for people and deadlier to germs”, says Viso Systems founder Christian Krause. “This relatively simple way of foreseeing UV distribution in real spaces adds professionalism to the UV business.”\nThe Danish producer of goniospectrometer solutions recently posted a YouTube video with a step-by-step introduction to the novel UV planning method – see https://youtu.be/A1RbYx-_S-c.\n“We hope more producers of UV lighting equipment will go after 3D UV measurement files”, agrees technical sales director Anne Bay and adds: “this business needs to supply credible germicidal predictions to clients – the sooner the better”.\nLaboratory light measurement result from a germicidal UV light source. The dominant wavelength is 254 nm. The light distribution curves represent spatial distribution crosswise and lengthwise.\nLight planning result with the UV light sources in Figure 1. The iso-curve values on the desks represent the irradiance (in mW/cm2) generated by the two UV lamps in the ceiling. The simulation is done in DIALux Evo 9.2.']	['<urn:uuid:d837bd60-7d73-4f31-a0f7-40872c3fdb77>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T18:05:20.075489	15	75	530
18	underwater photography camera flash vs antarctica wildlife photography flash	For underwater photography artificial light is regularly used to add color and assist with exposure, while in Antarctica flash photography is not allowed around wildlife. In Antarctica, the available natural light is abundant and beautiful, making flash units rarely usable even on board the ship.	['Take a Journey to Another Photographic World – Underwater Photography Part I\nLife can be full of exciting journeys when we seek them out, or vice versa. My latest journey began when I purchased an underwater housing and strobes for my digital SLR late last year. After completing Summer Intensive and Advanced Intensive training at Rocky Mountain School of Photography during the fall of 2009, I returned home to Florida eager to bring my camera along on my underwater adventures. Having seen some truly amazing sights since I first put on a mask and snorkel at the age of 5, I thought that the underwater world would provide a wealth of photographic opportunities provided I did my part. Fortunately, this hunch has been more accurate than I could ever have imagined.\nSince I began working with my camera underwater, I have had close encounters with some interesting creatures, both large and small. I have also had opportunities to make images of underwater seascapes that continue to engage my senses. What excites me most about underwater photography is that it allows one to display images from a world that is unfamiliar to many people. In this series of articles I will attempt to provide some basic tools, so you may begin your own underwater journey.\nComfort in the water and training for the conditions in which one intends to shoot are two prerequisite skills to possess before attempting any underwater photography. Comfort in the water can be an elusive skill. However, scuba certification (or more advanced training like closed circuit re-breather technology) is now widely available, so it is relatively easy to obtain a training certification level to meet virtually any need. Several of the most well-known certification organizations in the U.S. include the Professional Association of Diving Instructors (www.padi.com), National Association of Underwater Instructors (www.naui.org) and the International Association of Nitrox and Technical Divers (www.iantd.com). However, don’t forget that snorkeling requires no certification and is a perfectly acceptable way to begin your underwater adventures. I have had many successful underwater shoots while using only mask, snorkel and fins. Provided you have a body of water nearby (Note: swimming pools certainly qualify) sufficient comfort in the water and the appropriate training to suit your needs, there is little reason not to explore the world of underwater photography.\nBefore we cover more specifics about gear and techniques, I would like to address some concepts that are unique to underwater photography. One striking difference from terrestrial photography is how sunlight is affected by water. Water is almost 800 times denser than air. Water density strongly affects the colors of light that are present at varying depths. Warmer colors are quickly absorbed so that little or no red, orange or yellow hues are present at depths of greater than 30 feet. The most prominent colors at depths of more than 30 feet are green and blue. This color filtering can result in monochromatic photos that lack punch. On the other hand, it can also be used creatively once understood. Additionally, there may be low or no ambient light at relatively shallow depths depending on local water clarity. As such, artificial light is regularly used in underwater photography to add color and to assist with exposure. In addition to the color and level of ambient light, one must also consider the presence of particulate matter in the water. Especially turbid or murky water often results in illuminated particles in photos also known as “backscatter.” Therefore, it becomes very important to know under which conditions you should attempt any particular shot as well as proper strobe positioning to minimize backscatter.\nUnderwater photography also tends to involve more “moving parts” than other photographic disciplines. Addressing the typical complexities of terrestrial photography remains crucial. However, the additional gear required for the particular type of dive (snorkeling all the way to deep technical diving) can add a significant amount of variables to the photographic equation. Also, the movement of both the subject and photographer in the water must also be considered. All of the variables can quickly occupy your underwater time, thereby shortening the time available to react to each given scene. As such, an ability to pre-visualize a photograph before entering the water can often save valuable underwater time.\nDue to space constraints, I must now conclude this article. The next article in the series will cover specific issues regarding underwater photography gear from the beginner to more advanced user. It will also address the two most common types of underwater photography (close focus wide-angle and macro photography) as well as issues common to each. I hope this article has sparked an interest to begin your own journey into underwater photography. Until next time, take care.\nThere are many scuba certification programs available and exclusion from this list does not indicate a program is unreliable or disreputable. The author strongly recommends you perform your own research before attending any scuba training courses.', 'Photography In Antarctica\nTips for every camera and ability\nSuccessful photography can be a major component of your experience in one of the most spectacular and special places in the world, and we want to help you make the most of your chance to capture some wonderful images. Antarctica will show us dramatic landscapes, electrifying sunlight, intimate moments with wildlife, sites steeped in history, and doubtless some surprises along the way. Whether you are packing a camera-phone or a full DSLR rig, there will be endless opportunities to capture images that will be cherished for a lifetime. Understanding some of the challenges with photography in Antarctica ahead of time will help you be better prepared so that when the moment comes you’ll be well-equipped and have a few tricks up your sleeve to pull off the perfect shot.\nFirst things first – it’s important to pack the right camera equipment before you leave home so that once you’re on board the ship you’re ready to go. Below are a few frequently asked questions along with our Photography Coaches’ recommendations. Click here to learn more about our on-board Photography program.\nThere are a few non-essential items that could help you get more from your photography. These could include: a remote control (or other system for using the ‘Bulb’ feature of your camera), a Neutral Density filter to fit your ‘normal’ (35-105mm) lens (this is particularly helpful if you’re interested in special effects or video), a polarizing filter (also usually best for the ‘normal’ range lens), a ‘step up ring’ (useful for fitting one expensive filter to several lenses of varying thread diameter), a small towel to pat your equipment dry, lens cloth or LensPen™, spare battery and charger. We strongly recommend a waterproof bag or case to store and carry your equipment in.\nIf you’re a keen photographer taking a lot of images and you need the additional storage capacity and/or to do some photo-editing as part of your day to day, by all means bring one along. Many travelers are packing a tablet or laptop anyway. One important note on an issue that we do encounter: if you are using a Windows PC and are planning to shoot in RAW file format, be sure to install the RAW CODEC for your camera model and operating system before departure.\nMost likely if you’ve owned a DSLR or any camera system with a range of lenses for any length of time you have a pretty good handle on what you like to shoot and which lenses you’ll want to bring on the expedition. We recommend covering a wide range of focal lengths with as few lenses as possible.\nWildlife is one of the main attractions in Antarctica and capturing wildlife images is typically one of the more challenging aspects of photography because of the animals’ tendency to be elusive and at a distance – this is not always the case in Antarctica! The animals’ behavior and our guidelines allow us to be often relatively close. For wildlife photography, we recommend a telephoto lens with a focal length of at least 300mm (in 35mm equivalent) with up to 500mm as a bonus. Our Photography Coaches usually bring either an 80-400mm or a 200-500mm telephoto lens for their long shots. An 80-400mm has an especially versatile reach for a compact lens and is a good ‘traveler’. If you have a point and shoot, or other smaller camera, the manual will often give your camera lens range in 35mm equivalent. Many compact cameras have a surprisingly long reach!\nOther than a telephoto it will be nice to have coverage at wide-angles (~14-35mm) and normal ranges (35-105mm) for landscapes, special effects, portraits etc.\nThis is not an easy question to answer because it will depend entirely on the image file size your camera records and how many pictures you intend to shoot. We will be seeing plenty of amazing things, so we recommend at least four 16GB cards for a 10 mega-pixel camera and at least four 32GB cards for a 24 mega-pixel camera. Erring on the side of more rather than less is the safest bet. Regardless of the capacity of your cards, it’s always a good idea to bring at least two in case one fails. We recommend bringing more, smaller-sized cards rather than fewer, larger-sized cards to reduce the risk of loss or damage.\nThe delicate electronics in cameras need to be cared for and moisture can be highly detrimental, especially combined with salt-water. We do need to be extra careful to keep our camera equipment dry, especially moving in and out of the zodiacs. Keeping your camera in a hard, waterproof case like a Pelican™ Case is ideal, although a dry bag works quite well too – add a small towel to give your camera some cushioning. Otherwise any weather-resistant bag will help – or just keep your camera under your jacket! Remember to always secure zips and latches on camera cases each and every time. The more expensive your equipment is the more you should consider investing in protection for it.\nMuch of our shooting will be done from zodiacs in generally fair sea conditions, ashore at the landings, and on board the ship. We’ll be moving on and off shore a lot and a tripod can be a bit unwieldy. Most cameras (and/or lenses) come equipped with Image Stabilization which is well suited to the type of situations we’ll be in. Having said that, if working with a tripod is important to you (or if you are bringing a super-telephoto lens or plan to shoot video) and you’re accustomed to travelling with one then by all means bring it. A mono-pod is a good, more-compact alternative and can work well for many of the situations we will be in.\nThe Photography Coach will give a lecture on board at the early part of the voyage covering some fundamentals. The best advice we can give in advance of the expedition is to pick up your camera, grab your camera manual, and do a little reading and practice ahead of time. Even just seeing some of the terminology in advance will help better understand it once we start using some technical jargon on the trip. Some of the features to be familiar with include:\n- Image Size – Read up about the Image Size and Image Quality options of your camera and make sure it is set to the largest file size and best quality setting at the start of the trip. We’ll talk about making adjustments to that early on.\n- Focusing – If your camera has a focus area control (usually a thumb pad) read up a little on how it works and practice with it a little ahead of time – try using it to focus on a pet or a child. In the heat of the moment with wildlife moving around it will help you get sharp images if you have practiced a little in advance with selecting the focus area.\n- Exposure Compensation +/- – This is our go-to control for quick adjustments to the brightness or darkness of an image. With constantly changing light and the contrast of whites and blacks it’s one of the easiest ways to get your pictures just right.\n- Flash – Learn how to turn your camera flash off. We won’t use flash anywhere around the wildlife so knowing how to turn that off is important. If you were contemplating bringing a flash unit maybe think twice, we’ll only be able to use those on board and rarely in the field. In general, the available natural light is abundant and beautiful.\n- Sound – Learn how to turn off any audio/sound effects/alarms on your camera (BEEPS!) or any other electronics you may bring onshore – again, this is to minimize our disturbance to the wildlife.\nYes! There is a steady power supply on the ship and there are regular ‘down-times’ between landings and nighttime to recharge batteries, backup images, and get ready for the next shoot. Bringing a charger for your camera/battery is essential. Spare batteries are highly recommended as the cold tends to drain batteries faster.\nAgain, there’s no right or wrong camera to bring. However, if you’re itching to buy a new camera, but you’re concerned about the learning curve, don’t worry – take some time ahead to read the quick-start guide, and bring the full camera manual along. Most camera manuals are available online from the manufacturer for portability on your tablet, laptop or mobile. Our Photography Coach will be on hand to help one-on-one if you need help navigating around new equipment.\nThere will be a wide variety of cameras and photographers on board, from seasoned vets, hobbyists with professional equipment, and casual snappers with pocket cameras and iPhones. There’s no right or wrong camera to bring. Some cameras are able to capture higher-quality images in certain circumstances but there will be opportunities to learn about your own equipment and perhaps even be able to make some new purchasing decisions with some newfound knowledge. Every camera has its strengths and we always see amazing shots from iPhones to pro-DSLRs. At the end of the trip everyone will go home with a fantastic record of our adventures in our expedition keepsake.\nYou should bring the camera that you are most familiar with or would most like to practice using and learn more about. Keep it simple – outdoor photographers are always on the move, being efficient with equipment means more time enjoying the experience and less time fussing with overpacked bags.']	['<urn:uuid:8e5dfa8d-801f-41ba-b9ee-96a4d5d3e134>', '<urn:uuid:10816118-1a05-4c7e-9167-e9195b224592>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T18:05:20.075489	9	45	2427
19	why was operation eagle claw rescue mission iran 1980 considered failure	Operation Eagle Claw in 1980 was considered a failure due to multiple factors. The operation plan was excessively complex, requiring coordination between aircraft and personnel from all four military services. There was also a critical lack of intelligence due to rivalry between the CIA and other intelligence agencies, resulting in the Delta Force team having little to no relevant information about the embassy building they were supposed to assault. The operation was ultimately aborted after the loss of transport aircraft, which in hindsight was considered a wise decision as continuing could have led to higher casualties.	['Special Operation Forces (SOF) experienced multiple rises and falls throughout the second half of the 20th and the beginning of the 21st century. For instance, the success of the American SOF in Vietnam faded away after the disastrous hostage rescue attempt from the U.S. embassy in Iran. Nowadays, SOF managed to achieve success and rebuild its reputation as they are routinely deployed in a variety of missions — from direct counter-terrorist operations to training and advising allied forces. However, the very notion of success becomes rather vague when speaking about SOF and its operations.\nThe atmosphere of secrecy and specific features of SOF deployment create obstacles to writing about their activities. The main of those obstacles — usually, only the most apparent tactical results of the special operation are available for analysis after its ending. In other words, due to the lack of unclassified information, the authors can base their works only on the evident military “victories” or “defeats.” The analysis of the strategic aftermath or even the somewhat detailed breakdown of the tactical layer remains largely impossible since only military personnel has access to classified information. As a result, historians become able to cast light on SOF activities only in hindsight, the decades after the events.\nFor example, when Operation Eagle Claw, an attempt to rescue U.S hostages from the embassy in Tehran, ended on 25 April 1980, it was only possible to state its total failure. Only decades later, when information about tactical planning became available to the public, the scholars managed to point at specific problems that led to an embarrassing result. For instance, it became evident that the plan of operation was staggering in complexity and required the cooperation of aircraft and men from all four military services. In addition, Operation Eagle Claw was planned with a critical lack of intelligence due to a rivalry between the CIA and other intelligence agencies.\nAs a result, the Delta Force team had little to no relevant information about the embassy building that it was supposed to assault. Given these circumstances, the decision to abort Operation Eagle Claw after the loss of transport aircraft starts looking like a wise step rather than an act of cowardice. If Colonel Charles Beckwith had decided to continue the mission, that could have led to a failure with much higher casualties. However, making that conclusion became possible only in hindsight after the military historians gained access to details around the operation.\nAnother example of hindsight in writing about SOF performance comes from the Falklands War of 1982. During that war, SAS, the British special force, participated in several operations against Argentinian troops. Of those events, Operation PARAQUET almost ended with disaster; however, Operation PRELIM removed Argentinian air forces from the battlefield, and the assault on Mount Kent also finished successfully. In 1982 those tactical outcomes were the only available pieces of information for scholars and military historians. As a result, it was impossible to write about anything besides the clear British victory.\nIt took almost 40 years to analyze British SOF performance in the Falklands War from a scholarly perspective. Firstly, in 2007 Robert G. Spulak presented a Theory of Special Operations, in which he underscored the following SOF advantages over the conventional forces:\n- Relative superiority — the ability to gain a temporary decisive advantage even over the larger enemy;\n- Certain access — the ability to move rapidly and stay undetected;\n- Unconventional operation — the ability to act autonomously;\n- Integrated operation — the ability to cooperate with other military forces and agencies;\n- Strategic initiative — the ability to create and maintain initiative over the enemy on the strategic level\nThe Falklands War took place in 1982; the theory suitable for the performance analysis of British SOF during that war emerged in 2007. Only in 2020 the scholars managed to conclude that Operation PARAQUET lacked Relative Superiority and Certain Access, Operation PRELIM was perfect, and an assault on Mount Kent was closer to a conventional military operation. Similar to the case of Operation Eagle Claw, an opportunity to write a scholarly article on a special operation presented itself several decades after the events in question happened. Previously, the lack of theoretical framework and publicly available information left the possibility of writing only the most basic, descriptive articles.\nSurprisingly enough, the problem of special operations analysis and efficiency measuring is actual not only to the scholars but also to the SOF. In 2019 RAND Corporation, a think tank that conducts research for the United States Armed Forces developed a complex methodology for measuring special operation effectiveness. For example, if the special operation includes training of allied Partner Force, the following effectiveness criteria should be applied:\n- Strength and sustainability: manpower and equipment of Partner Force, and its ability to maintain them over time;\n- Operational capabilities: direct capabilities of Partner Force on the battlefield;\n- Morale and professionalism are tracked through the interviews, social media, and recruitment rates;\n- Enemy territorial loss linked with activities of Partner Force;\n- Disruption of enemy operations: change in enemy tactics and operations due to Partner Force’s effectiveness;\n- Enemy losses of personnel and materiel.\nThe existence of such complex algorithms provides a valuable research tool for scholars. While the exact information related to SOF activities would likely be classified, applying the algorithm to information available from open sources would still be possible. For example, applying RAND Corporation criteria to such Partner Force as the Islamic Republic of Afghanistan’s army would have likely pointed to its inability to defend Afghanistan without American support. In regard to direct military efforts, Spulak’s theory of special operations could be used to analyze tactical outcomes. A researcher might apply key SOF advantages defined by Spulak to their documented activities and evaluate whether SOF met those criteria. However, a significant amount of important information would remain classified, which could affect the accuracy of the analysis significantly.\nIsolating the impact of special operations appears to be an even more challenging task. Due to their small numbers and covert nature of activities, SOF is difficult to oversee even for well-informed observers. For instance, when four U.S. special operators died in Niger in 2017, the public and even the politicians in Congress were almost unaware of their presence in that African country. Therefore, any attempt to isolate the impact of SOF activities would probably be limited to the basic tactical level.\nA researcher would likely be able to highlight the immediate aftermaths, such as eliminating terrorist leaders or destroying enemy facilities. For example, the tactical results of the British SAS efforts during the Falklands War were quite apparent. However, isolating the strategic impact of special operations would probably require extreme patience and years or even decades of waiting. Overall, the reluctance to reveal classified information due to the possible political implications effectively prevents scholars from prompt analysis of SOF and special operations.\nByman, Daniel and Merritt, Ian A. “The New American Way of War: Special Operations Forces in the War on Terrorism,” The Washington Quarterly 41, no. 2 (2018): 79–93.\nCombs, Christina L. “Operation Kingpin and Operation Eagle Claw,” Journal of Homeland and National Security Perspectives 6, no. 1 (2019): 4–19.\nLong, Austin. “The Limits of Special Operations Forces,” Prism 6, no. 3 (2016): 35–47.\nRedding, Robert, Beier-Pedrazzi, Anna, Salvia, Gina, Mitchell, Stephanie and George, James. “War in the Falklands: Case Studies in British Special Operations,” Special Operations Journal 6, no. 1 (2020): 18–34.\nRobinson, Linda, Egel, Daniel and Andrew Brown, Ryan. Measuring the Effectiveness of Special Operations. Santa Monica: RAND Corporation, 2019.']	['<urn:uuid:58f4f70f-6df9-4156-b08f-0e297adcd80a>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T18:05:20.075489	11	96	1261
20	As a data scientist researching algorithmic bias in law enforcement systems, I'm curious about how prejudice can manifest in both automated decision-making and hate crime reporting - what are the key intersections between algorithmic discrimination and hate-motivated incidents?	Both algorithmic systems and hate incidents can perpetuate discrimination in several ways. In the law enforcement context, 'predictive policing' algorithms can create feedback loops that stigmatize certain communities based on data inputs. This connects to hate crimes, which are criminal offenses motivated by prejudice towards protected characteristics like race or ethnicity. While algorithms aim to be objective, they can systematically disadvantage groups through biased data or model design, similar to how hate incidents target specific communities. For example, both algorithmic systems and hate crimes can manifest as discrimination in areas like employment - from algorithmic personality tests rejecting candidates to hate-motivated workplace harassment. It's important that both issues are properly reported and monitored - hate crimes should be reported to police to prevent re-victimization, while algorithmic systems need transparency and accountability measures to detect and address unfair outcomes.	"[""- What is Hate Crime?\n- Why Report a Hate Crime?\n- How to Report a Hate Crime\n- Useful External Links\nHate crime is defined as ‘any criminal offence which is perceived, by the victim or any other person, to be motivated by hostility or prejudice towards someone based on a personal characteristic.’ There are five centrally monitored strands of hate crime based on:\n- race or ethnicity\n- sexual orientation\n- transgender identity\nThis common definition was agreed by the Crown Prosecution Service and the Association of Chief Police Officers.\nHate crime can include, for example:\n- verbal abuse\n- physical attack\n- offensive literature\n- damage to property\nMore information on Hate crime can be found on the Home Office website (external link).\nAs well as hate crimes, which are criminal offences, a person may be a victim of a hate incident. Something is a hate incident if the victim, or anyone else, thinks it was motivated on the grounds of the characteristics listed above. If you believe something is a hate incident as opposed to a hate crime, it should still be reported using the non-emergency options listed below.\nHate incidents can include, for example:\n- verbal abuse, like name-calling and offensive jokes\n- bullying or intimidation by children, adults, neighbours or strangers\n- hoax calls\n- online abuse, for example on Facebook or Twitter\n- throwing rubbish into a garden\n- malicious complaints, for example over parking, smells or noise.\nYou can report a hate crime or a hate incident even if it wasn't directed at you. You may have witnessed something happen to another person on public transport, or been told about a friend, relative, or co-worker being victimised.\nWhether you have been targeted yourself, or witnessed something happen to someone else, it's important to report hate incidents and hate crime because it allows police to build a picture of the issue. This can help nationally and locally -your report may stop other people being targeted through police intervention.\nAlthough the police will not be able to make arrests or bring prosecutions against people who haven't broken the law, they can still help to direct you to other forms of support and guidance when a hate incident has taken place.\nYou may be worried about wasting police time, or concerned that what happened doesn't fit the criteria to be classed as a hate crime or a hate incident - the police are committed to tackling hate crime and are best placed to assess whether what has happened fits the criteria or not. Reporting the incident is the safest thing to do and could prevent re-victimisation.\n- If a hate crime is happening, and you or others are in immediate danger, call 999\n- In a non-emergency, or to make a report after a hate crime has happened, you can call 101\n- You can also attend a local police station to make a report:\nTottenham Police Station, 398 High Road, N17 9JA (open 24 hours)\nTrue Vision (external link) is an online tool you can use to report hate crime or incidents perpetrated against you, or on behalf of another person.\nTo report graffiti (offensive), visit our Acts of Vandalism page.\nIf you have been a victim of, or witnessed a hate crime, please see the list of support and advice organisations below. These links will take you to external websites.\n- Victim Support - independent charity for victims and witnesses of crime\n- True Vision - hate crime reporting and resources\n- Tell MAMA - national organisation monitoring Islamophobia\n- Community Security Trust - national charity working for the safety and security of the Jewish community\n- Wise Thoughts - LGBT and BAME charity based in Haringey\n- Galop - advice and support charity for LGBT people\n- Citizens Advice Bureau - independent charity providing information and advice to the public\n- Inclusion London - promoting equality for London’s deaf and disabled people\n- Home Office Hate Crime Action Plan\n- Violence Against Women and Girls (VAWG) (internal link)\nPlease see Support Organisations for Centrally Monitored Strands (PDF, 469 KB) for a comprehensive list of organisations that can provide useful support and advice.\nPage last updated:\nTell us what you think about this page\nPlease use this space to tell us how we can improve this page, or the website in general.\nIf you have a service problem or complaint you need help with then please visit our contact pages.\n* = response required"", 'When you browse online for a new pair of shoes, pick a movie to stream on Netflix or apply for a car loan, an algorithm likely has its word to say on the outcome.\nThe complex mathematical formulas are playing a growing role in all walks of life: from detecting skin cancers to suggesting new Facebook friends, deciding who gets a job, how police resources are deployed, who gets insurance at what cost, or who is on a “no fly” list.\nAlgorithms are being used—experimentally—to write news articles from raw data, while Donald Trump’s presidential campaign was helped by behavioral marketers who used an algorithm to locate the highest concentrations of “persuadable voters.” But while such automated tools can inject a measure of objectivity into erstwhile subjective decisions, fears are rising over the lack of transparency algorithms can entail, with pressure growing to apply standards of ethics or “accountability.”\nData scientist Cathy O’Neil cautions about “blindly trusting” formulas to determine a fair outcome. “Algorithms are not inherently fair, because the person who builds the model defines success,” she said.\nO’Neil argues that while some algorithms may be helpful, others can be nefarious. In her 2016 book, Weapons of Math Destruction, she cites some troubling examples in the United States:\nPublic schools in Washington, D.C. in 2010 fired more than 200 teachers—including several well-respected instructors—based on scores in an algorithmic formula which evaluated performance.\nA man diagnosed with bipolar disorder was rejected for employment at seven major retailers after a third-party “personality” test deemed him a high risk based on its algorithmic classification.\nMany jurisdictions are using “predictive policing” to shift resources to likely “hot spots.” O’Neill says that depending on how data is fed into the system, this could lead to discovery of more minor crimes and a “feedback loop” which stigmatizes poor communities.\nSome courts rely on computer-ranked formulas to determine jail sentences and parole, which may discriminate against minorities by taking into account “risk” factors such as their neighborhoods and friend or family links to crime.\nIn the world of finance, brokers “scrape” data from online and other sources in new ways to make decisions on credit or insurance. This too often amplifies prejudice against the disadvantaged, O’Neil argues.\nHer findings were echoed in a White House report last year warning that algorithmic systems “are not infallible—they rely on the imperfect inputs, logic, probability, and people who design them.”\nThe report noted that data systems can ideally help weed out human bias but warned against algorithms “systematically disadvantaging certain groups.”\nZeynep Tufekci, a University of North Carolina professor who studies technology and society, said automated decisions are often based on data collected about people, sometimes without their knowledge. “These computational systems can infer all sorts of things about you from your digital crumbs,” Tufekci said in a recent TED lecture. “They can infer your sexual orientation, your personality traits, your political leanings. They have predictive power with high levels of accuracy.”\nSuch insights may be useful in certain contexts—such as helping medical professionals diagnose postpartum depression—but unfair in others, she said. Part of the problem, she said, stems from asking computers to answer questions that have no single right answer. “They are subjective, open-ended and value-laden questions, asking who should the company hire, which update from which friend should you be shown, which convict is more likely to reoffend.”\nFrank Pasquale, a University of Maryland law professor and author of The Black Box Society: The Secret Algorithms That Control Money and Information, shares the same concerns. He suggests one way to remedy unfair effects may be to enforce existing laws on consumer protection or deceptive practices.\nPasquale points at the European Union’s data protection law, set from next year to create a “right of explanation” when consumers are impacted by an algorithmic decision, as a model that could be expanded. This would “either force transparency or it will stop algorithms from being used in certain contexts,” he said.\nAlethea Lange, a policy analyst at the Center for Democracy and Technology, said the E.U. plan “sounds good” but “is really burdensome” and risked proving unworkable in practice. She believes education and discussion may be more important than enforcement in developing fairer algorithms.\nLange said her organization worked with Facebook, for example, to modify a much-criticized formula that allowed advertisers to use “ethnic affinity” in their targeting.\nOthers meanwhile caution that algorithms should not be made a scapegoat for societal ills. “People get angry and they are looking for something to blame,” said Daniel Castro, vice president at the Information Technology and Innovation Foundation. “We are concerned about bias, accountability and ethical decisions but those exist whether you are using algorithms or not.”']"	['<urn:uuid:c0b092d4-f467-45a5-90af-7a92036c63af>', '<urn:uuid:6ac4c8f9-4485-43b7-b764-b48a249516fb>']	open-ended	with-premise	verbose-and-natural	distant-from-document	three-doc	expert	2025-05-12T18:05:20.075489	38	138	1534
21	I'm planning a beach vacation in Sri Lanka and I'm concerned about the weather. What's the best time to visit the western and southern coastal areas, and how does the monsoon season affect different parts of the island?	The best time to visit western and southern coastal resorts in Sri Lanka is between December and April. During monsoon seasons, the weather pattern is interesting - when one part of the island experiences monsoon, the other part has good weather. Even during monsoon periods, you can still expect sunny warm days, though there might be evening showers. From May to October, while the western and southwestern parts experience monsoon, the eastern half of the island enjoys fine weather with warm sunny days.	"['Sri Lanka positions between the northern latitudes 5 55\' and 9 55\' and the eastern longitudes 79 42\' and 81 52\' 650km north of the equator. It comprises 65,610 sq km and has 1,330km of coastline. There is a 30km divide between the island and India. 1,215km to the west are the Maldives, and to the south nothing but thousands of kilometers of ocean until Antarctica.\nCurrent time in Sri Lanka\nSri Lanka is a year-round destination as it has two monsoons that occur in different halves of the island at different times. So if you wish to enjoy the western and southern coastal resorts when the weather is best, come between December and April. However, even during the monsoon, sunny warm days are common, although evening showers can be expected. When the monsoon sets in the West and the South western half of the island the eastern half experiences a fine weather with warm sunny days. This begins from May to October.\nSri Lanka owns 14 National Parks which offer the chance to see some of the country\'s 91 mammals (16 endemic) elephant, leopard, sloth bear, sambur, spotted deer, hog, mouse and barking-deer, wild boar, porcupine, ant-eater, civet cat, loris, giant squirrel, and monkeys such as the macaque, purple-faced leaf money and grey langur.\nThe largest of the parks is Yala, where jeep safaris provide close encounters with leopards and also abundant bird-life. The best park to see elephants is Uda Walawe.\nYes. Ayurveda is practiced more widely than Western medicine. Many hotels offer Ayurvedic treatment for guests and have qualified practitioners to advise you on how to improve your health, or give various types of baths and massages.\nEvery full moon day is a Buddhist public holiday, a POYA. The most important is in May Vesak Poya; a festival that marks the Buddha\'s birth, enlightenment and bereavement, worth seeing are the illuminated pandals (bamboo frameworks) hung with pictures depicting events in the Buddha\'s life.\nSri Lanka\'s best-known traditional festival is the Kandy Esala Perahera, held in Kandy over 10 days in late July to early August. Perahera means ""procession"" and that\'s exactly what occurs nightly - a magical passing-by of drummers, dancer\'s whip-crackers, acrobats and robed elephants.\nA caparisoned tusker carries the reason for the festival, the Sacred Tooth Relic of the Buddha for the people.\nMosquito repellent is essential since malaria exists throughout the country apart from the districts of Colombo, Kalutara and Nuwara - Eliya.\nThe compact size of Island and the accessibility of most major attractions mean that even a week will allow you to visit a number of different areas. But to experience the island properly, a two-week stay is advisable.\nThe island, with a constant sea temperature of 27 c, is an ideal location for wind surfing, water skiing, Jet Skiing, surfing, sailing, scuba-diving (including wreck-diving), snorkeling, speed-boating and banana-boating. In addition there are fresh water opportunities in rivers and ""tanks"" (reservoirs), such as kayaking and canoeing, and white-water rafting.\nIn the low country, loose cotton skirts or trousers and tops, and a long sleeved blouse for visiting temples, are ideal for woman. Men should wear cotton trousers or shorts and a T-shirt, or even the local sarong. Take a sunhat and sandals, slippers or open shoes that are easy to slip on and off. Being a conservative society, especially in the rural areas, very short skirts and shorts should be avoided.\nFor hill country strip pack a light sweater, and if you intend to sample a natural trail, bring a pair of walking shoes or trainers. If you are travelling with children, a sunhat, loose cotton shorts and tops, including long-sleeved tops to protect them from mosquito bites, will be needed.\nRice is consumed with curries (eggplant, potato, green banana, chicken, and fish) that range from delicately - spiced to near - dynamite. There are also hoppers (a pancake-like snack), string hoppers (steamed rice noodles) and pittu (four and coconut mixture). Lamprais - rice and accompaniments baked in plantain leaves – is a legacy of the Dutch.\nFresh fish, prawns, crab, squid and crayfish are readily available. Desserts include buffalo curd topped with palm-honey, and the caramel-like wattalapam.\nTasty snacks known as short eats are excellent for trips. Fruit includes mango, pineapple, banana and papaya, and the lesser- known but distinctive sapodilla, mangosteen, rambuttan, woodapple custard apple and beli. Colombo has an impressive range of restaurants specializing in international cuisine.\nThe local currency is the Sri Lankan rupee (LKR.), divided in to 100 cents.\nThere are notes of many denominations from Rs. 5,000 to Rs.10.\nBanks are open from 0900 hours to 1500 hours Monday to Friday. Some banks are open on Saturday mornings. It\'s easy to withdraw money across the island at ATMs using international credit cards or debit cards.\nSri Lanka has two official languages – Sinhala and Tamil – with English as a link language. Most people have some knowledge of English, and signboards are often in English. Sri Lanka boasts a literacy rate over 90%.\nNo antiques, rare books, palm leaf manuscripts and anthropological material may be exported without permission from the National Archives and archaeological Commissioner.\nduty free shops at the Bandaranayake International Airport stock an extensive range of liquor, electronic equipments, watches, perfumes and much more. Payments is accepted only in foreign currency.\n© Copyright 2012 — 2017 All Rights Reserved by Ceylontusker']"	['<urn:uuid:22d67577-a5cf-4524-9a55-c27852da8dd1>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T18:05:20.075489	38	83	897
22	How does physical location of genes affect DNA rearrangement?	The genome's three-dimensional organization within the cell's nucleus greatly influences where broken chromosome ends rejoin. Two sequences must be physically proximal and broken to join, and if sequences are frequently together but one doesn't break, or if they both break but are on opposite sides of the nucleus, the chances of translocation are very low or zero.	['We’ve known for decades that our chromosomes can break and reshuffle, especially in cancer cells. We also see this process, called translocation, in naïve B cells when they start to produce antibodies for the first time: the cell breaks, shuffles and recombines genes to decide which threat it will defend against.\nBut knowing these things happen doesn’t mean that we’ve understood the rules for how and why they happen. By combining two powerful methods of genomic mapping, a research team led by Frederick Alt, director of the Program in Cellular and Molecular (PCMM) Medicine at Children’s Hospital Boston and the Immune Disease Institute (IDI), has brought some of those rules into clearer focus. It turns out that the genome’s three-dimensional organization – where each of the genome’s thousands of genes lie spatially within the cell’s nucleus – holds great influence over where broken chromosome ends rejoin.\n“While chromosomal breaks and translocations are fundamental to many cancers, historically we’ve had no approaches to systematically study how they are generated,” says Alt. “About five years ago, our group set out to generate a high-throughput approach to address this important problem in cancer biology.”\nTo accomplish this goal, the Alt lab developed high-throughput genome-wide translocation sequencing (HTGTS). This technique maps “hot spots” in the genome, places where chromosome breaks and translocations are more likely to occur. In early HTGTS studies, they found that broken chromosomes often rearrange within themselves, as opposed to sharing pieces across different chromosomes.\nTo probe these findings more deeply, Alt’s laboratory joined forces with Job Dekker – co-director of the Program in Systems Biology at the University of Massachusetts Medical School – to combine HTGTS with another method called Hi-C. Developed by Dekker’s group, Hi-C measures how all the sequences in the genome are arranged relative to one another in three dimensions.\nWhat they found were several related but distinct principles that together highlight the relationship between proximity (how close two chromosomes or genes are physically), the genome’s 3D structure, and break frequency (the likelihood that a chromosome will break at a given gene sequence).\n“Two sequences have to be broken and physically proximal to join,” Alt explains. “If two sequences are together in most cells and frequently broken, they will translocate in many cells. If two sequences are frequently together but one of them doesn’t break, or if they both break frequently but always lie on opposite sides of the nucleus, the chances that they will translocate are very low or zero.\n“However, if both sequences break very frequently and are close together in only a subset of cells,” he continues, “they will very frequently translocate in that subset, contributing to recurrent translocations.” This last point, he notes, helps explain the recurrent translocations often seen in lymphomas.\nThis new understanding of the relative roles of proximity, organization and frequency in chromosomal translocations opens up new avenues for deciphering how the organization of cell’s nucleus affects the genomic disarray found in cancer and other diseases characterized by chromosome reshuffling. “We feel that our findings and the application of our approaches will provide a new lens through which to view the genomes of many different types of cancer,” Alt says.\n[Ed. note: To learn more, check out an article posted by the IDI.]']	['<urn:uuid:7b2fd3ef-ece2-48aa-9582-f4fbf1fe2225>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T18:05:20.075489	9	57	543
23	development sector professional why donations based overhead costs wrong approach charity selection	Focusing on overhead costs when selecting charities is misguided because what constitutes 'overhead' is often essential for a charity's effective functioning. The overhead ratio is completely irrelevant for measuring a charity's effectiveness. What matters is how much good the charity accomplishes per dollar spent, not what percentage goes 'directly' to interventions. High-performing charities often need to invest in skilled staff, proper management, and impact evaluation to maximize their effectiveness.	['by Adriano Mannino and Tobias Pulver\nImagine two charities that both fight a particular disease in a poor country. Donating to charity_1 means that 95% of your money goes directly to the cause (buying and distributing medicine), while only 5% is spent on salaries and other expenses. Donating to charity_2 means that 70% of your money goes straight to the cause, while 30% is spent on salaries and other expenses.\nWhich charity do you pick? Easy decision?\nA closer look\nNow imagine that before committing yourself to donating, you decide to look more into how each charity is allocating its funds:\nCharity_1 relies on many volunteers to keep expenses low. The volunteers are very motivated to work for the good cause, but they lack experience in management, health economics and other relevant domains. The charity chose to not invest an resources into tracking and evaluating their work. In case their interventions have any long-term negative side-effects, they likely wouldn’t find out.\nCharity_2 has hired a team of experts, people who could for instance be earning high salaries in business management. The salaries the employees receive are lower than what they could earn elsewhere, but they are still competitive enough to attract talented people with the required expertise. Charity_2 also has people working on impact evaluation, tracking the efficiency and even long-term cost-effectiveness of their interventions.\nYou also find out that an independent scientific charity evaluator has looked into both organizations and concluded that the interventions (which here includes the entire set of activities the charities engage in) carried out by charity_2 are likely about 80 times more effective at curing the disease than the interventions of charity_1 – i.e. for the same input money, you get an 80-fold greater output. This difference is due to significantly better logistics and efficiency benefits from large-scale distributions, comparatively higher general reliability of processes due to more professional staff, as well as from charity_2 having researched and implemented alternative and better methods to fight the disease in a more cost-effective way (e.g., prevention instead of cure). Charity_2 is reaping the benefits of having improved its operations several times based on the findings of its own impact evaluations.\nNow, which charity do you donate to?\nBusting the overhead myth\nThis example was hypothetical, but the general scenario is very plausible. Charity evaluator GiveWell has concluded that charities differ in effectiveness by orders of magnitude. And yet, instead of consulting the results of such impact-per-dollar evaluations, the most pressing question for most donors remains how much “overhead” a charity has¹,²: The less money spent on salaries and other administrative expenses, the better the charity.\nThis is worrying because this measure is completely irrelevant for the effectiveness of a charity; focusing on it is nonsensical and sometimes even counterproductive. As we have seen in the above example, what constitutes “overhead” of a charity is an essential ingredient to the effective functioning of any charity, and it often is what makes good charities particularly effective. There is no clear-cut difference between “money spent directly on the cause” and “overhead”. Good charities are no different from good businesses: Everything a good charities spends money on is there to help it achieve its goal: carrying out its charitable intervention as well and as often as possible. Imagine a hospital congratulating itself on the fact that 90% of its budget go “directly to the patients” – or indeed a standard for-profit boasting that “90% of our budget go directly into building cars, only 10% of our expenses go into planning, designing and advertizing them”. It’s an obvious way to fail – and it’s still accepted as a hallmark of excellence when people think about charity.\nOnce people pause to think about the actual (ir)relevance of overhead instead of blindly following the rule that it ought to be as low as possible, it quickly becomes obvious that this approach is misguided. In the end, what matters is how much good the charity accomplishes per dollar, not what percentage of the dollar goes “directly” into an intervention that may or may not be effective. If the case against overhead being relevant is so clear, why is it that it is still the most popular criterion for most donors?\nIn the excellent TED talk titled “The way we think about charity is dead wrong”, Dan Pallotta argues that the obsession with low overhead is a manifestation of a more general problem: People have a mindset that treats the non-profit sector fundamentally different from the for-profit sector. There is an expectation that charity work ought to be direct with no detours and that people need to work for it “out of a genuine motivation to help”, not because of good salaries. This is all fine, of course, but is it reason enough to demonize charities that want to offer competitive salaries in order to attract more skilled people?\nPallotta has the following to say on this: “We have a visceral reaction to the idea that anyone would make very much money helping other people. Interesting that we don’t have a visceral reaction to the notion that people would make a lot of money not helping other people!”\nThe incentives our attitudes create are very irrational – shouldn’t people get bigger rewards the more good they do? Treating charities completely differently from for-profits is bad, because if there’s one thing for-profit business is good at, it’s optimizing for a goal – profit. Charities could benefit a lot from a business-like mindset by applying it to their goal: helping others, reducing unnecessary suffering.\nOne contributor to overhead aversion is the fear of being exploited as a donor. This fear is strong: When we imagine that our donations don’t do any help and instead go to the pockets of mean-spirited scammers, we rightly feel very bad and upset. And in this regard, looking at a charity’s overhead ratio is even a decent heuristic: If the charity spends 99% of its funds directly on some intervention in Africa, then, even if the intervention is not helping at all, at least the people behind the charity aren’t trying to steal my money and have a laugh about it. If they were, they would budget higher salaries and fake expenses.\nIf the reason people focus solely on the overhead ratio when picking charities is fear of scammers, then it appears as though they care infinitely more about charities not being totally fake than they care about charities being positively competent and effective at helping people. Charities’ cost-effectiveness differs by orders of magnitude. If the non-fake charity you pick only reduces 5% as much suffering as the top-rated charities, this disparity is very significant, and the sub-optimal outcome is something that’s at least as important to avoid as falling victim to a total scam: The difference between a scam and the sub-optimal charity you pick is the magnitude of the 5% – while the difference between your sub-optimal charity and the best charities is the magnitude of the 95% disparity.\nIf you care about helping people effectively, avoiding scams is included in the considerations, but so is much more. Therefore, you should ignore a charity’s overhead ratio and instead consult information on charities’ output : input ratio (i.e., cost-effectiveness) as calculated by data-driven evaluations.\nReferences Gneezy, Uri, Elizabeth A. Keenan, and Ayelet Gneezy. “Avoiding overhead aversion in charity.” Science 346.6209 (2014): 632-635.\n Caviola, Lucius, et al. “The evaluability bias in charitable giving: Saving administration costs or saving lives?” Judgment and decision making 9.4 (2014): 303.']	['<urn:uuid:56513d66-b8ff-4bf2-8d01-b5d6ee420452>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	expert	2025-05-12T18:05:20.075489	12	69	1253
24	when gocstudio start operating	goCstudio was co-founded in 2012 by Jon Gentry and Aimée O'Carroll.	['goCstudio receives 2018 Emerging Firm Award from the Northwest and Pacific Region Chapter of the American Institute of Architects\nFirm known for their innovative, authentic and tactile buildings that enrich their cultural landscape.\nSEATTLE, November 16, 2018—Seattle-based goCstudio, an architecture studio best known for its innovative work and collaborations with artists and craftsmen, has received the 2018 Emerging Firm Award from the Northwest and Pacific Region Chapter of the American Institute of Architects (AIA). The award recognizes emerging firms less than ten years old that are challenging traditional perceptions and methods of architecture. The AIA Northwest and Pacific Region (NWPR) includes the states of Alaska, Washington, Oregon, Montana, Idaho and Hawaii, and the territory of Guam. The firm was previously recognized with a 2016 National AIA Small Projects Award for wa_sauna, one of a kind floating sauna that began as a conceptual installation project.\nSince co-founding goCstudio in 2012, Jon Gentry and Aimée O’Carroll have been deeply involved in the design and management of every project at the firm. “We are honored to receive this recognition from the AIA following our first five years as a studio. We are fortunate to be a part of a great design community here in the Pacific Northwest and we aim to build on this recognition as our studio grows” noted Gentry. “It is exciting to see our projects come to fruition and for a wider audience to acknowledge the passion and hard work that goes into each one” added O’Carroll.\nThe firm’s work includes the design of custom residences, as well as bespoke commercial and public spaces. The firm works simultaneously on conceptual and client-driven projects, mixing art-inspired collaborations with real-world problem solving. “We strive to produce conceptually rich projects that act as partner to their community and site, where architecture and creativity are combined with philosophy and craftsmanship to create lasting human environments and experiences. It is our intent that the projects use our limited natural resources responsibly and that each design enriches its site and cultural landscape,” notes O’Carroll.\nMini Mart City Park is a project that exemplifies the goCstudio process and platform. A community-focused project owned-and-initiated by the artist collaborative SuttonBeresCuller and designed by goCstudio, Mini Mart City Park involves the transformation of a former gas station site into a public park and cultural center. “The project demonstrates our commitment to architecture as spatial, social, environmental, and artistic construct, engaging both the technical and the poetic” notes Gentry.\nSound House is a new custom residence in the heart of Seattle’s Magnolia neighborhood with panoramic views of the city and the Puget Sound. The program for the house is unusual in that it is a single family dwelling providing plentiful living space for a family of eight as well as access to the many exterior terraces and roof decks that maximize the site’s views.\nCOR Cellars is a winery located outside of Lyle, Washington in the heart of the Columbia River Gorge. The architecture responds with the use of a courtyard (the heart at the center) as a central organizing system that protects workers and visitors alike from the sometimes harsh conditions of the natural environment characteristic of the Gorge. The design is focused on creating shelter, both physical (from the winds and tough climate) and psychological (a place of refuge in which to enjoy the company of friends).\nSubstantial, a digital product studio, is an office designed to accommodate a company’s growing needs. The entire 14,000-square-foot project occupies a floor of a 100-year-old historic brick building in the heart of Seattle’s Capitol Hill neighborhood.\n“One of our first projects, wa_sauna, was primarily funded by our local art and design community with fundraising events hosted around the city and then built by our studio. The community outreach that this small project demonstrated has inspired us to establish a new design/build program (Hiatus), that our studio plans to initiate in the summer of 2020” notes O’Carroll.\n“It’s important for us to design buildings that stand the test of time and bring a sense of wonder to their inhabitants. Our work aims to invoke a strong appreciation of the landscape and to express a thoughtful relationship between craftsmanship and the built form. We celebrate clients who want to actively participate in the design conversation. We love working with creatively minded individuals who are interested in uncovering the unique design opportunities that are inherent to each project” says Gentry.']	['<urn:uuid:95973dcb-34ac-495e-934a-f8a12b786300>']	factoid	direct	short-search-query	distant-from-document	single-doc	novice	2025-05-12T18:05:20.075489	4	11	735
25	difference between stock movement and volatility explain	Volatility is not based on directional trend in stock price - this is a common misconception. By definition, volatility is simply the amount the stock price fluctuates, without regard for direction. For example, if there were wide daily price ranges throughout the year, a stock would be considered historically volatile, regardless of whether the overall price trend was up or down.	"[""Definition of selling a put option17 comments\nAuto binary expert ratings\nSome traders mistakenly believe that volatility is based on a directional trend in the stock price. By definition, volatility is simply the amount the stock price fluctuates , without regard for direction.\nAs an individual trader, you really only need to concern yourself with two forms of volatility: Unless your temper gets particularly volatile when a trade goes against you, in which case you should probably worry about that, too. And if there were wide daily price ranges throughout the year, it would indeed be considered a historically volatile stock.\nThis chart shows the historical pricing of two different stocks over 12 months. However, the blue line shows a great deal of historical volatility while the black line does not. Like historical volatility, this figure is expressed on an annualized basis.\nBut implied volatility is typically of more interest to retail option traders than historical volatility because it's forward-looking. Based on truth and rumors in the marketplace, option prices will begin to change.\nThat drives the price of those options up or down, independent of stock price movement. Implied volatility can then be derived from the cost of the option. In fact, if there were no options traded on a given stock, there would be no way to calculate implied volatility. Implied volatility is a dynamic figure that changes based on activity in the options marketplace. Usually, when implied volatility increases, the price of options will increase as well, assuming all other things remain constant.\nConversely, if implied volatility decreases after your trade is placed, the price of options usually decreases. Implied volatility is expressed as a percentage of the stock price, indicating a one standard deviation move over the course of a year.\nObviously, knowing the probability of the underlying stock finishing within a certain range at expiration is very important when determining what options you want to buy or sell and when figuring out which strategies you want to implement. Market makers use implied volatility as an essential factor when determining what option prices should be.\nUsually, at-the-money option contracts are the most heavily traded in each expiration month. So market makers can allow supply and demand to set the at-the-money price for at-the-money option contract. Then, once the at-the-money option prices are determined, implied volatility is the only missing variable. Once the implied volatility is determined for the at-the-money contracts in any given expiration month, market makers then use pricing models and advanced volatility skews to determine implied volatility at other strike prices that are less heavily traded.\nYou can solve for any single component like implied volatility as long as you have all of the other data, including the price.\nHowever, watch out for odd events like mergers, acquisitions or rumors of bankruptcy. If any of these occur it can throw a wrench into the monkeyworks and seriously mess with the numbers. As mentioned above, implied volatility can help you gauge the probability that a stock will wind up at any given price at the end of a month period.\nHow can implied volatility help my shorter-term trades? The most commonly traded options are in fact near-term, between 30 and 90 calendar days until expiration. This tool will do the math for you using a log normal distribution assumption. Then, once you have made your forecasts, understanding implied volatility can help take the guesswork out of the potential price range on the stock.\nIn the stock market crash of , the market made a 20 standard deviation move. In theory, the odds of such a move are positively astronomical: But in reality, it did happen.\nAnd not many traders saw it coming. Because option trading is fairly difficult, we have to try to take advantage of every piece of information the market gives us. As you know, a stock can only go down to zero, whereas it can theoretically go up to infinity. Downward movement has to stop when the stock reaches zero. Normal distribution does not account for this discrepancy; it assumes that the stock can move equally in either direction. In a log normal distribution, on the other hand, a one standard deviation move to the upside may be larger than a one standard deviation move to the downside, especially as you move further out in time.\nOptions involve risk and are not suitable for all investors. For more information, please review the Characteristics and Risks of Standardized Options brochure before you begin trading options. Options investors may lose the entire amount of their investment in a relatively short period of time. Multiple leg options strategies involve additional risks , and may result in complex tax treatments.\nPlease consult a tax professional prior to implementing these strategies. Implied volatility represents the consensus of the marketplace as to the future level of stock price volatility or the probability of reaching a specific price point. The Greeks represent the consensus of the marketplace as to how the option will react to changes in certain variables associated with the pricing of an option contract.\nThere is no guarantee that the forecasts of implied volatility or the Greeks will be correct. Ally Invest provides self-directed investors with discount brokerage services, and does not make recommendations or offer investment, financial, legal or tax advice. System response and access times may vary due to market conditions, system performance, and other factors. Content, research, tools, and stock or option symbols are for educational and illustrative purposes only and do not imply a recommendation or solicitation to buy or sell a particular security or to engage in any particular investment strategy.\nThe projections or other information regarding the likelihood of various investment outcomes are hypothetical in nature, are not guaranteed for accuracy or completeness, do not reflect actual investment results and are not guarantees of future results. All investments involve risk, losses may exceed the principal invested, and the past performance of a security, industry, sector, market, or financial product does not guarantee future results or returns. The Options Playbook Featuring 40 options strategies for bulls, bears, rookies, all-stars and everyone in between.\nOr why your option prices can be less stable than a one-legged duck Some traders mistakenly believe that volatility is based on a directional trend in the stock price.\nHistorical volatility of two different stocks. Quick and dirty formula for calculating a one standard deviation move over the life of an option Remember: Meet the Greeks What is an Index Option?""]"	['<urn:uuid:c7d4c899-598d-4d95-b17c-a28a79d918ac>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T18:05:20.075489	7	61	1083
26	Which is harder to prosecute: conspiracy in tax fraud or PPP cases?	Tax fraud-related crimes are among the most difficult to prove, as the government must show beyond reasonable doubt that the defendant intentionally defrauded the IRS, complicated by the complex nature of the Internal Revenue Code and jury pools' distaste for tax law. For PPP loan fraud conspiracy cases, the burden of proof is on the federal government to show bad faith in loan applications or fund usage, but these cases can be more straightforward to prosecute since they involve clear documentation trails of loan applications and fund spending.	['Give back to Caesar what is Caesar’s, Jesus instructed. Taxes must be paid by everyone, except those who have the means and resources to find what is commonly referred to as a “loophole.” While the concept of taxes should apply to everyone, not everyone feels obliged to return to Caesar what is owed to Caesar. The basic inequity of taxes is that loopholes are legal; evasion and fraud are not.\nFederal Tax Crimes\nFederal tax fraud and tax evasion are cousins in crime. Tax evasion is the crime most often charged by Federal prosecutors. The offense is generally tied to the taxpayer’s deliberate misrepresentation of taxable income. For the individual, the crime is punishable up to five years in prison, a fine of up to $250,000 (or both), and an assessment for the cost of the prosecution. These crimes are most often prosecuted under Section 7201 of Title 26, the Internal Revenue Code.\nTax fraud, on the other hand, general involves a host of crimes: filing false tax returns, failure to pay taxes, filing false documents, failure to collect employment taxes, and failure to file a tax return. Tax fraud can trigger a host of criminal laws violations found in both Title 26 and Title 18 of the United States Code.\nTax fraud can be punished both as a criminal and civil offense. Civil offenses are usually prosecuted under Title 26 while criminal offenses are prosecuted under Title 18. If two or more people are involved in any tax fraud scheme, the Government will generally indict under the conspiracy statute in Title 18 (Section 371).\nTax fraud-related crimes are some of the most difficult to prove. The Government is required to prove beyond a reasonable doubt that the defendant intentionally defrauded the IRS out of taxable revenue. Because the Internal Revenue Code is so complicated, and because most people from whom a jury pool is chosen have a strong distaste for the complex nature of the code, the Government will often choose to civilly prosecute the offense because the standard of proof is significantly lower.\nThat’s what only .0022 percent of the taxpayers ever face criminal prosecutions, according to the IRS. Yet the IRS estimates that as many as 17 percent of taxpayers commit a tax law violation in some way each year. While the figure is subject to considerable debate, the IRS reports that 75 percent of the income tax fraud in this country is committed by individual taxpayers rather than corporations. The agency attributes this figure to services workers—such as restaurant workers, mechanics, and handymen—who commonly underreport their cash income.\n“Tax avoidance” can, and sometimes is, used as a defense in tax evasion offenses. The IRS, and the courts who decide tax-related cases, have carved out distinctions between tax avoidance and tax evasion. The latter involves (as does most tax fraud) a willful and knowing fraudulent action to reduce tax liability while the former involves legally permissible conduct to reduce tax liability. In 1934, the famed jurist Learned Hand once offered this classic definition of tax avoidance in United States v. Gregory:\n“Anyone may arrange his affairs that his taxes shall be as low as possible. He is not bound to choose the pattern of which best pays the Treasury, there is not even a patriotic duty to increase one’s taxes. Over and over again courts have said that there is nothing sinister in so arranging affairs has to keep taxes as low as possible. Everyone does it, rich and poor alike, and all do right, for nobody owes a public duty to pay more than the law demands.”\nThe IRS Manual (IRM 9781, § 412, Jan. 18, 1980) itself notes the “fine distinction” between tax avoidance and evasion: “One who avoids tax does not conceal or misrepresent. He shapes events to reduce or eliminate tax liability and, upon the happening of events, makes a complete disclosure. Evasion on the other hand involves deceit, subterfuge, camouflage, concealment, some attempt to color or obscure events, or to make things seem other than they are. For example, the creation of a bona fide partnership to reduce the tax liability of a business by dividing the income among several individual partners is tax avoidance. However, the facts of a particular case may show an alleged partnership was not in fact established and that one or more of alleged partners secretly returned his/her share of profits to the real owner of the business, who in turn did not report this income. That would be an instance of attempted evasion.”\nThe IRS also recognizes “negligence” as a defense when no signs of fraud exist. In these instances, tax auditors will consider mistakes in tax returns as careless errors.\nBoth tax evasion and tax fraud are considered a white-collar crime both at the federal and state level.\nTax Evasion in Texas\nTexas doesn’t have a personal income tax. So Texas’ main source of state revenue comes from its sales tax. Which means that, in Texas, tax evasion and fraud laws are usually applied to businesses or corporations.\nIf you have been arrested and charged with tax evasion in Texas, you will first want to contact an experienced tax evasion attorney who is qualified to defend your white-collar crime. Then, you will want to understand the meaning of your tax evasion charges and the penalties those charges hold.\nIf you don’t pay your taxes or commit a fraudulent act involving record-keeping or falsifying electronic sales records, you could face some pretty hefty fines and prison time.\nUnder the Texas Tax Code section 151.7032, knowingly or intentionally failing to pay the comptroller the required tax is considered tax evasion. Depending on the amount of the tax collected and not paid, there are different levels of charges that you could possibly face, such as:\nIf the amount of tax collected and not paid is under $10,000, you will face a Class C misdemeanor, which is punishable by a fine up to $500 and no jail time.\nIf the amount of tax collected and not paid is over $10,000 but under $20,000, you will face a state jail felony, which is punishable by a fine up to $10,000 and 180 days to 2 years in a state jail.\nIf the amount of tax collected and not paid is over $20,000 but under $100,000, you will face a third degree felony, which is punishable by a fine up to $10,000 and 2 to 10 years in a state prison.\nIf the amount of tax collected and not paid is over $100,000, you will face a second degree felony, which is punishable by a fine up to $10,000 and 2 to 20 years in a state prison.\nTax evasion and tax fraud are serious charges, but with an experienced Texas white-collar lawyer at your side, you will have the best chance at beating them, clearing your good name, and getting back to your business.', 'PPP Loan Fraud Charges\nIn March 2020, the Coronavirus Aids, Relief and Economic Security (CARES Act) was passed by Congress to offer stimulus to families and small businesses. One part of this stimulus included the Paycheck Protection Program (PPP) – at the time $349 billion in funds allocated as forgivable small business loans. The purpose of these loans was to cover payroll costs, most mortgage interest, rent and utility costs over the period the loan is made. As one would expect for a program created hastily in the midst of a crisis, it was rife with loopholes and overwhelmed with applications. As regulatory requirements have been rapidly evolving, many businesses have been confused about their eligibility or how they are supposed to allocate loan funds. This has put many businesses who received PPP loans at risk of federal investigation and legal penalties over alleged fraud.\nIf your business is facing an investigation for PPP loan fraud, or you are concerned about your risk of legal penalties, please contact our North Carolina PPP loan fraud attorneys today. Tarlton Polk Law is committed to representing small business owner’s and fighting for their rights.\nRisk Factors for PPP Fraud Investigation\nWith the media mostly publicizing cases of PPP fraud that are obvious scams, such as companies that don’t exist applying for loans, it may seem like regular businesses that follow the rules to the best of their knowledge should have nothing to worry about. However, any business that has submitted a PPP application should be aware of the program’s stipulations and the various things that could raise a red flag for federal investigators. The U.S Small Business Administration (SBA) released a list of frequently asked questions about eligibility for PPP loans to offer some helpful guidelines, but there are still several gray areas that leave business owners at risk of investigation. Some riskier aspects of the loan include:\n- Assessing “need” when your business has a source of liquidity: According to Question 31 of the FAQ, business owners who have some source of liquidity to support their business operations have to assess and certify whether they will need a loan “in good faith”. This issue can boil down to how convincing of a case you can make that the economic uncertainty caused by Covid-19 makes your liquid assets possibly insufficient.\n- Applying for PPP loans as a seasonal business: Seasonal businesses have struggled to predict what business would be liked during their typical peak activity periods as the pandemic crisis has been ongoing. Certifying payroll costs for impending seasonal employees can be especially challenging,\nWith these areas of subjectivity and uncertainty, businesses trying their best to stay in line with regulations can still face investigation and possible charges. While these issues should not deter you from applying for a loan you strongly believe your business needs, you should have experienced trustworthy lawyers you can contact if you are getting targeted for fraud investigation.\nTypes of PPP Loan Fraud\nThe laws surrounding PPP fraud are still developing, but for now the laws concerning PPP loans fall under three main categories:\n- Application Fraud: There are various acts and practices concerning whether a business presents honest and accurate information when applying for a PPP loan. When it comes to federal funds under the PPP program, only companies that qualify as having “small business concerns” under the Small Business Act, 15 U.S.C. § 632 are eligible for funds. The program also specifies that applications must be based on need due to “the uncertainty of current economic conditions making necessary the loan request to support the ongoing operations of the eligible recipient.”\n- Fradulent use of loan Funds: Funds obtained through a PPP loan may only be used for (1) payroll expenses, (2) interest and rent payments under pre-existing obligations, (3) insurance premiums and (4) utility costs. Use of the loan funds for any other purposes can be investigated as fraud.\n- Loan Forgiveness Certification Fraud: Some businesses can qualify to not have to repay their PPP loans, but they must first certify that they have complied with all the requirements of the program. If a company submits a false certification, including one with incomplete information, this could be considered PPP loan fraud.\nPPP Loan Fraud Charges\nUnder each of the above categories, there are a variety of possible charges business owners can face, some of which have very serious consequences. Some allegations you can be charged with include, but are not limited to:\n- Aggravated Identity Theft: If an individual is accused of using another company or individual’s information to obtain a PPP loan, they can face charges for identity theft under 18 U.S.C. § 1028A. This typically coincides with an underlying fraud offense (i.e. Bank Fraud), and entails additional years of federal prison time served after fraud penalties if convicted.\n- Bank Fraud: Under 18 U.S.C. § 1344, individuals and companies that submit fraudulent loan applications can also face charges for bank fraud. The penalty for bank fraud can be up to a $1 million fine and 30 years of federal imprisonment for knowingly defrauding or attempting to defraud a financial institution by means of false pretenses, representations or promises.\n- Conspiracy: Conspiring to commit PPP loan fraud is also considered a federal offense, under 18 U.S.C. § 371 and 18 U.S.C. § 1349. It can even lead up including the same penalties placed on successfully committing a PPP loan fraud offense.\n- False Claims Act Violations: The False Claims act (31 U.S.C. §§ 3729 – 373) prohibits submitting any false claim for payment under a federal benefit program. Submitting false information in a PPP loan application or in a PPP loan forgiveness certification both carry potential to lead to false claims charges.\n- Making False Statements to Federal Agents: When targeted for fraud investigation, individuals must be extremely cautious when sharing information, because sharing false information can lead to federal prosecution under 18 U.S.C. § 1001. Prosecution carries fines up to $250,000 for individuals or $500,000 for corporations and up to five years in federal prison. In general, it’s essential when facing any federal investigation to contact an experienced lawyer immediately to avoid giving federal agents any incriminating testimonies.\n- Making False Statements to the Small Business Administration (SBA): As the SBA is the agency responsible for administering PPP loans, it is also a federal violation to knowingly make any false statements to them, in accordance with 18 U.S.C. § 1014.\n- Tax Evasion: Charges for tax evasion can be filed against anyone who is accused of fraudulently obtaining PPP loans, as a result of unlawful use of loan funds. For example, claiming deductions for expenses paid with PPP funds can lead to federal tax evasion charges, under 26 U.S.C. § 7201. Federal tax evasion prosecution entails fines of up to $100,000 for individuals and $500,000 for corporations, and up to five years in federal prison.\n- Wire Fraud: Fradulent activity that involves internet communication or activity over the internet can be charged as wire fraud under 18 U.S.C. § 1343. As PPP applications are largely submitted online, wire fraud charges can be easily tagged onto other charges for PPP loan fraud.\nWhat to do if Your Company is Targeted by an Investigation\nThe burden of proof is on the federal government to show that you were acting in bad faith when applying for PPP loans or forgiveness certification or using the loan funds. It is in your best interest that you hire experienced fraud defense attorneys who can defend your innocence and work diligently to eliminate the many charges the federal government can file against you. You will also need the protection and guidance of legal experts when dealing with federal agencies that are looking to find any issues in the statements you give them.\nAdditionally, maintaining a thorough trail of your loan application process and spending of PPP loan funds will aid in defending your case. Keeping an ongoing record and demonstrating that you were trying to diligently follow program requirements will help show that you were acting in good faith.']	['<urn:uuid:de17e3ed-70d2-44ca-8f12-35e884949a2b>', '<urn:uuid:3bcc7fee-4177-4a09-9a39-9d7c1a84ea51>']	open-ended	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T18:05:20.075489	12	88	2503
27	risks benefits anesthesia pet dental cleaning procedures safety	While anesthesia poses some increased risk for older pets, it is the only safe and effective way to thoroughly examine and clean pets' teeth, especially below the gum line where two-thirds of the tooth lies. The risks can be managed through pre-procedure measures like thorough physical examination, blood tests, and fluid therapy. During the procedure, careful monitoring of heart rate, oxygen levels and blood pressure is performed. Non-anesthetic dental procedures are illegal in some places and can cause more harm than good.	['Pets Have Teeth, Too!\nCan you imagine going several years between visits to your dentist?\nWorse, can you imagine never brushing your teeth? Just as with you, your pet cats and dogs need regular dental care to help prevent serious and costly problems ranging from loose teeth to liver disease. Studies have shown most dogs and cats over four years of age have varying degrees of dental disease.\nHere are our answers to some of the most frequently asked questions about cat and dog teeth cleaning:\nIf your veterinarian has recommended your pet have a dental, dental prophy, or dental prophylaxis, it means there are obvious signs of dental disease and it is in your pet’s best interest to have their teeth professionally cleaned and examined by a veterinarian.\nIt’s actually much more involved than just cleaning.\nYour pet’s teeth, gums, tongue, and surrounding tissue will be examined for signs of dental disease such as gingivitis, periodontal disease, broken, chipped, or loose teeth, lesions, abscesses, etc. Only a licensed veterinarian can diagnose these issues.\nAnd the only way to safely and thoroughly examine your pet’s mouth is when they are anesthetized (under anesthesia).\nUsing proper equipment and medication such as anesthetic inhalant, an ultrasonic scaler, and hand tools (similar to what your own dentist uses to clean your teeth), any hard tartar build-up will be removed from both the exposed tooth and below the gum line (up to two thirds of your pet’s tooth is below the gum line).\nAs demonstrated in the video above, it is possible to safely clean below the gum line only when your pet is anesthetized.\nSimilar to what your own dentist does (when they probe your gums and call out, “one, one, two… one, three, one…two, two, four” ) the veterinarian also will probe around your pet’s teeth, call out the depth of receding gum tissue or pockets, and note any visible issues such as chipped teeth, deep pockets, loose teeth, etc., Meanwhile the trained veterinary technician will make notes in your pet’s medical record so we all can monitor the progression of your pet’s oral health.\nDental radiographs (X-rays) are sometimes necessary to assess the health of the tooth, roots, and bone impossible to see with the naked eye.\nWith a clear view of your pet’s entire tooth, root, and bone, veterinarians can assess whether or not your pet needs an extraction or other treatments. Again, our veterinarians will recommend further treatments only when absolutely necessary and in the best interest of your pet’s health.\nYour pet’s teeth will then be polished to remove any abrasions (small scratches) from the dental scaling tools. This is very important because if your pet’s teeth are left with abrasions, it makes for an easy spot for bacteria, plaque and tartar to accumulate very quickly.\nBecause it is not safe or effective.\nBecause they can cause more harm than good.\nYou may still see advertisements for services called gentle dental’s, holistic dental’s, standing dental’s, or anesthetic-free dental’s, all of which are illegal and can cause more harm than good (which is why the government stepped-in to put a stop to this dangerous practice).\nIf you have any questions or concerns about the health of your pet’s mouth and teeth, or the safety of anesthesia, please first chat with your veterinarian.\nGroomers, pet shop employees and anyone else performing non-anesthetic dental procedures on pets are breaking California law.\nWhile their scraping and scaling may make your pet’s teeth look whiter, your pet’s mouth and overall oral health is most likely not healthier because up to two-thirds of some teeth are below the gum line. And below the gum line is where the real problems occur.\nOnly licensed veterinarians are legally allowed to diagnose problems, recommend treatment, and perform dental procedures.\nMost of the bacteria (normal levels of non-harmful bacteria) are already present. Problems occur when the bacteria-laden plaque mixes with saliva. If not brushed off, it eventually calcifies into hard tartar.\nHarmful bacteria living in the calcified tartar below the gum line can get a free ride through your pet’s bloodstream and end up in your pet’s heart, liver, or even in their brain. This is preventable with proper dental care from your veterinarian.\nIf your pet has been living with periodontal disease, in addition to bacteria getting a free ride through your pet’s bloodstream as described in the video, every time your pet swallows, a significant amount of bacteria gets swallowed, too. Your pet’s liver can be assaulted with bacteria from periodontal disease or tooth infections while it is trying to do its regular job of removing toxins based on what your pet consumes on a daily basis.\n*One milligram of plaque can contain over one trillion bacteria.\nNo, it’s actually not. The odor in your pet’s mouth may be coming from the gasses released by the harmful bacteria living in your pet’s mouth and gums.\nAnd as always, we’d love to hear from you. Have you tried any of the dental health products noted in the video? What is your pet’s favorite toothpaste flavor? Do you have any questions about pet dental health? Please call. We’d be delighted to hear from you.\n*Virbac Press Release – dated 1-1-2010 PREVENTING ORAL DISEASE IN DOG AND CATS CAN BE EASIER AND MORE IMPORTANT THAN PET OWNERS THINK', 'Problems such as tooth root abscesses (a painful bone infection in the mouth) are often only discovered once your pet is asleep and the mouth thoroughly examined or x-rayed, with the only symptom of disease being the dog’s bad breath.\nWhile there is some increased risk under an anaesthetic when you pet is older, the risk can be managed by performing a thorough physical examination, a blood test and fluid therapy prior to the procedure, as well as careful monitoring of heart rate, oxygen level and blood pressure while the animal is asleep.\nIt is considered part of responsible pet ownership to maintain the health and welfare of your pet as he or she grows old and this includes treating any dental disease present.\nPlease contact your local vet or Sydney Pet Dentistry if you have any concerns. Make some comments below.\nOnce the appointment is arranged things are pretty straight forward. Don’t feed your pet on the morning of the consultation in case general anaesthesia is required. If possible make sure your pet goes to the toilet beforehand as well. Aim to arrive approximately 10 mins prior to your scheduled appointment time so your details can be entered into the system.\nThe consultation and history taking is thorough and an initial assessment of your pets oral health and pain will be given. Consideration will be given to any other health problems your pet may have. A range of options will then be given with an estimate of costs and the most appropriate option for you and your pet can be undertaken.\nMany pets will be admitted to hospital and anaesthetised. Anaesthesia is a safe, quick and painless procedure that allows a proper assessment and x-rays of your pets mouth. Then and only then can some dental conditions be properly diagnosed and appropriate treatment given.\nRecovering from anaesthesia is usually uneventful and your pet can go home that evening in most cases.\nSpecial note! The risk of anaesthesia is one of the biggest concerns for pet owners in having a dental procedure. Here is what we do toÂ minimise this risk:\n- Great care is taken in the pre-anaesthetic assessment of your pet\n- The Animal Referral Hospital is also an emergency / critical care centre and so is perfect for high risk patients\n- Monitoring of patients before, during and after anaesthesia is highly advanced\n- Each individual has an anaesthetic protocol catered specifically to their needs which takes into account any heart problems, body condition, age and concurrent disease\n- Specialist anaesthetists can be arranged at an extra charge\nPlease speak to Sydney Pet Dentistry directly if you have any concerns about your pet being anaesthetised.\nChristine is not a specialist veterinary dentist. She has done further study and training in small animal dentistry, and has limited her clinical practice to this field since 2007. Christine attained membership of the Veterinary Dentistry Chapter of the Australian College of Veterinary Scientists in 2006 – this involved undertaking rigorous written and oral examinations.\nThere are two registered specialist veterinary dentists in Australia, Dr David Clarke and Dr Stephen Coles, both are based in Melbourne, Victoria.\nComment on this below.\nPeriodontal disease is why we clean pets’ teeth\nPeriodontal disease is caused by plaque, a bacterial biofilm that adheres to the tooth surface (it is one of the only places you can actually see colonies of bacteria on your body!). If allowed to build up, plaque starts to accumulate and extend under the gumline, resulting in gingivitis and, over time, destruction of the ligaments and bone supporting the teeth (periodontal disease).\nHomecare programs are how we maintain oral health\nHome care programs are usually targeted at slowing down plaque accumulation, thus slowing down disease progression. They work best on a pain free and healthy mouth – one with no sore teeth and no plaque or tartar. No home care method will prevent plaque from forming altogether – even humans who brush their teeth twice a day get some buildup of plaque and tartar which needs to be removed by scaling and polishing.\nHome care does not replace the need for regular dental examinations and professional cleaning, but it will increase the time period between treatments. Before commencing a home care program, it is recommended that you ask your vet to check for any existing oral problems so these can be addressed up front. This makes home care more effective, and gives you peace of mind that you are not causing more discomfort if your pet has any painfulÂ or sensitive areas.\nMethods that may be recommended include those that physically remove plaque (such as brushing or chewing) or kill plaque bacteria (chemical rinses, gels or water additives). A combination of methods is most effective, and with a variety of options available, most pets can have a program designed that suits their individual needs and behaviour and fits in with your own lifestyle.\nHow do we brush pets’ teeth?\nTooth brushing is the most effective way of physically removing plaque from the teeth – that’s why human dentists recommend we brush twice a day! Spending a few minutes a day caring for your pet’s teeth can improve your pet’s health, improve their breath, make them more comfortable, and save you money on treatment.\nIf your pet will allow you to brush its teeth daily, you will significantly slow down the accumulation of plaque and, therefore, the discomfort and infection that follows.\nClick here for our ten top tips for brushing your pet’s teeth. We also strongly recommend that you take your pet to see your local vet who can detect and treat any painful teeth or gums that might already be present BEFORE you take up the toothbrush.\nOther homecare options\nDogs love to chew! Chewing has an abrasive action that helps remove plaque however it is important to offer something that is safe (not too small, hard or brittle) yet still effective. Cats tend to be are a bit more finicky than dogs, but can be convinced to chew if offered something interesting and tasty!\nBones are very popular in Australia and have the benefit of providing enjoyment and boredom relief. However they should be always be used with caution as there are some common complications:\n- Some pets will break their teeth on them, particularly the upper carnassials (large cheek teeth). This can lead to infection of the jawbone and tooth root abscess if left untreated.\n- Bones can cause gastrointestinal obstruction and trauma. Cooked bones should never be fed as they are brittle and prone to splintering. Match the size of the bone to the pet (ask your vet for zadvice if you are unsure) to try and minimise the risk of choking or obstruction.\nIn fact, the FDA (USA) has recently produced a consumer warning about the risks associated with feeding bones to dogs (click here for more information). The bottom line is that, while some pets may chew bones for years and never have a problem, others do get significant problems that require urgent veterinary attention. If, despite the risks, you choose to provide bones for your pet, you should supervise your pet and remove any bones if concerned about their chewing behaviour. Softer chewing options that are widely available include special dental diets and a range of chew treats and toys.\nAlthough dry foods may help disturb plaque, many are not particularly effective in slowing down periodontal disease as they shatter when bitten, and therefore do not require much actual chewing. However, some pet food companies have now released both canine and feline dental diets which have been scientifically proven to help reduce plaque and/or tartar buildup. These may work by physically cleaning the teeth (they do not fall apart easily when chewed) or by the addition of chemicals that prevent the hardening of plaque to form tartar.\nSome foods carry the Veterinary Oral Health Council (VOHC) seal which means they have been independently shown to be effective in reducing plaque and/or tartar accumulation. However, not all companies have sought to use this logo, so just because a product does not display it, this does not mean it is ineffective in slowing down periodontal disease.\nChoice of diet may be affected by other health issues, so it is a good idea to get professional advice on which diets are most suitable for your pet – the best place to start is your local vet hospital.\nChew treats and toys\nOther chewy options include specially designed dental chew treats (which may also be impregnated with chemicals that retard plaque growth), rawhides, pigs ears, and chew toys (such as rubber Kong toys). As with bones, careful consideration of size, hardness and brittleness is very important in minimising the risks. Hard toys (including some toy bones) should be avoided as they increase the risk of dental fractures. Tennis balls are very abrasive and can cause excessive tooth wear. As with bones, supervision of your pet while chewing is recommended.\nAntiseptics kill plaque bacteria, and are most effective when combined with a method that physically disrupts the plaque layer to allow them to penetrate properly (this is why dentists don’t recommend we use mouthwash alone as a substitute for brushing our teeth!). They are available in several forms, including rinses, gels, pastes and water additives. Cats in particular may find some of these offensive to their sensitive palates, so ask your local veterinary hospital for advice on which product is best suited to your pet.\nTalk to us!\nWe can give plenty of advice and would love to hear how you get on with cleaning your cat’s or dog’s teeth. Make a comment below.\nWhen dog teeth are in an abnormal position, they may or may not be causing pain or eating difficulties. If they are causing problems for the pet, there are generally three options that may be considered – extraction of the teeth, crown shortening to relieve trauma, or moving the teeth using orthodontic braces or plates. Orthodontics will not always be suitable for every case, but may be worth considering as it can be less invasive than extraction and allow relatively normal function to be maintained.\nIt should be noted that there are additional ethical issues to consider when weighing up the option of orthodontic treatment in animals that are used for showing or breeding.\nFeel free to contact us for advice on this or make a comment below.\nExtraction is a option for relieving pain and infection, and is therefore far more humane than allowing animals to suffer with a sore or unhealthy mouth. In many situations there are other treatment options available that can both relieve pain and infection, while preserving dental function.\nPet cats and dogs can live happy, healthy lives with missing teeth, or even no teeth. However, major teeth such as the canines (fangs) and carnassials (large cheek teeth) do serve important functions, both for chewing and in maintaining the position of the tongue and lips. Extraction of these teeth often requires aggressive removal of bone that can weaken the jawbone. Frequently, saving a tooth is less painful than extraction, and preserving dental function can maintain the pet’s current quality of life.\nIf periodontal disease is found early enough, many teeth can be saved with non-invasive treatments. Procedures such as root canal therapy or vital pulpotomy for fractured teeth are less painful and invasive for cats and dogs than extracting teeth with large, solid roots. Some painful orthodontic problems, where misplaced teeth are damaging the sensitive tissues of the mouth or other teeth, can be treated by orthodontic devices such as plates or braces, or reduction of the height of the offending teeth (vital pulpotomy).\nAdvanced tooth-preserving procedures are becoming more readily available for our pets, so it is worth considering all your options, including their benefits and drawbacks, prior to extraction.\nComment on dog root canal treatments below.\nThere is also a Position Statement on teeth cleaning in conscious dogs and cats on the AVDS website, which was written to draw attention to the need for general anaesthesia to allow a thorough dental examination and treatment in these species. It is the position of the AVDS that these procedures cannot be adequately performed in a conscious or sedated patient. Click here for further information.\nFeline resorptive lesions (otherwise known as feline odontoclastic resorptive lesions or neck lesions) are a very common cause of dental pain in our feline friends. In fact, studies have shown that once cats reach middle age, at least half of them will have one or more resorptive lesions near or under the gumline, with the number of lesions increasing with age. Similar lesions also occur in other species, including man and dogs, but are relatively rare.\nResorptive lesions start as small holes in the structure of the tooth, which progress in size until they cause large defects. They are not the same as caries (or cavities) in humans, which result from bacterial acids eroding the enamel (usually due to a sugary diet). Feline resorptive lesions are caused by the cat’s own cells (called odontoclasts) destroying the tooth from underneath the enamel. Affected teeth are very sensitive, and if the nerve is exposed they can be intensely painful. Often the crown of the tooth snaps off, leaving a painful retained root in the jaw. If your cat has unexplained â€œmissing teethâ€, they may well have been lost through this process.\nSigns that your cat may be suffering from resorptive lesions include reluctance to chew, chewing on one side, drooling, pawing at the mouth, lethargy or bad breath. However, many cats will not show their discomfort until the pain becomes unbearable. Visible defects in the tooth, localised inflammation, swelling of the gum to cover the lesion, or an area with increased buildup of tartar may be seen. Small lesions are often only detected under anaesthesia.\nTreatment usually involves extraction, as fillings are ineffective (the resorptive process simply continues inside the tooth structure underneath the filling). Dental radiographs are very useful in determining the extent of the lesions and the best course of action. Despite years of research, veterinary dentists still do not understand what exactly triggers the cat’s odontoclasts to attack its own teeth – this makes it hard to recommend any effective preventative measures.\nTell us about your cat’s experience with feline resorptive lesions. Comment below.\nBroken dog teeth (and cat teeth) should be assessed by your vet as soon as they are noticed. If the fracture is fairly small and does not expose the pulp (living tissue inside the tooth) there is a reasonable chance the tooth will survive. However, such teeth should be monitored closely as infection through the exposed dentine, or inflammation resulting from the injury may still cause the tooth to die in the following weeks. Sealing the exposed dentine can help protect the tooth\nfrom infection, this is a quick procedure and is done under anaesthesia.\nTeeth with exposed pulp will become infected and die if left untreated. In some cases, particularly in young animals with a very recent fracture, the tooth can be saved by a procedure called vital pulpotomy. However, in most cases root canal therapy or extraction are the best options. Doing nothing (‘wait and see’) is not a fair option for the animal as chronic\ninfection of the tooth and jaw will invariably occur.\nPlease comment on your cat or dog with a broken tooth below – we love to hear from people.']	['<urn:uuid:eaf42c36-4481-4275-8b85-e65a849ebb26>', '<urn:uuid:8ad2cff0-d6d1-48f6-8632-aa7be919fd5c>']	factoid	direct	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T18:05:20.075489	8	82	3473
28	I'm interested in coffee and being environmentally conscious. How are universities handling coffee waste on their campuses, and what innovative solutions exist globally for recycling used coffee grounds?	At the university level, there are initiatives like Darden School's Refreshing First Coffee project that aims to reduce waste from disposable coffee cups by transitioning to reusable mugs, helping reduce approximately 600 disposable cups per day. This initiative is part of their larger zero waste goal, which has already achieved a 59% waste reduction. On a global scale, innovative solutions for recycling used coffee grounds include their conversion into biofuels, biomass pellets, and barbecue coals, as demonstrated by companies like Bio-bean in London. Scientists have also discovered that used coffee grounds can be transformed into a material capable of storing methane gas when treated with potassium hydroxide and heated. Additionally, coffee grounds can be repurposed for various uses such as compost, de-icing material, hair dye, and skin exfoliator through initiatives like Ground to Ground in Melbourne, which connects cafes with customers who can reuse the grounds.	"[""|Courses in Sustainability: ||Yes|\n|Sustainability course is: ||Required|\n|Description: ||Business Ethics is a required first year course at Darden. Darden was one of the first business schools to incorporate such a requirement into the curriculum. The purpose of the course is to enable students to reason about the role of ethics in business administration in a complex, dynamic, global environment, often in the context of critical social and environmental challenges, and corporate social responsibility. Case discussions without easy answers focus on decision-making in situations involving multiple stakeholders, whether shareholders, employees, members of the local community, customers or governments. Students are encouraged to think deeply about the nature of business, the responsibilities of management, and how business and ethics can be put together. Students are pushed consider how they make decisions and develop their capacity to defend these decisions to other stakeholders. This is important as a way not only to foster integrity and responsible decision making, but also to push them to take leadership roles in dealing with complex and difficult choices they will face in their careers.\nThe Darden School of Business MBA program teaches that the role of business is to create value for all stakeholders. Integrated throughout our case-method curriculum is the theme that while business has one true bottom line, profitability, it must be achieved in an ethical, and socially- and environmentally-responsible manner.\nDarden is a general management program without formal majors. However, in choosing their second-year electives, students may opt for pursuing a concentrations, one of which is Innovation for Sustainability.\nDarden offers 20 elective courses that solely or partially focus on sustainability and ethical leadership/decision-making, and the role of business in society. Coursework includes tools essential for developing a successful sustainability strategy, such as stakeholder engagement and systems thinking. Specific courses are below. The majority of these courses also count toward the Innovation for Sustainability concentration [video: http://www.youtube.com/watch?v=clG32ObivqA&feature=youtu.be], which aims to:\n1.Equip students with the ability to create and execute collaborative sustainability strategies to: increase firm revenues through innovative products and services; lower costs and raise profitability through efficiencies and design; create and enhance brands; better manage supply chains; and mitigate business risk.\n2.Instill knowledge about the global and systemic impacts on natural systems and human well-being from accelerating trends such as urbanization, industrialization, climate change and population growth, as well as emerging scientific knowledge.\n3.Inform students of regional and global institutions and policy instruments that influence business operations and strategy.\n1. Business Ethics through Literature\n2. Business-Government Relations\n3. Climate Change: Science, Markets & Policy (taught at UVA Law School adjacent to our Grounds; Darden students may register)\n4. Creative Capitalism [video: http://www.youtube.com/watch?v=4VJMYu-_2TM&feature=youtu.be]\n5. Darden Capital Management: Rotunda Fund (Sustainability Investment Fund)\n6. Faith, Religion and Responsible Management Decision Making (includes segment on the environment with guest speaker, designer and architect Bill McDonough.\n7. Global Economics of Water\n8. Leadership and Diversity through Literature\n9. Leadership and Theater: Ethics, Innovation and Creativity [video: http://www.youtube.com/watch?v=6f13hoP1VBA]\n10. Leadership, Values and Ethics\n11. Markets in Human Hope\n12. National Debt\n13. Philosophy and Business: Business in Society\n14. Supply Chain Management\n15. Sustainability In-Depth: Studies in Innovation\n16. Sustainable Innovation and Entrepreneurship [video: http://www.youtube.com/watch?v=7ufR3VgdsSM&feature=youtu.be]\n17. Systems Design & Business Dynamics\nAdditionally, three of Darden's Global Business Experiences, one- or two-week academic trips to other countries that include classroom work as well as local company/NGO/government agency visits, are sustainability-themed, including Stockholm, Israel and South Africa.\n1. Sustainability, Innovation and Design in Scandinavia\n2. Israel: Entrepreneurship and Innovation in Israel\n3. Lessons from South Africa on the Need for Cooperation between the Public and Private Sectors to Grow an Economy\nCourses that are also recommended to enhance students’ ability to lead an organization effectively with an enterprise-wide perspective, capitalizing on sustainability risks and opportunities are:\n1. Innovation and Design Experience\n2. Leading Strategic Change\n3. Managing Innovation and Product Development\nDarden’s first year required coursework includes case discussions on sustainability and ethical leadership challenges. For example, in Strategy, students grapple with a case centered on BP’s evaluation of its long-term renewable energy portfolio. They must use strategy analysis tools to analyze what is best for the company in light of not only its core strengths and future projections, but also public perception after the oil spill.\nOther academic areas such as Accounting, Finance and Leading Organizations, similarly integrate cases addressing environmental and social issues.\nResearch Enhancing Curriculum\nDarden’s curriculum development is strengthened by five major research entities housed at Darden that conduct sustainability and ethics research geared toward both academic and business audiences.\n1. The Alliance for Research on Corporate Sustainability (ARCS)\n2. The Batten Institute for Entrepreneurship and Innovation\n3. The Business Roundtable Institute for Corporate Ethics\n4. The Institute for Business in Society (IBiS)\n5. The Olsson Center for Applied Ethics\nGiven that Darden is a case-method school, the case collection for Darden Business Publishing includes numerous cases written on the topic of sustainability. Examples include:\n1. Eastman Tritan: Case on launching a new BPA-free plastic and the associated supply challenges and opportunities. (Case was First Place winner of The Institute for Operations Research and the Management Sciences (INFORMS) Case and Materials Competition.)\n2. Government Incentives for the Electric Vehicle Industry: Case in which GM executives assess production capacity they should allocate to the Volt, in light of consumer trends, government incentives, and GM’s internal cost structure.\nDarden Business Publishing also has an online-store dedicated to cases on the environment and sustainability (http://store.darden.virginia.edu/environment-and-sustainability?orderby=15).|\n|Research Opportunities in Sustainability: ||Yes|\n|Description: ||In the 2013-2014 academic year, students’ for-credit research projects include:\n1.GreenBlue: create a business plan for a new service offering by an international non-profit focused on reducing packaging waste and chemical content in consumer products.\n2.Darden School of Business: write a case on new options for reducing paper use in course material distribution.\n3.Local Food Hub: Generate strategic recommendations for the organization’s business model as well as detailed market and financial analysis. The Local Food Hub is a nationally-recognized, revenue-generating, nonprofit food distributor focused on supporting small farm viability and improving access to local food in Central Virginia.\n4.Concordia Summit: Develop a three to five year strategic business plan, including operational and financial analyses, that will guide the organization’s transition from producer of events (i.e. the Summits) to a more established non-profit in the public-private partnerships area. The Concordia Summit convenes the world’s most prominent business, government and non-profit leaders on the subject of public-private partnerships.|\n|School's environmental commitment: || |\n|1: ||1.Refreshing First Coffee\nThrough our student-led Refreshing First Coffee project (http://refreshingfirstcoffee.wordpress.com/blog/), Darden is transitioning from using approximately 600 compostable coffee cups per day to using reusable mugs, overcoming the challenges of students, staff and faculty wanting to take cups of coffee to class and back to their desks. First Coffee is a core tradition in Darden’s tight-knit community, making this a highly visible behavior change effort. Students have dedicated countless hours to leading the charge, designing and piloting new processes. They wrote a grant and secured $10K in funds through U.Va.’s Greening Initiatives Funding Tomorrow (GIFT) grant program in order to support the purchase of reusable mugs for a large pilot of 300. They have effectively engaged through discussions and mini-pilots all stakeholders along the way, from Hospitality to Housekeeping, from students to faculty, to ensure a superior and efficient experience.\nStudent leaders have also done what is challenging in student life: to carefully and effectively build upon the efforts of their student leader predecessors who co-wrote a case Embedding Sustainability: Refreshing First Coffee at Darden, and subsequently organized a case competition in which all five first-year sections competed for the ever-popular Darden cup points. Taking the best ideas from each of the case presentations, the current student leaders have carefully investigated options and brought them to life.\nTheir efforts are instrumental in Darden’s effort to be zero waste to landfill by 2020. To date we have reduced our waste by 59%. Changing our signature community event, First Coffee, will help us take another large step to zero.|\n|2: ||2.Additional Progress Toward Zero Waste\nRelative to our 2007 baseline, by end of 2013 Darden reduced solid waste generation by 59%, due to composting and recycling. We estimate that organics comprise approximately 50% of our waste stream, due to operating a very busy restaurant. Thus, composting has been the primary lever in effecting this change.\nDarden also maintains a robust recycling program, which accepts e-waste, glass, metal, plastics #1-7 (including plastic bags, straws and utensils), paper and cardboard. Waste not separated into recycling bins goes first to a material recovery facility, where additional recyclables are removed, before the remainder is sent to the landfill. Our ultimate goal is to be “zero to landfill”.|\n|3: ||3.Progress toward Carbon Neutral\nRelative to our 2007 baseline, by end of 2013 Darden reduced our carbon emissions by 19%, even as our academic programs have expanded. This progress has been enabled by recommissioning our building systems, and has also resulted in significant avoided energy costs. To support our efforts, Darden is also a partner in the Better Business Challenge, a competition among 75 local businesses over 12 months to earn the greatest number of energy efficiency and waste reduction “points”. We have learned a great deal by helping design the Challenge, and sharing information with other businesses committed to achieving sustainability goals.|"", 'Global coffee production creates in excess of 23 million tons of waste per year, according to sustainability researcher Gunter Pauli, from the pulp of fresh coffee cherries through the packaging that brings the roasted beans to your favorite barista. At the consumer end of the supply chain, used coffee grounds are the most visible example of this waste, the bit we dump in the bin after making each fresh brew.\nMost coffee lovers don’t give much thought to their spent grounds, but these black, sodden remains of pulverized coffee seeds have inspired the freshly caffeinated imaginations of scientists, entrepreneurs and social innovators from Melbourne to London to Seoul, sprouting into ideas of real consequence for the coffee industry.\nSustainability and the Economy\nAcross the world, companies, researchers and engaged consumers are pioneering the next era of environmental sustainability. Enabled by policy changes and innovative technologies, a new paradigm known as the circular economy is emerging, one that holds the promise of reshaping the global economy and transforming our relationship with the natural world — the coffee tree very much included. The aim of the circular economy is to close the loop of our industrial system, reducing resource consumption and environmental pollution by transforming waste into input material for the next stage of production.\n“Sustainability has evolved significantly from the late 1990s to now,” says Nina Goodrich, executive director of GreenBlue, a nonprofit based in Charlottesville, Virginia, dedicated to the sustainable use of materials. “The most difficult aspect of sustainability has been integrating this into your business strategy, and the circular economy provides a better framework to help companies do that.”\nThe circular economy traces its origins back to the 1970s, but it is only in the past few years that the concept has taken off, driven in large part by the work of the U.K.-based Ellen MacArthur Foundation. As Goodrich explains it, a circular economy is based on the recognition that “we have to move away from what folks call our linear economy of take-make-waste and envision a next life for what currently is our waste. … It’s about how we build and make things so that one person’s waste becomes another person’s input materials.”\nOver the past several decades, nongovernmental organizations have gradually succeeded in persuading businesses to adopt a variety of sustainable practices that ultimately transformed the specialty coffee industry. In 2012, for example, 40 percent of global coffee production was produced in compliance with a voluntary sustainability standard, up from 15 percent in 2008, according to the State of Sustainability Initiatives, an organization that reports on global sustainability projects.\nAt the same time, the way for-profit businesses view sustainability has shifted. What began as a somewhat radical concept has evolved into a set of pragmatic managerial practices aligned with corporate objectives, something businesses of all sizes have, for the most part, embraced.\nTransitioning to a circular economy will be a bigger challenge, but just as the global specialty coffee sector pioneered the adoption of sustainable production practices, it now stands poised to lead the food sector in closing the loop of production to reduce resource costs and environmental damage. This concern for the environmental impact of coffee production isn’t just altruism or public spirit. Climate change and accompanying public concern present a real threat to the viability of the coffee industry, and consumers are increasingly concerned about the environmental and social impacts of the coffee they consume.\nClearly these challenges are multi-faceted and won’t be resolved by any single movement, no matter how expansive, but adopting a circular economic business model, and thus reducing waste and alleviating the environmental impacts of non-sustainably sourced input materials, could make a significant impact. Fortunately, scientists, entrepreneurs and coffee lovers around the world are creating innovative ways to move the industry forward.\nReduce, Reuse, Recycle\nShane Genziuk is founder of Ground to Ground, a social enterprise based in Melbourne, Australia, that educates coffee lovers about the myriad wonders of used coffee grounds and connects them to cafes that bag used grounds for easy pickup. He estimates he’s signed up nearly 1,000 cafes, working in partnership with a liates in London and Austin, Texas.\nGenziuk — who has a full-time job and a young family, and is completing a Ph.D. in business administration — runs Ground to Ground on passion and copious levels of caffeine. He is acutely aware of the waste that goes into making his favorite drink, and he wants to make a difference by leading a movement for social change. When you educate consumers about the environmental impact of coffee production, he says, they typically recognize their own contributions to the global problem and, in turn, are motivated to become part of the solution.\nGround to Ground is, at its heart, an effort to encourage greater recycling and reduce waste. Genziuk distributes educational material on his website and in person to cafes on the numerous and diverse uses for coffee grounds, such as compost, de-icing material, hair dye and skin exfoliator, among others. Participating cafes agree to bag used grounds and offer them to customers free of charge, helping to divert organic waste from the land fill while sparking greater awareness about sustainability and the impact of individual consumption on the environment.\nMoving Beyond Recycling\nAccording to Zhu Dajian, director of the Institute of Governance for Sustainable Development at Tongji University in Shanghai, initiatives like Ground to Ground are important, but insufficient as a response to global environmental challenges. What sets the circular economy apart is the imperative not just to reduce impact, but to create new value while doing so. Only in this way can the shift away from the linear economy be realized.\nDajian helped to create a circular economy strategy for the Chinese central government, which is struggling to manage the environmental damage produced by its rapid industrialization. The circular economy, he says, is “a new economic model that will reduce environmental impact and at the same time provide new jobs.”\nOne of the key ideas of the circular economy draws its inspiration from industrial ecology, which studies the flow of material and energy through industrial systems in a manner analogous to the natural ecosystem. Just as nature wastes nothing, recycling nutrients in a closed loop, so proponents of the circular economy aim to make use of all waste as input for further value creation, doing away with the idea of unwanted byproducts.\nIn recent years, there has been a surge of companies using coffee waste — both used grounds and discarded coffee cherry pulp — to create new products, including paper, our, 3D printer lament, charcoal, textiles and numerous others. One of the most widely touted examples is Bio-bean, a London firm that has attracted considerable media and investor attention for its plan to collect waste coffee grounds from the city’s cafes and convert them into biofuels. The concept is based on research from the University of Nevada, which analyzed used grounds for oil content and found they contained on average about 10 to 15 percent oil by weight.\nBio-bean is the first company in the world to industrialize this process, and recently opened a 20,000-square-foot factory in north London capable of processing 50,000 metric tons (about 55,000 tons) of used coffee grounds per year — about one-tenth the waste grounds from all the coffee consumed in the U.K. The factory is designed to turn waste coffee into biodiesel, barbecue coals and biomass pellets. The company is even exploring the possibility of selling these pellets back to coffee shops to be used to roast coffee or boil water, which would create a true circular economy, with waste becoming the input power for the production activities that created it.\nBiofuel produced from used coffee grounds is referred to as “second generation,” meaning it isn’t made from crops that could otherwise be used as food. In a world of food scarcity, where the production of fuel from corn and sugar cane has led to skyrocketing food prices for the world’s poorest, second generation biofuels are particularly promising.\nThe Challenges of Waste Collection\nBio-bean recently partnered with Network Rail, which owns and manages the U.K.’s railway network, to collect the coffee waste generated by its six biggest stations. These sorts of partnerships will be crucial as companies strive to create products from used coffee grounds, because unlike other agricultural commodities, where processing and waste production takes place in a central location, coffee grounds are used and disposed of in thousands of locations dispersed across a wide area.\nGenziuk says this has been one of the largest barriers to his own enterprise, and he’s not sure the economics can be resolved. For him, that’s not necessarily a deal-breaker.\n“Maybe the idea isn’t to get rich out of it,” he says, “it’s just to do the right thing by society.”\nOf course, the circular economy isn’t being promoted by organizations such as the World Economic Forum strictly because of its potential to do right by society; it is also a massive economic opportunity. A 2014 report by the global management consulting firm McKinsey & Company suggested the adoption of circular economic principles could lead to savings in materials costs exceeding $1 trillion a year by 2025.\nFor its part, bio-bean is doing its best to prove Genziuk wrong. In addition to partnering with Network Rail, the company has partnered with recycling company First Mile to run a coffee ground collection service, sending a van to pick up grounds from cafes in central London. The cafes give bio-bean the grounds free of charge, as they would otherwise have to pay disposal fees (which are based on weight) to have the grounds carted to land fills. To date, around 100 London cafes have signed up to have their coffee waste collected, saving them money and providing bio-bean with free input material for its production process.\nWhether or not this sort of collection method is scalable remains to be seen. London is one of the most densely populated cities in the developed world, making this method of collection relatively economical. With coffee being produced in thousands of different pots and machines throughout a city, the dream of putting all coffee waste to good use still faces enormous obstacles.\nChanging public policy could help. Genziuk notes that cafes in Melbourne pay for waste collection by the bin rather than by weight.\n“If they throw out 100 kilos, they pay the same as if they throw out 5 kilos,” he says. “In the U.K. it’s different. They pay by weight, and that influences behavior. Our system is geared up for volume of waste, and there’s no disincentive for generating waste, so they tend to have wasteful practices.”\nDajian agrees that governments can play a role in changing economic behavior and encouraging the adoption of circular economic practices.\n“If raw materials are cheaper than secondary materials, I think it’s not possible to make the linear economy into a closed loop,” he says. He advocates a tax on raw materials and subsidies for pilot projects and businesses that are putting circular economic models into practice and looking to scale.\nCoffee Grounds and Greenhouse Gasses\nBio-bean’s innovation has been to extract the value from the leftover oil and combustible material in used coffee grounds. Meanwhile, researchers in South Korea have found value in the absorbency that allows grounds to retain oil in the first place. In a recent paper in the scientific journal Nanotechnology, Christian Kemp and his fellow researchers at South Korea’s Ulsan National Institute of Science and Technology show that heating, or activating, used coffee grounds using potassium hydroxide (i.e., lye) creates a new material with high surface area and the ability to store large amounts of methane.\nMethane is a powerful greenhouse gas, 84 times more potent as a warming agent than carbon dioxide. When returned to room temperature, the material created from the used grounds is stable and retains methane, meaning it could be used for long-term storage of greenhouse gases produced in industrial processes.\nKemp is a South African physical chemist with a focus in material science. His primary interest is in environmentally friendly materials with catalytic or gas absorption properties. Scientists around the world in this field are using sophisticated production methods to create high-tech materials that can absorb and store the gases responsible for heating our atmosphere — including carbon dioxide and methane. These novel compounds have demonstrated breakthroughs in storing gas, but the starting materials are enormously expensive, which could limit their widespread adoption.\nKemp was working on one such project when he was forced to rethink his approach. Unsure of how to move forward, he did what many of us do: He sat down for a cup of coffee with his colleagues. The coffee worked its magic, and the scientists landed on the idea to try using the coffee grounds as their input material.\nThey combined the grounds (Kirkland brand, 100 percent Colombian coffee, dark roast, fine ground) with lye, then activated the mixture at 800 degrees C (1,472 degrees F) for an hour in a furnace. Turns out, that’s all it takes to make coffee capable of capturing the gas. The process is simple, Kemp says, and the cost for input material is next to nothing.\nResearchers have used other organic materials, including corn cobs and cocoa husks, to do something similar, but, Kemp says, “coffee is just so good at absorbing the moisture and absorbing the lye that the process was easier to do compared with other materials.”\nKemp also sees the material as a potential medium for storing methane in natural-gas-powered vehicles, which produce far fewer greenhouse gases than traditional vehicles. In cars currently running on natural gas, the gas is contained in a large metal tank in the trunk, a dangerous and heavy storage system. This limits the appeal, at least in the United States, for buses and heavy trucks.\nThe activated coffee material has the potential to remove these limitations. Because it stores methane at such low pressure, it doesn’t need to be shaped into a conventional round metal tube to prevent the gas from exploding. It can be molded into any shape desired, even eliminating the need for a gas tank.\n“We can build the gas storage into the contours of the car,” says Kemp. “Because it’s malleable, it doesn’t need to fit to strict dimensions.”\nThe findings already have attracted attention from companies interested in the potential application in lithium ion batteries, the technology that powers laptops and smartphones. Just as the nano-scale pores in the activated coffee grounds capture methane atoms, the material also can be used to capture lithium ions.\nFor Kemp, it’s all in keeping with his “one Earth” philosophy.\n“We only have one Earth,” he says, “so we should at least try our best to recycle what we have, and use it in an appropriate manner and not just waste as much as possible.”\nWhen it comes to coffee, Dajian notes that only a small fraction of the material used and produced by the industry ends up in the cup; the remainder, from the skin of the coffee cherry at origin to the grounds left after brewing, ends up as waste. Reducing this massive output of waste, and ultimately achieving a true circular economy, will take decades of work and the efforts of many — from entrepreneurs and scientists to coffee roasters and coffee lovers — but the tide does seem to be turning.']"	['<urn:uuid:4f42f042-7963-4b01-9600-d404c0efe674>', '<urn:uuid:7ea21c07-5f07-463a-9504-ecba9089bc4f>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T18:05:20.075489	28	146	4127
29	color blind contrast design game solutions	To ensure accessibility for colorblind players, extensive attention was given to maintaining readability of essential elements. This was achieved by creating visible contrast between colors, associated symbols, and backgrounds. The design process utilized a color-blindness simulation tool called Color Oracle, resulting in backgrounds that are relatively dark and less saturated than the main puzzle elements, striking a balance between legibility and immersion.	['She Remembered Caterpillars is a puzzle game where a set of coloured characters needs to be navigated through a maze of coloured obstacles. Each character can only cross certain obstacles while others obstruct its way.\nWhen I started working on this project, I was not yet aware of one of its major visual hurdles. At first I went the obvious way: bridges look like bridges, and walls look like walls. Character design could be anything but it would have to fit the world, of course. The results seemed promising. I could have stopped there. But I would not be writing this if I had.\nBut shortly thereafter, I came across an article about includification (a term coined by ablegamers) and how a game ideally should have options for those with colour-deficiencies. That made me think. Especially since there was a very early preview of our game on PCgamesN that started with the line,’this game is not for the colour-blind.’ After a bit of research, I realised that many games where the core mechanic uses a wide range of colours simply skip this part. But why? I wondered if it was that much more work.\nCould I redesign the characters and obstacles in such a way that even someone with a strong deficiency would be able to play the game? It was obvious to me from the start that including a colour shift option in the game would not work because the game features a spectrum of eight colours. This left one other solution: combine each colour with a specific symbol.\nWhile not a big challenge at first, it quickly dawned on me that not only did the symbols need to appear on the characters but also on each obstacle. What made all this even more difficult was the fact that in this game, players have the ability to merge two characters into a new colour which, in turn, meant that the symbols would have to explain this as well. And obviously it needed to be legible.\nOne might think that there is a wealth of information on this topic, given that 10% of humanity have a colour-deficiency of one form or another. However, apart from a few design articles I could not find anything that seemed helpful. Fortunately, I remembered some of my art education—specifically Wassily Kandinsky—and borrowed his way of associating basic shapes with primary colours. From there, I developed a set of matching merged symbols for the secondary colours. Consequently, black needed to be the combination of all of the symbols while white, the absence of colour (in this game), had to be a shape that was something else entirely.\nHowever, creating a game in such a way that a colourblind or colour-deficient person can play it without a lot of trouble is not solved by simply associating shapes and colours. Afterall it was always supposed to be a game with coloured backgrounds. Every detail, every colour, every movement creates an additional piece of information that needs to be processed and understood. For She Remembered Caterpillars this meant making sure that the essential elements needed to remain readable regardless of who was looking at them. This is the main reason why I tried to make sure that on all characters and obstacles there is always a visible contrast between the colour, the associated symbol, and the backgrounds. To achieve this I made extensive use of a colour-blindess simulation tool called Color Oracle. Below is composite of screenshots showing the differences.\nThe approach resulted in backgrounds that tend to be relatively dark and mostly not as saturated as the main puzzle elements. One of the biggest challenges for me was balancing the amount of detail. I tried for simpler backgrounds in an early version but they did not feel immersive enough for me. The result sometimes is (hopefully) a working trade-off somewhere between legibility and a good sense of place.\nand a final side by side comparison\nShe Remembered Caterpillars (caterpillar.solutions) is a fungipunk fantasy about love, loss, and holding on, told in the format of a colour-based puzzle game. A tale as the bond between parent and child, this lush and bewildering title will have players testing their wits against a variety of challenges, some devious, and others outright nefarious, but all beautiful and very, very strange.\nThe game is available on Steam, itch.io, and Humble Store.']	['<urn:uuid:30dfdc2d-e357-4995-baba-10dae462ab98>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T18:05:20.075489	6	62	729
30	When did Sarasate win his premier prix in violin?	He won the premier prix in violin and solfège in 1857, just one year after entering the Paris Conservatoire.	"[""Carmen Fantasy : Work information\n- Pablo (Martín Melitón) de Sarasate (y Navascuéz) ( Music, Images,)\n- Performed by\n- Aaron Rosand (Violin), Southwest German Radio Orchestra, Baden-Baden, Rolf Reinhardt (Conductor)\n- Work name\n- Carmen Fantasy\n- Work number\n- Op. 25\n- 1883-01-01 02:00:00\n- Recording date\nPablo (Martín Melitón) de Sarasate (y Navascuéz)\nPablo (Martín Melitón) de Sarasate (y Navascuéz) was born in Pamplona, in the Spanish province of Navarre, on 10 March 1844. The son of a military bandmaster, he began studying violin at the age of five and gave his first public performance at just eight years old at La Caruña. Such precocity resulted in the Condesa Espoz y Mina sponsoring him to study in Madrid with Manuel Rodríguez Saez. While there he became a favourite performer at the court of Queen Isabel II. It was she who in 1856, when Sarasate was only twelve, aided his parents in sending him to Paris to study with the famous teacher Delphin Alard, at the Paris Conservatoire. Just one year later he won the premier prix in violin and solfège, and he went on to be awarded a prize for harmony in 1859.\nThat year he began the concert tours which made his name famous in every country of Europe as well as in North and South America (1867–71 and 1889–90). While an early appearance in London in 1861 attracted little attention, later visits (between 1874 and 1878), playing at Philharmonic concerts and at such venues as Crystal Palace, resulted in his increasing popularity and led to many, more frequent, appearances. Indeed he began to attract the admiration and friendship of many famous composers, who began to dedicate works to him; at the Birmingham Festival of 1885 he played the concerto written for him by Alexander Mackenzie. Other works that were dedicated to him include Bruch 's Violin Concerto no.2 and his Scottish Fantasy, Saint-Saëns ' Concertos nos.1 and 3, and Introduction et Rondo capriccioso, Lalo 's Concerto in F minor and Symphonie espagnole, Joachim 's Variations for violin and orchestra, Wieniawski 's Concerto no.2 and Dvorák 's Mazurek (op.49). Many of these compositions have a Spanish 'flavour' attributable to the influence of Sarasate, who helped to popularise Spanish music throughout Europe through his playing, and his own compositions.\nSarasate's performances were noted for the unsurpassed sweetness and purity of tone, superb technique and perfect intonation. Writing about Sarasate's playing, Carl Flesch said that by ‘aesthetic moderation, euphony, and technical perfection … he represented a completely new type of violinist’, though he also criticized him for lacking some musical insight and emotional involvement, particularly in the more classical violin repertory. In spite of his virtuoso inclinations, Sarasate was also a keen string quartet player, both in private and in public chamber music performances.\nSarasate also achieved some fame as a composer of virtuoso violin music, creating more than fifty works, many with a strong Spanish heritage. Most well known amongst these are his Zigeunerweisen (1878), still a concert standard in the violin repertory, and the four books of Spanische Tänze for violin and piano (1878-1882) in which folk tunes are arranged.\nWhen Sarasate died of chronic bronchitis at his Biarritz home in 1908, he bequeathed his two Stradivari violins to museums: his favourite,dated 1724, to the Paris Conservatoire and the so-called ‘Boissier’, dated 1713, to the Madrid Conservatory.""]"	['<urn:uuid:10b54fbf-3404-4e6a-951f-d2842699d117>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T18:05:20.075489	9	19	561
31	How do LED lights help control a damaged drone?	Blinking LEDs mounted on the quadrocopter's arms indicate a virtual yaw angle, allowing the pilot to control the vehicle with the same remote control commands after a propeller failure.	['In this video update, we show that a quadrocopter can be safely piloted by hand after a motor fails, without the aid of a motion capture system. This follows our previous video, where we demonstrated how a complete propeller failure can be automatically detected, and that a quadrocopter can still maintain stable flight despite the complete loss of a propeller.\nIn the earlier video, we relied on an external motion capture system to measure the quadrocopter’s position and orientation. By moving more of the algorithm onto the vehicle, the quadrocopter can now be piloted by hand after the failure. The algorithm is executed on the quadrocopter’s onboard micro-controller, and the only sensors required are the quadrocopter’s angular rate gyroscopes. We use blinking LEDs, mounted on the quadrocopter’s arms, to indicate a virtual yaw angle, so that the pilot can control the vehicle with the same remote control commands after the failure. As an alternative to the LED system, an onboard magnetometer could be used to track the vehicle’s yaw angle. Alternatively, by using more sophisticated algorithms, the system could be made to work using only the rate gyroscopes.\nORIGINAL STORY 02/12/2013\nThe video in this article shows an automatic failsafe algorithm that allows a quadrocopter to gracefully cope with the loss of a propeller. The propeller was mounted without a nut, and thus eventually vibrates itself loose. The failure is detected automatically by the system, after which the vehicle recovers and returns to its original position. The vehicle finally executes a controlled, soft landing, on a user’s command.\nThe failsafe controller uses only hardware that is readily available on a standard quadrocopter, and could thus be implemented as an algorithmic-only upgrade to existing systems. Until now, the only way a multicopter could survive the loss of a propeller (or motor), is by having redundancy (e.g. hexacopters, octocopters). However, this redundancy comes at the cost of additional structural weight, reducing the vehicle’s useful payload. Using this technology, (more efficient) quadrocopters can be used in safety critical applications, because they still have the ability to gracefully recover from a motor/propeller failure.\nThe key functionality of the failsafe controller is a novel algorithm that I developed as part of my doctoral research at the Institute for Dynamic Systems and Control at ETH Zurich. This new approach allows such a vehicle to remain in flight despite the loss of one, two, or even three propellers. Having lost one (or more) propellers, the vehicle enters a continuous rotation — we then control the direction of this axis of rotation, and the total thrust that the vehicle produces, allowing us to control the vehicle’s acceleration and thus position.\nEven if the vehicle can no longer produce sufficient thrust to support its own weight, this technology would still be useful: one could, for example, try to minimize the multicopter’s velocity when it hits the ground, or steer the multicopter away from dangerous situations such as water, or people on the ground.\nThis control approach can also be applied to design novel flying vehicles — we will be releasing some related results soon.\nThis technology is patent pending.\nIf you liked this article, you may also be interested in:']	['<urn:uuid:c5be9241-b07d-475b-a7e7-bd94cc90f624>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T18:05:20.075489	9	29	531
32	smoke escape mechanisms medieval homes modern insulation	While Viking longhouses relied on a simple hole in the roof for smoke to escape, lacking chimneys and proper ventilation which often led to smoky interiors, modern insulation systems are far more sophisticated. Modern homes use various types of insulation materials and techniques that work by managing heat flow through convection, conduction, and radiation. The contrast is particularly stark in how they handle air circulation - Viking longhouses had poor air quality due to smoke accumulation, whereas modern insulation systems are designed with specific R-values and can be combined with proper ventilation systems for optimal indoor air quality.	['Longhouses are one of the most iconic relics of the Viking Age and Norse culture in general. They’re instantly recognizable, absolutely fascinating, and still garner a lot of attention and many curious questions.\nViking longhouses were large wooden buildings that were home to a few families. Multiple generations lived under one roof, and there wasn’t much privacy. Longhouses contained separate rooms for domestic animals. The people tried to make the houses as comfortable as possible and often held feasts.\nThis article explores the fascinating longhouses of the Viking Age. It presents 11 bite-sized facts that one must know if they’re interested in Norse culture.\n1. Viking Longhouses Were Made of Wood\nViking longhouses were mainly made of wood, an abundant material that was relatively easy to work with.  The roofs were sometimes covered by tufts, and walls were often lined with clay or mud for additional protection from the cold. Additionally, straw or moss would be stuffed between the walls for added insulation. \nLonghouses were made from planks vertically stuffed into the ground. There were also internal posts that supported the roof and held the whole building together.\n2. They Required Constant Repairs\nViking longhouses had to be repaired very often. The materials they were made of were often not sturdy enough to withstand the harsh weather conditions without sustaining damage. This was particularly the case in damp areas, where the humidity damaged the wooden walls, which had to be mended.\n3. They Were Crowded\nThe privacy that modern homes allow was not commonplace in the Middle Ages. It was especially not commonplace in the longhouses, which were mostly populated by more than one family and multiple generations.\nBigger houses often accommodated dozens of people, so they got rather crowded and offered little privacy. Life was communal, and people lived and worked together, most often in a single room.\n4. Vikings Kept Their Domestic Animals Inside With Them\nNorse homesteads usually didn’t contain separate buildings for domestic animals such as pigs or cattle. Instead, there were rooms for those animals inside the houses. This might seem awfully unhygienic to modern people, but it wasn’t so unusual in medieval Europe.\n5. They Were Smoky\nNorse longhouses did not have chimneys. The smoke escaped through a hole in the roof. This meant the houses were often filled with smoke if the smoke couldn’t escape. Those holes would also probably cause problems in rainy or snowy weather.\nThere was usually one big open fireplace used for cooking and keeping warm. People would often huddle near the fire to keep warm during the long cold Scandinavian nights.\nApart from there being no chimneys, there were also no windows. This must have made the longhouses very difficult to air out properly, adding to the aroma of smoke lingering inside them.\n6. They Were Lighted With Oil Lamps\nThe lack of windows meant that the only natural light that reached the houses came through the doors. This was not nearly sufficient, and the people living inside those houses needed artificial light.\nFor that purpose, they used oil lamps, which were not difficult to procure. These relied on fish oil or whale oil, which was abundant, considering that there were many fishermen and whalers in those coastal Scandinavian regions.\n7. The Vikings Tried To Make Their Houses Comfortable\nDespite the harsh living conditions, the Norse who occupied longhouses tried to make the houses as comfortable as possible. Wooden benches and beds were built next to the walls, and they were covered with skins for warmth, while the pillows were stuffed with feathers for additional comfort.\nRicher people would adorn their walls with shields, tapestries, rugs, and other decorations. Anything that could be used to decorate a longhouse was used to do so. Plus, this showed that the owner was wealthy and that the house was prestigious.\n8. Viking Longhouses Were Havens of Hospitality\nA big, long house that can host dozens of people is the perfect place for a feast. The Norse knew this and used this possibility to the maximum. Feasts were common, and there were rules on how to behave as a guest and toward guests, contrary to the idea many people have of the Norse as uncultured and rowdy barbarians.\nHospitality was also extended to weary travelers. Looking for shelter and food on a Norse homestead was normal, and the owner was expected to accept the travelers in his home.\n9. They Weren’t Just for Vikings\nStrictly speaking, the word “Viking” refers to the fierce seafaring warriors who conducted raids across Europe. The word comes from the Anglo-Saxon word wīċing, meaning simply “pirate” or “raider.” \nThis means that not every Norse person was a Viking. Most were common people, farmers, craftsmen, and traders, rather than raiders. However, longhouses were common dwellings for all of those common Norsemen.\nIn fact, longhouses were mostly farmhouses, which means that the people inhabiting them were mostly farmers and not fierce warriors. However, the word “Viking” colloquially designates all Norsemen, whether or not they took part in raids, so the houses were stuck with the name “Viking longhouses”.\nLonghouses were also common in Anglo-Saxon England, and they were quite similar to the ones in Scandinavia. They were also loved by different Germanic peoples across Europe, probably due to their simple design and practicality.\n10. Their Sizes Varied, But All Were Long\nThe longhouses varied in size, but the name definitely isn’t misleading. On average, they were 16.4 to 23 ft. (5 to 7 meters) wide and 49.2 to 246.1 ft.(15 to 75 meters) long.  Even for today’s standards, that would be a decently-sized house. With a bit of redecorating, an average longhouse could probably be turned into a decent modern family home.\n11. The Largest Discovered Longhouse Is Over 200 Feet Long\nThe biggest longhouse ever found was excavated in 1983 in Lofort, Norway. It was most likely a chieftain’s house, and it was initially 220 ft. (67 meters) long. It was built around 500 AD and upgraded around 700, extending it to 272 ft. (83 meters).\nAt the site, tourists can visit the Lofotr Viking Museum, which is a reconstruction of the longhouse. There are several more longhouses in the area waiting to be excavated, so they might find an even longer longhouse!', 'Insulation adds to the overall comfort and energy-efficiency of your home. How well it does its job depends largely on its r-value. Almost everyone has a basic idea of how insulation works. You place it in empty spaces between the walls to prevent air from passing through. It’s the mechanisms behind this that make it a little more complex.\nUnderstanding Convection, Conduction, and Radiation\nConvection, conduction, and radiation are the three basic mechanisms of heat flow.\n- Convection – refers to the circulation of heat through liquids and gases. It explains why warm air rises while cooler air remains low in your home.\n- 2. Conduction – describes the way heat moves through solid objects.\n- Radiation – radiant heat always travels in a straight line. It heats solid objects along the way that are capable of absorbing its energy.\nMost insulation materials are designed to slow conductive heat flow. It also slows convective heat flow, although to a lesser extent. You can also buy radiant barriers or reflective insulation that reduce radiant heat absorption by reflecting heat away instead of absorbing it.\nIn any setting, heat will flow from the warmer area to one that is cooler until it reaches the same temperature. In your home, that means that heated air inside will flow into adjacent areas that are cooler such as garages, basements, attics, and outdoors. This is why an insulated garage door can help make your entire home more comfortable and fuel-efficient. During the summer when air is cooled indoors, heat comes into your home. Insulation can be used to keep warm air indoors or outdoors.\nWhat Is an R-Value?\nR-value is the measurement of an insulating material’s thermal resistance, or ability to resist heat flow. The higher the R-value, the better it is at insulating. The R-value is assigned based on a number of factors including the type of material it is, the thickness, and its density. Some types of insulation have R-values based on temperature, moisture accumulation, and aging. Insulation that is made from layers of various materials are rated according to the R-values of each individual layer.\nChoosing Insulation for Your House\nThe more insulation you install in your home, the greater the R-value will be. Not all homes require the same degree of insulation. You need to consider the climate where you live, the type of heating and cooling used, and which areas you need to insulate.\nTypes of Insulation\nThere are many types of insulation on the market for you to choose from. You’ll need to know where you want to install the insulation and the R-value before you choose insulation. The proper installation makes a huge difference in whether the insulation performs to its R-Value. Some are suitable as DIY projects while you need to hire professionals to install others. The basic types of insulation are:\n- Blanket: Batts & Rolls – Made from mineral wool, plastic, or natural fibers and used in unfinished walls, floors, and ceilings.\n- Concrete Block Insulation – Includes foam board place on the outside of walls in new construction and on the inside of walls of existing homes. Good for unfinished walls and foundation walls.\n- Foam Board (Rigid Foam)– Made from polystyrene, polyisocyanurate, or polyurethane. Used in unfinished walls, floors and ceilings, and unvented low-slope roofs.\n- Insulating Concrete Forms (ICFs)– Foam boards or blocks used in unfinished walls for new construction.\n- Loose–Fill and Blown-In – Made from fiberglass, cellulose, or mineral wool. Used in enclosed existing or new wall cavities, unfinished attic floors, hard-to-reach areas.\n- Reflective System– This includes foil-faced kraft paper, polyethylene bubbles, plastic film, or cardboard. Used in unfinished walls, ceilings, and floors. A reflective system really doesn’t have an R-value due to the way it reflects heat flow instead of absorbing it.\n- Rigid Fibrous or Fiber Insulation– Made of fiberglass or mineral wool. Used in ducts of unconditioned spaces and other places where insulation is needed to withstand high temperatures.\n- Sprayed Foam and Foamed-In-Place – Materials include cementitious, phenolic, polyisocyanurate, and polyurethane. Used in enclosed existing walls, open new wall cavities, and unfinished attic floors.\n- Structural Insulated Panels (SIPs)– Made of foam board or liquid, foam insulation core, and straw core insulation. Used in unfinished walls, floors, ceilings, and roofs for new construction.\nOf these types, blanket insulation is the most commonly used type of insulation, most often in fiberglass. This type of insulation comes in widths that are suited for use in walls and attics, making self-application easier. When using insulation with a lower R-value than needed, this type of insulation can be used in more than one layer.\nDraft-Proof Windows and Doors\nInsulating your home starts with selecting the right insulation for all the empty spaces where air gets in or out. Draft-proofing all of your doors and windows helps even more. Weather stripping is an inexpensive and easy way to block drafts around windows and doors. Don’t overlook places like pipes that lead outside or old door sweeps.\nGarage doors are frequently overlooked when considering the overall energy-efficiency of a home. The fact is that it’s often the largest opening to the home. Whether your garage is heated or not, an insulated door can help stop the transfer of air between your home and the outdoors. There are some beautiful choices in insulated garage doors available today. They also come in a range of R-values, typically between R-8 and R-32.\nIf your garage door still has some life left in it, another option is to insulate your door. A simple DIY project could result in a more comfortable home and significant savings on your energy bill. If your door is outdated and in need of repair, it might be time to start shopping for an energy-efficient replacement.\nContact Coastal Garage Doors to get a free estimate and get an honest quote for a new insulated garage door. There’s a lot to love about insulated doors including greater durability, quieter operation, and better protection for your car. A warmer garage is more convenient and comfortable, even if you’re just passing through!']	['<urn:uuid:f9f95b57-a4a0-4fc6-b953-ed45a66a377b>', '<urn:uuid:e06c8e68-8627-438b-b4c9-902b3d113382>']	open-ended	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T18:05:20.075489	7	98	2063
33	What are palm oil's uses and its environmental sustainability challenges?	Palm oil has diverse uses including cooking oil, cosmetics, pharmaceuticals, bio-fuel and bio-lubricants. However, there are critical environmental challenges that the Palm Oil Innovation Group (POIG) aims to address, particularly breaking the link between palm oil expansion and deforestation, preventing peat clearance, and ensuring greenhouse gas accountability.	['Oil palm (Elaeis guineensis Jacq.), is a native of West Africa and popularly known as African oil palm or red oil palm.\nIt is known to be the highest edible oil yielding perennial crop. It produces two distinct oils, i.e., palm oil and palm kernel oil. Palm oil is derived from fleshy mesocarp of the fruit, which contains about 45-55% of oil. The palm kernel oil, obtained from the kernel of stony seed, is a potential source of lauric oil. Oil palm is the crop of the present and future vegetable oil economy of world as well as India.\nPalm oil has good consumer acceptance as cooking medium because of its price advantage. It is a good raw material for manufacturing oleo chemicals used in making soaps, candles, plasticizers etc. It has also a variety of uses, ranging from edible oil, cosmetics, pharmaceuticals to bio-fuel and bio-lubricant.\nIt is grown extensively in South-East Asian countries, (Malaysia, Indonesia and Papua New Guinea), African countries, (Nigeria, Ivory Coast, Ghana, Liberia, Sierra Leone, Cameroon, Republic of Congo and Zarie) and South American countries (Costa Rica, Panama, Columbia, British Guyana, Peru, Ecuador, Venezuela and Brazil). Malaysia, Indonesia and Nigeria are the leading producers of oil palm.\nOil palm was introduced to India at National Royal Botanical Gardens, Kolkata during the year 1886. The Maharashtra Association for Cultivation of Sciences (MACS), Pune later introduced African dura palms along canal bunds, home gardens and, to some extent, in forest lands near Pune during 1947 to 1959. Large scale planting of oil palm was launched from 1971 to 1984 in Kerala by Plantation Corporation of Kerala Ltd. (subsequently taken over by Oil Pal India Ltd.) and Andaman Forest and Plantation Development Corporation Ltd., in Little Andaman Islands of Andaman and Nicobar Islands during 1976 to 1985.\nIn the year 2020, the Reassessment Committee of ICAR-IIOPR has conducted a study to assess the potential area of Oil palm cultivation in the country including North-Eastern States and the Andaman and Nicobar Islands. As per the report of Reassessment Committee, a total 22 States have been identified with 27.99 lakh ha as having a potential area for Oil Palm cultivation in India.\nOil palm is a humid crop. Requires evenly distributed rainfall of 150mm/ month or 2500-4000mm/annum. Rainfall distribution in India is not even and adequate. Hence grow oil palm under assured irrigation conditions by adopting recommended practices. Crop comes up well between 29-33oC max. and 22-24oC min. temperatures and with bright sunlight for at least 5 hrs. per day. Humidity of more than 80% is required to come up well.\nBest-suited soils are moist, well-drained, deep, loamy alluvial soils, rich in organic matter with good water permeability. At least one-meter depth of soil is required. Avoid highly alkaline, highly saline, waterlogged and coastal sandy soils.\nTenera is the ruling hybrid and it is a cross between thick-shelled Dura and shell less Pisifera. Tenera has a thin shell, medium to high mesocarp content and high oil content.\nBest season for planting is June-December i.e., during monsoon. In case of planting during summer, adequate irrigation, mulching and growing cover crops like sun hemp in the basin would help in avoiding hot winds during summer. 12 -14 months old healthy seedlings with 1-1.3m height and 13 functional leaves are recommended for planting. While planting, 143 plants per hectare should be maintained with a spacing of 9m x 9m x 9m (triangular planting). Planting should be done in pit size of 60 cm x 60 cm x 60 cm (length, breadth and depth).\nApply 250g Di Ammonium Phosphate or 400g Single Super Phosphate, 50g Phorate and mix with the soil at the base of the pit. Immediately after planting, form basin and give copious irrigation.\nOil palm requires sufficient irrigation, as it is a fast growing crop with high productivity and biomass production. Do not grow oil palm if assured and adequate irrigation facility is not available. For grown up yielding palms of 3 years age and above, a minimum of 150 to 200 liters of water per day is required. However, in older plantations during hot summer this quantity may be increased up to 300 lit.\nBasin method of irrigation is to be taken up when irrigation water is not a constraint. Required quantity of water is to be given at 4-5 days interval. Prepare irrigation channels in such a way that the individual palms are connected separately by sub-channel. For light soils, frequent irrigation with less water to be given. In heavy soils, irrigation interval can be longer.\nDrip or Microjet irrigation method is practiced. If land is of undulated terrain, drip or micro sprinkler irrigation can be advantageous. If drip irrigation is installed, four drippers are to be placed for each palm. If each dripper discharges 8 liters of water per hour, 5 hr. of irrigation per day is sufficient to discharge 160 lit/day. In case of micro sprinklers (180o or 360o ) one each on either side of the palm can be installed. Drippers/jets should be periodically checked for proper discharge. Basins should be adequately mulched and covered with soil, which will help to conserve moisture.\nOil palm is a gross feeder and demands a balanced and adequate supply of macro, secondary and micronutrients for growth and yield. It is advised to apply fertilizers at every three months interval.\nFertilizer requirement of oil palm:\nFour equal split doses of fertilizers are to be applied starting from June/July at three month interval. For the newly planted crop, the first dose of fertilizer needs to be applied three months after planting. Add 50-100 kg FYM or 100kg green manure per palm along with the second dose of fertilizer application. Five kg neem cake/palm can also be applied. Broadcast the fertilizers around the clean-weeded basin, about 50 cm away from the palm base and incorporate into the soil with the help of fork. Irrigate the palms immediately after fertilizer application.\nTo calculate fertilizer for a crop, click here.\nDuring first year, basins of 1-m radius, second year 2- m radius, and the third year 3- m radius are to be taken around the palm by removing; the soil from inside so that the soil will not accumulate at the collar region. Basin area of oil palm represents its active root zone. Hence it must be kept clean and weed free to avoid competition for nutrients and water.\nTake up regular weeding manually or with the use of only recommended herbicides. Use preferably contact herbicides. Glyphosate (750ml/ha/ year or 17.5 ml/basin) is recommended for effective weed control. Herbicide mixtures of Paraquat with Atrazine, Monuron and Diuron sprayed on ground, twice a year can control the weeds, effectively.\nOil palm is a wide spaced perennial crop with a long juvenile period of 3 years. Inter and intra row space can be used to generate income during the juvenile phase of the crop. Inter crop selected should be compatible with the main crop and should not compete with oil palm for light, water and nutrients. Any remunerative crop can be grown, but the most suitable crops are vegetables, banana, flowers, tobacco, chillies, turmeric, ginger, pineapple etc. While growing inter crops in mature oil palm gardens of 8- 12 years age or palms attained a height of 3 meters, intercrops should be able to grow under partially shaded conditions and should not compete with oil palm for water, sunlight and nutrients (eg. cocoa, pepper, heliconia and ginger lilly).\nDo not cut the oil palm fronds. Do not tie oil palm fronds close to the stem for inter-cropping, which will reduce photosynthetic activity. Do not plough close to the palm base, which will cut the absorbing roots and thereby reduce intake of water and nutrients. Maximum number of green leaves should be retained on the palm.\nOil palm comes to flowering 14-18 months after planting. It produces both male and female flowers separately on the same palm. Male and female phases do occur naturally in consequent cycles in a palm.\nAblation is the removal of male and female flowers produced in the early stages of plantation. This enables the plant to gain adequate stem girth, vigour and develop adequate root system. Flowering starts from 14th to 18th month after planting. Start ablation immediately after the appearance of inflorescences on the palms. They can be removed easily by hand pulling or using the tool developed at ICAR-Indian Institute of Oil Palm Research. Ablation can be extended up to 2-1/2 to 3 years depending upon the plant growth and vigour.\nOil palm is a highly cross-pollinated crop. Wind and insects assist pollination, but wind pollination is not adequate. Effective pollinating insects like Elaeidobius kamerunicus helps in good pollination and fruit set. Release of this weevil after 2-1/2 year of planting is advisable. If the plants are not having good girth and vigour, release the weevils after 3 years.\nMulching of oil palm basins is essential to conserve moisture as well as to control weeds. Mulching can be done with dried leaves, male flowers, coconut husk, empty bunches etc.\nWhile harvesting a stalk length of 5 cm alone should be left. Harvesting should be done at 10-12 days interval. During rainy season, harvesting should be done at closer interval of 6-7 days as ripening is hastened after hot summer. In young plantations, we get more bunches with less bunch weight and in adult plantations the bunch weight is more but the bunch number is less.\nAt yield stabilizing period (4-8 years) : 12t/ha\nAt yield stabilized period (>8 years) : 20t/ha\nProduction technology of coconut and other oil see...\nProvides information about National Mission on Edi...\nThis topic covers information about crop managemen...', 'Jakarta – A group of progressive palm oil companies together with environmental and social NGOs today announced their intention to push the boundaries of current requirements of the Roundtable on Sustainable Palm Oil (RSPO). The group says that the new RSPO standards agreed by RSPO members at its meeting in Kuala Lumpur last April do not go far enough to adequatelyaddress the most critical issues facing the industry today.\nThe Palm Oil Innovation Group (POIG) released the following joint statement at the Tropical Forest Alliance meeting in Jakarta:\n“We are innovators in one or more aspects of palm oil production; and we are NGOs active in environmental and social issues and together we wish to show what responsible palm oil production could look like. We hope others who want to tackle the most burning issues facing our industry will join us.\nThe current negative image of palm oil needs to be reversed if the sector is to thrive. The palm oil innovation group will demonstrate that by setting and implementing ambitious standards that stretch the RSPO principles and criteria we can break the link between deforestation and social conflict and palm oil.\nOne of our objectives is to develop a Palm Oil Innovation Group Charter1 and to explore ways to increase the market demand for palm oil products that are produced by innovators within the industry. It’s not an easy challenge, but this is a call for other innovators to join us.”\nPalm Oil Innovations Group\nThe POIG aims to build on RSPO standards and commitments by both demonstrating innovation to implement RSPO existing standards but also on additional critical issues. With a focus on the three thematic areas of environmental responsibility, partnerships with communities, and corporate and product integrity, POIG members will strengthen their commitments to socially and environmentally responsible palm oil production.\nThe POIG is a constructive group that focuses on leveraging its experience to innovate the palm oil industry and act as advocates for these innovations. The current negative image of palm oil needs to be reversed if the industry is to thrive. The Palm Oil Innovation Group will demonstrate that by setting and implementing ambitious standards, the industry can in particular break the link between deforestation, and human, land and labour rights violations, and palm oil.\nObjectives and Purpose:\n• promotes and supports innovation and improvements in oil palm plantation and extraction mill management on a range of environmental, social and governance issues, and seeks to have the innovations rolled out across the palm oil industry and reflected in the RSPO standard.\n• creates added value for innovative and progressive producers through increased market recognition and demand for palm oil products from innovative and improved practices.\n• acts as a forum for open discussions and sharing of experience with innovations and improved oil palm plantation and extraction mill management practices.\n• acts as a forum to collectively engage with governments towards achieving recognition and support for innovations, and have them included in regulations and law.\nPOIG Charter Elements:\n1. Environmental Responsibility:\n1.1 Breaking the link between Palm Oil Expansion and Deforestation: Forest protection through conserving and restoring High Conservation Value (HCV) and High Carbon Stock (HCS) areas:\n1.2 No peat clearance and maintenance of peat lands:\n1.3 Greenhouse gas (GHG) Accountability:\n1.4 Pesticides use minimization:\n1.5 Chemical fertilizer:\n1.6 GMOs prohibition:\n1.7 Water accountability:\n1.8 Protect and conserve wildlife:\n2. Partnerships with Communities\n2.1 Comprehensive Free, Prior and Informed Consent (FPIC) of indigenous peoples and local communities:\n2.2 Food Security:\n2.3 Effective Conflict resolution:\n2.4 Social Conditions:\n2.5 Workers Rights:\n2.6 Support Palm Oil Small Holders:\n3 Corporate and Product Integrity\n3.1 Transparency and Corruption:\n3.2 Traceability and Segregation:\n3.3 Report on Social, Labour and Environmental Performance:\n3.4 RSPO Certification and Company Operations:\nRainforest Action Network runs hard-hitting campaigns to break North America’s fossil fuels addiction, protect endangered forests and Indigenous rights, and stop destructive investments around the world through education, grassroots organizing, and non-violent direct action. For more information, please visit: www.ran.org']	['<urn:uuid:ccdfa325-ab00-41d9-8568-198a3c311c86>', '<urn:uuid:40b00ee1-02fa-4ba5-a727-3c5497b7e290>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:05:20.075489	10	47	2280
34	chok1sv gs knockout cell line characteristics	The CHOK1SV GS Knockout (KO) host cell line is derived from Lonza's suspension adapted CHOK1SV host cell line. In this GS-KO cell line, both alleles of the endogenous glutamine synthetase gene have been knocked out, which creates a requirement for exogenous glutamine. The data shows good comparability between this new cell line and the original CHOK1SV cell line.	"['Since its introduction to the market in 1992, continuous advancements have been made to all components of Lonza’s industry-leading GS System™. Most recently, improvements to the commercial platform process, host cell line and vectors have resulted in the development of our new GS Xceed™ Gene Expression System. The new system produces high product titers in a robust, chemically defined, animal component-free (CDACF) environment and requires no MSX for stability. GS Xceed™ had been designed with future manufacturability in mind.\nThe GS Xceed™ System includes the CHOK1SV GS Knockout (KO) host cell line, which is a derivative of Lonza’s suspension adapted CHOK1SV host cell line. In this new GS-KO cell line, both alleles of the endogenous glutamine synthetase gene have been “knocked out”, leading to a requirement for exogenous glutamine. All characteristics of the new CHOK1SV GS-KO cell line show good comparability data to our original CHOK1SV cell line.\nFor fast and convenient cloning of GS vectors encoding antibodies, we offer a range of vectors which include the robust viral promoter, mCMV and contain gene-optimized antibody constant regions. The system also includes full protocols, access to Lonza’s Version 8 (v8) Media and Feeds CDACF Platform and technical support from Lonza’s expert GS Team.\nXceed Your Expectations and Reduce Timelines\nCell line construction, selection and development are critical milestones on the path to first in human studies. The new GS Xceed™ Expression System creates cell lines up to 6 weeks faster than the original GS System™.\nTime savings is achieved by:\n- Quicker outgrowth of transfectant pools during selection\n- Reduced time for screening and subculture due to no addition of inhibitor (MSX) after GS selection which also encourages improved cell growth and faster doubling times\n- Fast adaption into Lonza\'s commercial process\nHigh Titers and Worldwide Availability\nThe GS Xceed™ System has been shown to achieve titers of up to 6g/L using Lonza’s v8 Media and Feed Platform Process. Our GS technical specialists have developed this process with your future scale-up needs in mind. In addition, we offer a full line of Cell Line Construction Services using Lonza’s new GS Xceed™ System for mammalian protein production.\nThis advanced expression platform will now be available for use globally in all major pharmaceutical and biotechnology markets, including Asia.\nFor more information on Lonza’s continuous improvement strategies for improved expression speed and efficiency, view Dr. James Rance’s presentation from IBC’s Cell Line Development and Engineering Conference entitled, ""Accelerated Antibody Process Development. ""\nPrinciples of the GS Xceed™ System\nLonza’s GS Xceed™ System is based on the use of the glutamine synthetase (GS) gene (GLUL) as a dominant selectable marker to complement a glutamine auxotrophy caused by a lack of endogenous GS in CHOK1SV GS-KO cells. Synthesis of glutamine is catalysed by the GS enzyme (glutamate‑ammonia ligase) which utilizes glutamic acid, ammonia and ATP. Glutamine has multiple roles in cell metabolism, particularly as an energy source, protein constituent and as a nitrogen donor in nucleotide biosynthesis. Therefore, cells that lack a GS gene (e.g. CHOK1SV GS‑KO and NS0 host cell lines) have a requirement for exogenous glutamine. The GS gene used in the GS Xceed™ System expression vectors, was cloned from CHO cells and is supplied as part of the system components.']"	['<urn:uuid:3eae44a2-d4ed-4788-904a-88c01d3f9150>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T18:05:20.075489	6	58	537
35	single seizure vs recurring seizures medical transport	A single seizure does not necessarily indicate epilepsy, as epilepsy is defined as the tendency to have recurrent seizures. While a single seizure may allow driving after 5 years (if no further risk is demonstrated), recurring seizures (epilepsy) require a 10-year seizure-free period without medication for professional transport licenses. This distinction is crucial for PSV/PCV licensing, as these higher medical standards exist because drivers handle larger vehicles carrying passengers, making safety paramount.	"['by Harry Ingledew (Newcastle Medical Student)\nwith the occasional interruption from me\nWhat are seizures? – an Archie intro\nPut simply, and I am a simple person, a seizure is an electrical surge in the cortex – or at least that is one way to look at it. Basically, if I strap EEG leads onto your head, there will be a screen full of electrical squiggles (see below). Usually, they are all over the place and you have to have a different sort of degree, and a better quality EEG than mine, to make sense of them.\nI used to think the seizure was a chaos of activity but, actually, if you look at the tracing above, it is sort of the opposite. It’s almost as if suddenly, the complexity of cortical impulses, with all their nano-nuances, has been replaced by a rhythmical, coordinated electrical march across the brain. Little wonder that, when this happens, people behave oddly.\nTurns out, we function best when our brains are all over the place!\nWatch these two videos that illustrate how order can erupt from chaos. The first still perplexes me but I try to imagine, as the nails in the box line up, the same thing happening to some folk in the build up to a seizure.\nThe second is 2 of my kids on the swing. I should really have anchored it a bit better! When they swing out of phase, all is fine. When they swing in phase, it’s like the frame is about to tear out of the ground.\nSo, the deal is that there are loads of reasons for people to have seizures, and there are loads of different types of seizures. It can get pretty confusing if you are not careful. At it’s core, however, the concept if simple.\nThe next step is getting a handle on the different types of seizures that you might meet in your career as a doctor.\nTypes of seizure (over to you Harry)\nRight, so now we all know how to differentiate between a seizure, syncope, or non-epileptic attack (at least in theory), we can delve deeper into the types of seizure that patients can present with.\nThere are 2 main categories into which seizures can fall: focal and generalised.\nFocal seizures occur due to abnormal electrical activity that is restricted to a certain area of the brain – this can be a small area of cortex or extend to encompass much larger areas. By definition, it never involves the whole of the cortex, however.\nFocal seizures are subdivided into those in which the person does and does not retain awareness (as opposed to generalised seizures in which patients always lose awareness). Therefore, if a patient has any memory of their seizure it must have been, at least initially, focal.\nthis is a really crucial point and often glossed over. if you ask the right questions of your patient, and the witness, you can often not just diagnose the seizure but also whether it has a focal onset.\nFocal seizures in which the patient experiences a consistent, stereotyped warning each time may be due to a structural lesion. It is therefore useful to understand how focal seizures in each of the lobes present, as it can help to locate a lesion before even seeing a scan.\nConversely, if the warning changes with each seizure, it becomes less likely that a structural lesion is the source of the problem.\nFocal seizures can present with a myriad of weirdness; the type of weirdness depends on the area of the brain in which the electrical activity is occurring.\n- Frontal – the frontal lobe is tasked with a wide array of functions including being home to the motor centres and the control of behaviour and personality. Seizures in this area therefore include motor phenomena and changes in behaviour.\n- Parietal – the parietal lobe is mostly concerned with understanding sensory input; therefore, parietal lobe seizures may include feelings of warmth / tingling down one side of the body.\n- Occipital – seizures with a visual onset, often explosions of light or colours. Like a migraine aura but much shorter and more dramatic.\n- Temporal – seizures of the temporal lobes present with lots of very strange behaviour called automatisms which include actions such as lip-smacking, chewing, and blinking. As the temporal lobe is involved in memory, strange phenomena such as déjà vu (the feeling of having already experienced the current situation) and jamais vu (a feeling that the situation is very unfamiliar) may also occur.\nFocal seizures often remain focal, but can progress and become generalised.\nYou can see an example of a focal seizure in the lecture I mentioned at the top of the page.\nGeneralised seizures occur due to abnormal electrical activity that engulfs the entire brain, and is not restricted to a single hemisphere. Patients typically do not receive any warning of an incoming generalised seizure and, after the event, they will have no memory of what occurred (other than feeling terrible).\nThere are many types of generalised seizure, but there are 2 key types to learn:\nAbsence – previously called “petit mal”, and often still referred to as this by patients. When was the last time you found yourself daydreaming? When we daydream, which for most people occurs every day, we are often described as being “absent”. This is very different however, from an absence.\nAn absence is a type of generalised seizure in which the patient suffers a brief loss of awareness. Absence seizures almost always begin in childhood and the classic story is of a child who will completely stop talking in a conversation, stare blankly for 5-10 seconds and then “3, 2, 1, they’re back in the room”, continuing where they left off with no awareness that they just had a seizure.\nHave a look at these two videos. They show how brief the attacks can be. Hyperventilation can bring them on and this is often used during EEG recordings to increase the diagnostic yield.\nGeneralised tonic-clonic (grand mal) – This is what most people think of when they think of a seizure. But what does it actually mean? Well, lets break the name down. As we said earlier, “generalised” means the entire brain is affected, the patient gets no warning, and no memory after the event. First there is a “tonic” phase in which the person will go completely stiff and fall to the ground. Then comes the “clonic” phase, in which all 4 limbs will begin to convulse violently, often resulting in injury to themselves and people around them.\nIf you see one of these as a doctor, try to make sure that you describe it in as much detail as possible. It\'s really annoying if people just write ""GTC x1"" in the notes. Far better to say ""patient sent stiff, arched neck and back, let out a tight cry, arms extended, legs extended, stiff for 30 seconds, not responding, then jerking of arms and legs, blue lips, low sats, then cessation over 60 secs, gradual recovery over 5-10 minutes but confused and agitated"". The second description helps me in the next day, and possibly forever more!\nHere is a good example of a generalised tonic-clonic seizure. Notice all the different phases of the attack (and how calm the nurses are – no need for a crash call!). The head turn might suggest a focal onset, just for the record, but with very rapid generalisation.\nIt is possible to make it a tonne more complicated, and there is an relentless desire to keep reclassifying things, seemingly just when everyone had got the hang of the last classification.\nCurrently, this is the ""official"" International League Against Epilepsy classification (2017). Bet they reclassify it again soon, just to keep us on our toes.\n|Focal seizures affect one part of the brain, generalised affect both hemispheres|\n|If they get a warning or have a memory it is focal|\n|Focal seizures may be due to a structural lesion, particularly if the warning is consistent|\n|The symptoms of a focal seizure help to locate the problem|\n|Being absent is different from an absence seizure|\nEpilepsy – what is it?\nSo, we have spoken a lot about the types of seizure people can have, but we haven’t actually mentioned epilepsy at all. This is because simply having a seizure doesn’t necessarily mean it is epilepsy. Epilepsy is the tendency to have recurrent seizures.\nEpilepsy is very common, affecting around 600,000 people in the United Kingdom.\nEpilepsy can occur for a vast array of reasons, but the exact cause is not found in many. Underlying brain diseases that can result in epilepsy include:\n- a genetic predisposition\n- brain injury – stroke, trauma, bleed, AVM\n- developmental abnormalities\nOlder age at onset of epilepsy increases the likelihood that the cause is a structural lesion. These patients will receive a CT or MRI scan to rule this out – preferably an MRI in this day and age.\nThere are many types of primary epilepsy syndrome but, unless you’re a consultant neurologist (in which case you wouldn’t need me to explain any of this to you), there is no reason for you to learn them all. There is one, however, that is worth learning a little bit about – Juvenile Myoclonic Epilepsy (JME).\nthis is because within JME are lessons for how to approach the history, examination and investigation of all patients with epilepsy\n- Onset is in adolescence (<30 years of age)\n- Everyone with JME has myoclonic jerks (rapid, short-lived, uncontrolled movements), particularly when tired, typically when first awaking in the morning. They often report dropping their breakfast or spilling their tea soon after waking up. This is different from the physiological twitches and jerks many of us experience as we drift off to sleep, so please don’t go self-diagnosing with JME.\n- Generalised tonic-clonic seizures are also common in JME\n- Patients with JME often also have absence seizures\n- JME can be diagnosed via an EEG as it has a very classical trace (more about this later)\nThe diagnosis that a blackout event is a seizure, rather than syncope, is often a clinical decision. A history of recurrent seizures is therefore indicative of epilepsy. It is important, however, to try and establish the cause of the epilepsy to rule out any serious, underling brain disease.\nDespite the array of complex, often unnecessary, neurological tests that are often all too tempting to order straight away, when a patient presents with any kind of blackout the most important first test that should always be done is an ECG, to check for common cardiac problems that cause blackouts. A full cardiac assessment, including a 24-hour ECG trace and an echo may also be necessary.\nScans of the brain such as CT and MRI can be used to look for structural problems in the brain that may be causing the seizures. A consistent warning indicates the seizure is focal, meaning there is a focus, therefore potentially a lesion that could be identified on a scan.\nTo use one of Dr Archibald’s favourite phrases, an EEG (Electroencephalogram) “is as useful as a chocolate teapot” in most situations.\nIf a patient has presented with what clinically sounds like a frontal lobe seizure, and an MRI is done showing a tumour in the frontal lobe, an EEG isn’t going to add anything useful to the scenario.\nThere are, however, times when an EEG is useful. Some primary epilepsy syndromes, such as JME, have classic EEG signatures. As a rule, the younger the patient, the more likely the EEG will help.\nThere is an endless list of potential treatments and medications for epilepsy and understanding them all would be incredibly complicated – so don’t bother!\nThe good news is that for many patients with epilepsy, a single drug can stop their attacks. It can get much more difficult after that, but there are reasons for optimism when starting out, particularly if investigations are reassuring (no structural cause on MRI).\nMany anti-epileptic medications have side-effects, including confusion, headache, tremors and weight gain. As well as these some are teratogenic, causing foetal malformation if taken in pregnancy. Medications like sodium valproate should be avoided in all women of childbearing in age, because of this, and there is a concerted effort to ensure this is the case in the UK.\nThere is, however, a nice simple rule that will help you choose the right medication for your patient, if you find yourself in the rare situation of needing to start a patient on anti-epileptic medication as a junior doctor.\n“If it doesn’t begin with L, then don’t prescribe it”\nLamotrigine and Levetiracetam (commonly called Keppra) tend to be best tolerated and are often effective for both focal and generalised seizures – they are great all-rounders. They are not without their own problems, of course, but this is a decent rule of thumb.\nThe management of epilepsy during pregnancy is a balancing act. Too much medication increases the risk of foetal malformations, but reducing the medication too far increases the risk of having seizures, which in-turn can also cause damage to both mother and foetus.\nit is beyond the scope of this article to cover a definitive guide to epilepsy management, and better brains than mine are required. I will put some links that you might find helpful at the bottom.\nStatus epilepticus (SE) is a medical emergency in which a seizure lasts more than 5 minutes, or there is incomplete recovery between subsequent seizures. Previously, SE was defined as a seizure lasting more than 30 minutes, but this was changed in order to ensure earlier treatment. The earlier you can intervene the better – seizures beget seizures so letting them run too long means they are harder to stop, and have worse effects on the brain itself.\nSE either occurs due to pre-existing brain disease or a new change in the brain. It is therefore vital to find the underlying cause in a patient without a diagnosis that could explain the SE.\nPrompt management with a benzodiazepine, such as IV lorazepam is vital, preferably as soon as SE is suspected. Buccal midazolam is often given pre-hospital by paramedics. If benzos fail to control the seizure, IV phenytoin should be used but, hopefully, senior help has arrived by then and as a junior doctor you shouldn’t need to give this. Often, if the phenytoin doesn’t work then your patient is on their way to ICU.\ndon\'t fiddle about with crappy little doses of iv medication in status. if you are gonna load, LOAD! As a rule of thumb, most adults with decent kidneys and livers will need around 1.5g of iv something (phenytoin, valproate, levetiracetam). It doesn\'t really matter what (as long as it\'s not iv bleach, Donald)\nThis link to the NICE guidelines isn’t pretty, but it does cover the standard approach in the UK.\nThe SIGN guidelines are often a bit more readable. This link will take you there.\nSudden unexpected death in epilepsy (SUDEP)\nIf well managed, most people with epilepsy will have a normal life expectancy. Sometimes, however, a person with epilepsy may die during or soon after a seizure for no apparent reason. SUDEP most commonly occurs while a person is asleep. Around 1/1000 patients with epilepsy may die unexpectedly.\nSUDEP tends to be more common in those who experience frequent GTC seizures and those whose epilepsy is poorly-controlled. Better seizure control, with good compliance to an appropriate medication regimen, is therefore the best way to prevent SUDEP.\nDon’t worry about this, but I thought it was worth having an awareness that many neurological disorders can cause problems with driving and require notification of the DVLA.\nWhen someone has a blackout of any kind, fulfillment of all of the 3P’s are the key to being allowed to continue to drive. They indicate that the event was almost certainly provoked syncope, rather than an unprovoked seizure, making driving much safer.\nMake sure you get the 3 P’s straight in your head, and in the notes, and you will be well on your way.\n- Posture – the blackout occurred during a change in posture, such as rapidly standing up from a chair, or a prolonged period of standing\n- Prodrome – the event was preceded by feeling warm, nauseous, and unwell\n- Precipitating factors – the event happened at a time of pain, fear, or stress, such as when having blood taken\nDriving is a crucial consideration after a seizure or blackout. Always check if your patient drives and, if they do, tell them they need to stop until they have spoken to the DVLA. Remember, it is the DVLA that ultimately license us to drive - not you. You must be clear in both your advice and your documentation of this advice.\nThis link takes you to the DVLA guidelines. It is often helpful to look at these with your patient, so they know what to expect.\nUseful references and articles\nUseful article on the first fit, and how to approach it.\nThese “Bare Essentials” articles always age well and this is no exception. Worth a read.\nA tour de force in the Lancet – you will need your best library access for this one I’m afraid – they are mean at the Lancet!', 'Medical Requirements and Examination\nIn order to get your provisional PSV/PCV licence you will need to to get a Medical Examination Report (D4) filled in by a doctor to ensure you meet the DVLA Group 2 medical standards and then you will need to send it off with your D2 application form in order to get your provisional licence.\nThe D4 form is applicable for the following vehicle licenses and classes:\n- vehicle over 3.5 and 7.5 tonne, medium or Large Goods Vehicle (LGV) (C1, C1+E, C or C+E)\n- passenger service vehicle (PSV) / passenger Carrying Vehicle (PCV) over 8 passenger seats (D1, D1+E, D or D+E).\nThe D4 form must have been filled in within 4 months of your application to ensure that your health is still at the same level.\nMedical standards required for drivers of coaches and buses are higher than those required for car drivers due to the fact that you will be dealing will larger and potential more dangerous vehicles. If you’re of good health then the medical shouldn’t be a problem but if you have any of the following medical conditions you will not be able to hold a driving licence for Passenger Service Vehicles (PSV) over 8 seats.\nEpilepsy or liability to epileptic attacks – If you’ve had epilepsy in the past then you will need to be free of further epileptic attack without taking anti-epilepsy medication for 10 years before you will be considered for a licence. If you’ve only had one single epileptic attack or seizure then you may be entitled to drive after 5 years as long as you can satisfy that there is no further risk\nDiabetes – If you are an insulin treated diabetic you may NOT obtain a PSV licence UNLESS you held a HGV/PSV licence valid on 1 April 1991 and the Traffic Commissioner who issued the licence had knowledge of your insulin treatment priotr to 1 January 1991.\nEyesight – You must be able to read in good light with glasses or contact lenses if necessary, a number plate at 20.5 meters/67 feet (57mm characters) or 20 meters/65 feet (50mm characters). There are also other requirements that applicants must have related to visual acuity, the binocular field, and monocular vision and these will be discussed in your medical\nOther medical conditions\nYou are likely to be refused a PSV/PCV driving licence if you are affected by any of the following and cannot meet the recommended medical guidelines.\n- Within 3 months of a coronary artery bypass graft (CABG).\n- Angina, heart failure or cardiac arrhythmia which remain uncontrolled.\n- Implanted cardiac defibrillator.\n- Hypertension where the blood pressure is persistently 180 systolic or more and/or 100 diastolic or more.\n- A stroke or Transient Ischaemic Attack (TIA) within the last 12 months.\n- Unexplained loss of consciousness with liability to recurrence.\n- Meniere’s Disease, or any other sudden and disabling vertigo within the past 1 year, with a liability to recurrence.\n- Difficulty in communicating by telephone in an emergency.\n- Major brain surgery and/or recent severe head injury with serious continuing after effects.\n- Parkinson’s disease, multiple sclerosis or other chronic neurological disorders with symptoms likely to affect safe driving.\n- Psychotic illness, in the past 3 years.\n- Serious psychiatric illness.\n- If major psychotropic or neuroleptic medication is being taken.\n- Alcohol and/or drug misuse in the past 1 year or alcohol and/or drug dependency in the past 3 years.\n- Any malignant condition in the last 2 years, with a significant liability to metastasise (spread) to the brain.\n- Any other serious medical condition likely to affect the safe driving of a passenger carrying vehicle.\nYou can get more information about the medical standards and other requirements from The Drivers Medical Group, DVLA Swansea SA99 1TU or by phoning 0300 790 6807.\nArranging your appointment\nIf you have any doubts about your eligibility to apply for a PSV licence then it’s worth discussing this with your doctor/optician before you get the D4 form filled ni as this will be for free on the NHS where as you will have to pay £80 for getting the form filled in regardless of whether you pass or fail the medical.\nYou must arrange an appointment for a full medical examination from a doctor registered or practising in UK or in any other EC/EEA country. Getting your own doctor or GP to fill this form in will speed up the process as they will have information on your past medical history.\nThe form which you can download here has sections specifically for your doctor to fill in but you will be required to fill in Section 9 and Section 10 on page 8 of the D4 when you are with your doctor.\nThere is no specific YES/NO section on the form deciding whether you are fit to drive a passenger service vehicle as this is decided by the DVLA once they recieve your forms. Please note that the doctor’s signature will be valid for only four months so send off your D4 form, D2 application form, and your driving licence as soon as you can.\nIf you have any queries please phone 0300 790 6807 between 8am and 5.30pm Monday to Friday and Saturday 8am and 1pm.']"	['<urn:uuid:2b3a7bb2-e964-41e4-9d81-12a8b0289095>', '<urn:uuid:71f0c71c-9be2-4633-9a67-19d59a7beddc>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T18:05:20.075489	7	72	3780
36	I'm a contractor who works with floor installation and I'd like to know what causes moisture problems in new floors and how important is adhesive application in preventing installation failures?	Moisture problems in new floors are increasing due to several factors in modern construction methods. Up to 15% more water is being added to concrete mixes for improved flowability and strength. Chemical additives used to strengthen concrete slow down the drying process, and power floated surfaces trap moisture within the screed or slab. Fast-track building requirements compound these issues, often resulting in wet screeds. As for adhesive application, it's crucial for preventing failures. The adhesive must be applied correctly using the proper trowel at the right angle for correct coverage, and installation should only proceed within the recommended working time. The substrate must be clean, dry, and flat, as any contaminants like mud or drywall compound can cause the adhesive to peel away. Additionally, proper moisture testing using probe tests is critical before installation, as surface moisture alone doesn't provide accurate readings.	['Is below surface moisture a growing and problem? If so, why?\nDenver Coleman from Polyflor explains. With more than 50 years’ experience in the local and international flooring industry, Denver Coleman, Chairman of Polyflor SA, answers questions posed by installers, architects and readers. In this issue, he discusses the prevalence of below-surface moisture with solutions to circumvent this challenge.\n“Is it true that there is increasing evidence of moisture in subfloors? What are some of the reasons and how can moisture problems be avoided?” – James Kennedy, Kimberley\nBelow-surface moisture affects most types of flooring, including ceramic or porcelain tiles. In recent years there has indeed been increasing evidence of moisture in subfloors. One of the reasons why increased moisture is found is due to the fact that up to 15% more water is added to current concrete mixes for improved flowability and strength. Chemical additives are also increasingly being used to strengthen and harden the concrete, but this retards the drying process, while power floated surfaces also tend to slow down the dehydration and trap moisture within the screed or slab.\nAll these factors, compounded by today’s fast track building requirements, often result in wet screeds. Below-surface moisture can cause installation failures due to the effect that it has on the bond of the adhesive, which can emulsify and lose its grip strength when mixed with water. It may also cause delamination of a self-leveling product from the subfloor.\nMoisture testing, using correct testing methods, has therefore become of critical importance for all floors prior to installation. Subfloors need to be tested with a probe test, as surface testing alone will not provide an accurate indication of moisture. This is especially important if the plastic DPM is compromised. Results documented show very different readings between surface test methods and probe test equipment and international standards advise a minimum of 40% below surface test.\nWhen testing for below-surface moisture, it is also important to document the date, equipment used and reading obtained (preferably photographically) in site minutes. The contractor, client and specifying professional need to be informed of these readings and photographic evidence will also ensure an accurate record should there be any queries later on.\nWhere old vinyl is replaced with new vinyl, screeds can still have residual moisture which did not affect the previous installation. Remember that building methods were slower 25 years ago, giving surfaces sufficient time to dry. Older adhesives used years ago also had more solvents in them, making them slightly more resistant to moisture. Slower building methods gave them a chance to become fully cured before coming into contact with rising moisture, whilst using good quality wood floated screeds ensured the screed dried efficiently. As a result, the flooring was installed on a drier surface, so a fully cured and better adhesive bond was achieved.\nGiven that the moisture trapped under the screed has become more common and problematic, precautions need to be taken. To this end, a surface-applied damp proof membrane is advisable. If there are other influencing factors, such as marsh ground conditions, slopes on the upside of the project or perhaps paving or concrete parking areas where soak away is limited, it would be necessary to take additional precautions. These precautions will ensure the surface and/or underground water is diverted around the building and it avoids water traveling below the structure, which may cause problems in rainy seasons.\nIn many projects, clients might be reluctant to spend money or claim insufficient budget for products such as self-leveling screed applications and surface applied moisture or damp proof barriers. But, have they considered how much a delay in completing the project would cost due to a wet screed in terms of loss of turnover, rental, or delay in getting patients into a new hospital or care facility? For example, a delay in filling hospital beds can be anything between R5 000 – R10 000 per day. Multiply this over a period of 8-10 weeks, and it could cost the customer around half a million Rand, aside from any of the other costs incurred such as loss of theatre hours etc.\nIt is therefore clear that a delay and loss of income will far outweigh the cost of an effective liquid surface applied moisture barrier, and will be dramatically less than replacement costs in case of an installation failure.', 'Adhesives work, failure of the adhesive itself due to incorrect manufacture is extremely infrequent. If a bad batch gets out the manufacturer will be informed quickly. It is in the best interest of the manufacturer to take care of the issues promptly. All the manufacturers I know will do this to minimize the adverse fallout.\nNew formulation technology may be the one area where we have to be wary of some claims. However, in order to progress we have to give these new adhesives a chance. We’ve come a long way since the hot tar days. If we hadn’t tried new formulations we wouldn’t be where we are today with the high performance materials available. Most manufacturers introduce new products to a controlled set of knowledgeable users, and expand and modify the product as success is achieved. From these successes the products are made available to the industry.\nThe main issue with adhesives occurs most often down stream from the manufacturer. The issues typically involve common sense issues such as--- choosing the correct adhesive for the specific application; reviewing site conditions for acceptance; correcting inadequacies; and following the instructions of application. Let’s review the basics of using a wood flooring adhesive.\nWhat are some of the items to consider when choosing an adhesive for an application? Area environment can affect the application performance of the adhesive. For instance, moisture curing urethanes may not set up quickly enough in very dry environments and may set too quickly in very wet environments. Adhesives with a water component may affect factory finished flooring by causing cupping. Some adhesives may not be suitable for applications over radiant heating systems. This does not mean these products cannot be used under these circumstances; but, application technique may need to be adjusted for a particular application or environment. Check with the manufacturer.\nThe primary function of an adhesive is to affix the flooring to a substrate with a degree of permanency. This means the adhesive must perform in the typical environmental conditions of the area and keep the flooring in place for its expected useful life. To accomplish this, the contractor is responsible for the acceptance of the substrate.\nHow many times do we have to say the substrate must be, CLEAN, DRY, AND FLAT? I continue to get consumer comments and see inspection reports where the floor is loose and the adhesive has peeled away from the substrate because some “stuff” was on it and simply not cleaned off. Mud, drywall compound, and/or old cut back mastic, etc. are not structural components of the subfloor. They must be removed.\nThe next consumer comment is that my floor is cupping. What was the moisture reading of the substrate at installation? Unfortunately, more often than not, the answer is: “The slab looked and felt dry; it was OK,” or “The subfloor was where it should be.” And, flat? “I walked it and it was flat enough.” The flooring moisture content was “OK”. Meaning no tests were performed but it all looked good.\nGive the flooring a chance to perform as it should. As standard procedure on every wood flooring job, clean the substrate; check moisture conditions by an approved test method before installation; and measure the variation in flatness. Then assess the results. Are they acceptable? Unless conditions are acceptable don’t begin installation. Just because the flooring was delivered that day or the builder said it had to be installed today shouldn’t force you to begin installation if there are problem jobsite conditions. If the conditions are not acceptable, inform the responsible parties of the deficiency and the likely result of the deficiency. Follow the conversation with a written note or email to all the interested parties reviewing the situation.\nAnother adhesive function is to firmly attach the flooring thus minimizing movement of the flooring itself. That is, the adhesive must be strong enough to hold the flooring under any reasonable environmental condition. Flooring is wood and will expand and shrink seasonally. The adhesive must accommodate this horizontal movement while keeping any vertical movement to a minimum. The actual application of the adhesive is critical to this aspect of performance.\nRead all the directions for application. Look at the pictures. When site conditions are acceptable, begin installation. Select the proper trowel. Spread adhesive at the proper trowel angle for correct coverage. Spread only as far as you can install during the recommended working time. Use the proper technique for placing the flooring in the adhesive. Don’t “plop-n-slide.” Every now and then pull up a piece of installed flooring to check for proper coverage and adhesion to both the flooring and the substrate. Don’t install boards that are too crooked or warped. Protect the installed flooring from abuse. Clean extra adhesive promptly with the correct cleaner. Don’t wipe the excess with a rag you have already used to wipe other areas.\nRemember, if the adhesive is not the expected color or consistency check with the distributor for issues that may be related to freeze thaw cycles or age. Don’t use questionable adhesive. A replacement costs you at least twice as much as the original. Adhesive is not a leveling compound, don’t fill the depression with adhesive. Too much adhesive affects the cure rate and later foot traffic can loosen the bond. This leads us to keep any people off the floor until the adhesive has set. Don’t cover the flooring with a material that slows or inhibits the off gassing or cure of the adhesive. If you put a lid on top an incomplete cure can result affecting overall performance.\nWhere solid wood is to be placed over a wooden subfloor, NOFMA recommends fastening with mechanical fasteners, cleats or staples, unless specifically directed by the manufacturer to use adhesive. The typical purpose for gluing a solid wood product to a wooden subfloor is to reduce noises where the flooring has a too loose tongue and groove fit. The issue with this procedure is that when repairing an area or removing a problem board, the subfloor is significantly damaged. The irregularities created can be very difficult to flatten when the repaired boards are replaced. This damage can also affect the integrity of the subfloor system if damage is excessive.\nFinal thoughts; adhesive belongs on the bottom of the flooring not on the face. Customers should expect a clean floor. The occasional hollow sounding area is considered “OK” as long as the performance of the flooring is not affected. The flooring should not move, pop, or creak in these areas. When trying a new product, don’t select a 3,000- square-foot job as a test floor. Select the small application so you can spend the extra time and attention required and if a problem occurs, the repair cost will not break the bank.']	['<urn:uuid:e2e99ed8-4529-4aa5-85b7-e47c914b11aa>', '<urn:uuid:d5e8dfdc-7dea-4166-84ad-d620dc742e95>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:05:20.075489	30	142	1856
37	What tools and materials do I need to start doing needlework?	For needlework, you'll need several essential tools including: 6-inch plastic and wooden embroidery hoops, embroidery needles (like Clover's gold ones), water soluble pen, pinking shears, embroidery scissors, Fabri-Tac fabric glue, Thread Heaven conditioner, steam iron, light tablet, and a printer. For materials, you'll need six-stranded embroidery floss and cotton or linen fabric. The fabric choice depends on the type of needlework - for cross stitch you can use Aida fabric (10-18 count), Hardanger (22 count), or Linen. For embroidery, you can use cotton-poly blend, while for Crewel you'll need linen or canvas to prevent wool yarn from puckering.	['Eli asked me to make a post about the different types of needlework so when you are out thrifting you can recognize what to buy and what keywords to use when listing to sell on the different platforms. I have done better on eBay with selling needlework kits of every type because of the international buyers.\nI will also cover in another post the best types of needlework kits or patterns to buy.\nFirst I will cover what is used with what looks like a sewing needle. This leaves out Crochet, Knitting and other forms using other needles. Â I will cover those in another post.\n1 Counted Cross Stitch – Counted Cross stitch today is known mostly as cross stitch. Â Cross stitch is done on a fabric with squares. Â The pattern is on a piece of paper and you count the squares from the pattern to the fabric. (sounds hard but it is really easy). Â Kits come with different counts of fabric. Â Aida is from 10 to 18 count (squares per lineal inch) Â Hardanger is 22 count. Â Linen is used in cross stitch and most of the time is stitched over two threads (example 28 count linen stitched over two threads would make the stitch count 14). I love the look of Linen over aida because you don’t see the squares. Â NOTE: Make sure you read our next blog about buying completed finished needlework and what to look for. Â\n2. Stamped Cross Stitch – Stamped is cross stitch that is traced onto fabric, usually a cotton poly blend nowadays. Â There are squares you that you cross over with the floss. Â These squares are normally mixed in with lines to embroidery. Â You can do fancy stitches or straight stitches.\n3. Embroidery – Embroidery is stitched on fabric that has had the design traced onto it. Â You can do a number of fancy stitches. This is what you see on dish towels. My grandmother had towels for every day of the week. Â I always checked to see if she changed them daily….. and SHE DID. Â Embroidery is stitched with six stand floss. You separate the threads into one or two strands depending on the look you want.\n4. CrewelÂ Embroidery Â orÂ Crewelwork –, is a decorative form of surface embroideryÂ that uses wool yarn (acrylic is used sometimes)Â and a variety of different fancy stitchesÂ that follows a design on the fabric. Â Crewel is usually stitched on a linen or canvas so that the wool does not pucker. Â Make sure if you have kits that have part of the canvas silkscreened that you point this out. Â (Photo 3 is silkscreened partially)\n5. Hardanger – this has been called Hardanger embroidery but it is a counted form since you are still working in the blocks of the fabric. Â Traditionally is it worked on white cloth with white thread but today I see it in all colors. It is worked on even blocks counting and using drawn thread techniques. Â If you see any finished … just grab and send to me.. I collect it. Â You will see this in doiles, tablecloths, runners etc. Â It is really elaborate and if you find patterns they usually are good sellers but not big money. Â The art is still so limited on who has the patience to do it.\n6. Needlepoint – Needlepoint is stitched on a mesh canvas that is painted on the canvas. Â You will be doing a half cross. There is also counted needlepoint where you would have an empty canvas and use patterns on paper to count similar to cross stitch. Â You will see Mesh 12 or Mesh 14. This is how many squares are in a lineal inch. Â (Needlepoint is my least favorite of any of the needle arts.) It is usually done with yarn. Â Many new designers are using smaller mesh like 18 (squares per inch) and using floss for a different look. Â I like the smoother look as it is not as bulky.\n7. Petit (Petite) Point – Petit Point and Needlepoint are very similar in that they use canvas to stitch on and the same format but they are very different. Â Petit point is just that…. you make tiny tiny stitches and usually stitch on single threads of Penelope Canvas or fine needlepoint canvas or Congress cloth which is 24 squares per inch. Â Many people use Petit point to describe very fine cross stitch but since you are not crossing the full cross the term is used incorrectly.\n8.Â Plastic Canvas – Plastic canvas is basically needlepoint done on plastic mesh. Â You will see this used to make 3-d type items such as doll houses, magnets, photo frames etc. You cut out the designs after you have stitched and either attach to the edges to sew them together or finish off and cut the plastic so it i is not seen.\n9.Â Smocking – I am covering smocking here because there are a lot of kits out there to make pillows with smocking. It is basically pleating and can be seen in the tops of dresses and on bonnets. There are two different types, English and Modern.\n10.Â BrazilianÂ Embroidery – This is the one art I cannot tackle well. It is stunning and gorgeous but I don’t have the patience. Â It is mostly flowers done in silk or Rayon Threads.\nNO COUNT CROSS STITCH is the design silk screened on aida or linen with openings to do stitching to embellish the design. Â This is different from Stamped Cross stitch where on stamped you have the design on the plain fabric in little blue squares and you stitch those squares.\nThere are other forms of needlework that include drawn thread, schwalm, blackwork etc but the above types are what you are going to see the most of in kits when you are shopping.', 'TOOLS AND MATERIALS FOR EMBROIDERY\nIntroduction: Tools and Materials for Embroidery\nIve split these into categories – required and optional! The required lists are the basics you need to complete the embroidery projects in the class. The optional lists concern tools and materials that are nice to have, or for patternmaking and finishing your embroideries, but they are not necessary.\nRequired Tools Needed:\n- 6 inch plastic embroidery hoop\n- 6 inch wooden embroidery hoops (depending on how many of the patterns you want to frame – for all 3 projects youll need 3 wooden hoops)\n- A pack of embroidery needles- I love these gold ones by Clover\n- A water soluble pen\n- Pinking shears\n- Embroidery scissors – there are tons of them on Amazon\n- Fabri-Tac fabric glue\n- Thread Heaven\n- Steam iron\n- Ironing board\n- Light tablet or light box of some sort – this can even be a window\n- A printer/scanner to print and design patterns – I have the HP 4500 and I love it\nRequired Materials Needed:\n- Six stranded embroidery floss – you can either buy this set and a few skeins of black floss, or go to your local craft store and pick out colors you like\n- One yard cotton or linen fabric to embroider on – my favorite is a linen/rayon blend found here\nOptional Tools and Materials:\n- Unscented castile or laundry soap (used while washing embroideries)\n- A cork board and pushpins OR\n- A piece of heavy canvas larger than your embroidery, clothespins, sewing pins and an ironing board (this is used for blocking embroideries)\n- Spray bottle filled with water\n- Embroidery hoops in 3, 4, and 5 inch sizes for help with drawing borders on patterns\n- Clear sewing rulers for drafting patterns\n- Embroidery floss bobbins\n- Floss storage containers\nUnderstanding the Tools\nIn this section, Ill briefly go over each of the major tools and tell you why well be using them.\nEmbroidery hoops are used to hold fabric taut while stitching. We use plastic ones for stitching and wooden ones for framing embroideries.\nEmbroidery scissors are small and sharp, perfect for trimming floss cleanly and closely. Theyre essential. I like to make “necklaces” out of mine so I dont misplace them.\nPinking shears cut fabric so that it has a jagged edge. This keeps the edges of your fabric from unravelling as you work with it.\nEmbroidery needles vary wildly in size – I recommend buying a variety pack to find the size you like the most for each project.\nWater soluble pens are my preferred method of embroidery transfer. Theyre easy to use, come in a variety of thicknesses, and wash out easily with cool water.\nFabri-Tac Glue is the only glue I trust to finish my embroideries! It adheres permanently to wood and fabric, and dries clear and fast. Also great for securing knots or loose thread ends.\nThread Heaven is a a thread conditioner. It will make your floss pull through the fabric much more easily, and can also help you thread a needle with stubborn floss. One container of this will last you a very long time!\nEmbroidery floss comes in a few flavors, but well be using six-stranded floss for this class. My favorite brand is DMC, which can be found at nearly any craft store.\nEmbroidery fabric can be almost any type of fabric! However – linen, quilting cotton and muslin are all great beginner embroidery fabrics because theyre easy to work with. Linen is my favorite!\nAn iron and ironing board are extremely important to press fabric before you embroider. Any ironing board will do, but I recommend a steam iron with multiple temperature settings for best results.\nLight tablets are my preferred method of transferring embroidery patterns. They allow for much more accurate drawing than you get from a sunny window or homemade light box.\nYoull need a printer to print off the PDF patterns, and a scanner will come in amazingly handy if you decide you like to make your own patterns.\nClear quilting rulers come in very handy when it comes to patternmaking! These help you ensure that youre laying out text and drawings with consistency and efficency. 🙂']	['<urn:uuid:0ec352d0-ed45-484a-a096-6b1d52712f19>', '<urn:uuid:479b6f78-968a-4db9-8df8-97f2f4162cb1>']	open-ended	direct	concise-and-natural	distant-from-document	three-doc	novice	2025-05-12T18:05:20.075489	11	98	1719
38	compare preservation methods dead bodies capuchin catacombs paris catacombs storing remains	The Capuchin Catacombs and Paris Catacombs used different methods for handling remains. In the Capuchin Catacombs, bodies were preserved through a specific process: they were placed in 'straining rooms' on slats above pits for up to a year to drain fluids, then washed in vinegar and stuffed with straw. The preserved bodies were displayed standing or on shelves, fully clothed. In contrast, the Paris Catacombs function as an ossuary, storing only bones and skulls of the dead. The Paris Catacombs were created in the 1780s as a solution to overcrowded cemeteries, with the remains of about 6 million people transferred to underground quarries.	['By Chloe Wilson\nI’m not sure what drew me to read the one- and two-star reviews for Palermo’s Catacombe dei Cappuccini (Capuchin Catacombs). Perhaps it was their inherent potential for absurdity — there’s something so tenaciously churlish in taking the time and effort to write a scathing review of a centuries old burial site. How had so many people been unable to resist?\nThe reviews I read ranged from the practical (‘The times posted for admission were DEAD wrong [pun intended]’) to the frugal (‘No reduced price for kids’) to a slew of inexplicable comments which expressed visitors’ surprise and dismay that the catacombs contained dead bodies.\nAmong these reviews, however, there was one striking description: ‘Marco C.’ writes that ‘You will find rows upon rows of skeletons amassed like old rifles in an old mothballed army armoury.’\nThis image of an armoury is an apt one. It suggests the long, interconnecting corridors which comprise the catacombs, and the way in which the bodies are arranged standing tilted against the walls or stacked on shelves.\nIt also suggests how these bodies have attained a patina which from a distance renders them homogenous. Over time, they have all become a dulled grey-brown; coppery in places, dust-coloured in others, with the oxidised beige of bone poking through. This is due to the age of the corpses but also to their general state of disrepair; they are no longer maintained, and so it is not uncommon to see hay bursting forth from a chest or a leathery flap of skin sitting away from a skull.\nThe similarity between the preserved bodies is particularly apparent in the sections of the catacombs where friars are displayed. These were the original inhabitants of the catacombs (which were constructed by excavating crypts beneath the Palermo monastery once its cemetery had reached capacity). The oldest, Silvestro da Gubbio, died at the end of the 16th century, but he doesn’t stand out much from the others – even those who died centuries later. They are each displayed in their habits, and all, to a greater or lesser extent, show evidence of the Capuchins’ method of preservation.\nThis involved arranging the bodies in a ‘straining room’. They were placed on slats above deep pits and left for up to a year to drain of anything soft or liquid before eventually desiccating (‘like prosciutto’, writes A.A. Gill in National Geographic). They were then washed in vinegar, before each body was stuffed with straw and allowed to take its place along the corridors.\nI wasn’t able to find any information about how it became vogueish to be mummified along with the Capuchin friars, but eventually that’s what happened. The wealthy started paying considerable sums to have their loved ones undergo the friars’ burial treatment. To have a body in the Capuchin crypt was considered prestigious. Families would visit loved ones who were housed there, and often supply costume changes to keep the mummies looking spruce.\nFamilies also paid the monastery for upkeep; apparently, if payment was not forthcoming, a body was removed from its position and placed in a less illustrious spot — often a shelf stacked with other bodies — until the bill was settled.\nOver time, this practice expanded to the extent that the catacombs became their own little society, with strict and signposted demarcations. People are grouped according to profession and social position. There are also rooms dedicated to children and infants.\nMost visitors come to see Rosalia Lombardo, the ‘Sleeping Beauty’ of Sicily – a two-year-old who died of pneumonia in 1920 and was so well-preserved by embalmer Alfredo Salafia that some consider her miraculous.\nBut the room in the catacombs that I found most striking was that reserved for the virgins. It is one of the more sparsely populated rooms. Four women stand beneath a bare cross, a casket in front of them. They have been posed to look demure; each has her hands clasped. Further virgins are stacked on shelves against a side wall.\nAbove them, in solid black letters, are the following words:\n…SEGUONO L’AGNELLO DOVUNQUE VADA,\nThis translates to: ‘….We follow the lamb wherever he goes, we are virgins.’ The ‘because’ is implied.\nOften when I write a poem, the desire to do so arises from a gap, a fissure that has opened up somewhere, revealing something puzzling or incongruous. Here, this gap appeared between the way in which those who preserved the bodies of these professed virgins — their families, the Capuchin friars — intended this display to be understood, and the way the tableau appears now.\nThat is to say: the scene no longer reads as a sincere celebration of chastity and piety. To quote Ella Morton, writing for Slate:\nMany have wide-open mouths — due to a combination of decomposing facial ligaments and gravity — making them look as though they are screaming.\nThe quotation above them is of course not meant to suggest physical movement; it is presumably spiritually that the virgins ‘follow’. But it nevertheless creates an odd tension which both emphasises the virgins’ stasis and suggests the ways in which their lives were probably circumscribed while they were alive. The idea that ‘we follow…wherever he goes’ describes their lack of agency as much as anything else.\nYet, through the gradual process of disintegration, these corpses are beginning to challenge the words emblazoned above them. One hunches over, as if speaking to the woman next to her, who turns her head as if to listen. Another — in a manner inadvertently comic — stands with her bonnet pulled down over her face. As they slip out of the postures into which they were originally wired, the way in which the virgins’ bodies can be read will continue to change.\nThis is what I hoped to record in writing about the room of virgins — how, centuries after death, they have become unexpectedly dynamic. They resist, however unintentionally, and they move — however slowly — without following.\n Further to this, American GIs reportedly stole glass eyes from the bodies as souvenirs during WWII.\n Gill, A.A. “Where the Dead Don’t Sleep”. National Geographic Magazine. February 2009. http://ngm.nationalgeographic.com/print/2009/02/sicily-crypts/gill-text\n Until recently, it was believed that Salafia’s formula for embalming fluid died with him, but in recent years researchers have uncovered the recipe: formalin, zinc salt, alcohol, salicylic acid and glycerin. See Bommarito, Maria Lina. “Revealed: the Secret of Palermo’s Famous Mummy Child”. Times of Sicily. 8 February 2015. http://www.timesofsicily.com/revealed-secret-palermos-famous-mummy-child/\n Morton, Ella. “The Corpse-Lined Hallways of the Capuchin Monastery Catacombs”. Slate. 1 October 2014. http://www.slate.com/blogs/atlas_obscura/2014/10/01/the_catacombs_of_capuchin_monastery_in_palermo_sicily.html', 'The Catacombs of Paris or Catacombes de Paris is an underground ossuary in Paris, France. Located south of the former city gate (the “Barrière d’Enfer” at today’s Place Denfert-Rochereau), the ossuary holds the remains of about six million people and fills a renovated section of caverns and tunnels that are the remains of Paris’s stone mines. Opened in the late 18th century, the underground cemetery became a tourist attraction on a small scale from the early 19th century, and has been open to the public on a regular basis from 1874. Following an incident of vandalism, they were closed to the public in September 2009 and reopened 19 December of the same year.\nThe Catacombs are one of the 14 City of Paris’ Museums that have been incorporated since January 1, 2013 in the public institution Paris Musées.\nYou would have to look closely for one of its obscure entrances in the French capital of Paris.\nBut should you stumble upon one, it reveals an underground world of the dark, dank, narrow tunnels with a fascinating history.\nBelow the City of Light’s 12million residents lie the remains of 6million others – known as France’s Empire of the Dead, a world which is brought to life in a new documentary on CNN.\nThe Paris catacombs are a 200-mile network of old caves, tunnels and quarries – and much of it is filled with the skulls and bones of the dead.\nMuch of the catacombs are out of bounds to the public, making it illegal to explore unsupervised.\nBut nevertheless, it is a powerful draw for a hardcore group of explorers with a thirst for adventure.\nA tourist-friendly, legal entrance can be found off Place Denfert-Rochereau in the 14th arrondissement of Paris, near the Montparnasse district.\nHere, visitors from all over the world can descend into the city’s dark and dank bowels for a whistle-stop tour of a small section of the catacombs.\nOne visitor told CNN: ‘I think people are fascinated with death. They don’t know what it’s about and you see all these bones stacked up, and the people that have come before us, and it’s fascinating. We’re trying to find our past and it’s crazy and gruesome and fun all at the same time.’\nThe well-worn trail might be enough to satisfy the tourists, but other Parisians like to go further – and deeper – to explore the network.\nThe name given to the group of explorers who go into the cave network illegally and unsupervised is Cataphiles.\nThe top secret groups go deep underground, using hidden entrances all over the city. And they sometimes stay for days at a time, equipped with head lamps and home-made maps.\nStreet names are etched into the walls to help explorers navigate their way around the underground version of the city and some groups have even been known to throw parties in the tunnels or drink wine.\nFor catacomb devotees, the silence experienced deep in tunnels cannot be replicated anywhere else.\nThe number of dead bodies buried in Paris’s cemeteries and beneath its churches was so great that they began breaking through the walls of people’s cellars and causing serious health concerns.\nSo the human remains were transferred to the underground quarries in the early 1780s. There are now more than 6million people underground.\nThe space was the perfect solution to ease overcrowding in cemeteries but it presented disadvantages elsewhere.\nIt is the reason there are few tall buildings in Paris; large foundations cannot be built because the catacombs are directly under the city’s streets.\nThe tunnels also played their part in the Second World War. Parisian members of the French Resistance used the winding tunnels\nAnd German soldiers also set up an underground bunker in the catacombs, just below the 6th arrondissement.']	['<urn:uuid:8eff1e38-f4d8-42ad-aa56-048f30501c81>', '<urn:uuid:47e3402d-a118-4703-92df-c7f09ee4d43d>']	open-ended	direct	long-search-query	similar-to-document	comparison	novice	2025-05-12T18:05:20.075489	11	103	1722
39	which security method most effective encrypt messages tls ssl compare differences	TLS and SSL both use symmetric cryptography for encryption, but TLS provides additional security features. While SSL checks messages for integrity before delivery and optimizes CPU resources, TLS adds an extra security layer by requiring two authorized users to receive data before third-party delivery. When used together, these methods provide the highest level of security available. TLS specifically ensures network security over end-to-end communication and is the most broadly deployed safety protocol used today for web browsers, file transfers, VPN links, instant messaging, VOIP, and email messages.	['One of the most important aspects of web hosting security is the protocol that is used to transfer files from the web server to a visitor’s computer. This is especially true for web sites that frequently engage in eCommerce activity. Understanding the various types of web hosting security protocols can help you build discretion when selecting an FTP client or web hosting plan in the future. If you’re interested in learning more about the difference between SSL, TLS and SSH security protocols, then you may find the following information to be quite useful.\nSSL (Secure Sockets Layer) is a commonly used security protocol that provides supreme privacy when transmitting data over the internet. FTP clients and web shots utilise this protocol to ensure the utmost security during sensitive site browsing activities like eCommerce shopping and online banking. SSL protocol uses symmetric cryptography to perform state-of-the-art data encryption. SSL adds extra security by automatically checking the encrypted messages for integrity before delivering them to the recipient. SSL optimizes CPU resources to facilitate expedited communication, and can be used in conjunction with the following security protocols to increase effectiveness.\nTLS (Transport Layer Security) utilities two main components – TLS Handshake Protocol and TLS Record Protocol. TLS protocol also uses symmetric cryptography, and like SSL checks the messages before the are sent to ensure that they are thoroughly encrypted. TLS differs from SLL because the encrypted massage can only be delivered to a third-party user after two authorized users have received the data. This adds an extra layer of security to the encryption, when used in combination with SSL, this method of encryption provides the highest level of security available to the public.\nSSH (Secure Shell) provides encrypted channels through which encrypted data can travel. Thus, SSH acts more as a medium for the aforementioned encryption methods than as an actual encrypting mechanism, hence the term “shell.” SSH is commonly used to send commands to a computer from a remote location. Although SSH is not the strongest encryption method, it is viable method of secure communication, and is frequently used in the web hosting industry as a standalone security precaution with some applications. Standard FTP services do not utilize Ssh technology, however SFTP (Secure File Transfer Protocol) does utilize this technology along with the other two aforementioned security protocols.\nAlthough all three of these security protocols could be used by themselves with success, most web hosting providers now utilize all three of them simultaneously in one form or another. If you’re serious about keeping your web site data safe form intruders and hackers then you’ll need to pay close attention to the technology used by your web hosting provider, especially if you’re the owner of an eCommerce business. Make sure all of your checkout pages are encrypted and have an SSL certificate. The easiest way to check if a web page is secured is to look at the first letters of the web address. If the web address begins with “HTTPS” then it is secured, however if the web page begins with “HTTP” it is not secured. Never conduct eCommerce on an unsecured web page.', 'TLS Encryption and Email Security\nHackers and stalkers have taken social sharing to a stage where it makes us vulnerable to their attacks. The internet helps them grow faster, learn from each other and makes the act of snooping over someone’s private data easier than earlier. As wise men say – safety is the number one priority. All we can do is to ensure there is a proactive breach-proof technology in place when it comes to transmission of data, and stay ahead of the game.\nWhen we provide multiple levels of assurance, we can improve the user experience along with providing secure communication. We believe that any TLS (Transport Layer Security) is better than no TLS. This is because all certificates, despite their different assurance levels, work to provide session safety and encrypt any data transmitted over the website. Depending on your requirements, we suggest you make a choice that serves your needs best. But first, you need to know what TLS encryption is and what it exactly does, so here you go:\nWhat is TLS Encryption?\nTransport Layer Security (TLS) is a protocol that implements privacy and data integrity within two communicating entities.TLS encryption is a cryptographic protocol that ensures network security over end-to-end communication. It is the most broadly deployed safety protocol used today by web browsers and other applications that need to privately transfer data over a network. It includes file transfers, VPN links, instant messaging, VOIP and sending messages over email.\nTLS is composed of two layers, the TLS Record Protocol and the TLS Handshake Protocol. The Record Protocol renders connection securely. The Handshake Protocol requires the server and client to verify each other first and barter encryption algorithms and cryptographic keys before any data is transferred.\nThe key features of TLS encryption include-\n- Encrypted messages– TLS makes sure that your messages that are sent over emails are secured from hackers. It encrypts the message before sending it so that it is impossible for a third party to interpret it.\n- Authentication– This is the process of checking the sender and receiver ID. It is done to verify that the messages are being sent from authenticated sources and the process has no spoofing.\n- Standardized websitesTLS encryption is now becoming a standard practice for websites. While Google Chrome is striking out non-HTTPS sites, people are also becoming wary of entering websites that do not have an HTTPS security protocol.\n- WidespreadTLS encryption is not limited to only websites and emails, VPN Voice over IPs, and many other such servers who provide end to end communication.\nTo maximize the utilization of TLS, you should ensure that both the parties have secure SSL/TLS Sessions. Most of the leading ISPs are supporting TLS these days for better transfer of emails.\nHow does TLS work for an email?\nThe mechanism and language (protocol) by which one email server sends an email message to another email server is called SMTP (Simple Mail Transport Protocol). For a long time now, email servers have had the choice of using TLS to transparently encrypt the message transmission from one server to the other. TLS used with SMTP, when possible, guarantees that the content of the email is guarded during communication between the servers.\nWant to know more on SSL and TLS ?\nTLS is initiated with a sequence called TLS handshake. The TLS handshake establishes a cipher suite for communication which specifies the encryption kit that will be shared for that communication. Once this data is encrypted, it is then signed with a Message Authentication Code which can be verified by the recipient to test the authenticity of the data transferred.\nThe target email server must support TLS for TLS communication to be used. The sending computer or server must be configured to use TLS links when possible.\nHere are two examples of emails sent with and without TLS encryption. This will help clarify the meaning and use of encryption:\n1. Without TLS encryption\n2. With encryption\nImage 2 is the perfect example of TLS encrypted email and how the receiver can see it.\nBoth mail servers are required to support TLS for the encryption process to work. The server and the client recognize which encryption keys to use before anything is broadcasted. The negotiation itself is guarded as well.\nWhy is TLS important?\nWhen you use a regular POP or IMAP link to download your email (the most common technique in practice), your username and password are carried in clear text over the Internet. This means, that anyone using the same wireless connection, the same channels, picketing traffic at your ISP or anyone in a position to see your Internet traffic can conceivably “hijack” your web traffic and find out your username and password. With this knowledge, they can easily read all your email, steal classified data and/or send out spam emails on your behalf.\nTLS encryption shields the transportation of the content in email messages. However, it does not defend the security of the information before it is transmitted or after it reaches its destination. For that, other encryption tools may be used, such as PGP, S/MIME, or storage in a secure portal.\nTLS encryption ensures that any information transmitted between the server and client does not fall prey to a man-in-the-middle attack, i.e. spammers attacking the data before it reaches the server. Encryption ensures that data which is being transmitted does not fall prey to attackers.\nOpportunistic TLS for Email Security\nAre you able to use TLS encryption optimally? If no, then Opportunistic TLS is the solution. To arrive at a clear idea, if the sending server sends an encrypted email but if the receiving server doesn’t accept encrypted messages, then email is sent unencrypted and hence available for email spoofing. Hence, the essential thing is to have an email provider that uses a secure connection. Trusted email service providers (ESPs) like Pepipost have security measures that do not allow unauthorized users from sending messages.\nGoogle’s Email Transparency report can let you find out the statistics of encrypted mail interaction with Gmail Server. The benefits of an opportunistic TLS are a higher sender reputation and 100% spam free delivery.\nFailure in TLS Handshakes\nThe failure rate of TLS handshakes is a topic of concern for system engineers across the world as it causes authentication and security issues. TLS handshakes can be of two types – one based on RSA and other on Diffie-Hellman. These both work on algorithms and if an algorithm fails, it leads to a failure in the TLS handshake. The bottom line is TLS encryption alone is not sufficient to keep your emails secure and authentic.\nConsidering as a whole, TLS encryption is very important for sending and receiving emails in a secured manner. But, it can fall into some traps and you may suffer a data breach or unsuccessful emails in certain cases. Therefore, it is not a complete solution in case of exceptional situations and you need to handle them on your own.\nBut, the good news is that there are measures/technologies in place that can alleviate the impact of TLS handshakes. TLS false start is one such program where the server and client can\nYou might send an email normally without a secondary level of protection. This could be fine if your email is a general email. However, it becomes a matter of concern if you have to send any confidential data over emails. Although most of the popular ISPs support TLS and ensure double protection of your conversations over an email, you cannot ensure the security at the receiver’s end. If the receiver suffers a data breach or hacking at his end, you cannot control your data from being lost or stolen, even though it was secured through TLS encryption.\nTLS Vs. SPAM\nTLS encryption is assumed to be a secured connection between two ends. It is free from spam most of the time. However, it does not ensure 100% security. There are some companies that have a special kind of spam or anti-virus implemented, which could affect the security of the email message. The whole message may not be encrypted and you may need to ensure the security of the messages while sending them over the internet. Therefore, you must check whether the TLS system is working properly in advance and ensure that there are no loopholes in it.\nAlso, interestingly you can identify SPAM from Email Headers.']	['<urn:uuid:ee1ef7a3-a083-4824-b19b-a64266b68f50>', '<urn:uuid:e4a4a932-c2a7-4c42-9ec4-526adda36103>']	factoid	direct	long-search-query	distant-from-document	comparison	novice	2025-05-12T18:05:20.075489	11	87	1928
40	horse worm management techniques environmental factors	Horse worm management techniques include both chemical and environmental control methods. Pasture management is crucial, requiring removal of faeces at least twice weekly to break the parasite lifecycle. Grazing horses in rotation with sheep can be effective as sheep 'hoover up' horse parasites without risk since there's no overlap between sheep and horse worms. However, environmental factors significantly impact these management strategies - climate affects both egg development and larval survival. Eggs need warm conditions (6°C-38°C) to develop, with 25°C being optimal. Conversely, infective larvae survive better in cooler temperatures and can even tolerate freezing, while temperatures above 32°C reduce their survival. In subtropical climates, these conditions allow year-round development, making paddock spelling unreliable for creating 'safe' pasture.	['It is not surprising that many people are confused about worming. There is no one plan that is suitable for every yard and best practice for worm control in horses, ponies and donkeys has changed over the years to a more targeted approach – worming only when necessary, with a suitable product, at a suitable time.\nAll horses continuously experience some level of gastrointestinal parasitism (i.e. worm burdens) throughout their lives. It is now known that a low level of parasitism actually has some health benefits, particularly on the immune-system. High worm burdens, however, can cause a number of symptoms including weight loss (or lack of weight gain), loss of appetite, poor performance, colic, diarrhoea and, in some severe cases, can even lead to death.\nTraditional worming regimes have previously consisted of worming horses every 6-8 weeks, regardless of their parasite burden. A blanket approach tends to lead to many horses being treated unnecessarily, costing more, polluting the environment and, very importantly, increasing the chances of wormer resistance (so the worming products do not effectively kill the worms).\nTargeted worming involves using faecal worm egg counts to determine whether or not your horse actually needs worming. his approach typically reduces wormer use by 70-80%, saving you money, reducing wormer resistance and is better for the environment.\nA faecal worm egg count is a simple, widely available, procedure and is performed cheaply and quickly. A small sample of fresh faeces is all that is required. Ideally this should be taken from three different areas of fresh faeces and mixed to make a total amount about the size of a golf ball. The sample should be submitted as fresh as possible (it is possible to refrigerate it for several days if it cannot be submitted within 24 hours).\nYour vet will be able to advise on which product to use on the basis of the worm egg count, taking into account your horse’s specific situation. Some horses will repeatedly have higher worm egg counts than their field companions even if they are managed in exactly the same way and does not necessarily reflect an underlying problem.\nFaecal worm egg counts do have their limitations. Firstly they give no indication of the level of encysted cyathostomins, when redworm larvae “hibernate” in the gut wall. For this reason, it is often recommended to use moxidectin (found in Equest and Equest Pramox) in the autumn or early winter.\nSecondly faecal worm egg counts do not reliably give any indication of tapeworm burden. A blood test for tapeworm antibodies is available, but will remain high for several months even if the tapeworms have been killed. Therefore most horses should receive a tapeworm wormer (praziqauntel or double dose pyrantel) once or twice a year, depending on their risk. This is often combined with the autumn wormer for encysted redworms and the only product on the market that does this is Equest Pramox.\nWorm egg counts are usually repeated 3 – 4 times a year, depending on the results and which worming product was used (e.g. pyrantel does not last as long as ivermectin).\nPasture management is key to controlling gastrointestinal parasitism. The faeces should be removed from the pasture (“poo-pick”) at least twice a week (ideally every couple of days). It is also useful to graze sheep on the pasture as they “hoover-up” the horse parasites and there is no overlap of sheep and horse worms. Horses should not be moved onto clean pasture within 2 weeks of worming. This “dose and move” strategy used to be recommended but this significantly increases the chances of introducing resistance worms on the new pasture.\nSome horses on very well managed pastures may repeatedly have very low worm egg counts. In these cases the frequency of worm egg count testing may be reduced to 1-2 times a year and a tapeworm blood test may be considered instead of using a tapeworm wormer.\nConversely a stud farm may require more intensive management, as young stock are more susceptible to cyathostomin infection (redworms) as well as worms such as Strongyloides westeri and Parascaris equorum.\nResistance to wormers is already a widespread problem, and is on the increase. There are no new classes of wormer under development for horses and resistance to the most important types of parasites have been identified in all classes of wormers. If worming habits fail to change we will all have to accept the harsh realities of worm-related disease as there will be no simple treatment. Using faecal worm egg counts not only reduces the chances of resistance becoming a problem, but can be used to identify if resistance is already a problem to one or more classes of wormer in your horse/property. A faecal reduction test can be performed two weeks after worming to check that the wormer has been effective and therefore that there is no resistance. Once resistance is present in a worm population in a horse or on a property it cannot be reversed\nThere is no one plan that will suit all horses and yards. Advice on worming can be confusing and the number of worming products can be bewildering. Experts are agreed that the traditional approach of worming is misguided and dangerous. A targeted plan based on faecal worm egg counts should be used and your vet will be able to give tailored advice taking into account a number of factors, including previous worm control measures, pasture management, horse movements, horse ages, weather conditions and any known resistance problem.', 'Postdoctoral research fellow at the University of Queensland, Dr Anne Beasley, explains the importance of sustainable worm control and why we must no longer rely on the old-school interval treatment regime.\nThis article provides all the basics on parasite biology and the drugs available for treatment. This knowledge will empower you to do better and contribute to slowing down the slide towards resistance.\nMost people would agree that worm control in horses can be confusing and is further complicated by a plethora of available worming products containing different ingredients and conflicting advice on how to best use them. Unfortunately, there is no ‘one-size fits all’ approach to worm control – not even on a regional basis. You and your neighbour may well require different worming strategies to best manage worms on your property.\nThe good news is that researchers have moved in leaps and bounds towards understanding the many factors that contribute to sustainable worm control. The bad news is that worming practices of the past, which were developed under very different circumstances, are still being used today.\nThe slippery slope to resistance\nThe approach of treating all horses at regular, and sometimes short, intervals leads us down a slippery slope towards the emergence of drug-resistant worms. This article introduces a variety of information that can be used to create an individualised worm control program that aims to prevent heavy pasture contamination and, perhaps more importantly, slows down the development of drug-resistant worms.\nWhat worms are important?\nHorses are host to a large number of internal parasites, some more of a health risk than others. Rather than review them all in depth, here I will focus on those that should be the main target of your worming program in young and mature horses.\nFoals are born with very little resistance to worms and so they are particularly vulnerable to infection. The most important worm infecting foals is Parascaris equorum, the intestinal roundworm. The pre-patent period (time taken from ingestion to eggs being passed in faeces) is between 72 and 110 days, so the foal will have adult worms in the intestine from about three months and heavy infestations can quickly build up.\nSigns of infection in foals will vary depending on the stage of the worm life cycle and the number of worms present. Small numbers usually cause no harm, but large numbers of migrating larvae can cause symptoms of respiratory disease, such as fever, coughing and nasal discharge. Due to their exceptional size (about the dimensions of a pencil), large numbers of mature Parascaris in the intestine can cause blockage, severe colic and, in the worst instance, intestinal rupture.\nWithin the foal’s first year, strong immunity develops to Parascaris, so that by 12-15 months of age most will have cured themselves of infection.\nAs the foal ages and Parascaris becomes less of a problem, the cyathostomins, or small strongyles, step into the spotlight. Horses of all ages are susceptible, but particularly weanlings and yearlings while their immunity is still under development.\nCyathostomins are by far the most prevalent group of worms (there are 54 species!) and they cause disease in the horse when large numbers of larvae simultaneously emerge after a period of arrested development. The disease, known as larval cyathostomiasis, is characterised by a sudden onset of diarrhoea followed by severe weight loss.\nThe threadworm (Strongyloides westeri), pinworms (Oxyuris equi), stomach hair worms (Trichostrongylus axei), the spirurid worms (Habronema spp and Draschia megastoma) and bots (which are not worms, but larvae of the bot fly – Gasterophillus spp), are less likely to be associated with clinical symptoms and take a back seat to parascaris and cyathostomins as targets of treatment.\nOnce considered a major threat to horse health (and rightly so), were the large strongyles (Strongylus vulgaris and others), however, with the introduction of interval treatments using highly effective anthelmintics, these species are now much less prevalent.\nHorses usually harbour a mixed infestation of many of these worms and otherwise healthy horses can carry small or moderate burdens and continue to thrive with very little, if any, measurable impact on health and performance.\nSome of the worms you might recognise in your horse’s manure following worming are: cyathostomins (small stongyles) which are small, approximately 1cm in length, white or pink/red in colour; Parascaris (roundworms) which are large, approximately 20cm in length, white in colur; pinworm, which are large, approximately 10cm in length, white with characteristic tapering tail, and tapeworm, which are pale, about 1-2cm in length, slightly triangular in shape, wide and flattened.\nAnthelmintics (worming products)\nCurrent horse wormers contain active ingredients which belong to one of three separate anthelmintic classes, and sometimes a combination of two. Each class has a different mode of action against gastrointestinal parasites and slightly different efficacy against each, so careful consideration needs to be given when choosing an appropriate product. The anti-cestodal drug, ‘praziquantel’, is often used in combination with anthelmintics for the control of tapeworm.\nA useful summary of horse worming products and their effectiveness against various horse worms has been prepared by the NSW Department of Primary Industries in Primefact 976, Worm control in horses.\nAn important feature of each drug class is its ‘egg reappearance period’ (ERP), which is the time taken for eggs to reappear in the faeces after treatment. A summary is given in Table 1 below:\nTable 1. Classes of drugs available for treatment of horse worms\n- BENZIMIDAZOLES (BZ’s) Oxibendazole, Oxfendazole, Fenbendazole – The ERP is four weeks\n- TETRAHYDROPYRIMIDINES (THP’s) Morantel, Pyrantel – The ERP is four weeks\n- MACROCYCLIC LACTONES (ML’s) Abamectin – The ERP is six to eight weeks\n- MACROCYCLIC LACTONES (ML’s) Ivermectin, Moxidectin – The ERP is 12 weeks\nModern worm control\nOut with the old-school interval treatment approach to worm control\nThis system was originally recommended many years ago when the very harmful large strongyles (particularly Strongylus vulgaris) were highly prevalent and the predominant target for worm control. The aim was to keep egg-shedding as low as possible and, as such, anthelmintics were administered to all horses in the mob at short, regular intervals.\nHow do we move away from the interval treatment approach?\nThere are a number of factors that we can use to our advantage when designing a worm control program, primarily targeting cyathostomins (small strongyles) in mature horses, that relies on fewer worming treatments – remembering that our aim is now two-fold; firstly, to minimise pasture contamination with infective larvae and, secondly, to slow down the development of drug-resistant worms.\nWork with nature\nNature herself has a major impact on the availability of infective larvae on pasture. For eggs in faeces to develop to hatching stage, they need warm conditions (within the broad range of 6oC – 38oC, the optimum being around 25oC). The cooler it is, the slower the development. Once hatching has taken place, however, the infective larvae are affected by temperature in the opposite way. Warmer temperatures (especially above 32oC) greatly reduce their survival, whereas cooler temperatures are more favourable. In fact, larvae can even tolerate freezing.In the context of a south-east Queensland (sub-tropical) property, for example, the bad news is that climatic conditions allow egg development and larval survival year-round, making paddock spelling an unreliable method of preparing ‘safe’ pasture. Some handy knowledge though is that peak pasture infectivity usually occurs during Spring/early Summer and again in Autumn/early Winter.\nTreating horses immediately prior to these high-risk periods (i.e. in late Summer and late Winter) helps to reduce those peaks. Winter is also an effective time to treat for bots as the bot larvae ‘overwinter’ in the stomach and wait for warmer conditions to pass out of the horse and pupate into flies.\nUnfortunately, nature never gets the upper hand on parascaris, whose eggs are so resilient that they are found wherever there are young horses world-wide.\nNot all horses are equal\nPasture contamination with cyathostomin eggs is not generated equally by all horses in a herd. It is now known that the majority of parasites (80%) are harboured by a minority (20%) of individual horses, making these few the source of most of the contamination.\nIf these individuals can be identified (via a faecal egg count a couple of weeks after the ERP of the previous worming product), an opportunity for targeted treatment exists, whereby more treatments can be administered to these horses and fewer to those that shed fewer eggs.\nThe commonly used thresholds for ranking horses on faecal egg counts are: <200 epg = low egg-shedder; 200-500 epg = moderate egg shedder, and >500 epg = high egg-shedder.\nPasture hygiene can have a big impact on pasture contamination. Weekly (minimum) removal of manure from yards/small areas greatly reduces the number of larvae available to infect your horses.\nLet them hide\nPerhaps one of the biggest advances in the understanding of the development of drug resistance is the concept of ‘refugia’.\nPut simply, the portion of the parasite population that escapes exposure to the drug is considered ‘in refuge’ – for cyathostomins this comprises encysted larvae in the wall of the large intestine (most products do not kill these), larvae already on the pasture and worms in any untreated animals.\nIf there is little or no refugia, resistant worms surviving a drug treatment go on to make up the majority of the breeding population for the next generation. If there is a substantial pool of worms in refugia, then resistant worms are in effect ‘diluted’ among the susceptible worms, thereby slowing the progression of resistance. The historical approach of treating all animals on a property at the same time, and at excessively frequent intervals, removes a large amount of refugia and, therefore, encourages the development of resistance.\nAlways administer your worming treatments based on an accurate measurement of horse weight. Most people underestimate their horses’ weight by at least 10%, so if you can’t weigh your animals on an equine specific weighbridge, use a weight estimation formula or a weight tape and add 10% to be sure you are not under-dosing. Resistance develops rapidly when worms are exposed to a sub-lethal concentration of drug.\nRecommendations and advice\nEvidence-based, Targeted, Strategic worming (E.T.S.)\nEradication of worms from horses is not achievable and nor is it sensible. Rather, a balance should be sought that allows adequate exposure (for the development of immunity) and provides sufficient refugia (to slow down resistance), while maintaining clinical health of our horses.\nE. is for the ‘evidence-based’ part of this regime, which is based on periodically monitoring faecal egg counts of horses and only worming when necessary. It also means only using drug classes you know are still effective on your property. There are various diagnostic laboratories nationwide that offer faecal egg counting services, and you can carry out a faecal egg count reduction test by collecting and submitting samples from a group of horses on the day of treatment and again 10-14 days after treatment. Monitoring is often cheaper than worming a whole mob of horses unnecessarily or with a product that doesn’t even work!\nT. is for the ‘targeted’ part, which means identifying those horses on the property that require more frequent treatments and giving fewer treatments to those that are low egg-shedders.\nS. is the ‘strategic’ component, which comes from applying knowledge about the worm life-cycle and ecology. As discussed earlier, timing treatments to pre-empt high-risk seasons and using pasture hygiene (if practical) to break the life-cycle are very effective. Another very effective weapon (where available) is to graze horses in rotation with cattle/sheep (who harbour different worm species not infective to horses). The ruminants will ‘vacuum’ up much of the equine worms.\nThese approaches should help to reduce the number of treatments required every year for mature, grazing horses. This includes broodmares which, on such a monitored and controlled program, should not need an additional treatment at foaling to protect against Strongyloides westeri infection – leading parasitologists regard this worm as a non-issue.\nThere is less room for improvement in treating foals and young horses as they are more susceptible and their egg counts quickly rise to high levels following the ERP. These horses (weanlings, yearlings) should be grazed separately to older stock and it is still imperative to test the efficacy of your worming product (especially the ML class) against Parascaris, the main target for treatment in young horses. Despite the common recommendation that foals be treated as early as four-six weeks of age, there is no real value in treating earlier than eight weeks.\nRemember, the primary target should be mature Parascaris worms that are not yet passing eggs (this is the easiest stage to kill). Regular treatment will then usually be required within a couple of weeks of the ERP until the horse starts to regulate its worm burden through acquired immunity.\nHorses housed in stables or dirt yards that are not exposed to contaminated pastures should require less frequent treatments, provided manure is regularly removed. If the horse is wormed into the stable/yard, treatment every three-four months should suffice (maybe even less for low egg-shedders) – check that treatment is required first by monitoring faecal egg count.\nIt’s time we stopped relying solely on the convenience of the old-school interval treatment regime and put the effort into getting to know the ins and outs of sustainable worm control. It begins with learning the basics of parasite biology and a little about the products available for treatment – and when we know better, we can do better. The threat of resistance should be a motivating factor in changing the way we manage worms on our properties.\nTo download a handy 6-Step Guide to Worm Management click here. (Sign up required)']	['<urn:uuid:308e10d9-c3e6-4c12-bbc8-390635d56fad>', '<urn:uuid:3e95c908-14eb-4263-81cc-92a24f556bb4>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T18:05:20.075489	6	118	3196
41	Being involved in environmental monitoring systems, I'd like to know how automation technology is being applied for both pest control and waste management - what solutions have been developed for weed detection and water treatment?	In pest control automation, a novel system called Map and Zap has been developed to automatically detect and destroy pest weeds affecting New Zealand's environment and agriculture. For waste management, zeolite technology is being implemented in waste water treatment applications, where it enhances the efficiency of nitrogen and phosphate removal from municipal waste-water processing plants. The system is particularly effective in managing nutrient retention and leaching, as demonstrated in applications like sport stadium construction where zeolite-enhanced sand zones dramatically improve nutrient retention and minimize leaching to waterways.	"['Projects & Products\nRed Fern works and assists with new start-ups through to NZ\'s biggest companies. We have projects and products with both private and public entities, as well as research groups and organisations. We can share some details of a selection of our publicly funded or disclosable projects below.\nDigital Crop Estimation (Zespri Ltd)\nRed Fern has developed and produced the high-speed imaging and data capture units for the Zespri Ltd Digital Crop Estimation systems. They cover the entire width of the orchard rows, and are now in their third season of operation across the industry to assist with fruit density and size profiling for supply chain estimates.\nEzyLure® (support from PF2050 Ltd and DOC)\nFlexi-Comms (support from PF2050 Ltd)\nRed Fern and Boffa Miskell have designed and developed the low-cost Flexi-comms ground-based communication system, specifically for remote monitoring and communications of traps and pest management devices in New Zealand, with funding from PGF via PFNZ2050 Ltd, DOC, and MBIE. This product is being commercialised in 2022 through Critter Solutions, with units produced via Red Fern Manufacturing.\nMap and Zap (AgResearch Ltd)\nWe are involved in several leading-edge research projects, including the Map and Zap programme led by Dr Kioumars Ghamkhar at AgResearch Limited, and funded by the NZ Ministry of Business, Innovation and Employment (MBIE). In this project we have assisted in developing a novel, automated system for detecting and destroying pest weeds affecting New Zealand\'s environment and agriculture.\nPlease refer to the Map and Zap news release for more details.\nWe have been involved in a broad range of applications and industries, including:\nAnimal monitoring & behaviour\nFood processing & quality\nManufacturing process lines\nMedical & health\nExample Publications & Media\nAlongside commercially sensitive projects, Michael Hagedorn and Kenji Irie have independently been involved in a large amount of collaborative, published, material in the past 20 years (pre- Red Fern Solutions). Please note that the other authors listed are affiliated with other companies and institutions.\nPAWS Animal Monitoring:\n""High-tech sensors new weapon in war on pests"", NZ Herald, 2018: Link to article\n""Digital pest control In New Zealand"", BBC Click, 2015. Helen Blackie and Kenji Irie.\nListen at: http://www.bbc.co.uk/programmes/p02z77zw\n[Photo courtesy of Helen Blackie]\nAdapting the \'PAWS\' animal monitoring tool for surveillance of insects and other small animals"", Kenji irie, Helen Blackie, Paul Riding, Ian Woodhead, Charlie Eason. Presented at the Ecological Society of New Zealand conference, 2014.\n""An animal trap monitoring system (PAWS) for Dogs, Dingoes, and Foxes"", Kenji Irie, Helen Blackie, Paul Riding, Ian Woodhead, Charlie Eason, and Jess Roberts. Presented at Spatially Enabled Livestock Management Symposium, New Zealand, 2014.\n""New Automated pest detection and monitoring devices"", Helen Blackie, Brent Barrett, Ian Woodhead, Kenji irie, Paul Riding, and Shane Inder. Presented at the 16th Australasian Vertebrate Pest Conference, Brisbane, 2014.\n""Microwave detection of foreign objects inside bread loaves"", Kenji Irie, Ian Woodhead, Adrian Tan, Jeffrey Hsiao, Amanda Zhang, The International Journal of Food Science and Technology, Lincoln, New Zealand, 2015.\n""Characterising noise on a TDR image sensor"", Michael Hagedorn, Ian Platt, and Ian Woodhead. Presented at the IEEE Instrumentation and Measurement Workshop, Palmerston North, New Zealand, 2013.\n""Timber tomography using time domain reflectometry"", Ian Platt, Andrew Hayward, Ian Woodhead and Michael Hagedorn"", presented at the International Conference on Sensing Technology conference, New Zealand, 2012.\n""A technique for evaluation of CCD video-camera noise"", Kenji Irie, Alan McKinnon, Keith Unsworth, and Ian Woodhead. Published in IEEE Transactions on circuits and systems for video technology, Vol. 18, No. 2, February 2008.\n""A noise-bound method for detecting shadow-free scene changes in image sequences"", Kenji Irie, Alan McKinnon, Keith Unsworth, and Ian Woodhead. Published in the Journal of the Optical Society of America A., pp167-73, 27(2), 2010.\n""Automated hull-scanning for detection of exotic species"", Kenji Irie, Michael Hagedorn, Adrian Tan. Environlink report for Northland Regional Council, 2016.\n""A laser-based eye-tracking system"", Kenji Irie, Bruce Wilson, Richard Jones, Tim Anderson. Published in Behavior Research Methods, Instruments, & Computers, 34(4), pp561-72, December 2002.', 'BPM owns and operates a range of geological deposits and manufacturing facilities. These exist in the sensitive geothermal region, known as the Taupo Volcanic Zone in the central North Island of New Zealand.\nThe nature of our operations has the potential to influence the ecology and communities around our facilities. We are active in making a valuable enduring contribution to the design and development of new products aimed at addressing nitrogen management issues in the farming sector, improving lake water quality and other sustainable farming initiatives.\nResearch to benefit the environment\nBPM has partnered with Crown research institute, Scion, to develop a zeolite-based product called Aqual P, which has been used to combat algal problems in New Zealand lakes. When applied to the sediment layer in a lake, Aqual P absorbs rogue phosphorus, cutting off the food source for problematic algae.\nZeolite, in its unmodified and natural format, also enhances nutrient and moisture retention, which is good for the environment. Zeolite can also be used to soak up the nitrogen and phosphate from dairy farms; this impacts favourably in terms of minimising the impact of agriculture on streams and waterways.\nOther environmentally favourable avenues are also being explored, such as the use of zeolite in waste water treatment. In this application zeolite has scope to enhance the efficiency of nitrogen and phosphate from municipal waste water processing plants.\nZeolite is one of a number of test materials currently being used on a ‘living roof’ or ‘green roof’ project at Auckland University. This has potential applications for stormwater management through runoff minisation and natural filtration.\nDairy farming in New Zealand is coming under increasing pressure to reduce its environmental footprint. Recent policy decisions by Regional Councils will require intensive dairy and horticulture to obtain resource consents that require nitrate leaching outputs to be below a threshold, dependent on land class. The challenge of identifying systems that can reduce nitrogen and phosphorus pollution is currently a hot-topic.\nAnalysis of trial data confirms BPM zeolite based products when fed to livestock on dairy farms substantially assists in reducing urinary nitrogen. Research to determine the level of nitrogen partitioning achievable is on-going. These exciting developments pave the way for BPM to contribute to the sustainable future of the farming sector in New Zealand.\nZeolite minimising nutrient leaching\nA good illustration of the use of BPM zeolite in leachate reduction is in sport stadium construction, (along with high-traffic turf zones such as golf course greens construction), where the turf profile is required to rapidly dissipate ground water. This is achieved by using sand-based turf profiles which then discharge using storm-water pipework. The ability of a sand “root zone” to trap and retain soluble nutrient cations, prior to entering storm-water discharge, is very poor. The blended addition of our graded zeolite to this sand zone dramatically improves the nutrient retention, thus minimising any nutrient leaching to waterways.\nOther environmentally favourable avenues are also being explored, such as the use of zeolite in waste-water treatment. In this application zeolite has scope to enhance the efficiency of nitrogen and phosphate from municipal waste-water processing plants.\nZeolite in ‘Living Roof’ trials\nWhen it rains, water runs off roads, paving and buildings and the run-off directly enters streams, lakes and harbours via stormwater drains. Run-off results in pollutants washing into the waterways and increases the potential for stream channels to erode.\nAt Auckland University, zeolite is currently one of a number of test materials being used in a project geared at managing storm water. Zeolite is being used on their “living roof /green roof” project. Living roofs have been recognised as having the potential to minimise run-off and act as natural filters for harmful nutrients and heavy metals. A technical report documenting the extensive testing and recommendations for living roof design and construction has been prepared. The report focuses on using readily-available, locally sourced materials, such as zeolite, to build living roofs for stormwater management in New Zealand.\nBiogro organic certified products\nOur animal feed supplements are registered as BioGro certified organic inputs. BioGro is the leading voice for organics and certification, representing and certifying over 650 organic producers, farmers and manufacturers across New Zealand and the Pacific. BioGro certify all sectors of the organics industry from dairy to winemaking, fresh fruit and veggies, beverages, baby food, cosmetics and everything in between. BioGro certification is internationally recognised and accredited.\nBPM company director Paul Tidmarsh has encouraged native planting at BPM’s site. This has hardly been a small scale endeavour - more than 4000 native shrubs, grasses and trees have been planted on an expansive landscaped area behind the main building on the South Site.\nThis was not an unusual move for Paul – he planted a kauri forest on one of his previous farms. He values the natural environment and wants to enhance it, and so, land remediation is a BPM priority.\nThe ability of zeolite to adsorb heavy metals and to treat groundwater has opened up markets where contaminants have restricted full use of land. There is abundant documentation available about the use of zeolites in the remediation of soil and ground water from radioactive contamination - from Long Island and Chernoble, to as recently as the Fukushima earthquake and tidal wave. Closer to home, soils and ground water containing heavy metals can be satisfactorily treated with granular zeolite.']"	['<urn:uuid:714605ab-8989-4cf4-84ab-9e9618685e31>', '<urn:uuid:eadbbd8e-a153-4c09-80de-86149dd40c05>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T18:05:20.075489	35	87	1551
42	What are the different types of scoliosis based on their causes, and what treatment options are available for managing this condition?	There are several types of scoliosis with different causes. In 80-85% of cases, the cause is unknown (idiopathic scoliosis). Other types include congenital scoliosis which develops in the womb, neuromuscular scoliosis caused by disorders like cerebral palsy or spinal cord injury, and degenerative scoliosis that develops in the lower back due to aging. Treatment options vary depending on the severity of the curve. They include observation for mild curves (less than 25 degrees), bracing for curves between 20-30 degrees in growing children, and surgery for curves more than 45 degrees that are getting worse. Additionally, specific exercises like the Schroth method can help prevent the condition from worsening and provide benefits such as improved core stability, better posture, enhanced lung function, and pain reduction.	"['|Return to Savvy Consumer Information Center - Home Page|\nQuestions and Answers About Scoliosis\n- What Is Scoliosis?\n- Who Gets Scoliosis?\n- What Causes Scoliosis?\n- How Does the Doctor Diagnose Scoliosis?\n- Does Scoliosis Have To Be Treated? What Are the Treatments?\n- Are There Other Ways To Treat Scoliosis?\n- Which Brace Is Best?\n- If the Doctor Recommends Surgery, Which Procedure Is Best?\n- Can People with Scoliosis Exercise?\n- What Are Researchers Trying To Find Out About Scoliosis?\n- Where Can People Get More Information About Scoliosis?\nThis fact sheet defines scoliosis and provides information about how it is diagnosed and treated in children and adolescents. You may be interested in contacting one or more of the organizations referenced at the end of the fact sheet for more information.\nWhat Is Scoliosis?\nScoliosis is a sideways curvature of the spine, or backbone. The bones that make up the spine are called vertebrae. Some people who have scoliosis require treatment. Other people, who have milder curves, may only need to visit their doctor for periodic observation. The section ""Does Scoliosis Have To Be Treated?"" describes how doctors decide whether or not to treat scoliosis.\nWho Gets Scoliosis?\nPeople of all ages can have scoliosis, but this fact sheet focuses on children and adolescents. Of every 1,000 children, 3 to 5 develop spinal curves that are considered large enough to need treatment. Adolescent idiopathic scoliosis (scoliosis of unknown cause) is the most common type and occurs after the age of 10. Girls are more likely than boys to have this type of scoliosis.\nSince scoliosis can run in families, a child who has a parent, brother, or sister with idiopathic scoliosis should be checked regularly for scoliosis by the family physician.\nIdiopathic scoliosis can also occur in children younger than 10 years of age, but is very rare. Early onset or infantile idiopathic scoliosis occurs in children less than 3 years old. It is more common in Europe than in the United States. Juvenile idiopathic scoliosis occurs in children between the ages of 3 and 10.\nWhat Causes Scoliosis?\nIn 80 to 85 percent of people, the cause of scoliosis is unknown; this is called idiopathic scoliosis. Before concluding that a person has idiopathic scoliosis, the doctor looks for other possible causes, such as injury or infection. Causes of curves are classified as either nonstructural or structural.\nNonstructural (functional) scoliosis: A structurally normal spine that appears curved. This is a temporary, changing curve. It is caused by an underlying condition such as a difference in leg length, muscle spasms, or inflammatory conditions such as appendicitis. Doctors treat this type of scoliosis by correcting the underlying problem.\nStructural scoliosis: A fixed curve that doctors treat case by case. Sometimes structural scoliosis is one part of a syndrome or disease, such as Marfan\'s syndrome, an inherited connective tissue disorder. In other cases it occurs by itself. Structural scoliosis can be caused by neuromuscular diseases (such as cerebral palsy, poliomyelitis, or muscular dystrophy), birth defects (such as hemivertebra, in which one side of a vertebra fails to form normally before birth), injury, certain infections, tumors (such as those caused by neurofibromatosis, a birth defect sometimes associated with benign tumors on the spinal column), metabolic diseases, connective tissue disorders, rheumatic diseases, or unknown factors (idiopathic scoliosis).\nHow Does the Doctor Diagnose Scoliosis?\nThe doctor takes the following steps to evaluate a patient for scoliosis:\nMedical history: The doctor talks to the patient and the patient\'s parent or parents and reviews the patient\'s records to look for medical problems that might be causing the spine to curve; for example, birth defects, trauma, or other disorders that can be associated with scoliosis.\nPhysical examination: The doctor looks at the patient\'s back, chest, pelvis, legs, feet, and skin. The doctor checks if the patient\'s shoulders are level, whether the head is centered, and whether opposite sides of the body look level. The doctor also examines the back muscles while the patient is bending forward to see if one side of the rib cage is higher than the other. If there is a significant asymmetry (difference between opposite sides of the body), the doctor will refer the patient to an orthopedic spine specialist (a doctor who has experience treating people with scoliosis). Certain changes in the skin, such as so-called cafe au lait spots, can suggest that the scoliosis is caused by a birth defect.\nX-ray evaluation: Patients with significant spinal curves, unusual back pain, or signs of involvement of the central nervous system (brain and spinal cord) such as bowel and bladder control problems need to have an X ray. The X ray should be done with the patient standing with his or her back to the x-ray machine. The view is of the entire spine on one long (36-inch) film. Occasionally, doctors ask for more tests to see if there are other problems.\nCurve measurement: The doctor measures the curve on the x-ray image. He or she finds the vertebrae at the beginning and end of the curve and measures the angle of the curve. Curves that are greater than 20 degrees require treatment.\nDoctors group curves of the spine by their location, shape, pattern, and cause. They use this information to decide how best to treat the scoliosis.\nLocation: To identify a curve\'s location, doctors find the apex of the curve (the vertebra within the curve that is the most off-center); the location of the apex is the ""location"" of the curve. A thoracic curve has its apex in the thoracic area (the part of the spine to which the ribs attach). A lumbar curve has its apex in the lower back. A thoracolumbar curve has its apex where the thoracic and lumbar vertebrae join.\nShape: The curve usually is S- or C-shaped.\nPattern: Curves frequently follow patterns that have been studied in previous patients. The larger the curve is, the more likely it will progress (depending on the amount of growth remaining).\nDoes Scoliosis Have To Be Treated? What Are the Treatments?\nMany children who are sent to the doctor by a school scoliosis screening program have very mild spinal curves that do not need treatment. When a child does need treatment, the doctor may send him or her to an orthopedic spine specialist.\nThe doctor will suggest the best treatment for each patient based on the patient\'s age, how much more he or she is likely to grow, the degree and pattern of the curve, and the type of scoliosis. The doctor may recommend observation, bracing, or surgery.\nObservation: Doctors follow patients without treatment and re-examine them every 4 to 6 months when the patient is still growing (is skeletally immature) and has an idiopathic curve of less than 25 degrees.\nBracing: Doctors advise patients to wear a brace to stop a curve from getting any worse when the patient:\nis still growing and has an idiopathic curve that is more than 25 to 30 degrees;\nhas at least 2 years of growth remaining, has an idiopathic curve that is between 20 and 29 degrees, and, if a girl, has not had her first menstrual period; or\nis still growing and has an idiopathic curve between 20 and 29 degrees that is getting worse.\nAs a child nears the end of growth, the indications for bracing will depend on how the curve affects the child\'s appearance, whether the curve is getting worse, and the size of the curve.\nSurgery: Doctors advise patients to have surgery to correct a curve or stop it from worsening when the patient is still growing, has a curve that is more than 45 degrees, and has a curve that is getting worse.\nAre There Other Ways To Treat Scoliosis?\nSome people have tried other ways to treat scoliosis, including manipulation by a chiropractor, electrical stimulation, dietary supplements, and corrective exercises. So far, studies of the following treatments have not been shown to prevent curve progression, or worsening:\nExercise:(Studies have shown that exercise alone will not stop progressive curves. However, patients may wish to exercise for the effects on their general health and well being.)\nWhich Brace Is Best?\nThe decision about which brace to wear depends on the type of curve and whether the patient will follow the doctor\'s directions about how many hours a day to wear the brace.\nThere are two main types of braces. Braces can be custom made or can be made from a pre-fabricated mold. All must be selected for the specific curve problem and fitted to each patient. To have their intended effect (to keep a curve from getting worse), braces must be worn every day for the full number of hours prescribed by the doctor until the child stops growing.\nMilwaukee brace: Patients can wear this brace to correct any curve in the spine. This brace has a neck ring.\nThoracolumbosacral orthosis (TLSO): Patients can wear this brace to correct curves whose apex is at or below the eighth thoracic vertebra. The TLSO is an underarm brace, which means that it fits under the arm and around the rib cage, lower back, and hips.\nIf the Doctor Recommends Surgery, Which Procedure Is Best?\nMany surgical techniques can be used to correct the curves of scoliosis. The main surgical procedure is correction, stabilization, and fusion of the curve. Fusion is the joining of two or more vertebrae. Surgeons can choose different ways to straighten the spine and also different implants to keep the spine stable after surgery. (Implants are devices that remain in the patient after surgery to keep the spine aligned.) The decision about the type of implant will depend on the cost; the size of the implant, which depends on the size of the patient; the shape of the implant; its safety; and the experience of the surgeon. Each patient should discuss their options with at least two experienced surgeons.\nPatients and parents who are thinking about surgery may want to ask the following questions:\nWhat are the benefits from surgery for scoliosis?\nWhat are the risks from surgery for scoliosis?\nWhat techniques will be used for the surgery?\nWhat devices will be used to keep the spine stable after surgery?\nWhere will the incisions be made?\nHow straight will the patient\'s spine be after surgery?\nHow long will the hospital stay be?\nHow long will it take to recover from surgery?\nIs there chronic back pain after surgery for scoliosis?\nWill the patient\'s growth be limited?\nHow flexible will the spine remain?\nCan the curve worsen or progress after surgery?\nWill additional surgery be likely?\nWill the patient be able to do all the things he or she wants to do following surgery?\nCan People with Scoliosis Exercise?\nExercise does not make scoliosis worse. In fact, it is very important for all people, including those with scoliosis, to exercise and remain physically fit. Girls have a higher risk than boys of developing osteoporosis (a disorder that results in weak bones that can break easily) later in life. The risk of osteoporosis is reduced in women who exercise regularly all their lives; and weight-bearing exercise, such as walking, running, soccer, and gymnastics, increases bone density and helps prevent osteoporosis. For both boys and girls, exercising and participating in sports also improves their general sense of well being.\nWhat Are Researchers Trying To Find Out About Scoliosis?\nResearchers are looking for the cause of idiopathic scoliosis. They have studied genetics, growth, structural and biochemical alterations in the discs and muscles, and central nervous system changes. The changes in the discs and muscles seem to be a result of scoliosis and not the cause. Scientists are still hopeful that studying changes in the central nervous system in people with idiopathic scoliosis may reveal a cause of this disorder.\nResearchers continue to examine how a variety of braces, surgical procedures, and surgical instruments can be used to straighten the spine or to prevent further curvature. They are also studying the long-term effects of a scoliosis fusion and the long-term effects of untreated scoliosis.\nWhere Can People Get More Information About Scoliosis?\nNational Scoliosis Foundation\n5 Cabot Place\nStoughton, MA 02072\n(800) NSF-MYBACK (673-6922)\nFax: (617) 341-8333\nThis is a nonprofit voluntary organization that provides pamphlets, a newsletter, and other information materials on childhood and adult scoliosis. The foundation also provides support group information and lists of physicians in each State who specialize in scoliosis.\nThe Scoliosis Association, Inc.\nP.O. Box 811705\nBoca Raton, FL 33481-1705\nFax: (561) 994-2455\nThis association publishes a quarterly newsletter and pamphlets. A single copy of their fact sheet is available free with a self-addressed, stamped envelope. The association also provides information about local chapters and support groups.\nThe Scoliosis Research Society\n6300 North River Road\nRosemont, IL 60018-4226\nFax: (847) 823-0536\nThe society is a professional organization for orthopaedic surgeons interested in scoliosis. It provides pamphlets about the diagnosis and treatment of scoliosis. Price information for ordering pamphlets is available from the society. The society also can provide referrals to physicians.\nAmerican Physical Therapy Association\nP.O. Box 37257\nWashington, DC 20013\nTo obtain a copy of the American Physical Therapy Association\'s brochure about scoliosis, please send a self-addressed, stamped envelope to the above address.\nThe NIAMS gratefully acknowledges the assistance of Dr. Stuart Weinstein, Professor of Orthopedic Surgery, University of Iowa, in the preparation and review of this fact sheet, and Dr. John Lonstein, Department of Orthopaedic Surgery, University of Minnesota, and Dr. James Panagis, NIAMS, in the review of this fact sheet.\nThe National Arthritis and Musculoskeletal and Skin Diseases Information Clearinghouse (NAMSIC) is a public service sponsored by the NIAMS that provides health information and information sources. The NIAMS, a part of the National Institutes of Health (NIH), leads the Federal medical research effort in arthritis and musculoskeletal and skin diseases. The NIAMS sponsors research and research training throughout the United States as well as on the NIH campus in Bethesda, MD, and disseminates health and research information.\n|Return to Savvy Consumer Information Center - Home Page|\nGet the Savvy Consumer Newsletter! (FREE)', 'Scoliosis Symptoms To Watch Out For In Your Children\nAs with all conditions and diseases, early screening - and therefore early detection can lead to improved quality of life in the long run. That’s why it’s so important to watch out for these scoliosis symptoms, especially in children before they go through their adolescent growth spurts. Read on to learn more.\nWhat Causes Scoliosis?\nAbout 80% of scoliosis cases have no known cause. These fall under Idiopathic Scoliosis, where no exact reason is discovered for the curve of the spine.\nOther causes of scoliosis are as follows:\nCongenital scoliosis, which is a rare condition, develops in the womb before birth. Development problems in the vertebrae cause the spine to curve. It can also be the result of vertebrae failing to divide properly.\nNeuromuscular scoliosis is caused by disorders like spina bifida, cerebral palsy, or injury to the spinal cord. This damages the muscles, decreasing support for the spine and causing back curvature.\nDegenerative scoliosis develops in the lower back as a result of spine degeneration, caused by aging. This type of scoliosis commonly affects adults.\nScoliosis Symptoms In Children: Why It’s Important To Catch It Early\nScoliosis is a condition that manifests in the spine twisting and curving to the side. It affects people of all ages, but is most commonly detected in adolescents aged between 10 to 15 - an age where many undergo growth spurts that accelerate the visibility and effects of scoliosis symptoms. Because of this, it’s especially important to detect the presence of the condition early on, as proper treatment will allow for easier management and improvement of the spinal curvature. Early treatment will also ensure that the spinal curvature does not worsen.\nMany scoliosis patients improve with treatment. The goal is to detect it early, and to keep a watch so as to provide timely treatment should it worsen. For this purpose, it is recommended that parents begin screening their children as early as when they are 10 years old. Girls should be screened twice from ages 10 to 12, whilst boys should be screened once from ages 11 to 13.\nWhat Are Some Symptoms of Scoliosis?\nWhile the scoliosis symptoms may not show up the same way for all patients, these symptoms are general indicators.\nOne shoulder blade that is more prominent than the other\nOne hip higher than the other\nOne side of the rib cage jutting forward\nProminence on one side of the back when leaning forward\nTreating The Symptoms of Scoliosis\nTreatment for scoliosis varies depending on the severity of the curve. That’s why it’s important to speak to a certified practitioner to find the right treatment plan for you. Some scoliosis patients may require bracing, whilst others can benefit from scoliosis specific exercises.\nThe Schroth method incorporates scoliosis specific exercises that prevent the illness from worsening, saving cost and avoiding the trauma of surgery. Depending on the patient’s age range, degree of curvature, and bone maturity, bracing may be included in the treatment. Other benefits of the treatment include:\nReduction of Cobb’s angle (prior to skeletal maturity)\nHalting of curve progression\nImproved core stability, balance, strength and symmetry\nImproved postural appearance\nImproved vital capacity (improves lung function and breathing)\nReduction and elimination of pain if present\nImproved overall movement patterns and function\nImproved self-management and understanding of the spine\nImproved overall aesthetic of the spine and body']"	['<urn:uuid:d82b4a20-d0fb-430f-8176-07b14d5f1d5a>', '<urn:uuid:4e780f4b-e376-40d8-92a6-6e5f9c77f787>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T18:05:20.075489	21	124	2917
43	student teaching models benefits limitations	The Dick & Carey Model of Instructional Design emphasizes constructivism and offers key benefits including focus on learner outcomes, consideration of prior knowledge, thoughtful skill development, and ability to revise and improve instruction. However, when implementing such teaching models during challenging times like the pandemic, there are limitations as teachers face difficulties in maintaining student engagement and need to adapt to remote or blended environments, making it harder to implement structured approaches to learning.	"['Dick & Carey Systems Model of Instructional Design\nMarch 17, 2013. Original design by Mel Burgess\nThe Dick & Carey Systems Model of Instructional Design is a step-by-step guide of how to deliver educational content. The Model itself is not the delivery method, it is simply an example of how learning can be structured in a way that puts the learners\' interests first, while incorporating feedback at all levels of the design in order to improve further teaching lessons.\n- 1 History\n- 2 The Dick & Carey Model\n- 3 Components of the Dick & Carey Model\n- 3.1 Assess Needs to Identify Goal(s):\n- 3.2 Conduct Instructional Analysis:\n- 3.3 Analyze Learners and Contexts:\n- 3.4 Write Performance Objectives:\n- 3.5 Develop Assessment Instruments:\n- 3.6 Develop Instructional Strategy:\n- 3.7 Develop and Select Instructional Materials:\n- 3.8 Develop and Construct Formative Evaluation of Instruction:\n- 3.9 Revise Instruction:\n- 3.10 Design and Conduct Summative Evaluation:\n- 4 The Dick & Carey Model and Individualized Education\n- 5 References\nWalter Dick received his undergraduate from Princeton, then his PhD from Penn state University in Educational Psychology before teaching at Florida State University. Dick studied alongside Robert Gagne and was heavily influenced by the work of Gagne, specifically his “The Conditions of Learning” published in 1965. Lou Carey was a graduate student of Walter Dick and later was co-author with Dick to “The Systematic Design of Instruction” published in 1978. Her husband, James, joined in the effort to help write this book due to his expertise in educational technology and new media technologies. Together, Dick and Carey developed a model of Instructional Design described as the “Systems Approach Model” which was presented within “The Systematic Design of Instruction”. This model has simply become known and the “Dick & Carey Model”.\nThe Dick & Carey Model\nThis systems approach model incorporates 10 interrelated parts, which depend upon each other, and offer feedback for each other to accomplish an educational goal or set of goals. Based on many years of research, learning theory, and practical experience, the Dick & Carey Model is used to plan, design, implement, and evaluate instruction . Described by Gustafson and Branch (2002) as ""one of the most popular and influencial ID models"", the Dick & Carey Model places emphasis upon constructivism, which at its focus, describes learning as being “constructed” as new information combines with previous knowledge and/or experiences, and therefore dependent upon appropriate learning environments designed by the teacher.\nComponents of the Dick & Carey Model\nThe 10 components (Dick and Carey, 1990) of this systems approach relate to each other directly, as indicated by the solid lines in Figure 1, and also indirectly, through feedback during and after learning, as represented by the dashed lines in Figure 1.\nFigure 1. The Dick & Carey Model of Instructional Design\nAssess Needs to Identify Goal(s):\nEducators must determine what it is that we wish learners to learn through assessment, experience, analysis, practical requirement, and/or descriptive goals.\nConduct Instructional Analysis:\nThe entry behaviours required of learners must be determined, that is, the skills, knowledge, and attitudes necessary to begin instruction\nAnalyze Learners and Contexts:\nThe learners’ current level of skill, preferences, and attitudes are determined as is the environment in which learners will learn and then use new skills.\nWrite Performance Objectives:\nThe teacher writes specific outcomes of student learning. These outcomes outline the skills to be learned, the conditions under which skills are shown, and criteria for success.\nDevelop Assessment Instruments:\nThe assessments utilized by the instructor should parallel the learners’ ability to show what was described by the learning objectives. Measurement of the desired behaviours is represented by the assessment.\nDevelop Instructional Strategy:\nThe teacher will identify strategies to be used that best elicit learning. Strategies include pre-instructional activities, how information will be presented, as well as how students will practice and receive feedback before testing. An emphasis on current learning theories and research is placed on strategy selection within the context of learning.\nDevelop and Select Instructional Materials:\nThe artifacts of instruction are presented, such as a learner’s manual, tests, and all other materials of instruction (ie. modules, learning activity packages or “handouts”, interactivities, resources, etc.).\nDevelop and Construct Formative Evaluation of Instruction:\nData is collected on how to improve upon instruction through evaluation of the preliminary learning. Evaluations in the form of one-to-one evaluation, small-group evaluation, and field evaluation are performed. The results of these evaluations provide valuable feedback on how to improve instruction.\nThe results of the evaluation shed light on the validity of the instructional analysis and assumptions of leaners’ behaviours and characteristics. The instructional strategy is reviewed and altered accordingly to be more effective. This final step can also be seen as the first step when repeating instruction.\nDesign and Conduct Summative Evaluation:\nBecause summative evaluation does not involve the creator of instruction (usually), this component is often regarded as being separate from the design process. This evaluation determines the overall value of the instruction.\nThe Dick & Carey Model and Individualized Education\nSome of the key benefits (Dick and Carey, 1990) to using this systems model include:\n- A focus is placed on what learners will be able to do when learning is complete.\n- A focus is placed on what learners must know in order to begin instruction.\n- Skills and knowledge are thoughtfully taught.\n- Conditions for learning are considered.\n- Instruction is designed to be replicated, and therefore revised and improved upon.\n- A wide variety of potential users, including teachers and others involved in instructional design.\nIt is important to note, that this model of ID is used to design instruction, and is not in itself the delivery method. Instructors, modules, and computers are examples of ways that learning is delivered. The Dick & Carey Model can be used to design individual learning as well as design instruction of groups, whether the instruction itself is being teacher-led or self-directed in nature. As such, the Dick & Carey model serves as a valid model of systems design for initiatives such as ""21st century learning"".\nGustafson, K. L., & Branch, R. M. (2002). What is instructional design. Trends and issues in instructional design and technology, 16-25.\n- Dick, W., & Carey, L. (1990). The systematic design of instruction. New York: Harper Collins. Chapter 1: Introduction to instructional design (pp. 2-11).\n- (2006). Dick & Carey - OpenContent Wiki. Retrieved February 20, 2013 from http://www.opencontent.org/wiki/index.php?title=Dick_%26_Carey.\n- (2005). Robert M. Gagné - Wikipedia, the free encyclopedia. Retrieved February 21, 2013, from http://en.wikipedia.org/wiki/Robert_M._Gagn%C3%A9.\n- (2004). Instructional design - Wikipedia, the free encyclopedia. Retrieved February 20, 2013, from http://en.wikipedia.org/wiki/Instructional_design.\n- ""Ernst & Young\'s Practice of Designing Global E-Learning Program."" 2012. 21 Feb. 2013 <http://www.richardhe.com/2008/ernst-young-practice-of-designing-global-e-learning-program-a-review-from-the-isd-perspectives/>.', 'BY LAURA RIGOLOSI & SHERRISH HOLLOMAN\nProblems. There’s no shortage of them these days — the pandemic has spurred countless challenges and intense despair; there are too many to list. Teaching during the pandemic has been a challenge in and of itself, as we are always looking for ways for students to be engaged with curricula and drive their learning, and that’s hard to do whether we’re teaching in person or remotely.\nIf you think back to your college or grad school days, you may recall the constructivist thinkers, such as Jean Piaget, who believed that students learn best when they construct their own learning. Problem- and project-based learning offers teachers an opportunity to do just that — instead of telling students the answers, you can create a learning environment in which students learn through discovery, thinking, tinkering, reflecting, and developing answers on their own.\nWe may already be familiar with ways that we can bring this type of learning to in-person classrooms, but it can also be delivered to students who are learning in remote or blended environments.\nProblem-based learning vs. project-based learning\nProject-based learning is situated in real-life learning. The Buck Institute for Education defines project-based learning as a “teaching method in which students gain knowledge and skills by working for an extended period of time to investigate and respond to an authentic, engaging, and complex question, problem or challenge.” If you ever walk into a classroom and see students working on a project with an exciting buzz in the room, chances are, their teachers have designed a project-based learning task.\nIn our own lives, we know that when working on a project, we often discover a problem we didn’t realize we had — but once it surfaces, it demands a solution. (Remember those early pandemic days when we were acclimating to teaching remotely, but also trying to solve the problem of having no dedicated teaching space at home?) In teaching, this idea rings true, too. As we are learning more about a topic, we may discover a problem alongside our students, and this is the breeding ground for an exciting new project. This is the foundation of problem-based learning.\nProblem-based learning also offers students real-life learning opportunities, as well as the chance “to think creatively and bring their knowledge to bear in unique ways” (2020 Schunk, p. 64). Problem-based learning can look differently depending on the content and grade level, but often includes group discussions that allow for multiple perspectives on a topic, a simulated situation that involves role playing, or group work that includes both collaborative work and time to complete tasks individually.\nProblem-based learning promotes autonomous learning, self-assessment skills, planning time, project work, and oral and written expression skills. According to a July 2020 article from the Hechinger Report, problem-based learning has gained tremendous momentum, because it allows students to work more freely and at their own pace — a key advantage when learning remotely. In problem-based learning, the content and skills are organized around problems, rather than as a hierarchical list of topics. It’s also inherently learner-centered because the learner actively creates their own knowledge as they attempt to solve the problem.\nPutting the “Problem” into Practice\nAs former English teachers, we both understand the challenge of putting new professional learning into practice. For teachers who need a refresher on how to design a problem-based learning experience for their students, Problem Based Learning: Six Steps to Design, Implement and Assess breaks down the steps to move PBL into practice as follows:\nTo help put these problem-based steps into perspective, we can look to our recent work with partners from a high school in the South Bronx. The chemistry team there decided to use an anti-racist lens while addressing a problem that was very real to their students — fireworks. During the summer of 2020, there was a record number of firework incidents in New York City. According to an article in the New York Times, the city received over 1,700 fireworks complaints in the first half of June alone. Our partners used this problem as an opportunity for students to research fireworks from multiple lenses, and imagine how they might present their findings and recommendations to local officials. After all, shouldn’t New York Governor Andrew Cuomo hear from high school students in the Bronx about the effects fireworks have on their communities?\nHere’s what the framework might look like in this example:\nFrom here, we can imagine the possibilities for this framework, considering how students might address the underlying problem from different perspectives and content areas:\nTeaching and learning throughout a global pandemic has presented more than its share of challenges. Out of necessity, tremendous innovation has taken place with the use of technology, pedagogy, and curriculum. With problem-based learning, we can continue this innovation in our classrooms, offering our students opportunities to solve real world problems, demonstrate critical thinking, and collaborate with their peers. We would love to hear what problem-based learning tasks you are designing for your classrooms!']"	['<urn:uuid:1f5d025b-12d8-4b6c-8d24-2c8449a5448c>', '<urn:uuid:2285de50-3939-41b9-a10b-f1d64c1e528a>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T18:05:20.075489	5	74	1959
44	self sufficient house water storage techniques	The EarthShip home achieves water self-sufficiency through rainwater harvesting from its metal roof into cisterns, reusing water three times before disposal. For rainwater storage, two main methods are available: direct storage in tanks/cisterns for immediate use, or recharging groundwater aquifers through various techniques like recharge pits, soak ways, dug wells, or percolation tanks. The storage units can range from small ceramic jars to large Ferro cement tanks depending on the scale of the system.	"[""USED TIRES PACKED WITH DIRT and rocks, each weighing 300 to 400 pounds, form three exterior walls on an “EarthShip” home being built by the Severin family of Pomona. Allen Severin, left, and Mary Kay Severin, right, along with their son, Arron, have been working for about a year on the 2,000-square-foot “green” home. The northeast corner of the home is shown. The interior row of tires shown is being used as scaffolding. They are standing on an area of backfill and to the right of them is a “cooling tube.” A “How to Build an Earthship'' hands-on workshop is offered May 30, 31 and June 1. For more on that call the Severins at 417-274-9885. (Quill/Hampton)\nPosted: Monday, May 19, 2014 1:17 pm | Updated: 1:21 pm, Mon May 19, 2014.\nBY TERRY HAMPTON, Quill Staff Writer | 0 comments\nKeeping 800 used tires and 8,000 empty water bottles out of a landfill in a creative way is as easy as building a house with them. Which is to say it's not easy but worth it, said Allen and Mary Kay Severin who are building an “EarthShip” house near Pomona.\nThey are combining the cast-off tires and bottles along with using other recycled and natural materials and investing thousands of hours of labor to eventually be able to live off of the grid, free from public utilities.\nThe Quill first introduced readers to the Severin family on Oct. 31, 2013. Allen and Mary Kay and their son, Arron, moved to the Pomona area from Wisconsin Rapids, Wisc., in April 2013 with the goal of building a 2,000-square-foot “green” home. They started groundwork over the summer and filled the first row of tires in September. The Quill recently visited the Severin’s property to get an update on progress.\nThey said that the extreme winter weather conditions presented many challenges and put a halt to work on many days. The family has been living in a camper since arriving in the Ozarks, which was rough during the cold. “But we survived,” said Allen with a smile. “We're from Wisconsin, so it was the best winter we'd seen in a while.”\nThe Severin's determination to remain upbeat and undaunted is an asset which serves them well in what sometimes must surely seem to be an overwhelming task. Asked if they have second thoughts about building the house, they both said no. “If we hadn't gone to visit an EarthShip home we might ask ourselves, 'What the heck are we doing?' But we know how great it's going to be, so we have no regrets,” said Mary Kay.\nThree of the exterior walls of the Severin house are being built from used tires that are filled with rocks and dirt, compacted to make giant “bricks.” Construction is about halfway done on those walls which are being earth bermed, completely surrounded by dirt on the outside. The south side of the house will be a glass wall and will incorporate a green house.\nSince the tires weigh 300 to 400 pounds each when filled, they are put in place before filling. They are laid one row at a time. They are over halfway done with the fifth row, which means they are almost halfway done with the exterior walls. The Severins have done most of the work themselves, but in recent times have had some volunteers helping them for which they say they are extremely grateful.\nThe tires and dirt provide thermal mass which means the house will not require heating or air conditioning units. The sun’s warmth which is collected and released by the home in the winter and the use of cooling tubes in the summer will provide year-round inside temperatures of 70 to 75 degrees. There will be three cooling tubes buried underground where the temperature is a constant 58 degrees. Air from the tubes which will be naturally drawn through the house by opening a skylight will serve to cool the home in the summer.\nSolar panels will provide electricity which will be sufficient for almost every ordinary use, except generating the significant heat required to run a clothes dryer. An outdoor clothes line is already in place.\nRain will run off the home’s metal roof to be collected and stored in large plastic cisterns. The rainwater will be used three times, including to water plants in the greenhouse, before it goes into a septic system after its final use to flush the toilet. The interior walls will be plastered with a natural stucco, tinted with three different shades of clay gathered from the property.\nA CHANGE IN PLANS\nThey are following a technique which has been developed over the past 40 years by architect Michael Reynolds who holds the registered trademark on the name EarthShip. Some of the details of their plan are evolving as they go. They had initially intended to use tires as the interior, load-bearing walls to divide the rooms and support the roof, but they realized they would be losing a lot of square footage and have changed their plans. They are going to use plastic water bottles which would otherwise be discarded. They estimate they'll need 8,000.\nThe 16.9-ounce bottles are filled with dirt, packed down in layers. The process takes about 10 minutes per bottle. The bottles will be placed side by side in rows to form walls and mudded together to hold them in place. Mary Kay said, “This is how we are going to build our interior kitchen, living room, dining room and bathroom walls. The filled bottles are 20 times stronger than a brick, fire proof and provide thermal mass as well. It fits right in with an EarthShip.”\nWhen finished the Severin’s home will be completely self-contained, and it will allow them to be self-reliant which has been their desire for many years. They would also like to encourage others to pursue a sustainable lifestyle and are willing to share their methods with anyone who is interested in learning the process.\nOn May 30 and 31 and June 1 the Severins will host a “How to Build an EarthShip” hands-on workshop. It is free but limited to 20 attendees. For those who would like to attend from outside the immediate area, primitive camping sites are available. Contact the Severins at 417-274-9885 for specific information about times and what to bring. Prior to the workshop they will be looking to build up their supply of water bottles. They encourage schools, churches and other groups to save drinking water bottles, caps must be included. They are willing to pick up bottles in the Pomona-West Plains area. Call to make arrangements.\nFINISHING THE HOUSE\nFor anyone who would like to lend a hand with construction, the Severins would appreciate volunteers and say it's a great way to learn the process. Some might not be physically able to “pound a tire,” but almost anyone can fill the bottles, a process which will take well over a 1,000 hours.\nTheir original goal was to be done with the house by Christmas of this year but the rough winter changed that plan. They do anticipate having the exterior walls and roof on by sometime in June and then they will be working on the interior. “We don’t want to spend a second winter in our camper so we plan to have the house done enough to stay in it this winter,” said Mary Kay, “even if we have to camp out in it.”\nMary Kay is chronicling their journey online with her blog at severinearthship.weebly.com. All are also invited to like their page on Facebook “Severin Earthship in the Ozarks.” The website earthship.com provides more information about “radically sustainable buildings.”\nMary Severin writes about her families embarkment on a learning adventure to build an Upcycled Home using Earthship Principals. To find out what these Principals are click on the Earthship Principals link above."", 'Water is considered as the basis of life as it is a vital natural resource for sustaining life on Earth. It is essential for both domestic use as well as for commercial use. Water is used for drinking, washing, electricity generation, irrigation, recreation, navigation and so on. Thus, the widespread use of water resources has increased the need for determining new sustainable sources of water resources. One such source of water resources is rainwater harvesting. Two methods of rainwater harvesting are explained below.\nRainwater harvesting is an effective and sustainable method of water utilization. In general, it is the method of collection and storage of rainwater into tanks, cisterns and other natural and artificial storage units.\nIt is a very effective technique for collecting ample amount of rainwater which can be used for various domestic and other purposes.\nMostly, in the rainwater harvesting, the runoff from the pervious and impervious surfaces such as roofs is allowed to infiltrate into the suitable storage units or into underground aquifers.\nIn this regard, rainwater harvesting can be defined as the technique of collection, storage and utilization of the rainwater that falls on roofs and other surfaces for immediate as well as future use by means of storage tanks or recharging of the collected rainwater into subsurface sources such as tube wells.\nThe main objectives of the rainwater harvesting can be listed as follows:\na. To meet the water demand of the growing population and solve the problem of inadequacy in the water supply.\nb. To increase the underground water levels.\nc. To ensure sustainable use and storage of water.\nd. To increase agricultural production and improve the groundwater quality of a particular area by dilution.\ne. To improve the ecological balance.\nSUITABILITY OF RAINWATER HARVESTING\nIt is desirable to establish a rainwater harvesting system in the areas where;\na. Underground water levels are significantly depleting at a frequent interval.\nb. Availability of groundwater is minimal, particularly during the dry season.\nc. Subsequent de-saturation of the underground water sourced has occurred.\nd. Other sources of water are not adequate.\nMETHODS OF RAINWATER HARVESTING\nBasically, two major methods of rainwater harvesting are used namely rooftop rainwater harvesting and surface runoff harvesting. These methods of rainwater harvesting have been further described below.\n1. Surface Runoff Water Harvesting:\nSurface Runoff Water Harvesting is commonly used in urban areas.\nIt can be defined as the method of collection, accumulation, treatment and storage of the runoff water for immediate and future use. Most commonly the collected water is used for the recharging of subsurface aquifers. The method can also utilize the runoff from other catchment areas, manmade surfaces, road surfaces, parks, gardens etc.\nIf an appropriate method is used, this method can efficiently collect a large amount of water. However, a suitable method of treatment must also be provided as the runoff water may contain certain impurities.\nSurface runoff harvesting method can be used for solving the problem of water scarcity as well as the problem of submergence.\nUsually, for the storage of the collected water, reservoirs provided with earthen bunds or embankments are excavated in the ground. The size of such reservoir may range up to 12 ha.\nBENEFITS OF SURFACE RUNOFF HARVESTING\nSome of the benefits offered by surface runoff harvesting can be listed as follows:\na. This method can be used to collect water for agricultural use and irrigation.\nb. The water collected can be efficiently used for other productive and livelihood purposes.\nc. This method of rainwater harvesting is environmentally compatible.\nd. It prevents the loss of freshwater to sea and oceans.\n2. Rooftop Rainwater Harvesting\nIn rooftop rainwater harvesting; rainwater is collected directly from the rooftop of buildings.\nIn this system, the roof of the building serves as the catchment from which the water is transferred to the storage reservoirs and artificial recharge systems.\nRooftop rainwater harvesting is a commonly adopted method of rainwater harvesting as it is economical and highly useful.\nCOMPONENTS OF ROOFTOP RAINWATER HARVESTING\nA typical rooftop rainwater harvesting system consists of the following major components:\na. Rooftop (Catchment)\nThe rooftop serves as the catchment in rainwater harvesting. The catchment is the component of the rainwater harvesting system that directly receives the rainfall i.e. the surface upon which the rainwater falls directly. Such catchment may be a terrace or rooftop of a building. The catchment basically serves as the water contributor in the system.\nb. Pipes (Conveyance)\nAnother essential component of the rainwater harvesting system is the pipes.\nThe pipes are also commonly referred to as the conveyance as it serves as the medium to carry the collected rainwater from the catchment to the storage reservoirs and tanks.\nIn general, the accumulated rainwater from the rooftops and terraces are caught by means of the gutters from which the water is further conveyed by means of the pipe.\nMostly, the UV resistant pipes such as HDPE pipes, PVC pipes etc are used for the purpose of rainwater harvesting.\nIt must be noted that the mouth of the drain must be provided with wire mesh to check the entry of the debris and other floating matters.\nc. First Flush\nUsually, the rainwater that is collected may contain certain impurities initially. Thus, the first accumulated rainwater is flushed off in order to prevent the contamination(like acid rain) of the water stored in the reservoirs. The device used for this purpose is known as the first flush. The first flush must be provided in each outlet of the drain pipe.\nFilters are the components of the rainwater harvesting system used for the treatment of accumulated rainwater.\nFiltration of rainwater is done so as to remove the possible impurities that may be present in the water, remove the pathogens, turbidity in water etc.\nThe water from the first flush is further passed to the filter where purification of the water takes place.\nVarious types of filters can be used as per the requirement. Some of the commonly used filters are as follows:\n1. Sand Gravel Filter\n2. Charcoal Filter\n3. PVC Pipe Filter\n4. Sponge Filter\ne. Storage Unit\nThe storage unit is used to store the collected water. A wide range of storage units can be used depending upon the scale of the harvesting system.\nIn case of the small scale roof harvesting system, small storage units such as storage tanks, ceramic or plastic jars, drums etc are commonly used.\nOn the other hand, for large scale harvesting system, large storage tanks usually made up of Ferro cement or cement rings etc are mostly used.\nTYPES OF ROOFTOP RAINWATER HARVESTING\nThe rooftop rainwater harvesting can be further classified into the following types:\na. Rooftop Rainwater Harvesting Method of Storage for Direct Use\nIn this method, the rainwater accumulated at the catchment is directly diverted or passed to the storage unit. Usually, a large storage tank is used as the storage unit which is primarily designed based on the water demand, requirements and the expected rainfall.\nThe system basically consists of the catchment, conveyance pipes, first flush, filters and the storage tank. In addition, an overflow system is also provided to account for the excess water collected. Alternately, the excess water may also be used for the underground recharge.\nThis is a very common method of rainwater harvesting as it is highly cost-effective.\nThe rainwater can be effectively stored during the rainy season and thus ample amount of water will be available for the dry periods. This method is also energy efficient as it prevents the distribution and transportation of water.\nb. Rooftop Rainwater Harvesting Method for Recharging Groundwater Aquifers\nIn this method, the accumulated water from the catchment is used primarily for the purpose of recharging the subsurface aquifers.\nIt can be further classified into the following:\n1. Recharging of bore wells\n2. Recharging of dug wells.\n3. Recharge pits\n4. Recharge Trenches\n5. Soak ways or Recharge Shafts\n6. Percolation Tanks\nThese methods have been explained in brief below.\n1. Recharging of Bore Wells\nIn this method, the rainwater from the catchment is conveyed to the settlement tanks or filtration tanks by means of the pipes.\nThe settlement tank or filtration tank of appropriate capacity is used on the basis of the catchment area, expected rainfall, recharge rate etc. The primary objective of providing the settlement tank or the filtration tank is to check the entry of contaminants and other impurities that may cause clogging of the recharge components.\nAfter the settlement or the purification of the water, the filtered water is conveyed to the boreholes for the recharging of subsurface aquifers.\n2. Recharge Pits\nIn this type, the groundwater or aquifer recharge is accomplished by means of recharge pits.\nRecharge pits refer to the small pits made up of brick masonry or stone masonry usually rectangular, square or circular in shape.\nThe pit consists of a dug hole at the centre and is provided at regular intervals. To prevent the contamination, perforated covers or wire meshes are provided at the top of the pit whereas the bottom portion of the pit is duly covered with appropriate filter media.\nUsually, the dimensions of the pit are selected on the basis of the depth of the previous layer and may range from 1 to 2 m in width and 2 to 3 m in depth.\n3. Soak ways or Recharge Shafts\nIn general, the soak ways refer to the boring holes that are usually 30 cm in diameter and about 10 to 15 m deep.\nThis type of method is similar to the recharge pits and is mostly used in the areas where the soil consists of an upper alluvial layer or less porous layer.\nThe bored holes are also provided with the lining made up of slotted or perforated PVC or MS pipe so as to prevent the falling off of the vertical sides of the boring side.\nLike the recharge pits, the soak ways are also provided with perforated covers on the top and filter media at the bottom.\n4. Recharging of Dug Wells\nIn this method, dug wells are used as the recharge structure for the recharge of subsurface aquifers.\nThe rainwater from the catchment is duly filtered and conveyed to the dug wells. Proper maintenance of the dug well must be ensured.\n5. Recharge Trenches\nThis method is similar to the recharge pits method and is adopted in the areas where the upper impervious layer of the soil is relatively shallow in nature.\nThe dimensions of the recharge trench can vary from 0.50 to 1.0 m in width and 1.0 to 1.5 m in depth.\n6. Percolation Tank\nIn this method, a percolation tank is used as the recharge structure for the recharge of the underground aquifers.\nThe percolation tanks basically refer to the manmade or artificially created water bodies that submerge a land with adequate permeability so as to recharge the groundwater.\nRead More: Water Distribution System\nRead More: Water Treatment Process\nWe are sorry that this post was not useful for you!\nLet us improve this post!\nTell us how we can improve this post?']"	['<urn:uuid:157ce390-c51d-4982-ab0b-4b3816a10818>', '<urn:uuid:2cd93195-15bc-4c8b-82cc-eb6600e9077a>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T18:05:20.075489	6	74	3164
45	What methods were used to copy Buddhist texts in 1970s Korea?	Because photocopiers weren't easily available, copies were made using mimeograph stencils. The text was handwritten onto the stencil using a pen, and then the stencil was hand inked over a flat piece of paper, producing a copy of the text.	['We just finished reciting the Thousand Hands Sutra, and starting tomorrow (Korea time), I’ll begin posting Daehaeng Kun Sunim’s translation of the Heart Sutra. Several years ago, I wrote a paper that had a section about the history and motivations behind her translations of these sutras and the daily ceremonies from their traditional, classical Chinese characters (hanmun) into vernacular Korean (Hangul). It occurred to me that some of you may be curious about this as well, so I’ll post that section here. It was an academic paper, so that’s why the tone may seem a little dry.\nwith palms together,\n- Seon Master Daehaeng’s Translation of Buddhist Ceremonies\nBy 1977, Seon Master Daehaeng had started work on translating the Heart Sutra and the Thousand Hands Sutra, which form the core of daily ceremonies used at the temple she founded, Hanmaum Seon Center, as well as in Buddhist temples across Korea.\nStarting with the hanmun text, and using the understanding born of her own spiritual practice and awakening, Seon Master Daehaeng would write out the meaning of the verses using vernacular Hangeul. She would sometimes have an assistant work ahead of her, looking up and writing down all the related meanings of select Sino-Korean characters. (This can be a time consuming task, which is probably why she delegated it.) When she was finished, she would give that section of the daily ceremonies to key laypeople and sunims to look over. If there were any questions, they would go over the text again until it expressed the meaning Seon Master Daehaeng had intended. After several repetitions of this, the text was given to the sunims at the center and made available to lay members in a copied format.\nBecause photocopiers weren’t easily available, copies were made using mimeograph stencils. The text was handwritten onto the stencil using a pen, and then the stencil was hand inked over a flat piece of paper, producing a copy of the text. In this way, each section of the daily ceremonies was made available as it was finished.\nInterestingly, members of Hanmaum Seon Center who were present at the time report that no one even questioned the need for Hangeul translations of the daily ceremonies. Everyone seemed to take it for granted that they were both needed and welcome.\nAs her new translations became available, some quickly became used in the daily ceremonies instead of the traditional versions. However, as will be explored below, she didn’t translate all of the ceremonies, and instead continued to use the original hanmun versions for some parts of the ceremonies. It appears the need for balance between the modern and the traditional was largely behind this.\nA formal, complete collection of the daily ceremonies with Seon Master Daehaeng’s Hangeul translations was published in late 1987, titled Shinhaeng Yojeon(“Essentials of Faith and Practice”).\nIn addition to the Heart Sutra and the Thousand Hands Sutra, this collection also included Hangeul translations of most of the other elements of the daily ceremonies, such as the morning bell chant, the evening ceremonies, and other texts used during funerals and memorial ceremonies, such as Beobseong gye(法性偈, “Song of Dharma-nature”), and Musang-gye(無相戒, “The Truth of Formlessness”).\nIn 1990, a cassette recording was produced, featuring the Thousand Hands Sutra, the Heart Sutra, and other central elements of the daily ceremonies. At the time, this recording was sold in most of the Buddhist stores in Korea. The chanting was done by Bowon Sunim.\nIn later years, Seon Master Daehaeng also finished a translation of the Diamond Sutra(金剛經), and completed a substantial portion of the Flower Ornament Sutra (華嚴經) before passing away in 2012.\nShinhaeng Yojeon raises an important question for future research, because according to people who were present then, not all of the Hangeul translations it includes are by Seon Master Daehaeng. While all of the contents were reviewed and approved by her, the process and sources for some of the Hangeul texts is unclear.\nSome, such as the Heart Sutra and Thousand Hands Sutra are unquestionably her work. Others, such as the Seven Homages(七頂禮) and the Morning Bell Chant(朝禮鍾頌), have such depth of meaning and expression that it seems unlikely anyone else could have produced them. Still others, such as Song of Dharma-nature(法性偈), and The Truth of Formlessness(無相戒) closely follow the traditional hanmun versions, without any of the expressions unique to Seon Master Daehaeng’s works. It seems that if she made any changes, it was just an occasional word or two.\nThis ambiguity is likely due to the passage of time, as well as the fact that people knew that Seon Master Daehaeng had approved of all the contents.\n- Reasons for undertaking translations of daily ceremonies\nOn the surface, the need for people to actually understand what they are chanting is obvious. This is keenly felt among the Buddhist monks and nuns of present-day Korea, for they can see that chants in what are essentially a foreign language do little to inspire or motivate people, particularly young people.\nIf the chants were magical formulas, the invocation of which would summon outside powers to do one’s bidding, then understanding them would be irrelevant. However, they are not such things. Instead they are deep teachings, emphasizing the interconnected nature of all existence, and offering us guidance for spiritual practice and our daily lives.\nBy chanting them, we memorize and input those contents deep into our subconscious. Many, many people have reported that relevant verses from the chants have popped into their awareness when they have been confronted by suffering or difficult situations, in so doing, showing them a way forward.\nSome people may appreciate the traditional chants for the sense of familiarity and nostalgia they bring, returning them to times when they heard the same ceremonies as they sat next to their parents or grandparents. However, this sense of tradition can provide little sustenance to younger people.\nThe extent of this problem can be seen in many, if not most, temples in Korea during the daily services. Even on weekends, the vast majority of attendees are older, gray-haired women. Young people and middle-aged men are almost never seen at many temples.\nThe temples Seon Master Daehaeng founded have, perhaps not coincidentally, one of the most vibrant youth and young adult groups of any temples in Korea. Nearly every year, they win the Jogye Order’s top prizes for their activities, as well as the creative activities and art they make for the Buddha’s Birthday. Obviously this can’t be attributed solely to translations of the ceremonies, but those can’t hurt, and indicate a willingness to reach out to people and involve them at more than a rote level of participation.\nYet a vital temple wasn’t foremost in Seon Master Daehaeng’s motivations for translating the daily ceremonies. When asked in the late 1980s why she had gone through the effort of translating them, her reply was rather chilling:\nIf the planet continues as it has been, it won’t last for much longer. Thus it’s urgent that the deep meaning of these ceremonies be known to the whole world. This is why I wrote out the meanings of these sutras. If people across the world can encounter the truth contained within these, it will lead to an increase in their spiritual depth and lead them to become people who can save the world.\nA more mundane point she made was that, simply put, languages change over time. An expression that works well with one generation may not have anything like it’s original nuance in a hundred years. To say nothing of a thousand.\nOne example is a term used for an essential element of hwadu, or koan, practice: Great Doubt(大疑). One of three essential qualities needed for awakening(the others being Great Faith and Great Determination), the practitioner is supposed to have “great doubt” about their hwadu. And it is this translation, as Great Doubt, that is used in the West.\nThe problem with this term is that in this age “doubt” implies a strong sense of skepticism or even cynicism. But a careful examination of the original context reveals that the intent is closer to “questioning” or “a desire to understand.” Yet if one were to approach practice from a perspective of heavy skepticism, it’s hard to imagine how faith could arise or how one could persist in their practice. In this age, it seems that Great Questioning is perhaps a better expression for conveying the original intent.\nWhen asked about her translation of the Heart Sutra, which varies in places from traditional versions, Seon Master Daehaeng said:\nA few days ago, a visiting Buddhist monk said to me, “The Heart Sutra, just as it is, has an incredibly profound power to it. Why did you translate it into modern Korean?” What he meant was that the Buddha’s teachings, as written down so long ago, should be chanted without any modifications.\nHowever, I gave him the example of what we used to call an “A-frame porter.” These were the guys who used an A-frame pack to carry and deliver stuff. When someone needed help moving something they would send for a porter, but nowadays people use a truck or a moving service.\nIf you were to go around calling for a “porter,” nobody would respond, would they? Because they’re not “porters,” they’re “movers” or “moving companies.”\nEven a few decades can be long enough for words to change meaning. The earliest known copies of the Heart Sutra date from the 7th century C.E., so how much more could its intended nuances have changed? How then can we correct for this?\nAs Seon Master Daehaeng explains, the only true way to correct these changes is to awaken and understand for ourselves what these sutras ceremonies are pointing towards.\n- Seon Master Daehaeng’s view of the sutras and the skills necessary to translate them\nIn general, Seon Master Daehaeng felt that awakening, or at least a deep practice was essential to being able to translate the sutras. One had to be able to see the whole, to look beyond just the visible words, and perceive the unseen meaning they were expressing. It was only in this way that one could truly express the same meaning in a new language.\nIt’s Seon practitioners who should translate the sutras. If scholar-type practitioners translate them, the meanings will be quite different. Perhaps more than half the meaning will be different. Further, to scholars, texts often seem as if they were written such that they don’t flow smoothly. [So they are tempted to cause the text to flow in ways consistent with their understanding.] But the truth is naturally in good order. Whether or not the translator has combined the spiritual realms and the material realms into one will make all the difference.\nIf one had attained this level, then, understanding the intent, it was possible to change the text around in order to convey the meaning to people living in an utterly different culture, a thousand years or more after the texts were first composed.\nIt is very strange to me, for when I look at the Flower Ornament Sutra, it contains such deep meanings, but it’s expressed in a way that if other people look at it, the sentences often seem like nothing special or just words randomly scattered around. However, inside the words, there are such immeasurably vast and hidden meanings. In order to accurately convey these in words, I often have to add some extra words.\nSeon Master Daehaeng also made a strong point that intent and meaning of both the sutras and the daily ceremonies was to teach us about that which is inherent and complete within us. Yet, if one’s perspective is still drawn towards outer things, then it becomes easy to miss the deep nuances of these texts and chants.\nThus, the Buddha’s teachings shouldn’t be changed into texts that pray to outer powers. The chant Song of Dharma Nature has such wonderful meaning. It truly does. So too does the Morning Bell Chant, but to someone who [doesn’t realize we are all connected] it can look like prayers to Buddhas and Bodhisattvas. If that’s all someone sees, it’s better to leave it untranslated and use the original Sino-Korean version.\nThis statement would undoubtedly be seen by some as quite radical, but it is in line with the basic Seon idea of uncovering what is inherently within us. Interestingly, it is also brings to mind the exhortation made by the Buddha several times in his teachings, as well as in the final days of his life, “Be a lamp unto yourself, be your own refuge, with no one else as your refuge, with the Dharma as your lamp and refuge.”\nAs will be seen, this focus on what is already within us, versus something coming from outside of us, or relying upon someone outside us, characterizes Seon Master Daehaeng’s translations of the daily ceremonies. It also gave rise to a somewhat different interpretation of the hanmun that makes up the Heart Sutra.\n Shinhaeng Yojeon, (1987), (Essentials of Faith and Practice,) Hanmaum Seon Center, Anyang, South Korea.\n No, Manho, June 19, 2015, personal communication\n Daehaeng, Seon Master, (2001). Jinsilhan mideumeseoui hansaenggag(The Sincere Belief in ‘One Thought’). Walking on Emptiness: First Sunday Dharma Talks, Vol. 1, 94.\n Dharma talk to assembled sunims, October 17, 1994.\n Dharma talk to assembled sunims, October 17, 1994.\n Daehaeng, Seon Master, (1999). Mujongui tteus(The Meaning of ‘Without Characteristics’). Walking on Emptiness: Regular Dharma Talks, Vol. 2, 254.\n Digha Nikaya, 16, 2.26']	['<urn:uuid:1b45b022-a16d-4b20-ad94-5cf445d64bfa>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T18:05:20.075489	11	40	2254
46	studying chemical bonds want to know what type attraction exists between ch4 gas particles	The intermolecular forces that exist between molecules of CH4 are called dispersion forces.	['Methanol has a higher boiling point than methane because it has stronger intermolecular forces (IMFs), which are attractions between individual molecules. This makes its molecules harder to separate so it takes more energy, thus the higher boiling point.\nWhy does methanol have weak intermolecular forces?\nExplanation: Methanol is not an ionic molecule and will not exhibit intermolecular ionic bonding. Methanol is polar, and will exhibit dipole interactions. It also contains the -OH alcohol group which will allow for hydrogen bonding.\nDoes methane have high intermolecular forces?\nJust London (dispersion forces). Because methane is a non-polar molecule it is not capable of hydrogen bonding or dipole-dipole intermolecular forces. … The only intermolecular forces in methane are London dispersion forces. The major intermolecular forces would be dipole-dipole forces and London dispersion forces.\nWhat is the difference between methanol and methane?\nis that methane is (organic compound|uncountable) the simplest aliphatic hydrocarbon, ch4, being a constituent of natural gas while methanol is (organic compound) the simplest aliphatic alcohol, ch3oh; a colourless, toxic, inflammable liquid, used as a solvent, antifreeze, in the chemical industry, and in the …\nDoes methane have weak intermolecular forces?\nHydrogen, ammonia, methane and pure water are also simple molecules. All have strong covalent bonds between their atoms, but much weaker intermolecular forces between molecules. When one of these substances melts or boils, it is these weak intermolecular forces that break, not the strong covalent bonds.\nWhat is the strongest of the 4 types of intermolecular forces?\nExplanation: Ionic bonds are the strongest type of bonds, followed by covalent bonds, hydrogen bonds, and lastly, van Der waals forces.\nWhat is the strongest intermolecular forces for cacl2?\nHydrogen bonds are seen to be the strongest of the intermolecular forces, although some ion-dipole interactions can be comparable.\nHow does methane turn into methanol?\nConversion of methane to a liquid fuel is a more desirable alternative to compressed natural gas due to ease of storage. The two-step process produces liquid methanol by the steam reforming of methane to synthesis gas followed by the high pressure catalytic conversion of the synthesis gas to methanol.\nWhy is methanol more polar than methane?\nMethanol looks like methane, with a central carbon, but has one H replaced with an OH creating an unbalanced dipole. Methane is thus a polar molecule. When hydrogen is bonded to one of the three most electronegative elements, F, O or N, the molecule exhibits hydrogen bonding.\nHow do you synthesize methanol?\nOn an industrial scale, methanol is predominantly produced from natural gas by reforming the gas with steam and then converting and distilling the resulting synthesized gas mixture to create pure methanol. The result is a clear, liquid, organic chemical that is water soluble and readily biodegradable.\nWhat is the strongest intermolecular force present in benzene?\nTherefore, the only intermolecular force of attraction existing in benzene molecule is dispersion forces.\nWhat forces exist between molecules of methane?\nThe intermolecular forces that exist between molecules of CH4 are called dispersion forces. Examples of intermolecular forces include the London dispersion force, dipole-dipole interaction, ion-dipole interaction, and van der Waals forces.']	['<urn:uuid:7e15cd3d-1814-41d5-a057-fea154209efa>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T18:05:20.075489	14	13	515
47	I'm curious about the scientific publishing process - what's the point of peer review, and how good is it at catching mistakes in research?	The peer review process is meant to ensure only good science is published and is carried out by all reputable scientific journals. However, it has significant limitations. While peer review evaluates if research is original, methodologically sound, and follows ethical guidelines, it is not designed to detect fraud, expose experts who are fudging data, or reveal authors' hidden biases. This means that about two-thirds of findings published in leading medical journals are refuted within a few years.	['After my first non-fiction book was published, something unusual happened to me. Overnight, I suddenly became an “expert”. During book tours, I did newspaper, magazine, television and radio interviews. I was invited to speak at conferences and writers’ festivals and schools during National Book Week. Each presentation led to more invitations. I became the go-to writer for foreign Sunday paper editors who needed a West Coast perspective on my subject. I was even offered my own regular weekly broadcast gig on a popular TV newsmagazine show.\nWhy? Because I’d written a bestselling book. By the time my second book came out a couple years later, I was already a pretty well-known local “expert” on my subject matter. Even my publisher treated me with newfound respect (and a hefty advance).\nOf course, I wasn’t really an expert at all. I had, as David H. Freedman describes it, “simply done a good job of gathering up a lot of information”. I never pretended to be an expert, but writing books and doing live presentations or media interviews can apparently make you one. That’s how physicians paid by pharmaceutical companies to help shill their drugs get to be called “thought leaders”.\nFreedman himself did a radio interview last month with Phil Dobbie of the Australian edition of BNet News about his own book, called “Wrong: Why Experts Keep Failing Us – and How to Know When Not to Trust Them.”\nHis book reveals the following disturbing realities about those we really do consider to be true experts. For example:\n- About two-thirds of the findings published in leading medical journals are refuted within a few years.\n- As much as 90% of physicians’ medical knowledge has been found to be substantially or completely wrong.\n- There is a one-in-12 chance that a doctor’s diagnosis will be so wrong that it causes the patient significant harm.\n- Economists have found that all studies published in economics journals are likely to be wrong.\n- Professionally prepared tax returns are more likely to contain errors than self-prepared returns.\n- Half of all newspaper articles contain at least one factual error.\nSo why, then, Freedman wonders, do we blindly follow experts?\nHe suggests it’s because we live in a very complex world. We like simple answers. We don’t want to hear an expert go on and on about how things might be true for some people.\nWe want to hear “This drug is better than that drug!” or “Eat this, not that!” or “Invest your money here, not there!” These statements, he claims, typify the kinds of experts who get published. And the media likes to further sensationalize what the experts say.\nEditors of prestigious medical journal, for example, look for article submissions that look as if these experts are advancing science and giving us important answers.\nBut doesn’t the peer review process help to protect us from the potential errors of medical research experts? Not necessarily, says Freedman.\n“Peer review is not designed to turn up fraud, or to expose experts who are fudging data, or to reveal author’s hidden biases. Peer review does not protect us.”\nFreedman adds that unconscious personal bias is also an important problem in science.\n“We have biases about what we hope is true. If we’re researching a pill to cure a certain medical condition, we have an unconscious bias towards reaching that simple probable answer that might solve our problems.”\nThis is also what’s known as confirmation bias. Confirmation bias refers to a type of selective thinking in which we tend to notice and to look for what confirms our beliefs. Not only that, but we then want to ignore, or undervalue, or not even look for the relevance of anything that contradicts those beliefs.\nAs a heart attack survivor who was misdiagnosed with acid reflux in the E.R. and sent home (despite presenting with textbook heart attack symptoms like chest pain, nausea and pain radiating down my left arm), I’m no stranger to the damage caused by experts who are wrong.\nWomen under the age of 55 are in fact seven times more likely than men are to be misdiagnosed in mid-heart attack and sent home. About 5% of autopsies find clinically significant conditions that were missed and could have affected the patient’s survival. And over 40% of medical malpractice suits are for “failure to diagnose.”\nA 2003 article* in the journal Academic Medicine describes the mistakes that medical experts make as “thinking errors”. They include:\n- Anchoring bias – locking on to a diagnosis too early and failing to adjust to new information.\n- Availability bias – thinking that a similar recent presentation is happening in the present situation.\n- Confirmation bias – looking for evidence to support a pre-conceived opinion, rather than looking for information to prove oneself wrong.\n- Diagnosis momentum – accepting a previous diagnosis without sufficient skepticism.\n- Overconfidence bias – over-reliance on one’s own ability, intuition, and judgment.\n- Premature closure – similar to confirmation bias but more “jumping to a conclusion”\n- Search-satisfying bias – The “eureka” moment that stops all further thought.\nFreedman adds that when errors are published in scientific journals by experts, it’s often the original erroneous finding that gets quoted and built upon in later papers. If and when a serious error/retraction is exposed, it’s often ignored.\nScientists are still our best bets, flaws notwithstanding, but sometimes the unskilled gurus out there get it right and the scientists get it wrong, warns Freedman.\n“I wish I had a simple 5-step recipe, but I don’t. Let’s take information from as many sources as possible. Educate yourself. Get a good cross-section of opinions from credible sources. Use common sense. Be as informed as possible.”\nIn the book, The Invisible Gorilla: And Other Ways Our Intuitions Deceive Us, Christopher Chabris and Daniel Simons explain the “illusion of knowledge” in a scientist’s brain. Examples include the world’s leading geneticists predicting that humans have between 25,000 to 153,000 genes – when the right answer is actually closer to 20,500. And Leon Rozenblit and Frank Keil did dozens of experiments which all showed that experts are “blissfully unaware of the shortcomings in our own knowledge.”\nTo fend off critics who may accuse author David Freedman of being one of these “experts” himself, he told the Australian edition of BNet News:\n“I’ve included an entire chapter in my book about all the ways I’ve been wrong. Don’t take my word for anything. Even though I’ve gathered up a lot of information and done the best I can do, I don’t ever claim to be an expert.”\n- Nagging 101 – the basics you’ll need to understand research terms\n- The 18-Second Rule: How Doctors Missed Your Heart Attack Diagnosis\n- Seven Ways To Misdiagnose a Heart Attack\n- Cardiac Gender Bias: We Need Less TALK and More WALK\n- Is Your Doctor a “Thought Leader”?\n- Putting A Positive Spin On Medical Research Results\n- Warning: Clinical Trials Funded By Drug Companies May Appear More Truthful Than They Actually Are\n- When Medical Research is Funded to Favour The Drug, Not the Facts\n- Dr. Harriet Hall Explains Tooth Fairy Science\n- Clinical Practice Guidelines vs Routine Screening Mammograms\n* Acad Med. 2003 Aug;78(8):775-80.\nCouldn’t agree more. We believe what we get from these “experts” with their clever news headlines or slick ‘miracle product’ ads. As I read in one of your posts here a while back, “For every PHD there is an equal and opposite PHD!” Who to trust? Thx for reminding us to be cautiously skeptical of just about everybody and not to be so gullible or intimidated by “experts”.\nClear thinking, sounds like an interesting book. Thanks for this.\nBuffer, I just finished reading this book while on vacation, after learning about it here. GO READ IT! It’s a page-turner, well researched, and worth the read. And my family has already had many lively dinner conversations about it.\nI have to say that I treat my physicians as “experts”, even though I realize it’s impossible for one human being to have comprehensive expertise in so many possible facts. I cannot even imagine asking for a second opinion, for example. But at my age, I’ve been raised with this “DOCTOR AS GOD” premise. So this is an interesting and disturbing article for me to ponder. Thank you for this provocative topic!\nPingback: Dr. Joe Today', 'The practice of peer review in Natural and Engineering Sciences is to ensure that only good science is published. It is an objective process at the heart of good scholarly publishing and is carried out by all reputable scientific journals. Therefore, our referees play a vital role in maintaining the high standards Transport Policy and all manuscripts are peer reviewed following the procedure outlined below.\nInitial manuscript evaluation The Editor first evaluates all manuscripts. It is rare, but it is possible for an exceptional manuscript to be accepted at this stage. Manuscripts rejected at this stage are insufficiently original, have serious scientific flaws, have poor grammar or English language, or are outside the aims and scope of the journal. Those that meet the minimum criteria are passed on to at least 2 experts for review.\nType of Peer Review Transport Policy employs double blind reviewing, where both the referee and author remain anonymous throughout the process.\nHow the referee is selected Whenever possible, referees are matched to the paper according to their expertise and our database is constantly being updated.\nReferee reports Referees are asked to evaluate whether the manuscript: - Is original - Is methodologically sound - Follows appropriate ethical guidelines - Has results which are clearly presented and support the conclusions - Correctly references previous relevant work.\nLanguage correction is not part of the peer review process, but referees may, if so wish, suggest corrections to the manuscript.\nHow long does the review process take? The time required for the review process is dependent on the response of the referees. Should the referee’s reports contradict one another or a report is unnecessarily delayed, a further expert opinion will be sought. In rare cases for which it is extremely difficult to find a second referee to review the manuscript, or when the one referee’s report has thoroughly convinced the Editor, decisions at this stage to accept, reject or ask the author for a revision are made on the basis of only one referee’s report. The Editor’s decision will be sent to the author with recommendations made by the referees, which usually includes verbatim comments by the referees. Revised manuscripts might be returned to the initial referees who may then request another revision of a manuscript.\nFinal report A final decision to accept or reject the manuscript will be sent to the author along with any recommendations made by the referees, and may include verbatim comments by the referees.\nEditor’s Decision is final Referees advise the editor, who is responsible for the final decision to accept or reject the article.Becoming a referee for Transport Policy If you are not currently a referee for Transport Policy but would like to be considered as a referee, please contact the editor. The benefits of refereeing for Transport Policy include the opportunity to read see and evaluate the latest work in your research area at an early stage, and to contribute to the overall integrity of scientific research and its published documentation. You may also be able to cite your work for Transport Policy as part of your professional development requirements for various Professional Societies and Organisations.']	['<urn:uuid:4ac1316f-2c23-4ee8-b93c-e5edc87b179e>', '<urn:uuid:fe68a75a-b06a-4eb9-a46f-a4d20b766139>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T18:05:20.075489	24	77	1925
48	What happens in the first step of image processing?	The first step of image processing involves importing the image via image capturing tools.	['Till now, we have read about Image processing being a technique to carry out a particular set of actions on an image for obtaining an enhanced image or extracting some valuable information from it. The input is an image, and output may be an improved image or characteristics/features associated with the same.\nIt is essential to know that computer algorithms have the most significant role in digital image processing. Developers have been using and implementing multiple algorithms to solve various tasks, which include digital image detection, image analysis, image reconstruction, image restoration, image enhancement, image data compression, spectral image estimation, and image estimation. Sometimes, the algorithms can be straight off the book or a more customized amalgamated version of several algorithm functions.\nImage processing algorithms commonly used for complete image capture can be categorized into:\nLow-level techniques, such as color enhancement and noise removal,\nMedium-level techniques, such as compression and binarization,\nand higher-level techniques involving segmentation, detection, and recognition algorithms extract semantic information from the captured data.\nTypes of Image Processing Algorithms\nSome of the conventional image processing algorithms are as follows:\nContrast Enhancement algorithm: Colour enhancement algorithm is further subdivided into -\n- Histogram equalization algorithm: Using the histogram to improve image contrast\n- Adaptive histogram equalization algorithm: It is the histogram equalization which adapts to local changes in contrast\n- Connected-component labeling algorithm: It is about finding and labeling disjoint regions\nDithering and half-toning algorithm: Dithering and half-toning includes of the following -\n- Error diffusion algorithm\n- Floyd–Steinberg dithering algorithm\n- Ordered dithering algorithm\n- Riemersma dithering algorithm\nElser difference-map algorithm: It is a search algorithm used for general constraint satisfaction problems. It was used initially for X-Ray diffraction microscopy.\nFeature detection algorithm: Feature detection consists of -\n- Marr–Hildreth algorithm: It is an early edge detection algorithm\n- Canny edge detector algorithm: Canny edge detector is used for detecting a wide range of edges in images.\n- Generalized Hough transform algorithm\n- Hough transform algorithm\n- SIFT (Scale-invariant feature transform) algorithm: SIFT is an algorithm to identify and define local features in images.\n- SURF (Speeded Up Robust Features) algorithm: SURF is a robust local feature detector.\nRichardson–Lucy deconvolution algorithm: This is an image deblurring algorithm.\nBlind deconvolution algorithm: Much like Richardson–Lucy deconvolution, it is an image de-blurring algorithm when point spread function is unknown.\nSeam carving algorithm: Seam carving algorithm is a content-aware image resizing algorithm\nSegmentation algorithm: This particular algorithm parts a digital image into two or more regions.\n- GrowCut algorithm: an interactive segmentation algorithm\n- Random walker algorithm\n- Region growing algorithm\n- Watershed transformation algorithm: A class of algorithms based on the watershed analogy\nIt is to note down that apart from the algorithms mentioned above, industries also create customized algorithms to address their needs. They can be either right from the scratch or a combination of various algorithmic functions. It is safe to say that with the evolution of computer technology, image processing algorithms have provided sufficient opportunities for multiple researchers and developers for investigation, classification, characterization, and analysis of various hordes of images.\nPreviously we have learned what visual inspection is and how it helps in inspection checks and quality assurance of manufactured products. The task of vision-based inspection implements a specific technological aspect with the name of Image Processing.\nImage processing is a technique to carry out a particular set of actions on an image for obtaining an enhanced image or extracting some valuable information from it.\nIt is a sort of signal processing where the input is an image, and output may be an improved image or characteristics/features associated with the same. Over the years, image processing has become one of the most rapidly growing technologies within engineering and even the computer science sector too.\nImage processing consists of these three following steps:\n- Importing the image via image capturing tools;\n- Manipulating and analyzing the image;\n- Producing a result where the output can be an altered image or report that is based on image analysis.\nImage processing includes the two types of method:\nAnalogue Image Processing: Generally, analogue image processing is used for hard copies like photographs and printouts. Image analysts use various facets of interpretation while using these visual techniques.\nDigital image processing: Digital image processing methods help in the manipulation and analysis of digital images. The three general steps that all types of data have to undergo while using digital image processing techniques are - pre-processing, enhancement, and information extraction.\nThis article discusses primarily digital image processing techniques and various phases.\nDigital Image Processing and different phases\nDigital image processing requires digital computers to convert images into digital form using digital conversion method and then process it. It is about subjecting various numerical depictions of images to a series of operations to obtain the desired result. The primary advantages of Digital Image Processing methods lie in its versatility, repeatability and the preservation of original data.\nMain techniques of digital image processing are as follows:\n- Image Editing: It means changing/altering digital images with the use of graphic software tools.\n- Image Restoration: It means processing a corrupt image and taking out a clean original image to get back the lost information.\n- Independent Component Analysis: It separates a variety of signals computationally into additive subcomponents.\n- Anisotropic Diffusion: This method enables reducing image noise without having to remove essential portions of the image.\n- Linear Filtering. It’s another digital image processing method, which is about processing time-varying input signals and generating output signals.\n- Neural Networks: Neural networks are the computational models used in machine learning for solving various tasks.\n- Pixelation: It is a method for turning printed images into digitized ones.\n- Principal Components Analysis: It is a digital image processing technique that is used for feature extraction.\n- Partial Differential Equations: This method refers to dealing with de-noising\n- Hidden Markov Models: This technique is used for image analysis in 2D (two dimensional).\n- Wavelets: Wavelets are the mathematical functions used in image compression.\n- Self-organizing Maps: a digital image processing technique that classifies images into several classes.\nImage recognition technology has grown up to be of great potential for wide adoption in various industries. This technology has seen significant usage with each passing year, as enterprises have become more time-efficient and productive due to the incorporation of better manufacturing, inspection and quality assurance tools and processes. Big corporations and start-ups such as Tesla, Google, Uber, Adobe Systems, etc heavily use image processing techniques in their day to day operations. With the advancements in the field of AI (Artificial Intelligence), this technology will see significant upgrades in the coming years.']	['<urn:uuid:47f8c689-6a49-4dfc-88ac-3247eaae3c49>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T18:05:20.075489	9	14	1110
49	What are the key distinctions between Fire Resistance Level requirements for structural adequacy and the specific provisions for heavy timber sizing in different construction types, and how do they relate to each other?	Fire Resistance Level (FRL) specifies performance in minutes for three criteria: load-carrying capacity, prevention of flame/hot gas passage, and temperature rise limits on the unexposed face. For example, a 120/120/120 rating means 120 minutes for each criterion. In contrast, heavy timber provisions are based on minimum cross-section sizes of the timber elements, as specified in IBC Section 2304.11, with specific dimensional requirements for columns, floor framing, roof framing, and walls. These provisions recognize that larger timber members naturally provide fire resistance through char formation, while still maintaining structural adequacy.	['There are two types of methods of suppression used in building fire safety design: passive and active. Passive suppression uses materials, systems, building elements, and/or building layout to prevent or resist ignition, to limit its spread to other combustible contents in the room and to contain the fire within the room or zone to prevent its spread to other sections of the structure.\nActive suppression is the employment of mechanical devices such as sprinklers or extinguishers to extinguish the fire in its early stages to prevent its spread. Passive suppression uses the natural properties of materials and products that are part of the building design to suppress the migration of the fire. It is well known in the construction industry that the single most important characteristic of gypsum drywall is its fire resistance. This is provided by the principal raw material used in its manufacture, CaSO₄∙2H₂O (gypsum). Gypsum is non-combustible, which means that it contributes no fuel to a fire. As the chemical formula shows, gypsum contains 21% by weight chemically combined water, also called crystalline water that is part of the gypsum crystal itself.\nWhen gypsum drywall panels are exposed to fire, the heat of the fire converts the crystalline water to steam. The heat energy that converts water to steam is thus absorbed, keeping the opposite side of the gypsum panel cool as long as there is water left in the gypsum, or until the gypsum panel is breached.\nFIRE RESISTANCE LEVEL (FRL)\nFire rating requirements of the Building Code of Australia are specified in terms of Fire Resistance Level (FRL). The FRL specifies the performance, in minutes, for each of the following three design criteria when specimens are fire tested to the requirements of the Australian Standard AS 1530 Methods for Fire Tests on Building materials, Components and Structures — Part 4: Fire-Resistance Tests of Elements of Building Construction:\nThe specimen can no longer carry its load (self-weight and superimposed loads).\nCracks or openings develop that allow the passage of flames or hot gasses.\nThe unexposed face temperature rises by more than 140°C on average or 180°C for a single point. For example, a wall system under fire test that carries its load for 120 minutes and maintains its integrity and insulation for 120 minutes is given a FRL of 120/120/120, ie 120 minutes structural adequacy, 120 minutes integrity and 120 minutes insulation. Systems that achieve a particular FRL can be used to satisfy the requirements for a lesser FRL.\nAny structure required to support a fire rated system must have a fire resistance structural adequacy level of at least that of the system. This includes vertical support to ceilings and walls and lateral support to the top of walls which may be provided from both sides. Refer to BCA for specific requirements.\nThe BCA requires that building elements, other than roof sarking or certain roof battens, must not pass through or cross a fire rated wall unless the Fire Resistance Level of that wall is maintained. Where trusses and beams pass over or through a fire rated partition, the following measures can be taken to ensure that the Fire Resistance Level of the partition is not degraded due to a failure of these members in the case of fire:\n- Construct a fire rated ceiling that protects the structural members\n- Fire protect the structural member, or\n- Ensure the partition can carry loading from the fire affected structural member and that the member can still carry its loading when it is supported on a partition (for trusses, this can mean the inclusion of additional webbing above the partition). Ensuring the partition can carry these new loadings may require:\n- Turning it into a load bearing partition\n- Constructing the partition with a protected column within it, or\n- Constructing unprotected columns on both sides of the partition.\nPORTAL FRAME BEHAVIOUR\nIn portal frames affected by the fire, the rafters often push outwards on the column members until the ridge sinks and then pulls the columns inwards. Should drywall be used to provide a fire separation within portal framed buildings, the above mode of failure needs to be recognised by the designer.\nAs mentioned above, load bearing elements may need to be incorporated within, or adjacent to, the partition to maintain support to the roof structure during a fire event.\nDIRECTION OF ATTACK BY FIRE\nIn most cases, the direction of attack by fire is assumed to be from both sides of the partition. In some cases, for example in exterior walls adjacent to a fire source feature (as defined in the BCA), the rating may be required from one side only.\nFor conventional fire rated plasterboard ceiling systems, direction of attack by fire is always from below, while for spanning ceilings it can also be from both sides or from above. Applicable fire attack direction is indicated for each fire rated system listed in this manual.\nMaximum heights for fire rated steel stud partitions are the lesser of maximum fire heights and structural heights for a given wall configuration and stated lateral pressure.\nMaximum fire heights for USG Boral fire rated steel stud walls are derived from full scale tests carried out by CSIRO, BHP, BRANZ and from fire engineering principles. Maximum structural heights have been obtained by computation and from extensive mechanical testing. These heights meet the requirements of the National Construction Code and have been certified by Wynton Stone Australia Pty Ltd and Taylor Thomson Whitting of Melbourne.\nRESISTANCE TO INCIPIENT SPREAD OF FIRE (RISF)\nThe NCC stipulates instances when a ceiling system must be resistant to the incipient spread of fire. This requirement determines the ability of the ceiling to provide adequate thermal insulation to combustible materials within the ceiling plenum, thus avoiding the danger of the materials being ignited.\nMany USG Boral fire rated ceiling systems carry an RISF rating which is noted as such. RISF is a more onerous requirement than FRL. Systems that achieve a particular RISF may be used to satisfy the requirements of a lesser RISF.\nFIRE HAZARD PROPERTIES\nWall and ceiling lining materials in certain types of buildings must comply with the Fire Hazard Properties requirements of the BCA.\nAll USG Boral gypsum board lining products are classified as Group 1 (least hazardous) materials and have a smoke growth rate index less than 100 and average specific extinction area less than 250m2/kg when tested in accordance with the BCA.\nIn accordance with the BCA, gypsum boards can be used wherever a non-combustible material is required.\nGAS RETICULATION IN FIRE RATED WALLS\nOxygen or combustible fluid reticulation systems should not be located within fire rated walls unless designed, fire tested and constructed to suit this application.\nUSG Boral offers a wide range of fire rated building systems including;\n- lightweight wall systems up to FRL –/240/240 and 180/180/180\n- lightweight ceiling systems up to FRL 120/120/120\n- acoustic systems with Rw+Ctr=50 or higher\n- masonry upgrades and\n- beam/column fire protection systems up to FRL 180/–/–', 'Code Applications, Construction Types and Fire Ratings\nBy Richard McLain, PE, SE and Scott Breneman, PhD, PE, SE\nFor many years, exposed heavy timber framing elements have been permitted in U.S. buildings due to their inherent fire-resistance properties. The predictability of wood’s char rate has been well-established for decades and has long been recognized in building codes and standards.\nToday, one of the exciting trends in building design is the growing use of mass timber—i.e., large solid wood panel products such as cross-laminated timber (CLT) and nail-laminated timber (NLT)—for floor, wall and roof construction. Like heavy timber, mass timber products have inherent fire resistance that allows them to be left exposed and still achieve a fire-resistance rating. Because of their strength and dimensional stability, these products also offer an alternative to steel, concrete, and masonry for many applications, but have a much lighter carbon footprint. It is this combination of exposed structure and strength that developers and designers across the country are leveraging to create innovative designs with a warm yet modern aesthetic, often for projects that go beyond traditional norms.\nThis excerpt is from a paper written to support architects and engineers exploring the use of mass timber for commercial and multi-family construction. It focuses on how to meet fire-resistance requirements in the International Building Code (IBC), including calculation and testing-based methods. Unless otherwise noted, refers to the 2018 IBC.\nMass Timber & Construction Type\nBefore demonstrating fire-resistance ratings of exposed mass timber elements, it’s important to understand under what circumstances the code currently allows the use of mass timber in commercial and multi-family construction.\nA building’s assigned construction type is the main indicator of where and when all wood systems can be used. IBC Section 602 defines five main options (Type I through V) with all but Type IV having subcategories A and B. Types III and V permit the use of wood framing throughout much of the structure and both are used extensively for modern mass timber buildings.\nType III (IBC 602.3) – Timber elements can be used in floors, roofs and interior walls. Fire-retardant-treated wood (FRTW) framing is permitted in exterior walls with a fire-resistance rating of 2 hours or less.\nType V (IBC 602.5) – Timber elements can be used throughout the structure, including floors, roofs and both interior and exterior walls.\nType IV (IBC 602.4) – Commonly referred to as ‘Heavy Timber’ construction, this option has been in the building code for over a hundred years in one form or another, but its use has increased along with renewed interest in exposed wood buildings. This construction type is unique in that fire-resistive behavior is based in part on the inherent and long-demonstrated fire resistance of large solid wood framing. Structural wood components are permitted in floors, roofs and interior walls when they meet minimum cross-section sizes. Per IBC Sections 602.4.1 and 602.4.2, exterior walls required to have a fire-resistance rating of 2 hours or less are also permitted to use FRTW framing, or CLT when covered with FRTW sheathing or noncombustible materials. In general, heavy timber components used in Type IV construction (called Type IV-HT in the 2021 IBC) can be exposed.\nHeavy Timber Requirements\nThe fire-resistance of heavy timber construction is based on the inherent size of large timber elements. In a fire, large members tend to retain their load-carrying ability longer than members with small cross-sections; this is because a char layer forms on the surface while the interior remains undamaged and structurally sound. IBC Section 2304.11 (IBC 2015 Section 602.4) provides minimum heavy timber sizes for columns, floor framing and decking, roof framing and decking, and walls.\nThese minimum nominal sizes are applicable to solid sawn wood members. However, in consideration of engineered wood products, IBC Table 2304.11 (IBC 2015 Table 602.4) provides equivalent glulam and SCL sizes that qualify as heavy timber.Fire-Resistance Rating Requirements\nFor most building elements other than heavy timber, passive fire-resistive requirements are in the form of a required fire-resistance rating (FRR). The IBC defines FRR as the period of time a building element, component or assembly maintains the ability to confine a fire, continues to perform a given structural function, or both, as determined by the tests, or the methods based on tests, prescribed in Section 703.\nFRR based on Construction Type\nThe construction type of a building determines many of the minimum required fire-resistance ratings for different building components, as shown in IBC Table 601.\nA couple features of this table are relevant to mass timber.Footnote ‘c’ allows for timber components meeting the requirements of heavy timber to be used in the construction of all roofs having a fire-resistance rating of 1 hour or less in lieu of the required fire-resistance rating. This means that a mass timber roof meeting the minimum size requirement of heavy timber can be used in construction Types I-B, II-A and II-B which otherwise prohibit the use of combustible framing.\nInterior nonbearing walls and partitions generally do not have a fire-resistance rating requirement, except for Type IV construction. In IBC 2015, note that the Table 601 reference to Section 602.4.6 should instead be to 602.4.8, which requires partitions in Type IV construction to be of solid wood construction or have a 1-hour fire-resistance rating.\nOther FRR Requirements\nIn addition to requirements related to construction type, there are other requirements for fire-resistance ratings in the IBC. For multi-unit residential buildings, walls and floors between dwelling or sleeping units are required to have a fire-resistance rating of 1/2 hour in Type II-B, III-B and V-B construction when sprinklered throughout with an NFPA 13 system, and 1 hour for all other construction types (IBC 420, 708 and 711). Multiple separated occupancies (IBC 508.4), incidental uses (IBC 509), and special provisions (IBC 510) also require fire-resistance ratings of select components and assemblies.\nSelecting a Construction Type\nSelection of construction type for mass timber projects is one of the more significant design considerations. While it’s common to choose construction type based on structural material—i.e., to assume that steel and concrete structures should be Type II, light-frame wood should be Type V, and exposed heavy timber should be Type IV—this approach can lead to additional costs. While Type IV construction can be used for exposed mass timber projects, a full understanding of the allowable use of materials in all five construction types, as well as the unique allowances and limitations associated with each, will help to inform the most efficient design.To optimize a building design from a construction type perspective, it is best to start with Type V-B construction, and work up toward higher construction types. This avoids unnecessary defaults or assumptions—and unnecessary costs. The fact that certain materials are being used doesn’t mean there is only one option for construction type. Similarly, a mix of occupancy groups doesn’t dictate that certain materials, construction types or building configurations are required. For example, a mass timber building may have isolated steel, concrete or masonry structural elements, but this doesn’t mean that Type I or II construction is necessary, nor does it mean that some or all of the building can’t be framed with mass timber. Likewise, a building with mass timber elements has options other than Type IV construction. Note IBC Section 602.1.1:\n602.1.1 Minimum requirements. A building or portion thereof shall not be required to conform to the details of a type of construction higher than that type which meets the minimum requirements based on occupancy even though certain features of such a building actually conform to a higher type of construction.\nThis section permits the use of elements commonly used in a higher construction type without requiring that the entire building meet all of the provisions of that construction type. For example, if a building’s size permitted the use of Type V-B construction, it could still be completely framed with noncombustible materials while being classified as V-B. Similarly, a Type III or V building could be framed with a combination of combustible and noncombustible materials.\nAs noted, from a cost efficiency perspective, it is usually best to start a building analysis with Type V-B construction as this provides the most flexibility in terms of allowable use of materials throughout the building while minimizing requirements for fire resistance-rated assemblies and structural elements. However, Type V-B is also the most restrictive in terms of allowable building size. All three of these factors—allowable building size, allowable use of structural materials, and required fire-resistance levels—are interconnected.\nConstruction type, which is a direct function of the types of materials used in a building (and vice versa) looks at limiting the size of the building based on fire and life safety risks. In other words, how much of a building’s contents and structure are combustible has an impact on presumed risk, which in turn affects the code’s allowances for building size. Fire-resistant measures required of the building’s structure dictate the length of passive fire endurance (i.e., 1 hour, 2 hours).\nIf Type V-B construction doesn’t allow as large a building as desired, the next step is to check Type V-A. The main differences between V-B and V-A are fire-resistance rating requirements and allowable building size. If Type V-A doesn’t allow the desired size, Type III-B is the next choice, with Type III-A following. Type IV construction has similar allowable building size limits as Type III-A; however, there are nuances to the selection of one or the other.\nRead the full-length paper for a discussion of these nuances as well as methods of demonstrating fire-resistance ratings of mass timber; fire performance of glued-laminated timber (glulam), nail-laminated timber (NLT), tongue and groove decking, structural composite lumber, and cross-laminated timber (CLT); fire protection of connections; and interior finish requirements. It is available as a free download on the WoodWorks website, along with an Inventory of Fire Resistance-Tested Mass Timber Assemblies and Penetrations.\nWoodWorks – Wood Products Council provides free technical support as well as education and resources related to the code-compliant design of commercial and multi-family wood buildings. A non-profit organization staffed with architects, structural engineers and construction experts, WoodWorks has the expertise to assist with all aspects of wood building design, including (but not limited to): allowable heights and areas/construction types, structural detailing of wood and hybrid systems, fire resistance and acoustical-rated assemblies, efficient and code-compliant lateral system design, alternate means of code compliance, and energy-efficient detailing. For assistance with a project, visit www.woodworks.org/project-assistance or email firstname.lastname@example.org.']	['<urn:uuid:72c74c5e-15c7-4ffa-bad4-7597db2ca9a7>', '<urn:uuid:ac88609a-e7df-425b-bb4b-f52ad6b8ce83>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T18:05:20.075489	33	89	2903
50	What are the economic implications of implementing proper utility locating techniques versus dealing with accidental utility damages, particularly in terms of repair costs and project expenses?	Implementing proper utility locating techniques through Subsurface Utility Engineering (SUE) proves far more economical than dealing with accidental utility damages. According to the Purdue Study commissioned by FHWA, SUE investments showed a return of $4.62 for every $1.00 spent, with the largest return being $208/$1. The cost of not implementing proper techniques is substantial - for example, in Sweden alone, approximately 8,000 telecom cable cuts annually cost around $30 million in repairs. SUE typically costs less than 0.5% of construction costs while providing 1.9% savings on construction costs. When implemented properly, SUE could result in minimum savings of $1 billion per year.	"['A branch of ENGINEERING PRACTICE that involves managing certain risks associated with: Utility Mapping at appropriate ASCE Quality Levels Utility Coordination Utility relocation and adjustment through conflict matrix resolution Utility relocation design and cost estimates Communication of utility data to concerned parties Implementation of Utility Accommodation Policies and utility design\nCI/ASCE Standard 38-02 The American Society of Civil Engineers (ASCE) has developed a National Consensus Standard, CI/ASCE 38-02, titled “Standard Guideline for the Collection and Depiction of Existing Subsurface Utility Data”. This National Consensus Standard (NCS) is used by courts and lawyers, along with contractual instruments, to assist in both defining a professional’s standard of care and level of responsibility.\nASCE Standard – Quality Levels QL-B QL-A QL-C QL-D\nASCE Quality Level D (QL-D) Existing Utility Records Involves the use of existing utility records, permits, plans to depict the ‘approximate’ horizontal position of underground utilities.\nASCE Quality Level C (QL-C) Survey of Visible Features Involves surveying visible above ground utility facilities to assist with determining ‘approximate’ horizontal position of underground utilities. Used with QL-D. Manholes Power poles Hydrants\nASCE Quality Level B (QL-B) Utility Designating Involves the use of geophysical prospecting equipment to determine the existence and horizontal position of underground utilities. Paint markings Flags\nDesignating Approach: QA/QC All data is checked and cross- checked for accuracy – Records research vs. designating file Field sketch Designating data point & numbering system Utility # of points Comments/ Notes/ Location W120East side of road W24West side of road W312NB lane\nASCE Quality Level A (QL-A) Utility Locating Involves the use of non-destructive digging equipment at critical points to determine the precise horizontal and vertical position of underground utilities, as well as the type, size, material, and other characteristics.\nLocating Approach Permitting Minimal disturbance Saw cut in pavement Air vacuum excavation Centerline of facility Permanent reference points Proper restoration Compaction in 6” lifts Perma-Patch\nLocating Approach: QA/QC Test Hole Data Collection – 2 photographs of every test hole for permanent record Final QA/QC of all Subsurface Utility Engineering work is done by registered Professional Engineer\nWhen to Use SUE? Type of Project Piping and Facility Design (storm, sanitary, water) Site and Plant Design Roadway / Bridge const. Widening / Intersection Improvements Signal Replacement Location of Project Urban / Suburban Rural\nWhen to Use SUE? Utilities involved Major or Minor Conflicts with utilities Compensable interest Limited Access R/W Accuracy required ASCE Quality Level? Level of Risk\nWhat are the Risks? Project delays Damage to utilities Safety of workers Safety of public Redesign costs Higher bids Change orders Extra work orders Construction claims Higher insurance / financing / construction costs Detours for travelling public Negative publicity\nHow Do We Use SUE? Commitment to avoid unnecessary utility relocations Communicate with Utilities early & often Records Research – QLD Utility Designating – QLB Topographic Survey – QLC Review of QLB, QLC & QLD Preliminary Design Utility Conflict Analysis Utility Locating – QLA Design Alternatives\nGeometric Alignment Change grade Shift alignment Widen on one side of the road Shift ramps or driveways\nDesign Alternatives Structure Design Alter footing / piling designs Provide alternative foundations Modify bridge layout Shift or modify retaining walls\nDesign Alternatives Drainage Design Dual trunk line in lieu of single trunk Shift ditches Shift structure locations (inlets, pipes, manholes) Open vs. closed system Modify side slopes or ditch slopes\nHow Do We Use SUE? Project Example Roadway Reconstruction Added Travel Lanes Drainage Improvements\nHow Do We Use SUE? CONFLICT 4 - G CONFLICT 3 - G CONFLICT 6 - G CONFLICT 5 - T CONFLICT 1 - G CONFLICT 2 - G\nHow Do We Use SUE? Utility Conflict Analysis / Matrix Identify every utility conflict with the proposed design Conflict Number Station and Offset (BL) Utility Identified Conflict Test Hole Needed Test Hole No. Utility Impact with Cost (""As- designed"") Recommended Resolution *Benefit of Resolution\nHow Do We Use SUE? Utility Conflict Analysis / Matrix Conflict Number Station and Offset (BL) Utility Identified Conflict Test Hole Needed Test Hole No. Utility Impact with Cost (""As- designed"") Recommended Resolution *Benefit of Resolution 1 43+78 25’ L 2” G Storm pipe Yes#107 Adjust gas main $5K Adjust drain pipes Avoid impact 2 43+75 27’ L 16” G Storm pipe Yes#108 Adjust gas main $20K Adjust drain pipes Avoid major impact 3 44+24 10’ L 16” G Storm pipe Yes#109 Adjust gas main $20K Adjust drain pipes Avoid major impact 4 44+25 8’ R 20” G Storm pipe Yes#142 Adjust gas main $20K Remove pipe Avoid major impact\nWhy Use SUE? Make Informed Design Decisions Designers require accurate utility information, including constructability of multi-phase projects. Avoid Using Unreliable Underground Utility Information Avoid uncertainty and second guessing where a utility may be located.\nWhy Use SUE? Avoid Costly Conflicts / Utility Relocations By knowing the exact horizontal and vertical locations of underground utilities, costly conflicts and utility relocations may be avoided, along with not having to depend upon the utilities to relocate before construction can occur. Savings and Safety Inaccurate information can result in costly conflicts, utility damage, construction delays, service disruptions, redesign, claims, and even injuries and loss of life.\nRelative Cost Savings & Benefits Purdue Study Commissioned by FHWA 71 projects studied in 4 states (VA, NC, OH, TX) Projects valued at >$1B SUE was < 0.5% of construction costs Both QLB & QLA performed\nRelative Cost Savings & Benefits Purdue Study Cost savings of $4.62 per $1.00 spent on Subsurface Utility Engineering Largest return on investment was $208/$1 1.9% savings on construction costs Quantitative costs only (Qualitative costs not included) SUE is a viable practice that reduces project costs related to the risks associated with underground utilities If used properly it could result in a minimum savings of $1 billion per year\nSUE Current Practice Owner and Engineer Responsibility In an increasing number of states, lawyers and courts are using guidelines, agency policies, and contractual instruments to define and hold professionals accountable for their standards of care. It is an Engineer’s responsibility, on behalf of an Owner, to utilize all available resources and methods to gather and provide the most accurate information possible.', 'If you suddenly find yourself without an Internet connection, there’s a good chance that somewhere a team of construction workers just uttered a collective “uh-oh” because their backhoe dug up a telecom cable. Oops. It turns out that this problem is so common that it is costing millions upon millions of dollars in repairs every year. Backhoes, drilling and digging are serious cable killers. A SMALL EXAMPLE TO START: SWEDEN In Sweden, admittedly quite a small country, around 8,000 telecom cables are cut off by backhoes every year. In fact, a construction crew managed to do this very thing not far from the Pingdom office a few months ago. The repair costs alone for these accidents are roughly $30 or so million per year in Sweden (an estimate from a recent Computer Sweden article). If we have that many incidents in a small country like Sweden, how common are they in a big country like the United States? Let’s have a look. THE US BILLION-DOLLAR FAIL Finding recent exact data for the US proved to be tricky. A Wired article from 2006 gave us some numbers to start with. In 2004, there were 675,000 excavation incidents where cables and pipes of various kinds were damaged in the US (often referred to as “underground utility damages”). More than a quarter of those, 27.5%, were telecom-related. That would mean 185,625 cases where US telecom cables were damaged in 2004. If the cost of repairing a cable is similar in Sweden and the US, telecom cable repairs may have been around $700 million in 2004. However, there are reports that things have improved. In 2007 there were an estimated 256,000 underground utility damages in the US, a lot less than in 2004. If a quarter of those were telecom-related, that leaves us with around 64,000 incidents where underground telecom cables were damaged. Even after such a big improvement, the yearly repair costs still end up in the hundreds of millions of dollars. Over a longer period of time, say 5-10 years, the repair costs will count in the billions. AND WHAT ABOUT WORLDWIDE? We have just looked at Sweden and the US here. Now imagine the costs worldwide. Those will easily amount to several billions of dollars and hundreds of thousands of telecom cable breaks due to our careless digging. Every year. And a ton of downtime for various networks, we suspect. PREVENTIVE MEASURES In the US there is a project called One Call where both companies and individuals can call in to find out what cabling exists where they are planning to dig. An interesting option has been explored in Denmark, where a central, nationwide website provides information about the underground cabling that exists in various areas. This website can be consulted by people and companies preparing an excavation. This has cut (no pun intended) the number of accidents by 70% which is a huge improvement. Sweden has just launched a similar initiative, so we’ll see if it works as well here. Now we just hope that that the operator of that huge backhoe that recently drove by doesn’t get any funny ideas… this is article is from http://royal.pingdom.com/\nUtility Locating and Marking Best Practices\n2.14 Sub Surface Utility Engineering Best Practice Statement: When applied properly during the design phase, Subsurface Utility Engineering (SUE) provides significant cost and damage-avoidance benefits and the opportunity to correct inaccuracies in existing facility records.1 - See more at: http://commongroundalliance.com/best-practices/best-practices-guide/214-subsurface-utility-engineering-sue#sthash.pHrgTCys.dpuf Practice Description: In certain cases and environments, it may be difficult or impossible to determine the locations of all utilities and/or impediments with sufficient accuracy to avoid damage or delay during construction. In these cases, SUE is applied during the design phase to locate, identify, and characterize all existing utility infrastructure (and other relevant nonutility features) found within a given project/area. SUE is applied in a structured manner in accordance with practices and quality levels found in ASCE 38-02 “Standard Guideline for the Collection and Depiction of Existing Subsurface Utility Data.” The project owner dictates the required quality levels as well as the amount of effort expended by the SUE provider on each. Although the standard is more detailed and comprehensive, the following is a brief summary of the quality levels defined therein: QL-D involves utility records research and interviews with knowledgeable utility personnel. QL-C involves surface survey and identifying and recording aboveground features of subsurface utilities, such as manholes, valves, and hydrants. QL-B involves application of “surface geophysical methods,” such as EM-based locating instruments, GPR, radar tomography, metal detectors, and optical instruments, to gather and record approximate horizontal (and, in some cases, vertical) positional data. QL-A involves physical exposure via “SOFT-DIGGING” (VACUUM EXCAVATION or hand-digging) and provides precise horizontal and vertical positional data. SUE results are integrated into the design process, in which design engineers use the information to create construction plans that accommodate existing infrastructure, thereby reducing the overall risk of conflicts and/or damage.2 - See more at: http://commongroundalliance.com/best-practices/best-practices-guide/214-subsurface-utility-engineering-sue#sthash.pHrgTCys.dpuf References: U.S. Department of Transportation—FHWA (12/1999). Cost Savings on rojects Utilizing Subsurface Utility Engineering. Pub. No. FHWA-IF-00-014 U.S. Department of Transportation—FHWA (3/2001). Subsurface Utility Engineering: Enhancing Construction Activities. Pub. No. FHWA-IF-01-011 ASCE 38-02 Standard Guideline for the Collection and Depiction of Existing Subsurface Utility Data Pennsylvania state law - See more at: http://commongroundalliance.com/best-practices/best-practices-guide/214-subsurface-utility-engineering-sue#sthash.pHrgTCys.dpuf 1. TR-2007-02: Modification to statement approved by the CGA Board on August 24, 2007 2. TR-2004-03: Amendment approved by the CGA Board on March 4, 2005 - See more at: http://commongroundalliance.com/best-practices/best-practices-guide/214-subsurface-utility-engineering-sue#sthash.pHrgTCys.dpuf US Utility Potholing & Air Excavation uses provides quality level A and quality level B Sub Surface Utility Data.\nPotholing saves time and money.\nPortable vacuum excavators have been a regular sight on job sites for years. These unique units were originally used to clean septic tanks and car wash pits and to remove slurry from horizontal directional drilling projects. Now contractors are discovering that these machines have a wide range of uses on the job site, from potholing for utilities to cleaning valve boxes. By Greg Ehm Construction Equipment News Letter http://www.constructionequipment.com/potholing-utilities-saves-time-and-money July 28, 2008 Portable vacuum excavators have been a regular sight on job sites for years. These unique units were originally used to clean septic tanks and car wash pits and to remove slurry from horizontal directional drilling projects. Now contractors are discovering that these machines have a wide range of uses on the job site, from potholing for utilities to cleaning valve boxes. Vacuum excavators are self-contained units that use pressurized air or water to displace soil and create a dry or wet spoil. The displaced dry or wet spoil is removed from the area through a hose using high-velocity suction and stored in a holding tank on the vacuum. Vacuum excavators can be mounted to a trailer or the back of a truck and range in size from 100 to 1,200 gallons of capacity. Since vacuum excavators use low-pressure air or water to remove spoil, they are perfect for potholing or identifying existing utilities during underground construction projects. ""Damaging existing utilities can be costly in terms of project downtime and potential contractor fines,"" says Dave Gasmovic, president of McLaughlin Boring Systems in Greenville, S.C. ""The low-pressure water and air will not damage existing utilities like a backhoe, compact excavator or shovel. In fact, the air and water move around the existing utilities, giving the operator a clear view."" Operators can select the amount of air or water pressure appropriate for the utility. A lower pressure of 1,500 psi should be used for gas and fiber lines in order to not damage the line coating. A higher pressure can be used for water lines. Line Exposure While locators are becoming more accurate, it\'s still important to see exactly where the line or pipe is located. Contractors are not allowed to dig in the safe zone, which may be from 18 inches up to 3 feet from either side of the marked line. The required distance varies by state. Contractors are only allowed to dig by hand or use a non-destructive method like vacuum excavators in the safe zone. Using a vacuum excavator instead of a shovel has advantages. A shovel against a water pipe is non-destructive, but on a fiber optic line a shovel can be as destructive as a backhoe, especially in hard ground conditions. ""A lot of cable has been installed using horizontal directional drills (HDD) rather than trenchers, so you don\'t have the old-fashioned ditch line like in the past,"" says Gasmovic. When lines are installed using a trencher or backhoe, a lighter material like sand is placed around the line. As a contractor digs, the ground gets softer. This indicates the line is in close proximity. Lines installed using HDD don\'t disturb the ground or leave a ditch line, so the ground is the same hardness and it is difficult to know if you are getting close to the line or cable. Since the ground may be hard, you can easily cut a cable line with a shovel. Using a vacuum with air or water at a non-damaging pressure will safely expose the line. Selecting The Right Unit Vacuum excavators come in all sizes and options, so it\'s important to select a unit that will best fit your intended use. Water-based units typically dig faster through a wide variety of spoil types and reduce the volume of the excavated material. These units move more displaced wet spoil into a holding tank than an air system. However, the displaced spoil is wet and cannot be returned to the site immediately without drying. While spoil from air systems can be directly returned to the site, these systems do not cut as well in hard ground conditions, such as clay. ""I encourage contractors to look for a unit with a good-quality vacuum blower, the heart of the vacuum,"" say Gasmovic. ""They should also select a tank that has the capacity to hold a half-day\'s or day\'s worth of spoil. This will reduce the number of trips you need to make to dump the holding tank."" If you are working in areas with cobble rock, then a unit with a 4-inch hose and 1,025-cfm blower unit will be more productive. Cobble soils will require a larger blower to effectively remove the spoil. The larger diameter of 4-inch hose will help reduce the potential for clogging. In areas without rocks, a 575-cfm system and 3-inch hose will suffice. The blower size also affects the amount of engine power required — a larger blower will increase the cost of the unit. Gasmovic recommends that contractors pay special attention to the filtration system and select a system that will filter the spoil and avoid clogging. Finally, be sure to select a strong trailer frame that will support the weight of the unit and a full tank of spoil. There are a number of options available: controls that allow the contractor to reverse the flow of the vacuum to blow the spoil back into the hole; booms that support the weight of the hose, placing less effort on the operator; combo units that include a jetter to clean sewers and remove the resulting trash; and automatic tank clean-out systems and auxiliary hydraulic systems that allow the contractor to run a concrete saw or breaker off the unit. The Payoff Taking the extra steps to pothole may seem like an added expense or more time, but Gasmovic stresses that safety is important. ""Hitting a gas line with a backhoe, trencher or HDD could be catastrophic. A water line hit could put a hospital out of business,"" says Gasmovic. ""The cost of shutting down a project for a day is sure to exceed the cost for a $3,000 locator and a little extra time."" In this photo Chris is starting a Pothole to verify existing utilities with the use of air / vacuum excavation. This is an example of our employee Mario digging a utility test hole or utility pothole to collect Sub Surface Engineering Data using air vacuum excavation method. We collect SUE Data through these methods at US Utility Potholing & Air Excavation.']"	['<urn:uuid:197e138b-45d2-4fdd-89a6-877706faeea1>', '<urn:uuid:2c52c253-66da-4e18-a666-896baf004532>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T18:05:20.075489	26	102	3029
51	What's the link between Turner syndrome and Noonan syndrome?	While both Noonan syndrome and Turner syndrome can present with similar features like short stature and lymphatic abnormalities, they are distinct conditions. Noonan syndrome affects both males and females and used to be referred to as the male version of Turner's syndrome, but they have different genetic causes. Turner syndrome is specifically caused by a chromosomal abnormality (45,XO) and occurs only in females, with an incidence of 1 in 2,000 to 1 in 10,000 female births.	"['To use all functions of this page, please activate cookies in your browser.\nWith an accout for my.bionity.com you can always see everything at a glance – and you can configure your own website and individual newsletter.\n- My watch list\n- My saved searches\n- My saved topics\n- My newsletter\nNoonan Syndrome (NS) is a relatively common congenital genetic condition which affects both males and females. It used to be referred to as the male version of Turner\'s syndrome; however, the genetic causes of Noonan syndrome and Turner syndrome are distinct. The principal features include congenital heart malformation, short stature, learning problems, indentation of the chest, impaired blood clotting, and a characteristic configuration of facial features. The syndrome is named after Dr Jacqueline Noonan.\nIt is believed that between approximately 1 in 1,000 and 1 in 2,500 children worldwide are born with NS. It is one of the most common genetic syndromes associated with congenital heart disease, similar in frequency to Down syndrome. However, the range and severity of features can vary greatly in patients with NS. Therefore, the syndrome is not always identified at an early age.\nAdditional recommended knowledge\nRecurrence in siblings and apparent transmission from parent to child has long suggested a genetic defect with autosomal dominant inheritance and variable expression. A person with NS has up to a 50% chance of transmitting it to a child. The fact that an affected parent is not always identified for children with NS suggests several possibilities:\nIn most of the families with multiple affected members, NS maps to chromosome 12q24.1. In 2001, it was reported that approximately half of a group of patients with Noonan syndrome carried a mutation of the PTPN11 gene at that location, which encodes protein tyrosine phosphatase SHP-2. The SHP2 protein is a component of several intracellular signal transduction pathways involved in embryonic development that modulate cell division, differentiation, and migration, including that mediated by the epidermal growth factor receptor. The latter pathway is important in the formation of the cardiac semilunar valves. It has recently been shown that activating mutations in SOS1 also give rise to NS. Shp2 and SOS1 both have roles as positive regulators of the Ras/MAP kinase pathway suggesting that dysregulation of this pathway may play a major role in the genesis of this syndrome. Additional mutations in KRAS  and RAF1 genes have been reported to cause Noonan syndrome in a smaller percentage of individuals with the syndrome.\nManifestations by organ system\nThe most prevalent (common) signs are highlighted in bold with frequency listed in parentheses.\nHEART —(2/3 of patients have a heart defect)\nArnold Chiari Malformation (Type 1) has been noted in some patients with Noonan Syndrome\nBy physical appearance\nDespite identification of four causative genes, the diagnosis of Noonan syndrome is still based on clinical features. In other words, it is made when a physician feels that a patient has enough of the features to warrant the label indicating association. The patient can be tested for mutations in the PTPN11, SOS1, or KRAS genes, however absence of a mutation will not exclude the diagnosis as there are more as yet undiscovered genes that cause NS. The principal values of making such a diagnosis are that it guides additional medical and developmental evaluations, it excludes other possible explanations for the features, and it allows more accurate recurrence risk estimates.\nJacqueline Noonan was practicing as a pediatric cardiologist at the University of Iowa when she noticed that children with a rare type of heart defect, valvular pulmonary stenosis, often had a characteristic physical appearance with short stature, webbed neck, wide spaced eyes, and low-set ears. Both boys and girls were affected. Even though these characteristics were sometimes seen running in families, chromosomes appeared grossly normal. She studied 833 patients at the congenital heart disease clinic, looking for other congenital abnormalities, and in 1962 presented a paper: ""Associated non-cardiac malformations in children with congenital heart disease"". This described 9 children who in addition to congenital heart disease had characteristic faces, chest deformities and short stature. Both males and females were found to be similarly affected, and the chromosomes were apparently normal.\nDr John Opitz, a former student of Dr Noonan, first began to call the condition ""Noonan Syndrome"" when he saw children who looked like those whom Dr Noonan had described. Dr Noonan later produced a paper entitled ""Hypertelorism with Turner Phenotype"", and in 1971 at the Symposium of Cardiovascular defects, the name \'Noonan Syndrome\' became officially recognized.\nshort stature (Aarskog-Scott syndrome, Cockayne syndrome, Cornelia de Lange Syndrome, Dubowitz syndrome, Noonan syndrome, Robinow syndrome, Silver-Russell dwarfism, Seckel syndrome, Smith-Lemli-Opitz syndrome)\novergrowth (Beckwith-Wiedemann syndrome, Sotos syndrome, Weaver syndrome)Marfan syndrome - Alport syndrome - Bardet-Biedl syndrome - Zellweger syndrome\n|Other||spleen: Asplenia - Splenomegaly\nendocrine glands: Persistent thyroglossal duct - Thyroglossal cyst\nSitus inversus - Conjoined twinsCowden syndrome - Hamartoma', ""Dr. Anil B. Jalan\nIntrodution :- Hydrops Fetalis is a diagnosis that in the past , was made after delivery and was described as excess collection of fluid in several neonatal body cavities .\nBallantyne described the first case of Hydrops Fetalis 100 yrs. ago , and 50 yrs. later ,Potter described nonimmune hydrops fetalis .\nNonimmune Hydrops Fetalis is responsible for 3 % of overall perinatal mortality .\nIn 1970 , MaCaffe et.al. reported that 82 % of hydrops cases were caused by immune diseases ; by 1992 , 87 % were caused by nonimmune conditions.\nDefinition :- The presence of excess extracellular fluid in two or more sites without any identifiable circulating antibody to red blood cell antigens .\nIncidence :- Most of the large serieses have reported an incidence of 1 : 2000 to 1 : 3000. Almost 85 - 90 % of these hydrops fetalis incidents are due to nonimuune causes .\nSince the advent of ultrasonography more and more cases of fetal hydrops are being detected by gynecologists & sonologists. At times it becomes diffcult to predict the outcome of such hydropic fetuses . The causes of hydrops fetalis are many , but few common ones must be remembered at the time of sonography , so that we look for other evidence which will give support to our provisional diagnosis .\nSeveral large serieses have suggested that cardiac malformations are amongst the most common causes , because they are often found in hydropic fetuses . Yet , cardiac malformations are one of the most common defects and only seldom are associated with nonimmune hydrops fetalis . Thus , the presence of a heart defect doesnot prove that hydrops results from heart failure .\nAt present three main hypotheses have been proposed for the pathophysiological mechanisms underlying hydrops :-\n1 ] Inadequate cardiac output due mainly to\na. Obstructive out-flow\nb. Diverted blood flow\nc. Inadequate blood return\nd. Inadequate ventricualr filling\ne. Inadequate inotropic force\nf. Regurgitation of blood across Cardiac valves .\ng. Reversal of blood flow in I.V.C.\nAll the above mechanisms lead to cardiac failure , cardiomegaly , and elevated umbilical venous pressure , also seen in Rh immune hydrops , due to portal hypertension .\nPortal hypertension in immune hydrops is due to hypertrophy of Hepatic Erythropoeitic tissue .\n2 ] Lymphatic Abnormalities Found in association with :-\na. Cystic hygroma\nb. Noonan syndrome\nc. Turner's syndrome\nd. Pulmonary or peritoneal lymphangiectasia .\ne. Lymphatic venous anastomosis .\nf. Connective tissue malformation e.g. - skeletal dysplasia .\ng. S.O.L. in thorax e.g. -Cystic adenamatoid malformation of lung & Diaphragmatic hernia .\nPlease note that in the above situation the umbilical venous pressure is normal .\n3 ] Reduced osmotic pressure :- Hypoproteinemia .\nThe fetal outcome & the genetic counselling entierly depends upon the exact etiology of such hydrops . Without repeated USG & colour doppler it is impossible to judge the progress of the anasarca . It is quite evident in our cases . Over a period of 4 wks. we could demonstrate progressive anasarca & suspicion of chromosomal anomaly . This helped tremendously to gynecologists & parents in decision making , which was MTP . However in certain cases especially in the immune hydrops cases , in-utero intervention is possible but the decision of undertaking such heroic measures in the yet unborn patient entirely depends on the parental willingness . The causes of hydrops fetalis are given below .\nFollowing is a list of few common causes of hydrops fetalis .\nCauses of hydrops fetalis :-\na. Rhesus hemolytic anemia\nb. Alfa thalassemia\nc. Fetal erythro - leukemia\nb. CMV infection\ne. Other fetal infections .\na. Anagioma of palcenta or fetus\nb. Renal or umbilical vein thrombosis\nc. Cardiovascular malformation .\nd. Fetal tachycardia\ne. Cardiac rhabdomyoma ( Tuberous sclerosis )\na. Fetal haemorrhage\nb. Twin to twin tranfussion syndrome\na. Fetal red cell enzyme defects\nb. Lysosomal enzyme defects\na. Cystic adenomatoid malformation of the lung\nb. Extralobar pulmonary sequestration\nc. Pulmonary hypoplasia\nd. Pulmonary lymphangiectasia\ne. Tracheal atresia\nf. Diaphragmatic hernia\na. Hepatitis or hepatic necrosis\n8 . Lower urinary tract abnormalities\n9 . Congenital neuroblastoma , teratoma, glioma\n10. Some types of short limbed dwarfism\n11. Noonan syndrome\n12. Nuchal bleb syndrome\n13. Optiz frias syndrome\n14. Turner syndrome\n15. Trisomy 18 and 21\n17. Maternal nephrotic syndrome\nSuggested investigations :-\nMaternal tests with live fetus inutero :\n1. BL.Gr. Serology and typing including titre for anti-d antibodies.\n2 . Serum tests for Syphilis ( VDRL ) & TORCH titre .\n3 . Kleihauer test for feto maternal haemorrhage .\n4 . Fetal USG for cardiac anomalies and tumours .\n5 . U.S.G. for placental hemangioma\nCord blood at delivery - irrespective of outcome .\n1 . Blood Group Serology and tying\n2 . Chromosome analysis.\n3 . Total protein and albumin\n4 . TORCH titre\n5 . Hb and Hb - EPP . ( H.P.L.C. )\n6 . Vacuolated lymphocytes\n7 . Enzymology ( red cell and lysosomal enzyme ).\nIn the event of infant death detailed antopsy and H.P. is indicated especially to look for anomalies :-\n1 . Cardiovascular system .\n2 . Kidneys .\n3 . Central nervous system .\nTissues from all usual organs must be taken for H.P.\nIf possible viral culture and chromosome analysis is indicated .\nFrom the above discussion it appears that the list of causes is extensive . However it will be helpful to know that hydrops fetalis is common in certain conditions and only occasionally seen in others .\nHydrops fetalis is frequent in :-\n1 ) Achondrogenesis , type I .\n2 ) Fibro chondro genesis .\n3 ) Monozygotic twinning and structural defects .\n4 ) Osteogenesis Imperfecta syndrome type II .\n5 ) XO - syndrome ( Turner syndrome ).\nOccasional in :-\n1 . Achondrogenesis - Hypochandrogeneis , type II\n2 . Chondrodysplasia punctata - X Linked Dominant type .\n3 . Down - syndrome .\n4 . Generalised gangliosidosis , type I & severe infantile type .\n5 . Lethal Multiple Pterygium syndrome .\n6 . Morquio syndrome .\n7 . Mucopolysacchroidosis type VII\n8 . Short Rib Polydactyly syndrome type II ( Majewski type SRP )\nIt is impossible to discuss all the causes and pathologies of hydrops fetalis but we would like to highlight few conditions of clinical interest , especially syphilis and parvo - virus infection .\nHydrops Fetalis in Turner?Syndrome :- The Turner?syndrome is one of the commonest cause for fetal hydrops especially in the Indian situation . The frequency of Turner?syndrome amongst liveborn infants is 1 in 2,000 females according to D.W.Smith and 1 in 10,000 females according to J.L.Simpson & M.S.Golbus . There are not many studies available for the incidence amongst spontaneous abortions , but one by DR. J.L.Simpson gives 8.6 % due to Turner ( 45 , OX ) in spontaneous abortion cases .\nMonosomy X ( i.e. Turner ) is the signal most common chromosomal abnormality in spontaneous abortions , accounting for 20 - 25 % of abnormal specimens . There are no definite figures available for hydrops fetalis cases .\nEtiology :- Faulty chromosomal distribution leading to XO individual with 45 chromosomes . The paternal chromosome is the one more likely to be missing . There has been no significant older age factor for this neuploidy . On the contatry it is more common in the young mothers ( Warburton et. al. 1980 ) .This has been our observation also, especially amongst the hydropic fatuses and in the first trimester abortion cases . It is generally a sporadic event in a family , although there are as yet no adequate data on risk of recurrence . Mosaicism doesnot ensure survival till term . However the incidence of sex chromosome mosaiciam is higher in liveborns than in aborted 45 XO Fetuses .\nPatients who had one abortion with 45 XO karyotype doesnot mean that we will have only the similar type of recurrence . Wharton et. al. 1987 , has showed that the mothers who had 45 XO fetuses also had normal Fetuses , Monosomies , Triploidies in future .\nThe above discussion definitely emphasises one point that we must advise prenatal diagnosis during the next pregnancy .\nThe clinical features of Turner syndrome are summarised below . The interested readers are referred to the classical text book - Smith?recognizable patterns of human malformation , 5 th Edn.\nClinical Features of Turner?Syndrome :-\nCommon Features :-\n1 . Short stature\n2 . Edema of fingers and toes\n3 . Mild pectus excavatum\n4 . Anomalous auricles , mostly prominent .\n5 . Narrow maxilla and palate .\n6 . Relatively small mandible .\n7 . Inner canthal folds .\n8 . Short neck and low posterior hair line .\n9 . Webbed posterior neck .\n10. Cubitus valgus.\n11. Medial Tibial exostosis .\n12. Broad chest and widely spaced nipples .\n13. Short 4 th metacarpals/ metatarsals\n14. Bone dysplasia with coarse tubular pattern .\n15. Narrow hyperconvex or deep set nails .\n16. Excessive pigmented naevi .\n17. Distal A.T.D. angle .\n18. Loose skin about neck in infancy\nOther anomalies :-\n1 . Horse shoe kidney .\n2 . Double or cleft renal pelvis .\n3 . Cardiac defects - V.S.D. , Coarctation of aorta etc.\n4 . Perceptive hearing deficits.\n5 . Abnormal angulation of radius to carpal bones .\n6 . Short mid phalanx of the 5 the finger ( cleinodactyly ).\n7 . Scoliosis , kyphosis .\n8 . Spina bifida .\n9 . Vertebral fusion .\n10. Cervical rib.\n11. Anomalous sell turcica .\n12. Ptosis .\n13. Strabismus .\n14. Blue sclerae .\n15. Catarct .\n16. Mental Retardation .\n17. Haemangiomata of intestine .\n18. Idiopathic hypertension .\nFetal Changes :-\n1 . Hydrops fetalis .\n2 . Cystic Hygroma .\n3 . Single Umbilical artery .\n4 . I.U.G.R.\n5 . Congenital Heart Defects .\nAdult Turner?Syndrome with additional features :-\n1 . Hypertension .\n2 . Diabetes Mellitus .\n3 . Delayed Puberty , Amenorrhea ,\nMenstrual disturbances and\nHydrops in other Cytogenetic abnormalities :\nDR. L. P. Shulman from Memphis\n( U.S.A. ) published his data of 18 cases of hydrops in 1998 where hydrops was detected as space suit hydrops in the first trimester of the pregnancy and C.V.Bx or Amniotic Fluid karyotyping was done on all the patients . His results are given below :\nShulman et. al. :-\nTotal No. of cases 18\nChromosome abnormalities 15\n( 83.3 % )\nSex chromosome abnormal 7\nNormal Chromosomes 3 ( 16.7 % )\nTurner Syndrome ; 45 , XO - classical 6 / 15 ( 40.0 % )\nTurner Syndrome Mosaic ; 45 , XO / 46 , XY 1 / 15 ( 6.67 % )\nTrisomy - 21 ; 47 , XX or XY + 21 5 / 15 ( 33.33 % )\nTrisomy 18 ; 47 , XX or XY + 18 3 / 15 ( 20.00 % )\nThe ratio of 7 : 8 of sex chromosome to autosome abnormalities is greater than that observed ( approximately 1 : 4 ) among fetuses with isolated prominent nuchal translucency .\nHydrops in Syphilis :- Hydrops is quite variable in its frequency . Tan et. al. Found it in 40 % of his series , while any number of others failed to mention it at all ! The edema may be so severe that infant is thought be the product of a severely Rh sensitized multiparous mother . It is mandatory that congenital syphilis be considered as the cause in every instance of perinatal non immune hydrops .\nParvovirus Infection :- This is one of the most important viruses responsible for fetal hydrops . Human Parvovirus was first discovered in 1975 and it was first associated with adverse pregnancy outcome in 1984 . This virus is a small single stranded DNA virus , singularly dependent on host cell function . Autonomous parvoviruses including the human parvovirus B . 19 are able to replicate without the help from another virus , but only in the proliferating cells . In general , mammalian parvoviruses are species specific . To date the only human parvovirus identified is B - 19 .\nThe diseases caused by Parvoviruses are :-\n1 . Aplastic crisis in children with sickle cell disease .\n2 . Erythema Infectiosum . ( E.I. )\n3 . Febrile illness with arthargia and arthritis .\n4 . Chronic anemia in immune compromised host .\n5 . Haemo-phagocytic syndrome .\nInfection with B - 19 is relatively common , seroprevalence studies indicate that 30 % to 60 % of adults have been infected . Modes of transmission of the virus have not been completely defined but appear to include respiratory secretions . Viremia occurs approximately one week after inoculation and is accompanied by rash and joint symptoms , approximately 10 days later . There is transient reticulocytopenia . There may be slapped cheek appearance of EI .\nLaboratory Investigations :-\n1 . Serologic diagnosis - Ig G & Ig M - B - 19 specific antibodies .\n2 . Detection of DNA by P.C.R. appears to be the most sensitive test for detection of this virus .\nEffects on pregnancy and fetus . :-\nFetal infection at all stages of pregnancy has been documented with a spectrum of consequences -\n1 . Spontaneous abortions .\n2 . Still birth .\n3 . Severe non immune hydrops .\nFetal damage is not inevitable following maternal infection , and infact , a healthy infant is the most common product of a pregnancy complicated by parvovirus infection .\nThe risk of unfavourable outcome varies from less than 10 % to 38 % in different serieses published .\nThe evidence for treatogenicity is weak and till to date there is only one report of a congenital anomaly in a B - 19 infected fetus .\nIn the infected fetuses , the principal organ affected is the bone marrow . The red cell survival in fetuses is reduced from normal 120 days to between 45 and 70 days and profound anemia results from B - 19 induced erythroid bone marrow aplasia . Fetal blood sampling in one affected fetus revealed a Hb of 1.8 Gm / dL and reticulo-cytopenia . Usually the bone marrow recovers in 7 - 10 days .In immuno-immature infants prolonged infection may occur . Transfusion of infected fetuses with packed red blood cells has been successfully used to treat the intrauterine hydrops but experience is limited .\nLimited data suggests that elevation of M.S.A.F.P. values may be a marker for the development of hydrops fetalis .\nA possible immunologic origin of idiopathic nonimmune hydrops fetalis has been suggested . Of 324 cases of prenatally diagnosed NIHF , 49 (15.1 % ) could be classified as idiopathic . The proportion of parents sharing 4 or 5 H.L.A. antigens was increased significantly in the 38 patients of the idiopathic group as compared to 38 age and parity paired controls . Besides in 8 patients , an increased paternal histo-compatibility and a decreased incidence and percentage of lympho-cytotoxic antibodies was observed .\nMaternal complications of nonimmune hydrops fetalis :-\n1 . Poly-hydramnios :- commonest\n2 . Anemia\n3 . Preterm labour\n4 . Pregnancy induced hypertension\n5 . Post partum haeorrhage .\nProtocol for management of nonimmune hydrops :-\nMaternal causes :-\n1. Obstetric history\n2. Past medical history\n3. Family H/O genetic disorders\n4. Recent infections\n5. Fetal activity\na. Blood group and antibodies\nb. Viral screen / TORCH titre\nd. G.T.T / Glycosylated Hb\ne. Auto-antibody screen\nf. Kleihauer - Betke count\ng. Full blood count\nh. Hb Electrophoresis\nFetal causes :-\nU.S.G. Evaluation :\nDoppler Blood Flow Studies :\na. Umbilical artery\nb. Middle cerebral artery\nd. Umbilical artery pressure\nFetal Blood Sample :\n1. Full blood count\n2. Bl.Gr. & Coombs test\n3. Fetal blood abnormal Hb studies\n5. Blood gases\n6. Serum protein\n7. Viral screen / TORCH titre\n8. Metabolic disorders study\nAborted Fetus Or Dead Fetus :\n1 . Complete autopsy of fetus\n( if parents are willing ) .\n2 . Clinical photographs of the fetus .\n3 . X ray - complete babaygram.\n4 . Collect blood as follows -\n( Use Vacutainer tubes only ) .\n5 ml in E.D.T.A. tubes\n5 ml in heparinised tube\n10 ml in plain tube .\n5 . Two pieces of placenta of 1?1?\neach in normal saline & formalin\n( two separate sterile containers ).\nTo keep all the material at 2 - 8 centigrade temp. in fridge .\nTo inform geneticist within 24 hrs. or send the samples as eraly as possible to lab along with X-ray & clinical photographs of the fetus .\n1 . Avron Y. Sweet & Edwin G. Brown , Fetal & Neonatal Effects of Maternal Diseases , 1991 , Page no. 162 - 163 .\n2 . Duru Sushil Shah , An Introduction To Genetics and Fetal Medicine. Editor : Dr. Kamini Rao , Page no. 130 - 136 .\n3 . J.L. Simpson & Golbus , Genetics in Obstetrics and Gynecology , 2nd Edition.Page no. 61 & 187 .\n4 . L.P. Shulman & O.P. Phillips , Prenatal Diagnosis and Therapy , 1998 ,Proceedings of 8th international Conference on prenatal Diagnosis of genetic disorders ., Page no. 22 - 24 .\n5 . Mary L. Kumar M.D., Fetal & Neonatal Effects of Maternal Disease by Avron Y. Sweet & Edwin G. Brown ( 1991 ) , Page no. 39 to 44.\n6 . Smith's Recognisable Patterns of Human Malformation , 5 th edition , 1997 Page 843 - 844 .""]"	['<urn:uuid:2850c052-410a-4313-bc6f-093973e8982a>', '<urn:uuid:825fc625-73d2-400f-a4c1-2c832bd5f387>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T18:05:20.075489	9	76	3777
52	As someone interested in animal navigation, I'd like to know what's the connection between a pigeon's ability to detect magnetic fields and how neurons normally communicate in the brain?	Pigeons detect magnetic fields through electromagnetic induction in their inner ear, which activates specialized hair cells containing voltage-gated calcium channels. This process integrates with normal neuronal communication, where neurons in the vestibular nuclei are activated and communicate through chemical synapses - releasing neurotransmitters when calcium channels open at the active zone. The magnetic information is thus converted into standard neural signals that can be processed by the brain.	"['Using maps, compasses, and sextants, mariners in the early 1500’s developed the first methods to navigate the open sea; heralding an age of exploration as humanity set sail for thehorizon. Yet long before this time evolution had equipped life on the planet with a biological global positioning system that was far superior to those early navigational tools – the Magnetic Sense. While there is unequivocal behavioural evidence demonstrating that this faculty exists, it is the least understood of all senses. The location of the primary sensors, the underlying biophysical mechanisms, and the neurological basis of the sense are unknown. Currently, there are three ideas that aim to explain how magnetosensation might work: (1) magnetite based magnetoreception; (2) a light sensitive radical pair based model; and (3) electromagnetic induction (See Figure).\nOur goal is to identify the molecules, cells and circuits that underlie the magnetic sense in pigeons. To achieve this objective we employ an assay that assesses neuronal activation within the pigeon brain, following exposure to magnetic fields generated by Helmholtz coils. These experiments have confirmed that magnetic stimuli results in the activation of neurons in the vestibular nuclei, implicating the inner ear of pigeons in the magnetic sense. We have shown experimentally and by physical calculations that magnetic stimulation can induce electric fields in the pigeon semicircular canals that are within the physiological range of known electroreceptive systems. This in turn led to the discovery of a splice isoform of a voltage-gated calcium channel (CaV1.3) in the pigeon inner ear that has been shown to mediate electroreception in skates and sharks (Nimpf et al, Current Biology 2019). These data have led us to propose that pigeons detect magnetic fields by electromagnetic induction within the semicircular canals that is dependent on the presence of apically located voltage-gated cation channels in a population of electrosensory hair cells.\nAre hair cells the primary magnetosensors? Having established a physiologically relevant readout for the magnetic sense, we can now ask if hair cells are the primary sensors. To do so we are employing anatomical perturbations (e.g. hair cell ablation with antibiotics), which we expect will be superseded by genetic methods.\nWhat neuronal circuits process magnetic information? To gain further insight into the underlying circuitry that processes magnetic information we have established an iDISCO clearing protocol for the pigeon brain that results in a translucent brain that can be stained with markers such as c-fos followed by light sheet microscopy (Fig. 2). We are employing this technology to identify the circuits that process magnetic information and to interrogate the underlying biophysical mechanisms.\nHow is magnetic information encoded in the avian brain? To address this question brain we have built a 2-photon microscope that permits in vivo calcium imaging while exposing pigeons to precise magnetic stimuli. Coupled with a genetically encoded calcium indicator (e.g GCaMP6) delivered by an adeno associated virus we are able to study which components of the magnetic field elicit neuronal activity (i.e. intensity, polarity and inclination) and how this information is integrated into existing neuronal networks. Are there magnetic place cells?\nFigure 2: Image showing a whole pigeon brain prior to the iDISCO clearing protocol (left), after bleaching (middle), and following the removal of lipids (right). Coupled with immunostaining and light sheet microscopy this method allows us to globally assess neuronal activation in the pigeon brain.\nCollaborators: Dr Jeremy Shaw (University of Western Australia), Dr Matthew Mason (University of Cambridge), Dr Michael Eisterer (Technical University Vienna).\nFunded by: ERC Starting Grant – The Cellular and Molecular Basis of Magnetoreception, ERC Consolidator Grant – The Neurological Basis of the Magnetic sense.', 'A diagram of a typical central nervous system synapse. The proteins of the active zone are represented as dark brown pyramids on the upper neuron terminal\nThe active zone or synaptic active zone is a term first used by Couteaux and Pecot-Dechavassinein in 1970 to define the site of neurotransmitter release. Two neurons make contact through structures called synapses allowing them to communicate with each other. As shown in the diagram on the right, a synapse consists of the presynaptic bouton of one neuron which stores vesicles containing neurotransmitter (uppermost in the picture) and a second, postsynaptic neuron which bears receptors for the neurotransmitter (at the bottom). When an action potential reaches the presynaptic bouton, the contents of the vesicles are released into the synaptic cleft and the released neurotransmitter travels across the cleft to the postsynaptic neuron (the lower structure in the picture) and activates the receptors on the postsynaptic membrane.\nThe active zone is the region in the presynaptic bouton that mediates neurotransmitter release and is composed of the presynaptic membrane and a dense collection of proteins called the cytomatrix at the active zone (CAZ). The CAZ is seen under the electron microscope to be a dark (electron dense) area close to the membrane. Proteins within the CAZ tether synaptic vesicles to the presynaptic membrane and mediate synaptic vesicle fusion, thereby allowing neurotransmitter to be released reliably and rapidly when an action potential arrives.\nThe function of the active zone is to ensure that neurotransmitters can be reliably released in a specific location of a neuron and only released when the neuron fires an action potential. As an action potential propagates down an axon it reaches the axon terminal called the presynaptic bouton. In the presynaptic bouton, the action potential activates calcium channels (VDCCs) that cause a local influx of calcium. The increase in calcium is detected by proteins in the active zone and forces vesicles containing neurotransmitter to fuse with the membrane. This fusion of the vesicles with the membrane releases the neurotransmitters into the synaptic cleft (space between the presynaptic bouton and the postsynaptic membrane). The neurotransmitters then diffuse across the cleft and bind to ligand gated ion channels and G-protein coupled receptors on the postsynaptic membrane. The binding of neurotransmitters to the postsynaptic receptors then induces a change in the postsynaptic neuron. The process of releasing neurotransmitters and binding to the postsynaptic receptors to cause a change in the postsynaptic neuron is called neurotransmission.\nThe active zone is present in all chemical synapses examined so far and is present in all animal species. The active zones examined so far have at least two features in common, they all have protein dense material that project from the membrane and tethers synaptic vesicles close to the membrane and they have long filamentous projections originating at the membrane and terminating at vesicles slightly farther from the presynaptic membrane. The protein dense projections vary in size and shape depending on the type of synapse examined. One striking example of the dense projection is the ribbon synapse (see below) which contains a ""ribbon"" of protein dense material that is surrounded by a halo of synaptic vesicles and extends perpendicular to the presynaptic membrane and can be as long as 500 nm! The glutamate synapse contains smaller pyramid like structures that extend about 50 nm from the membrane. The neuromuscular synapse contains two rows of vesicles with a long proteinaceous band between them that is connected to regularly spaced horizontal ribs extending perpendicular to the band and parallel with the membrane. These ribs are then connected to the vesicles which are each positioned above a peg in the membrane (presumably a calcium channel). Previous research indicated that the active zone of glutamatergic neurons contained a highly regular array of pyramid shaped protein dense material and indicated that these pyramids were connected by filaments. This structure resembled a geometric lattice where vesicles were guided into holes of the lattic. This attractive model has come into question by recent experiments. Recent data shows that the glutamatergic active zone does contain the dense protein material projections but these projections were not in a regular array and contained long filaments projecting about 80 nm into the cytoplasm.\nThere are at least five major scaffold proteins that are enriched in the active zone; UNC13/Munc13, RIMs (Rab3-interacting molecule), Bassoon, Piccolo/aczonin, ELKS, and liprins-α. These scaffold proteins are thought to be the constituents of the dense pyramid like structures of the active zone and are thought to bring the synaptic vesicles into close proximity to the presynaptic membrane and the calcium channels. The protein ELKS binds to the cell adhesion protein, β-neurexin, and other proteins within the complex such as Piccolo and Bassoon. β-neurexin then binds to cell adhesion molecule, neuroligin located on the postsynaptic membrane. Neuroligin then interacts with proteins that bind to postsynaptic receptors. Protein interactions like that seen between Piccolo/ELKS/β-neurexin/neuroligin ensures that machinery that mediates vesicle fusion is in close proximity to calcium channels and that vesicle fusion is adjacent to postsynaptic receptors. This close proximity vesicle fusion and postsynaptic receptors ensures that there is little delay between the activation of the postsynaptic receptors and the release of neurotransmitters.\nNeurotransmitter release mechanism\nThe release of neurotransmitter is accomplished by the fusion of neurotransmitter vesicles to the presynaptic membrane. Although the details of this mechanism are still being studied there is a consensus on some details of the process. Synaptic vesicle fusion with the presynaptic membrane is known to require a local increase of calcium from as few as a single, closely associated calcium channels and the formation of highly stable SNARE complexes. One prevailing model of synaptic vesicle fusion is that SNARE complex formation is catalyzed by the proteins of the active zone such as Munc18, Munc13, and RIM. The formation of this complex is thought to ""prime"" the vesicle to be ready for vesicle fusion and release of neurotransmitter (see below: releasable pool). After the vesicle is primed then complexin binds to the SNARE complex this is called ""superprimed."" The vesicles that are superprimed are within the readily releasable pool (see below) and are ready to be rapidly released. The arrival of an action potential opens voltage gated calcium channels near the SNARE/complexin complex. Calcium then binds to changes the conformation of synaptotagmin. This change in conformation of allows synaptotagmin to then dislodge complexin, bind to the SNARE complex, and bind to the target membrane. When synaptotagmin binds to both the SNARE complex and the membrane this induces a mechanical force on the membrane so that it causes the vesicle membrane and presynaptic membrane to fuse. This fusion opens a membrane pore that releases the neurotransmitter. The pore increases in size until the entire vesicle membrane is indistinguishable from the presynaptic membrane.\nSynaptic vesicle cycle\nThe presynaptic bouton has an efficiently orchestrated process to fuse vesicles to the presynaptic membrane to release neurotransmitters and regenerate neurotransmitter vesicles. This process called the synaptic vesicle cycle maintains the number of vesicles in the presynaptic bouton and allows the synaptic terminal to be an autonomous unit. The cycle begins with (1) a region of the golgi apparatus is pinched off to form the synaptic vesicle and this vesicle is transported to the synaptic terminal. At the terminal (2) the vesicle is filled with neurotransmitter. (3) The vesicle is transported to the active zone and docked in close proximity to the plasma membrane. (4) During an action potential the vesicle is fuses with the membrane, releases the neurotransmitter and allows the membrane proteins previously on the vesicle to diffuse to the peri-active zone. (5) In the peri-active zone the membrane proteins are sequestered and are endocytosed forming a clathrin coated vesicle. (6) The vesicle is then filled with neurotransmitter and is then transported back to the active zone.\nThe endocytosis mechanism is slower than the exocytosis mechanism. This means that in intense activity the vesicle in the terminal can become depleted and no longer available to be released. To help prevent the depletion of synaptic vesicles the increase in calcium during intense activity can activate calcineurin which dephosphorylate proteins involved in clathrin-mediated endocytosis.\nThe synapse contains at least two clusters of synaptic vesicles, the readily releasable pool and the reserve pool. The readily releasable pool is located within the active zone and connected directly to the presynaptic membrane while the reserve pool is clustered by cytoskeletal and is not directly connected to the active zone.\nThe releasable pool is located in the active zone and is bound directly to the presynaptic membrane. It is stabilized by proteins within the active zone and bound to the presynaptic membrane by SNARE proteins. These vesicles are ready to release by a single action potential and are replenished by vesicles from the reserve pool. The releasable pool is sometimes subdivided into the readily releasable pool and the releasable pool.\nThe reserve pool is not directly connected to the active zone. The increase in presynaptic calcium concentration activates the calcium sensitive phosphatase, calcineurin. Calcineurin dephosphorylates a protein, synapsin, that mediates the clustering of the reserve pool vesicles. Dephosphorylation of synapsin mobilize vesicles in the reserve pool and allows the vesicles to migrate to the active zone and replenish the readily releasable pool.\nThe periactive zone surrounds the active zone and is the site of endocytosis of the presynaptic terminal. In the periactive zone, scaffolding proteins such as intersectin 1 recruit proteins that mediate endocytotis such as dynamin, clathrin and endophilin. In Drosophilia the intersectin homolog, Dap160, is located in the periactive zone of the neuromuscular junction and mutant Dap160 deplete synaptic vesicles during high frequency stimulation.\nRibbon Synapse Active Zone\nThe ribbon synapse is a special type of synapse found in sensory neurons such as photoreceptor cells, retinal bipolar cells, and hair cells. Ribbon synapses contain a dense protein structure that tethers an array of vesicles perpendicular to the presynaptic membrane. In an electron micrograph it appears as a ribbon like structure perpendicular to the membrane. Unlike the \'traditional\' synapse, ribbon synapses can maintain a graded release of vesicles. In other words the more depolarized a neuron the higher the rate of vesicle fusion. The Ribbon synapse active zone is separated into two regions, the archiform density and the ribbon. The archiform density is the site of vesicle fusion and the ribbon stores the releasable pool of vesicles. The ribbon structure is composed primarily of the protein RIBEYE, about 64-69% of the ribbon volume, and is tethered to the archiform density by scaffolding proteins such as Bassoon.\nProteins of the Active Zone\n|ELKS (ERCs or CAST)|\n|Docking and Priming|\n|syntaxin||Located on the synaptic membrane and binds to SNAP-25 and synaptobrevin to mediate vesicle fusion.|\n|Voltage-dependent calcium channel (VDCC)||Allows the rapid influx of calcium during an action potential.|\nMeasuring Neurotransmitter Release\nNeurotransmitter release can be measured by determining the amplitude of the postsynaptic potential after triggering an action potential in the presynaptic neuron. Measuring neurotransmitter release this way can be problematic because the effect of the postsynaptic neuron to the same amount of released neurotransmitter can change over time. Another way is to measure vesicle fusion with the presynaptic membrane directly using a patch pipette. A cell membrane can be thought of as a capacitor in that positive and negative ions are stored on both sides of the membrane. The larger the area of membrane the more ions that are necessary to hold the membrane at a certain potential. In electrophysiology this means that a current injection into the terminal will take less time to charge a membrane to a given potential before vesicle fusion than it will after vesicle fusion. The time course to charge the membrane to a potential and the resistance of the membrane is measured and with these values the capacitance of the membrane can be calculated by the equation Tau/Resistance=Capacitance. With this technique researchers can measure synaptic vesicle release directly by measuring increases in the membrane capacitance of the presynaptic terminal.\n- Chemical synapse\n- Neurotransmitter vesicle\n- Vesicle fusion\n- Paired Pulse Facilitation\n- Craig C. Garner and Kang Shen. Structure and Function of Vertebrate and Invertebrate Active Zones. Structure and Functional Organization of the Synapse. Ed: Johannes Hell and Michael Ehlers. Springer, 2008.\n- R. Grace Zhai and Hugo J. Bellen. The Architecture of the Active Zone in the Presynaptic Nerve Terminal. Physiology 19:262-270, 2004.\n- Phillips GR et al. The presynaptic particle web: ultrastructure, composition, dissolution, and reconstitution. Neuron 32: 63–77, 2001\n- Mark L. Harlow et al. The architecture of active zone material at the frog\'s. neuromuscular junction. NATURE, VOL 409, 25 JANUARY 2001\n- Siksou et al. Three-Dimensional Architecture of Presynaptic Terminal Cytomatrix. The Journal of Neuroscience, June 27, 2007 • 27(26):6868–6877\n- Ziv and Garner. CELLULAR AND MOLECULARMECHANISMS OF PRESYNAPTIC ASSEMBLY. VOLUME 5, MAY 2004, 385 - 399.\n- Georgiev, Danko D .; James F . Glazebrook (2007). ""Subneuronal processing of information by solitary waves and stochastic processes"". In Lyshevski, Sergey Edward. Nano and Molecular Electronics Handbook. Nano and Microengineering Series. CRC Press. pp. 17–1–17–41. ISBN 978-0-8493-8528-5.\n- Heidelberger et al.(1994) Calcium dependence of the rate of exocytosis in a synaptic terminal. Nature. Vol. 371. 513-515\n- Stanley EF Single calcium channels and acetylcholine release at a presynaptic nerve terminal. Neuron 11:1007 (1993)\n- Atasoy and Kavalali. Neurotransmitter Release Machinery: Components of the Neuronal SNARE Complex and Their Function. Structural and Functional Orgnanization of the Synapse Hell and Ehlers (eds.) 2008\n- Z. Pang and T. Sudhof. Cell biology of Ca2+-triggered exocytosisCurrent Opinion in Cell Biology. Volume 22, Issue 4, August 2010, Pages 496-505\n- C. Carr and M. Munson. Tag team action at the synapse. EMBO reports (2007) 8, 834 - 838\n- Nadja Jung and Volker Haucke. Clathrin-Mediated Endocytosis at Synapses. Volume 8, Issue 9, pages 1129–1136, September 2007\n- Cesca et al. (2010) The synapsins: Key actors of synapse function and plasticity. Progress in Neurobiology. Vol. 91. 313-348.\n- Dergai et al. Intersectin 1 forms complexes with SGIP1 and Reps1 in clathrin-coated pits. Biochemical and Biophysical Research Communications. Volume 402, Issue 2, 12 November 2010, Pages 408-413\n- Marie et al. Dap160/Intersectin Scaffolds the Periactive Zone to Achieve High-Fidelity Endocytosis and Normal Synaptic Growth. Neuron. Volume 43, Issue 2, 22 July 2004, Pages 207-219\n- George Zanazzi & Gary Matthews. The Molecular Architecture of Ribbon Presynaptic Terminals.Mol Neurobiol (2009) 39:130-148\n- Gersdorff H. and Matthews G. (1994) Dynamics of synaptic vesicle fusion and membrane retrieval in synaptic terminals. Nature. Vol 367. 735-739']"	['<urn:uuid:1a788c16-88fb-455a-8ea8-879d2331bedf>', '<urn:uuid:f439abdd-32e8-4517-800a-f86235014da3>']	factoid	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T18:05:20.075489	29	68	3010
53	How do communication skills help professionals in both marketing and temperature monitoring industries serve their customers effectively?	In marketing, communication skills are crucial for understanding complex problems, conducting market research, and engaging with customers to stimulate demand. Similarly, in the temperature monitoring field, clear communication is essential for helping healthcare workers, medical staff, and patients understand visual indicators that show whether products are within safe temperature parameters. Both fields require professionals to effectively convey technical information - marketers need to understand and explain complex business aspects, while temperature monitoring experts must communicate how their indicators work across various applications from pharmaceutical to food industries.	['We were lucky enough to be joined by Cliff Ramsdell, PhD, Product Development Manager for Flow Cytometry in North America for Thermo Fisher Scientific. Cliff spoke about careers in commercial sciences as a often overlooked but richly rewarding way to use your analytical and research skills!\nHere are some highlights from the seminar:\n- Incorrect misnomer that leaving academic science leads to job dissatisfaction. This is not the case!!\n- Job descriptions are the “ideal” candidate and often do not represent the person that ultimately gets hired. Apply for jobs even if you don’t hit all the requirements! You don’t necessarily know the minimums until you apply for a job.\n- Networking ultimately nets you the job: its the networking you do NOW that can help you land your future position. Cliffs’ first job outside of industry was through a friend from graduate school who had changed jobs.\n- Sales provide scientists with solutions to their problems.\n- Product Managers: liaise with scientists and create new products from the suggestions from the scientific community\n- Market development manager: help connect researchers with what they may need to make their experiments more efficient.\nThings to consider:\n- Get really competent in one application area, and this can help you with conversations on the business side.\n- Build your network!\n- Get out of the lab, build relationships and friendships!\n- Have colleagues in other departments (help with your research and your career!)\n- Participate in activities, inside and outside work!\n- Talk to sales reps and industry contacts.\n- Collaborate and communicate with researchers outside of your field!\n- Attend conferences and external courses (sometimes you want to invest in yourself).\n- Work on your communication skills (both written and verbal)\n- Know your strengths and weaknesses\n- Leverage your strengths\n- Work on your weak areas\n- Have some direction – get insight on what you may be interested in.\n- Honest self awareness – what do you really value? What do you really want?\n- Develop options – don’t get pigeon-holed, try to expand your options no matter what you do.\nInitial job opportunities outside academia:\n- Field Application Specialist\n- Hybrid position of a narrow focus area.\n- Do a lot of demos, product education.\n- They are knowledgeable in all facets of the technical information.\n- Scientific talks on applications.\n- They don’t really sell, although they collaborate closely with sales managers.\n- Tend to travel a lot depends on the territory (although Boston is relatively dense).\n- Different flavors: Product development (develop new products), application scientist (generate experimental data to back up products) and often communicate to scientists.\n- Tech Support\n- If you are not as comfortable with face to face contact but great training\n- Can be good hours\n- Less structured but very flexible\n- Have goals and benchmarks to hit quarterly\n- Application Specialist\n- “Super” FAS.\n- Very skilled scientists as resources to solve problems\nwith additional certification:\n- Patent law/technology transfer – need scientists who understand the science to be able to patent the process.\nWhat you should consider when you are looking for a new job:\n- Is it a challenge? Doesn’t want to be too easy or too difficult\n- Balanced life style (different for everyone!)\n- Sense of purpose. Leaving the bench DOES NOT mean that you are leaving this behind!!\n- Knowing the options\nWhat is marketing?\nIt’s unlikely to leave academia and go straight in, but it is achievable in 3-5 years\n- Advertising is only part of it.\n- A marketing professional stimulates demand for supply in the market place and engages all parties involved.\n- Marketing is used to identify the customer, to satisfy the customer and keep the customer.\n- It’s about understanding the business!\n- How the company brands themselves, what they focus on.\n- Great book to read about marketing: Made to stick by Chip Heath and Dan Heath\n- Marketing in life sciences:\n- No matter what: Need to be passionate about what you do!\nWhy a PhD?\nMarketing is about understanding a complex problem. You need:\n- market research\n- psychology, behavioural studies\n- communication skills\nMost important aspects of a PhD:\n- teach yourself to learn\n- scientific process\n- communication skills\n- challenge of the unknown: test, fail and redirect\nHow to get in:\nWhat are the 3 most important skills needed to excel in the field:\n- Communication, subject matter knowledge and humility\nWhat is the general career trajectory:\nGenerally you start as sales or field application specialists (salary is ~ 6 figures) and can move up to other careers\nIs being an international academic (J-1), a crux in entering this field?\nMostly yes. Getting sponsorship is costly and expensive\nHow applicable would these opportunities be to an MD?\nDepends on training and career goals\nAnd remember: You are AWESOME! You are in an amazing place with amazing research going on. DEVOUR everything! All of the science, watch on a broad scale everything that is going on in Boston and continue to be engaged. What differentiates you from other people, perhaps more experienced is that you are hungry! Every time you go into an interview, portray that! Go GET YOUR MONEY! You are worth a lot of money!\n(direct quote from Adam, Account Manager for Thermo Fisher!).\nI hope you all enjoyed this fantastic seminar on an often overlooked non-academic career!', 'PRECISE TEMPERATURE MONITORING\nThe simple to use indicators offer precise monitoring of time since activation or length of a temperature breach.\nTimestrip indicators are activated by a simple squeeze of button. The modern design allows users to gather data with a glance. Adhesive backing on the small indicators make it simple to attach to the product, shipment or item to be monitored.\nTimestrip irreversible temperature indicators monitor the length of temperature from -20°C to 38°C. There are three specialty lines of temperature monitoring labels that have been created for the Food, Pharma and Blood Industries:\n- Timestrip Plus, Monitoring a range of ascending temperature thresholds from -20°C to 38 °C for cold chain monitoring in food, pharma, healthcare and life sciences industries.\n- Timestrip Food Temp indicators monitor food for temperatures from 3°C - 10°C.\n- Timestrip Blood Temp indicators monitor the temperature of blood products during transport and storage. The Blood Temp 10 is a FDA 510(k) approved medical device for 10°C monitoring of blood bag transportation.\n- TIR (temperature and report indicators) and Timestrip Complete TC489 and TC490 indicators, electronic temperature indicators, single-use, able to monitor both ascending and descending temperatures. They are ideally suited for monitoring pharma, vaccines and products that must respect the cold chain.\nMain field of application for Timestrip indicators:\n- Food and Seafood - Time-Temperature indicators (TTIs) that have been specifically developed for food shipping and storage.\n- Life Sciences - Temperature indicators curated for the life science industry.\n- Pharma - Time and Temperature indicators for the pharmaceutical industry.\n- Pharma and clinical studies – TIR temperature indicators for continuous temperature monitoring.\n- Healthcare - Temperature indicators for the healthcare and blood product industries.\nFOOD & SEAFOOD\nTimestrip Food Temperature Indicators are a simple solution for food safety compliance and cold chain temperature monitoring. The user friendly and cost effective indicators are used worldwide by a number of leading brands in food services and shipping.\nFood and Seafood temperature indicators are perfect for seafood, shellfish and food shipping with monitoring ascending temperatures 3°C, 5°C, 8°C & 10°C.\nIn particular Timestrip 3°C, developed following the FDA regulations, can be used for proof of temperature breach in reduced oxygen packaging shipments.\nTimestrip Plus temperature indicators are used in the following industries: life science, pharma, healthcare e food. Indicators dimension is 19 mm x 40 mm and they all have activation blister on top.\nIn Life Sciences, Timestrip indicators application is in particular in pharma, biotechnology, biomedical device production industries and healthcare services. In this last field, specifically, Timestrip indicators indicators are a simple solution to help healthcare workers, since they offer invaluable visual aids to medical staff and patients.\nIt is a range of products for the medical and pharmaceutical sectors, offering invaluable visual aids to doctors, patients and healthcare workers. Timestrip indicators clearly indicate whether a product is within its date and/or temperature parameters and therefore acceptable to use.\nMoreover they visually show how long a temperature controlled drug shipment was exposed to unsafe temperatures or how much time is left before a pharma product is out of date.\nIn addition to the Timestrip Plus range previously shown, in the pharma industry it is also often used Timestrip Complete Card that simultaneously monitors ascending and descending breaches on a\nConvenient carrying card.\nPerfect for 2 -8 °C cold chain monitoring.\nPHARMA AND CLINICAL STUDIES\nThe Timestrip TIR indicators 0°C and 2-8ºC are devices for continuous temperature monitoring and they provide an alarm if this is exceeded, so as to prevent any damages to the products that are being controlled or, at least, to be aware that they may have been damaged.\nThe Timestrip TIR 0°C and 2-8ºC are thin and small enough to go on clinical kits and sales units of pharmaceuticals. They are durable enough to stay on the product, as the battery lasts 18 months after activation. Timestrip TIR 0°C and 2-8ºC manage deviations for up to 6 alarm levels. The alarm status is always visible and can be downloaded wirelessly as a PDF with a convenient Smartphone App.\nA PDF file is generated by automatically establishing an evaluation report with embedded data.\nThese indicators are cost effective, easy to use and they can be used without problems by third parties. These devices are perfectly calibrated and produced complying with the ISO standard 19005-1 for the long-term preservation of electronic documents.\nTimestrip Complete TC489 and TC490 indicators\nThe new Timestrip TC489 and TC490 electronic indicators monitor temperature ranges from 2 to 8°C and use three LEDs to signal clearly and unequivocally if the temperature falls below 2°C or rises above 8°C.\nMoreover, the TC490 indicators feature NFC technology, which allows the user to view and download data very simply, using the scanner of a smartphone.\nThese temperature indicators are ideal for the monitoring of pharmaceuticals, vaccines, blood, diagnostic kits, home deliveries and food products.\nHealthcare industry necessities are more or less the same as those we find in the pharma field. Here the main need is to monitor blood products, since their temperature exposure cannot go over 6°C and 10°C.\nTimestrip has developed a special range dedicated to this application with Blood temp 6 and Blood temp 10 indicator (this one FDA approved).']	['<urn:uuid:3353cc16-1adb-4379-9528-7204a58d728b>', '<urn:uuid:1258adcb-1794-4107-ab58-2942b215157d>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T18:05:20.075489	17	87	1778
54	what is stone breaking machine basics	A jaw crusher is a type of crusher that breaks materials in a crushing chamber. It works by using compression, with two plates that open and shut like animal jaws. The material enters through the top opening and exits through the bottom opening when crushed. The size of a jaw crusher is designated by the rectangular or square opening at the top of the jaws.	['Index Terms - Bottle Crusher, Can Crusher, Crusher, Crusher Design, Jaw Crusher I. INTRODUCTION Plastic recycling is the process of recovering scrap or waste plastic and reprocessing the material into useful products. Since plastic is non-biodegradable, Concept Development We have come up with a concept of designing a crusher in such a way.Concepts of jaw crusher in iron ore industries Products. As a leading global manufacturer of crushing, grinding and mining equipments, we offer advanced, reasonable solutions for any size-reduction requirements including, Concepts of jaw crusher in iron ore industries, quarry, aggregate, and different kinds of minerals.\nJaw crusher design jaw crusher type. jaw crusher designed specifically for crushing iron ore minerals. liming can supply you PE series iron ore jaw crusher and JCE type A New Design Concept For A Continuous Double-toggle Jaw.The right jaw design can make a big difference for your jaw crusher. Our customers have proven that the right jaw plate design can make a dramatic difference, depending on the application. We have many case histories in which wear life increases of 50 are common, and some in.\nDec 09, 2014 • Types Of Crushers • Jaw crushers • Gyratory crushers • Difference • Difference between these jaw crushers is the feed rate and product rate and design, we are, however, going to talk about jaw crushers in this particular presentation. 5. • Jaw Crusher • Jaw crusher is a type of crusher which produces coarse particle.Jan 01, 1990 New ideas in primary Jaw Crusher design — Design and manufacture of 66-inch 84-inch (167.64-cm 213.36-cm) Jaw Crusher utilizing open feed throat concept, power savings and automation features.\nJaw crusher is one of the main t ypes of primary crushers in a mine or ore processing plant. The size of a jaw crusher is designated by the rectangular or square opening at the top of the jaws.A theoretical design concept of a continuous Blake type jaw crusher has been elucidated in this article, where forward and return strokes of the moving jaw can both be utilised for crushing. It may be presumed that the design concept as presented here if put into developing a continuous type of crusher, may deliver nearly twice the output of.\nWorking principles of jaw crusher pdf. The working principle of jaw crusher would help you to understand the . The News Pumice Cone Crushing Machine . pumice crusher machine design pdf. Jaw Crusher Working Principle 911 Metallurgist. Jul 2, 2018 A sectional view of the single-toggle type of jaw crusher is shown below.Jun 07, 2020 The jaw crusher is designed by using the jaw crusher plates, which is used to break the different type’s stones into small pieces because these plates have done most of the work in the system. Jaw crusher plates are playing an important role in the jaw crusher. This curved jaw design has a high production capacity.\nJaw crushers These are the oldest type of and the most commonly used crushers in use and have changed little from the original design. In Jaw Crusher the feed is compressed between a stationary and a movable surface. A few of common types of Jaw crushers, in use, are described below • Double toggle jaw crusher • Single toggle jaw crusher.Jun 26, 2020 Design Of Crusher Foundation Fact Jeugd Noord. Jaw crusher concrete foundation design jaw crusher concrete foundation designoundation jaw crusher youtubepr 10 2017 details contactp jaw crushers lay foundation for mining recently the professional r d rusher wikipedia crusher is a machine designed to reduce large rocks into smaller rocks gravel or rock dust in a mobile.\nJaw Crushers With Types And Differences. jaw crushers with types and differences. july 20, 2021 by ames. jaw crushers are used as primary crushers, or the first step in the process of reducing rock. they crush primarily by using compression. the distinctive feature of this class of crusher is the two plates which open and shut like animal jaws. the jaws are set at an acute angle to.Design Single-toggle jaw crushers are characterized by the swing jaw being suspended directly on the eccentric drive shaft and the lower part of the swing jaw being braced against the crusher frame by means of a toggle plate. The kinematics of this type of toggle.\nDec 25, 2014 The working principle of the jaw crushers is very simple. Powered by a diesel or gas motor, the jaw crusher brakes materials in a crushing chamber. The materials are pushed inside the chamber from the top opening and when crushed they are released through the bottom opening. The crushing power of the jaw crushers depends on the size of the chamber.Jaw Crushers are sized by the top opening of the crushing chamber. For example, a 32 x 54 Jaw Crusher measures 32 from jaw die to jaw die at the top opening or gape opening and 54” across the width of the two jaw dies. The narrower bottom opening of the crushing chamber is used to size the discharge material.\nCone Crushers. A Cone Crusher is a compression type of machine that reduces material by squeezing or compressing the feed material between a moving piece of steel and a stationary piece of steel. Final sizing and reduction is determined by the closed side setting or the gap between the two crushing members at the lowest point.Static Jaw Crusher. Rugged, field proven designs and almost 200 years of combined jaw crusher manufacturing experience provide the foundation for our Terex MPS jaw crusher lines. Industry leading brand names like Cedarapids and Jaques provide the platform from which other machines are measured. With rated capacities from 50 tph to well over.\nJaw crusher movement is guid-ed by pivoting one end of the swinging jaw. and an eccentric motion located at the opposite end . 1.3 Different Types of Jaw Crusher . According to the amplitude of motion of the moving face Jaw crusher are classified as follows - 1.3.1 Blake Type Jaw Crusher . Blake type jaw crusher, primary crushers in the mineral.Jaw crusher movement is guided by pivoting one end of the swinging jaw. and an eccentric motion located at the opposite end.  The size of a jaw crusher is designated by the rectangular or square opening at the top of the jaws .For instance, a 22 x 30 jaw crusher has an opening of 22 by 30 , a 46 x 46 jaw crusher has a opening of 46 square.\nGroup, has installed over 10 000 jaw crushers since the 1920s. Today the Nordberg C Series is indisputably the world’s favourite jaw crusher. Where cost-efficient primary reduc-tion of hard, abrasive materials is concerned, C Series crushers represent the highest tech-nical and manufacturing knowledge. All C Series jaw crushers are based on a.Sep 14, 2018 Continuous Jaw Crushers A new Design Concept for a Double-toggle System (A.K. Muk If this is your first visit, be sure to check out the FAQ by clicking the link above. You may have to register before you can post click the register link above to proceed.\nHammer crusher jaw crusher impact crusher manufacture in China. The main products are Jaw Crusher, Impact Crusher, Heavy Hammer Crusher, Vibrating . Our unique design concepts ensure our products the perfect balance among .Bench scale and pilot scale design for comminution circuits Factors influencing the selection of comminution circuits Types and characterisation of crusher equipment and circuit flowsheet Selection and sizing of primary crusher Computer aided design of Jaw Crusher Selection and sizing of secondary and tertiary crushers Optimising the Eccentric speed of cone crusher Selection and sizing of High.']	['<urn:uuid:34044b15-2d6a-4917-99e1-70f883733e27>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T18:05:20.075489	6	65	1263
55	migrating exchange on premise to azure cloud security differences between hybrid deployment and direct migration	Both hybrid deployment and direct migration to Azure cloud have distinct security characteristics. In a hybrid deployment, organizations can maintain data security by keeping sensitive data on their in-house server hardware while simultaneously benefiting from Office 365's features. The hybrid approach also allows centralized administration through Exchange Admin Center. For direct Azure cloud migration, security is managed through multiple tools like Azure Active Directory for identity management, Azure Key Vault for secure storage of encrypted keys and certificates, and Azure Security Center for unified security management and advanced threat protection. The choice depends on specific security needs and requirements.	"[""Microsoft Exchange is a robust email and calendaring server widely used by small businesses, medium organizations, and large enterprises across all folds. Most organizations run an on-premises Exchange Server version that requires setting up and managing the server at their office location. However, with increasing risks of attacks on on-premises Exchange Servers, organizations are steadily moving towards the cloud-based Office 365.\nOffice 365 is a secure and scalable Exchange Online solution that relieves organizations from setting up or managing their on-site server hardware.\nOrganizations may also opt for a Hybrid Exchange Deployment, which integrates on-premises Exchange with Office 365—providing a feature-rich user experience by offering the best of both worlds. In Hybrid deployments, organizations can keep data on their in-house server hardware while reaping the benefits of Office 365.\nThe hybrid deployment also serves as an intermediate step to migrate an on-premises Exchange organization to Microsoft 365 or Office 365.\nIn this article, we will explain how you can set up a Hybrid Exchange Office 365 environment and move mailboxes from your on-premises Exchange Server to Office 365 or Exchange Online.\nSteps to Setup a Hybrid Exchange Office 365 Environment\nBefore setting up a hybrid Exchange Office 365 environment, update your on-premises Exchange to the supported version of Exchange Server.\nTo set up a hybrid Exchange deployment, you need to integrate the local Active Directory data with Office 365 environment. Microsoft provides Azure AD Connect to synchronize data from Active Directory on on-premises Exchange with Azure AD on Office 365.\nNext, you need to configure the coexistence between on-premises Exchange and Exchange Online using the Hybrid Configuration wizard and then move the mailboxes.\nThe detailed steps are as follow,\nStep 1: Synchronize Active Directory\nYou or may not synchronize the Active Directory based on your user management and authentication preference. You can choose between any of the following directory synchronization relationships,\nAlso Read | How To Hire Freelance Web Design Agency?\n● Manage users in the on-premises AD and authenticate at on-premises AD without Azure directory synchronization (Federated))\n● Manage users in the on-premises AD and authenticate at on-premises AD with Azure directory synchronization (Password hash sync)\n● Manage users in the on-premises AD and authenticate at Azure AD (Pass-through)—requires directory synchronization\nTo synchronize the local Active Directory with Azure AD, follow these steps,\n● On your on-premises Exchange Server, download and run the Microsoft IdFix tool. The tool scans your local Active Directory for objects and attributes that may lead to issues during directory synchronization.\n● Then download the Microsoft Azure AD Directory Connect (AAD Connect) tool and run it on your on-premises Exchange Server. Click ‘Continue> Use express settings’\n● Enter Azure AD Credentials (Office 365 tenant credentials) and click ‘Next.’\n● Enter your on-premises administrator credentials in Connect to AD DS and click ‘Next.’\n● Check ‘Start the synchronization process when configuration completes’ and check ‘Exchange hybrid deployment’ options. Then click ‘Install.’\nThis will install AAD connect on your on-premises Exchange Server and synchronize the local AD to Azure AD. After the synchronization, go to https://portal.office.com/ and log in using any user account to verify it works.\nStep 2: Configure Hybrid Deployment\nTo configure hybrid Exchange office 365 deployment, login to Exchange Admin Center (EAC) as administrator and then follow these steps,\nAlso Read | Cost To Build An E-Commerce Marketplace\n● In EAC, go to ‘hybrid’ and click ‘enable’ or ‘configure.’\n● Click ‘sign in to Office 365’ and then log in to Office 365 tenant as Global Administrator.\n● After login, the interface redirects you to download the Hybrid Configuration Wizard tool. Click on ‘Click Here to download the tool.\n● Click ‘Install’ and follow the prompts to launch the Hybrid Configuration Wizard.\n● In the Hybrid Configuration Wizard window, click ‘Next.’\n● The Hybrid Configuration Wizard (HCW) will auto-detect your on-premises Exchange Server. Click ‘next.’\n● Click ‘sign in’ to login as Global Administrator and add Office 365 tenant. Click ‘next’.\n● Once your credentials are validated, click ‘next.’\n● Choose ‘Full Hybrid’ and click ‘next.’\n● Select the topology—Classic or Modern Hybrid and click ‘next.’\n● Click ‘enter…’ and provide Office 365 Global Administrator credentials. Click ‘next’\n● The HCW will install, register and validate the Hybrid Agent. Click ‘next.’\n● Choose the ‘Configure my client access….(typical)’ option if you use the client mailbox and mailbox server. Otherwise, choose ‘Configure my Edge Transport servers…’ and click ‘next.’\n● Choose your Exchange Server and click ‘next>next.’\n● Enter FQDN and click ‘next.’\n● In the Ready to Update window, click ‘update.’\nThis will configure and enable Hybrid deployment for your organization.\nAlso Read | AI In Software Testing: A Detailed Explantion\nStep 3: Move Mailboxes\nBefore moving the mailboxes, assign licenses to the user on Office 365. You can use Remote Move, Staged, or Cutover migration to move mailboxes from your on-premises to Office 365 tenant. Staged Migration is available for Exchange 2003 and 2007 only, while Cutover supports Exchange 2003 and later versions. The remote Move option is also available for Exchange 2010 and later versions.\nThe steps are as follows,\n● In EAC, go to recipients> migration.\n● Click the ‘+’ icon and choose ‘Migrate to Exchange Online.’\n● Choose the migration option from the list. We choose ‘Remote Move…’ and click ‘Next.’\n● Click the ‘+’ icon to add mailboxes you want to move to Office 365 and click ‘Next.’\n● Select the migration endpoint and click ‘Next.’\n● Enter the FQDN and click ‘Next.’\n● Enter batch name and choose the ‘Move primary…’ option. Click ‘Next> Finish.’\nTo check the move status, connect to Office 365 tenant using Exchange Management Shell. The commands are as follow,\nConnect-ExchangeOnline –Credentials $Office365 –ShowProgress $true\nGet-MoveRequest –Identity ‘[email protected]’\nGet-MoveRequestStatistics -MoveRequestQueue “MBXDB02”\nThis will show you if the mailbox(s) is synched to Office 365 tenant. To verify, you can go to ‘recipients>mailboxes’ in EAC and refresh the mailboxes. The Mailbox Type should change to Office 365.\nRepeat the steps to move all mailboxes. You may also use the PST upload method to export mailboxes from your on-premises Exchange to PSTs and then import them to Office 365 via the network upload method.\nHowever, this is a time-consuming process and may need a few hours to several days based on the number of mailboxes you need to migrate from on-premises Exchange to Office 365. Besides, you have no control over what data to migrate or filter.\nUsing an EDB converter tool, such as Stellar Converter for EDB, you can overcome these limitations and move all mailboxes at once from your on-premises Exchange database to Office 365 tenant directly while saving a significant amount of effort and time.\nThe software provides a much faster, reliable, and secure way to move mailboxes from your Exchange database to Office 365. It automatically maps the source and destination mailboxes—making your job easier. It also exports up to four mailboxes simultaneously to the target Office 365 tenant—speeding the migration process by 4x. You can download the free demo version to get started.\nTo Wrap Up\nSetting up a Hybrid Exchange Office 365 server is a recommended approach whether you want to move the entire Exchange to Office 365 or experience the best of the two worlds. With greater flexibility and scalability, Office 365 adds to the peace of mind while Exchange on-premises add to the data security as you can choose to keep your data in-house.\nAfter a hybrid deployment, if you wish to migrate on-premises completely to Office 365 eventually, you should stop publishing Exchange on-premises to the internet after the migration is complete. This is an important housekeeping step often overlooked by administrators. However, if you are facing issues, have any questions, or need more help, leave a comment below and the Codersera team will be there to help you out.\n- What is a hybrid Exchange environment?\nIn a hybrid exchange environment, an organization is able to connect its on-premises Microsoft Exchange users and cloud users into a single system. There are both advantages and disadvantages to creating an Exchange hybrid environment. Whether it's right for an organization depends on a number of unique factors.\n- What is the purpose of hybrid Exchange?\nAn on-premises Exchange Hybrid Server provides a centralized administration point. This will allow administrators to access a single Exchange Admin Center from their on-premises environment to access both the on-premises and Exchange Online Admin Center from a single browser."", ""How to secure your Azure cloud\nIn our previous post we recalled and emphasized a number of important cloud security challenges, reported by the Cloud Security Alliance (CSA) in 2019. Reported security threats are related to, for example; Identity and Access Management, data identification and classification, threat and vulnerability management…\nWhen adopting a cloud solution, you should understand and leverage the numerous tools that are available to secure your cloud services.\nFor example, the Microsoft Azure Cloud Platform offers multiple capabilities in order to manage cloud security risks and to secure your Azure cloud environment.\nIn this article we will discuss a number of solutions provided by Microsoft Azure that can help administrators to perform their security tasks in an effective and efficient way.\nAzure Cloud Solutions\nMost importantly, the Azure cloud solutions enable you to apply a more streamlined approach to manage security tasks automatically and centralized.\nBelow, we will discuss following solutions in more detail:\n- Azure Active Directory (AD), MFA and SSO\n- Azure Role-Based Access Control (RBAC)\n- Azure Vault\n- Azure Conditional Access\n- Azure Information Protection (AIP)\n- Azure Data Discovery and Classification\n- AIP policy\n- Azure Privileged Identity Management (PIM)\n- Azure Security Center\nMicrosoft Azure provides several licensing possibilities, each of them with different features. Therefore, the Azure solutions discussed below might not be included in your specific license. Details can be verified in the following location: https://azure.microsoft.com/nl-nl/pricing/#product-pricing\n1. Identity, Credentials and Access Management\n1.1. Azure Active Directory and SSO\nAzure Active Directory (Azure AD) is Microsoft’s cloud-based identity and access management service. Note: Azure AD is not the same as Active Directory Domain Services. If you would like to know more: Microsoft published an article explaining the differences, see Compare Active Directory to Azure Active Directory.\nAzure AD enables users to sign in and access both external resources (e.g. Office 365, The Azure portal, and numerous SaaS applications) and internal resources (e.g. self-developed cloud apps or apps on your corporate network).\nWhen using Azure AD with single sign on (SSO), a user must no longer remember application-specific passwords to sign in to each application. SSO enables users to sign in only once with their account to access SaaS applications, web applications, company resources etc. Moreover, IT staff can centralize user account management, and automatically add or remove user access based on group membership. Beware that your applications are as safe as your weakest protected device; an unprotected laptop gives readily access to all your resources.\n1.2. Multi Factor Authentication (MFA)\nApplying MFA is critical to mitigate the risk of user accounts becoming compromised. MFA is an authentication method which relies on the user’s normal credentials and at least one other item of information. For example: something the user knows (password) combined with something the user has (mobile phone). If the username and password are stolen, hackers cannot use the credentials without also stealing the second authentication.\nBy combining MFA and SSO, perimeter security increases while the authentication process is simplified (i.e. by enabling a single, centrally managed IAM solution).\n2. Azure Role-Based Access Control (RBAC)\nBasically, role-based access control (RBAC) restricts access based on a person's role within an organization. The roles refer to the levels of access that employees have. “Much as with a traditional Active Directory, user account permissions should be configured using such a role-based approach in order to provide users the least amount of privileges required to perform their job tasks” .\nAzure RBAC helps you to manage who has access to your Azure cloud resources, what they can do with those resources, and what areas they have access to. This is done by creating role assignments. A role assignment consists of three elements: security principal, role definition, and scope.\n- Security Principal: “A security principal is an object that represents a user, group, service principal, or managed identity that is requesting access to Azure resources” \n- Role definition: “A role definition is a collection of permissions. A role definition lists the operations that can be performed, such as read, write, and delete. Roles can be high-level, like owner, or specific, like virtual machine reader” \n- Scope: “Scope is the set of resources that the access applies to. When you assign a role, you can further limit the actions allowed by defining a scope. In Azure, you can specify a scope at multiple levels; management group, subscription, resource group, or resource. Scopes are structured in a parent-child relationship” \nMore details can be consulted via: https://docs.microsoft.com/en-us/azure/role-based-access-control/overview.\nIn addition, Microsoft has published several blueprints that can help you configuring user account permissions. https://docs.microsoft.com/en-us/azure/governance/blueprints/samples/\n3. Azure Conditional Access\nThe change to a more cloud-centric model results in a security perimeter that extends beyond an organization's network to include user and device identity. As a consequence, controlling the access to your corporate resources is more challenging.\nConditional Access Policies can be applied as: IF a user wants to access a resource, THEN a certain condition should be met first. When making a policy decision, conditional access can take into account a number of signals, such as;\n- The user’s roles and the groups he or she belongs to\n- The IP address of the user or the geographic location of the user attempting to log in\n- The application\n- Users attempting to access specific applications can trigger different Conditional Access policies\nUsing these attributes, administrators can use Conditional Access to adjust security by  :\n- Restricting users from logging in from foreign countries or restricting access to specific sensitive information when logging in from locations other than their offices\n- Forcing users to use MFA when logging in on IP addresses external to the organization\n- Requiring users to use compliant devices when attempting to access organizational resources\n4. Azure Information Protection (AIP)\nOrganizations might be challenged with detecting their data and classifying this data according to their policies and procedures. Some useful tools to implement data classification are :\n4.1. Azure Data Discovery and Classification\n“This tool can help you to discover, classify, label and report sensitive data stored within Azure’s SQL databases” . Moreover, Azure SQL auditing enables the possibility to generate dashboarding and reports to have a view on what data is stored within the databases and who is accessing this data.\n4.2. AIP policy\nAzure Information Protection Policies can be developed to assist your organization in protecting documents and emails based on specific conditions. Documents and emails can be labeled (labels apply a certain classification value) based on triggers, for example words or phrases that are used when writing a document or email.\n5. Key Vault\nAzure Key Vault is a tool to securely store and manage your encrypted keys, passwords or certificates. This way, access to your key vaults is only allowed by authorized applications and users in order to protect your data.\nIn summary, Azure Key Vault helps solve the following problems :\n- Secrets Management – “Securely store and control access to tokens, passwords, certificates, API keys, and other secrets”\n- Key Management – “Azure Key Vault facilitates the creation and controlling of the encryption keys used to encrypt your data”.\n- Certificate Management – “Azure Key Vault is also a service that lets you easily provision, manage, and deploy public and private Transport Layer Security/Secure Sockets Layer (TLS/SSL) certificates for use with Azure and your internal connected resources”.\n- Store secrets backed by Hardware Security Modules – “The secrets and keys can be protected either by software or FIPS 140-2 Level 2 validated HSMs”\n6. Azure Privileged Identity Management (PIM)\nPrivileged Identity Management enables you to manage the assigned privileged roles throughout your organization and to gain insights about the activities of these privileged roles. “Users assigned to a PIM protected role, must elevate to use the granted privileged access rights, for example; perform MFA, obtain approval or provide a reason of activation. In addition, the tool enables to track these elevations via notifications and the audit event logs” .\n7. Azure Security Center\nAzure Security Center unifies security management and enables advanced threat protection for workloads in the cloud and on-premises. “Not only does the Security Center offer recommendations throughout the Azure portal in order to further secure the cloud, but individual resources can be integrated with the Security Center to allow for health checks, patch management, and security alerts”  .\nSecurity Center provides you with the tools to :\n- Strengthen your security posture: “Security Center will assess your environment and enables you to understand the status of your resources, and whether they are secure”.\n- Protect against threats: “Security Center assesses your workloads and raises threat prevention recommendations and security alerts”.\n- Get secure faster: “Deployment of Security Center is easy, providing you with auto-provisioning and protection with Azure services”.\nWhen security policies are being broken, the Security Center will notify administrators and provide recommended solutions to resolve the incident. Moreover, administrators are able to streamline the incident response process by developing rules to automatically apply efforts to resolve known or common issues.\nThe Security Center provides you with a dashboard of organization databases. Identified critical vulnerabilities will be notified with recommendations to resolve the incident.\nMany more Microsoft security tools are and might become available for cloud administrators. Organizations are moving more and more of their activities and data to a cloud environment, therefore it is important to get to know the tools that are available to mitigate the security risks to secure your cloud services.\nWhile these tooling’s can increase an organization’s security posture, it is still critical that management continue to perform traditional responsibilities, such as user access reviews and periodic review of role matrices, to verify that excessive access is not provisioned to users.\n Microsoft Azure, https://docs.microsoft.com/en-us/azure/?product=featured\n Crowe LLP, Zak Thoreson, 2020; https://www.crowe.com/cybersecurity-watch/securing-the-azure-cloud-dgs?utm_source=linkedin&utm_medium=social&utm_campaign=mo2114-002a\nFor more information about securing your cloud environment, please contact:\nMicrosoft, Windows, and Azure are either registered trademarks or trademarks of Microsoft Corp.""]"	['<urn:uuid:98dd6d3c-def0-4ec4-8c9d-3de8d0a1680f>', '<urn:uuid:cb8bce81-713b-4c1b-9ab7-06b039ec87f2>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	expert	2025-05-12T18:05:20.075489	15	99	3057
56	compare hypertonic saline use bronchiolitis hyponatremia conditions treatment	Hypertonic saline is used differently in bronchiolitis versus hyponatremia. For bronchiolitis in infants, it is administered through nebulization every 4 hours mixed with epinephrine, and treatment continues until the patient is ready for discharge. In contrast, for hyponatremia, 3% hypertonic saline is given intravenously, particularly in severe cases to prevent cerebral edema. Additionally, while bronchiolitis treatments include supplementary oxygen and hydration at physician discretion, hyponatremia treatment specifically warns against providing normal saline solution or fluids, and instead recommends fluid restriction in mild cases.	"['Hyponatremia is a medical condition termed for a low concentration of sodium in the blood (serum). By definition, hyponatremia occurs when serum sodium levels in the plasma fall below <135mEq/L. This has been shown to occur in up to 30% of ultra-endurance participants. Hyponatremia is mainly caused by overhydration, but can also be caused by intake of hypotonic fluid in excess of sweat and urine output, excessive sodium losses, or other hormonal dysfunctions that affect the maintenance of sodium stores in the body. The table below shows the risk factors associated with hyponatremia.\nHow do you prevent hyponatremia?\n- Have a hydration plan in place\n- Supplement water with electrolyte beverages, especially if exercise is lasting longer than 1 hour\n- Universal guidelines are not realistic due to the following factors\n- Variation in individual sweat rate\n- Variation in individual sweat sodium concentration\n- Environmental conditions\n- Record body weight before and after exercise to monitor fluid consumption\n- Know the sweat rate to determine fluid consumption during exercise\n- This also helps establish individual hydration plans\n- Know the signs and symptoms of hyponatremia\n- Have an emergency plan in place for dealing with hyponatremia\n- Monitor the duration and intensity of exercise for determining risk of hyponatremia\n- Educate athletes of risks from fluid overload and encourage moderate hydration.\n- Establish individualized hydration protocol based on personal sweat rate and sports dynamic.\n- Consume adequate dietary sodium.\n- Allow 8-14 days of training in the heat for acclimatization.\n- Identify pre-exercise hyponatremia by recording body weight each day\nWhat puts an individual at risk for hyponatremia?\n|Exercise duration greater than 4 hours or slow pace|\n|Low body weight|\n|Excessive drinking (<1.5 L/hour) during the event|\n|Abundant availability of drinking fluids at event|\n|Nonsteroidal anti-inflammatory drugs|\n|Other drugs associated with SIADH (SSRI’s)|\n|Extreme hot or cold environment|\nLook for these symptoms in athletes when hyponatremia is suspected:\n- Signs and Symptoms vary depending on severity and are related to cerebral edema caused by the osmotic flow of fluid into the brain cells\n- Patients that are asymptomatic or mildly symptomatic can present with any of the following:\n- Weakness, dizziness, headache, nausea, and/or vomiting and the resulting serum sodium levels range from 129-134mEq/L\n- Patients with more severe hyponatremia can present with any of the following:\n- Serum sodium levels less than 129mEq/L, presents with signs and symptoms of seizures, coma and death\nHow do you know if this is hyponatremia?\n- Indication of hyponatremia based on onset of symptoms\n- Type, duration, and intensity of exercise\n- Amount of fluid consumed\n- Post exercise body weight is greater than pre exercise body weight\n- Measurement of blood sodium levels\n- A measure <130mEq/L would indicate moderate-severe hyponatremia and coincide with observation of symptoms\n- Measurement of vitals\nWhat else could this be?\n- Exertional Heat Stroke\n- Heat Exhaustion\n- Heat Cramps\n- Cardiac Condition\n- Exertional Sickling\n- Respiratory Condition\nHow do you treat an individual with hyponatremia?\n- Treatment varies depending on severity of hyponatremia\n- DO NOT provide normal saline solution or fluids\n- Asymptomatic or mildly symptomatic\n- Treated with fluid restriction and observed until either serum sodium levels return to within normal limits or there is a resolution of symptoms and spontaneous diuresis\n- Consume oral hypertonic saline (e.g. bouillon) or salty foods such as potato chips, pickles, jerky\n- Hypertonic saline IV should be considered if a blood sodium level can be measured\n- Severe Hyponatremia\n- 3% hypertonic saline should be administered immediately due to the risk of cerebral edema that can ensue if treatment is delayed\n- It is also recommended that patients presenting with hyponatremia receive supplemental oxygen in case cerebral edema leads to hypoxia· The following flow chart represents when an athlete should be transported to the nearest hospital\nWhen can the individual return to activity?\n- Athlete will need to follow up with his/her primary care physician\n- Blood sodium levels will need to measure within normal limits (>135mEq/L)\n- Return to full activity should follow a graded exercise protocol similar what would be done during a period of exercise/heat acclimatization\n- Athlete will need to be educated on proper hydration before, during and post exercise to avoid the risk of suffering from hyponatremia again.\nRecommended Equipment List\n- Hypertonic saline\n- IV equipment\n- Portable blood Na+ analyzer kit (e.g. i-stat)\n- Salty foods (e.g. bouillon cubes, pretzels, canned soup, and potato chips, pickles)\n- Rectal thermometer (used to rule out exertional heat stroke)\n- Blood pressure cuff\n- Almond CS, Shin AY, Fortescue EB, et al. Hyponatremia among runners in the boston marathon. N Engl J Med. 2005;352(15):1550-1556.\n- Armstrong LE, McDermott BP. Exertional hyponatremia. In: Casa DJ, eds. Preventing Sudden Death in Sport and Physical Activity. Sudbury, MA: Jones & Bartlett Learning. 2012:185-199.\n- Binkley HM, Beckett J, Casa DJ, Kleiner DM, Plummer PE. National Athletic Trainers’ Association position statement: exertional heat illnesses. J Athl Train. 2002;37(3):329-343.\n- Casa DJ, Clarkson PM, Roberts WO. American College of Sports Medicine roundtable on hydration and physical activity: consensus statements. Curr Sports Med Rep. 2005;4:115-127.\n- Casa DJ, Armstrong LE, Hillman SK, Montain SJ, Reiff RV, Rich B, Roberts WO, Stone JA. National Athletic Trainers’ Association position statement: fluid replacement for athletes. J Athl Train. 2000;35(2):212-224.\n- Convertino VA, Armstrong LE, Coyle EF, et al. American college of sports medicine position stand. exercise and fluid replacement. Med Sci Sports Exerc. 1996;28(1):i-vii.\n- Hew-Butler T, Ayus JC, Kipps C, Maughan RJ, Mettler S, Meeuwisse WH, Page AJ, Peid SA, Rehrer NJ, Roberts WO, Rogers IR, Rosner MH, Siegel AJ, Speedy DB, Stuempfle KJ, Verbalis JG, Weschler LB, Wharam PM. Statement of the second international exercise-associated hyponatremia consensus development conference, New Zealand. Clin J Sport Med. 2008;18(2):111-121.\n- Montain SJ, Sawka MN, Wenger CB. Hyponatremia associated with exercise: risk factors and pathogenesis. Exer Sport Sci Rev. 2001;29(3):113-117.\n- Noakes TD, Sharwood K, Speedy D, et al. Three independent biological mechanisms cause exercise-associated hyponatremia: Evidence from 2,135 weighed competitive athletic performances. Proc Natl Acad Sci U S A. 2005;102(51):18550-18555.\n- Rosner MH. Exercise-associated hyponatremia. Semin Nephrol. 2009;29(3):271-281.\n- Rosner MH, Kirven J. Exercise-associated hyponatremia. Clin J Am Soc Nephrol. 2007;2(1):151-161.\n- Siegel AJ, Verbalis JG, Clement S, et al. Hyponatremia in marathon runners due to inappropriate arginine vasopressin secretion. Am J Med. 2007;120(5):461.e11-461.e17.\n- Speedy DB, Noakes TD, Rogers IR, et al. Hyponatremia in ultradistance triathletes. Med Sci Sports Exerc. 1999;31(6):809-815.\n- Toy BJ. The incidence of hyponatremia in prolonged exercise activity. J Athl Train. 1992;27(2):116-118.', ""Track topics on Twitter Track topics that are important to you\nThe investigators reasoned that a hypertonic saline concentration higher than 3% could be safe and more efficacious in the treatment of bronchiolitis, alleviating severe symptoms and preventing the need for hospitalization in some instances.\nWe conducted a double blind, randomized, parallel-group clinical trial to compare the efficacy and safety of hypertonic saline 3% and 5% versus normal saline for the treatment of acute bronchiolitis.\nThe study was conducted in the short stay unit of the Pediatric Emergency Center of Hamad General Hospital, the only pediatric emergency facility in the State of Qatar. The Center serves an average of 200,000 patients annually and manages 42 beds in the short stay unit. Patients admitted to the unit were assessed at least every 6 hours by a pediatrician to determine readiness for discharge. The length of stay in the unit for bronchiolitis range from 6 to 168 hours.\nInfants aged ≤18 months presenting to the unit for the treatment of moderate-severe viral bronchiolitis were eligible for the study. Moderate-severe bronchiolitis required having a prodromal history consistent with viral upper respiratory tract infection followed by wheezing and/or crackles on auscultation and a Wang bronchiolitis severity score of ≥ 4 on presentation.\nPatients were excluded from the study if they had one or more of the following characteristics: Born preterm ≤34 weeks gestation, previous history of wheezing, steroid use within 48 hours of presentation, obtundation and progressive respiratory failure requiring ICU admission, history of apnea with in 24 hours before presentation, oxygen saturation ≤ 85% on room air at the time of recruitment, history of a diagnosis of chronic lung disease, congenital heart disease, or immunodeficiency.\nThe six attending physicians who covered the 18 beds in the respiratory section of the short stay unit were trained in scoring and its practical application on our bronchiolitis patients in the unit before the study began.\nWritten and informed consent, was sought from one of the parents or legal guardians for eligible patients as soon as the patient was admitted to the unit. The study was approved by the hospital institutional review board.\nPatients were examined on presentation and those requiring further treatment or observation were admitted to the short stay unit. Those with bronchiolitis were assessed for study eligibility within 2 hours of the initial physician assessment. Patients for whom consent was obtained underwent plain chest radiography and nasopharyngeal swabs were taken for RSV detection (RSV Respi-Strip, Coris Bioconcept, Gembloux, Belgium). Then a computer-generated list of random numbers was used by the enrolling physicians in consecutive order to identify a sealed envelope containing one of three codes identifying one of three different bags of 500 mL of sterilely-prepared blinded study solution, made fresh each morning by a pharmacist blinded to patient assignment.Patients received the study nebulization mixed with 1.5 ml of epinephrine in a double blinded fashion on enrollment and every 4 hours thereafter until they were ready for discharge.Additional nebulized epinephrine 5ml delivered the same way was administered with blinded study solution at a maximum frequency of every hour and additional treatment (e.g., supplementary oxygen, hydration) given at the discretion of the treating physician. Patients were withdrawn from the study if oxygen saturation within 30 min after nebulization fell below 85% on room air or clinical deterioration was determined to warrant transfer to the pediatric intensive care unit. Patients were discharged when the treating physician determined the patient did not need supplementary oxygen, was feeding adequately without intravenous fluids, and had minimal or absent wheezing, crackles, and chest retractions provided he/she had an oxygen saturation ≥ 94% and severity score < 4 on discharge.\nAt discharge, patients were sent home with albuterol metered-dose inhalers . Follow-up by study nurse by telephone was mandatory daily, for one week after discharge. The patient could return to the pediatric emergency center earlier if desired or needed.\nAllocation: Randomized, Control: Active Control, Endpoint Classification: Safety/Efficacy Study, Intervention Model: Parallel Assignment, Masking: Double Blind (Subject, Caregiver, Investigator, Outcomes Assessor), Primary Purpose: Treatment\nTreatment 1. (5% Hypertonic saline + Epinephrine), Treatment 3. (3% Hypertonic saline + Epinephrine), Treatment 2. (Normal saline + Epinephrine)\nPediatric Emergency Center, Al-Saad\nHamad Medical Corporation\nPublished on BioPortfolio: 2014-07-23T21:11:46-0400\nBronchiolitis is one of the main reasons for consultation in primary care and emergency services, as well as the leading cause of hospitalization for lower respiratory tract infection in c...\nHypertonic saline is used to treat elevated intracranial pressure. Intraosseous vascular access has been used to administer fluids and medications. This study combines these to administer ...\nThe study goal is to compare the management of increased intra-cranial pressure (ICP) using 3% hypertonic saline vs. mannitol (given in same osmolar loads). Primary hypothesis: 1. ...\nA comparison of albuterol treatments using hypertonic saline (3%) versus standard saline (0.9%) in patients with admitted patients COPD in regard to Modified Borg Dyspnea scale scores afte...\nThe study is designed to evaluate the efficacy and safety of intravenous infusion of hypertonic saline in BPH patients, candidates for TURP surgery using the monopolar electric resectoscop...\nHemorrhagic shock-induced lung edema and inflammation are two of the main reasons for the rejection of lungs donated for transplantation. Hypertonic saline (HS) induces intravascular volume expansion ...\nThe optimal fluid therapy in children with DKA is a matter of debate, especially if we take into account its association with cerebral edema, the most important complication. Hypertonic Saline Solutio...\nThe role of hyaluronic acid plus hypertonic saline (HA+HS) as a mucoactive treatment in patients with bronchiectasis is still unknown. This study evaluated whether HA+HS solution enhances similar sput...\nTo quantitate the effect of intravenous hypertonic saline on elevated intraocular pressure (IOP) among three groups of glaucoma patients or suspects.\nHypertonic saline (HTS) has potent immune and vascular effects. We assessed recipient pretreatment with HTS on allograft function in a porcine model of heart transplantation and hypothesized that HTS ...\nHypertonic sodium chloride solution. A solution having an osmotic pressure greater than that of physiologic salt solution (0.9 g NaCl in 100 ml purified water).\nA racemic mixture of d-epinephrine and l-epinephrine.\nProduct of epinephrine O-methylation. It is a commonly occurring, pharmacologically and physiologically inactive metabolite of epinephrine.\nA genus of HALOBACTERIACEAE distinguished from other genera in the family by the presence of specific derivatives of TGD-2 polar lipids. Haloarcula are found in neutral saline environments such as salt lakes, marine salterns, and saline soils.\nA family of gram-negative, moderately halophilic bacteria in the order Oceanospirillales. Members of the family have been isolated from temperate and Antarctic saline lakes, solar salt facilities, saline soils, and marine environments.\nIn a clinical trial or interventional study, participants receive specific interventions according to the research plan or protocol created by the investigators. These interventions may be medical products, such as drugs or devices; procedures; or change...\nPediatrics is the general medicine of childhood. Because of the developmental processes (psychological and physical) of childhood, the involvement of parents, and the social management of conditions at home and at school, pediatrics is a specialty. With ...\nAsthma COPD Cystic Fibrosis Pneumonia Pulmonary Medicine Respiratory Respiratory tract infections (RTIs) are any infection of the sinuses, throat, airways or lungs. They're usually caused by viruses, but they can also ...""]"	['<urn:uuid:e490651c-5647-48eb-833f-7930c913fdbd>', '<urn:uuid:a7580fe2-d0ed-40a7-9964-9fe7a11e85e2>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	novice	2025-05-12T18:05:20.075489	8	83	2303
57	What are signs of a bad filter and how do returns help?	Signs of a bad fuel filter include hard starting, engine misfires, power loss, stalling, and poor gas mileage. The return line in fuel filters helps prevent these issues by recirculating some fuel back to the tank to keep it cool and prevent vapor lock, while also stopping carburetor flooding when the engine is turned off.	"['One of the most neglected parts of an engine is the fuel filter. Even though this is such an important component, very few people pay it any attention.\nIt’s also easy to neglect since it’s out of sight making it easy to forget about. Despite seeming insignificant, the fuel filter plays a crucial role in keeping the vehicle running smoothly.\nThis filter ensures no dirty fuel makes its way into the fuel system and clogs the engine. It also helps to prevent other serious engine problems related to the fuel system.\nFor ongoing vehicle maintenance, it’s important that the fuel filter is always clean and functioning properly. In order for this to happen, you need to know how to tell when you have a bad fuel filter or if it’s failing.\nTo make things easier for you, in this post we’ll go over the symptoms of a bad fuel filter and some other important information you should know. This is bound to be helpful if you think you’re having some fuel filter issues so let’s jump right into it!\nSymptoms of a Bad Fuel Filter\nThe first step for maintaining your fuel filter is to know when you should check it and whether you need to clean or replace the filter. This ensures it remains in good shape so that your engine can continue to run smoothly.\nDepending on the vehicle, the symptoms of a bad fuel filter will be different in various makes and models. However, here are some of the most commonly manifested symptoms that you should be worried about if you’re experiencing.\nThe Vehicle is Hard to Start\nOne of the first warning signs that might reveal you have a bad or failing fuel filter is that the vehicle will be hard to start. In some instances, the vehicle may fail to start completely and you won’t be able to get the engine running.\nWith a dirty or clogged fuel filter, the flow of fuel is restricted or inconsistent as it makes its way into the engine. Since the engine needs fuel to start, this will make it harder or even impossible to get the engine started.\nIt’s possible that the vehicle will still start after several attempts, even with a damaged, clogged, or broken filter. If you’re having trouble starting your car, you should consider some other issues before blaming it on the filter, such as a faulty starter or dead battery.\nEngine Misfiring Randomly\nThere’s also a high possibility you have a bad or clogged fuel filter if you’re getting random engine misfires. It’s even more likely if this happens when the vehicle is carrying a heavy load or going uphill, situations where it uses more fuel.\nWe mentioned engine backfires in our posts on bad idle air control valve symptoms as well as motorcycle backfiring. This can also happen when you have a clogged fuel filter, which prevents enough fuel from getting into the engine.\nWith older vehicles, these misfires will be more common and could occur when the vehicle is idling or moving. it’s also possible for the vehicle to stutter or shake since the amount of fuel getting to the engine will vary due to the severity of the clog.\nAlthough newer vehicles are less likely to misfire when idling from a clogged fuel filter, it’s still possible. If this is an issue you’re experiencing, start by checking the idle air control valve and the fuel filter at the same time.\nSignificant Drop in Engine Power\nAfter driving your vehicle for an extended period of time, you should have a good idea how the engine feels and how much power the vehicle can generate. This means it’s easy to spot a significant drop in engine power, which is a sign of a clogged fuel filter.\nThese drops in power will be most noticeable when you’re driving uphill. The vehicle will have difficulty maintaining its speed and you may experience issues with acceleration while driving uphill or carrying a heavy load.\nThis happens since the fuel supply from the gas tank is not able to meet the demands of the engine. Fuel simply isn’t following in the right quantity and the engine isn’t able to keep up.\nEngine Stalling Frequently\nWith an automatic vehicle you shouldn’t be experiencing any engine stalling issues. While this may seem like a catastrophic engine problem, it’s possible that a clogged fuel filter could be the reason.\nThis happens when a clogged fuel filter is left dirty and not attended to while it becomes even more clogged. The engine gets starved of fuel, resulting in the engine stalling from a lack of fuel supply.\nYou may notice that the engine stalls when you’re trying to accelerate or putting extra load on the engine. There simply isn’t enough fuel getting through to generate the extra power, so the engine bogs down and stalls.\nThis could definitely be a symptom of a clogged fuel filter. It’s something to look into if you’re regularly experiencing the engine stalling along with these other issues we’ve mentioned.\nLower Gas Mileage\nIt may seem counterintuitive, but a bad fuel filter that’s restricting fuel from getting to the engine can lead to poor fuel economy. Since the filter would be clogged, there is a higher demand on the fuel pump to provide fuel to the engine.\nThis results in more fuel being used and the vehicle will experience lower gas mileage. The engine would also be struggling and there would be more difficulty for the engine to move the vehicle, which burns more fuel.\nCheck Engine Light Comes On\nThe vehicle’s computer should be able to detect a problem in the fuel system, such as a bad or failing fuel filter. Once the problem is detected, the check engine light will be switched on by the ECU.\nIf you already suspect you have a bad fuel filter and the check engine light comes on, this could be one of the problems. At this point it would be a good idea to use your OBD-2 scanning tool to see what the error code is.\nKeep in mind that the computer won’t detect a problematic fuel filter, it would simply detect that the engine is being strained or starved of fuel. Once this happens, the check engine light will illuminate, and you’ll know there is a problem you need to look into.\nDamaged Fuel Pump\nOne of the main causes of a damaged fuel pump would be a bad or clogged fuel filter. This is a common vehicle problem, which is typically a sign of having a bad filter that should be replaced.\nWhen the fuel filter is unable to run properly, there is a lot of pressure being put on the fuel pump. All of this added pressure will eventually damage the fuel pump while preventing enough fuel from getting to the engine.\nShould you suspect issues with the fuel pump, checking the fuel filter is a good place to start. Otherwise, you may continue to have fuel pump problems even if you put in a new one.\nShould You Replace or Clean the Fuel Filter?\nNow that you have a better idea of the bad fuel filter symptoms to watch out for, the next step is to properly address the issue. Ultimately, there are two possible solutions to dealing with a bad fuel filter, which are cleaning its or replacing it.\nWhether the filter needs to be cleaned or replaced depends on the situation. It’s a good idea to start with cleaning the fuel filter if you don’t have time or resources to buy a new one.\nThis is also a good option if the filter isn’t heavily clogged or it’s washable. You’ll need to know how to clean a fuel filter properly, though there are plenty of helpful videos for this.\nFor a better long-term solution it’s a good idea to replace the filter. This way you’ll know it’s working properly and you won’t have any debris inside or worry about getting it clogged as soon.\nWhen Should You Replace a Fuel Filter?\nReplacing the fuel filter depends on the symptoms and the situation you’re in. Simply replacing it means you’ll get to avoid dealing with the consequences of a bad fuel filter.\nHowever, this might not always be necessary. Here are the different reasons why and when you should replace the filter.\nEvery 20,000 to 60,000 Miles\nReplacing the fuel filter is part of your ongoing preventive vehicle maintenance. Even if it seems to be in great working condition, or you’re not having any fuel system issues, it still needs to be replaced on a regular basis.\nStart by checking the owner’s manual to see when they recommend replacing the fuel filter.This normally happens between 20,000 and 60,000 miles depending on the age of the vehicle.\nOlder vehicles will require more frequent fuel filter changes. There should be some helpful videos on YouTube for a tutorial on how to change it correctly.\nRunning Out of Gas With an Old Fuel Filter\nIt’s never a good idea for a vehicle to run out of gas. If this happens accidentally, this would be a good time to replace the filter.\nWhat happens when you run out of gas is that any dirt and debris sitting in the gas tank gets forced through the fuel pump. This collects on the filter and clogs everything up due to the high volume of dirt and debris.\nOnce the fuel filter is clogged, there is now extra strain on the fuel pump. Since the gas flow is affected, it would be important to replace the filter before running more fuel through the system.\nWhen Repairing the Fuel System\nAnother great time to replace the fuel filter is when you’re working on or repairing any part of the fuel system. Since you’ll already be under the hood, it wouldn’t hurt to replace the filter with a fresh one.\nThis also means the filter isn’t the weak link of the fuel system. You would then know that the engine is running at optimal performance, assuming everything else is in prime shape.\nKeep in mind that fuel filters on diesel vehicles should be replaced more often than those on gas vehicles. This is because diesel tanks are more prone to bacteria and fungus growth.\nIt doesn’t hurt to run some fuel injector cleaner through the diesel system either. This also helps to prevent any unwanted growth and keeps the fuel filter in better shape.\nSince the fuel filter is an important component of the vehicle’s engine, it shouldn’t be overlooked when it comes to keeping the vehicle running at peak performance. It should now be easy for you to tell when you’re experiencing symptoms of a bad or failing fuel filter.\nThankfully, fixing a bad fuel filter is rather simple and affordable. Despite this being an easy fix, don’t overlook some of the other issues that could be plaguing the fuel system in your vehicle.\nWhen in doubt, bring your vehicle to a professional mechanic who could let you know exactly what’s wrong. Otherwise, make the effort yourself to get everything working again for your own peace of mind.', 'GAS FUEL FILTER WITH RETURN\nricksfreeautorepairadviceImage: ricksfreeautorepairadviceThis gas filter has a built in return line. The return line has an internal restriction that allows a small amount of fuel to recirculate back to the gas tank to keep fuel in pump/fuel line/filter cool,reducing the chance of vapor lock. Return line also allow fuel pressure to carb to quickly goto zero,preventing possible flooding of carb when engine is turned off.\nFuel Filter Universal Metal 1/4"" w/Return Line\nWas this helpful?People also askWhat does a gas filter do?What does a gas filter do?The filter is supposed to catch and remove any contaminants and impuritiesfrom the fuel line that could damage the engine. Sediment can get into a vehicle’s fuel tank when you pump fuel from the bottom of a storage tank at a gas station,and that can clog the filter or restrict the flow..Do I Need to Replace My Fuel Filter? - New and Used CarsSee all results for this questionWhat does inline fuel filter do?What does inline fuel filter do?The inline fuel filter is the device responsible for cleaning the fuel that goes inside your engine. Most often,car owners don’t pay attention to it until they experience power loss and difficult acceleration. This can be a symptom of a dirty inline fuel filter.How to Replace a Car\'s Inline Fuel Filter | DoItYourselfSee all results for this questionWhat is a gas filter?What is a gas filter?Gas filtration. Gas filtering refers to the separating of particles from a gas- e.g. from air,or removing particulate pollution from a gaseous waste. It may also be needed to maintain indoor air quality in some cases - e.g. filtering the outlet from vacuum cleaners. 1 Methods. 2 Making of fabric filters.Gas filtration - Appropedia: The sustainability wikiSee all results for this questionFeedback\nAmazon: FRAM G3583DP In-Line Fuel Filter: Automotive\nThese filters were used on mid-\'70s Chrysler products and provide a return line to the tank to bleed of aerated fuel. I have used them many times to cure vapor lock problems. They should be mounted near the carburetor with the return fitting turned to the top for best results.4.5/5(9)\nChevy Fuel Filter, GF432, For Cars With 1/4"" Return Line\nEckler\'s Late Great Chevy has the exact replacement for your original GF432 AC Delco fuel filter. The original GF432 has an extra take off for the 1/4"" return line that runs behind the water pump. Trust Ecklers Late Great Chevy for your Impala, Caprice, Biscayne & Bel Air parts needs always at the best possible price. 1969-74 Chevy Gas Filter\nFuel Filters | O\'Reilly Auto Parts\nOrder Fuel Filters for your vehicle and pick it up in store—make your purchase, find a store near you, and get directions. Your order may be eligible for Ship to Home, and shipping is free on all online orders of $35+. Check here for special coupons and promotions.\nFuel Filters - Inline Fuel Filter Style - 3/8 in. Inlet\nFind Fuel Filters Inline Fuel Filter Style, 3/8 in. Inlet Size, Gasoline Fuel Type and get Free Shipping on Orders Over $99 at Summit Racing!\nAmazon: 3/8"" Universal Inline Gas Fuel Filters\n3/8"" Universal Inline Gas Fuel Filters Industrial High Performance - 5 pack. FITS 3/8"" FUEL LINE - 3/8"" FITTING OR CONNECTOR AT EACH END. IN LINE FUEL FILTER. BULK PACKAGING - FILTERS NOT PACKAGED - SHIPPED IN BULK - SAVES THE EXPENSE OF RETAIL PACKAGING. 5 NEW FILTERS. CLEAR PLASTIC SHELL WITH ORANGE INTERIOR.4.6/5(82)Price: $22Brand: AK Garage\nFram G3583 fuel filter with vapor return | For B Bodies\nJan 18, 2019I was planning to run some 1/4"" aluminum line back to the tank as a fuel/vapor return & connect it to the 3/8"" nipple where fuel normally comes OUT (reverse flow). I read somewhere that those filters (small nipple turned upwards) are useful for a vapor return system MUCH cheaper than an actual factory vapor return cannister.Fuel vapor separator filter | For B Bodies Only ClassicAug 16, 2020Fuel filter(s) and vapor separator questions 70 440 6bblDec 12, 2019fuel return line | For B Bodies Only Classic Mopar ForumAug 18, 2018fuel vapor separator on 440 6pak? | Page 3 | For B BodiesJun 15, 2018See more results\nfuel filter with return line - what oem application | The\nAug 23, 2012Ford trucks with 460\'s came with what Ford called the ""hot fuel option"" it uses a return line from the filter also. I can tell you that I run this set up on my 460\'s, sure stops vapor lock. I have never worried about clocking the return line.Hot Rods - fuel return line ? | The H.A.M.B.Oct 10, 2018Technical Gas tank. oshpo or Evapo Rust?Oct 26, 2017Falcon fuel sending unit w/aftermarket gauges?Feb 06, 2013RUNTZ volt dpop to fuel gaugeFeb 19, 2009See more results\nFuel Line/Return Issue - The AMC Forum - Page 1\nFeb 06, 2013The return line from filter is rubber as well. I would have preferred to run threaded and braided line from the pump directly through a filter and then to carb. A nice pro look using threaded braided line. However, because I have to use the stock type filter, I\nCJ RETURN FUEL LINES, IS IT REALLY NEEDED? - Off-Road\nJun 18, 2002It\'s not *just* for fuel vapor. The purpose of the return line is to ALSO send excess pressure/fuel back to the tank, because the fuel pump supplies more than CJ7 gas tank fuel & vapor lines to pump & carb em.Jul 10, 2002What does the fuel return hose really do?Apr 24, 2002See more results\nFuel Filter Universal Metal 1/4"" w/Return Line\nThis gas filter has a built in return line. The return line has an internal restriction that allows a small amount of fuel to recirculate back to the gas tank to keep fuel in pump/fuel line/filter cool, reducing the chance of vapor lock. Return line also allow fuel pressure to carb to quickly go to zero, preventing possible flooding of carb when engine is turned off.\nRelated searches for gas fuel filter with return\nfuel filters with return linefuel filter with return portfuel filter with vapor returngas line fuel filterswix fuel filter with returnfuel return valvefuel filters for salefuel line filters']"	['<urn:uuid:ac6d421d-43ba-47c9-bc38-ab187e3e1e58>', '<urn:uuid:641bcce0-be4d-4ac0-846e-179d8e5b2bec>']	factoid	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:05:20.075489	12	55	2893
58	radiation monitoring tools first responders uses measurements what kind of devices track exposure	First responders use several types of radiation monitoring devices. For daily operations, they employ Electronic Personal Dosimeters (EPDs) which provide continuous monitoring with alarm warnings at preset levels and live readout of accumulated radiation dose. They also use portable multi-functional survey instruments like the ADM 300A series, which can detect and measure dose rate levels of Gamma radiation and has Beta detection capability. For regular radiation workers, there are additional options including film badge dosimeters (which show radiation absorption through changes in film emulsion), thermoluminescent dosimeters (TLDs, which measure radiation exposure by crystal light emission when heated), and quartz fiber dosimeters (which measure radiation through charge leakage). The EPD and TLD types are becoming the preferred options, replacing older technologies like film badges and quartz fiber dosimeters.	"[""In the UK, it is not unusual for first responders to be in possession of very little (or, in some cases, nothing) in the way of detection, identification or monitoring (DIM) equipment in the course of carrying out their daily duties.\nIn the event of a major incident however (such as a terrorist attack, major fire, flood or a collapsed building) they may be called upon to support the activities of a specialist Hazardous Area Response Team (HART).\nIn such scenarios, it is vital that all personnel are comfortable in the use of their detection, identification and monitoring equipment - be it chemical detection devices - such as lightweight chemical detectors - or radiological equipment such as electronic personal dosimeters (EPDs), electronic radiation detectors or radiation contamination meters.\nIn this blog post we summarise the features and functionality of four specific items of equipment in common use by CBRNe response teams - and we explore the role that their simulator equivalents can play in the context of training exercises.\nRadiological survey training\nThe ADM 300A series of portable multi-functional survey instruments facilitate the detection, measurement and digital display of dose rate levels of Gamma radiation.The instrument also has a Beta detection capability.\nA simulator equivalent (such as the Argon ADM300A V1B-SIM radiation training simulator) can serve as an ideal training instrument for radiological survey missions. Crucially, this instrument enables CBRNe response teams to safely and realistically train without the need for a potentially hazardous ionizing radiological source.\nFor the CBRNe instructor, there is also the advantage of retaining full control of the exercise, with the ability to conduct multi-detector or multi-threat training within the same operational scenario.\nMonitoring of personal exposure\nWhen attending CBRNe incidents, emergency services personnel also need to be able to monitor their own exposure to any forms of potentially harmful radiation - and to react quickly in the event that acceptable dose rate levels are exceeded.\nIn such circumstances they rely on the services of an electronic personal dosimeter (EPD), such as the EPD Mk2, which has been designed to respond to gamma, beta and X-radiation.\nFor the purposes of training however, the use of a simulator equivalent such as the EPD Mk 2 SIM retains all the user interface of the real dosimeter but which responds to safe, environmentally friendly, simulation radionuclide sources.\nSafe and realistic chemical detection\nThe detection of chemical warfare agents (CWAs) or toxic industrial chemicals (TICs) relies on the use of a person worn device such as the Smiths Detection LCD3.3.\nA key benefit of this form of detector is its ability to identify specific CWAs, however personnel safety, and environmental considerations, can render many forms of simulant-based training impractical for CBRNe exercises.\nWhen the highest levels of realism are required, a simulator detector such as the LCD3.3-SIM can prove an invaluable training resource. The LCD3.3-SIM has been designed to respond to electronic sources to safely replicate all the features and functionality of the real detector, while at the same time mitigating all risk.\nRemote control features, together with powerful After Action Review (AAR) also ensure that instructors retain complete control of the exercise and that trainees understand if, or when, they've gone wrong.\nPortable chemical contamination control\nA portable chemical contamination control device such as the AP4C can be used to directly detect a wide range of chemical agents (and their precursors) including compounds of phosphorus (G and V agents), sulfur, arsenic compounds and gases such as ammonia, Cyanogen chloride and Cyahydric acid. For training purposes however, being able to operate without the need for harmful simulants can be a major asset.\nWhen authenticity is required, but with no compromise on trainee safety, a simulator detector such as the AP4C-SIM can provide an invaluable solution. This particular device can also be used in conjunction with detection instruments that work on different technology principles - to provide compatibility with other simulator products (such as CAMSIM, LCD3.2e-SIM, RAID-M100-SIM and the S4PE-SIM) for multi-detector / multi-substance training exercises.\nMajor terror incidents such as the Salisbury nerve agent attacks and the Manchester Arena bombing provide a stark reminder of the constantly evolving nature of CBRNe threats - and of how heavily we rely on the knowledge and expertise of our region's specialist CBRNe response teams.\nEffective CBRNe training requires the highest levels of realism and authenticity, but whilst still ensuring there is zero compromise on personnel safety. In such scenarios, simulator detector technology can provide a viable and effective solution that puts instructors firmly in control of their exercises."", 'A radiation dosimeter is a device that measures exposure to ionizing radiation. It has two main uses: for human radiation protection and for measurement of dose in both medical and industrial processes.\n- 1 Personal dosimeters\n- 1.1 Types\n- 1.2 Radiation protection dose quantities\n- 1.3 Protection quantities\n- 1.4 Operational quantities\n- 1.5 Instrument and dosimeter response\n- 2 Process irradiation verification\n- 3 Gallery\n- 4 See also\n- 5 References\n- 6 External links\nThe personal ionising radiation dosimeter is of fundamental importance in the disciplines of radiation dosimetry and radiation health physics and is primarily used to estimate the radiation dose deposited in an individual wearing the device.\nIonising radiation damage to the human body is cumulative, and is related to the total dose received, for which the SI unit is the sievert. Workers exposed to radiation, such as radiographers, nuclear power plant workers, doctors using radiotherapy, those in laboratories using radionuclides, and HAZMAT teams are required to wear dosimeters so a record of occupational exposure can be made. Such devices are known as ""legal dosimeters"" if they have been approved for use in recording personnel dose for regulatory purposes.\nDosimeters can be worn to obtain a whole body dose and there are also specialist types that can be worn on the fingers or clipped to headgear, to measure the localised body irradiation for specific activities.\nCommon types of personal dosimeters for ionizing radiation include:\nElectronic personal dosimeter (EPD)\nThe electronic personal dosimeter (EPD) is an electronic device that has a number of sophisticated functions, such as continual monitoring which allows alarm warnings at preset levels and live readout of dose accumulated. These are especially useful in high dose areas where residence time of the wearer is limited due to dose constraints. The dosimeter can be reset, usually after taking a reading for record purposes, and thereby re-used multiple times.\n1. The MOSFET dosimeter is direct reading with a very thin active area (less than 2 μm).\n2. The physical size of the MOSFET when packaged is less than 4 mm.\n3. The post radiation signal is permanently stored and is dose rate independent.\nGate oxide of MOSFET which is conventionally silicon dioxide is an active sensing material in MOSFET dosimeters. Radiation creates defects (acts like electron-hole pairs) in oxide, which in turn affects the threshold voltage of the MOSFET. This change in threshold voltage is proportional to radiation dose. Alternate high-k gate dielectrics like Hafnium dioxide and Aluminum oxides are also proposed as a radiation dosimeters.\nFilm badge dosimeter\nFilm badge dosimeters are for one-time use only. The level of radiation absorption is indicated by a change to the film emulsion, which is shown when the film is developed.\nQuartz fiber dosimeter\nQuartz fiber dosimeters are charged to a high voltage. As the gas in the dosimeter chamber becomes ionized by radiation the charge leaks away, causing the fiber indicator to change against a graduated scale.\nThermoluminescent dosimeter (TLD)\nA thermoluminescent dosimeter measures ionizing radiation exposure by measuring the intensity of visible light emitted from a crystal in the detector when heated. The intensity of light emitted is dependent upon the radiation exposure.\nBoth the quartz fiber and film badge types are being superseded by the TLD and the EPD.\nRadiation protection dose quantities\nThe dosimeter plays an important role within the international radiation protection system developed by the International Commission on Radiological Protection (ICRP) and the International Commission on Radiation Units and Measurements (ICRU). This is shown in the accompanying diagram.\nThe protection quantities are used as ""limiting quantities"" to specify exposure limits to ensure that the occurrence of stochastic health effects is kept below unacceptable levels and that tissue reactions are avoided. These quantities cannot be practically measured and are a calculated value of irradiation of organs of the human body, which is arrived at by using an anthropomorphic phantom. This is a 3D computational model of the human body which attempts to take into account a number of complex effects such as body self-shielding and internal scattering of radiation.\nAs protection quantities cannot practically be measured, operational quantities are used to relate them to practical radiation instrument and dosimeter responses.\nOperational quantities are aimed at providing an estimate or upper limit for the value of the protection quantities related to an exposure. They are used in practical regulations or guidance. These relate real-life operational instrument measurements and responses to the calculated protection quantities.\nPersonal dose equivalent, Hp(d), is defined by the ICRP as the dose equivalent in soft tissue at an appropriate depth, d, below a specified point on the human body. The specified point is usually given by the position where the individual’s dosimeter is worn.\nInstrument and dosimeter response\nThis is an actual reading obtained from such as an ambient dose gamma monitor, or a personal dosimeter. The dosimeter is calibrated in a known radiation field to ensure display of accurate operational quantities and allow a relationship to known health effect. The personal dose equivalent is used to assess dose uptake, and allow regulatory limits to be met. It is the figure usually entered into the records of external dose for occupational radiation workers.\nThe ""slab"" phantom is used to represent the human torso for calibration of whole body dosimeters. The IAEA states ""The slab phantom is 300 mm × 300 mm × 150 mm depth to represent the human torso"".\nProcess irradiation verification\nManufacturing processes that treat products with ionizing radiation, such as food irradiation, use dosimeters to calibrate doses deposited in the matter being irradiated. These usually must have a greater dose range than personal dosimeters, and doses are normally measured in the unit of absorbed dose: the gray (Gy). The dosimeter is located on or adjacent to the items being irradiated during the process as a validation of dose levels received.\n- Comparison of dosimeters\n- Geiger counter\n- Scintillation counter\n- Ionisation chamber\n- Royal Observer Corps\n- Operational instruments of the Royal Observer Corps\n- ""Advances in kilovoltage x-ray beam dosimetry"". Physics in Medicine and Biology 59: R183–R231. doi:10.1088/0031-9155/59/6/R183.\n- ""Polymer gel dosimetry"". Physics in Medicine and Biology 55: R1–R63. doi:10.1088/0031-9155/55/5/R01.\n- V. S. Senthil Srinivasan and Arun Pandya, ″Dosimetry aspects of hafnium oxide metal-oxide-semiconductor (MOS) capacitor″,  Thin Solid Films Volume 520, Issue 1, 31 October 2011, Pages 574–577\n- Frame, Paul (2007-07-25). ""Pocket Chambers and Pocket Dosimeters"". Health physics historical instrument museum collection. Oak Ridge Associated Universities. Retrieved 2008-11-08.\n- ICRP publication 103, paragraph 112\n- ICRP publication 103, paragraph B50\n- ""In 1991, the International Commission on Radiological Protection (ICRP)  recommended a revised system of dose limitation, including specification of primary limiting quantities for radiation protection purposes. These protection quantities are essentially unmeasurable"" - IAEA Safety report 16\n- ICRP publication 103, paragraph B64\n- ICRP publication 103, paragraph B146\n- ICRP publication 103, paragraph B147\n- ICRP pub 103 glossary\n- IAEA safety report 16\n|Wikimedia Commons has media related to Dosimeters.|']"	['<urn:uuid:8f066c56-799a-461e-a522-be9ac1e08196>', '<urn:uuid:817785ae-c753-444b-a06e-12ee10244bed>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T18:05:20.075489	13	127	1916
59	first click last click attribution which better	Neither first-click nor last-click attribution is ideal as both are overly simplified. First-click gives all credit to the initial interaction but underestimates lower-funnel channels like retargeting and fails to capture offline influences like TV. Last-click strongly overemphasizes search since it tends to be the final engagement before conversion, while overlooking TV impact and seasonality. More sophisticated approaches like discrete choice models provide better accuracy by considering multiple factors including offline influences, individual consumer paths, and both frequency and recency of exposures.	['Advertisers don’t always have a clear idea about how much value each channel or campaign adds to their companies’ success.\nDespite the availability of big data, advertisers don’t always have a clear idea about how much value each channel or campaign adds to their companies’ success. To solve this problem, marketers need a clear methodology to attribute credit to the different marketing channels that correctly measures the value of each marketing contact. While this statement may seem obvious, it is hardly a zero-risk proposition. The wrong attribution methods can lead advertisers to incorrectly assess the value of marketing efforts—driving them to move dollars away from successful channels and toward less-effective ones.\nTo address the attribution question, Neustar MarketShare, conducted a major big data study to identify which of the most common attribution models are the most accurate. The study simulated a year’s worth of advertising on a population the size of the United States (316 million people; hence, in the study, 316 million software “entities”). The aim of the project was to create a complete population in which it was clear how advertising influenced purchase decisions —and then to run various attribution approaches to see how close these approaches came to actually measuring the real influence of ads.\nThe simulated individuals were exposed to online and offline advertising; and were programmed to increase their likelihood to buy after every ad exposure—up to a saturation point. The entities were programmed with varying degrees of innate interest in the product; and many of them went in and out of market to shop over the course of the year. The resulting analysis was huge—generating 2 terabytes of simulated tracking data over two weeks’ time.\nOnce we had created the simulation, we ran attribution analyses on the ways advertising influenced the simulated population’s purchase decisions. We analyzed four of the most common digital attribution models: first click, last click, matched pairs, and discrete choice models. Since the simulation already “knew” exactly how advertising had influenced purchase decisions, we were able to match the various attribution analyses with the actual influence that ads had had on purchases. We then scored the attribution methodologies to see which matched most closely with the ads’ actual influence.* The result showed us which attribution models were the most—and least—effective at understanding the customer’s path to purchase.\nFirst Click attribution gives all credit to the first touch along the conversion chain—based partially on the premise that the first consumer/brand interaction was the one to “kick off” the entire path to conversion.\nComparing first-click attribution with the real influence of ads reveals a number of “fatal flaws” inherent to the approach. The method clearly over-estimates the impact of search and branded display—both which drive high levels of initial awareness—but underestimates lower-funnel channels like retargeting and targeted display. In addition, because first-click is based purely on cookie level information recorded in digital log files, it fails to capture the influence of offline influences—like TV; as well as factors outside of marketing that can have an enormous influence on sales, such as seasonality.\nA “close cousin” of first-click, last click attribution gives complete credit to the touch that immediate precedes conversion. If a customer saw a TV ad, ran a web search that drove her to a site, and then saw a retargeting ad, last-click will give credit to the retargeting ad—and ignore the influence of the TV ad or the search. Both last click and first-click are widely viewed as overly simplified, but are commonly used for a quick “snapshot” of sources of marketing leads.\nComparing last-click attribution with the real influence of ads shows that last click strongly overemphasizes the impact of search—not surprising, as search tends to be the final marketing engagement before a consumer arrives at a site. Last click also was shown to overlook the enormous impact of TV and seasonality—once again unsurprising, as last-click is both bottom-funnel and cookie-based, and so is likely to fail in addressing non-addressable and/or top-funnel influences.\nEssentially a large post-hoc test and control study, matched pairs finds sets that are identical except for one attribute, and compares outcomes both with and without that attribute. For instance, to understand the impact of targeted display, matched pairs attribution might look at conversion rates for consumers who have been exposed to all media including targeted display, as compared to the conversion rate of those who have not been exposed to targeted display but who have been exposed to all other media. According to this approach, the difference in conversion rates between the groups should reveal the effectiveness of targeted display.\nThere were several areas of discrepancy that we observed between the matched pairs approach and the true answer that the simulation provided. Investigating the discrepancies suggested four main problems:\nMatched pairs relies on measuring the impact of consumers’ different experiences—for instance, the impact of being exposed to retargeting versus not being exposed. However, many of the strongest influences on ad effectiveness, such as seasonality and TV exposure, are nearly universal, and so there is simply no difference to measure. For this reason, matched pairs is not designed to measure the impact of these shared experiences. This would largely explain why TV and seasonality are both completely overlooked by the matched pairs analysis.\nOn a related note, matched pairs relies almost exclusively on cookie-level data—which provides insight into who has or has not been exposed to a given type of advertising. Cookie level data alone, however, will not identify the influence of events that are not recorded at the cookie level, such as much of offline advertising and seasonality.\nMatched pairs judges ad effectiveness by comparing conversion rates of populations who have been exposed to an ad with populations that have not. There is an inherent bias problem in this analysis, however—as advertisers naturally tend to target groups who are highly likely to convert. For this reason, it can be extremely difficult to separate the impact of the ad on a group of consumers from the natural tendency of that group to buy the product, regardless. In other words, it’s easy to over-estimate the impact of an ad on a population that needs very little convincing to buy. To address these problems in our matched pairs analysis, we adjusted the data for intention bias—matching groups with like inherent intent to buy, and factoring for behavior that indicates an imminent purchase.\nHowever, it is impossible to weed out all such biases from population-level studies. This likely explains why, even with our adjustments of the sample, the approach still significantly overestimates the impact of targeted display, which obviously favors buyers who are likely to purchase. (It is also worth noting that, in practice, many digital attribution matched pairs practitioners do not account for these biases at all. To learn more about what this study revealed about attribution at consumers’ pre-existing intent to purchase, see our separate brief on attribution and customer intent.)\nMatched pairs asks a binary question—were consumers exposed to a type of advertising, or not?— and measures ad influence accordingly. That approach fails to account for the incremental impact of additional ad exposures. A consumer will respond very differently to one, two, or fifteen ad exposures; since these are not binary distinctions, matched pairs struggles to address that difference.\nA second result of this binary approach is that it leaves recency out of the calculation, even though an ad exposure from today is very likely to have a different influence than an ad exposure from last week. Since matched pairs looks primarily at who was exposed to an ad versus who was not, the approach gives the same treatment to all ad exposures, regardless of when they were delivered.\nThis binary approach likely accounts for the fact that matched pairs under-attributes branded display by a great deal, and also underestimates branded paid search and retargeting. In their own ways, all of these channels serve a branding function—and so they stand to become more effective the more the consumer is exposed, and the more recently the exposure has been. Matched pairs, however, is not designed to register the impact of those factors.\nDiscreet Choice Models\nWidely used in machine learning applications, discrete choice models ask which attributes predict a given action, as well as how changes in those attributes might bring an action closer. Discrete choice models ask “who” the customer is, what actions that customer has taken in the past, and how these may work as predictors of an imminent conversion. The answers to these questions might lie in path data—such as prior interactions with the brand’s advertising and website—that suggest a logical next step in a journey. It could also use segmentation data, such as demographics and psychographics—as different populations will be more or less likely to take specific actions.\nThe high degree of accuracy that we see in the discrete choice model can be attributed to a number of factors:\n- Because the model takes offline factors into account, the models are not inaccurately biased toward online media alone.\n- Because discrete choice models operate at the individual consumer level, they can fully control for sampling and intention. As mentioned above, matched pairs can adjust sampling, but only at the segment level. Inevitably, accuracy will be lost in the blunter approach. Discrete choice modelling examines individual consumer paths, not group behaviors—and so can create a much more precise picture of ad effectiveness.\n- The model takes both the frequency and recency of exposures into account—again, both enabled by tracking and analysing customer journeys at the individual level\nIn short, discrete choice modelling provides a more complete and exact understanding of what drives individuals toward conversion. This, in turn, produces a more accurate understanding of which marketing investments work, and how. These are amongst the reasons MarketShare employs the discrete choice approach to digital attribution.', 'Breaking Down AdWords Attribution Models\nCheckout this post to get your bearings on how to manage attribution models in Google Ads.\nUnderstanding the customer journey is fundamental for brands to compete online today. Yet despite its importance, true attribution in digital marketing can be an extremely cumbersome undertaking. When it comes to paid media channels such as Facebook ads, programmatic display and of course, PPC, choosing the right attribution model can make all the difference in understanding the customer journey.\nIn the past 2 years, there have been several enhancements to the newly re-branded Google Ads platform in terms of modeling options when setting up conversions.\nEach different option caters to different growth strategies and can have very different results depending on what you choose to go with.\nThroughout this post, we’ll explain each different model available in Google Ads and provide examples of why you would pick one of the other depending on your goals.\nLet’s look at the different attribution models available to us directly in the Google Ads engine. Here’s Google’s official explanation:\n- Last Click: Gives all credit for the conversion to the last-clicked ad and corresponding keyword.\n- First Click: Gives all credit for the conversion to the first-clicked ad and corresponding keyword.\n- Linear: Distributes the credit for the conversion equally across all clicks on the path.\n- Time Decay: Gives more credit to clicks that happened closer in time to the conversion. Credit is distributed using a 7-day half-life. In other words, a click 8 days before a conversion gets half as much credit as a click 1 day before a conversion.\n- Position-Based: Gives 40% of credit to both the first- and last-clicked ads and corresponding keyword, with the remaining 20% spread out across the other clicks on the path.\n- Data-Driven: Distributes credit for the conversion based on past data for this conversion action. (This is only available to accounts with enough data.) Data-driven attribution is different from the other attribution models, in that it uses your conversion data to calculate the actual contribution of each keyword across the conversion path. Each data-driven model is specific to each advertiser. It’s also important to keep in mind, however, that attribution modeling is only available for Search Network and Shopping clicks and isn’t available for interactions with GDN campaigns.\nA customer finds your sports shoes website by clicking on your ads after performing each of these searches: “orthopedic shoes,” “running shoes,” “comfortable running shoes,” and then “comfortable orthopedic running shoes.” They make a purchase after clicking on your ad that appeared with “comfortable orthopedic running shoes.”\n- In the “Last click” attribution model, the last keyword, “comfortable orthopedic running shoes,” would receive 100% of the credit for the conversion.\n- In the “First click” attribution model, the first keyword, “orthopedic shoes,” would receive 100% of the credit for the conversion.\n- In the “Linear” attribution model, each keyword would share equal credit (25% each) for the conversion.\n- In the “Time decay” model, “comfortable orthopedic running shoes” would receive the most credit because it was searched closest to the conversion. The “orthopedic shoes” keyword would receive the least credit since it was the furthest from the conversion.\n- In the “Position-based” model, “orthopedic shoes” and “comfortable orthopedic running shoes” would each get 40% credit, while “running shoes” and “comfortable running shoes” would each get 10% credit.\n- In the “Data-driven” attribution model, each keyword would be attributed part of the credit, depending on how much it contributed to driving the conversion.\nLast Click: Most advertisers and PPC managers are familiar with this model and is the standard. The pros are that it allows you to maximize your PPC efficiency but it severely undermines other channels in the conversion path and can overvalue brand search and remarketing. You would want to choose this model if your growth strategy is more on the conservative side.\nFirst Click: This would be the exact opposite of the one above and gives all credit to the first interaction. The benefits of this model is that it gives credit to your top of funnel efforts which can lead to more customer acquisition in the long term. One drawback however, is that it might make very broad competitive terms look more profitable.\nLinear: If first and last click models are too finite for your business, you might consider linear attribution. In this model, all clicks that lead to the conversion are given the same amount of credit]. Here, every touch point is considered which caters to a moderate growth strategy.\nTime Decay: This model is conservative and gives more credit to clicks that happened closer in time to the conversion. Credit is distributed using a 7-day half-life. In other words, a click 8-days before a conversion gets half as much credit as a click 1-day before a conversion. I tend to like this model a lot because it can provide efficiency at the bottom of the funnel while still giving credit to the top of funnel. The downside of this one, similar to last click, is that it might overvalue brand and remarketing.\nPosition Based: This model can undervalue touchpoints at the middle of the funnel since it gives most credit to the first and last interactions. This can be detrimental to brands with long sales cycles as they can have a less clear understanding of those minor touch points that eventually lead to conversions.\nThe good news for marketers is that Google Ads is equipped with an Attribution Modeling report that allows you to what the various models say about your data, without actually making a change to your default reporting. This attribution reports lets you compare two different attribution models side-by-side.\nSo what’s the best attribution model to choose from? The answer to that is… it depends! Each industry and brand is different so there is no golden rule on what to do. Using these attribution models will help you identify which parts of your campaigns are contributing to overall revenue and even though they may not be bringing in direct conversions.']	['<urn:uuid:f4aa27bc-e573-42eb-90f4-470d0551e53c>', '<urn:uuid:7601b677-deab-4030-bca8-3577b9818d95>']	open-ended	direct	short-search-query	distant-from-document	comparison	novice	2025-05-12T18:05:20.075489	7	81	2654
60	I need IT help at work - how much does each support ticket cost?	The cost per ticket is calculated monthly by dividing the total operating expenses (including salaries, benefits, and related expenses) by the total number of tickets or contacts handled during that period.	['Understanding how well your service management initiatives are translating to business success is fundamental to planning your ITSM roadmap. The measurements you take need to give you direction for your improvement activities—so make sure you’re measuring the right things.\nIn this article, I look at the seven most commonly used metrics and KPIs that are used to indicate the quality of service management performance, how these can be measured, and the value they provide. I also suggest some new measures that provide insights into the way we manage modern IT services.\nThe traditional measures we will discuss here are:\n- Service availability\n- Time to resolve\n- First-call resolution rate\n- SLA breach rate\n- User/customer satisfaction\n- Cost per contact\n- Net promoter score\nThe measures I would like to add into the mix are:\nMost IT organizations define availability for IT services. Availability is typically determined by:\nAvailability is most often calculated as a percentage. This calculation is often based on agreed service time (as defined in the SLA) and downtime.\nI do have some issues with the way many organizations report on availability. Most SLAs will have a percentage of allowable downtime. Imagine this is two hours per month. A major outage could easily eat up all the downtime specified in the SLA—generally customers will understand in this situation.\nBut think about the situation where a critical service has constant, but short, interruptions. If the same service is unavailable 60 times each month for 2 minutes at a time, the customers will likely be less impressed as each of these periods of service interruption represents a disruption to productivity.\nTip: Ensure that your reporting represents not just the total downtime for the month, but also the number of service disruptions that your customer experienced during the same period of time.\nTime to resolve\nThe mean time to resolve (MTTR) metric generally gives the average time taken to resolve an incident, once it is reported to the service desk. This is likely to be broken down by priority. This metric is closely tied to customer satisfaction: the faster you resolve issues, the faster your customer can get back to work.\nYou can make MTTR more meaningful by linking it more specifically to:\n- Business units\nI caution, however, that this metric can be easily ‘gamed’. If calls are closed too quickly, without confirming with the customer that everything is working as expected, the MTTR may look good—but your customers will not be happy.\nThis leads to the phenomenon known as watermelon metrics. Your dashboards shine green, but when you cut it open, customer satisfaction is in the red.\nTip: Incentivising improvements in MTTR numbers can be counterproductive.\nFirst contact resolution rate\nFirst Contact (Call) Resolution (FCR) rate is the percentage of issues properly resolved during the customer’s first call, therefore eliminating any subsequent calls or interactions on the issue.\nFCR rate is a valuable KPI, firstly because it results in a happy customer with no further action required.\nBeyond that, there is a compelling business case for this metric. Service desk resources are far less costly than Level 2 and Level 3 support. The cost of an escalated call can be several times that of a call that the service desk is able to resolve on first contact.\nTip: This is another metric that can be gamified. Like MTTR, closing calls quickly—without assurance the issue is completely resolved—contributes to customer dissatisfaction.\nSLA breach rate\nA service level agreement (SLA) breach rate is the percentage of tickets that have broken, or breached, an SLA. For instance, if a critical ticket was resolved in eight hours instead of five hours, as required by an SLA, this ticket has breached the SLA.\nInsufficient staffing and incapability of your staff are the two main culprits of SLA breach. Increased SLA breach numbers often leads to adding more training or more staff to prevent future breaches.\nTip: Regular breaches of SLAs can indicate that the SLA is unrealistic. There may be a case for going back to the customer to renegotiate or revisiting the underpinning operational level agreements (OLAs) to ensure they properly support the SLA agreed with the customer.\nEnd-user satisfaction is typically measured through a periodic survey or other feedback mechanism. Without measuring user satisfaction, a service desk can sometimes find itself focused too much on KPI—and not enough on the business outcome that the KPIs are intended to measure.\nThis is another area where you may notice the watermelon effect. Service management KPIs are all looking good, everything on the dashboards is glowing green, but those pesky customers are still not happy. When this happens, it is a good indicator that you’re either:\n- Measuring the wrong things\n- Analysing with the wrong lens\nTip: The primary purpose of service management is to provide services that customers and users are happy with, meet business expectations, and enable the organization to make progress towards its vision and mission. If your customers are not happy then it does not matter how good your other metrics look!\nCost per ticket\nCost per ticket (aka cost per contact) is essentially the dollar amount of operating expense of the Service management organization divided by ticket or contact volume. Typically, this is calculated on a monthly basis. Operating expenses often include:\n- Salaries and benefits for personnel\n- Other related expenses\nWhile this metric will not impact on the service your customers experience, it is an important business measurement. Understanding the cost of support and assessing this in conjunction with other metrics such as customer satisfaction will give an understanding about whether the organization is under- or over-investing in support.\nTip: A very high customer satisfaction score along with a high per-ticket cost may indicate that you are spending too much on support. Consider reducing or redeploying some resources.\nNet Promoter Score\nNet Promoter Score (NPS) is a proven way of gauging—and improving—customer satisfaction and loyalty with a product, service, or company. One type of customer satisfaction scoring, NPS attempts to make customer satisfaction scores understandable by focusing on a single statistic.\nYou’ve likely taken an NPS survey if you’ve been asked the question, “On a scale of 0 to 10, how likely are you to recommend us to a friend or colleague?” NPS is calculated by subtracting the percent of low-satisfaction customers (scored 6 or below) from the high-satisfaction customers (scored 9 or 10), ignoring neutral scores (7 and 8).\nTip: NPS can be a useful KPI for ITSM organizations when considered alongside other critical metrics.\nMeasuring the number of calls reopened—because you closed the incident before fully resolving it—is a good way to peel back the skin of the watermelon. If you are going to measure first call resolutions and time to resolve, then this metric becomes critical as it deters agents from being in too much of a rush to get their tickets closed.\nAim for complete satisfaction on an issue. Calls should only be closed when the person working on the issue is completely satisfied that it has been fixed.\nTip: A customer calling the service desk again to complain that you prematurely closed a call will have a far greater impact on customer satisfaction than a slower, but complete, resolution would have.\nKnowledge articles published\nSelf-service knowledge articles are a newer but increasingly necessary tool. Improve customer satisfaction and reduce the service desk’s workload by giving customers the tools they need to resolve their own IT related issues with clear and concise knowledge articles.\nNew entrants to the workforce have a high level of digital literacy. They expect to be able to resolve most IT issues themselves. In their home lives, they will search for answers to IT problems online, so it makes sense to give them the tools they need for self-service in the office environment. (Of course, it does require some knowledge management.)\nTip: Measuring the number of knowledge articles that are published over a period of time, and seeing that number increase, will give a good indication of the health of your knowledgebase. There are obviously other important measures of knowledgebase health but start with this one.\nEnd user knowledge article access\nThere are several things you can learn from this metric. Understanding how often your customers use the knowledgebase articles you are publishing will give an indication of the effectiveness of the information you are providing.\nIf articles are not being used, then it may simply be a matter of providing guidance and educating your customers about what self-help is available to them. After all, if they don’t know it’s there, they can’t use it.\nTip: Another useful insight that can be gained from this metric is how often particular issues arise. This metric can be valuable in the problem management space. While customers may be able to resolve an issue themselves, that issue will still contribute to a loss of productivity—which may warrant problem management intervention.\nChoosing your metrics wisely\nMetrics are, quite obviously, valuable tools that will allow you to manage IT services more effectively. However, they need to be treated with caution, as per Mark Twain’s cautionary statement:\nThere are three kinds of lies: lies, damned lies, and statistics.\nIt is very easy to game metrics, or to interpret them to support your own selected narrative.\nThe best advice I can give, which I’ve written about in detail, is this: Take a well-balanced selection of metrics that will allow you to see a holistic view of the ITSM landscape. Never view a single metric in isolation.\nRemember that the final word belongs with your customer—their perception of how well you are performing is really the only one that matters. All other metrics are there to support the customer experience. If your metrics say you are doing well, but your customer disagrees, you need to take a closer look at what you are measuring.\nTo go deeper on ITSM metrics and KPIs, explore the BMC Service Management Blog and these articles:\n- Introduction to Critical Incident Response Time (CIRT): A Better Way to Measure Performance\n- MTTR Explained: Repair vs Recovery in a Digitized Environment\n- Pitfalls of Choosing the Wrong IT Service Desk Metrics\n- Top 5 Service Desk Metrics\n- SLAs vs OLAs: Comparing Service and Operational Agreements\n- SLA Best Practices for ITIL, Help Desk & Service Desk']	['<urn:uuid:56ed7b8b-fc92-4f8f-8edd-c6f9d99ed07b>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T18:05:20.075489	14	31	1726
61	Which features make a cloth suitable for screen maintenance?	Screen cleaning cloths should be soft, nonabrasive, and lint-free. Microfiber is a popular choice, as its fibers are 200 times thinner than human hair, creating more cleaning surface area than regular cloth fibers. It's important to avoid all-purpose cloths, rags, paper towels, and other abrasive fabrics, as these can scratch or damage screens. It's best to keep a designated screen cleaning cloth to prevent picking up sharp debris that could damage the screen.	['Electronic screens get pretty grimy from day-to-day use. From greasy fingerprints to dust and dirt, our screens can collect distracting and unsightly smudges. A screen cleaner can help clean delicate and expensive devices. Screen cleaners safely clean your device without damaging the screen or protective coating.\nThe best screen cleaners get rid of fingerprints, dust, and other grime. They remove smudges and oils without leaving streaks or residue behind. From sprays, wipes, dry cloths, and more, these recommendations may help you find the best screen cleaner for your needs.\n- BEST OVERALL: WHOOSH! Screen Cleaner Kit\n- BEST BANG FOR THE BUCK: Calyptus Screen Cleaner Spray Kit\n- BEST SPRAY KIT: Tech Armor 120 ML Pro Cleaning Kit\n- BEST PORTABLE WIPES: ZEISS Mobile screen wipes 30ct Pouch\n- BEST MICROFIBER CLOTH: MagicFiber Microfiber Cleaning Cloths\n- BEST FOR KEYBOARD: OXO Good Grips Sweep & Swipe Laptop Cleaner\n- BEST FOR VR HEADSETS: Nikon 7072 Lens Pen Cleaning System\n- BEST FOR TRAVEL: i-ScreenClean 3-Pack Touch Screen Cleaners\nWhat to Look for When Buying the Best Screen Cleaner\nElectronic devices have different screen materials and protective coatings, so it’s best to check the manufacturer’s recommendations when choosing a screen cleaner. In general, steer clear of abrasive cleaners or harsh chemicals. Using the wrong cleaner can cause cloudy patches, scratches, or other damage. The best screen cleaners won’t damage your screen’s anti-glare, waterproof, or fingerprint-resistant coating.\nSize/Volume of the Screen Cleaner\nScreen cleaners come in a range of sizes and formats, from travel-size wipes to large spray bottles. Consider the size of the device you clean most often. A small wipe might be the right size to clean a phone screen but much too small to clean a flat-screen TV, whereas microfiber cloths are generally an appropriate size for most devices.\nPortability is another aspect to consider. If you clean your screens on-the-go or want a space-saving option to keep at work, look for compact or individually wrapped products. There are also travel-size options that are ideal for slipping into a carry-on, work bag, or purse.\nIt’s best to steer clear of harsh ingredients that can damage delicate screen coatings. Cleaning ingredients like ammonia and bleach can damage anti-glare, waterproof, or fingerprint-resistant coatings. Household cleaning sprays (including Windex) are not formulated for electronic screens.\nYou can easily avoid this problem by choosing a properly formulated screen cleaner. Alcohol-free and ammonia-free cleaners are preferable. However, some electronics manufacturers approve the use of alcohol products for occasional disinfection. But for regular cleaning of fingerprints and smudges, it’s best to reach for a gentler screen cleaner.\nScreen cleaners come in unscented or scented versions. Physical cleaners, like microfiber cloths and dust brushes, are unscented. For those who have fragrance allergies or sensitivities, physical screen cleaners are an ideal choice.\nMany sprays and wipes don’t have added fragrance, but like any cleaning solution, they have a scent from the ingredients. The smell will usually dissipate quickly after cleaning.\nThere are also a handful of scented options available. These can impart fresh scents like mint or calming scents like lavender. A pleasant scent can make cleaning your device feel more enjoyable and leave your device smelling fresh.\nScreen cleaning cloths come in a variety of different materials and sizes. The important features to look for are soft, nonabrasive, and lint-free materials. Microfiber is a popular choice, but there are also suede and other smooth, nonabrasive options.\nSome cloths can be used with or without sprays. They can come in a range of sizes, in multipacks, or as cleaning pads or tools, which some find easier to hold.\nIt is best to avoid all-purpose cloths, rags, paper towels, and other abrasive fabrics. Although these may feel soft to the touch, they can have a rough texture that can scratch or damage your screens. It’s best to keep a designated screen cleaning cloth. This prevents the cloth from picking up sharp debris that can damage the screen.\nOur Top Picks\nGrimy, dusty, and fingerprint-coated device screens can be annoying and unsightly. But don’t just reach for any old cleaner, as improper cleaning can cause clouding, scratches, and damage. The following screen-safe cleaners are designed for safe and effective cleaning. They remove dirt and oils without damaging the screen or protective coating.\nAs the chosen screen cleaners in Apple stores worldwide, WHOOSH! works on all electronic devices, from smartphones to TVs. This kit includes three microfiber cleaning cloths and two Screen Shine cleaning sprays. One spray is a large 3.4-ounce size and the second a 0.3-ounce travel size. The travel-size spray is the ideal for sliding into a work bag or carry-on. These Screen Shine sprays are designed to make your device screens shine with a streak-free clean, almost like new.\nThe sprays are manufactured in North America and are free from alcohol, ammonia, dye, and fragrance. The microfiber cloths are treated with an antimicrobial agent to prevent odor. The clothes can be washed up to 50 times and still maintain their softness and streak-free clean, as long as you do not use a fabric softener or put them in the dryer.\nThis affordable screen cleaning set from Calyptus includes two 4-ounce screen sprays and two 8-inch by 8-inch cleaning cloths. With 8 ounces of spray total, there is plenty of cleaner to wipe down all your devices with this complete kit. The powerful, plant-based cleaning spray is safe for all screens, from phones to 4K, OLED, and LED TV screens. The spray is natural, nontoxic, and fragrance-free.\nThe two Calyptus Pixel Shining Cloths are made of microfiber and are antistatic, low-friction, and lint-free. They offer a streak-free clean with no residue. These large cloths make this set a good pick for cleaning devices like computer and television screens, too.\nThe complete Pro Cleaning Kit from Tech Armor includes an array of screen cleaning products. This kit includes two TechClean Gel Screen cleaner sprays, 20 cleansing wipes, and two microfiber cloths. The sprays come in one 2-ounce size and one 4-ounce size. The variety of products allows you to keep your screen clean at home or on the go.\nThe products in this kit are free from alcohol and ammonia. They are safe for all electronic screens, including LCD, LED, AMOLED, Retina displays, and more. The spray and wipes contain Tech Armor’s TechClean Purify Go formula. This formula cleans screens and leaves a clear-coat finish, which makes your screens smoother to the touch.\nPioneering optics manufacturer ZEISS has spent over 170 years learning about lenses and how to clean them. They channeled their experience into creating these effective and gentle mobile screen wipes. Their safe but powerful formula removes dirt, fingerprints, and grease without damaging your screen. The wipes are premoistened and quick drying.\nEach wipe is individually wrapped to stay moist. They are small and portable to offer a deep clean anytime and anywhere. While they are well suited for travel, the wipes can also be used at home on computer screens, tablets, and TVs. These useful wipes leave a smudge-free and streak-free finish.\nMagicFiber’s ultrasoft cleaning cloths are made from high-quality microfiber material. The fibers are 200 times thinner than a human hair. The thin fibers create more cleaning surface area than regular cloth fibers. This ultrafine material absorbs and removes oils, dust, and dirt, without the need for extra cleaners.\nThis two-pack of microfiber cloths is affordable and versatile. Each cloth measures 6 inches by 7 inches. They are safe to use on any screen, from phones to TVs, and leave no streaks, lint, or scratches behind. To keep the cloths as good as new, each pack of MagicFiber cloths includes a polybag for storage and protection. Each cloth is washable and reusable for long-lasting cleaning.\nThis nifty device from OXO is useful for keeping your computer clean. The double-sided tool includes a microfiber pad on one end and a soft brush on the other. The microfiber pad is ideal for cleaning fingerprints, smudges, and grime from screens. On the other end, the soft brush sweeps away loose dirt and dust from keyboards.\nThe brush is retractable, and the microfiber pad includes a cover to keep it protected when not in use. When the brush is retracted, this handy tool measures 2.75 inches by 2.75 inches. The small size makes it a well-suited option to take to work or while traveling. While it is designed for laptops, it can also be used on a range of electronics, including phones and tablets.\nThis lens pen from Nikon can help clean inside the nooks and crannies of a VR headset. Originally designed for delicate camera lenses, this lens pen safely removes oils, dust, and other debris. It’s gentle enough to use on a range of optical products. The dual-sided device includes a cleaning tip with a nonliquid cleaning compound. The compound is on a chamois tip to effectively clean away smudges. The other end is a soft retractable brush that sweeps away dust and debris. The cleaning tip cap and retractable brush ensure each end is protected during storage.\nThe narrow pen design can help with detailed and gentle cleaning of the lens of a VR headset. The small cleaning tip allows you to reach spots that may be tough to clean with a regular microfiber cloth or cleaning wipe.\nThese handy screen cleaner balls are a convenient choice to bring on the road. With no liquids or multi-item steps, these screen cleaners are a simple solution for travelers. Each ball has a terry cloth side and a microfiber side. The terry cloth side removes dust and fingerprints. The finer microfiber side removes stubborn smudges and marks. There are no sprays or other cleaners needed; just wipe the soft cleaner ball across the screen.\nThese screen cleaners work on any device, but the small size makes them ideal for phones, tablets, and laptops. The package includes three screen cleaners (two small and one medium) and a keychain attachment. The liquid-free cleaning and optional keychain make these ball screen cleaners especially convenient for traveling.\nFAQs About Screen Cleaners\nScreen cleaners are a simple and affordable way to keep your devices free from fingerprints, dust, and smudges. They offer an effective clean without damaging the fragile screen or screen coating. A proper screen cleaning method can prolong your device screen lifespan and keep it looking like new. Here are a few questions to consider when using a screen cleaner.\nQ. Are disinfecting wipes safe for cleaning screens?\nIt is best to check with the device manufacturer to confirm which options are safe for your device. Some disinfecting wipes may be safe to use, but it’s not wise to assume they all are.\nQ. What is screen cleaner made of?\nLiquid screen cleaners are water-based and often include a surfactant to remove smudges from screens. Fabric screen cleaners are most commonly made of microfiber.\nQ. How do you properly clean a screen with screen cleaner?\nCleaning wipes, dry cloths, and other cleaning devices can be used alone to gently wipe the screen. If using a spray cleaner, spray the solution on a screen cleaning cloth and then wipe the screen.\nQ. Can you use vinegar to clean a computer screen?\nVinegar may cause damage to some protective screen coatings. Unlike with household chores, like scrubbing surfaces and window screens, it is best to avoid vinegar on electronic screens.\nQ. What should you avoid when cleaning an LCD screen?\nAvoid harsh chemicals like ammonia and alcohol when cleaning delicate LCD screens. Also, avoid using abrasive cloths. It is best to use a soft, lint-free cloth (like a microfiber cloth).\nQ. How do you ensure that you don’t leave streaks after cleaning a screen?\nIf using a liquid cleaner, ensure that the cloth or wipe is not too wet. A dry microfiber cloth can be used after the initial clean to gently polish out any streaks.']	['<urn:uuid:ed1cbf65-9b41-4268-a913-12626be3b302>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T18:05:20.075489	9	73	1983
62	How do internet addiction and cyber espionage affect daily life?	Internet addiction affects daily life by disrupting personal relationships, work, and education, causing physical symptoms like poor hygiene and vision problems, and emotional issues like mood swings and time management difficulties. Meanwhile, cyber espionage impacts daily life through massive theft of industrial secrets and personal information, affecting businesses and government agencies, with Chinese hackers targeting everything from defense contractors to Olympic committees, leading to significant economic and security consequences for Western societies.	['Cyber-Espionage and China’s Dream of National ‘Return’\nIs the PRC “winning” the cyber-espionage competition?\nMay 2, 2013 - 1:05 am\nCyber-attacks upon American computer networks, and the theft of massive amounts of information by means of cyber-espionage — both against private industry and against the U.S. government itself — are very much in the news of late, and the People’s Republic of China is increasingly being fingered as the culprit.\nSince the mid-2000s, Western cyber-security experts have been reporting a dramatic rise in cyber-attacks apparently originating in China, a phenomenon that has come to be known in such circles as the “advanced persistent threat” (APT). The office of the U.S. government’s National Counterintelligence Executive recently reported that “Chinese actors are the world’s most active and persistent perpetrators of economic espionage.” China hardly has a monopoly on modern cyber-espionage against Western targets, but Chinese hackers are acquiring an unequaled notoriety from garden-variety industrial espionage, to attacks on U.S. defense contractors and government entities, to intimidation and message-control games such as the cyber-attacks on the New York Times after it ran an embarrassing exposé of apparent corruption in the family of then-Chinese premier Wen Jiabao.\nBecause of the ease with which cyber-attackers can conceal their points of origin, however, it has always been difficult to “prove” what has nonetheless seemed ever more clear to cyber-security experts for several years — namely, that the Chinese government is itself responsible for much of the APT, directly orchestrating such attacks itself, enlisting cyber-“privateers” to invade Western networks on its behalf, or (more likely) doing both of these things. Officials in Beijing strenuously deny any such involvement, and have lately been trying to turn the rhetorical tables by playing to modern China’s well-nursed sense of historical victimization by alleging that a blameless China is in fact the innocent target of malevolent Western cyber-campaigns.\nIn the past couple of years, however, notwithstanding the difficulty of attributing the origin of attacks undertaken in cyberspace, cracks have been appearing in Beijing’s wall of denial. In 2011, the web security company McAfee released a report detailing the results of its own efforts to trace a series of cyber assaults back to a cyber-attack command-and-control server in China used by an entity that McAfee experts nicknamed “Shady Rat.”\nPerhaps the most interesting thing about the Shady Rat episode is the window it seemed to provide into likely PRC state sponsorship of APT attacks. The McAfee team decorously declined publicly to point fingers in this regard, but having themselves essentially hacked back in to the attacker’s computer, the McAfee experts downloaded and published the logs of Shady Rat’s cyber-attack targets since mid-2006. The list is illuminating, for in addition to containing a good many of just the sort of potentially lucrative industrial espionage targets one would expect a freelance, organized crime, or corporate cyber spy to pursue, Shady Rat’s target list included a range of non-remunerative political targets of the sort only really likely to be attractive to the PRC regime itself.\nAmong these political targets, in the period leading up to the 2008 Summer Olympics in Beijing, were the International Olympic Committee, various Western and Asian national Olympic Committees, and the World Anti-Doping Agency. China believed the Olympics were an event of inestimable propaganda and political importance and the leadership treated the Games as a sort of debutante ball to mark China’s emergence as a great power after long years of “humiliation” by Western imperialists. They invested enormous energy in trying to ensure that China outshone the rest of the world there.\nAlso targeted by Shady Rat during the period for which McAfee experts downloaded its records were various political non-profit organizations that the Chinese regime dislikes, including one Western outfit devoted to promoting democracy around the world, a U.S. national security think tank, a second U.S. think tank, a major U.S. news organization, the United Nations itself, the Secretariat of the Association of Southeast Asian Nations (ASEAN), twelve U.S. government agencies, some U.S. state and local governments, some U.S. defense contractors, and government agencies in both India and Canada. (Shady Rat apparently never targeted anybody in China, by the way.)\nBeijing’s mask of denial slipped further later that year when a Chinese news documentary about the PRC’s military broadcast on a government-run television channel aired a clip appearing to show a technician actually launching a cyber-attack on a U.S.-based web address belonging to the Falungong spiritual group that PRC authorities detest and have fiercely persecuted since 1999. After Western reporters called attention to the cyber-attack braggadocio in the mere six second of “B roll” footage in the documentary, it was quickly removed from the Internet. The episode, however, provided yet another data point suggesting that the longstanding talk in PRC military and strategic journals about the imperative of preparing for what is termed “informationized” warfare is no longer just talk.\nEarlier this year another important watershed occurred: the first instance of cyber-security experts being willing publicly to identify the PRC regime itself as the point of origin for key components of the APT. Last month, the security company Mandiant released a report publicly identifying a component of the 2nd Bureau of the People’s Liberation Army (PLA) General Staff Department’s 3rd Department — a military outfit awkwardly known as Military Unit Cover Designator 61398 — as probably having been the originator of a series of APT assaults on at least 141 organizations across multiple industries. In fairness, Mandiant apparently only traced the attacks to the Shanghai neighborhood where Unit 61398 is based, but this is generally believed to be a distinction without much of a difference. (As The Economist recently put it, it is unlikely that a well-funded group of professional cyber-attackers are based in the noodle joints that surround Unit 61398’s PLA-controlled high-rise building.)\nMandiant’s report is thus very significant, for it has explicitly injected into the public discourse on cyber conflict what security experts have privately emphasized for years: that our massive data-losses to the usually-anonymous Chinese entities of the APT are in significant part thefts orchestrated by the CCP regime itself, apparently as part of a broad economic, political, and indeed strategic policy devoted to the PRC’s advancement vis-à-vis the West in general and the United States in particular. Such state-sponsorship is not news to policy community insiders, but the PRC’s cyber-strategy is now a subject of open discourse for the first time — and Mondiant’s report has indeed made something of a splash in the media.\nFor those with some familiarity with China’s engagement with the West since the mid-19th century, however, this cyber-campaign is novel only in its use of computer networks as the tools and targets of a technology strategy. Acquiring national power through the borrowing and adaptation of Western technical knowledge has been an enduring and hugely important element of Chinese approaches ever since the Qing Dynasty’s first painful encounters with British military technology in the Opium War. Early diplomatic envoys were transfixed by Western technology from the start — a theme reflected in Chinese travel diaries as early as 1868 — and Chinese officials’ near-obsession with obtaining it, and the power it represented and helped make possible, has been a constant of Chinese interactions with the West ever since.\nIt has been a central theme of China’s narrative of the West for a century and a half that China must learn the West’s technological ways as an essential part of China’s own return to greatness after painful years of “humiliation” at Western hands. Debate has raged, and still rages, about the degree to which China must in this process Westernize — that is, whether it is really possible to acquire Western techniques and Western levels of global power without traducing some “Chinese” essence — but obtaining for themselves the tools of Western-style technological modernity has always been, and remains, a fixation of China’s leadership.\nSustained efforts to seek out foreign knowledge is not traditionally China’s cup of tea, as it were. The only precedent for such outreach in the generally extraordinarily insular Middle Kingdom’s long history can be found in China’s reaction to the arrival of Buddhism from India — in connection with which at least 56 expeditions were sent from China before the 10th century to acquire knowledge from the fountainhead of such sacred wisdom. In an era when this journey required struggling over some of the worlds most inhospitable deserts and the planet’s highest mountains, or alternatively working one’s way in perilous hops around the coastline of Southeast Asia, Chinese scholars and monks traveled eagerly abroad to acquire Buddhist learning for China in the form of sacred sutras that were subsequently translated into Chinese.\nIn the modern era, China has once again embraced the acquisition of foreign learning, eagerly “going to get the sutras” of modernity — and the power that such knowledge has been seen to impart since the Qing’s painful encounter with Western imperialism — by seeking out foreign technology. Whether this has occurred through the dispatch of Chinese students to foreign universities, quasi-capitalist trade and business relationships characterized by vehement Chinese insistence upon technology-transfer provisions, the acquisition and reverse engineering of foreign arms and military technology, or rampant intellectual property theft and cyber-espionage, the deliberate and systematic acquisition of technology has been a central plank of modern China’s encounter with the non-Chinese world in the 19th, 20th, and 21st centuries.\nSo this is the historical context in which the most recent revelations about PRC cyber-espionage must be seen: this is no mere passing phase, but merely one manifestation of a powerful continuing theme. There is much more at issue than merely Chinese entrepreneurs’ aggregated desire to nick industrial secrets in a rapidly growing “Wild West” economy still lamentably unconstrained by the rule of law. The continuum of acquisition, from legitimate means to outright theft, is a deep part of Chinese strategic policy, and has been for generations. It is inextricably bound up with modern China’s obsession not merely with national “rise” but with a notion of “restoration” or “return” to first-rank power and status that is inherently competitive and zero-sum in its conceptual underpinnings. Acquiring the technological “sutras” of modern power is felt to be a precondition for China’s great dream of return, and the Middle Kingdom is not picky or squeamish about its methods. Cyber-espionage is merely the latest variation on this well-established theme.\nHow well is this working? All the evidence so far points to a massive, sustained, and pretty sophisticated Chinese cyber effort to steal technology, trade and industrial secrets, and other intellectual property, and to penetrate information systems across Western high-technology sectors and in the government. As far as can be ascertained by publicly available sources, moreover — and despite recent efforts by PRC propagandists to respond to such reports by depicting China as being the real cyber-victim — this flow largely goes in only one direction. (The PRC regime is really only “victimized” by electronic information flows in the sense that it takes umbrage at the difficulty of controlling the political content of Internet-facilitated communication to and by its citizens, which is a very different issue.) In this sense, one might perhaps say that the PRC is “winning” the cyber-espionage competition.\nWhile its strategic policy of cyber-facilitated theft has clearly helped give the PRC considerable benefits and has contributed to China’s “return,” however, it is not necessarily the case that simply being the better cutpurse is “winning” in the deepest sense. The continuing fixation of Chinese leaders upon technology acquisition and the largely one-way nature of the information flow, in fact, suggest both that the PRC still considers itself to be “behind” the West. (Moreover, we implicitly agree. After all, Western governments don’t seem to regard stealing intellectual property from China to be all that important to our national strategy. This may perhaps have something to do with still not being all that impressed by it.)\nSo while the West seems clearly to have been suffering massive information losses to Chinese cyber-espionage, the very lopsidedness of modern technological cyber theft may thus signal that Beijing does not yet feel that it has succeeded in acquiring the “sutras” of technological modernity it has so long sought from the West. In the bigger picture, therefore, while the embeddedness of PRC cyber-espionage in the great project of “return” may reveal a very problematic strategic intent of zero-sum competition and Sinocentric primacy, the fact that China apparently still feels the need for such theft suggests a continuing undercurrent of insecurity and weakness. Technology acquisition is designed to change weakness into strength, of course, but the feverish pace of ongoing cyber-theft is certainly a signal that China feels it isn’t there yet, and that we are still looked upon as creators and privileged holders of the sacred knowledge, as it were, that Beijing covets.\nNor, I think, is it guaranteed that China will be able to use and benefit to the fullest possible extent from what it steals. During the Cold War, for instance, the Soviets stole a fair amount of Western technology, but with certain important exceptions, they weren’t able to exploit it too well — especially in their broader economy and in ways that augmented their national strength across the spectrum. Modern China is probably much better positioned to do this than the Soviets were, but there are still no guarantees.\nThere is an old saying in English about how giving someone a fish allows him to eat for a day, but teaching him how to fish allows him to feed himself forever. This may provide some insight here. Stealing what another system has learned and developed is an important shortcut, but it is not quite the same thing as being able to make progress on one’s own — which is a more important test of sophistication and advanced modernity. To the extent that it still has to subsist on technological scraps it steals from the tables of more advanced states, China clearly has not yet succeeded in its great project of a national “return” to greatness.\nAnd this evokes to the great 19th and early 20th century Chinese debates over whether China actually can really be said to possess the “sutras” of modernity just by acquiring particular technical skills, and while still clinging to older forms of centralized and authoritarian socio-cultural organization. It may be that in some deep way, sustainable technological and economic modernity requires the adoption of a truly modern socio-economic “operating system” as well: that is, a vibrant, open, and pluralistic way of organizing society so that one can become a source of innovation and brilliance in the world rather than merely a borrower of other’s ideas.\nThis could end up being a big challenge for the Communist Party oligarchy in Beijing, for it may be that authoritarian rule is incompatible with true modernity, and with China’s ultimate success in achieving national greatness.', 'What Is Internet Addiction?\nDo you play video games on the Internet in excess? Are you compulsively shopping online? Can’t physically stop checking Facebook? Is your excessive computer use interfering with your daily life – relationships, work, school? If you answered yes to any of these questions, you may be suffering from Internet Addition Disorder, also commonly referred to as Compulsive Internet Use (CIU), Problematic Internet Use (PIU), or iDisorder. Originally debated as a “real thing,” it was satirically theorized as a disorder in 1995 by Dr. Ivan Goldberg, M.D. who compared its original model to pathological gambling. Since this hoax of sorts, the disorder has rapidly gained ground and has been given serious attention from many researchers, mental health counselors, and doctors as a truly debilitating disorder. Though not officially recognized as a disorder in the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV), its prevalence in American and European cultures is staggering – affecting up to 8.2% of the general population. However, some reports suggest it affects up to 38% of the general population. The widely variable difference in prevalence rates might be contributed to the fact that no true and standardized criteria has been selected for Internet Addiction Disorder. It is researched differently among scientists and mental health professionals. And, it is researched differently across ethnic cultures.\nThe advancement in study of Internet Addiction Disorder has been negatively impacted by the lack of standardization in this area. It has been generally accepted among researchers, however, that Internet Addiction is only a subset of technology addiction in general. As the name states, its concentration is on compulsion with the Internet – as other areas of media addiction can be seen in television addiction, radio addiction, and other types of media addiction. Due to the explosion of the digital age, Internet Addiction Disorder has taken the reigns as the top culprit is technology addiction as of late. The troubling thing about this disorder is that if you are suffering from it, you are endlessly surrounded by technology. In the digital age, the Internet has taken over. Most of what we do, as a general population, can be done on the Internet. Can’t find that shirt you want in the store? No worries – the Internet has it! Need to place an order for pizza? Why call? Complete an online order! Can’t call over a friend to play a video game at 3am when you’re suffering from insomnia and can’t go back to sleep? I bet there’s someone across the globe that is awake and ready to play! That’s, in essence, why this disorder can be so troubling – even treatment-wise. It’s hard to live these days by getting rid of the Internet. We’re always surrounded by it – and for most of us, we use it daily.\nJust because you use the Internet a lot – watch a lot of YouTube videos, shop online frequently, or like to check social media does not mean you suffer from Internet Addiction Disorder. The trouble comes when these activities start to interfere with your daily life. In general, Internet Addiction Disorder is subdivided into varying categories. The most commonly identified categories of Internet Addiction include gaming, social networking, email, blogging, online shopping, and inappropriate Internet pornography use. Other researchers suggest that it is not the amount of time spent on the Internet that is particularly troublesome – rather, it is how the Internet is being used. That is, the riskiness of Internet use can be just as important as the amount of time spent. Do you have a teenager using teen dating sites that could have child molesters lurking on the site? This is risky – and one of the multidimensional aspects of Internet Addiction Disorder. Other identified multi-dimensional risk factors of Internet Addiction Disorder include physical impairments, social and functional impairments, emotional impairments, impulsive Internet use, and dependence on the Internet.\nWhat Causes It?\nLike most disorders, it’s not likely to pinpoint an exact cause of Internet Addiction Disorder. This disorder is characteristic of having multiple contributing factors. Some evidence suggests that if you are suffering from Internet Addiction Disorder, your brain makeup is similar to those that suffer from a chemical dependency, such as drugs or alcohol. Interestingly, some studies link Internet Addiction Disorder to physically changing the brain structure – specifically affecting the amount of gray and white matter in regions of the prefrontal brain. This area of the brain is associated with remembering details, attention, planning, and prioritizing tasks. It is suggested one of the causes of Internet Addiction Disorder is structural changes to the prefrontal region of the brain are detrimental to your capability to prioritize tasks in your life, rendering you unable to prioritize your life, i.e., the Internet takes precedence to necessary life tasks.\nInternet Addiction Disorder, in addition to other dependency disorders, seem to affect the pleasure center of the brain. The addictive behavior triggers a release of dopamine to promote the pleasurable experience activating the release of this chemical. Over time, more and more of the activity is needed to induce the same pleasurable response, creating a dependency. That is, if you find online gaming or online shopping a pleasurable activity and you suffer from an addiction to the Internet, you will need to engage in more and more of the behavior to institute the same pleasurable feeling prior to your dependency.\nThe variable reinforcement effects of Internet addiction is another cause of this behavior. According to the Variable Ratio Reinforcement Schedule (VRRS) theory, the reason why you might be so addicted to Internet activity (e.g., gaming, gambling, shopping, pornography, etc.), is because it provides multiple layers of rewards. That is, your constant surfing of the Internet leads to multiple rewards that are unpredictable. Perhaps your addiction to Facebook provides a multiple and unpredictable layer of rewards in the sense that every time you sign on to read your updates, you get repeated and unexpected good news. Maybe you found out one of your great friends just got engaged. The next time you sign on, you learn another friend just had a baby! Or, perhaps the man you are really interested in just posted an update that he and his longtime girlfriend just broke up. Each sign on gives you unpredictable results that keep you entertained and coming back for more. Certain games, such as MMROPGs (massively multiplayer online roleplaying games) – including World of Warcraft and Everquest may lead to Internet addiction because, in effect, they never end.\nBiological predispositions to Internet Addiction Disorder may also be a contributing factor to the disorder. If you suffer from this disorder, your levels of dopamine and serotonin may be deficient compared to the general population. This chemical deficiency may require you to engage in more behaviors to receive the same pleasurable response compared to individuals not suffering from addictive Internet behaviors. To achieve this pleasure, individuals may engage in more behavior to the general public, increasing their chances for addiction.\nPredispositions of Internet addiction are also related to anxiety and depression. Oftentimes, if you are already suffering from anxiety or depression, you may turn to the Internet to relieve your suffering from these conditions. Similarly, shy individuals and those with social awkwardness might also be at a higher risk of suffering from Internet addiction. If you suffer from anxiety and depression, you might turn to the Internet to fill a void. If you are shy or socially awkward, you may turn to the Internet because it does not require interpersonal interaction and it is emotionally rewarding.\nWhat are the Symptoms?\nSigns and symptoms of Internet Addiction Disorder may present themselves in both physical and emotional manifestations. Some of the emotional symptoms of Internet Addiction Disorder may include:\n- Feelings of guilt\n- Feelings of Euphoria when using the Computer\n- Inability to Prioritize or Keep Schedules\n- No Sense of Time\n- Avoidance of Work\n- Mood Swings\n- Boredom with Routine Tasks\nPhysical Symptoms of Internet Addiction Disorder may include:\n- Carpal Tunnel Syndrome\n- Poor Nutrition (failing to eat or eating in excessively to avoid being away from the computer)\n- Poor Personal Hygiene (e.g., not bathing to stay online)\n- Neck Pain\n- Dry Eyes and other Vision Problems\n- Weight Gain or Loss\nWhat are the effects of Internet Addiction Disorder? If you are suffering from this disorder, it might be affecting your personal relationships, work life, finances, or school life. Individuals suffering from this condition may be isolating themselves from others, spending a long time in social isolation and negatively impacting their personal relationships. Distrust and dishonesty issues may also arise due to Internet addicts trying to hide or deny the amount of time they spend online. In addition, these individuals may create alternate personas online in an attempt to mask their online behaviors. Serious financial troubles may also result from avoidance of work, bankruptcy due to continued online shopping, online gaming, or online gambling. Internet addicts may also have trouble developing new relationships and socially withdraw – as they feel more at ease in an online environment than a physical one.\nHow is it Diagnosed?\nThough it is gaining traction in the mental health field – and recently added to the Diagnostic and Statistical Manual of Mental Disorders as a disorder that needs more research, a standardized diagnosis of Internet Addiction Disorder has not been discovered. This is also a significant contributing factor to the overall variability in the disorder as a whole and wide range of prevalence in the population from 0.3% to a whopping 38%.\nOne of the more accepted diagnostic assessments of Internet Addiction Disorder has been proposed by KW Beard’s 2005 article in CyberPsychology and Behavior. Beard proposes five diagnostic criteria in the identification of Internet Addiction Disorder in the general population:\n- Is preoccupied with the Internet (constantly thinks about past use or future use)\n- Needs to use the Internet with increased amounts of time to gain satisfaction\n- Has made unsuccessful efforts to control, cut back, or stop use of the Internet\n- Is restless, moody, depressed, or irritable when attempting to control Internet use\n- Has stayed online longer than originally intended\nIn addition, Beard (2005) suggests at least one of the following must also be present in a diagnosis of Internet Addiction Disorder:\n- Has jeopardized or risked the loss of a significant relationship, job, educational, or career opportunity because of the Internet\n- Has lied to family members, therapists, or others to conceal their involvement with the Internet\n- Uses the Internet as a way of escaping from problems or to relieve a dysphoric mood (e.g., guilt, anxiety, depression, helplessness)\nIf you have sought help with an Internet Addiction Disorder, you have likely been given a mental test or questionnaire of some sort to assess your dependency on the Internet. The most common assessment tools used to help make a diagnosis of Internet Addiction Disorder include:\n- Young’s Internet Addiction Test\n- the Problematic Internet Use Questionnaire (PIUQ)\n- the Compulsive Internet Use Scale (CIUS)\nWhat are the Treatment Options?\nThe first step in treatment is the recognition that a problem exists. If you do not believe you have a problem, you are not likely to seek treatment. One of the overarching problems with the Internet is that there is often no accountability and no limits. You are hidden behind a screen – and some things that you may say or do online are things you would never do in person.\nThere is debate in the literature whether treatment is necessary in the first place. Some believe Internet Addiction Disorder to be a “fad illness” and suggest that it usually resolves itself on its own. Studies have show that self-corrective behavior can be achieved and successful. Corrective behaviors include software that controls the Internet use and types of sites that can be visited – with the majority of professionals in agreement that total abstinence from the computer is not an effective method of correction.\nSome professionals argue that medications are effective in the treatment of Internet Addiction Disorder – because if you are suffering from this condition, it is likely that you are also suffering from an underlying condition of anxiety and depression. It is generally thought that if you treat the anxiety or depression, the Internet Addiction may resolve in step with this treatment approach. Studies have shown that anti-anxiety and anti-depressant medications have had a profound affect on the amount of time spent on the Internet – in some cases decreasing rates from 35+ hours a week to 16 hours a week. Physical activity has also been indicative of effective in increasing serotonin levels and decreasing dependency on the Internet.\nSome of the more common psychological treatments of Internet Addiction Disorder include:\n- Individual, group, or family therapy\n- Behavior modification\n- Dialectical Behavioral Therapy (DBT)\n- Cognitive Behavioral Therapy (CBT)\n- Equine Therapy\n- Art Therapy\n- Recreation Therapy\n- Reality Therapy\nBecause of the prevalence of the disorder in the general population, treatment centers and programs have started to pop up in the US and across the globe. In some cases, electro-shock therapy was used to wean individuals off the Internet – this method has since been banned. The ReSTART residential treatment facility was started in 2009 in Seattle, WA for pathological computer use. In 2013, a USB-connected keyboard device was created to provide a very low voltage shock to users who visited particular websites. In other places nationwide and internationally, de-addiction centers have been started to aid individuals suffering from Internet Addiction Disorder.\nIn many instances, multimodal treatments have been employed to treat Internet Addiction Disorder. In this method of treatment, if you are suffering from this condition, you might be prescribed both medications and psychotherapy to treat your addiction to the Internet.\nContinued or Questionable Existence?\nThough originally diagnosed as a “hoax” disorder – the increased digital age has propelled us into the Internet age and Internet addiction has become a truly real “thing.” However, many researchers are uncertain of whether Internet Addiction Disorder is a disorder in its own existence or rather a symptom of other underlying conditions.\nCreating an even more problematic interaction is the fact that everything is online nowadays. It’s hard to make a distinction between online and offline worlds. Everything is Internet-based. From ordering food, interacting with friends, playing games, and even watching tv. Adding an additional layer of confusion and distinction is that other digital technology is taking over the world as well – make access to computers even easier. Now, we don’t have to be physically sitting in front of the computer – we can do anything from anywhere with just our phones, tablets, or other electronic devices.\nStill, other researchers question whether excessive Internet use is an addiction or an obsessive-compulsive or impulse-control disorder. Indeed, the Diagnostic and Statistical Manual of Mental Disorders is correct in its acknowledgement that much more research is needed to study this disorder.']	['<urn:uuid:c4776e17-0146-4a50-b7c5-28aaa8846c53>', '<urn:uuid:4521d3a8-4d42-4c7e-ac3b-e2011c4a66cd>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:05:20.075489	10	72	4985
63	What did people do to speed up choosing a new Pope?	In Viterbo, people took extreme measures to pressure the cardinals into selecting a new pope. They threw rocks at the cardinals, cut off their food supply both at the papal palace and their homes, and even removed the roof of the conference room. As a result, early in his reign, Pope Gregory established procedures to hasten future elections by restricting cardinals to spartan living arrangements so they would focus on the selection process.	"['Question: Is it true that the Church went for more than thirty years without a pope during the Western Schism?\nAnswer: No. If anything, the Church suffered from having too many popes during the period that extended from 1378 until as late as 1429. At that time, two, and even three men claimed to reign as pope; and the situation was such that reasonable people could disagree as to which one was actually the successor of Peter. 1 (See the June 1996 ""Q&A"" column for a more detailed explanation of the Western Schism.)\nHowever, one might speak of this period as a papal interregnum (the time between papal reigns) in the sense that a sincere Catholic of the time might have found himself in the position of not knowing which pope to look to as the true head of the Church. This is significant in that it demonstrates the falsity of those who hold that the Church must always be clearly and unambiguously lead by a single visible head. It is historical fact that for a long period people, saints included, disagreed over who was in truth the pope. (St. Vincent Ferrer was the confessor of Benedixt XIII, while Saint Catherine of Sienna advised his rival, Urban VI.) ""The gates of hell will not prevail,"" but indefectibility is not packaged up in the person of the pope. Popes, after all, die and take a while to replace, are sometimes replaced with not so good ones, and occasionally are replaced by too many popes. But the Church is indefectible, and, with the passage of time, puts all such problems behind Her.\nThe longest actual interregnum that we could find began with the death of Pope Clement IV on 29 November 1268 and did not end until the election of Pope Bd. Gregory X on 1 September 1271. In fact, it lasted longer than that because, while papal reigns are calculated from the date of election, on that date the newly elected Tedaldo Visconti was only a deacon and was in the holy land on crusade. He unable to return to Rome and was not ordained priest and consecrated bishop until late March of 1272.\nThe conclave that eventually elected Bd. Gregory at Viterbo is one of the best remembered in history. It demonstrates the urgency among Catholic people to have a pope:\nThe Viterbese grew restless. The conclave was being held at the papal palace instead of the cathedral, and each afternoon when the cardinals finished their deliberations for the day, they returned to their own homes. The Viterbese began to throw rocks at them. When the cardinals went a few days without balloting, the people threw rocks at the palace. The allotment of food to the palace was first cut down, then cut out. At their own houses, the cardinals found the same food shortage.\nChagrinned, the English Cardinal John Tolet commented one day that maybe to roof of the palace should be removed to give the Holy Ghost easier access to the minds of the cardinals. Word of his observation reached the people. Next morning when the cardinals arrived at the palace the roof of the conference room was gone....2\nEarly in his reign, Bd. Pope Gregory established procedures to hasten the conduct of future elections. While not removing the roof so that the cardinals might be rained on, it became the custom to restrict the cardinals to relatively spartan living arrangements so that they might devote their undivided attention to the business at hand.']"	['<urn:uuid:0648bbe9-b48c-42af-a755-a277cc771fc8>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T18:05:20.075489	11	73	586
64	What technology is used to track vehicles in real-time, and how does this connect to rules about managing construction waste materials?	Vehicles are tracked using GPS technology, sensors, and onboard diagnostic codes through a 'black box' that connects via the vehicle's onboard diagnostics port with a SIM card, communicating through cellular networks or satellite communication to a central server. For construction waste management, contractors must follow the Definition of Waste Code of Practice (DoWCoP) regulated by CL:AIRE, which requires implementing a Materials Management Plan to track material movements and ensure proper disposal, with all documentation submitted to the Environment Agency for approval before work begins.	['New legislation on landfill taxes introduced this year places the onus on contractors and developers to demonstrate the responsible disposal and re-use of materials for construction projects. Equally important, the Environment Agency requires advance proof that a management plan is in place ahead of the start of work on site, with retrospective submissions no longer an option. Failure to provide timely proof of compliance can be costly and may delay projection completion, as outlined in our previous article Site Reuse of materials: Be Clear on the Rules\nEnsuring compliance on waste\nDemonstrating where materials will be used and in what measure requires that clients and developers achieve a sufficiently robust cut and fill mass balance. In addition, any surplus, other than naturally occurring ground materials, will need to be disposed of off-site in accordance with the latest waste legislation, including manufactured ground materials.\nThe sustainable remediation of contaminated land and groundwater is regulated through the Definition of Waste Code of Practice (DoWCoP) developed by independent body, Contaminated Land: Applications in Real Environments (CL:AIRE). To comply, works must be carried out under a Materials Management Plan (MMP).\nWhat is an MMP?\nThe Materials Management Plan (MMP) offers a mechanism to site developers to demonstrate excavated materials satisfy the ‘end of waste’ process and that where materials are re-used the necessary treatment and testing procedures are in place. Embodied within the MMP is the remediation strategy, which sets out the criteria for re-use for approval by the regulator, also containing information about the project as well as supporting evidence such as risk assessments and site investigation reports.\nNo work can commence until the document has been submitted to and approved by the Environment Agency, with assessment by an independent qualified person required prior to submission. During on-site works, contractors are required to keep track of material movements and ensure all recovered materials are suitable for reuse in accordance with the sampling and verification regime set out in the remediation strategy. Any changes made during construction must be recorded and reconciled and a final verification report submitted on project completion, with the potential for retrospective auditing extending to two years after conclusion of the project.\nComprehensive in its reach, the MMP does not however remove the need for an environmental permit, which must also be applied for and approved prior to excavation.\nKeeping costs down\nThe reuse of materials offers significant environmental benefits, optimising the use of resources, as well as potential savings for developers by reducing disposal costs and the need for additional materials purchase. Additionally, the MMP covers the transfer of virgin materials to other sites with a similar MMP in place.\nSubmitting permit applications can be a complex process however. Added to this, the documentation and recording required at every stage of the MMP, may appear a daunting and time-consuming prospect.\nWith extensive site remediation expertise, Ecologia can help developers and clients navigate through the legislative minefield to ensure compliance requirements are fully met and reduce the risk of unexpected tax bills.\nLet us help you with your next project – see for more information on our SERVICES.', 'May 15, 2023\nToday, many businesses use telematics to monitor operations and make data-driven decisions. The field has applications for fleet management and transportation logistics, providing real-time insight into the usage and efficiency of vehicles and drivers.\nBut what does telematics mean and could it improve your operations? This article will look at how telematics systems work and explore the pros and cons of telematics for business.\nTelematics combines two scientific disciplines.\nIt enables users to transmit and receive information remotely, typically from multiple devices over long distances.\nIn vehicles, telematics systems use GPS technology, sensors, and onboard diagnostic codes to collect data on things like location, engine diagnostics, and driver behavior.\nA GPS receiver or “black box” that can send, receive, and store data is installed in each vehicle.\nThe telematics system connects via the vehicle’s onboard diagnostics (ODB-II) or CAN-BUS port with a SIM card. It uses an onboard modem to communicate with the company’s central server, transmitting via a cellular network, satellite communication, 4G mobile data, or General Packet Radio Service.\nYou might be wondering, what’s the difference between GPS and telematics?\nTelematics systems include GPS tracking, but have greater data-handling capabilities. Once the information reaches the server, it’s analyzed and processed into usable information and made accessible to other company devices through a secure website or mobile app.\nExamples of telematics for business usually relate to vehicle tracking for delivery and courier services, car rental and leasing companies, and field sales teams.\nIt’s used in supply chain management and logistics, as well as in construction and mining industries to monitor safety.\nMany modern commercial vehicles come pre-loaded with GPS tracking, while smart vehicles already connect to the internet and support Wi-Fi. You can enhance your telematics by integrating it with fleet management software, as well as tools for inventory management, customer service, payroll, and enterprise resource planning (ERP).\nERP works similarly to telematics in that it can offer you real-time insights into different aspects of your business processes. It allows you to analyze performance and identify areas for improvement.\nIn this way, these two solutions go hand-in-hand, so modern businesses can process data to streamline management and gain a competitive edge. Let’s look at a few other benefits of telematics.\nAs we’ve seen, telematics systems can deliver real-time insights into vehicle activity, which enables fleet operators to make improvements where needed.\nFuel cost is a significant expense for any business with its own fleet of vehicles, and telematics helps you find ways to make your journeys more efficient. For example, the system will automatically calculate distances driven and fuel costs for different vehicles and geographical areas.\nAs well as monitoring fuel consumption, you can measure engine idle time and train drivers on how to maximize fuel efficiency. By comparing the statistics for different drivers and vehicles, you can easily see who needs guidance.\nUsing GPS tracking and real-time traffic information, telematics enables you to plan the shortest routes between destinations. Not only does this reduce unnecessary mileage and fuel usage, it’s also better for the environment.\nYou can measure the weight of each load and consider reducing it to further reduce fuel consumption. If you’re tracking freight, you can even access gyroscopic information to detect tipping or rollover of containers, then adjust future loads or packaging to avoid damage to goods.\nCustomers expect strong communication from businesses, whether you’re updating the status of a delivery from your ecommerce warehouse or telling them when their taxi will arrive. With telematics, you can send regular, accurate updates at every stage of the journey.\nSince you’ll know exactly where your vehicles are at all times, you can easily pass this information on to your customers. For instance, during last-mile delivery, you can send the customer a live-tracking link with a highly accurate ETA (estimated arrival time).\nIf a customer calls to ask about delivery, your support team can access the real-time data, too. Telematics also helps you to guide drivers to the exact address, ensuring more deliveries arrive on time, which will improve your CSAT scores.\nAnother benefit of telematics for business is that it helps to keep dispatching schedules running smoothly. Because it’s easy to view vehicle and driver data, you can map out accurate timescales and base scheduling decisions on average speeds.\nWith the ability to track vehicles in real time, you can get an overview of your fleet’s locations and ensure you send the nearest available driver when a new job comes in.\nYou can quickly adjust the route if there’s heavy traffic or a diversion and alert drivers to dangers like icy roads or flooding.\nTelematics devices can also tell you if one of your vehicles is used outside of a designated area. In addition, you can attach GPS trackers to trailers and other non-motorized assets to ensure they’re in the right places—and locate them if they go missing.\nAs well as monitoring fleet activity, telematics gives you vital information on the condition of your vehicles. You’ll be able to run engine diagnostics remotely and spot any issues with battery voltage or fuel leakage, then get them fixed before they turn into expensive problems.\nFrom usage data such as odometer readings and engine run-time, you’ll know when to schedule vehicle check-ups and preventative maintenance. This will help to keep your vehicles operational for longer and guard against potential safety violations.\nWhen your vehicles are inspected regularly, they’ll be more efficient as well as safer for drivers.\nAutomated telematics technology helps you handle payroll and staff management, even if you have hundreds of employees across the globe. You can verify that your employees are receiving the right amount of pay and working the correct number of hours.\nThe system collects accurate data on hours of service (HOS), automating the process so that you don’t have to manually feed this information into your payroll system.\nReducing the administrative burden makes your business more productive and minimizes potential errors.\nTelematics is also useful for staff scheduling. You could integrate it with an HR mobile app to help you track attendance and vacations—and employees can log in to view their shifts, paychecks, and taxes.\nTelematics enables you to identify risky driving behavior and potential fraud. The onboard device can monitor vehicle speed as well as things like harsh braking and rapid acceleration. It can check if drivers are wearing their seat belts and ensure they’re not working beyond their contracted hours.\nMany telematics systems include dashcams, so you can see actual footage of drivers and use it for training purposes.\nYou can also show data to your insurance company to help reduce your premium. Courts often accept video footage as proof, if you get into an accident.\nTracking systems mean drivers can’t submit fraudulent timesheets and fuel card records or use company vehicles for personal reasons. They increase the chances of recovering the vehicle if it’s stolen and you can also combat theft by placing trackers on your cargo.\nSo, what about the disadvantages of telematics for business? There are a few challenges, although the benefits tend to outweigh them.\nIt can be difficult for people to adapt to new tech, especially when they’re used to doing things a certain way. Drivers may object to having their behavior monitored or filmed—they might feel that you don’t trust them or fear being publicly called out on their driving.\nHowever, it’s essential to communicate the importance of telematics to your team and explain how it benefits everyone, especially in terms of safety.\nStaff should be trained on how to use the technology and you could incentivize it by rewarding driver performance.\nIf your drivers use their own cars for work aka grey fleet or take vehicles to their home after a shift, they may worry that you’ll continue to track them outside of working hours. Some telematics providers offer a “privacy mode”, enabling fleet tracking to be switched off when necessary.\nMany business owners have concerns about data security when they sign up for telematics. With information transmitted via cellular or wireless networks, what if it gets intercepted?\nThe answer is to protect your operations by choosing a solution with top-notch security and make sure your other systems are compliant, too.\nHow much does a telematics system cost? It depends on how many vehicles you have and what type they are, as well as how advanced the tech is. You could use basic GPS tracking, but it doesn’t give you the insight that telematics does.\nIf you want a complete telematics solution with all the bells and whistles, it will cost you more—but you should see a much better ROI. The data will reveal hidden costs and show you ways to make savings through efficiency.\nDon’t forget to factor in installation (ideally by specialists), staff training time, and ongoing maintenance costs.\nUsing telematics for business brings some challenges, but the benefits are widespread. You can track your entire fleet at a glance, monitoring the health of vehicles as well as driver behavior. Fleet managers can then use this data and analytics for efficient route planning, saving on fuel costs.\nTelematics also enables you to provide regular and accurate customer communication and improve supply chain visibility. Automated systems boost productivity and reduce errors, while the data insights let you improve driver safety and compliance—and lower insurance premiums.']	['<urn:uuid:7186502c-7718-4cbb-a1a7-2035eccaf9cc>', '<urn:uuid:ab130a5c-d7ea-4873-9d00-d81db9b4c4dd>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T18:05:20.075489	21	84	2073
65	What are the key differences between how property ownership works under a traditional Will versus an e-contract in terms of when the transfer takes effect?	In a Will (Wasiyat), the transfer of property only takes effect after the testator's death, and the testator retains complete control over the property while alive - the beneficiary cannot interfere with the property until after death. In contrast, with e-contracts, the transfer of property/goods takes effect immediately upon formation of a valid contract through offer and acceptance, though actual delivery typically occurs within 30 days of the order unless otherwise agreed. While both are legally binding instruments for transferring property, the timing of the transfer is fundamentally different - posthumous for Wills versus immediate for e-contracts.	"['When we talk about the notion of Will in Muslim Law, we’re talking about a type of deal between two opposing tendencies. The first is the Prophet’s belief that following a person’s death, his property must be handed to his heirs, and that this idea is recognized as divine law, and that any violation of it is considered undesirable. The other is that it is every Mussalman’s moral obligation to make right arrangements for his property after his death.\nA Will or Testament, also known as a Wasiyat, is an instrument through which a person makes a disposition of his property that will take effect after his death.\nWill is defined by Tyabji as the transfer of a right of property in a specific object, a profit or advantage, or a gratuity to take effect upon the testator’s death.\nA Will is distinguished by the fact that it takes effect after the testator’s death and is revocable.\nUnlike any other disposition (such as a sale or gift), the testator retains complete control over the property bequeathed while he is still alive: the legatee or beneficiary under the Will is prohibited from interfering in any way with the legator’s power of enjoyment of the property, including its disposal or transfer (in that case the Will becomes revoked).\nWho can create a Will?\n- A Will can be written by any major Muslim (above the age of 18) who is of sound mind. The Indian Majority Act of 1875 governs the age of majority, which states that a person reaches majority at the age of eighteen (or on completion of 21 years, if he is under supervision of Courts of Wards). As a result, the testator must be 18 or 21 years old at the time of the Will’s execution. The testator must be of sound mind when executing a Will (that is, when it is being written).\n- According to Muslim law, the legator must have a thoroughly disposed mind, which means he must be capable of completely understanding the legal repercussions of his actions not only at the time of the declaration, but also for a long time thereafter.\n- A Will that is performed under the fear of death is lawful, while a Will that is executed after attempting suicide is void in Shia law.\n- A minor is incapable of making a Will (such a Will is invalid), but a Will made by a minor can be validated by his ratification after he reaches the age of majority.\n- A Will obtained by undue influence, force, or fraud is invalid, and the court takes extreme caution in allowing a pardanashin lady’s Will to be accepted. As a result, a legator must execute a Will with his voluntary permission.\n- At the time of creating or executing the Will, the legator must be a Muslim. A Will is only effective after the legator’s death; it is only a statement on the basis of which the legatee may receive the property in the future.\n- Under Muslim law, a Will executed by a Muslim who ceases to be a Muslim at the time of his death is still valid.\nTo whom can a Will be made?\n- A legatee under a Will can be any individual capable of owning property (Muslim, non-Muslim, lunatic, minor, kid in its mother’s womb, etc.). As a result, a bequest can be taken regardless of gender, age, faith, or religion.\n- A legitimate bequest can be given for the benefit of a legal person or an entity (but it should not be an institution that promotes a religion other than the Muslim religion viz. Hindu temple, Christian church etc.).\n- Under Islamic law, a child in a mother’s womb is considered as a live person and hence is a competent legatee under two conditions. To begin with, he must be alive in the mother’s womb at the time of the Will’s pronouncement. Second, under Sunni law, the kid must be born alive within six months after the Will’s execution, and under Shia law, within ten months.\n- Only after the death of the legator does a Will take effect. As a result, an avaricious and impatient legatee may kill the legator in order to get possession of the property as quickly as feasible. A legatee who murders or causes the death of the legator, whether purposefully or inadvertently, is not permitted to take the Will or the property. However, in Shia law, if a legatee unwittingly, recklessly, or accidently kills the legator, he is competent to inherit the property and the Will is considered genuine.\nFormalities of a will:\nNo particular requirements for the execution of the Will are stipulated in Muslim law. When it comes to certifying a Will, the legator’s purpose is vital. In order to be effective, the aim must be plain, unambiguous, and unmistakable.\nWills can be made orally, in writing, or even by gestures.\nA legal Will can also be made by an oral statement. It is not necessary to follow any certain procedure or formality in order to create a Will. It is sufficient to make an oral declaration. However, the burden of proof for such a Will is heavy. An oral Will must eventually be established with high authenticity in terms of date, time, and location.\nThere is no formal form for declaring a Will in writing. Even if the legator does not sign or the witnesses do not swear to the Will, it is legal. It makes no difference what the document’s name is. It will be recognized as a legitimate Will if it has the fundamental qualities of a Will.\nA Will can be formed with gestures under Islamic law. For instance, if a sick person makes an endowment but is unable to speak owing to weakness, provides a broad nod with his head and it is understood what he is attempting to transmit, and then dies without regaining his capacity to speak, the bequest is legal and legitimate.\nSubject matter of will:\nAny sort of property, whether physical or intangible, mobile or immovable, can be the subject of a Will. However, a legator can only bequeath a property in his Will if he owns it at the time of his death.\nIt is necessary for the property to be transferred.\nA property donated under a Will may or may not exist at the time of its execution, but it must be in the legator’s possession at the time of his death. This rule’s rationale is pretty straightforward. A Will takes effect after the legator’s death, and property is transferred to the legatee from the date of the legator’s death, not the date of the legatee’s death.\nA Will takes effect after the legator’s death, and property is transferred to the legatee from the date of the legator’s death, not the date of execution.\nRevocation of will:\nThe legator has an emancipated right in Muslim law to withdraw the Will or any portion of the Will he has executed at any moment. Similarly, he can make a fair addition to the Will.\nA legator can expressly or implicitly revoke the Will.\nAn oral or written express revocation is possible. For example, if a legator bequeaths some of his property to one person and then bequeaths the same property to another person in a future Will, the first Will is immediately annulled.\nIf a legator burns or shreds a Will he has executed, the Will is considered to be expressly cancelled as well. It should be emphasized that just denying a Will does not imply that it has been cancelled. The legator must take an action that expresses his unambiguous desire to revoke the Will.\nAny conduct by the legator that is contradictory to the bequest will cause the Will to be revoked. In other words, an act that results in the annihilation of the bequest’s subject-matter is seen as an implicit revocation of the Will.\nProbate of a will:\nThe Indian Succession Act, 1925 defines a probate as follows: “Probate” implies a copy of a Will certified under the seal of a court of competent jurisdiction, along with a grant of administration to the testator’s assets.\nA Will states a person’s desire for his intentions to be carried out after his death by particular people who are normally mentioned in the Will. The executors are the people specified in the Will to carry it out. A probate is the process of certifying a Will under the seal of a court. Finally, the Will is established and authenticated by probate. A probate is definitive confirmation that the Will was properly executed, is authentic, and is the final Will of the testator.\nWho is eligible to petition for a probate?\nOnly the executors listed in the Will have the authority to apply for probate. The executor must submit an application for probate under the court’s seal, certifying the Will. If there are many executors, probate might be awarded to them all at once or when the application for probate is filed. If no executor is named in the Will, the court will just issue a basic letter of administration rather than a probate.\nHow to apply for probate?\n- An application for probate must be made to the court by the executor. The actual Will must be attached to the application by the executor. The executor must include the names and addresses of the deceased’s lawful heirs in the application so that they can be notified before the Will is probated.\n- The petitioners must normally prove the facts of the testator’s death, which is usually done with the assistance of a death certificate issued by the local authorities. The executors must also prove that the Will presented to the court is the deceased’s most recent Will.\n- Following the submission of the application, it is confirmed, and notices are sent to the legal heirs of the dead informing them that an application for probate has been received by the court. A wide notice is also published, providing anyone the opportunity to protest to the probate grant. The court will issue the probate if no objections are filed. If the court receives objections to the probate being issued, the application becomes a testamentary litigation.', '|Legal Service India - E-Contracts and Its Legality|\n|Legal Advice | Find a lawyer | Constitutional law | Judgments | forms | PIL | family law | Cyber Law | Law Forum | Income-Tax | Consumer laws | Company laws|\n|Latest Articles | Articles 2013 | Articles 2012 | Articles 2011 | Articles 2010 | Articles 2009 | Articles 2008 | Articles 2007 | Articles 2006 | Articles 2000-05|\nToday with the recent advancement in the areas of computer technology, telecommunications technology, software and information technology have resulted in changing the standard of living of people in an unimaginable way. The communication is no more restricted due to the constraints of geography and time. Information is transmitted and received widely and more rapidly than ever before. And this is where the electronic commerce offers the flexibility to business environment in terms of place, time, space, distance, and payment. This e-commerce is associated with the buying and selling of information, products and services via computer networks. It is a means of transacting business electronically, usually, over the Internet. It is the tool that leads to ‘enterprise integration’. With the growth of e-commerce, there is a rapid advancement in the use of e-contracts. But deployment of electronic contracts poses a lot of challenges at three levels, namely conceptual, logical and implementation. In our article we have discussed the scope, nature and legality and various other issues related to e-contracts.\nDefinition: E-contract is a contract modeled, specified, executed and deployed by a software system. E-contracts are conceptually very similar to traditional (paper based) commercial contracts. Vendors present their products, prices and terms to prospective buyers. Buyers consider their options, negotiate prices and terms (where possible), place orders and make payments. Then, the vendors deliver the purchased products. Nevertheless, because of the ways in which it differs from traditional commerce, electronic commerce raises some new and interesting technical and legal challenges.\nFor recognition of e-contracts following questions are needed to be considered:# Whether e-contract is a valid contract?\n# Would a supplier making details of goods and services with prices available on a website be deemed to have made an offer?\n# Whether e-contracts satisfy the legal requirements of reduction of agreements to signed documents.\n# Whether e-contracts interpret, adopt and compile the other existing legal standards in the context of electronic transactions?\nRecognition E-contractsOffer: The law already recognizes contracts formed using facsimile, telex and other similar technology. An agreement between parties is legally valid if it satisfies the requirements of the law regarding its formation, i.e. that the parties intended to create a contract primarily. This intention is evidenced by their compliance with 3 classical cornerstones i.e. offer, acceptance and consideration. One of the early steps in the formation of a contract lies in arriving at an agreement between the contracting parties by means of an offer and acceptance. Advertisement on website may or may not constitute an offer as offer and invitation to treat are two distinct concepts. Being an offer to unspecified person, it is probably an invitation to treat, unless a contrary intention is clearly expressed. The test is of intention whether by supplying the information, the person intends to be legally bound or not. When consumers respond through an e-mail or by filling in an online form, built into the web page, they make an Offer. The seller can accept this offer either by express confirmation or by conduct.\nAcceptance:Unequivocal unconditional communication of acceptance is required to be made in terms of the offer, to create a valid e-contract. The critical issue is when acceptance takes effect, to determine where and when the contract comes into existence. The general receipt rule is that acceptance is effective when received. For contracting no conclusive rule is settled. The applicable rule of communication depends upon reasonable certainty of the message being received. When parties connect directly, without a server, they will be aware of failure or partial receipt of a message. Such party realizing the fault must request re-transmission, as acceptance is only effective when received. When there is a common server, the actual point of receipt of the acceptance is crucial in deciding the jurisdiction in which the e-contract is concluded. If the server is trusted, the postal rule may apply, if however, the server is not trusted or there is uncertainty concerning the e-mail’s route, it is best not to apply the postal rule. When arrival at the server is presumed insufficient, the ‘receipt at the mail box’ rule is preferred.\nConsideration and Performance:Contracts result only when one promise is made in exchange for something in return. This something in return is called ‘consideration’. The present rules of consideration apply to e-contracts. There is concern among consumers regarding Transitional Security over the Internet. The e-directive on Distance Selling tries to generate confidence by minimizing abuse by purchasers and suppliers. It specifies---\n# A list of key points, must be supplied to the consumer in ‘a clear and comprehensible manner.’\n# Written confirmation, or confirmation in another durable medium available and accessible to the consumer, of the principle points.\n# The right of withdrawal enabling consumers to avoid deals entered into inadvertently or without sufficient knowledge, providing for seven-day cooling-off period free from penalty or reason to return the goods or reimburse the cost of services.\n# Performance should be delivered within thirty days of order unless otherwise expressly agreed.\n# Reimbursement of sums lost to fraudulent use of credit cards. It places the risk of fraud on the credit card Company, requiring them to take steps to protect their position.\n# On the other hand, there is also need to protect sellers from rogue purchasers. For this, the provision of ‘charge-back clauses’ and encouragement of pre-payment by buyers is recommended.\n# Thus, this Directive adequately protects rights of consumers against unknown sellers and sellers against unknown buyers.\nLiability And Damages:A party that commits breach of an agreement may face various types of liability under contract law. Due to the nature of the systems and the networks that business employ to conduct e-commerce, parties may find themselves liable for contracts which technically originated with them but, due to programming error, employee mistake or deliberate misconduct were executed, released without the actual intent or authority of the party. Sound policies dictate that parties receiving messages be able to rely on the legal expressions of the authority from the sender’s computer and this legally be able to attribute these messages to the sender.\nIn addition to employing information security mechanisms and other controls, techniques for limiting exposure to liability include:\n1. Trading partner and legal technical arguments\n2. Compliance with recognized procedures, guidelines and practices\n3. Audit and control programmers and reviews\n4. Technical competence and accreditation\n5. Proper human resource management\n7. Enhance notice and disclosure mechanisms and\n8. Legislation and regulation addressing relevant secure electronic commerce issuing.\nDigital Signatures: Section 2(p) of The Information Technology Act, 2000 defines digital signatures as authentication of any electronic record by a subscriber by means of an electronic method or procedure. A digital signature functions for electronic documents like a handwritten signature does for printed documents. The signature is an unforgeable piece of data that asserts that a named person wrote or otherwise agreed to the document to which the signature is attached. A digital signature actually provides a greater degree of security than a handwritten signature. The recipient of a digitally signed message can verify both that the message originated from the person whose signature is attached and that the message has not been altered either intentionally or accidentally since it was signed. Furthermore, secure digital signatures cannot be repudiated; the signer of a document cannot later disown it by claiming the signature was forged. In other words, digital signatures enable ""authentication"" of digital messages, assuring the recipient of a digital message of both the identity of the sender and the integrity of the message. The fundamental drawback of online contracts is that if there is no alternate means of identifying a person on the other side than digital signatures or a public key, it is possible to misrepresent one’s identity and try to pass of as somebody else.\nE-contracts are well suited to facilitate the re-engineering of business processes occurring at many firms involving a composite of technologies, processes, and business strategies that aids the instant exchange of information. The e-contracts have their own merits and demerits. On the one hand they reduce costs, saves time, fasten customer response and improve service quality by reducing paper work, thus increasing automation. With this, E-commerce is expected to improve the productivity and competitiveness of participating businesses by providing unprecedented access to an on-line global market place with millions of customers and thousands of products and services. On the other hand, since in electronic contract, the proposal focuses not on humans who make decisions on specific transactions, but on how risk should be structured in an automated environment. Therefore the object is to create default rules for attributing a message to a party so as to avoid any fraud and discrepancy in the contract.\n1. Bakshi P.M & Suri R.K, Cyber and E-commerce Laws, Bharat Publishing House, edn 1, 2002.\n2. Ryder D.Rodney, Guide to Cyber Laws, Wadhwa & Co. Publishers, edn.1, 2001.\nStandard Form Contracts\nEvidentiary Value of E-Contracts\nVirtual Cheques - A Distant Reality\nThe Contractual Validity of E-Contracts\nE-contracts and issues involved in its formation\nThe author can be reached at: firstname.lastname@example.org / Print This Article\n• Know your legal options\n• Information about your legal issues\nCall us at Ph no: 9650499965\nCopyright Registration Online\nRight from your Desktop...\n*Call us at Ph no: 9891244487\nLegal AdviceGet legal advice from Highly qualified lawyers within 48hrs.\nwith complete solution.\nYour Name Your\nlawyers in Delhi\nlawyers in Chandigarh\nlawyers in Allahabad\nlawyers in Lucknow\nlawyers in Jodhpur\nlawyers in Jaipur\nlawyers in New Delhi\nlawyers in Nashik\nProtect your website\nlawyers in Mumbai\nlawyers in Pune\nlawyers in Nagpur\nlawyers in Ahmedabad\nlawyers in Surat\nlawyers in Dimapur\nTrademark Registration in India\nlawyers in Kolkata\nlawyers in Janjgir\nlawyers in Rajkot\nlawyers in Indore\nlawyers in Guwahati\nProtect your website\nTransfer of Petition\n|Lawyers in India - Search by City|\nlawyers in Chennai\nlawyers in Bangalore\nlawyers in Hyderabad\nlawyers in Cochin\nlawyers in Agra\nlawyers in Siliguri\nLawyers in Auckland\nlawyers in Dhaka\nlawyers in Dubai\nlawyers in London\nlawyers in New York\nlawyers in Toronto\nlawyers in Sydney\nlawyers in Los Angeles\nCheque bounce laws\nLok Adalat, legal Aid and PIL\nAbout Us |\nJuvenile Laws |\nDivorce by mutual consent |\n| Submit article |\nLawyers Registration |\nlegal Service India.com is Copyrighted under the Registrar of Copyright Act ( Govt of India) © 2000-2015\nISBN No: 978-81-928510-0-6']"	['<urn:uuid:ff7e4574-3de1-44fd-99db-dc16b296e488>', '<urn:uuid:46bb986f-bdd4-4843-9d75-4b4311b56570>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T18:05:20.075489	25	97	3514
66	How does Brahman relate to infinity, and how does our brain process visual information?	Brahman is described as the true infinite and immortal reality - nothing else, not even heaven, can be truly infinite. It is omnipresent, existing both inside and outside all things. As for visual processing, our brains handle visual information through specialized centers like V1, which identifies simple forms like lines and edges, while downstream centers (V2-V4) assemble more complex forms, motion and color. This processing occurs similarly in all humans since our genomes are 99.6-99.9% identical, making our neural processing nearly identical regardless of background.	['Cosmos ,Sanatan Dharma.Ancient Hinduism science.\nThe Vedas are the essence of life and the Upanishads are the essence of the Vedas. The Gita is the cream of the Upanishads. This is why we call the Gita, “The essence of all the essences”.\nFor a better understanding of the Upanishads, it is good to go through the Gita and to understand the Gita going through the Upanishads is the best.\nGreat Saint VedaVyas, before penning down the Gita in the epic, The Mahabharata, described the Gita as ‘the summary of all the Upanishads’. So, let us study the Upanishads along with the Gita [the summary of the Upanishads].\n“He who knows this Upanishad [the Truth, the Knowledge of Brahman] shakes off all sins and becomes firmly established in the infinite and the highest Heaven”\n———-The Kena Upanishad: 4: 9\nThere are many verses in various Scriptures which teach that the ultimate goal of a man is to reach the Abode of God, the Heaven and to live an infinite life therein.\nMost of us MISUNDERSTAND it to be the finite heaven which is a recurring as well as a temporary abode.…\nWhenever heaven is qualified with adjectives like—–“Infinite” and “Highest” and phrases like—–“Abode of God”, then this is a metaphorical use which refers to Brahman and not heaven because there is nothing that is “infinite” and “greatest” except Brahman not even the heaven.\nBoth statements i.e. being “infinite” and attaining “immortality” can be true only when It is Brahman. Heaven can neither be “infinite” nor can one attain “immortality” by entering it.\nBy the study of the Upanishads it has been made clear that a man can never become immortal by entering the finite heaven but only by realising his oneness with Brahman. So, definitely, by the METAPHORICAL comparison of Brahman and heaven a message is passed—–\n“Let men craving for finite pleasures of finite heaven know that there exists an Infinite Heaven with infinite pleasures, entering [realising] Which a man becomes immortal and that is BRAHMAN. So, do not crave for a finite pleasure. Do crave for Brahman Who is infinite and never-ending bliss, Who is Heaven of heavens…..Pleasure of pleasures”\nSo, whenever you come across lines like—–“Heaven is the ultimate goal and we are going to live there for infinite period of time”, take this Heaven as Brahman i.e. diffusing in Brahman is the ultimate goal for this is the only way for both the statements “Infinite” and “Immortality” to be true at once. Make sure to let those who misunderstand This infinite Heaven with the finite one know this too.\nA man dying in the ignorance of the Self is condemned to roam in the world through an interminable chain of births and deaths. Therefore, a man should make every effort to attain the goal of life and terminate the chain of births and deaths by realising Brahman.\nLiving and non-living; Brahman is immanent in all. It [Brahman] alone exists. Names and forms are falsely superimposed on It [Brahman] through avidya [अविद्या] and Maya [माया].\nHe who knows Brahman becomes Brahman. The realisation of oneness with Brahman is the teaching of the Upanishads.\nThere is a verse in the BHAGAVAD GITA which says the same as the ISHA UPANISHAD: 5. After all, the Gita is the summary of the Upanishads. They are to be referred together for the best understanding of Brahman [God].\nIt [Brahman] moves and moves not; It [Brahman] is far and likewise near. It [Brahman] is inside all this and It [Brahman] is outside all this.\n———-The Isha Upanishad: 5\nCan you present a verse from the Bhagavad Gita which is similar to the above verse of the Isha UPANISHAD\nHarih Aum! Aum, the word, is all this [i.e. the whole universe]. A clear explanation of it is as follows: all that is past, present and future is, indeed, Aum. And whatever else there is, beyond the threefold division of time—–that also is truly Aum.\n———-The Mandukya Upanishad: 1: 1\nSamsara or the relative world is maya [illusory] and not real. Maya is superimposed on Brahman as Upadhis [limiting adjuncts]. This is why apparently, existence of many atmans is felt, but in reality i.e. when we view it from a perspective beyond maya, Atman is one without a second.\nHe is the sun dwelling in the bright heavens. He is the air dwelling in the interspace. He is the fire dwelling on earth. He is the guest dwelling in the house. He dwells in men, in the gods, in truth, in the sky. He is born [present] in the water, on earth, in the sacrifice, on the mountains. He is the true and the great. ———-The Katha Upanishad: 2: 2: 2\nThere are not different Atmans in different bodies. The same Atman dwells in all bodies. He is omnipresent. There is only one Atman without a second, which is the all-pervasive Self of the universe. Atman, being non-different from Brahman, completely excludes multiplicity and difference. Atman is omnipresent.\nThere is a city of eleven gates belonging to the Unborn ATMAN [Self] of undistorted Consciousness. He who meditates on Him grieves\nno more; liberated [from the bonds of ignorance], he becomes free.\n———-The Katha Upanishad: 2: 2: 1\nThe body is the city, the 11 gates are the apertures—–the two eyes, the two ears, the two nostrils, the mouth, the navel, the two lower organs and the aperture at the top of the head through which the life breath of a yogin goes out at the time of death.\nAs rainwater falling on a mountain peak runs down the rocks in all directions, even so he who sees the attributes as different from Brahman verily runs after them in all directions.\n———-The Katha Upanishad: 2: 1: 14\nThe perception of multiplicity, resulting from ignorance, is the cause of all suffering. As rainwater running down the sides of a steep hill, divides itself into innumerable channels and ultimately becomes lost without serving any fruitful purpose, so also h…e who sees multiplicity of Brahman, becomes lost in the relative world and does not attain Liberation, which is the ultimate goal.\n“Therefore, one should discard the fallacious advice of the misguided logicians who proclaim the multiplicity of Atman and the erroneous counsel of the heretics who teach Its [Atman’s] non-existence, and should realise Atman’s non-duality as taught by the Vedas”\n——Adi Guru Shankaracharya\n“It is through Atman through that one knows form, taste, smell, sounds, touches and carnal pleasures. Is there anything that remains unknown to Atman?”\n———-The Katha Upanishad: 2: 1: 3\n“Oh scion of Bharata, you should understand that I [Ishvar] [God] am the embodied soul [jeevaatma] [जीवात्मा], the knower in all bodies”\n———-Ishvara [ईश्वर] (Vasudeva Shri Krishna) in the Gita: 13: 2\nIt is said that the first signs of entering YOGA are lightness of body, health, thirstlessness of mind, clearness of complexion, a beautiful voice, an agreeable odour and scantiness of excretions.\n———-The Shvetashvatara Upanishad: 2: 13\nForms that appear like snow, smoke, sun, wind, fire, firefly, lightning, crystal and moon, precede the manifestation of BRAHMAN in Yoga practice.\n———-The Shvetashvatara Upanishad: 2: 11\nIf these forms gradually manifest themselves during Yoga practice, one can be sure that the manifestation of Brahman is not very far off. These represent the various stages of mental modifications as one progresses towards the ultimate realisation.', 'No matter where we call home, where we were raised, or what we ate for breakfast, our brains process information pretty much the same as anyone else in the world. Which makes sense—our genomes are 99.6-99.9% identical, which makes our brains nearly so. Look at a landscape or cityscape and comparable computations occur in your brain as in someone from another background or country.\nConsider my recent walk through China’s Zhangjiajie National Forest Park, an inspiration for James Cameron’s Avatar. Some of our first steps into the park involved a 1,070 foot ascent in the Bailong elevator, the world’s tallest outdoor elevator. Crammed within the carriage were travelers from Japan, India, China, the U.S.A., and Korea.\nNo matter our origin, the Wulingyuan landscapedidn’t disappoint: the towering red and green rock formations stretched towards the sky as they defied gravity. Gasps and awes were our linguistic currency while our visual cortices gleefully fired away.\nThe approximately 3000 quartzite sandstone pillars, with their unusual red and green contrasts, mesmerized our visual centers, demanding our attention.\nOne of the brain’s earliest visual processing centers, V1, lies at the middle of the back of our head. V1 identifies simple forms like vertical, horizontal, and diagonal edges of contrasting intensities, or lines. Look at a vertical line, and neurons that are sensitive to vertical lines will fire more quickly; look at a horizontal line, and our horizontal neurons buzz away.\nDownstream visual centers (conveniently called V2, V3, V4) weave together these basic visual forms to create the beginnings of a visual tapestry. More complex forms like squares and circles are assembled; motion and color are added to the picture.\nThe content of visual information—for example, the density and direction of the lines—determines how much of our attention a visual stimulus grabs. Studies have shown that a high density of lines, for example, grabs our attention in what is known as bottom-up orienting.\nThe brain’s orienting network selects important, behaviorally relevant information from what we see, hear, and touch. Imagine walking down Shanghai’s Bund in the cool evening air. As we gaze across the river at the Pudong District’s glowing skyscrapers we choose to shift our attention from skyscraper to skyscraper. This is called goal-directed, or top-down orienting.\nContrast this deliberate shift of attention with the reflexive, almost forced shift of attention when, staring at the skyline, a motorcycle taxi honks, demanding that we acknowledge we’re in his way. This is called stimulus-driven, or bottom-up orienting.\nVisual scenes with busy, crisscrossing lines grab and keep our attention by engaging stimulus-driven orienting networks—those line neurons fire fast enough to collect their slice of consciousness. Bright, bold, or contrasting colors do the same.\nThe combination of towering vertical lines and other-worldly fluorescent lights were the substance of the Bund’s nighttime enchantment, which attracted a global cadre of visitors—visitors that responded with smiles of approval or thoughtful gazes.\nWhether he’s painting trees billowing in the wind, “lost” abandoned buildings, or reeds in a swamp (in which he concealed a cuboid), his pieces strongly feature geometric shapes, lines. In some pieces, he teases you with mere shadows of lines, forcing your brain piece them together as you will.\nFor the last 12 years, Sima has repeatedly painted Shanghai’s most dramatic architectural display of lines: the Jin Mao tower. Completed in 1999, the Jin Mao tower captures traditional Chinese architecture and modern design within an exoskeleton of crisscrossed aluminum pipe. In his series of paintings, Sima details Jin Mao’s nearly 1380 feet of brain-tickling lines.\nWhen I asked how Sima planned each project, he told me that before each painting, he stakes out the tower and snaps a photo to use as a guide. His favorite view is at about 45 degrees from one of the faces:\n“The angle is very important. I take a photo to keep my perspective. I am very careful, like a scientist,” he told me with a grin, “But I do it by eye, by feel, by experience. I reduce the details using the photo to focus my lines and give me perspective. Taking away some details allows me to express emotion, making it more abstract.”\nSima’s trick was to blur the upper edges and set the tower within an intensely solid blue, yellow or red background that evoked awe, turmoil, passion. By subtracting detail and adding a tinge of atypical color, Sima had hit his mark.\nStanding in Sima’s half-basement studio, I wondered if the act of perception, of inference from incomplete information formed the substance of emotion, whether emotional networks were really extrapolation or inference circuits, evolution’s quick solution for patchwork processing. Indeed, while combinations of lines are strong snaggers of attention, scenes that are ominous or emotionally provocative are the most potent mesmerizers.\nSima’s choice of a solid, intense color combined with dense combinations of lines fascinated me. With the help of Baidu’s translation app, we discussed a classic study of visual attention by neuroscientists at John Hopkins University. The researchers wanted to know whether image features like color, intensity (average RGB luminance), and line orientation affected bottom-up attention. They used an eye-tracking task to measure how much each feature attracted participants’ eyes and discovered that, in cityscapes, color intensity and line orientation were most important in luring participant’s brains to attention.\n“Pretty cool eh?” I said. We chuckled at how Sima’s paintings were, in no small fashion, a neuroscience experiment.\nPointing at his head, Sima replied, “Same brain.”\nFurther Reading on Attention\nBarron DS, Castellanos FX (In Press). “Attention Networks.” In: Pfaff DW, Volkow N, Calderon DP (eds.) Neuroscience in the 21 st Century, 2 nd Edition. Springer Press. A primer on how neuroscientists study the attention system and why attention is divided into orienting, alerting, and executive networks. Emphasis is also placed on what and how lesion-deficit, behavioral, and neuroimaging data can teach us about the brain.\nCorbetta, M., & Shulman, G. L. (2011). Spatial neglect and attention networks. Annual Review of Neuroscience, 34, 569–599. doi:10.1146/annurev-neuro- 061010-113731. An in-depth yet highly readable review of attention networks and how they relate to the effects of stroke.\nPetersen, S. E., & Posner, M. I. (2012). The Attention System of the Human Brain: 20 Years After. Annual Review of Neuroscience, 35(1), 73–89. doi:10.1146/annurev-neuro-062111-150525. A more in-depth review of the brain’s attention system, with emphasis on the latest studies.']	['<urn:uuid:7b51447c-dc24-441e-a93f-75db932302f7>', '<urn:uuid:d56cd2c0-fe2e-48fd-860d-54e6f97de611>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:05:20.075489	14	85	2281
67	As someone studying the history of rational thinking, I'd like to know how ancient philosophers viewed contemplation compared to Descartes, and what role did mathematics play in their approaches?	In imperial Platonism, theoria (contemplation) was considered the most noble activity, particularly as developed by Antiochus of Ascalon who restored the primacy of contemplation to Platonism. This theoretical life was valued alongside practical activities in what was called the 'mixed life.' Descartes similarly emphasized contemplative study, advocating for a 'provisional morality' that provided a stable environment for serious philosophical inquiry. Regarding mathematics, Descartes viewed it as a model for achieving certainty in knowledge, using mathematical reasoning as an exemplar for philosophical method. He proposed that since mathematics had achieved the certainty humans desire, it should serve as a model for advancing human knowledge more broadly.	"['Mauro Bonazzi, Jan Opsomer (ed.), The Origins of the Platonic System: Platonisms of the Early Empire and their Philosophical Contexts. Collection d\'Études Classiques 23. Louvain/Namur/Paris/Walpole, MA: Éditions Peeters; Société des Études Classiques, 2009. Pp. 227. ISBN 9789042921825. €49.00 (pb).\nReviewed by Ryan C. Fowler, Knox College\nThis 23rd volume in the Collection d\'Études Classiques series contains seven articles that examine the trend by Plato\'s followers of systemizing Plato\'s philosophy from the 1st c. BCE onward. This process followed a variety of directions and was subjected to a number of philosophical influences, especially Aristotelian, Stoic, and Pythagorean. The result was a broad variety of ""Platonisms"" without strict orthodoxy, a situation that would change only with Plotinus.\nThe editor\'s introduction (p. 1-2) grounds the task as an attempt to determine precisely the contribution and content of ""the key moments that together make up the long history of Platonism."" For the editors, the moment in which devotees of Plato became Platonists and started to look for doctrines that could be organized into a systemized whole is of major importance . This endeavor (""arguably"") started at the end of the Hellenistic era, ""but especially in the early days of the Empire"" (2). Much of what shaped this period, beyond the systemization itself, was due to the fact that many key doctrines were still undecided even after centuries of debate. No less significant were the other forces with which Platonism was confronted: a renewed interest in Aristotelian texts; a renewed Pythagorean tradition; the continued importance of Stoicism; and the integration or rejection of the Hellenistic (New) Academy, the Platonism of which was almost unrecognizable as such (beyond its extreme aporetic stance). What follows is a brief review of each of the studies in the volume.\n1. It is no surprise to see the importance of θεωρία (""l\'idée de contemplation"") in either imperial Platonism (in particular Plotinus), or in the conceptions of philosophy and happiness as expressed by both Plato and Aristotle. Its relatively weak presence in Hellenistic philosophies, Bénatouïl writes, should not lead us to import this characteristic into Platonism in some broader sense, since it would be reductive to imagine that the Hellenistic period had no role in the history of θεωρία/A. Thus Bénatouïl explains the role of Antiochus of Ascalon (c. 125-68 BCE) in the gradual return of the contemplative life to Platonism. To do so, Bénatouïl examines the state of θεωρία (""le statut de la θεωρία"") in Hellenistic Stoicism, which he takes to be Antiochus\' point of departure. Antiochus, by finding that reason is in fact ""eclipsed"" (20) by theoretical and practical activities (contrary to Stoicism), would restore to Platonism the primacy of contemplation as the most noble activity, as thought (la pensée).\nNext Bénatouïl looks at the Didaskalikos of Alcinous as a point of arrival; he compares its positions to those of Antiochus in order to confirm that the latter represents a starting point for imperial Platonism. Bénatouïl argues that Antiochus\' positions on θεωρία constitute real criticisms of the Stoics, and are inspired more by Plato than Aristotle, a view contrary to some current (and ancient, e.g., Cicero) interpretations. What makes the task difficult is reconciling Antiochus\' seeming preference for the theoretical life with Augustine\'s insistence (Civ. Dei 19.3) that the philosopher preferred a third type of life comprised of both theoretical and practical activities (the ""mixed life""). Bénatouïl explains that Antiochus wanted to follow both Aristotle and Theophrastus in stressing the theoretical life, as well as the first Academics (perhaps Plato himself) in valuing the mixed life. Alcinous\' hierarchization of the contemplative and practical lives, then, continues and completes the criticism of Stoicism started by Antiochus in his analysis of the various natural human activities.\n2. Antiochus\' importance to imperial philosophy has been clear since Dillon (1996), Glucker (1978), and Theiler (1930).1 Here, Bonazzi looks to explain the precise nature of Antiochus\' position and philosophical affiliation. Many studies of the philosopher take extreme positions by underplaying either Antiochus\' debt to Plato and Aristotle or the significance of Stoicism to Antiochus\' stance, or overplaying his polemical attitude as signifying either his disinterest in Stoicism or his outright opposition to it.Bonazzi steers a middle course by claiming that Antiochus was neither a closet Stoic nor an anti-Stoic Platonist, but a Platonist concerned with reconciling the two schools. Far from being a simple, slapdash \'eclectic\', however, he was interested in subordinating Stoicism to Platonism. To show this, Bonazzi focuses on the problem of the telos and the doctrine of the passions.\nWas Antiochus interested in apatheia (freedom from emotions, as the Stoics), or metriopatheia (restraint over the passions, as the Peripatetics)? By taking advantage of the polysemy of apatheia and showing that apatheia is in fact eupatheia (49), Antiochus tries to integrate Stoic apatheia into his own brand of Platonism. As speculative as his conclusion must remain, Bonazzi\'s attempt to solve this puzzle is appreciated, especially given the dearth of reliable or consistent evidence. In the end, then, Antiochus\' task fell between Skeptical Platonism and Platonizing Stoicism: how to tackle Stoicism without submitting to it.\n3. Staab\'s purpose is to understand the point of contact between Platonism and Neopythagoreanism during the Empire. He works to correct the early misinterpretation that Platonism fully eclipsed Pythagoreanism at the time. To do so, he looks at Eudorus (1st c. BCE), who begins with a view of the world informed by the dualism of Pythagoras and ends with a decidedly Platonic monotheistic view. (82)\nThe point of departure for the Pythagoreans\' methodology in the Empire is the conviction that Plato\'s ideological structure is rooted in Pythagoras. This approach made it possible to describe all of the teachings within the Platonic tradition as able to be interpreted in Pythagorean models and terms. Thus Staab shows that Pythagoreanism did not ""become"" Platonism, but was an independent sect of Platonism in the Empire. He means to contradict those misinterpretations which both deny a defined Pythagorean current of thought and dissolve Pythagoreanism completely into the Platonism of this period. (58) With this in mind, Pythagoreanism might better be termed a ""denomination"" within the Platonism of the Empire. (56-7)\nStaab\'s bibliography in particular is extensive and profitable.\n4. Eudorus of Alexandria is known for being involved in the two major developments in 1st c. Platonism: the rebirth of dogmatic Platonism and a renewed interest in the treaties of Aristotle\'s school. Chiaradonna studies the extent to which Eudorus is involved with both the construction of a Pythagorean Platonism with an orientation toward theology, and the interaction between Platonism and Aristotelianism. The point of departure for Chiaradonna is Eudorus\' work on Aristotle\'s Categories.\nFor Chiaradonna, if one overemphasizes the fact that Eudorus criticized certain passage of the Categories, an important point is lost: his criticisms relate to details, but never call into question the doctrines of the Categories as such. His work probably aimed to attach the Categories to a brand of Pythagorean Platonism, while correcting some individual elements. Thus, for Chiaradonna, Eudorus was at the origin of the various ""Middle-Platonic"" attempts to incorporate the Categories into Platonism. Several contemporary examples of this approach are found, with important variations, in the work of the anonymous commentator on the Theaetetus, in Alcinous, and in Plutarch.\nIn their own ways, both Eudorus and Porphyry aimed at building a unified philosophical tradition by subordinating the re-examined and corrected doctrines of Aristotle to their own views of Platonism. The few fragments of Eudorus we have, however, are not enough to develop this parallel with certainty. What is more, the oeuvre of the important authors of the 2nd and 3rd centuries, e.g., Alexander of Aphrodisias and Plotinus, on which the complex integration of Aristotelianism and the Platonism of Porphyry is based, has very little in common with Eudorus and his conceptual background.2\nIn short, the first reception of the Categories of Aristotle into the Platonism at the time of Eudorus is indebted to a general trend found between the first century BCE and the first century CE. ""If"" there are any elements of continuity that connect the Platonism of this time to that of later centuries (in particular, in Plotinus and Porphyry), they are definitively not to be found in the uses of the Categories in this period.\n5. Trabattoni helpfully identifies the targets of Philo\'s controversial De opificio mundi 2.7-12: Who is it who admires the world more than its maker? Are these the same men who deny providence? Can one attribute the metaphysical theory in sections 8-9 (i.e., on involving an active, creative cause and passive object) to a particular school? To what extent can it be said that this treatise was influenced by Plato\'s Timaeus?\nTrabattoni works through all the possible answers to these questions systematically and clearly, and his thoroughness is apparent. His argument, therefore, is worth a careful reading. In essence, this section of the De opificio mundi is consistent with the ideas Philo would have come away with after reading the Aristotelian Peri philosophias and the De mundo, which may help identify the men who seem to admire the world more than its creator (or who support a thesis that ""praises the world and blames God""). For Philo the idea of the eternality of the world prevents any idea of providence, which is predicated on the act of creation of the world by God and His continued involvement with it: one leads essentially to the other. This stance is consistent with both Mosaic and (aspects of) Platonic philosophy, since God\'s creative action turns the passive into the most perfect masterpiece, the world (though it is only Moses who has reached ""the very summit of philosophy,"" given the influence of the opening of Genesis). For Trabattoni the impetus of Philo\'s drive to reconcile Platonic with Mosaic philosophy is the fact that Platonism provides the Ideas, which are absent from Genesis, while the Mosaic contribution accounts for their creation, a notion absent from the Timaeus.\n6. Opsomer attempts a philosophical portrait of Plutarch\'s teacher, the mysterious Ammonius (1st c.). He portrays the Egyptian as a mild and tactful instructor who discussed aspects of Platonic theory (e.g., the doctrine of recollection), and the hermeneutic tool of allegorical interpretation of texts. He is represented as reverent to the Delphic God, Being and the One, and (thoughtful) numerology.\nOpsomer gives one important possible difference between Plutarch and his teacher, in that the latter has no interest in the irrational soul, while the student uses it to explain disorder in the world (cf. De animae procreatione in Timaeo), instead of blaming matter\'s penchant for chaos.\nOpsomer helpfully suggests that, instead of looking for specific influences, we should imagine a general tradition of interpreting the theory of flux in a certain way (e.g., leading to a lack of personal identity, which alleviates the fear of death), and that Philo, Seneca, and ""Ammonius""/Plutarch were aware of it. This suggests that Ammonius was not directly involved in a Neopythagoreanism that was independent of Platonism, but rather was guilty of moderate Pythagorean tendencies in a New Academic/skeptical framework. This explanation could account for the various philosophical elements in Ammonius\' famous speech in Plutarch\'s De E.\nOpsomer\'s presentation of all the evidence is exhaustive, and his judgment balanced. His twenty-six numbered conclusions concerning Ammonius at the end of the piece are enormously helpful (174-178); the bibliography is an excellent source for works on Plutarch and Ammonius.\n7. Donini answers some of the questions that surround Plutarch\'s De genio Socratis. While there have been recent attempts to understand how the two topics of Socrates\' daemon and the execution of the anti-Spartan Thebans in 379 are connected in this work, by looking specifically at Epaminondas as a thematic connection, Donini looks to explain Plutarch\'s philosophical point through the characterization of his protagonist.\nBy using both philosophical discourse and historical narrative, Plutarch offers Epaminondas as proof of the unification of two opposing interpretive principles: one construing Plato (and even Aristotle) as a progeny of Pythagoras (or Pythagoreanism), the other presenting Plato as the disciple of Socrates and progenitor of the New Academy. Plutarch was constrained to argue for the coexistence (indeed, a convergence), of these different threads in Platonism and the Academy\'s unification of both traditions (as reflected in the title of his lost work, ""There was only one Academy of Plato""). Pythagorized Platonism and the Socratic-Academic tradition could co-exist readily in Plutarch\'s characterization of Epaminondas.\nRead in this way, Plutarch\'s De genio Socratis contains allusions and novelistic references that transmit a message that has great historic, philosophical, and conceptual value. Plutarch emerges as a true inheritor of his model\'s style: Plato also uses invention and careful attention to detail to help form convictions without necessarily dictating them.\nAt the end of the volume is a reasonable ""Index of Ancient Names"" (including names important to the volume, but no ""Simmias,"" ""Galaxidorus,"" etc.); a helpful ""Index of Modern Names""; and a five-page ""Index Locorum.""\nThe topics of all of these pieces clearly interrelate and help explain aspects of the imperial systemization of Plato. This volume is a welcome addition to the continued effort to push against the unhelpful label \'eclectic\', which can be coupled with dismissal without much investigation. Anyone with an interest in the origins of Platonism at the start of the Roman Empire would do well to study this volume.\n1. Theiler, W. Die Vorbereitung der Neuplatonismus. Berlin 1930.  Dillon, J., 1996. The Middle Platonists , 2nd ed. Ithaca: Cornell University Press. Glucker, J., 1978. Antiochus and the Late Academy, Göttingen: Vandenhoeck & Ruprecht. One may add Tarrant (e.g., Scepticism or Platonism? The Philosophy of the Fourth Academy. Cambridge 1985), whose work seems to be relatively neglected in much current scholarship and might warrant more than a single reference in this volume.\n2. Reading the grammatical referent of elle after the semicolon to be the more remote l\'intégration, rather than the more proximate l\'oeuvre. (108)', '|Philosophy Pages||Dictionary||Study Guide||Logic||F A Q s|\nLife and Works\n. . Method\n. . Animals\n. . Doubt\n. . Cogito\n. . God\n. . Error\n. . Extension\n. . Dualism\n. . Cartesianism\nThe first great philosopher of the modern era was René Descartes, whose new approach won him recognition as the progenitor of modern philosophy. Descartes\'s pursuit of mathematical and scientific truth soon led to a profound rejection of the scholastic tradition in which he had been educated. Much of his work was concerned with the provision of a secure foundation for the advancement of human knowledge through the natural sciences. Fearing the condemnation of the church, however, Descartes was rightly cautious about publicly expressing the full measure of his radical views. The philosophical writings for which he is remembered are therefore extremely circumspect in their treatment of controversial issues.\nAfter years of work in private, Descartes finally published a preliminary statement of his views in the Discourse on the Method of Rightly Conducting the Reason (1637). Since mathematics has genuinely achieved the certainty for which human thinkers yearn, he argued, we rightly turn to mathematical reasoning as a model for progress in human knowledge more generally. Expressing perfect confidence in the capacity of human reason to achieve knowledge, Descartes proposed an intellectual process no less unsettling than the architectural destruction and rebuilding of an entire town. In order to be absolutely sure that we accept only what is genuinely certain, we must first deliberately renounce all of the firmly held but questionable beliefs we have previously acquired by experience and education.\nThe progress and certainty of mathematical knowledge, Descartes supposed, provide an emulable model for a similarly productive philosophical method, characterized by four simple rules:\nWhile engaged in such a comprehensive revision of our beliefs, Descartes supposed it prudent to adhere to a modest, conventional way of life that provides a secure and comfortable environment in which to pursue serious study. The stoic underpinnings of this ""provisional morality"" are evident in the emphasis on changing oneself to fit the world. Its general importance as an avenue to the contemplative life, however, is more general. Great intellectual upheavals can best be undertaken during relatively calm and stable periods of life.\nIn this context, Descartes offered a brief description of his own experience with the proper approach to knowledge. Begin by renouncing any belief that can be doubted, including especially the testimony of the senses; then use the perfect certainty of one\'s own existence, which survives this doubt, as the foundation for a demonstration of the providential reliability of one\'s faculties generally. Significant knowledge of the world, Descartes supposed, can be achieved only by following this epistemological method, the rationalism of relying on a mathematical model and eliminating the distraction of sensory information in order to pursue the demonstrations of pure reason.\nLater sections of the Discourse (along with the supplementary scientific essays with which it was published) trace some of the more significant consequences of following the Cartesian method in philosophy. His mechanistic inclinations emerge clearly in these sections, with frequent reminders of the success of physical explanations of complex phenomena. Non-human animals, on Descartes\'s view, are complex organic machines, all of whose actions can be fully explained without any reference to the operation of mind in thinking.\nIn fact, Descartes declared, most of human behavior, like that of animals, is susceptible to simple mechanistic explanation. Cleverly designed automata could successfully mimic nearly all of what we do. Thus, Descartes argued, it is only the general ability to adapt to widely varying circumstancesand, in particular, the capacity to respond creatively in the use of languagethat provides a sure test for the presence of an immaterial soul associated with the normal human body.\nBut Descartes supposed that no matter how human-like an animal or machine could be made to appear in its form or operations,\nit would always be possible to distinguish it from a real human being by two functional criteria.\nAlthough an animal or machine may be capable of performing any one activity as well as (or even better than) we can, he argued,\neach human being is capable of a greater variety of different activities than could be performed by anything lacking a soul.\nIn a special instance of this general point, Descartes held that although an animal or machine might be made to utter sounds resembling human speech in response to specific stimuli,\nonly an immaterial thinking substance could engage in the creative use of language required for responding appropriately to any unexpected circumstances.\nMy puppy is a loyal companion, and my computer is a powerful instrument, but neither of them can engage in a decent conversation.\n(This criterion anticipated the more formal requirements of the Turing test.)\n|History of Philosophy|']"	['<urn:uuid:bd84cc00-cea0-4544-8625-52852efd1fe5>', '<urn:uuid:33524e9c-29b6-4b89-b813-8e98302851b1>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T18:05:20.075489	29	105	3107
68	does cbt work for anxiety disorders research says	Yes, research strongly supports the effectiveness of CBT (Cognitive Behavioral Therapy) for anxiety disorders. Comprehensive reviews of studies have found that CBT has consistently strong efficacy for anxiety disorders, showing large positive effects on worrying, anxiety, and depression compared to control groups. These improvements were observed both in self-reported measures and clinician-rated instruments.	['Exposure Therapy and Behavioral Experiments\nCognitive Behavioral Therapy is the most validated form of therapy for anxiety disorders. In a huge review of studies exploring the effectiveness of CBT, the researchers found “The efficacy of CBT for anxiety disorders was consistently strong” (click here for a link to a free copy of this study). In another comprehensive review of studies exploring CBT for generalized anxiety, the authors concluded “when compared to waiting list control groups, these treatments have large effects on worrying, anxiety and depression, regardless of whether effects were measured with self-report measures or with clinician-rated instruments” (click here for a copy of this study). Many people would prefer their anxiety just went away but research suggests few people who do nothing, reduce their anxiety. As the studies above suggest, one of the things people can do to reduce their anxiety is cognitive-behavioral therapy and at the core of CBT for anxiety is exposure therapy and behavioral experiments.\nSimply put, exposure therapy involves exposing the client to their feared situations in a gradual, repeated, and prolonged manner until the client becomes desensitized. For example, if a client is terrified of public speaking they may work with their therapist to create a list of anxiety provoking situations (speaking in class, watching videos about public speaking, taking a class on public speaking, imagining speaking in public, speaking in a meeting at work, talking to a cashier at a store, doing a speech at Toastmasters, etc.). The client then ranks these situations from least anxiety provoking to most anxiety provoking, this is called an anxiety hierarchy. Then starting with a situation that does evoke some anxiety but is not overwhelming, the client would repeatedly expose themselves to these situations, over and over, until they become desensitized to this situation. A useful example of this most people can relate to is learning to swim as children. Many children are afraid or at least tentative about the water, but their parents encourage them to become more comfortable over time, often enrolling them in swim classes. As the child is gradually exposed to the water over time, they become less afraid. Alternatively, the children that are terrified of the water, and refuse to ever go in the water continue fear the water.\n*For more information about exposure therapy see my article “Overcoming Anxiety and Avoidance” by clicking here, or download our free anxiety workbook by clicking here.\nBehavioral experiments can take a number of different forms. Similar to exposure therapy, the client and therapist may work together to create a list of anxiety provoking situations. Then the client and therapist will work together to identify the client’s fears about what could go wrong in these situations. Often these fears are exaggerated and extreme but the client sees them as perfectly reasonable. So then we create a little experiment in which the client enters the feared situation and observes to see if their fears come true. Then the client runs the experiment several times in an attempt to gather more information. Once the client has run the experiment several times, they reconvene with the therapist to discuss what they have learned about their fears and their abilities to cope.\nFor example, perhaps one of the situations on the client’s anxiety hierarchy is saying “no” to other people. Perhaps the fear in this situation is something like “If I say no to my mother when she asks me to do something, she will call me selfish, the rest of my family will hate me, and I won’t be able to handle that.” So then the client and therapist run an experiment in which the client talks with their mother, explains they are trying to be more assertive and wants to work on saying “no” sometimes. Then the next time their mother asks them to give them a ride to the store, the client explains they cannot help that day due to other commitments, then observes what happens. In this situation, the client’s mother may very well call the client “selfish” and their family may complain, but the client also learns they can survive being called selfish and occasionally listening to complaints. Or perhaps the client’s mother understands the client has other commitments and they schedule a plan in which the client gives their mother a ride another time, or the client’s mother finds another way to the store. See the below table for an example how I would write-up a behavioral experiment assignment with a client.\nAnother type of behavioral experiment involves the client identifying what they suspect other people believe then conducting a survey to assess the accuracy of the client’s assumptions. For example, the client may believe that people believe that women over a particular weight are “unattractive.” Clients may start by simply ask some trusted friends or family about this assumption. They may take photos of themselves or others to friends or family and ask people about their impressions about the people in the photos. In this example, the client ideally learns that attractiveness is not directly related to something as arbitrary and simplistic as weight.\nHow it works:\nIn my experience, exposure therapy and behavioral experiments have been immensely helpful for clients. Clients usually report dramatic shifts in their anxiety in only a short period of time. In CBT we assume the client becomes less anxious in situations they are exposed to because they learn that their initial assumptions about the dangerousness of the situation is exaggerated and their beliefs about their abilities to cope with the danger posed by those situations is minimized. In other words, their cognitions change. After exposing themselves to a variety of different situations, the client learns they have a habit of exaggerating danger and minimizing their abilities to cope, and so they become less anxious in other situations they have not exposed themselves too.\nFor example if you are anxious about travelling but you muster the courage to go to Mexico, you might learn that Mexico isn’t as dangerous as you expected. Then if you go to Germany you might learn that Mexico and Germany are safe. Then if you go to India you might learn that Mexico, Germany, and India can be travelled to safely. Then you might learn that travelling in general can be done safely, not only to those countries you have been to in the past.\nOne challenge in using exposure and behavioral experiments is explaining the importance of actually facing fears to the client. Some clients have spent decades avoiding situations that make them anxious and the thought of deliberately exposing themselves to these situations is terrifying. People tend to want to avoid situations that make them anxious. Unfortunately, it is this very avoidance which perpetuates the anxiety indefinitely.\nAnother barrier to effective exposure and behavioral experiments is called “safety behaviors.” Safety behaviors can sometimes resemble the compulsions of someone who struggles with OCD. Safety behaviors are unnecessary, excessive, or unhelpful activities or strategies people use in anxiety provoking situations to protect them from “something going wrong.” When the client is anxious about going to a new restaurant they may excessively research the restaurant online to create a plan to prevent something “bad” from happening. In this example the excessive research is the safety behavior the client uses to keep themselves “safe.” Then when they go to the new restaurant and nothing “bad” happens they convince themselves it is because they researched prior to going. In this situation, the client has not learned that they can safely go to a variety of different restaurants, but instead they have learned they can go to a restaurant when they excessively research it in advance. Other examples include holding glasses very tightly when you’re afraid of spilling, distracting yourself with your phone when afraid of standing in lines, and never disagreeing with people when you are afraid of conflict. Research suggests it is imperative safety behaviors are identified so the client can either avoid using them or gradually reduce the use of safety behaviors over time.\nSometimes a client’s anxiety is about situations to which they cannot be exposed. For example, the client might be terrified of earth quakes, someone dying, their son being in a car accident, getting fired, or being homeless. When the client cannot be directly exposed to their feared situations, we have got to get creative. We can watch videos of these situations, we can do research, we can read stories, we can write then recite our own stories in which the client is exposed to these situations, etc. This is based on the idea that thinking about the feared situation can actually desensitize the client. Some of these interventions are called “imaginal exposure.”\nResearch suggests CBT is an effective treatment for anxiety disorders. Exposure therapy and behavioral experiments are core elements of CBT for anxiety. By facing fears the client learns their feared situations are not as catastrophic as originally predicted and their ability to cope is better than expected.']	['<urn:uuid:9592eadd-6db6-42e7-a33c-12a57ef1c170>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T18:05:20.075489	8	53	1500
69	I want to buy some supplements but I'm concerned about quality - how reliable are the ingredients listed on supplement labels at stores?	The quality of supplements is poor, with studies showing that 50% of supplements sampled on shelves contain none of the ingredients listed on the label. Additionally, some supplements are even tainted with prescription drugs.	"['Supplements vs drugs - the birth of a misleading dichotomy\nIn my last email, I illustrated the supplement double standard - a tendency to think that supplements are effective and have no side effects. In this email, I will explore how this perception came to be. Why we erroneously think of supplements as being different than drugs.\nDrugs and supplements defined\nDrugs are defined as compounds that are novel and synthetic. They are synthesized in a lab and not found in nature. Drugs include prescription medications and over-the-counter medications such as Advil or Nexium.\nA supplement, on the other hand, is a medication found in nature - a natural medication. It covers any of the following:\n* vitamins and minerals\n* botanical or herbal products: including algae and macroscopic fungi\n* amino acids\nOne and the same\nApproximately 50% of drugs, however, are derived from natural sources. Aspirin, penicillin, cortisone, statins, taxol, colchicine, and opioids to name a few. This makes the dichotomy of medications into natural (supplements) vs non-natural (drugs) misleading. There is no biological difference between supplements and drugs. They both end up targeting the same biologic systems, they both can be beneficial and harmful, and they both should be treated with equal respect and caution. (I will expound on the biologic similarities between supplements and drugs in my next email)\nThe legal dichotomy of medications\nIn reality, the natural vs non-natural dichotomy was a product of the law. It was about regulation, ownership, and patentability. Supplements are not patentable because they already exist in nature and drugs are patentable because they do not exist in nature. Lets explore why this medication dichotomy became popularized, and how it changed the way we view medications.\nA brief history of DSHEA\nAppreciating their immense complexity and potential, the FDA wanted natural medications to fall under its jurisdiction; to be further researched, scrutinized, and regulated like drugs. The supplement industry wanted deregulation. In the 1990’s the fight was coming to a head. In favor of deregulation, senator Orin Hatch, hatched The Dietary Supplement Health and Education Act (""DSHEA”). With help from industry and a large media campaign the act was passed in 1994 (Watch Mel Gibson in this hilarious commercial from the campaign trying to protect the right to his “vitamins”). This took medications found in nature out of the hands of the FDA, leaving the ndustry to regulate itself.\nDSHEA says supplements are safe\nTo justify deregulation, DSHEA had to declare that supplements are safe. To do this, it granted natural medications with the same status as food. Food is safe and does not need scientific approval to be produced or used, neither should medications sourced from nature. It even gave natural medications a new name, “dietary supplements”, reflecting their new status as food. Now if a company wants to release a new supplement, no safety or effectiveness testing is required. Instead, the supplement company will police itself. It is solely responsible for determining the quality and safety of its products. The fox is guarding the henhouse.\nDSHEA does not allow any disease claims\nThere was a price to pay for deregulation. If a supplement could actually treat a disease, it must be potent and have side effects and should be regulated. Thus, supplements cannot claim to treat disease. This was necessary in order for DSHEA to maintain its declaration that supplements are safe.\nTurmeric cannot claim to prevent cancer. Zinc cannot claim to treat colds. CoQ10 cannot claim to prevent heart disease. In fact, the following disclaimer is supposed to be added to supplement labels and marketing: “These statements have not been evaluated by the Food and Drug Administration. These products are not intended to diagnose, treat, cure, or prevent any disease.”\nUnfortunately, many supplement companies are noncompliant with this restriction. They make specific disease claims or neglect to include the disclaimer. The FTC is unable to police the marketing of some 87,000 supplements in the US - and companies know this.\nDSHEA allows structure/function claims\nInstead of disease claims, DSHEA allows supplement companies to make structure/function claims. Let me explain what this means. When food is deficient, the structure of the body is compromised. Bones don’t grow, muscles atrophy, etc. Thus, one can claim that food “supports” these structures of the body. Likewise, when food is deficient, the functioning of the body is compromised. The heart won’t pump properly, the immune system will not fight off infection, etc. Thus, one can claim that food “supports” these functions of the body.\nWith respect to the marketing, DSHEA requires a new language for supplements. Words like “prevents” or “treats” are to be avoided. Instead they should be replaced by “supports“ or “boosts”. Specific diseases are not to be mentioned. Instead they should be replaced by organ systems. For example, DSHEA prohibits “Our supplement treats arthritis”; but would allow, “Our supplement supports joints”. DSHEA would prohibit, “Our supplement prevents the flu”, but would allow ,“Our supplement boosts the immune system”.\nIt turns out, the ambiguity of the structure/function claim works to the advantage of supplement companies. It has led to a supplement language that is far more appealing and casts a wider net than the one used by drug companies. Wouldn’t you rather take a medication that “boosts the immune system” than one that claims to prevent the flu? A medication that only prevents a flu sounds limited. Why not have the entire immune system “boosted”? The structure/function claim was the loophole supplement companies were looking for.\nThe consequences of DSHEA\nAlthough DSHEA protects the consumer right to access products from nature, the costs may be too great. The specifics of the act and the way in which industry and the public has responded, has led to several problems:\n1. Supplements are perceived as safe when in fact they are not. There are 23,000 ER visits per year in the US due to supplements and 20% of drug induced liver injury is caused by supplements. This is only the tip of the iceberg because side effects are not formally tracked.\n2. Supplements are perceived as being able to prevent and treat disease. Some companies make specific disease claims breaking the law while others use the ambiguous nature of structure/function claims to their advantage. Consumer confidence is confirmed by public surveys and financial statements - we spend $38 billion per year on supplements in the US, and growing.\n4. The quality of supplements is poor. DSHEA has no requirements for quality control nor has it any punitive measures for poor quality. Without quality control, products can contain anything. This probably explains why 50% of supplements sampled on shelves contain none of the ingredients on the label. In addition, some supplements are ironically tainted with prescription drugs. (see FDA site listing known tainted supplements for weight loss ).\n4. Private research on natural medications has stagnated. Companies are no longer required to prove safety and effectiveness. They rarely spend money on good clinical trials. Instead, they spend money on marketing the loopholes of DSHEA. So we don’t know that much more about the true safety and effectiveness of these natural medications today than the day DSHEA was passed in 1994.\n5. DSHEA promotes unscientific thinking. In particular, the appeal to nature fallacy: if something is natural it is safe, if it is unnatural it is unsafe. This leads to the public’s misunderstanding of science.\n6. DSHEA leads to a negative perception of drugs. If supplements are safe and effective, why use synthetic drugs with all their side effects? White Willow bark has the same active ingredient as Aspirin; yet, only Aspirin is forced to be adorned with warning labels.\nDSHEA is responsible for officially separating medicinal compounds into supplements and drugs in the US. It has done so primarily based on regulation, ownership and patentability - creating a misleading dichotomy. It’s consequences include: a false belief that supplements are effective and safe, an increase in the usage of unsafe and ineffective supplements, poor supplement quality, a stagnation of private research, and the promotion of unscientific thinking.\nBiologically, there is no difference between a supplement and a drug. It does not matter whether a medication is regulated or unregulated, patentable or unpatentable, natural or synthetic. What matters is if it works. There is no “supplement” form of medication. There is only medications that work or medications that do not work.']"	['<urn:uuid:1af62df7-a9ef-45e0-85dc-7d6bc141492d>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T18:05:20.075489	23	34	1396
70	I'm interested in the historical development of adaptive learning - could you tell me when computers first started being used for this educational approach and what system was initially developed?	Adaptive learning emerged in the 1970s when computers were becoming prominent. During this time, the SCHOLAR system was developed as the initial adaptive learning system, based on the belief that computers through artificial intelligence would be able to adapt to learner needs.	['Adaptive learning is a method in education through which specific teaching devices or instruments are employed in an effort meet the specific learning needs of individuals. In other words, adaptive learning is personalized learning. Through the use of advanced technologies tools, instructors can transform traditional, broad-based learning into precision learning. With this, adaptive learning can be viewed as a non-traditional form of learning in that it departs from previous pedagogical methods that cannot account entirely for the myriad of unique and individual characteristics held by students. As time goes on, adaptive learning will likely become the new norm in education and serve as an invaluable approach to closing some of the existing gaps in education and knowledge procurement.\nDeveloping adaptive learning\nThe concept of adaptive learning emerged from the 1970s when computers were “up-and-coming.” The initial belief was that computers through artificial intelligence would be able to adapt in accordance with the needs of the learner. Initial adaptive learning systems are associated with the SCHOLAR system and laid the groundwork for subsequent learning systems.\nEducational software and basic technologies alike now serve the needs of both instructors and learners. For example, adaptive learning is linked to video learning where learners are engaging with programs and lessons that take place on the screen and incorporate different feature so as to connect with divergent learning styles. The concept also lends itself to the use of systems that are interactive and provide multidimensional interface with learners.\nSuch a pedagogical practice moves beyond the confines of passive learning, enabling learners to interact with the educational programs on the topic for which they were designed, play a role in how they learn in addition to what they learn, and ultimately become a part of the teaching devices while simultaneously remaining students or learners.\nAdaptive learning application\nLanguage learners can take part in active learning by engaging in computer-human activities. Here, the learner can communicate with a program, listen to questions and provide responses, and take part in specially designed quizzes, activities, and learning sessions. Additionally, the language-learner example is useful for highlighting the interest and potential for a learner to choose their own learning path (given that there is nothing inherently linear about learning) instead of having it imposed on them. Learning paths and objectives are not rigidly tied together. Whether the same, similar, or entirely different, learning objectives ultimately involve a mix of learning paths.\nAdaptive learning merges learning styles and cognitive styles with the availability of (educational) technology, and data/learning analytics. Audio, visual, and kinesthetic or tactile learning types can benefit from this concept of learning because the concept becomes a practice of science. Though all three learning styles can be incorporated into a given curriculum, one or two styles can be emphasized and thus tailored to the specific learning needs of an individual. When you take a course that is designed specifically for that individual, taking into account their strengths and weaknesses or areas of opportunity, their learning style, attention and retention attributes, you have taken a step into the realm of adaptive learning.\nProspects and challenges\nDespite concern about associated financial costs, involved adaptive learning demands no more attention to development and implementation than traditional forms of instruction. When systems have been designed and created, they can be customized to meet the needs of another learner at relatively little cost. Curriculum as practice can be divided into distinct categories of development over time. Through learner type or style classifications, adaptations can be a rather smooth process. Interactive lessons need not be long, but rather effective in engaging the learner and making the learning objectives clear, concise, and attainable.\nWith the collection of more information and increasing development and collection of course materials, the establishment of a robust pedagogical arsenal would leave an abundance of course content available for instructors. The collection of various material means that instructors will already have a healthy body of go-to resources. Instructors would likely find the application of adaptive learning resource to a larger body of students easier over time.\nHowever, a word of caution is needed when it comes to the general application of adaptive learning. Adaptive learning may not be suitable or connect cleanly with all disciplines and subject areas. Moreover, the increased use of adaptive learning, especially depending on the type or level of class in question, raises some question marks about the role of instructors and educators in practice.\nClassrooms at every level, from primary school to post-secondary education, are increasingly filled with students with diverse learning and knowledge background, coupled with radically divergent interests and goals beyond their years of formal education and training. With this in mind, newer and more innovative teaching and learning methods are and will be required, and instructors can serve as information gatherers on what is required to meet these challenges by identifying them.\nWe should not forget that when discussing the prospective benefits of adaptive learning, discussions should not be limited to in-class learners. Mobile phones have become a powerful way of bringing the classroom to students around the world, when they face the reality of no school or classroom being available, or accessible only for a limited time. Others can experience degrees of exclusion as a result of learning or physical disabilities. As such, mobile phones and adaptive learning apps can build bridges that assist in the crossing or closing of knowledge gaps.\nNot only is adaptive learning suited for bringing a variety of learning formats to the table, it is able to help instructors and researchers in the field of education understanding how learning is distinctive, differing from one learner to another, what subject areas are most desirable, and what the most fundamental problems associated with them are at a given time.']	['<urn:uuid:1bf31eac-72d9-4841-b7fb-19ccf83879e3>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T18:05:20.075489	30	42	959
71	What are the decay modes and products of Iridium-192?	Iridium-192 has two decay modes: it decays 95.13% of the time through negative beta emission to 192Pt (daughter nuclide), and 4.87% of the time through electron capture to 192Os. During this process, a gamma photon is released with an average energy of 0.38 MeV (max 1.06 MeV).	['- Iridium-192 Definition\n- Iridium-192 Properties\n- Iridium-192 Fact Sheet\n- Half Life\n- Iridium-192 Lifetime\n- Iridium-192 Decay Modes\n- Iridium-192 Isotopes\n- Specific Activity\n- Iridium-192 Production\n- Iridium-192 Radiation Safety\n- Iridium-192 Applicator\n- Iridium-192 Low Dose Supply\n- Iridium-192 Uses\n- Iridium-192 Implants\n- Iridium-192 Benefits\n- Iridium-192 Drawbacks\n- Iridium-192 Cancer Treatment\n- Iridium-192 Toxicity\n- Iridium-192 Health Hazards\nIt is a radioactive isotope of Iridium with symbol 192Ir.\nIt is a man-made radioactive element that is produced by nonradioactive Iridium metal in a nuclear reactor.\nIt is a dense metal that is shiny and silvery-white in appearance.\nKnow about some of the chemical as well as physical properties of this element.\n- Its binding energy per nucleon is 7.938986 MeV (Megaelectronvolts).\n- It has a melting point of 2,446°C.\n- It has a boiling point of 4,428°C.\n- Its specific gravity is 22.562 (at 20°C).\n- It is a dense metal and has a relative density of 22.42.\nIridium-192 Fact Sheet\nKnow about some important facts associated with this substance.\n- The mass number of this radioactive element is 192.\n- The atomic number of this element is 77.\n- Its neutron number is 115.\n- The atomic mass of this radioactive substance is 191.962605012 u (unified atomic mass units).\n- It has a mass excess of -34.833207 MeV(Megaelectronvolts).\nThe half-life of this isotopic substance is 73.828 days.\nThe lifetime of this radioactive element is 106.51 days.\nIridium-192 Decay Modes\nIts decay modes are Beta Particles and Gamma Radiation. It decays 95.13% of the time through negative beta emission to 192Pt (daughter nuclide). For the remaining 4.87% of the time, it decays through electron capture to 192Os. A gamma photon with an average energy of 0.38 MeV (max 1.06 MeV) is released in the process.\nAs aforesaid, Iridium-192 is a radioactive iridium isotope. It is also the most common isotope used for high dose rate brachytherapy applications. Once World War II ended, new isotopes such as cobalt-60, caesium-137 and iridium-192 became available for Industrial Radiography. Consequently, the use of radon and radium decreased.\nIts specific activity differs depending on the concentration of 192Ir in the source. For applications of high dose, its specific activity is 2.4×102 TeV/g.\nFor commercial use, 192Ir is produced in a nuclear reactor by reaction of 191Ir with neutrons. This technique has numerous benefits. It ensures minimal generation of unwanted isotopes and large cross section of isotopes for interaction of neutrons. As a result, high concentration of 192Ir is produced comparatively easily.\nIridium-192 Radiation Safety\nBeing a radioactive substance, necessary safeguards must be taken while using it. It should be kept safe in order to avoid accidental exposure or deliberate misuse and disposed following state guidelines. In case of an accidental exposure, victims should remove clothes and discard them permanently. It is important to take showers. Immediate medical help should be sought.\nHigh-dose rate (HDR) Iridium-192 Brachytherapy is often facilitated with the aid of a flexible applicator. It is also used for medical treatment, such as for the cure of stomal recurrence after Tracheostomy is performed for subglottic carcinoma.\nIridium-192 Low Dose Supply\nIridium-192 suppliers render this radioactive element in low doses in wires of 100 – 140 mm in length and 0.3 mm in thickness that can still be cut into smaller pieces as needed. The wires are non-reactive and flexible.\nFor high dose rate, Platinum/Iridium alloy capsules that are 3.5 mm long and 0.6 mm diameter in size are used. These, used a high 192Ir concentration, provides it with a high activity. The tablet comes coated in Platinum of thickness 1 mm, which weaken the consistency of any electrons produced during decay. The tablet is also welded to the end of a wire allowing it to be deployed using a HDR Remote Afterloading machine.\nThis radioactive element does not necessarily require sterilizing by end user when deployed for high dose rate (hdr) as it is rendered in a sealed sterilized package.\nKnow about some of the main applications of this radioactive substance.\n- It is a commonly used isotope in high dose rate Brachytherapy.\n- It is also used for medical reasons in Brachytherapy for the treatment of various types of cancer.\n- It also has applications in industrial radiography. It is used to capture x-ray images of heavy metal objects.\n- Radioactive Ir-192 is principally applied for non-destructive testing (NDT). It is also used to a lesser extent as a radio- tracer in the oil industry.\nIr-192 implants have medical uses in healthcare industry. These are used for curative reasons, primarily in the breast and the head. These implants are manufactured in wire form and are introduced into the target area through a catheter. The implant wire is removed after being left in place for the time required to deliver the desired dose. This procedure is very effective at providing localized radiation to the tumor site while minimizing the patient’s whole body dose.\nThere are numerous advantages of using this radioactive element which can be summarized as follows:\n- Its application can be tailored to high or low dose rate, as per requirement.\n- It is relatively easy to manufacture.\n- It has a stable daughter product.\n- It can be reused.\nSome of the main disadvantages of this element are:\n- Broad spectrum of photons is emitted during the emission process.\n- It involves frequent recalibrations as a consequence of radioactive decay.\n- It needs replacement after every three to four months.\nIridium-192 Cancer Treatment\n192Ir is used as a source of gamma radiation for curing cancer with the application of Brachytherapy. Brachytherapy is a type of radiotherapy that involves the placement of a sealed radioactive source inside or adjacent to the region that requires treatment. Specific treatments involve procedures such as:\n- High Dose Rate Prostate Brachytherapy\n- Bilary Duct Brachytherapy\n- Intracavitary Cervix Brachytherapy\nVery little is known regarding the toxicity of iridium compounds. This is due to the fact that they are used in extremely small amounts. However, the radioisotopes of iridium are known to be quite dangerous. The same can be said for 192Ir, which is a radioactive Iridium isotope. Accidental exposure to 192Ir radiation can lead to injuries. High-energy 192Ir gamma radiation can elevate the risk of cancer.\nIridium-192 Health Hazards\nExternal exposure to this substance can lead to problems like\n- Radiation poisoning\n- Acute radiation sickness\nInternal Iridium-192 exposure can only happen if anyone swallows its tablet or other forms sold commercially. Ingestion of 192Ir can result in burning of the linings of the intestines and the stomach. Swallowed tablets are usually excreted in feces. Long-term effect would depend on how powerful the swallowed contents were and how much duration they stayed in the body. 192Ir as well as other isotopes of Iridium such as 192mIr and 194mIr tend to get deposited in the liver. This can result in health hazards from both gamma and beta radiation.']	['<urn:uuid:3a6a0810-d1bd-409c-afeb-ea8568771a38>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T18:05:20.075489	9	47	1158
72	grape variety identification challenges solutions	The key challenge in grape identification is determining when a variant becomes different enough to be considered a separate variety. Traditional ampelography looked at physical characteristics, while modern DNA testing examines genetic markers. However, each method has limitations - DNA tests can't detect physical mutations (like color changes in Pinot varieties), while ampelography can make identification mistakes. The newest solution is the transposon marker test, which analyzes 50-100 genetic markers instead of the standard 10-20, providing more definitive results and helping resolve unclear cases. This test is unique worldwide and offers year-round testing capability using either fresh leaves or woody material.	"['Those of you who are familiar with this blog are aware that this is an issue that has caused me to spill a considerable amount of ink. My most complete treatment of the issue can be found in the Roter Traminer article, though I do touch on the issue in my Malvasia Nera post as well as my Pigato post. The question I\'m wrestling with in all of those posts concerns the point at which a clonal variant becomes differentiated enough to be considered a separate grape variety. Where do we draw that line? How different is different enough to be considered separate?\nBelieve it or not, this question was actually much easier to answer before the arrival and widespread use of DNA testing for grapes. Ampelography was (and technically still is, I suppose, though there are precious few ampelographers in the traditional sense of the word anymore) the science of grapevine identification and differentiation using the physical characteristics of the vine itself. Leaf shape and size was perhaps the most widely used measure for ampelographers, but other factors such as grape size and color and bunch size and shape were used not only to identify unknown vines in the vineyard but also to differentiate grape varieties from one another. This was not a perfect system by any means and there have been some fairly famous gaffes made by ampelographers in the past (like the mistaking of Carmenere with Merlot or of Sauvignon Blanc with Friulano in Chile, for example, though to be fair both of those cases were also eventually sorted out by ampelographers), but there\'s a certain kind of elegance and intuitive sense to classifying plants this way.\nThe areas where ampelography has been found the most wanting, especially in recent years, concern those cases of vines that are likely clonally descended from a common ancestor, but which have been spread out geographically over a long period of time and have developed mutations along the way. All of the mutations that happen to grapevines which have an effect on the physical appearance of the plant are called somatic mutations. These mutations can affect everything from leaf shape to bunch size to grape size. Some are subtle while others are dramatic, though most of the dramatic mutations probably started out subtle and were exaggerated over time through clonal selection. The most famous somatic mutations involve Pinot Noir and its tendency to mutate into vines that produce white berries, which are known as Pinot Blanc, or pink berries, which are known as Pinot Gris. These somatic mutations cannot be picked up with current DNA analysis techniques because those techniques do not analyze the entire DNA string of a grape. I\'ve mentioned DNA analysis enough on this site that I really should take some time to try to explain exactly what it is and how it works so buckle up because there\'s some serious science ahead.\nAs I\'m sure you already know, DNA is one of the basic building blocks of life. It is composed of two strands of molecules which run parallel (technically anti-parallel, but let\'s not quibble) to one another in a twisting design known as the double helix. These strands are made up of four different nucleobases, each of which has its own unique partner on the opposite strand that it is connected to by sugar or phosphate molecules. The nucleobases are Guanine (G) which pairs with Cytosine (C) and Adenine (A) which pairs with Thymine (T). Some of the arrangements of nucleobases are involved in synthesizing proteins, and these areas are known as genes, but some of the sections of DNA are just filler or what I like to call Junk DNA. It turns out that there are a lot of Junk DNA sites throughout the entire DNA sequence of an organism, but the ones that we\'re interested in are called microsatellite sequences. Microsatellite sequences are repetitions of between one and six nucelobases that are scattered randomly throughout an organism\'s DNA. So a typical bit of DNA might look something like this:\nThat bit in the middle where AG repeats is a microsatellite site, and it turns out that there are between 1,000 and 10,000 of these sites scattered throughout the genomes of most complex multi-cellular organisms (not all of them are repeats of just two nucleobases...it could just be one [AAAAAAAA] or as many as six [GATCACGATCACGATCACGATCAC]). It also turns out that some of these sites are highly variable between individuals and are inherited in very specific ways. What scientists have done is to find where these sites are in humans as well as in grapes and they have pinpointed the areas that are the most variable between individuals. These sites are given names like VVS1, ZAG64, and VVMD28 among others and they are the regions that scientists look at when they are trying to differentiate between different grape varieties or confirm synonymities between differently named grapes that are genetically identical. All individuals have microsatellite sites in the same places on their genome, but the size of the site will be different between individuals, so the number of base pairs in the repeat are counted up and compared across subjects to see which are identical and which are different (this is not very important in humans because humans are obviously distinctive from one another and are not propagated clonally as grapes are...see my Ciliegiolo post for a discussion of this issue). Most studies use between 6 and 10 different microsatellite sites in their analyses in order to differentiate between individual varieties because fewer sites don\'t give enough discriminatory power while more sites don\'t give you any added benefit. Parentage studies use the same technique, but look at different microsatellite sites which have been found to be directly inherited from each parent. This is the same technology, incidentally, used in paternity tests in humans and made famous by such trash-talk heavyweights as Jerry Springer.\nIt\'s important to remember that these microsatellite sites are Junk DNA that are essentially just taking up space. They don\'t encode for any proteins and don\'t have any effect on the physical appearance of the plant. Remember for a moment what we said about somatic mutations: they are mutations that have an effect on the physical appearance of the plant, which means that they are mutations that occur in areas of the plant\'s DNA that are actually doing some kind of work. The kinds of mutations that affect berry skin color are happening in areas of the plant\'s DNA outside of the areas that microsatellite site analysis is looking at, so these mutations are essentially undetectable with this kind of technology. This can be a good thing, as it allows us to trace the clonal history of grapes like Pigato, Favorita and Vermentino, which are geographically dispersed, to a single grape ancestor, but it can also possibly lead us to overly reductionistic conclusions. Pigato, Favorita and Vermentino are genetically identical according to the microsatellite analysis, but does that mean they\'re the same grape?\nThe case of Pinot Noir is significant here, I think. Pinot Noir, Pinot Blanc and Pinot Gris are genetically identical, but are considered to be different grape varieties by virtually every expert on earth, while grapes like Nielluccio, Sangiovese and Prugnolo Gentile are considered to be clonal variants of the same grape, Sangiovese. The difference in the two cases is obvious: the Pinot mutation has an obvious physical manifestation, namely the color of the grape skin. But it\'s more than just that. The color of the grape skin has a direct effect not only on the wines that are made from that grape, but also in the kinds of wines that can be made from it and the methods that one must employ to make those wines. The physiological difference entails a different approach to making wines from that grape, while the physiological differences between the Sangiovese clones are much more subtle. One could mistake wines made from the Sangiovese clones with one another while it would be much more difficult to make that mistake with wines made from the Pinot varieties.\nThe situation with Pigato, Favorita and Vermentino is, unfortunately, not so clear cut. There are obvious physical differences: Pigato has a characteristic splotchiness on the grape skins that Favorita and Vermentino lack while the leaves, clusters and buds of Favorita are different enough from those of Vermentino that the Italian Ministry of Agriculture decided that it was a separate grape in 1964, stating unequivocally that ""Favorita and Vermentino must be considered as two distinct cultivars,"" but given the new information that DNA analysis has provided, can we still say the same thing today with the same amount of confidence? The splotchiness of the skins of Pigato has no real effect on the wines made from the grape or on the range of wines that could possibly be made from it, and the physiological differences between Favorita and Vermentino seem to be merely cosmetic. One could argue that wines made from each of the three grapes are distinctive from one another, but one would expect that anyway given the range of places and climates where each is primarily found (Pigato in Liguria, Vermentino in Sardinia and Liguria and Favorita in Piemonte). Wines made from Chablis are much different from those made in Pugliny-Montrachet, for example, but Chardonnay is the grape in both wines.\nSo what are we to do? It would be ideal if the genomes of Pigato, Favorita and Vermentino were completely sequenced so that an analysis could be done to see where the differences are in their DNA and to see just how widespread they are. We know that they\'re ultimately descended from the same plant and have mutated independently over time to the extent that they are visually distinguishable from one another, but we just don\'t know how significant the mutations have been on a molecular level and we can\'t know unless we have the entire DNA sequence of each grape. Sequencing an individual grape variety is no small task, though, and I doubt seriously that there\'s much clamor for that kind of work on any of these three grapes. Given the standards that are currently used to differentiate grape varieties, I don\'t see any way to consider Pigato, Favorita and Vermentino as separate cultivars. The differences just don\'t seem significant enough to me to merit differentiation. If any readers have any thoughts, please share in the comments section as I\'m very interested to see how others might feel about this (if anyone other than me ever thinks about this kind of stuff).\nThe grape we call Favorita is grown exclusively in Piemonte around the Roero region and is thought to have arrived in this area over 300 years ago via Ligurian oil merchants. The grape is known for having large berries and has been a popular table grape in Piemonte for many years. It is thought that moniker ""Favorita"" was bestowed as a result of its prowess and esteem as a table grape and not as a wine grape. When it was used for wine, historically it was blended with Nebbiolo in an effort to smooth over some of Nebbiolo\'s rough edges. Varietal wines made from Favorita have enjoyed some success recently, but plantings lag well behind those of Arneis and Chardonnay in the Langhe. Many examples that we find here in the US are slightly spritzy, though I\'m not sure how prevalent that style is elsewhere.\nI was able to find a bottle of the 2009 Gianni Gagliardo ""fallegro"" Favorita at Bauer Wine and Spirits for about $17. In the glass the wine was a medium lemon gold color. The salesperson at Bauer told me this wine was supposed to be a little fizzy, but mine was completely flat. The nose was a total blank with maybe a suggestion of lemon or apple, but it was really a total cipher. This is apparently a characteristic of the grape itself, as Nicolas Belfrage in his Barolo to Valpolicella refers to the grape as ""non-aromatic [with] any scented character it may have coming by illicit blending."" On the palate the wine was medium bodied with medium acidity. It was somewhat cidery with ripe apple and apple peel fruit along with a touch of pear and melon as well. The wine was bottled with a synthetic cork so I wonder if perhaps there was a faulty seal which may have caused my bottle to go flat and take on a bit of oxygen. I haven\'t had a chance to try another Favorita-based wine, so I\'m really not sure how representative my bottle is. This bottling is supposedly the benchmark for Favorita wines, but it didn\'t do much for me.', 'New genetic test for vine varieties-21 September\nSeptember 22, 2015 0 Comments\nA new type of genetic test being performed by a liquid-handling robot at Lincoln University is taking DNA testing to the next level with its ability to deliver five times the standard amount of data used to identify grapevine varieties.\nDeveloped by Dr Darrell Lizamore at Lincoln University and delivered by Zebra Biotech, the new transposon marker test is a unique DNA testing process not available anywhere else in the world, offering an alternative to and improvement on existing methods.\nWhile ampelographers have traditionally identified and classified varieties by their leaf shape, DNA testing is now the preferred technique, with short repeat marker tests the most popular form of identification. Sample genetic profiles are screened against an extensive profile database, allowing comparison with over 1000 varieties. Analysing specific genetic markers provides information on variety and species, giving nurseries confidence that they are selling the right varieties, and preventing grape growers from planting the wrong vines.\nHowever, short repeat marker tests only analyse between 10 and 20 markers, and on occasion produce ambiguous results due to changes within this limited set. The transposon marker test analyses between 50 and 100 markers per sample, and focuses on the most variable parts of the plant’s DNA, so is able to resolve unusual results from standard testing.\n“Only a limited number of laboratories worldwide offer the short repeat marker tests, and Zebra Biotech is the only accredited provider in New Zealand” says Dr Lizamore. “However, these tests only check a few genetic markers. The transposon test provides results that can be treated with a lot more confidence, so if you need definite or more specific results it’s very useful.” Lincoln University’s collection of over 100 locally-grown varieties is often used as a source of reference material for comparison as there is no public database for the transposon test.\nZebra Biotech’s genetic testing doesn’t stop when vines are dormant. “We offer both tests year round, using fresh leaves in summer and woody material in winter. As far as we know, no-one else is offering this service. The local service is particularly beneficial to growers in New Zealand because of restrictions around the import and export of live tissue. It’s cheaper and faster and they get personal service.”\nDr Lizamore’s experience in DNA testing goes back to his student days at Stellenbosch University in South Africa where he worked on the development of genetic profile tests for abalone to assist police in tracking caches of the smuggled seafood, which has a high market value.“The short repeat DNA tests we carry out on vines are similar to those used for paternity testing, crime scenes and so on. The main difference is how we extract the DNA,” says Dr Lizamore.\nIt was his PhD research, supervised by Dr Chris Winefield at Lincoln University, which led to the development of the new type of test. They studied the activity of transposons: regions of the genetic code in most species that respond during times of environmental stress, such as drought. These elements shuffle the genetic information of an organism, resulting in new mutations. Dr Lizamore’s work identified which transposon types are active in modern cultivated vines, along with conditions that trigger the grapevine’s DNA to change.\nTogether Dr Winefield and Dr Lizamore are now using their new understanding of the natural role transposons play in altering vine DNA to create new vines with specific qualities such as disease resistance. This offers an alternative to current breeding programmes as a way to improve the crops planted in New Zealand. With the support of New Zealand Wine, they have already created about 200 new plants which will be of interest to local nurseries and growers.']"	['<urn:uuid:9b2fdbcf-a64c-4fea-bd6c-5f2de5844681>', '<urn:uuid:cbadd963-43ff-4b39-bba5-7c3384d1c6fa>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T18:05:20.075489	5	101	2747
73	meaning persian expression jegar compare english proverb maxim	The Persian expression 'jegar' (liver) and English maxims serve different purposes in their respective languages. In Persian, 'jegar' is used informally, sometimes to express love (like 'jigæreto bokhoræm' meaning 'I love you very much') or to call a beautiful person, though these uses are considered impolite. In contrast, English maxims are defined as wise sayings that recommend conduct or express ultimate truths, derived from Latin 'maximus' meaning 'greatest', and are used formally to present fundamental principles or rules of conduct.	"['All of the sentences and phrases on this page were submitted by readers. I have responded by translating them into conversational Persian. While most of these sentences do not fit our weekly lessons, they are still helpful for you to speak Persian naturally.\n101. “I hope you’re fine” in Persian\nI hope that you are fine (to follow the sentence “How are you”). /omid va:ræm ha:letu:n khu:b ba:sheh/.\nomidvaram haletun khub basheh\n102. “How is my old rooster?” in Persian\nHow is my old rooster? /khoru:s e piræm chetoreh?/.\nkhorus e pir\n103. “Would you like to go for tea?” in Persian\nWould you like to go for tea? /ma:yeli berim ye cha: i: ba: ham bezanim?/.\n104. “Are you cheating on me?” in Persian\nAre you cheating on me? /da:ri særæm kola: miza:ri/?\ndari saram kola\n105. “You are punishing me” in Persian\nYou are punishing me so? /da:ri in ju:ri moja:za:tæm midi?/.\ndari injuri mojazatam\n106. “You’re a babe” in Persian\nYou’re a babe! /bæch.cheh i:/.\n107. “Get jiggy with me” in Persian\nWould you mind getting jiggy with me? /ma:yeli ye dor ba: hæm berægh sim?/.\nmayeli ye dor\n108. “Your smile is amazing” in Persian\nYour smile is amazing! /læb khændet mæh shær e/.\n109. “Wisdom” in Persian\n110. “Courage” in Persian\nCourage /del gærmi/.\n111. “Please be patient with me” in Persian\nPlease be patient with me. /lotfæn ba: mæn sæbu:r ba:sh/.\nlotfan sabur bash\n112. “We are different but equal” in Persian\nWe are different but equal. /ma: æz hæm motefa:vet væli yeksa:nim/.\nma az ham motefavet\n113. “Please be gentle with me” in Persian\nPlease be gentle and slow with me. /lotfæn ba: mæn mehræba:n væ a:ra:m ba:sh/.\nlotfan mehraban bash\n114. “I’m fine” in Persian\nI’m fine. /mæn khu:bæm/. (In response to: how are you?)\n115. “Can you repeat that?” in Persian\nCan you repeat what you said? /momkeneh lotfan hærfeto tekra:r koni/?\nmomkeneh tekrar koni\n116. “Sorry, I’m late” in Persian\nI’m sorry for my late? /bebækh shid keh dir kærdæm/.\nbebakhshid dir kardam\n117. “I can’t hear you” in Persian\nExcuse me, I can’t hear you. /bebækh shid, seda:t o nemish nævæm/.\n118. “Sweetheart” in Persian\n119. “Honey” in Persian\nHoney (the sweet substance produced by bees) = /æsæl/. Also (to call your sweetheart, either man or woman) = /æzizæm/.\n120. “You have made me very happy” in Persian\nYou have made me very happy. /kheili khosh ha:læm kærdi/.\nkheili khosh halam kardi\n121. “I am so lucky to have you in my life” in Persian\nI am so lucky to have you in my life. /kheili khosh sha:nsæm keh to tu: zendegi ye mæni/.\n122. “Yes” in Persian\nYes (in more polite form) = /bæleh/.\nAlso (in conversational form that also used very often) = /a:reh/.\n123. “No” in Persian\nNo = /næh/. Also /kheir/. Also (in conversation or for emphasis, people usually combine these two) = /næh kheir/.\n124. “Please” in Persian\n125. “May I have a fork” in Persian\nMay I have a fork please? /misheh lotfæn yeh chænga:l beh mæn bedin/?\nmisheh lotfan ye changal\nYou may simply replace fork with spoon /gha:shogh/\nor knife /cha:ghu:/\n126. “Good day” in Persian\nGood day /ru:z beh kheir/.\n127. “Good evening” in Persian\nGood evening /æsr beh kheir/.\n128. “I like spending time with you” in Persian\nI like spending time with you. /du:st da:ræm keh ba: to væghtæm o sær konæm/.\ndust daram vaght sarf konam\n129. “We are perfect for each other” in Persian\nWe are perfect for each other. /mæn o to va:seh hæm sa:khteh shodim/.\nmano to vaseh ham\n130. “You are so sweet to me” in Persian\nYou are so sweet to me, you deserve a thousand kisses. /to bæra: ye mæn kheili æzizi, heza:ra:n bu:seh mi ærzi/.\n131. “Are we dabbling?” in Persian\nAre we dabbling or what? (This sentence has several equivalents in Persian and most of them are either impolite or used among close friends only. So, be careful when you are using it! The translation presented here is, possibly and hopefully, the most polite form which you can use. It’s Persian equivalent reads: Are my words the wind of the air? – it’s a kind of expression in Persian which means ‘Am I talking nonsense? OR bullsh…..!) /yæni hærfa:m ba:d e hæva:st/?\n132. “You are the most important thing in my life” in Persian\n‘Ghoncheh’ (girl’s name), you are the most important thing in my life – and I love you so much – and I hope that the same girl I fell in love with will come back to me. /ghoncheh, to mohem tærin bækhsh e zendegi æm hæsti – væ mæn kheili du:set da:ræm – væ omidva:ræm keh hæma:n dokhtæri keh a:sheghæsh shodæm beh pishæm bær gærdeh/.\n133. “Liver of my soul” in Persian\nThere is no definite phrases i would like to get translated, but i would like to know some (or as many as possible) expressions using “jegar” or liver. I know you can say sweet things like “liver of my soul”, but I heard that liver expressions can be nasty also. So I hope you could think of some and add them to “your words” page on your great site.\nWell, the above paragraph is sent to me in a message and I decided to post the message first to put you in the picture. As you just read in the message, jigar /jigær/ or jegar /jegær/ is sometimes used by the Iranians. The main meaning of this word is ‘liver’. However, the Iranains (usually the naughty ones!!) use it in different contexts. Sometimes, it is used to call a beautiful girl who is just passing by! For example, you may say (please don’t use it on my behalf!!) ‘jigar’ to a girl whom you don’t know but you just want to say something to have some fun! (You are not advised to do that for this is nasty or impolite!). Sometimes, you may say /jigæreto beræm/\nor /jigæreto bokhoræm/\nwhich, in word for word translation, mean ‘may I eat your liver!’. But it means ‘I love you very much’. Such expressions are not very polite to express your love with. However, you may use them for your very close friends.\n134. “No bird soars too high” in Persian\nNo bird soars too high, if he soars with his own wings. (The very close equivalent of this phrase in Persian is this) /kæs nækha:ræd posht e mæn joz na:khon e ængosht e mæn/.\nNo one will scratch my back except my own finger\nkas nakharad poshte man\n135. “I am grateful for your love” in Persian\nYou bring out the real happiness in me. I am forever grateful for your love – and even though I am so far, I will be back with my love soon. /to khoshi ye va:gheh i: ro beh mæn midi – hæmisheh mæmnu:n e eshghetæm – væ hær chænd kheili æzæt du:ræm – bezu:di ba: eshghæm pishet bær mi gærdæm/.\nto khoshi ye vaghe\n136. “Stand back” in Persian\nStand Back /du:r tær va: ista:/.\n137. “Alone” in Persian\n138. “Don’t shoot” in Persian\nDon’t shoot /shel.lik nækon/.\n139. “Horse” in Persian\n140. “Gun” in Persian\n141. “Do you speak English?” in Persian\nDo you speak English? /englisi bælædi/.\n142. “Peace” in Persian\n143. “The first Sunday in every month” in Persian\nThe first Sunday in every month we have a meeting, if you would like to come. /ævælin yek shænbeh ye hær ma:h ma: dor e hæm jæm mi shim. ægeh du:st da:shti biya:/.\n144. “Life without you isn’t worth living” in Persian\nLife without you isn’t worth living. /bi to zendegi ærzesh e zi:stæn ra: næda:ræd/.\nbi to zendegi\n145. “The best gift is you” in Persian\nGod gave me the best gift, and that’s you. /khoda: behtærin hedya ro beh mæn da:deh væ u:n to i:/.\nkhoda behtarin hedye\n146. “Wash your hands” in Persian\nGo and wash your hands. /boro dæsta:to beshu:r/.\n147. “Stop fighting” in Persian\nStop fighting. /dæva: nækonin/.\n148. “Where are you hurt?” in Persian\nWhere you got hurt? /koja:t dærd gereft/.\n149. “Who hurt you?” in Persian\nWho hurt you? /ki zædet/.\n150. “Make me some Iranian food” in Persian\nCook me some Iranian food! /ye ghæza: ye ira:ni va:sæm dorost kon/.\nye ghaza ye irani', 'Definitions for saying\nHere are all the possible meanings and translations of the word saying.\nsaying, expression, locutionnoun\na word or phrase that particular people use in particular situations\n""pardon the expression""\nA proverb or maxim; something often said.\nSamuel Johnson\'s Dictionary\nExpression; words; opinion sententiously delivered.\nEtymology: from say.\nI thank thee, Brutus,\nThat thou hast prov’d Lucilius’ saying true. William Shakespeare.\nMoses fled at this saying, and was a stranger in Median. Acts.\nMany are the sayings of the wise,\nExtolling patience as the truest fortitude. John Milton.\nOthers try to divert the troubles of other men by pretty and plausible sayings, such as this, that if evils are long, they are but light. John Tillotson, Sermons.\nWe poetick folks, who must restrain\nOur measur’d sayings in an equal chain,\nHave troubles utterly unknown to those,\nWho let their fancy loose in rambling prose. Matthew Prior.\nThe sacred function can never be hurt by their sayings, if not first reproached by our doings. Francis Atterbury.\nthat which is said; a declaration; a statement, especially a proverbial one; an aphorism; a proverb\nA saying is something that is said, notable in one respect or another, to be ""a pithy expression of wisdom or truth"". There are a number of specific types of saying: ⁕Adage – An aphorism that has gained credibility by virtue of long use. ⁕Aphorism – A concise definition, notably memorable. ⁕Apophthegm – ""…an edgy, more cynical aphorism; such as, \'Men are generally more careful of the breed of their horses and dogs than of their children.\'"" ⁕Bromide – A phrase or platitude that, having been employed excessively, suggests insincerity or a lack of originality in the speaker employing it. ⁕Cliché – An overly commonplace, hackneyed or trite saying. ⁕Epigram – A poetic form of comment on a particular idea, occurrence, or person. ⁕Epithet – A descriptive word or phrase that has become a popular formulation. ⁕Gnome. A type of saying, especially an aphorism or a maxim, that is designed to provide instruction in a compact form. ⁕Idiom – ""…an expression whose meaning can’t be derived simply by hearing it, such as \'Kick the bucket.\'"" ⁕Mantra – A religious or mystical syllable or poetic phrase. ⁕Maxim – A principle or rule. A maxim is a wise saying, especially one intended to advise or recommend a course of conduct. In comparison to its approximate synonyms: saying, adage, saw, motto, epigram, proverb, aphorism, the term maxim stresses the succinct formulation of an ultimate truth, a fundamental principle, or a rule of conduct. The word derives from the Latin word maximus, ""greatest"", via an expression maxima propositio, ""greatest premise"".\nVerb form of the word say.\nThey were saying how much they were looking forward to their wedding day.\nSubmitted by MaryC on February 19, 2020\nBritish National Corpus\nSpoken Corpus Frequency\nRank popularity for the word \'saying\' in Spoken Corpus Frequency: #773\nWritten Corpus Frequency\nRank popularity for the word \'saying\' in Written Corpus Frequency: #202\nThe numerical value of saying in Chaldean Numerology is: 5\nThe numerical value of saying in Pythagorean Numerology is: 3\nSome people think that you have to be the loudest voice in the room to make a difference, that is just not true. Often, the best thing we can do is turn down the volume. When the sound is quieter, you can actually hear what someone else is saying. And that can make a world of difference.\nThis time, the emphasis in operations will be more on soldiers, the prime minister has spoken quite decisively, saying \'all districts will be cleansed of terrorist elements, from street-to-street, house-to-house if necessary\'.\nI can tell you there will be one loud voice in our party saying, think of the unintended consequence, think about what we\'re going to accomplish and whether it will work before we go to war.\' I promise you that will always be something I take very, very seriously.\nWhat we\'re saying today is that you\'re either part of the solution or you\'re part of the problem.\nImagine saying yes ... let\'s do this thing now!\nPopularity rank by frequency of use\nTranslations for saying\nFrom our Multilingual Translation Dictionary\n- proverbi, ditaCatalan, Valencian\n- přísloví, rčeníCzech\n- Sprichwort, AusspruchGerman\n- ρητό, απόφθεγμα, παροιμίαGreek\n- dicho, diciendo, proverbio, refránSpanish\n- közmondás, mondás, szólásmondásHungarian\n- dicto, proverbioInterlingua\n- detto, proverbio, massimaItalian\n- Sproch, SprachwuertLuxembourgish, Letzeburgesch\n- поговорка, изрека, пословицаMacedonian\n- gezegde, spreekwoordDutch\n- dizendo, provérbio, ditadoPortuguese\n- zicală, proverbRomanian\n- пoслoвицa, пoгoвoркaRussian\n- poslovica, izrekaSerbo-Croatian\n- príslovie, rčenieSlovak\n- ordstäv, ordspråkSwedish\n- misemo, msemoSwahili\n- spot, dijhêyeWalloon\nGet even more translations for saying »\nFind a translation for the saying definition in other languages:\nSelect another language:\n- - Select -\n- 简体中文 (Chinese - Simplified)\n- 繁體中文 (Chinese - Traditional)\n- Español (Spanish)\n- Esperanto (Esperanto)\n- 日本語 (Japanese)\n- Português (Portuguese)\n- Deutsch (German)\n- العربية (Arabic)\n- Français (French)\n- Русский (Russian)\n- ಕನ್ನಡ (Kannada)\n- 한국어 (Korean)\n- עברית (Hebrew)\n- Gaeilge (Irish)\n- Українська (Ukrainian)\n- اردو (Urdu)\n- Magyar (Hungarian)\n- मानक हिन्दी (Hindi)\n- Indonesia (Indonesian)\n- Italiano (Italian)\n- தமிழ் (Tamil)\n- Türkçe (Turkish)\n- తెలుగు (Telugu)\n- ภาษาไทย (Thai)\n- Tiếng Việt (Vietnamese)\n- Čeština (Czech)\n- Polski (Polish)\n- Bahasa Indonesia (Indonesian)\n- Românește (Romanian)\n- Nederlands (Dutch)\n- Ελληνικά (Greek)\n- Latinum (Latin)\n- Svenska (Swedish)\n- Dansk (Danish)\n- Suomi (Finnish)\n- فارسی (Persian)\n- ייִדיש (Yiddish)\n- հայերեն (Armenian)\n- Norsk (Norwegian)\n- English (English)\nWord of the Day\nWould you like us to send you a FREE new word definition delivered to your inbox daily?\nDiscuss these saying definitions with the community:\nUse the citation below to add this definition to your bibliography:\n""saying."" Definitions.net. STANDS4 LLC, 2022. Web. 7 Dec. 2022. <https://www.definitions.net/definition/saying>.']"	['<urn:uuid:3df78075-81e6-441b-b2d7-c135f19a81aa>', '<urn:uuid:3df9da3c-0272-4cb5-9812-0db9a6a2ee3f>']	open-ended	direct	long-search-query	distant-from-document	comparison	expert	2025-05-12T18:05:20.075489	8	80	2357
74	Which poses a greater challenge when drying water-damaged walls: removing wallpaper or removing wet furniture cushions?	Removing wallpaper poses a greater challenge as it requires special products to dissolve the adhesive and tools like a 3-6 inch scraping tool to remove stubborn sections. Additionally, wallpaper acts as a seal that traps moisture against the wall. In contrast, wet furniture cushions simply need to be removed and leaned against something to dry thoroughly.	['When your home has suffered a leak or flood damage, it can be very mentally taxing. All of the damage that has been done to your home can seem absolutely overwhelming at first.\nThe standing water even has the potential to present a huge danger to you and your family. You need to be able to take action in order to get things fixed up as quickly as possible.\nWater Damage Restoration\nMost people are going to need to call in water damage restoration professionals to get things fixed up right. There are also going to be some things that you can do on your own to try to get things back to normal.\nThis article is going to focus on providing you with some important tips to keep in mind when your home has suffered water damage. Look at all of the information and use it to get your home back to normal in the safest way possible.\nBe Mindful of Electricity\nGoing into rooms that have standing water while the electricity is still on is a very bad idea. You don’t want to get electrocuted and you need to understand that this is a potentially deadly situation.\nWork to get the electricity shut off in the safest way possible. You will not want to start doing anything in rooms that have standing water until the electricity is confirmed to be shut off.\nAlso, you need to avoid using electrical appliances of any kind while you’re on wet carpeting or floors. You need to work to minimize the risk of electrocution. This may make things more tedious but it’s best to err on the side of caution here. Be safe and be mindful of electricity so that you can work on water damage restoration without encountering any issues.\nRemove as Much Water as You Can\nIt may seem obvious but you want to remove as much water as you can. You can start out by mopping up water as thoroughly as possible. Blotting water will also be helpful during this period of time. If there is a significant amount of standing water, then you may need to use buckets to start removing it a little bit at a time.\nYou don’t want to try to use a normal household vacuum cleaner to remove water from your home. This would be very dangerous and it wouldn’t work properly anyway. There are certain types of vacuums known as “shop vacs” that will do a good job, though. If you have a shop vac, you still need to exercise caution due to the electricity.\nConsider removing all of the wet area rugs and pieces of carpeting that you can. Depending on the extent of the damage, your carpeting might not be able to be saved. If you are unsure of what to do, you can always ask for a professional opinion by contacting water damage restoration services.\nTake All Valuables to a Safe Place\nYou should try to remove all of your valuables from the home and put them in a safer place. This should apply to anything that is important to you. Works of art, family heirlooms, and other expensive items will be a high priority when it comes to relocation. You should be able to move many of your most important things to get them out of harm’s way while you’re trying to clean up the water.\nDon’t forget about things that have sentimental importance either. Your family photo albums and other life mementos might be irreplaceable. Take your time to gather these items when it is safe to do so. Removing them from the situation is going to be best and it’ll put your mind at ease.\nDry Things Out\nDrying things out might take a little bit of time. You have to start drying things out right away in order to make your life easier. Any books that have been impacted by the water issue should be removed from bookshelves. You’ll want to spread them out so that they can start to dry.\nYou should also open up your drawers, closets, and luggage to allow them to start drying out. Any wet fabrics should be removed and hung somewhere that they can dry properly. Remember to hang your furs and leather items somewhere separately as these need to dry at room temperature. You’ll also need to remember to lift your drapes and curtains off of the floor so that they can start drying.\nMaking use of a dehumidifier might be helpful. If it is 60 degrees Fahrenheit or higher outside, then dehumidifiers might help you to dry things out.\nDuring the cooler seasons, it can be helpful to leave your heat on when you’re drying things out. If you don’t have a dehumidifier during the hot months, then your air conditioner will prove to be useful.\nTake Care of Your Furniture\nYour furniture really needs to be tended to if you want to try to save it. Placing your furniture on wood blocks will keep it up and off of the floor.\nSome people choose to place aluminum foil underneath the furniture legs when they don’t have access to wood blocks. The basic point is to keep the water from touching the furniture legs so that the furniture can avoid being damaged further.\nIt’s also imperative to take all of the cushions off of your furniture so that they can dry. Wet furniture cushions need to be placed somewhere to dry.\nTry to lean them against something so that they can dry out thoroughly. This might take some time but try to take special care with your furniture. Replacing furniture is expensive so you want to be mindful of how you’re taking care of things.\nConsider Contacting Professionals\nEven if you’re planning on trying to do parts of the water damage restoration yourself, you might want to stop to consider contacting professionals. It’s important to try to save all of your household items. You want your home to get back to normal in a timely fashion.\nWater damage restoration businesses have the right equipment to handle the job and they will know what needs to be done.\nYou can contact them as soon as possible to get things taken care of right. It’s going to be much safer and easier to rely on true professionals to take care of this task. If you want to get your home back to normal quickly, then you need to call the experts right away. Don’t wait if you’re in need.', 'When it comes to plumbing repairs, having the right tools in your arsenal c\nImagine waking up one morning, ready to start your day, only to find that t\nPosted by July 31, 2022on\nAn event that leads to water damage in the home may affect the floors and the walls too. When walls are not dried quickly, it can lead to a number of problems from mold growth to a lack of structural integrity. It’s important to contact a local restoration company to dry the walls and reduce the chance of further damage that’s expensive to fix. In this article, we will explain how drywall is saturated and how you can attempt to dry it yourself.\nWhen water enters an area at ground level, it will cover the floors and the floor coverings until they become fully saturated. At this point, the water will “wick” up the walls due to capillary action which is a common characteristic in porous materials. The speed and full extent of this upward water movement will depend on a few factors, such as: the volume of water present, the height of the water, the construction of the wall and more.\nIn this scenario, drywall is a bad material because it’s extremely absorbent and it contains cardboard and gypsum on both sides which are vulnerable to wicking. This may come as a surprise, but drywall can wick water up to a height of around 30 inches. This is on both sides of the drywall and the water can go higher on the inner wall surface due to restricted evaporation. Many people believe that water damage is visible, this is only partially true, the drywall can be soaked and it may display no signs of moisture.\nThe first consideration is whether you are going to hire a professional company to deal with the problem or if you’re going to attempt a DIY approach. We will always recommend the former. This may seem like bias, but drying out wet walls is a lot of work. The situation is exacerbated if the water damage is hidden and/or extensive in nature. There are many cases where a homeowner has attempted to dry out walls with no water damage restoration training and they have missed damaged areas. If you are determined to tackle this task, the following section may help.\nIf the source of the localized flooding is a burst pipe, then it’s important to shut-off the water source immediately. A local plumber should be contacted to identify and fix the section of broken pipe before you can dry the walls. When the repair is completed, you can open the windows and doors to speed up the drying process. Using floor fans to increase the air circulation in the affected areas in another smart move. If you have a dehumidifier, set it up near or in the room to remove the moisture from the air. This will remove more moisture from the damp walls indirectly.\nSpeeding up the evaporation process can help too. Remove the baseboards and molding to prevent the buildup of moisture behind them and store them in a dry location. If there are wall hangings on affected walls, they should be removed and put somewhere safe. If you have wallpaper, it should be removed and this can take a while when the paper is soaked. But, this is a crucial stage, the wallpaper will act as a seal to hold moisture next to the wall until it’s removed. You can find wallpaper removal products to dissolve the adhesive at your local hardware store. Then you will need a 3 inch up to a 6 inch scraping tool or flat broad knife to remove stubborn sections of wallpaper.\nWhen you dry out walls, it’s important to have patience because the process can take a long time. It may be tempting to rush the process or to start refinishing when the walls feel touch dry. But, this is a bad idea because the walls must be fully dry before you refinish affected rooms. One stage that many people miss is to check for the presence of mold growth in affected areas of the wall. Mold will release mold spores which are unhealthy to breathe and they will weaken the wall in the medium to long-term.\nIt’s important to dry the well walls quickly and correctly to reduce the potential for further damage. The walls will not fully dry if the problem is ignored and it will get worse which will drive up the eventual repair bill. The DIY approach is not recommended, a water damage remediation specialist has the experience and tools to dry walls faster.\nA restoration professional will have a tried and tested approach to your water damage problems. The technician will start with a survey of the affected areas to ascertain the true extent of the water damage. At this stage, they may use a non-invasive moisture meter which tests the walls with radio waves to prevent further damage. Another professional tool is the infrared imaging camera which displays where the water is located in the walls. When walls are wet there is more evaporation and this makes them look cooler than the walls that are still dry. Again, this is a non-invasive process and this helps the professional to identify the most affected areas for drying.\nIf the affected walls are not insulated, the restoration technician can dry the walls with no holes and the baseboard can stay in place. They will use high capacity air movers at 10-14 feet spacings along the entire wall, These will remove the excess moisture from the surface of the wall and then evaporate it quickly. A low grain refrigerant dehumidifier can be installed in a damp wall to prevent mold growth and reduce the humidity. Small holes may be drilled above the sill plate to get air into the wall cavity. This approach is a fast and effective way to dry water damage, but moisture barriers such as latex paints and wallpaper must be perforated or removed entirely. If the wall contains foil or plastic on the inner wall surface, it may be necessary to remove it because the wall will not dry properly.\nIf the affected walls are insulated, the process is similar to above if fiberglass insulation with paper backing is in place. But, foil-backed fiberglass, styrofoam or blown-in cellulose cannot be dried. The damaged wall sections will be removed with the insulation to start the rapid drying process and to prevent the growth of mold.\nThe restoration specialist will monitor the drying equipment at least once per day to check the progress. Moisture measurements will be taken to determine the drying times and when the dryness levels are equal to the unaffected areas the process is completed.\nIf you need plumbing pipe repair and water restoration services, contact your local professional plumber today.']	['<urn:uuid:c5382ebb-6ca4-4045-94ec-11122299758c>', '<urn:uuid:fdc0f2de-8c7b-4043-8f27-8a637816bd1c>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T18:05:20.075489	16	56	2237
75	How should journalists ensure fairness and handle conflicts of interest?	Journalists must approach every assignment with open minds and without prejudice, actively seeking opposing views. A story is only fair if it includes all significant facts, remains relevant, doesn't mislead viewers, and avoids subtle biases. Regarding conflicts of interest, journalists must avoid real or perceived conflicts, remaining free of associations that could compromise credibility. They should not take secondary employment, engage in political involvement, or serve in community organizations if these compromise journalistic integrity. Any unavoidable conflicts must be disclosed, and journalists must be vigilant about holding those with power accountable.	['Standards of Practice and Code of Ethics\nStandards of Practice and Code of Ethics\nNews and Information Services Division\nTelevision Broadcasts Limited\n- The primary purpose of all employees working in the News and Information Services Division of Television Broadcasts Limited (TVB News) is to inform the public of events of importance and appropriate interest in a manner that is fair, balanced, accurate and comprehensive. This primary purpose shall override all other purposes.\n- TVB News fully recognizes that the power it has inherited as one of the dominant news media in Hong Kong carries with it special responsibilities:\n- to listen to the voiceless;\n- to avoid any and all acts of arrogance;\n- to face the public politely and candidly.\n- TVB News is pledged to an aggressive, responsible and fair pursuit of the truth without fear or favor.\n- TVB News reporters, news editors, public affairs reporters/producers, executive producers, program directors, cameramen and tape editors are pledged to approach every assignment with the fairness of open minds and without prejudicial mindset. The search for opposing views must be as a matter of course. Comment from persons accused or challenged in stories must be included. The motives of those who press their views upon TVB News must as a matter of course be examined, and it must be recognized that those motives may be noble or ignoble, obvious or ulterior.\n- Conflict of Interest\nTVB News is pledged to avoid conflict of interest or the appearance of conflict of interest, wherever and whenever possible. We have adopted stringent policies on these issues, conscious that they may be more restrictive than is customary in the world of private business. In particular:\n- We pay our own way.\n- We accept no gifts from news sources. We accept no free trips. In cases in which reporting trips have been arranged by a news source, such as Central Liaison Office, and there are no other ways to cover such news events, every effort must be made to reimburse the news source for the trips at cost.\n- We neither seek nor accept preferential treatment that might be rendered because of the positions we hold. Exceptions to the no–gift rule are few and obvious—invitation to meals, for example.\n- Free admissions to any event that is not free to the public are prohibited. The only exception is for seats not sold to the public, as in a press section of a sports event. Whenever possible, arrangements should be made to pay for even such seats.\n- We work for no one except TVB News. Many outside activities and jobs are incompatible with the proper performance of work in an independent news organization. Connections with governments and financial institutions such as banks, investment firms, brokerages and stock markets are among the most objectionable.\n- We freelance for no one and accept no speaking engagements without permission from the Controller (News and Information Services). Permission for freelancing will be granted only if TVB News has no interest in the story and only if it is to appear in an overseas medium not circulated or broadcast in Hong Kong, Macau, Taiwan or Mainland China. It is important that no freelance assignments and no honoraria be accepted that might in any way be interpreted as disguised gratuities.\n- We make every reasonable effort to be free of obligation to news sources and to special interests. We must be wary of entanglement with those whose positions render them likely to be subjects of journalistic interest and examination. Our private behavior as well as our professional behavior must not bring discredit to our profession or to TVB News.\n- We are barred from active involvement in any partisan causes—politics, community affairs, social action, demonstrations—that could compromise or seem to compromise our ability to report and edit fairly.\n- Selection of Material\n- Material for newscasts and public affairs programs will be selected on the criteria of significance, community and regional relevance, appropriate human interest, service to TVB News’ defined viewerships.\n- Such selection standard excludes sensationalism or misleading emphasis in any form; subservience to external or “interested” party’s efforts to influence news selection and presentation, whether from within the broadcasting industry or from without.\n- Such selection standard also precludes undue use of sound and visual effects when such techniques add no relevant information to a news story or program, but are merely intended to pander to the visual and/or aural sensation of the viewer.\n- Such selection standard requires that bombastic or misleading promotional descriptions of newsroom facilities and personnel be rejected; and that promotional or publicity material be sharply scrutinized before use and identified by source or otherwise when broadcast.\n- The Role of TVB News reporters, news editors, public affairs reporters/ producers, executive producers, program directors, cameramen and tape editors\n- Reporters, news editors, public affairs reporters/producers, executive producers, program directors, cameramen and tape editors should make every effort to remain in the audience, to stay off the stage, to report the news, not to make the news.\n- In gathering news, reporters, news editors, public affairs reporters/ producers, and cameramen will not misrepresent their identity. They will identify themselves only as journalists, and not as police officers, physicians or members of any other profession or trade.\n- All undercover investigations, clandestine videofilming and unannounced audio recording must have prior approval of the executive news editor or the executive producer. (Also see Paragraph 15)\nTVB News is pledged to avoid errors and to correct those that occur, during the same newscast, public affairs program or online news update, if possible; otherwise, in the next newscast, public affairs program or online news update. Persons who call errors to our attention must be accorded a respectful hearing and given a courteous answer.\n- Attribution of Sources\n- TVB News is pledged to disclose the source of all information when at all possible. However, when a TVB News staffer agrees to protect a source’s identity, that identity will not be made known to anyone outside of TVB News.\n- Before any information is accepted without full attribution, reporters and producers must make every reasonable effort to get it on record. If that is not possible, reporters and producers should consider seeking the information elsewhere. If that in turn is not possible, reporters and producers should request an on–the–record reason for concealing the source’s identity and should include the reason in the story.\n- In any case, some kind of identification is almost always possible—by department or by position, for example—and should be reported.\n- No pseudonyms are to be used, unless clearly stated that they are pseudonyms and the reasons for using them instead of the interviewees’ real names.\n- Plagiarism and Credit\nAttribution of material from other media, whether print, electronic or online, and news agencies must be total. Plagiarism is one of journalism’s unforgivable sins. It is the policy of TVB News to give credit to other media that develop exclusive stories worthy of follow–up by TVB News.\nReporters, news editors, public affairs reporters/producers, executive producers, program directors, cameramen and tape editors of TVB News are committed to fairness. Fairness results from a few simple practices:\n- No story is fair if it omits facts of major importance or significance. Fairness includes completeness.\n- No story is fair if it includes essentially irrelevant information at the expense of significant facts. Fairness includes relevance.\n- No story is fair if it consciously or unconsciously misleads or even deceives the viewer. Fairness includes honesty. We must level with the viewers.\n- No story is fair if reporters and/or producers hide their biases or emotions behind subtle pejorative words. Fairness requires straight–forwardness ahead of flashiness.\nTVB News feels that it can serve its viewers best by providing them with the fullest information possible without tainting it with the reporter’s or editor’s/ producer’s own opinion on any given issue. However, this facts–only policy is not intended to eliminate from our coverage honest, in–depth reporting, or analysis, or commentary when plainly labeled as such.\n- Public Interest\nTVB News is vitally concerned with public interest. We believe this interest is best served by the widest possible dissemination of information. The claim of public interest by a government or business official does not automatically equate it with the interest of the public.\nTVB News is an organization that respects taste and decency. We shall avoid profanities and obscenities unless their use is so essential to a story of significance that its meaning is lost without them. In no case shall profanities or obscenities be used without the approval of either the executive news editor, managing editor, supervising producer, executive producer or a more senior supervisor.\n- Use of Hidden Camera and/or Microphone\n- Hidden cameras and/or microphones may be used only when the information obtained is of profound importance. It must be of vital public interest, such as revealing great “system failure” at the top levels, or it must prevent profound harm to individuals.\n- Hidden cameras and/or microphones may be used only when all other alternatives for obtaining the same information have been exhausted.\n- Hidden cameras and/or microphones may be used only when the harm prevented by the information revealed through deception outweighs any harm caused by the act of deception.\n- Hidden cameras and/or microphones may be used only when the journalists involved have conducted a meaningful, collaborative, and deliberative decision making process.\n- Hidden cameras and/or microphones may not be used for winning a prize, beating the competition, getting the story with less expense of time and resources, doing it because our competitors do it regularly, or the subjects of the story are themselves unethical.\n- Observing and Updating These Standards and Code\n- These Standards and Code are to be observed by all employees of TVB News in carrying out their duties and obligations as journalists.\n- Should there be questions on any of these Standards and Code, employees are encouraged to discuss same with their supervisors and/or news executives.\n- No rules and regulations are carved in stone; neither are therefore these Standards of Practice and Code of Ethics. When changes in community standards, circumstances and technology necessitate revisions of these Standards and Code, TVB News will do so accordingly, but only after extensive consultation with all employees.\nEnglish version updated in May, 2012.', 'The Wisconsin Center for Investigative Journalism is an independent, nonprofit and nonpartisan news organization that strives to uphold high standards of fairness and accuracy.\nThe Center’s ethics standards include the Society of Professional Journalists Code of Ethics, adopted in 1996 and endorsed by thousands of journalists around the world. That code is reprinted below, with permission. WCIJ’s Board of Directors have also adopted a conflict of interest policy and a diversity statement, which appear after the SPJ Code of Ethics.\nAdditional standards guiding the Center’s operations include:\n- The Center’s Policy on Financial Support, which requires that the Center’s news coverage be independent of donors and that all providers of revenue will be publicly identified.\n- Membership standards of the Institute for Nonprofit News (originally Investigative News Network), the nation’s first consortium of nonprofit investigative news organizations. The Center is a founding member of INN and the standards, developed with assistance of the Center’s leaders, require members to disclose information about donors and financial practices, produce nonpartisan investigative journalism, and apply high journalistic standards for accuracy and fairness.\nSociety of Professional Journalists Code of Ethics\nMembers of the Society of Professional Journalists believe that public enlightenment is the forerunner of justice and the foundation of democracy. The duty of the journalist is to further those ends by seeking truth and providing a fair and comprehensive account of events and issues. Conscientious journalists from all media and specialties strive to serve the public with thoroughness and honesty. Professional integrity is the cornerstone of a journalist’s credibility. Members of the Society share a dedication to ethical behavior and adopt this code to declare the Society’s principles and standards of practice.\nSeek Truth and Report It\nJournalists should be honest, fair and courageous in gathering, reporting and interpreting information.\n- Test the accuracy of information from all sources and exercise care to avoid inadvertent error. Deliberate distortion is never permissible.\n- Diligently seek out subjects of news stories to give them the opportunity to respond to allegations of wrongdoing.\n- Identify sources whenever feasible. The public is entitled to as much information as possible on sources’ reliability.\n- Always question sources’ motives before promising anonymity. Clarify conditions attached to any promise made in exchange for information. Keep promises.\n- Make certain that headlines, news teases and promotional material, photos, video, audio, graphics, sound bites and quotations do not misrepresent. They should not oversimplify or highlight incidents out of context.\n- Never distort the content of news photos or video. Image enhancement for technical clarity is always permissible. Label montages and photo illustrations.\n- Avoid misleading re-enactments or staged news events. If re-enactment is necessary to tell a story, label it.\n- Avoid undercover or other surreptitious methods of gathering information except when traditional open methods will not yield information vital to the public. Use of such methods should be explained as part of the story.\n- Never plagiarize.\n- Tell the story of the diversity and magnitude of the human experience boldly, even when it is unpopular to do so.\n- Examine their own cultural values and avoid imposing those values on others.\n- Avoid stereotyping by race, gender, age, religion, ethnicity, geography, sexual orientation, disability, physical appearance or social status.\n- Support the open exchange of views, even views they find repugnant.\n- Give voice to the voiceless; official and unofficial sources of information can be equally valid.\n- Distinguish between advocacy and news reporting. Analysis and commentary should be labeled and not misrepresent fact or context.\n- Distinguish news from advertising and shun hybrids that blur the lines between the two.\n- Recognize a special obligation to ensure that the public’s business is conducted in the open and that government records are open to inspection.\nEthical journalists treat sources, subjects and colleagues as human beings deserving of respect.\n- Show compassion for those who may be affected adversely by news coverage. Use special sensitivity when dealing with children and inexperienced sources or subjects.\n- Be sensitive when seeking or using interviews or photographs of those affected by tragedy or grief.\n- Recognize that gathering and reporting information may cause harm or discomfort. Pursuit of the news is not a license for arrogance.\n- Recognize that private people have a greater right to control information about themselves than do public officials and others who seek power, influence or attention. Only an overriding public need can justify intrusion into anyone’s privacy.\n- Show good taste. Avoid pandering to lurid curiosity.\n- Be cautious about identifying juvenile suspects or victims of sex crimes.\n- Be judicious about naming criminal suspects before the formal filing of charges.\n- Balance a criminal suspect’s fair trial rights with the public’s right to be informed.\nJournalists should be free of obligation to any interest other than the public’s right to know.\n- Avoid conflicts of interest, real or perceived.\n- Remain free of associations and activities that may compromise integrity or damage credibility.\n- Refuse gifts, favors, fees, free travel and special treatment, and shun secondary employment, political involvement, public office and service in community organizations if they compromise journalistic integrity.\n- Disclose unavoidable conflicts.\n- Be vigilant and courageous about holding those with power accountable.\n- Deny favored treatment to advertisers and special interests and resist their pressure to influence news coverage.\n- Be wary of sources offering information for favors or money; avoid bidding for news.\nJournalists are accountable to their readers, listeners, viewers and each other.\n- Clarify and explain news coverage and invite dialogue with the public over journalistic conduct.\n- Encourage the public to voice grievances against the news media.\n- Admit mistakes and correct them promptly.\n- Expose unethical practices of journalists and the news media.\n- Abide by the same high standards to which they hold others.\nMore information about SPJ and its Code of Ethics is available at www.spj.org.\nWisconsin Center for Investigative Journalism Conflict of Interest Policy\nThe following Financial Conflict of Interest Policy (“Conflict of Interest Policy”) is an effort (i) to ensure that the deliberations and decisions of the Wisconsin Center for Investigative Journalism (“WCIJ”) are made solely in the interest of promoting the quality of journalism in the state of Wisconsin, and (ii) to protect the interests of WCIJ when it considers any transaction, contract, or arrangement that might benefit or be perceived to benefit the private interest of a person affiliated with WCIJ (each, a “WCIJ Representative”). As used in this Conflict of Interest Policy, a WCIJ Representative includes any director, advisory board member, financial advisor, legal counsel or employee.\n- Duty to WCIJ. Each WCIJ Representative owes a duty to WCIJ to advance WCIJ’s legitimate interests when the opportunity to do so arises. Each WCIJ Representative must give undivided allegiance when making decisions affecting the organization. Similarly, WCIJ Representatives must be faithful to WCIJ’s non-profit mission and are not permitted to act in a way that is inconsistent with the central goals of the organization and its non-profit status.\n- Gifts. No WCIJ Representative shall personally accept gifts or favors that could compromise his or her loyalty to WCIJ. Any gifts or benefits personally accepted from a party having a material interest in the outcome of WCIJ or its employees by a WCIJ Representative individually should be merely incidental to his or her role as a WCIJ Representative and should not be of substantial value. Any gift with a value of $250 or more, or any gifts with a cumulative value in excess of $250 received by a WCIJ Representative in any twelve-month period from a single source, shall be considered substantial. Cash payments may not be accepted, and no gifts should be accepted if there are strings attached. For example, no WCIJ Representative may accept gifts if he or she knows that such gifts are being given to solicit his or her support of or opposition to the outcome or content of any WCIJ publication.\n- Personal Loans. WCIJ may not loan to, or guarantee the personal obligations of any WCIJ Representative.\n- Conflicts of Interest. The following are examples of conflicts of interest which must be promptly disclosed to the WCIJ Board of Directors pursuant to Section 4 below by any WCIJ Representative with knowledge of such conflict of interest:\n- (a) any real or apparent conflict of interest between a donor or the subject of a WCIJ publication or report and a WCIJ Representative;\n- (b) a WCIJ Representative’s ownership of an equity interest in a person or entity that is or will be the subject of a WCIJ publication or report; and\n- (c) failure to disclose to WCIJ all relationships between the subject of any WCIJ publication or report and any WCIJ Representative or close relatives of the WCIJ Representative.\n- Conflict Procedure:\n(a) If a WCIJ Representative or party related to a WCIJ Representative has an interest in any contract, action or transaction to be entered into with WCIJ, a conflict of interest or potential conflict of interest exists. Any WCIJ Representative having knowledge that such a conflict of interest exists or may exist (an “Interested WCIJ Representative”) will so advise the Board of Directors promptly. An Interested WCIJ Representative will include in the notice the material facts as to the relationship or interest of the Interested WCIJ Representative in the entity proposing to enter into a contract, action or transaction with WCIJ.\n(b) Notwithstanding anything herein to the contrary, the Board of Directors may authorize any committee appointed pursuant to the WCIJ by-laws (a “Committee”) to act in lieu of the Board of Directors in determining whether an action, contract or transaction is fair to WCIJ as of the time it is authorized or approved by the Committee.\n(c) At any time that a conflict of interest or potential conflict of interest is identified, the Chair of the Board or a Chair of the applicable Committee will ensure that such conflict of interest is placed on the agenda for the next meeting of the Board of Directors or the Committee, as applicable. The notice of such meeting of the Board of Directors or the Committee, as applicable, will include, to the extent available when the notice is sent, a description of the conflict of interest matter to be discussed. By notice before the meeting or at the meeting, the directors on the board or the Committee, as applicable, will be advised that a vote will be taken at the meeting and that, in order to authorize the relevant contract, action or transaction, an affirmative vote of a majority of disinterested directors present at the meeting at which a quorum is present will be required and will be sufficient, even though the disinterested directors constitute less than a quorum of the Board of Directors or the Committee.\n(d) Reasonable effort will be made to cause the material facts concerning the relationships between the individuals and WCIJ which create the conflict to be delivered to and shared with the members of the Board of Directors or the Committee, as applicable, prior to the meeting to enable the directors to arrive at the meeting prepared to discuss the issue. In the event it is not practicable to deliver the information prior to the meeting, it will be delivered to the directors at the meeting, and the directors can act upon the matter with the same authority as if notice had been given prior to the meeting.\n(e) The Board of Directors or the Committee, as applicable, will invite all parties to the conflict of interest to attend the meeting, to make presentations and to be prepared to answer questions, if necessary. The Board or Directors or the Committee, as applicable, will also invite outside experts if necessary.\n(f) At the meeting, providing a quorum is present, the conflict will be discussed to ensure that the directors present are aware of the issues and the factors involved. The interested directors may be counted for purposes of a quorum, even though they may not take part in any vote on the issues.\n(g) The Board of Directors or the Committee, as applicable, must decide, in good faith, reasonably justified by the material facts, whether the action, contract or transaction would be in the best interest of WCIJ and fair to WCIJ as of the time it is authorized or approved.\n(h) All interested directors must abstain from voting and, if necessary, leave the room when the vote is taken.\n(i) The Board of Directors or the Committee, as applicable, will maintain a written account of all that transpires at the meeting and incorporate such account into the minutes of the meeting and disseminate it to the full Board of Directors. Such minutes will be presented for approval at the next meeting of the Board of Directors and maintained in the corporate record book.\n(j) To the extent that the conflict of interest is continuing and the contract, action or transaction goes beyond one (1) year, the foregoing notice and discussion and vote will be repeated on an annual basis.']	['<urn:uuid:16b61d8f-c8e9-4e00-b485-c387a5e17d22>', '<urn:uuid:7e0634d5-69bf-41dd-a6c5-4a41c40d40e7>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:05:20.075489	10	91	3905
76	I'm curious about unique architectural views - what's the difference between how Becune Point in St. Lucia and Aveo Bella Vista in Sydney are positioned relative to their surroundings?	Becune Point juts out to sea at the northwest tip of St. Lucia and is open to the sea on three sides, offering views of Pigeon Island. In contrast, Aveo Bella Vista in Sydney features a staggered arrangement of buildings within its masterplan, with its facade designed to create different pressure zones for natural ventilation.	"[""In an interview with Rose Styron in 1997, Derek Walcott described the place where he lives in St. Lucia as “here . . . near the sea, up at Becune Point.” Becune Point juts out to sea at the northwest tip of St. Lucia. It is open to the sea on three sides and looks out to Pigeon Island and the ghosts of St. Lucian history Walcott unearthed through Major Plunkett’s dreams of a soldier ancestor stationed at Pigeon Island during the Battle of the Saintes. The house in which he lives was purchased shortly after his being awarded the Nobel Prize. As Hilton Als described it in the New Yorker in 2004:\nWalcott’s house is actually three houses resting on a bluff above the sea. There’s the main house, where he and Sigrid eat and sleep; his studio; and another little house, for guests. At the center of the structures is a white lap pool. The interior of the main house is dark, and the rooms are like cabins on a ship. There are couches and bookcases. Walcott’s studio has a loft with a bed. On the lower level, where he works, some of his paintings are stacked on the floor or tucked into big wooden flat files. His manual typewriter, an Olivetti, faces the sea. Sigrid told me, “When Derek won the prize, he said, ‘Quick, find a house!’ He never really owned much of anything before.”\nWe set out to walk to Becune Point, looking for the spot from which Walcott had painted his beautiful watercolors of the Point-that “incredible blue with its bird-inviting cloud” as he writes in Omeros. We started our walk at the old Great House at Cap Estate, formerly a 1,500 acre plantation now subdivided into lots for luxury homes. Next to the house (now a restaurant) stands the open-air Derek Walcott Center Theater.\nWe made our way down the hill past a housing development , heading towards the sea and the Point. A young man on horseback pointed us in the direction of the water, where, as if in a vision, we found the horses Walcott describes in his St. Lucian epic....\nQuesta introduzione è presa da REPEATING ISLANDS http://repeatingislands.com/2009/03/24/derek-walcott-and-becune-point/\nBY DEREK WALCOTT\nStunned heat of noon. In shade, tan, silken cows\nhide in the thorned acacias. A butterfly staggers.\nStamping their hooves from thirst, small horses drowse\nor whinny for water. On parched, ochre headlands, daggers\nof agave bristle in primordial defense,\nlike a cornered monster backed up against the sea.\nA mongoose charges dry grass and fades through a fence\nfaster than an afterthought. Dust rises easily.\nHaze of the Harmattan, Sahara dust, memory’s haze\nfrom the dried well of Africa, the headland’s desert\nor riders in swirling burnooses, mixed with the greys\nof hills veiled in Impressionist light. We inherit\ntwo worlds of associations, or references, drought\nthat we heighten into Delacroix’s North Africa,\nveils, daggers, lances, herds the Harmattan brought\nwith a phantom inheritance, which the desperate seeker\nof a well-spring staggers in the heat in search of—\nheroic ancestors; the other that the dry season brings\nis the gust of a European calendar, but it is the one love\nthat thirsts for confirmations in the circling rings\nof the ground dove’s cooing on stones, in the acacia’s\nthorns and the agave’s daggers, that they are all ours,\nthe white horsemen of the Sahara, India’s and Asia’s\nplumed mongoose and crested palmtree, Benin and Pontoise.\nWe are history’s afterthought, as the mongoose races\nahead of its time; in drought we discover our shadows,\nour origins that range from the most disparate places,\nfrom the dugouts of Guinea to the Nile’s canted dhows.\nStupefatta arsura del mezzogiorno. Nell'ombra, mucche pezzate dal serico mantello\nsi nascondono tra le acacie spinose. Vacilla una farfalla.\nPestando gli zoccoli per la sete, cavallini sonnecchiano\no nitriscono per l'acqua. Sui promontori riarsi, color d'ocra, gli aculei\ndelle agavi si rizzano in primordiale difesa,\ncome un mostro con le spalle al muro appoggiato contro il mare.\nUna mangusta ruba un poco d' erba secca e svanisce traverso uno steccato\npiù veloce di un riflesso. Polvere si alza indifferente.\nFoschia dell' Harmattan, polvere del Sahara, foschia della memoria\ndai pozzi prosciugati d'Africa, il deserto del promontorio\no cavalieri in mantelli volteggianti, confusi con i grigi\ndelle colline velate da una luce Impressionista. Noi ereditiamo\ndue mondi di associazioni, o di riferimenti, la siccità\nche esasperiamo nel Nord Africa di Delacroix,\ne le vele,i pugnali,le lance, che raduna il vento di Harmattan portato\ncon irreale retaggio, in cui il cercatore disperato\ndi una fresca sorgente barcolla nella calura alla ricerca di --\neroici antenati; l' altra cosa che la stagione secca porta\nè la raffica di un calendario Europeo, ma è il solo amore\nche ha sete di conferme negli anelli volteggianti\ndel tubare delle tortore sulle rocce, tra le spine\ndelle acacie e gli aculei delle agavi, e sono tutti nostri,\ni bianchi cavalieri del Sahara, la mangusta pelosa e\nla palma crestata d'India e d'Asia, Benin e Pontoise.\nSiamo il riflesso della storia, come la mangusta corre\nin anticipo sui tempi; nella siccità scopriamo le nostre ombre,\nle nostre origini che spaziano dai luoghi più diversi,\ndalle piroghe della Guinea al curvo sambuco del Nilo."", 'Aveo Bella Vista in Sydney, NSW Residential Apartment Building, Australian Residence, Architecture Images\nWaratah, Aveo Bella Vista in Sydney\nWaratah: Contemporary Real Estate in New South Wales, Australia – design by Jackson Teece\n16 Sep 2018\nAveo Bella Vista\nLocation: Sydney, New South Wales, Australia\nArchitects: Jackson Teece\nAveo Bella Vista\nWaratah, Aveo Bella Vista represents one of the largest applications of CLT in a multi-residential development in Australia. This independent seniors living project designed by Jackson Teece raises the bar not only in providing superior amenity to the end-user but also in pushing the boundaries of what is presently perceived possible in mass timber design and construction technology.\nAs designers, engineers and builders increase their knowledge and confidence around this evolving construction methodology, the market will become more educated and therefore more competitive, and we will continue to see more daring hybrid timber buildings developed.\nWARATAH – Aveo Bella Vista is, in a way, the first of its’ kind. The design for this 10 storey residential apartment building (containing approximately 2700m3 of CLT) breaks the mould of the typical mass timber designs we have seen coming out of Europe and being realized internationally over the last decade. Originally conceived as a conventional build, Aveo were guided by Jackson Teece and Stongbuild to incorporate CLT into the design, to demonstrate industry leadership.\nWhat sets this project apart from other recently completed mass timber buildings is the increased focus on architectural expression and functional planning over a design purely driven by construction efficiencies. Mass timber buildings to date are typically ‘boxy’ looking (no curves) with squared-off, strictly stacking floor plans and inbound balconies (not cantilevers, within the façade outline, bound by 3 walls), while the application of hybrid construction methods (combining engineered timber, steel and/or concrete) is not yet well established.\nThis project had to achieve a balance between the desired architectural design outcome and the construction rationale. Some of the challenges involved complicated floor slab junctions, in order to realize the desired apartment layouts and sophisticated hybrid steel and timber connections to achieve the curved, seemingly cantilevered balconies. Due to the market perception of timber in construction, CLT elements were also clad and wrapped in fire-rated material. As a reflection of the internal structure of the building, exposed timber elements in a series of Glulam columns support large glazing elements to the main entry lobby.\nKey to the successful delivery of this project was early client education, enthusiasm and commitment, specialist consultant input relating to the construction methodology, and a capable builder. Jackson Teece, together with TDA and Wood Solutions organised a study tour to Austria and Switzerland with the client, builder and TTW structural engineers. This deepened the team’s understanding of timber engineering, fabrication and construction technologies and as a result, has increased the prominence of this sustainable construction methodology in Australia.\nThe raw material was sourced through BinderHolz in Austria, a CLT provider whose production sites meet and are certified under strict PEFC standards. Their no-waste principle during fabrication means that 100% of every logged tree is used. Production by-products such as bark, cut-off pieces, wood chips, sawdust and shavings are processed into biofuels in the form of pellets and briquettes or converted into climate-neutral thermal energy or green electricity. Such highly efficient and streamlined fabrication processes, currently not available in Australia, offset the embodied energy generated through transportation to a large degree.\nEnergy efficient design has been fundamental throughout the project’s development. The staggered arrangement of buildings within the masterplan as well as the building’s façade articulation aids natural ventilation by creating different pressure zones along the face. Zonal control of the mechanical ventilation allows for reduction of energy use.\nAll residential corridors have access to natural light and ventilation, providing air to the units and further reducing the need for mechanical ventilation. Mechanical services have been specifically designed for the additional air tightness achieved in this mass timber building, thus reducing ongoing operational costs for heating and cooling. Continuous carbon storage within the significant volume of mass timber used in the building further reduces its carbon footprint, during its construction and in future operation.\nThe proposal aspires to raise the bar not only in providing superior amenity to the user, but also spearheading sustainable construction technology through the use of CLT. The advantages of using this product are not only seen in the environmental benefits and speed of construction, but also in the upfront design resolution, ultimately resulting in an improved project outcome and a high quality end product.\nAveo Bella Vista, Sydney – Building Information\nProject size: 8800 sqm\nSite size: 59413 sqm\nProject Budget: $70000000\nCompletion date: 2018\nBuilding levels: 10\nArchitects: Jackson Teece\nPhotographer: Brett Boardman\nAveo Bella Vista in Sydney images / information received 160918\nAddress: Aveo Bella Vista, Circa Retail Shopping Complex, Bella Vista, New South Wales 2153\nPhone: 13 28 36\nNorbrik Drive, Bella Vista, NSW, 2153\nArchitecture in Sydney\nContemporary New South Wales Buildings\nNew Moore Park Stadium, Driver Avenue, south-east Sydney\nArchitects: Bates Smart\npicture from Bates Smart\nNew Moore Park Stadium Building\nDesign: SAOTA and TKD Architects\nphotograph : Justin Alexander\nThe Brass House, Newcastle, New South Wales\nphoto : Jon Reid\nContemporary House in Newcastle, NSW\nSovereign Houses, Sylvania\nArchitects: Tony Owen Partners\nphoto : John Gollings\nSovereign Houses in Sylvania\nComments / photos for the Aveo Bella Vista in Sydney page welcome\nAveo Bella Vista in Sydney NSW']"	['<urn:uuid:13da7c71-f8aa-4873-a23f-8877bc00bcd1>', '<urn:uuid:2d81ab2e-8e8f-45e6-a805-a1f46c434573>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T18:05:20.075489	29	55	1783
77	lutheran church bible version sacraments differences between denominations	Lutheran denominations differ in their practices while sharing some common elements. The Evangelical Lutheran Church in America (ELCA) primarily uses the New Revised Standard Version (NRSV) of the Bible for their liturgical resources and study. Regarding sacraments, Lutherans recognize only two sacraments: baptism and the Lord's Supper (Eucharist), in contrast to the seven sacraments of the Catholic Church. The different Lutheran denominations include the Evangelical Lutheran Church in America (ELCA) and the Lutheran Church-Missouri Synod (LCMS), which have distinct practices, such as the LCMS being more conservative and not ordaining women, while the ELCA does.	['- 1 Does the Lutheran Church recognize saints?\n- 2 Can Lutherans pray to saints?\n- 3 Is Catholic and Lutheran the same?\n- 4 Do Lutherans use the rosary?\n- 5 Do Lutherans believe in the Virgin Mary?\n- 6 Do Lutherans do the sign of the cross?\n- 7 What Do Lutherans think about saints?\n- 8 Do Lutherans believe in confession?\n- 9 Do Lutherans do Ash Wednesday?\n- 10 What religion is similar to Lutheran?\n- 11 What religion is close to Catholic?\n- 12 Can a Lutheran marry a Catholic?\n- 13 Which version of the Bible do Lutherans use?\n- 14 Why don t Lutherans do the sign of the cross?\n- 15 What are the 2 sacraments of the Lutheran Church?\nDoes the Lutheran Church recognize saints?\nnever been officially canonized by the Roman Catholic Church; the Lutheran Church has a different view of sainthood, has no machinery for canonization, and is very inconsistent in the use of the title ” Saint ” before a name.\nCan Lutherans pray to saints?\nLutheran views The Lutheran confessions approve honoring the saints by thanking God for examples of his mercy, by using the saints as examples for strengthening the believers’ faith, and by imitating their faith and other virtues. However, the confessions strongly reject invoking the saints to ask for their help.\nIs Catholic and Lutheran the same?\nDoctrinal Authority: Lutherans believe that only the Holy Scriptures hold authority in determining doctrine; Roman Catholics give doctrinal authority to the Pope, traditions of the church, and the Scriptures. Lutherans also reject many elements of Catholic sacraments such as the doctrine of transubstantiation.\nDo Lutherans use the rosary?\nLutherans can pray the rosary, but generally do not. There are a number of reasons for this. (The Lord’s Prayer, which is said as part of the rosary, is in the Small Catechism.) The major theological sticking point is the “Hail Mary” portion, which involves praying to Mary rather than to God or Jesus.\nDo Lutherans believe in the Virgin Mary?\nLutherans have always believed that Mary is the Theotokos, the God-bearer. Martin Luther said: [S]he became the Mother of God, in which work so many and such great good things are bestowed on her as pass man’s understanding. Therefore she is truly the mother of God and yet remained a virgin.\nDo Lutherans do the sign of the cross?\nAmong Lutherans the practice was widely retained. For example, Luther’s Small Catechism states that it is expected before the morning and evening prayers. ‘” Since then, the sign of the cross has become fairly commonplace among Lutherans at worship. The sign of the cross is now customary in the Divine Service.\nWhat Do Lutherans think about saints?\nTraditional Lutheran belief accounts that prayers to the saints are prohibited, as they are not mediators of redemption. But, Lutherans do believe that saints pray for the Christian Church in general.\nDo Lutherans believe in confession?\nBeliefs. The Lutheran Church practices ” Confession and Absolution” [referred to as the Office of the Keys] with the emphasis on the absolution, which is God’s word of forgiveness. Indeed, Lutherans highly regard Holy Absolution.\nDo Lutherans do Ash Wednesday?\nAsh Wednesday is observed by Western Christianity. Roman Rite Roman Catholics observe it, along with certain Protestants like Lutherans, Anglicans, some Reformed churches, Baptists, Nazarenes, Methodists, Evangelicals, and Mennonites.\nWhat religion is similar to Lutheran?\nAlong with Anglicanism, the Reformed and Presbyterian (Calvinist) churches, Methodism, and the Baptist churches, Lutheranism is one of the five major branches of Protestantism.\nWhat religion is close to Catholic?\nOverall, Anglican/Episcopal and Lutheran are closest to the Roman Catholic / Eastern Orthodox Church.\nCan a Lutheran marry a Catholic?\nTechnically, marriages between a Catholic and a baptized Christian who is not in full communion with the Catholic Church (Orthodox, Lutheran, Methodist, Baptist, etc.) One is Catholic and the other is either Lutheran or Presbyterian.\nWhich version of the Bible do Lutherans use?\nThe Lutheran body to which I belong, The Evangelical Lutheran Church in America (ELCA) has a strong preference for the New Revised Standard Version (NRSV). Our Liturgical resources all use the NRSV, the Lutheran study Bible we use and the accepted text to use at seminary and I assume the colleges as well is the NRSV.\nWhy don t Lutherans do the sign of the cross?\nDr. Maschke: [Making] the sign of the cross is …a physical action that draws the whole self into the act of worship. Some people may consider this is a “Catholic” practice, and in the past this connotation caused many Lutherans to abandon its use.\nWhat are the 2 sacraments of the Lutheran Church?\nAs opposed to the seven sacraments of the medieval Catholic Church, the Lutheran reformers quickly settled on only two: baptism and the Lord’s Supper ( Eucharist ).', '- 1 What do Missouri Synod Lutherans believe about communion?\n- 2 Does Missouri Synod practice closed communion?\n- 3 Can ELCA take communion at LCMS?\n- 4 What Lutherans believe about communion?\n- 5 Does Missouri Synod have female pastors?\n- 6 What Bible do the Missouri Synod Lutherans use?\n- 7 What is the difference between the Missouri Synod and Wisconsin Synod?\n- 8 Is the Missouri Synod conservative?\n- 9 Why should Christians practice closed communion?\n- 10 What does ELCA stand for in the Lutheran church?\n- 11 What is the difference between Lcmc and ELCA?\n- 12 What is the difference between Lutheran and Pentecostal?\n- 13 Do Lutherans believe you have to be baptized to go to heaven?\n- 14 What are the two types of Lutherans?\n- 15 Can a Catholic take communion at a Lutheran church?\nWhat do Missouri Synod Lutherans believe about communion?\nThe LCMS endorses the doctrine of close or closed communion —the policy of sharing the Eucharist ordinarily only with those who are baptized and confirmed members of one of the congregations of the Lutheran Church— Missouri Synod or of a congregation of one of its sister churches with which it has formally declared altar\nDoes Missouri Synod practice closed communion?\nConfessional Lutheran churches, including the Lutheran Church– Missouri Synod and the Wisconsin Evangelical Lutheran Synod, practice closed communion and require catechetical instruction for all people before receiving the Eucharist. Failing to do so is condemned by these Lutherans as the sin of unionism.\nCan ELCA take communion at LCMS?\nUnlike certain other American Lutheran church bodies, the ELCA practices open communion, permitting all persons baptized in the name of the Trinity with water to receive communion. Some congregations also commune baptized infants, similarly to Eastern Orthodox practice.\nWhat Lutherans believe about communion?\nLutherans believe that the Body and Blood of Christ are “truly and substantially present in, with and under the forms” of consecrated bread and wine (the elements), so that communicants eat and drink both the elements and the true Body and Blood of Christ himself in the Sacrament of the Eucharist whether they are\nDoes Missouri Synod have female pastors?\nThe General Lutheran Church ordains women. The Lutheran Church– Missouri Synod (LCMS), which is the second largest Lutheran body in the United States, does not ordain women. The Lutheran Congregations in Mission for Christ (LCMC) also allows for the ordination of women.\nWhat Bible do the Missouri Synod Lutherans use?\nThe Lutheran body to which I belong, The Evangelical Lutheran Church in America (ELCA) has a strong preference for the New Revised Standard Version (NRSV). Our Liturgical resources all use the NRSV, the Lutheran study Bible we use and the accepted text to use at seminary and I assume the colleges as well is the NRSV.\nWhat is the difference between the Missouri Synod and Wisconsin Synod?\nThe main facets of doctrinal difference between the WELS and the Lutheran Church– Missouri Synod ( LCMS ) include: The LCMS teaches that only the pastoral office is divinely established, while all other church offices are human institutions.\nIs the Missouri Synod conservative?\nLutheran Church– Missouri Synod, conservative Lutheran church in the United States, organized in Chicago in 1847 by German immigrants from Saxony (settled in Missouri ) and Bavaria (settled in Michigan and Indiana) as the German Evangelical Lutheran Synod of Missouri, Ohio, and Other States.\nWhy should Christians practice closed communion?\nThe practice of giving Holy Communion to only those of your denomination is what many Catholic, Orthodox, confessional Lutheran and other Christians call “ closed Communion.” It upholds a deep, reverent understanding of the Lord’s supper and limits it to those instructed about the supper and all other doctrines of\nWhat does ELCA stand for in the Lutheran church?\nBACKGROUND. Of the three leading Lutheran organizations operating in the United States, the Evangelical Lutheran Church in America (ELCA) is considered the most welcoming and inclusive of LGBTQ members.\nWhat is the difference between Lcmc and ELCA?\nLCMC is congregational in structure, rejecting the historic episcopate which was adopted by the ELCA, the denomination to which many LCMC members had previously belonged, in the (CCM) Call to Common Mission. The beliefs of the LCMC are based on the Bible and the Lutheran confessions in the Book of Concord.\nWhat is the difference between Lutheran and Pentecostal?\nPentecostal: Perhaps the main difference between Lutheran and Pentecostal churches is that Pentecostal churches tend to emphasize the importance of personal and spiritual “experiences” (such as “Baptism in the Holy Spirit” and speaking in tongues), while the Lutheran doctrine emphasizes the importance and centrality of\nDo Lutherans believe you have to be baptized to go to heaven?\nAccording to the Lutheran church, baptism isn’t necessary for salvation. A baby’s entrance into Heaven doesn’t depend on whether his parents had the time to get him baptized prior to his death.\nWhat are the two types of Lutherans?\nThe Evangelical Lutheran Church in America was formed in 1988 by the merger of two major Lutheran denominations, the American Lutheran Church and the Lutheran Church in America, along with the much smaller Association of Evangelical Lutheran Churches.\nCan a Catholic take communion at a Lutheran church?\nCatholics believe these become the body and blood of Christ; some Protestants, notably Lutherans, say Christ is present in the sacrament. Protestants are currently allowed to receive Catholic communion only in extreme circumstances, such as when they are in danger of death.']	['<urn:uuid:80a81d90-ba58-4401-9ad2-ccd7e3eb8c72>', '<urn:uuid:542e1b9f-6ce0-46db-8382-592ea652ba47>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T18:05:20.075489	8	95	1722
78	textile industry environmental impact water usage	The textile industry is the second largest polluter of the planet after the coal industry and is a massive consumer of natural resources like land and water. A striking example is that producing a single cotton t-shirt requires 3,000 liters of water. In contrast, some innovative sustainable fabrics have been developed that use just 22 liters of water to produce a t-shirt, which is less than 1% of the water needed for a regular cotton tee.	['Technology Is The New Black | Article by Graham Ross\nBack in 2000 when the concept of ‘fast’ fashion first took off, the industry produced 50 billion garments per year. With streamlined supply chains and greatly reduced production timelines, fashion companies on average went from offering two collections a year to five. H&M offers at least twelve and Zara now produces 24 collections a year – that’s two a month! In just four years those 50 billion garments doubled.\nNot so many years ago we hit the 100 billion mark for the number of garments produced worldwide annually. The numbers are almost impossible to process, but what’s truly disturbing is that 85% of all textiles produced globally still end up in landfill.\nWhen we discovered that the textile industry is the second largest polluter of the planet behind the coal industry and that it’s also a massive user of precious natural resources like land and water, we were shocked.\nThen we discovered that the favourite cotton t-shirt that we all have in our wardrobes needs an astonishing 3000 litres of water to make. We thought there had to be a better way. So we developed our own fabrics.\n“Less than 1% of material used to produce clothing is recycled into new clothing.”\nWe combined technology and plants to create four unique sustainable fabrics, and then we made a t-shirt. ‘The Greenest T-shirt on the Planet’ was successfully funded via crowdfunding platform Kickstarter in 2015.\nOur Greenest Tee uses just 22 litres of water to produce – that’s less than 1% of the 3000 litres needed for a regular cotton tee… With minimal chemicals, i’s also biodegradable, so at the end of it’s life cycle ‘The Greenest Tee’ doesn’t become landfill,\nit returns to the earth.\nThe waste statistics in fashion are frightening: Less than 1% of material used to produce clothing is recycled into new clothing, a loss of more than $100 USD billion worth of materials each year.\nEvery second, the equivalent of one garbage truck of textiles is landfilled or burned.\nThink about that for a moment…\nLike many industries, the fashion industry must re-imagine the way it does business. The way it’s structured today is not set up to solve the problems we have created for ourselves as a society. We can no longer afford to be passive bystanders.\nWe all know there’s no easy solution and we can’t expect a quick fix from individual governments or rich philanthropists … we need to collaborate and solve this together.\nIn industries with problems like enormous waste, huge use of natural materials and negative impact on the environment, operating a linear supply chain makes no sense. A circular economy is the blueprint to connect up the global ecosystem, one that thrives on connectivity, transparency, social purpose and sustainability. It’s the ideal approach to solve problems as big as those we face in fashion.\nTechnology is the tool that can unlock the circular economy.\nI’m not talking about the technology we call wearables, those clever materials and garments that can monitor our body’s activity – heart rate, breath, sweat rate and sleep or even those odd shirts that use ‘Colour Change Technology’. I wonder what happened to them?\nThe time is now for industry and consumers alike to call for truly ‘moonshot’ innovations, ideas and products that spring from free thinking and harness technology that will give rise to the fashion industry 2.0 and beyond – Where future products have minimal impact on the environment, where an industry operates with reuse and recycle, and where materials return to earth.\nWhere did we get lost? Polyester, the world’s most used material, hasn’t changed much since it was patented way back in the 1950s. Thankfully, there are individuals and companies leading industry change, unlocking new approaches to old methods, combining our current day and future technologies with nature. Footwear company Vivobarefoot wanted to tackle the harmful Synthetic & Petrochemical foams that form part of many shoes, so they teamed up with US company Bloom to create a shoe that’s made from algae biomass.\nGerman company Algalife also saw the beauty in algae and realised its innovative and sustainable applications through biotechnology to develop pigments and fibres from the tiny micro-organisms.\nOld world materials can also breathe new life, technology is helping the Infinited Fiber Company make new cotton-like fibres from recovered cotton cellulose and other biomaterials.\nThere is no doubt technology is clever and a whole lot more interesting than textile waste management but let’s not undervalue the very important role recycling has to play, especially while the industry continues to rely on planet-damaging materials and over supply.\n“We can no longer stand to be passive bystanders.”\nIn the short term, recycling means a focus on closing the loop on waste, if consumers continue to feed the machine that is fast fashion, then attitudes to post-consumer waste must evolve and evolve quickly.\nIn the time it takes to read this article, another 6,000kg of Australian textiles has been dumped in landfill.\nIn the mid term, the quest for textile professionals and the consumers who buy our products is to find new ways to evolve what the term ‘recycling’ means to us today.\nCurrent textile recovery methods are clunky and damaging to materials that could maintain their high quality state. This is driven by not looking for a better end product, rather settling for a market opportunity that doesn’t advance an industry’s directive.\nChemical recycling offers plenty of advantages, recovering fibres at a higher quality than mechanical means but this process is challenged by chemical and energy usage. Both methods must play their part in closing the loop, the opportunity for technology is to support their uptake by making them more efficient and greener.\nEU funded research project Resyntex is leading the way for current and future textile recyclers, by using industrial symbiosis to make new raw materials derived from unusable textile waste. The technology releases the valuable resources that would otherwise be discarded.\nWe can no longer afford to be passive bystanders.\nAdopting a circular economy will begin to reverse environmental damage and change attitudes, but like Vivobarefoot, Algalife and Infinited Fiber Company, the future fashion industry will require bolder thinking that finds and releases connections between man-made genius and the planet that gives us life.']	['<urn:uuid:2af4dae7-c967-4ae7-97f0-035c7cd52214>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T18:05:20.075489	6	76	1054
79	As a trauma therapist, I'm curious about how EMDR therapy facilitates healing from psychological trauma, and what role it might play in promoting post-traumatic growth?	EMDR therapy facilitates healing by directly impacting the brain's information processing system. During EMDR, bilateral stimulation helps clients process disturbing memories through eye movements or other sensory stimuli, transforming painful events on an emotional level. For instance, abuse victims can shift from feelings of horror to recognizing their strength and survival. EMDR helps the mind heal from psychological trauma similarly to how the body heals from physical wounds - once blocks are removed, natural healing processes resume. Beyond trauma treatment, EMDR can be life-changing in terms of improved functioning and reduced traumatic symptoms. Studies show it is highly effective - in one study, over 61% of participants no longer met PTSD criteria after EMDR treatment. This therapy can contribute to post-traumatic growth, as it helps people develop new perspectives and meaning from their experiences, potentially leading to positive psychological changes including greater personal strength, improved relationships, and enhanced appreciation for life.	['What is EMDR?\nEMDR (Eye Movement Desensitization and Reprocessing) is a psychotherapeutic process that helps people heal from the symptoms and emotional distress caused by disturbing life experiences. This treatment approach has proven to be beneficial for those suffering the emotional and physical reactions of abuse, assault, accidents and combat. It is also a useful tool to help treat individuals with other forms of trauma such as betrayals, conflicts and bullying.\nMany people believe that severe emotional pain requires a long time to heal. EMDR therapy shows that the mind can in fact heal from psychological trauma much as the body recovers from physical trauma. When you cut your hand, your body works to close the wound. If a foreign object or repeated injury irritates the wound, it festers and causes pain. Once the object is removed or the repetitive injuries cease, healing resumes. EMDR therapy demonstrates that a similar sequence of events occurs with mental processes. The brain’s information processing system naturally moves toward mental health. If the system is blocked or imbalanced by the impact of a disturbing event, the emotional wound festers and can cause intense suffering. Once the block is removed, healing resumes. By employing EMDR’s detailed protocols and procedures, clients can activate their natural healing processes.\nIn addition to treating “Big T” trauma, EMDR can be very effective in treating the “everyday” memories that are the reason people have low self-esteem, feelings of powerlessness, and all the myriad problems that bring them to therapy.\nHow does EMDR work?\nIn many instances when a person is very upset due to a traumatic event or situation, their brain cannot process information as it does ordinarily. One moment becomes “frozen in time,” and remembering a trauma may feel as disturbing as going through it the first time because the images, sounds, smells, and feelings haven’t changed. Such memories have a lasting negative effect that interferes with the way a person sees the world, the way they relate to others, and the thoughts and feelings they have about themselves.\nEMDR seems to have a direct impact on the way that the brain processes information. Following a successful course of EMDR, a person no longer finds his/herself as disturbed by the images, sounds, sensations and feelings when the traumatic event is brought to mind. A person will still remember what happened, but it will be less upsetting. Therefore, EMDR can be thought of as a psychotherapeutic intervention that helps a person see disturbing material in a new and less distressing way.\nEMDR is a non-invasive technique employing the bilateral stimulation of the brain. Bilateral stimulation is a method of alternately engaging both sides of your brain through a series of visual or tactile movements or through sound.\nEMDR therapy is a multi-phase treatment incorporating assessment, stress reduction techniques, processing using bilateral stimulation, and evaluating progress. In treatment I first work my clients to determine which traumatic memory to target first. I then ask the client to hold different aspects of the event or thought in mind and to use his eyes to track hand movements or beams of light as it moves back and forth across his field of vision. As this happens, internal associations arise and the client begins to process the memory and disturbing feelings. In successful EMDR therapy, the meaning of painful events is transformed on an emotional level. For instance, an abuse victim shifts from feeling horror and self-disgust to holding the firm belief that, “I survived the abuse and I am strong.” Unlike talk therapy, the insights clients gain in EMDR result not so much from the therapist’s interpretation, but from the client’s own accelerated intellectual and emotional processes. The end result is that clients conclude EMDR therapy feeling empowered by the very experiences that once debased them. Their wounds have not just closed, they have transformed.', 'Post-Traumatic Stress Disorder (PTSD) is a mental health disorder that occurs in people who have experienced a stunning, terrifying, or deadly event. It is normal to experience fear both during and after a distressing event. In order to defend against or avoid danger, fear causes several split-second changes in the body. A typical response designed to protect a person from harm is the “fight-or-flight” response.\nThe fight or flight response is a physiological response that occurs automatically in response to a stressful or frightful experience. Threat perception engages the sympathetic nervous system, causing an acute stress reaction that primes the body for either fighting or running away. These reactions are evolutionary adaptations that improve survival chances in dangerous circumstances.\nAlmost everyone reacts differently to trauma, although the majority of people naturally overcome the initial symptoms. Those who still have issues may be given a PTSD diagnosis. Even when there is no threat, PTSD sufferers may experience worry or fear.\nIt is important to keep in mind that not everyone who experiences a traumatic event develops PTSD. In actuality, the majority of people won’t get the condition. Whether someone will experience PTSD depends on a variety of factors. An individual is more prone to experience PTSD if certain Risk Factors (like no social support) exist. Resilient Factors (like venting out to friends) are the elements that can lower the risk of the condition.\nEven though trauma can trigger a horrifying and overwhelming reaction, it can also occasionally act as a catalyst for constructive transformation. In the ideal scenarios, it might even encourage development, toughness, and resilience. When you are capable of overcoming trauma and turning hardship to your advantage, post-traumatic growth takes place.\nPost-Traumatic Growth refers to the positive psychological shift that some people go through following a crisis or traumatic incident. Deep sorrow is not discounted by post-traumatic growth, which instead holds that adversity can unwittingly lead to changes in how one view themselves, other people, and the wider world. In actuality, post-traumatic stress disorder and post-traumatic growth can coexist.\nAccording to Dr. Marianne Trent, a clinical psychologist and the director of the company ‘Good Thinking Psychological Services’, “Post-Traumatic Growth (PTG) is when someone who has been affected by PTSD, and finds a way to take new meaning from their experiences to live their lives in a different way than before the trauma.”\nAccording to studies, after a traumatic episode, about half of trauma survivors experience post-traumatic growth. Now let’s have a look at the elements which lead to post-traumatic growth\nTHE ELEMENTS OF GROWTH\nOne must first educate themselves about post-traumatic growth in order to make the transition from trauma to growth. When our presumptions are refuted, it is perplexing and unsettling and frequently results in worried, thoughts: Why did this happen? Who is in charge? Now, what should I do? We are compelled to reconsider who we are, who we surround ourselves with, where we live, and what our future holds. It may cause extreme discomfort. But as evidenced by the study, it can also bring about change that will be for the better. Learning and comprehending that truth must be our first priority.\nI have a friend who lost her mother in the middle of the pandemic. For a while, she was in complete shock, but eventually, she realized that her new situation would force her to reconsider who she was: She often sat and revaluated by saying things like – “Now I have to figure out what is next in this life. ‘I know I have to do this, but a part of me doesn’t want to.” It was when she made the decision to seek therapy in order to recover from it, and that was the first step in her transformation into a person who had more sympathy for herself and could accept limitations.\nTo do any learning, one must be in the right frame of mind. That starts with managing negative emotions such as anxiety, guilt, and anger, which can be done by shifting the kind of thinking that leads to those feelings. Instead of concentrating on losses, failures, uncertainties, and worst-case scenarios, try to remember successes, take into account best-case scenarios, consider your own or your organization’s resources and preparation, and consider what you can reasonably accomplish both individually and collectively.\nTalking about a dining chain CEO, After his board removed him from the position, the creator of one restaurant company learned that emotional control was essential. The news came as a complete shock, and at first, he was enraged, he said. But he did so after being advised by his investor father to “get [his] head around being supportive.” He began considering how he might maintain his composure and professionalism while still helping the company move forward, instead of dwelling on his rage and the sense that he had been betrayed. He ultimately came back to run the business.\nBy paying attention to your emotions as they arise, you may control them directly. Exercise and meditative techniques like breathing exercises are also beneficial. Use these methods yourself and spread the word about them to assist others. Recognize that the current situation is still difficult and unpleasant, but maintain your calm. Additionally, promote more frequent conversation to help people feel less alone and better understand their shared emotional resilience.\nThis stage of the process is when you discuss what has happened and is happening, including any short- and long-term implications as well as personal and professional, individual and organizational, small- and broad-scale, immediate, and ongoing effects. By expressing these ideas, we are able to make sense of the trauma and transform negative thoughts into more useful observations.\nAsking numerous questions while listening to someone describe their experience of the crisis may come across as an unwanted inquiry motivated more by curiosity than by genuine concern. It is best to concentrate on the impact and the main issues being raised by your counterpart.\n4. Narrative development\nTo accept the chapters that have already been written and visualize creating the upcoming ones in a meaningful way, the next step is to create an authentic narrative about the trauma and our life after it. Your story—and the stories of the people you’re helping—can and should involve a trying past that ultimately leads to a happier present.\nTake the case of a nonprofit CEO who had been dismissed go from two prior jobs due to sexual harassment claims. He and his wife were involved in a terrible crash one night while driving on the expressway. They crashed into a stopped car that had no lights on. His wife suffered just minor wounds, but he spent a month unconscious and required physiotherapy to walk and speak once more. His new narrative went something like this: “Many would believe that it was this accident that endangered my life. However, I was already in terrible danger. I was hurting people, destroying my job, and moving towards a life without my wife and kids. I had to pause due to the accident, which gave me time for introspection and showed me what love is.\nNarrative development is very crucial in order to understand the true meaning of life and learn from the experiences of the past to make you a better human being for the future.\nPeople recover more quickly from trauma if they find work that helps others—either those who are close to them, members of their community, or those who have been through comparable experiences. My acquaintance who lost a child established a nonprofit organization to assist bereaved families in finding others who could relate to their suffering. As a result of the leadership of those who have experienced comparable losses and the desire to impart the strength they have gained, the organization is flourishing forty years later.\nKen Falke, who spent more than 20 years as a bomb-disposal specialist in the U.S. Navy, is yet another outstanding example of service. He was motivated to aid others in healing after witnessing the effects of war personally. Beginning with paying hospitalized combat veterans a visit, he and his wife Julia thought that this wasn’t enough. As a result, they established the company where I am currently employed, the Boulder Crest Institute, whose Retreat for Military and Veteran Wellness programs are based on the posttraumatic development model.\nOf course, you don’t have to establish a foundation or a nonprofit organization to be helpful. Growth can result from concentrating on how you can help alleviate the ongoing crisis, whether by providing content or sewing masks, stocking shelves or retraining teammates, supporting small businesses, or acquiescing to a temporary wage cut. Simply expressing thanks and exhibiting compassion and understanding towards others are also acceptable.\nPOST-TRAUMATIC GROWTH IN THERAPY\nTaking therapy is also highly recommended for Post Traumatic Growth after PTSD.\n- Eye Movement Desensitization and Reprocessing (EMDR)\n- Compassion-Focused Therapy (CFT)\n- Trauma-Focused Cognitive Behavioral Therapy (TF-CBT)\nAccess to efficient, scientifically supported trauma treatments “may be life-changing,” according to Trent. In terms of improved functioning and less traumatic symptoms, “the effects of post-treatment can be like night and day for people.”\nThe interactive psychotherapy technique known as eye movement desensitization and reprocessing (EMDR) is used to reduce psychological distress.\nThe technique is based on the idea that when painful and traumatic memories aren’t fully processed resulted in post-traumatic stress disorder. You then relive those unprocessed memories when certain sights, sounds, words, or odors trigger them. The mental distress and other symptoms associated with post-traumatic stress disorder are brought on by this reliving (PTSD).\nA tiny 2018 study emphasized the advantages of EMDR for PTSD-afflicted Syrian refugees for PTSD-afflicted refugees. Over 61% of the 18 participants who had EMDR as part of a group therapy intervention subsequently no longer met the requirements for a PTSD diagnosis. People who had EMDR also reported having fewer depressive symptoms.\nCompassion-focused therapy (CFT) encourages patients to have compassion for both themselves and others in order to aid in the promotion of mental and emotional healing. CMT (Compassionate Mind Training) refers to the common methods for assisting people in feeling compassion and fostering various aspects of compassion for oneself and others. Compassionate mind training is the main therapeutic method used in CFT (CMT). Through the use of specialized training and supervised exercises created to aid people in further developing non-judging and non-condemning qualities, CMT seeks to promote compassionate motivation, sympathy, sensitivity, and suffering tolerance.\nTrauma-Focused Cognitive Behavioral Therapy (TF-CBT)\nAn evidence-based treatment approach called trauma-focused cognitive behavioral therapy (TF-CBT) aims to help kids, teens, and their families get over the effects of a traumatic experience.\nTherapists trained in TF-CBT are frequently able to assist children experiencing the emotional long-term consequences of trauma address and resolving these effects. This evidence-based approach has been demonstrated to be helpful for treatment after numerous traumas or a single traumatic incident.\nIn TF-CBT, humanistic, cognitive behavioral, and familial techniques are combined with interventions designed specifically to suit the needs of children and adolescents facing emotional and psychological challenges as a result of a trauma. Since more than 80% of traumatized children make progress within this time, this treatment is usually only given for a maximum of 16 sessions.\nAny improvement in preventing trauma, PTSD, and co-occurring mental health conditions including substance addiction and depression is likely to be helpful because they can all have a significant negative impact on wellbeing. Post-traumatic growth is the only way to enhance one’s physical, psychological, and emotional well-being after trauma.\n- Personal strength\nPeople’s resilience in the face of trauma frequently astounds them. They are now better prepared to handle difficulties in the future. That also holds true for groups and businesses. After going through such challenges, groups frequently have a better understanding of their overall knowledge, abilities, resiliency, and growth potential.\n- New possibilities\nWe must adapt and innovate when new realities make it impossible to resume previous roles, routines, and approaches. To demonstrate to their followers that change should be welcomed rather than feared, leaders must have the bravery and zeal to test these new approaches.\n- Improved relationships\nThese frequently result from the urge to offer and receive assistance during difficult times. Trauma can encourage the development of new bonds and increase gratitude for existing ones. Sharing a problem together strengthens relationships.\n- Appreciation for life\nWe frequently get better at appreciating what we still have but may have previously missed when we are faced with dread and loss. Leaders can set an example for others by valuing the fundamentals of living and working. “We are a fantastic team. Our work is valued by our clients. For the benefit of everyone who still works there, we have maintained the company. Our company works toward a more noble goal”. Even a simple compliment like “that coffee tastes excellent this morning” counts.\nThis is the result of thought on the “great questions” that are frequently disregarded in daily life. As a result of the challenges to our fundamental beliefs that trauma presents, many people are compelled to study philosophy or theology on their own to create a life that is worthwhile. Are we conducting our business ethically? is an existential question that organizations may also face. Do we put our beliefs into practice? Is another question for you to think about for spiritual growth\nIt is extremely difficult to evolve in the midst of the trauma, but the reflection in its aftermath can provide a foundation for growth. Trauma survivors who want to cultivate growth can strive to process the experience once they distance themselves from it. Survivors can examine how their outlook has changed if they have a newfound appreciation for life, whether their relationships have become closer, or whether they have developed spirituality as a result of the event. Practitioners should be careful to meet clients where they are mentally and refrain from underestimating their pain or offering early answers. However, by assisting clients in reflecting on learnings from their experience, such as realizing they were stronger than they thought they were or having rearranged their priorities in life, the ideas of post-traumatic growth can be introduced gradually and subtly.\nEnroll in our Accredited PTSD & Trauma Therapy Online Course\n- Use this PTSD Treatment Plan in Your Counseling Practice | Udemy\n- PTSD & Trauma Counseling with Combination of Psychotherapy | Udemy\n- Beyond Trauma: Counseling for Post Traumatic Growth | Udemy\n- Overcoming Complex PTSD – CPTSD | Udemy\n- EMDR Practitioner Toolbox – Treatment of Complex PTSD | Udemy\nDownload Free worksheet of this article']	['<urn:uuid:dd1210b1-56f1-4516-9d77-16375d5214bb>', '<urn:uuid:16d6946a-0dd6-4e2b-a647-db38e99e198f>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T18:05:20.075489	25	151	3065
80	Which is more dangerous for health - being overweight or having sleep apnea?	Both conditions are dangerous for health, with significant overlapping risks. Being overweight increases risk for multiple diseases including type 2 diabetes, high blood pressure, heart disease, sleep apnea, fatty liver disease, kidney disease, and pregnancy problems. Sleep apnea, meanwhile, can lead to serious health problems such as high blood pressure, heart disease, stroke, and in severe cases death, while also causing daily issues like extreme fatigue, memory problems, and mood swings. The conditions are interrelated, as being overweight is actually a risk factor for developing sleep apnea. Both conditions require lifestyle changes and possibly medical intervention for proper management.	['Our weight. It’s a topic that’s become taboo to discuss but we have to pay attention. Body weight is an ever-increasing problem for Americans, with over 71% of Americans being classified as either overweight or obese (1). In a previous article regarding the seven keys to longevity, we introduced the seven keys to a long and healthy life, as found by the Alameda County Health study.\n- Sleeping seven to eight hours a night\n- No eating between meals\n- Eating breakfast regularly\n- Maintaining proper weight\n- Exercising regularly\n- Moderate or no drinking of alcohol\n- Not smoking\nLet’s take a look at how maintaining a proper weight can not only add years to your life but more importantly, add life to your years.\nBeing overweight increases risk for disease\nBeing overweight is arguably the most outstanding indicator for risk of disease. The National Institute of Diabetes and Digestive and Kidney Diseases says that having excess weight may be a contributing factor in diseases such as:\n- Type 2 diabetes – In fact, 90% of those that are diagnosed with diabetes are overweight or obese (3).\n- High blood pressure\n- Heart disease\n- Sleep apnea\n- Fatty liver disease\n- Kidney disease\n- Pregnancy problems, such as high blood sugar, high blood pressure, and increased risk of cesarean delivery.\nHow can you know if you weigh too much?\nGaining a few pounds as the years go by may not seem like a big deal, but losing control of your weight can put you in a dangerous place. BMI (Body Mass Index) is the standard way to assess weight and health. BMI is based on weight and height. This measure is not perfect especially for those that have very little fat and have above average muscle mass (because muscle is heavier than fat), but BMI is an appropriate measure for the majority of individuals. For adults, a BMI over 30 is considered obese. Between 25 and 29.9 is considered overweight, and a BMI between 18.5 and 24.9 is considered the “ideal weight.”\nHow to calculate your BMI\nThe easiest way to find your BMI is to find a BMI calculator online, but there is a manual way as well.\n- Take a calculator and multiply your weight in pounds by 703.\n- Then, divide that twice by your height in inches.\nFor example, if you weigh 150 pounds and you’re 65 inches tall (5 feet and 5 inches), that would be (200 x 703) ÷ 65 ÷ 65 = 24.9. This would give you a BMI of 24.9, which is on the border of normal and overweight.\nHow to reduce your BMI\nDiets fads are a dime a dozen. There always seems to be some new way to lose weight quickly and “easily”. In reality, the best way to maintain a proper weight is to change your lifestyle.\nIn a recent study, individuals were assessed based on their nutritional lifestyle. This was the largest study that compared obesity rates of those eating a plant-based diet. Those that were meat eaters topped the list with an average BMI of 28.8 – very close to being obese.\nFlexitarians (people that eat meat on a weekly basis, rather than daily) did better with an average BMI of 27.3 but were still overweight. Pesco-vegetarians (people who avoid meat except for fish) were better still with a BMI average of 26.3. So far none of the groups have an average BMI in the normal range.\nWhat about vegetarians? Even vegetarians had an average BMI of 25.7, which is close to being overweight but not quite in the normal range. The only group that was found to have an average BMI in the normal range was the vegan group (people eating exclusively plant-derived foods) with an average BMI of 23.6.4\nThe closer you get to a plant-based lifestyle the better your chances are of reducing your BMI and getting closer to a healthy and lower risk weight.\n1. Center for Disease Control and Prevention. Obesity and Overweight. https://www.cdc.gov/nchs/fastats/obesity-overweight.htm\n2. National Institute of Diabetes and Digestive and Kidney Diseases. Health Risk of Being Overweight. https://www.niddk.nih.gov/health-information/weight-management/health-risks-overweight\n3. Ginter E, Simko V. Type 2 diabetes mellitus, pandemic in 21st Century. Adv Exp Med Biol. 2012;771:42-50.\n4. Tonstad S, Butler T, Yan R, Fraser GE. Type of vegetarian diet, body weight, and prevalence of type 2 diabetes. Diabetes Care. 2009;32(5):791-6.', 'Breathe in. Breathe out.\nTake a deep breath in and, without releasing your deep breath, read to the end of the following paragraph.\nSleep apnea is a sleep disorder characterized with pauses in breathing, shallow breathing or infrequent breathing. These pauses can last several seconds or several minutes. More than extreme snoring, the paused breathing causes carbon dioxide to build up in the bloodstream. When the brain and body don’t get the proper amount of oxygen, many harmful and sometimes life-threatening side effects can occur.\nNow release your deep breath. How do you feel? Imagine being asleep and doing that over and over throughout the night without realizing it. This is a small example of what people suffering from sleep apnea go through – every night.\nTypes of sleep apnea\n- Obstructive sleep apnea (OSA) – While the muscles of your body naturally relax during sleep, the muscles in the back of your throat relax as well. Your airway, which is made up of collapsible walls of soft tissue, can cause a breathing obstruction during sleep.\n- Central sleep apnea (CSA) – This type of sleep disorder is caused by your brain’s respiratory control center to be imbalanced during sleep. As carbon dioxide levels increase in the blood stream, this mechanism in the brain ceases to respond quickly enough to maintain an even respiratory rate.\nSymptoms of sleep apnea\nMany people are unaware they suffer from sleep apnea. Diagnosing sleep apnea involves an evaluation of symptoms combined with a participation in an overnight sleep test. Some of the symptoms include:\n- Breathing problems during sleep\n- Extreme daytime fatigue\n- Fighting falling asleep throughout the day\n- Slowed reaction times\n- Vison problems\n- Morning headaches\n- Memory or learning problems\n- Not able to concentrate\n- Feelings of irritability, depression or mood swings\n- Dry mouth or sore throat upon waking\n- Serious health problems such as high blood pressure, heart disease, stroke, weight gain and in severe cases, even death.\nThere are many risk factors for sleep apnea that include but may not be limited to:\n- Over the age of 65\n- A smoker\n- Deviated septum\n- Enlarged tonsils or adenoids\n- Allergies or other medical conditions that lead to nasal congestion or blockage\nCSA specific risk factors\n- Severe illness such as heart disease, stroke, neurological disease, spinal or brain injury\nSleep apnea can be scary when considering the risk factors and symptoms, but there are ways to help alleviate sleep apnea with some lifestyle changes.\n- Exercise regularly\n- Quit smoking\n- Avoid alcohol, sleeping pills and sedatives before bed\n- Stay away from caffeine and heavy meals before bed\n- Maintain regular sleep hours\n- Sleep on your side\n- Prop your head up while sleeping\n- Open nasal passages with saline spray, breathing strips or a neti-pot\nIf your sleep apnea condition can’t be self-regulated there are medical treatments that may be prescribed by your doctor.\n- Continuous positive airway pressure (CPAP) device used while sleeping\n- Automatic positive airway pressure (APAP) device used while sleeping\nSleep apnea is a very serious health and sleep condition that can be regulated with the support of a medical professional. If you feel you are at risk for sleep apnea or believe you are presenting symptoms please consult your doctor.']	['<urn:uuid:64b543af-9847-46d5-ac39-6d6898614a66>', '<urn:uuid:02b57259-8797-4647-a79d-cf5bdd878d3b>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T18:05:20.075489	13	99	1287
81	what conditions does flowering cherry tree need to grow well soil sunlight care	Flowering cherry trees need an open and sunny site that isn't too exposed, as strong winds can blow away spring blossoms. They prefer well-drained soil enriched with organic matter that retains moisture but doesn't become waterlogged. They grow well in most soil types, particularly chalky and alkaline soils. Once established, they need minimal care - just water during long dry periods in summer and feed with general granular plant food in spring. Pruning should be minimal and only done from May to August when the tree is actively growing to prevent diseases.	['There’s no doubt that if you want a tree with the wow factor in spring, then an ornamental or flowering cherry (Prunus) is the perfect choice. They are literally smothered in blossom and many provide excellent autumn foliage colours.\nMany varieties are perfect for even small gardens and they come in a range of shapes that include columnar, spreading or weeping. And, of course, there are varieties of cherry that produce deliciously tasty cherry fruit.\nHow to grow flowering cherries\nFlowering cherries prefer an open and sunny site, which isn’t too exposed. Strong winds in spring will blow away much of the blossom, reducing the overall flowering display.\nThey prefer a good, well-drained soil enriched with lots of organic matter which holds plenty of moisture in spring and summer and doesn’t dry out or become waterlogged. They will grow in most soils types, especially chalky and alkaline soils.\nFlowering cherry varieties\nThere are numerous species and varieties, some being small or very slow growing making them suitable for all but the tiniest garden. Flower colours range from white through to deep pink. They may be single flowers or double with lots of petals which provide an even more spectacular display. Foliage colour can either be green or purple and many have beautiful autumn leaf colour too. One or too even have gorgeous mahogany or deep copper coloured bark.\nThe tree shape is an important consideration when choosing. Columnar or flagpole cherries are perfect for small spaces whereas spreading cherries are suitable for larger spaces and to provide a focal point or centrepiece. Weeping varieties don’t grow that tall and provide a waterfall of blossom.\nThe following are all excellent trees, suitable for small gardens.\n- Prunus Accolade has a spreading growth habit and is covered in masses of light pink, semi-double flowers in April. In autumn, the leaves turn a rich orange or red.\n- Prunus Amanogawa, the flagpole cherry, is thin and upright, bearing lots of semi-double, pale pink flowers. In autumn, the leaves turn orange and red.\n- Prunus Kiku-shidare-zakura, Cheal’s weeping cherry, is a small, compact weeping tree. The blousy, double flowers are rich pink. The leaves are a gorgeous bronze colour when young and turn a mellow orange in autumn.\n- Prunus Kojo-no-mai is a very small, slow growing and compact cherry – growing more like a shrub than a tree. It is perfect for growing in containers. Its branches have an attractive zig-zag pattern which look good even in winter. Profusely flowering even when young.\n- Prunus serrula Dorothy Clive is a small, round-headed tree, with coppery-brown bark, white flowers and yellow autumn foliage colours.\n- Prunus x subhirtella Autumnalis produces white, semi-double flowers intermittently from autumn to spring depending on the weather. Leaves turn yellow in autumn.\nPlanting flowering cherries\nPlant bare-root trees between November and March, and container-grown ones any time of year but preferably not winter. Dig a hole 60x60cm (2x2ft) and 30cm (12in) deep. Add a layer of organic matter such as compost or well-rotted manure to the base of the hole and dig in.\nPlace the roots in the planting hole and adjust the planting depth so that the tree is planted at the same depth as it was originally growing and the top of the roots are level with the soil surface.\nMix in more organic matter with the excavated soil and fill in the planting hole. Stake the tree with a rigid tree stake and two tree ties so that it is fully supported against the prevailing winds. Water in well and apply a granular general feed over the soil around the tree. Add a 5-7.5cm (2-3in) deep mulch of well-rotted garden compost or bark chippings around the root area.\nIf planting in the lawn, create a turf-free circular bed around the tree with a minimum diameter of 60cm (2ft).\nSuggested planting locations and garden types\nFlower borders and beds, focal point, city and courtyard gardens.\nHow to care for flowering cherries\nOnce planted and properly established, flowering cherries need very little aftercare. They may need watering during prolonged dry periods in summer. Feed with a general granular plant food in spring.\nUnless essential, it is best not to carry out any pruning. If pruning is needed, it should be carried out when the tree is actively growing – from May to August – to prevent problems with bacterial canker and silver leaf diseases.\n|Flowering season(s)||Spring, Autumn, Winter|\n|Foliage season(s)||Spring, Summer, Autumn|\n|Soil type||Chalky, Clay, Loamy, Sandy|\n|Soil moisture||Moist but well-drained|\n|Ultimate height||Up to 10m (33ft)|\n|Ultimate spread||Up to 5m (16ft)|\n|Time to ultimate height||10-15 years|']	['<urn:uuid:ffcd3252-a5b7-4c48-b945-90db4acc03e0>']	open-ended	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T18:05:20.075489	13	92	768
82	How does the significance of communal eating and drinking practices in Morocco manifest both in tea ceremonies and formal dining customs, and what specific etiquette rules govern these social interactions?	In Morocco, both tea ceremonies and dining customs emphasize communal sharing and strict etiquette rules. During tea ceremonies, the preparation and serving process is a shared experience, with the tea being poured from height to enhance aroma throughout the room, and all guests participating in the three-cup ritual. Similarly, during meals, food is served in communal bowls, and guests must follow specific protocols such as taking food only from the portion closest to them. The host plays a central role in both customs - in tea service, the eldest male prepares and serves the tea, while during meals, the host blesses the food and signals when to begin eating. Cleanliness and respect are paramount in both situations: guests must use only their right hand for eating, and specific hand-washing rituals are observed before meals. These practices demonstrate the importance of hospitality in Moroccan culture, where hosts insist on serving their guests generously, whether it's multiple cups of tea or additional portions of food.	['All over the world, tea is a celebrated drink that forms part of daily life and social rituals. And although people across the world drink their tea differently, the simple pleasure of drinking tea unites us all. Whether it is white, green, black or herbal, tea is believed to relax the mind, offer consolation, improve mental clarity and cure all kinds of afflictions. The tea drinking ritual itself strengthens friendships, connects family members and shows hospitality to strangers. Let’s take a closer look at some of the tea rituals and customs on the African continent, where type of tea and style of serving varies from region to region.\nWelcoming guests in Morocco is not done properly without a glass of hot Maghrebi mint tea, also known as Touareg. Traditionally, the tea is prepared by the eldest male in the household from green tea leaves, lots of dried or fresh mint and heaps of sugar. It is poured into tall glasses from a standing position in an elegant gesture, to allow the tea to air and release its aroma throughout the room. Each guest is served three cups and each time the flavor varies, as they say: “The first glass is as gentle as life, the second is as strong as love, the third is as bitter as death.” Refusing any one of these servings is considered bad manners.\nRooibos tea (or red bush tea) is a popular herbal tea native to South Africa. The plant is found in the Western Cape region. Dutch settlers are said to have turned to the drink as an alternative to expensive imported black tea from Europe. Rooibos makes a bright red tea, typically served on its own without sugar or milk. The tea has a naturally mild and sweet flavor and can be consumed all day but especially before bed time as it has no caffeine. Many health benefits are attributed to rooibos, and the tea is consumed to combat headaches, insomnia and allergies.\nEgyptians love their tea, or shaias they call it. Traditionally, they drink tea with guests and after lunch and in the afternoon, the tea is accompanied with sweets, such as baklava, basbousa, or konafa. Koshary tea is black tea steeped in boiled water and flavored with sugar, milk and mint leaves. Saiidi tea is a long boiled and bitter black tea sweetened with copious amounts of sugar. Herbal teas and tisanes are also popular and are often consumed for health benefits. The bright red sweet-sour Karkadeh is made of dried hibiscus flowers and can be drunk hot or cold. It is considered good for the heart and often served at Egyptian weddings.\nTea culture in Senegal is defined by a ritual called attaya which is an important part of daily social life. Because the preparation of the tea takes up a long time – between one and three hours – there is plenty of time for conversation and the ceremony connects friends, family and guests.\nThe attaya ceremony consists of three stages. Green tea is boiled strong and bitter in a kettle over a charcoal stove. It is then poured from standing height into tall glasses called kas. The tea is then poured slowly from one glass to another until a thick foam is formed on top. This is a strong and bitter tea. In the next stages fresh mint is added and more and more sugar as you go so you end up with a thick and very sweet cup. The custom is to slurp when drinking, to avoid burning the tongue.\nIn Kenya, tea is served practically throughout the day, for breakfast, at mealtimes and also during Kenya’s teatime. It is also served showing hospitality to guests. Kenyan teatime embraces different cultural influences, from the British tradition of afternoon tea at 4pm, with finger food to the Indian way of preparing tea with masala spices. Most Kenyans drink black tea milk and sugar, which they call chai though there are some who prefer it just black. To make chai, bring the milk to a boil on the stove, add the black tea leaves and, if you like, the masala spices. Stir the mixture until frothy and pour through a strainer into a cup. Then add sugar. Kenyans like to drink their tea very hot!\nThis article appeared in Inzozi Magazine, Summer 2018 issue under the title “African Tea Rituals”', 'As you may already have assumed from previous posts, Moroccan people are extremely hospitable and always ready to lend a hand. It is not uncommon for a new friend to invite you into their home and proudly share a meal with you. While Moroccans are very accepting and eager to welcome foreigners it is very important for that visitor, in this case you, to be familiar with Moroccan social and table etiquette and to respect the customs and traditions that encompass the family dinner table. Social etiquette and table manners are taken quite seriously in Morocco and people are judged on their behavior in public.\nTraditional Moroccan table etiquette has its roots in Islam and these traditions and customs are still adhered to today, even among the youth. If you receive an invitation to a Moroccan home there are a few key behaviors and traditions that should be followed to ensure the proper respect and gratitude is shown to your host.\nWhen invited for dinner at a Moroccan household it is seen as a sign of respect and gratitude to your host to present a small gift of nuts, dates, or flowers with you. Dressing well and taking off your shoes at the door is also a sign of respect and should be followed.\nOnce invited inside, the host will show you to the dinner table, most likely a knee-high table surrounded by pillows or the traditional Moroccan sofas that line the walls of the room. As the honored guest, you will be sat directly next to your host.\nLooking at the table you will notice that there is no silverware, don’t panic! Silverware is not used at Moroccan dinner tables because it is the same material that is used in currencies and is a non-Islamic practice. Instead, Moroccans eat with their right (not left) hands using only their thumb and first two fingers. They also use the famous Moroccan bread as a means to scoop up food and sop up any sauce. As hands are used and cleanliness is very important to Moroccans, a bowl is presented to each guest to wash his/her hands. The host, or member of the household, will pour water over your hands for you; don’t pour the water your self!\nAfter all hands have been washed the food will be presented. Saffron and orange scented couscous, a bubbling tajine full of succulent lamb and roasted vegetables, a large loaf of fresh crusty Moroccan bread – don’t let your stomach get the best of you! It is extremely important to wait until the host has blessed the food and started eating before you dig in!\nAll of the food is presented in communal bowls and each member of the table takes a portion and places it on their plate. Make sure you take food from the part of the bowl nearest to you; don’t reach all the way over the food for that one really yummy looking piece of lamb. As the guest of the meal all of the best cuts of meat will be presented to you anyway, so you won’t have to reach far to get a good piece.\nIt is important to accept and try everything that is offered to you by your host. Even if you just take a nibble. Insisting food upon a guest is a sign of hospitality so don’t feel overwhelmed if they keep telling you to eat more. If you feel you have eaten your weight in food and simply can’t eat another bite take a very small amount from the bowl and take very small bites chewing slowly. It will tell the host that you appreciate their hospitality and respect their food.\nGuests may also be offered souak, or swak, to wash and clean their teeth after the meal. Souak is black walnut dried bark that is whittled into a stick and due to its whitening and antiseptic qualities is used as a natural toothpaste after Moroccan meals.\nAfter all is said and done you can leave your host’s house knowing that you showed the upmost respect and gratitude for the wonderful company and delicious meal.']	['<urn:uuid:e265aa55-f723-413f-a123-2b658008e1f8>', '<urn:uuid:a4379f45-061b-40a8-8258-929e0f66e7c7>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T18:05:20.075489	30	163	1421
83	Which gives softer wool between medieval sheep and Southdowns?	Southdown wool tends to be softer, as it is one of the finest wools among British breeds with a consistent micron count of 23-29, while medieval sheep wool was more variable, ranging from very fine to very coarse (10-128 microns) with some containing up to 65% coarse medullated fibers.	"['...approximating those from the Middle Ages\nThe breeds that were around in the Middle Ages no longer exist. There weren\'t really ""breeds"" in the modern sense until the very late 18th century, and virtually all the known modern breeds were developed in the last 200 years. Even a breed labeled as ""primitive"" or ""ancient"" has not remained in a vacuum; modern sheep farmers may select for different traits than their ancestors did, which has an impact on the flock\'s characteristics.\nI have compiled a list of some modern breeds that bear a strong enough resemblance to sheep of the Middle Ages that their fleece would be preferable to use in reproduction projects. For the moment I am focusing on English sheep; since England was a strong exporter of wool in this period, their wool ended up across Europe and so is appropriate for a much wider area than just England. (The Spanish merino is a whole different story, which I shall tackle at a later date...)\nMiddle Ages: The Sheep\nThere was a great variability among medieval livestock. So, while general observations can be made, keep in mind that the range was much greater.\nIn general, medieval English sheep are pictured as small white sheep with long tails and no horns. Occasionally rams are shown with horns, while ewes almost never are. However, sheep skulls in the archaeological record prove that while rams were indeed horned, ewes could be either horned or polled (no horns). Skeletal remains also show that sheep were much smaller than modern breeds, and were closer to the size of the modern Soay (ewes average 53 lbs., rams 84 lbs.) or Shetland (ewes 75-100 lbs., rams 90-125 lbs.) By contrast, some modern breeds can reach 300 lbs. or more.\nMiddle Ages: The Wool\nColor: While colored sheep did exist in the Middle Ages, as evidenced by pictorial depictions and surviving textiles, white wool was far more common.\nStaple length: Given that most surviving wool has already been made into something, staple length is difficult to determine. Four unspun wool samples from Perth show a range of 1""-2.4"" in length, but of course that is only four samples. One reason for the popularity of English wool was that it was reportedly longer than its Continental cousins, and therefore made better worsteds, where the fibers are combed instead of carded. I think it is safe to assume, however, that staple lengths like the modern longwools (developed in the late 18th century) of 10"" or more were unlikely to have been around.\nFiber diameter: This is much easier to determine from remaining wool samples and textiles. One study of English medieval wool from two sites, Perth and Aberdeen, shows a range of fiber fineness and percentage of hairy fibers. M. L. Ryder\'s hypothesis is that the two main wool classifications of the Middle Ages, coarse and fine, correspond to what he calls ""hairy medium"" and ""general medium."" The summarized measurements of these two groups are below. Range is the modal range, in which the outliers have been omitted. For the full chart, see M. L. Ryder\'s ""Medieval Sheep and Wool Types"" page 27.\n- Hairy Medium (medieval ""coarse""): 20-40 microns, 17% medullated\n- General Medium (medieval ""fine""): 20-31 microns, 3-4% medullated\nWhile the majority of the 105 samples fell into these two categories, there was quite a range overall of both diameter and medullated fibers: 10-128 microns and 0-65% medullated. This encompasses everything from the modern Merino (with ranges from 11-26 microns) to the sturdiest carpet wools-- a testament to the wide variety of sheep in the era before recognized breeds.\nHere is a list of modern breeds with enough characteristics to medieval sheep and wool that their wool would be preferable to use for reproduction projects. None of these sheep is a perfect match, remember; hundreds of years have come and gone, with changes in everything from livestock breeds to the English language. I have indicated what makes these sheep ""close enough"" to medieval sheep, and what does not quite match up. Breeds are listed in alphabetical order.\nThe Sheep: Cheviots are white-faced sheep. Both sexes are polled (no horns). Ewes average 120-160 lbs., rams 160-200 lbs., larger than medieval sheep but not quite so hefty as the more modern meat breeds. They are also relatively independent and need less husbandry than some breeds.\nThe Wool: Staple length is about 3-5"" and 27-33 microns. The wool\'s helical crimp gives it both durability and resilience, important qualities to consider when making an item that will receive some wear. Cheviot wool probably fits in between the medieval ""coarse"" and ""fine"" classifications, and would depend on the individual sheep.\nThe Sheep: If any sheep has a possibly legitimate claim to being virtually unchanged since the Middle Ages, it\'s the Icelandic. They were brought to Iceland by the Vikings about a thousand years ago where they remained isolated. It is now illegal to bring any other sheep to Iceland. As a result, this breed is genetically the same as it was in the Viking era. Icelandic sheep were first brought to North America in 1985, then again in 1990, and all Icelandic sheep in North America are descended from these two groups. Keep in mind that any Icelandic sheep outside of Iceland might have been crossed with something else at some point; check the sheep\'s records to be sure. Why am I including a breed from Iceland in a discussion of English sheep? Because of their isolation, Icelandics show many characteristics of medieval English sheep, and as the Vikings visited England in addition to other places, it is likely that this type of sheep was found in a much wider range at the time. All Icelandics are mostly horned, though some of either sex can be polled. Despite their ancient ancestry, Icelandics are still larger than skeletal remains of medieval English sheep: ewes are 130-160 lbs. and rams 180-220 lbs. They also have a naturally short tail while most medieval depictions show long-tailed sheep. These sheep are quite hardy, have strong immune systems and are relatively independent.\nThe Wool: Icelandic wool is dual-coated, a more primitive feature that there is some evidence of in medieval textiles. The coarser outercoat has a staple length of 4-18"" and 27-31 microns; the softer, wooly undercoat is 2-4"" long and 19-22 microns in diameter, definitely a good medieval ""fine"". Both coats can be spun together or separately. Icelandics exhibit a dizzying variety of colors and patterning, including all-white (a good medieval choice), black, brown, gray, and various patterns including spotted.\nThe Sheep: Ryeland sheep seem to be the descendants of the famous ""Lemster"" (Leominster) wool sheep. They are relatively small for modern sheep, with ewes averaging 141 lbs. and rams 192 lbs., though this still makes them much larger than medieval sheep. Both rams and ewes are polled. Varieties in the British Isles and Australia have retained the older characteristics than those in New Zealand.\nThe Wool: The staple length is 2-5"" and the micron count (for the British Isles varieties) is 25-28. This puts the wool within the ranges of medieval samples, for both length and diameter, and would be a good medieval ""fine"" wool. The shorter staples would be ideal for carding, while the longer ones could be combed. Both white and colored varieties exist.\nThe Sheep: As you might expect, this sheep has a black face. Both sexes are horned, with ewes averaging 140 lbs. and rams 180 lbs.\nThe Wool: The fleece is always white. Staples are long, 6-14"", so you would want to select something on the shorter end to approximate medieval wool. While the diameter ranges from 28-40 microns with some kemp, the fiber is generally described as both coarse and strong. This would be a good choice to match the medieval ""coarse"" classification.\nThe Sheep: This is another breed that retains many primitive characteristics. It is small, with ewes usually 75-100 lbs. and rams 90-125 lbs. It is similar in size to the skeletal remains of medieval sheep. Both sexes can be either horned or polled, although ewes are normally polled.\nThe Wool: Shetlands are known for variety! They can be either single or double coated, the latter of which is a more primitive feature; there is some evidence of double-coated fleeces being used for textiles in the Middle Ages. There is a great variety in fleece color and patterning, and while there were some colored sheep in the medieval period, solid white was more common. Shetland staples are usually 2-4"", a good medieval length, though some can have staples up to 10"" (which should probably be avoided for a medieval representation). Fiber diameter ranges from 20-30 microns. Fleeces without much kemp would probably be considered a medieval ""fine"" wool, while the more kempy varieties would be ""coarse.""\nThe Sheep: This is the size of medieval sheep: tiny, compared to modern breeds. Ewes average 53 lbs. and rams 84 lbs. They display the usual medieval horn patterning too: rams are horned, and ewes can be horned or polled. The Soay is considered a feral breed and displays many characteristics of ancient sheep: it is quite variable in appearance, it naturally sheds its wool, and has a double coat. It does show signs of alteration through breeding programs though, so it is not a completely wild variety.\nThe Wool: The staple length of 1.5""-4"" falls perfectly within the medieval range, as does the typical fiber diameter of 29-36 microns, with a fair amount of kemp, making it a medieval ""coarse"". However, this sheep always has some type of coloring pattern; in the Middle Ages, while there were colored sheep, most were white.\nThe Sheep: Let me be clear: the Southdown breed was developed in Sussex in the late 1700s. It is not a medieval sheep. However, it is older than many modern breeds and has characteristics that are similar to its medieval ancestors. For instance, it is still on the smaller side, with ewes weighing in at 130-180 lbs. and rams at 190-230 lbs. Both sexes are naturally polled.\nThe Wool: A staple length of 2-4"" and micron count of 23-29 with very little kemp makes it a good choice for approximating a medieval ""fine"" wool. Nearly all Southdown fleeces are white with only a few colored individuals, which exhibits a typical medieval mix.\nThe Sheep: Welsh Mountains have naturally long tails, like many sheep do (including those in the Middle Ages). Ewes are polled and rams are usually horned. Adults average 100-130 lbs., making them a smaller sheep and closer to the medieval size than most.\nThe Wool: Welsh Mountains can be all white, white with black markings on the face (""badgerface""), or all-black. For the medieval effect, all-white is preferable, though the occasional black sheep would not be out of place. (The all-black breed known as Black Welsh Mountains was developed in the mid-1800s by from selecting and breeding those few black individuals occasionally found in a white flock.) The staple length is 2-4"" but can be longer, and the diameter 30-40 microns with coarser kemp. This makes it an ideal medieval ""coarse"" wool.\nCrowfoot, Elizabeth, Frances Pritchard, Kay Staniland. Textiles and Clothing 1150-1450. Boydell Press: 2012.\nIcelandic Sheep Breeders of North America http://www.isbona.com/\nThe Livestock Conservancy http://www.livestockconservancy.org/\nNorth American Mule Sheep Society http://www.mulesheep.com/\nOklahoma State University Department of Animal Science http://www.ansi.okstate.edu/breeds/sheep/\nRobson, Deborah and Carol Ekarius. The Fleece & Fiber Sourcebook. Storey Publishing: 2011.\nRyder, M. L. ""Medieval Sheep and Wool Types"" The Agricultural History Review, Vol. 32 No. 1, 1984, pp. 14-28.\nShetland Sheep Society http://www.shetland-sheep.org.uk/\nSt. Kilda Soay Sheep Project http://soaysheep.biology.ed.ac.uk/', ""They are a great breed that are docile and easy to manage. Next page -> How the Wool is Cleaned, Picked, and Carded. Encyclopaedia Britannica's editors oversee subject areas in which they have extensive knowledge, whether from years of experience gained by working on that content or via study for an advanced degree.... Be on the lookout for your Britannica newsletter to get trusted stories delivered right to your inbox. You can see from her stubby legs, Southdowns are small sheep … They have wool on their faces, ears and legs. If you would like to spin, knit, crochet, or weave with it, we have wool and yarn for sale in Our Shops. Rams and ewes are usually polled, that means they have no horns. “They’re not a wool sheep they’re a fat lamb sheep.” He will sell his 200 hinds over the next over two years. They are laid flat to dry. Since it has more barbs per inch than other wool types, it is also ideal to blend with angora or other slick fibers since it clings so well. Black Friday Sale! His work was continued by Jonas Webb of Babraham in Cambridgeshire, who developed the larger animal of today. Babydoll Southdown wool is one of the finest wools of all the British breeds. We have drawings of these Southdown ancestors and they most certainly didn’t look like the distinctive ‘teddy-bear’ faced Southdowns we recognise today. You will find it has good durability. The tear ducts should be free of wool just below the eyes. Southdown sheep are medium to large sized beautiful animals with distinctive appearance. Babydoll Southdown wool is one of the finest wools of all the British breeds. The wool is fine to medium, with a staple length of 4-6 cm (or 1.5-2.5 inches), average fleece weighs 3-5 pounds, and is coarse with a micron count of 58s-60s. See the Table of Selected Breeds of Sheep for further information. They are a great breed that are docile and easy to manage. The oldest of all British breeds of sheep, it has an ideal body conformation for meat production. An estimated 110,000 sheep were in Sussex as early as 1341. Babydoll wool is great for socks, mittens, hats, blankets, and sweaters. They are broken down into two types the Original Southdown and the Babydoll Southdown that are a bit smaller than the original breed. Let us know if you have suggestions to improve this article (requires login). https://www.britannica.com/animal/Southdown, Black-Faced Highland, also called Scottish Blackface, originally Scotland, now also U.S., Italy, Argentina, developed in N.Z., now also in U.S., Australia, developed in England, now in U.K., U.S., Australia, small wool yield; out-of-season lambs; horned and hornless varieties. The breed was standardized around 1780 by John Ellman who realized the breed potential and set out to increase the quality of the breed. Southdown duvets — Our hypoallergenic and moth proofed wool duvets will transform your sleep in countless ways. Thanks for looking! They are usually of white color with mouse colored faces. The micron count typically ranges from 23 to 29 which means many people can wear it comfortably next to the skin. Not to be confused with the Olde English 'Babydoll' Southdown sheep. Any wool covering in front of the eyes should be short so that at no time can the sheep be woolblind. By 1837 William Youatt regarded this boundary to have ceased to exist, showing that the distribution of the types of sheep across Sussex had changed again. Around 1341 there was and estimate of nearly 110000 Southdown sheep in Sussex and their wool quality came in second to that only of the Hereford sheep breed. developed in England, now also widespread in U.S. originally Central Asia, now also Africa, Europe, U.S. coats of very young lambs called Persian lamb, originally England, now U.K., North America, originally England, now also Australia, N.Z., North and South America, coarse, long wool is used chiefly for carpets, originally Spain, now also Australia, North America, South Africa. Sheep, ! Southdown are the breed in which all others in the “Down Family” can cite in their back round. It is short (about 2 to 3 inches) and springy, soft and bouncy, with a surprisingly strong underlying disposition. Southdown is a soft fiber with a micron count of 23-29 and as low as 19 on some Babydoll flocks. Many considered Southdown to be the English fleece closet to Merino in fineness. Its ability to wet felt is very low, although it is fantastic for needle-felting! Corrections? Fiber diameter: 23.5 to 29 microns Wool Type: Fine, dense, medium-wool fleece Length: Good staple length of 1.5 to 2.5 inches. However is generally known as a medium grade fleece with a blocky rectangular staple. They are usually of white color with mouse colored faces. This scarf was knitted using bulky Babydoll 2-ply yarn. But here…the socks I’ve made from 100% Babydoll wool get put in a lingerie bag, then thrown in the washer with the other laundry. 840. After washing, the scarf measured 64″ long and was 6″ wide. Southdown Sheep. Fleeces are considered medium wool type with a fibre diameter of 23.5 to 29.0 microns. Both breeds fleece … I wash with warm water and rinse with cool. The Southdown is still noted for fine wool, although the fleece of course belongs to the British shortwool type and is not as fine as that of the Merino. Premium Membership is now 50% off! Southdowns are popular in many parts of the world, especially in The Commonwealth and the United States. New Zealand or French Southdown wool does not exhibit the same qualities we are looking for. Although you may feel that all sheep are roughly equal, the wool they produce is not. There are technically 3 different types of Southdowns: American, Babydoll, and Miniature. Their wool was integral to what may have been the first ever recorded sheep-to-sweater type of exhibition. So rather than shrinking, it redistributed. If my calculations are correct, it widened exactly the same percentage as it shortened. It should be emphasized that the modern Southdown should have a moderate amount of wool about the face and eyes. Consequently, not all wool bedding is the same and it’s easy to make a costly mistake. Often the fiber is sold in wool pools, but it can be an interesting hand spinning experience. We sell our wool and other woolly items in Our Shops. Locks are disorganized and “downy” (it is, after all, the defining sheep breed of “down” type wools). Southdown Sheep Characteristics. AND WOOL TYPES I7 which was used in the development of all other Down breeds from the late eighteenth century onwards. AND WOOL TYPES I7 which was used in the development of all other Down breeds from the late eighteenth century onwards. “Most are sold privately through an agent but there is an annual deer sale in Taihape.” He said he will be sad to see the deer go. So it actually shortened in length, but became slightly wider! The stitches are still defined, just fluffier. Sheep, ! It is short (about 2 to 3 inches) and springy, soft and bouncy, with a surprisingly strong underlying disposition. From this, we know that there are only about 3,000 pedigreed Southdown ewes currently living in the UK, one of which we proudly present here. Its fleece is close and is the finest of the British breeds; but, though white and of good quality, the wool is short and the fleeces relatively light in weight. To give you some idea of how Babydoll wool stands up to machine washing and even machine drying, this was an experiment I did when first knitting with it. Our editors will review what you’ve submitted and determine whether to revise the article.\nHow To Type I On Ti-84, Ground Beef Side Dishes, Baby Led Weaning Food Size By Age, How To Find A Good Ent Doctor, Tartar Sauce Pioneer Woman, Dryer Heating Element Kenmore, Architectural Styles Timeline,""]"	['<urn:uuid:ce79fbe0-d996-4a4e-a7c4-7250ce1e9df4>', '<urn:uuid:9e7e80f4-2653-4647-b1aa-d776b34ea5fe>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T18:05:20.075489	9	49	3268
84	As someone focused on minimalist hiking gear, I'm curious how the base pack weight (BPW) calculation compares to the total pack weight when accounting for water and consumables?	The base pack weight (BPW) only includes non-consumable items and should be under 5 lbs for SUL backpacking, while the total pack weight includes all consumables like water (which weighs about 2 lbs per liter), food, and fuel. As an example shown in the documents, a SUL setup had a 3.99 lb BPW but an 11.57 lb total pack weight when including 7.58 lbs of consumables.	['“SUL” stands for Super UltraLight weight backpacking.\n“SUL” is defined as a “BPW” (Base Pack Weight) less than 5 lbs.\n“BPW” is defined as everything in a backpack that is not consumed.\n“SUL” means everything in your pack that isn’t eaten or used weighs less than 5 lbs.\nUp to this point I have been everything from a “Heavy Hauler” down to an UltraLight weight backpacker. However, over the last few months I have been shaving my gear list down closer to the 5 pound (or “SUL”) mark. This is due to the countless hours of “internet research” on a lot of gear, detailed combing over my current gear lists and of course using my different gear set-ups while on the trail. However, I also owe much thanks to John Abela for an enormous amount of discussion and insight, to my buddy Gizmo Joe for actually hiking with me and sharing the same backpacking thoughts and style (and actually being somewhat of an “accountability” partner) as well as to the many folks over on all the different backpacking forums and YouTube channels that I frequent. Thanks to this combined effort, I am now about to embark on my first, true “SUL” backpacking trip…\nWhen it comes to an “SUL” base pack weight, I can remember looking at other people’s gear list and thinking “How in the world?!” But then with enough “dedication” I can finally say that I am one of those people. And at this point, my take on the whole “SUL” mindset is that it is not as bad as I thought it would be…at least not yet… (although, based on my use/knowledge with the gear in my list, I do not feel like this will change.)\nMuch of my progress to arrive at an “SUL” base pack weight was in simply leaving items out/behind. Don’t get me wrong, I didn’t cut out my entire FAK (First-Aid Kit) in order to get my pack weight lower, but I did decide to only carry as many band-aids as I may need rather than the whole box! And I maintained this mind-set when it came to all of the items that go inside my pack. I repackaged some items such as my Dr. B’s, bleach and Germ-X. I counted out the number of each pill that I would take and only packed that many. And I found that I didn’t need to carry the instructions on how to use my repair kit for my air pad… I just read the instructions and familiarized myself with how to use it… Let’s not forget, the mind is the most important piece of gear that you will bring along on any hike…\nAnother method I used to get my pack weight in the “SUL” range is by using multiuse items. This means using items that serve more than one purpose. The biggest example of this is my GoLite Poncho Tarp, although this is not the only example. Even something as simple as a 1 gallon Ziploc bag has many uses (water”proof” stuff sack, back-up pillow, laundry bag and sink… to name a few different uses), yet all of these uses weigh in as one, very light weight item!\nAnd of course, I also replaced some of my heavier gear with lighter weight gear that served the same purpose (if not more than one purpose). I will admit, there is not much getting around this one. Some items, such as a backpack will need to be lightweight (at least to a point). Many of the conventional, mainstream backpacks can weigh as much as my entire base pack weight (or more). So, finding the right items will involve seeking out these specialty items that fill the lightweight niche. (Or if you are crafty enough, you can make your own.) However, this method usually does involve spending some more money, whereas the first 2 methods did not, so much… Although, my disposable spoon was free from work, and my “cup” was less than $2 for 3 at a local retail store, so even though it may cost some extra money, that doesn’t mean that it has to be expensive…\nHowever, I feel like a huge component of an “SUL” gear list is the fact that when putting everything together, it should be thought of as a system, rather than just a piece of gear, which is pretty much a conglomeration of all the “methods” I mentioned above. Just as a puzzle, everything inside the backpack (and even including the backpack) should have its own place, but also it should all fit together to form the “final picture.” I feel like by making my entire system work together, I have managed a truly great kit…\nAnyway, moving on…\nI feel like I should add, that as a gear obsessed backpacker, this should come as no surprise that I have (finally) gotten my weight down to this point. However, I will admit that it is more than just my gear obsession that has given me a reason and want to go so lightweight…\nA buddy of mine (Gizmo Joe) and I are about to embark on an epic journey (for us at least). We are planning to hike the 74+ miles of the Appalachian Trail (AT) that runs through the Great Smoky Mountains National Park…in less than 3 days! Our first day will be 23 miles…a big start to a hike for sure…but it only gets bigger from here… Our second day will be a whopping 28.4 miles, and this does not count the extra distance that we have to hike off the AT to get to our shelter for the night (nor any other side trails we may have to take for any other reasons). Then the third day eases off just a bit, back down to another 23 mile day…\nSo, considering these big mileage days (for us anyway) we were looking to get our pack weight down as light as we possibly could, of course while still maintaining all the items we need to be safe while on the trail, as well as comfortable once we make it to camp! It only makes sense to us that, the lighter we can get our packs, the easier it will be to make it up and over all of those mountains in such a short period of time…\nOur plans are simple (they can be because our packs are also simple). We plan to be up and hiking by 5 am each morning. This will allow us some time to be a few miles down the trail before the sun even comes up, however, it will also allow us some time to get some “night” hiking done (which I am looking forward to). However, it also just so happens that this time of the year has the longest days… With the sun coming up early and setting late, this will allow us plenty of daylight hours, which should allow us plenty of time to make it from point “A” to point “B” (or at least relatively close).\nOf course too, this time of the year will be the hottest time of the year, which means we can get away with even less gear than we would normally need should the weather be a bit colder. So, this presents the perfect opportunity to go with an “SUL” weight backpack…\nSo, when working on my gear list, I have split up my “gear” into 4 different categories:\n1. Worn Items:\nThese are all of the items that I of course wear. This consists of my shoes, socks, underwear, pants, shirt and visor. As well, I also count in my watch, survival bracelet and even my knee straps that I wear with hopes of keeping my ITBS in check…\n2. Carried Items:\nThis consists of all the items that I carry, whether it be in my hands or in my pants pockets. This consists of my trekking poles, wallet, towel, map, phone and a few of my “10 Essential” items (knife, light, fire steel, whistle and compass).\nThis is of course everything that I plan to consume while on the trip. This category is actually broken down into 4 smaller categories:\n- Food: This represents all of the food that I will carry with me on a given trip. For this trip I am planning a “cold” breakfast, 6 snacks which I plan to consume about every 2 hours throughout the day and a warm dinner, complete with a cup of hot cocoa and a desert. (I will be making a more detailed video/write-up on this in a few days.)\n- Water: This represents the approximate amount of water that I plan to carry at a time. For this trip, I have decided to go with two 20 oz Gatorade bottles; one on each shoulder strap. I typically carry a single 32 oz Gatorade bottle, but due to this “SUL” weight, a single 32 oz bottle on one shoulder strap is very noticeable, so I have decided to balance it out with two 20 oz bottles on each side. Other than this, I will be carrying a 2L Sawyer soft bladder which I do not plan to carry filled, but do have the option to do so if need be.\n- Fuel: This represents the amount of fuel for my chosen stove system. For this trip I have decided to go with Esbit, or a solid fuel. I feel like this fuel is easy and safe to use, as well as lightweight and hot. I am carrying 4 Esbit tablets on this trip, of which I am allocating 1 tab per dinner. With the cooking system I am carrying on this trip, I can bring 2.5 cups of water to a boil with a single Esbit tablet. (And possibly more.) This will allow me at least 1 cup of water for my cocoa, and 1 – 1.5 cups of water to rehydrate my meals.\n- Miscellaneous Consumables: This represents any other item in my pack that gets consumed, but that is not included in one of the other sub-categories (food, water & fuel). This is a new one on me, and to be honest, I wasn’t sure that I wanted to add it in, and this also seems to be a bit different on everyone’s gear list, so it was harder to pin this one down. Anyway, I decided to tackle it on this list, and have included my TP, Dipenhydramine tablets, Pepto-Bismol tablets, Ibuprofen tablets, and Protonix tablets in this category.\n4. (Base) Packed Weight:\nThis of course represents everything in my pack that does not fall into one of the above mentioned categories. This represents the basic gear that will go into my pack for any overnight hike, whether I am out for a single overnight hike, or a 10 day (or more) hike. These items are the same no matter how many days I am out, considering these days are all within the same hiking season.\nThe BPW is also the weight that those of us that are in the lightweight crowd like to throw around. This is the weight that defines us and places us into a particular “weight-class.”\nMy base pack weight for this trip is currently at the 3.99 pound mark! As I mentioned at the beginning of this article, a “SUL” backpacking weight falls under 5 pounds, which this weight clearly is… 🙂\nSo, rounding it out, here are the figured weights for each category for this planned hiking trip:\n- Worn Items: 3.44 lbs (55.10 oz)\n- Carried Items: 1.12 lbs (17.95 oz)\n- ~Fuel: 0.13 lbs (2 oz)\n- ~Water: 2.5 lbs (40 oz)\n- ~Food: 4.96 lbs (79.3 oz)\n- ~Misc. Consumables: 0.27 lbs (4.35 oz)\n- Base Packed Weight: 3.99 lbs (63.77 oz)\nFor some combined figures:\n- BPW: 3.99 lbs (63.77 oz)\n- Total Consumables Weight: 7.58 lbs (121.3 oz)\n- Total Pack Weight: 11.57 lbs (185.07 oz)\n- Skin-Out Weight: 16.13 lbs (258.12 oz)\nSo, at this point I am at a sub 4 pound base weight, if even only by a hair (but then again, at these weights, even the hairs count). However, I am still a full pound under the 5 pound “SUL” cut-off mark, so I am pretty happy, heck, I am stoked with the set-up that I have put together. I will admit that this set-up will not work for every situation, but for this one in particular, I believe it will work out very well. But then again, only use will tell…and I plan to find out very soon… 🙂\nAnd I have got to say that another nice perk of having such a light base weight that I am looking forward to, is that the total pack weight will be very close to the weight that will be in the pack on the last day of the hike. (Remember, the consumables will have been consumed.) This means that on the last day of my hike, my total pack weight will start out around 7 – 8 pounds (counting food and water for that day) and will finish up around 4 – 6 pounds (depending on water). I gotta say, I am looking forward to this… 🙂\nAs usual, I have decided to do some video to show my particular gear in this “SUL” gear list. I will admit though that it took me more time than I had planned, so I had to make it into a 2 part video series…\nAs well, if you are interested in checking out my full gear list for this hike, spread out all nice and neat on a Google Doc spreadsheet, you can do so by clicking on the link below. Also within this same gear list, I have a second sheet labeled “Food Planner” which details all of my food that I plan to pack for this hike. Simply click on the tab in the bottom left hand corner of the spreadsheet to open the Food Planner sheet.\nSo, I am looking forward to marking 2 new milestones on this hike. One, my “SUL” pack weight and two, completing 3 of the biggest hiking days I have hiked so far. I can even go as far as saying a third milestone will be “thru-hiking” the Appalachian Trail that runs through the Great Smoky Mountains National Park. Regardless, as I said at the beginning of this post, it is going to be an “Epic Hike” for me…\nThanks for reading, and watching the videos.', 'Being properly hydrated when hiking or backpacking is not only vitally important for your health, but for your enjoyment as well. Dehydration can lead to serious health problems, which can be especially dangerous if you’re on a backpacking trip in the middle of no where for days at at time.\nThe biggest issue that comes with stay hydrated is that water is heavy and inconvenient to carry! So in order to keep weight you’re carrying as low as possible while also ensuring that you stay hydrated, it’s best to figure out just how much water you expect to need during your hike.\nHydration Best Practices\nPre-hydrate – hydrating before hiking gives your body a “head start” so that once you start hiking, you’ll last longer before needing more water. If you’re starting already dehydrated, it’s going to be more difficult to remain hydrated during your hike. Drink about a liter of water in the few hours before setting off if you can. Drink it slowly so that your body an absorb it – and then make sure to pee before you leave!\nSip Often – start sipping water every 10-15 minutes right at the beginning of the hike. Don’t wait until you’re thirst to start drinking. The more you can keep yourself hydrated throughout the hike, the more enjoyable your trip will be.\nDon’t Chug – if you’re sipping water every 10-15mins, you’re constantly re-hydrating your body. The small amounts allows the water to be better absorbed. If you’re chugging water, it’s going to go through you more quickly as it’s too much at one time for your body to absorb.\nDon’t Over Do It – it is possible to drink too much water. While it’s very unlikely that you will actually do this, too much water can also make you pee a lot, which reduces your hiking time and just makes you feel full and bloated. It can also dilute sodium levels in your blood. (Salty snacks are also a good idea to enjoy on your hike!)\nReplace Electrolytes, Not Just Water – when you exert yourself and sweat, you lose electrolytes, which are also an important part of keeping your energy levels up. If you’re hiking for more than an hour, you should considered trying to replace electrolytes during this hike. Sodium and potassium are the most important, but calcium and magnesium are too. One of the easiest ways to do this is just to add an electrolyte powder to your water.\nPost-hydrate – once you’re done with your hike, drink extra water to re-hydrate yourself and get back to normal. If you’re backpacking, it’s especially important to drink water at camp in between hikes (when it’s available) as both a post and pre-hydrate. When you’re in the wilderness for days at a time, you want to keep your hydration levels up as much as possible since it will likely decrease over time the longer you’re away from steady water supplies.\nHow much water should you drink when you’re hiking?\nRoughly a 1/2 liter (2 cups or 16oz) of water every hour is what you should be aiming for when hiking during normal conditions in order to stay hydrated.\nWhen hiking in heat, humidity, or at higher altitudes, this doubles to roughly 1 liter every hour.\nAlso know yourself and listen to your body. While this is a good general rule of thumb to follow, it will vary from person to person. Some people just naturally drink and need more water than others. But if you’re not drinking 1/2 liter every hour, you really should try to increase this to avoid problems later on.\nAlso to reduce weight, it’s a good idea to bring a water bottle with filter or purification tablets with you if you know there’s going to be a water source on the trail. That way you can just drink right out of the steam water that carrying all of your water with you!\nCalculating How Much Water To Bring When Hiking\nIf you want to get more specific and take into account the myriad of other factors that can influence how much water you should drink, here’s a more detailed calculation for that:\nHere are the factors you need to know for this calculation:\n- elevation gain\n- backpack weight\nGet ready to do some math!:\n((Miles x 20) + (Elevation Gain / 16.66) + ((Temperature – 80) -> (IF POSITIVE /20) x 120) + ((Altitude – 10000) -> (IF POSITIVE /2000) x 120) + ((Weight – 50) -> (IF POSITIVE /25) x 120) / 120 = Total Liters\nThen it’s also a good idea to add another .5 liters in there for safety.\nA 10 mi hike with an elevation gain of 2000ft in 100 degrees at an altitude of 12,000ft, carrying a 60lb pack. (Phew that sounds awful!)\n10 Miles x 20 = 200 minutes\n2000ft / 16.66 = 120 minutes\n100 degrees – 80 = 20, which is a positive number, so we continue with the rest of this part of the equation; 20/20 = 1 unit of 20; 1 X 120 = 120 min\n12,000ft – 10,000 = 2,000, which is a positive number, so we continue; 2,000/2,000 = 1 ; 1 x 120 = 120min\n60lbs – 50 = 10, which is positive, so we continue; 10/ 25 = .4; .4 x 120 = 48mins\nTotal minutes = 608 / 120 = 5.06 liters (+.5 for safety)\nHow this calculation works:\nWe are trying to reduce all of these variations into time, which can then be converted to liters.\nIn general, the average person walking on a flat path will finish a mile in less than 20mins. So Miles x 20 = Minutes to finish those miles\nIn general, every extra 1000ft of elevation gain will take an additional hour. 1000ft elevation gain = 60mins.\nIf it’s hot (over 80 degrees), you should bring an extra liter of water for every 20 degrees. Temperature – 80 = Degrees over 80. If this number is positive: Degrees over 80 / 20 = Units of 20. For every unit of 20, we want to bring an additional liter of water, but we need to write this in time rather than liters so that it can be added to the other metrics. In the general estimate of time to liters mentioned above, .5 liter = 60mins. So 1 unit of 20 = 1 liter = 120mins.\nYou should also bring an additional liter for every 2,000ft if hiking over altitudes of 10,000ft. Altitude – 10,000 = altitude over 10,000ft. Altitude over 10,000ft/2000 = Units of 2000. For every unit of 2000, we want to bring an additional liter of water, but we need to write this in time rather than liters. So 1 unit of 20000 = 1 liter = 120mins\nFor very heavy backpacks (over 50 lbs), we want to bring an additional liter per 25lbs. Weight – 50 = weight over 50lbs. Weight over 50lbs/25 = Units of 25. For every unit of 25, we want to bring an additional liter of water, but we need to write this in time rather than liters. So 1 unit of 25 = 1 liter = 120mins\nAdd all of that together and you get the total value of these variables in minutes. Divide this by 120 (120 is 60 (minutes in an hour) x 2 (.5 liter every hour))\nHow Much Water Should I Carry When Backpacking?\nWater is heavy – probably one of the heaviest things you’re going to carry. It generally weights about 2lbs per liter, which can add up really quick!\nIf you know there’s going to be water along the trail, I highly recommend bringing a water bottle with filtration system, purification tablets, or a gravity water filter (this is great for once you set up camp to purify larger amounts of water!) so that you can just drink from the available water and not have to bring all of it with you.\nAs a general rule of thumb, you should be prepared for .5 liters every hour. Plus an additional liter for every 1,000ft of elevation gain. If you want a much more detailed and complicated equation taking into consideration several factors, that’s available above.\nKnow your pace and plan out your trip so that you only have to carry the water that you need for each section between water sources. This will ensure that your pack stays as light as possible while still carrying the necessary amount of water.\nAlso keep in mind that a lot of times, water is also needed for your food. Take a look at your meal plan and see how much water you’re going to need to make your food. Dehydrated food takes a lot of water – sometimes .5 – 1 liters per pouch! – and this needs to be accounted for in your calculations for water. This is also why gravity water filters are really helpful when setting up a campsite where you plan on spending a few days since you can filter a large amounts of water at one time.\nCarrying your Water: Bottle vs Bladder\nWhy not both?! I always bring both a bladder and bottle with me when I’m backpacking since each one has it’s own strengths and weaknesses.\nI use the bladder while I’m hiking since it’s so convenient and then will fill up a water bottle with the local water source on the trail and use this for cooking or to refill the bladder.\nHydration packs or bladders\nPros: Really convenient when on the move and carries a lot of water (usually 3 liters). They allow you to have your hands free since the water is on your back and also make it very easy to sip your water since the hose is easily available over your shoulder all the time.\nCons: More difficult to refill, get water out of, and clean. Harder to know when it’s almost empty Also due to the large volume, if you want to add electrolytes or flavors, it’s going to effect all of your water supply.\nReusable bottles can either come with a water filtration system automatically built in, or just a good sturdy regular bottle. When choosing a water bottle, the mouth size can be an important thing to consider. If you plan on using this bottle to collect water along the trail (drips from a seep or a small stream), a larger mouth is generally better.\nPros: Easy to refill in streams, from seeps, or other water sources. Easier to get water out of if needed for cooking.\nCons: Carries a smaller amount of water. Harder to drink from (need to stop to get it out of your pack) or you have to carry it in your hands.\nHow do you know how much water is left in your hydration pack?\nThis can be a bit tricky. I’ve definitely incorrectly estimated how much water I was drinking while hiking and had much less than expected by the time we got to camp.\nCurrently there isn’t a way to monitor the water levels in your hydration pack without just looking at it. There have been a few purposed products such as HydrateMate and Hydration System, which would do this, but as far as I know has not come to exist yet.\nYou really just have to guesstimate. Know yourself and how quickly you usually drink water and if you think you’re feeling particularly thirty during this hike. Expect to drink more when it’s hot and you’re at higher elevations.\nIt’s also a good idea to stop and check your bladder whenever you’re near a water source even if you don’t think you need to refill.']	['<urn:uuid:c35c4b87-5953-4572-9e34-25586dc11ddc>', '<urn:uuid:d6a9cbc5-287e-4a89-bce8-a7627e0f69a5>']	factoid	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T18:05:20.075489	28	66	4388
85	need help with challenging behaviors at home what resources missouri mental health department offers	The Missouri Department of Mental Health offers a Behavior Resource Team that provides various supports. They offer training and consultation on Positive Behavioral Supports (PBS) and Tools of Choice to develop proactive, positive supports and preventative strategies for challenging behaviors. The team conducts environmental evaluations to help prevent challenging behaviors and provides consultation on positive, teaching-based support strategies for individuals in their natural home. They also offer crisis support via phone and direct services, and help support teams (including staff, providers, and families) in developing positive support strategies and safety crisis plans for individuals with high intensity needs.	"['Presentation on theme: ""Planning for the Future: Understand DMH-DD Systems and Service Options Presented By: Kadesh Burnett; St. Louis County Regional Office Family Support Coordinator.""— Presentation transcript:\nPlanning for the Future: Understand DMH-DD Systems and Service Options Presented By: Kadesh Burnett; St. Louis County Regional Office Family Support Coordinator\nDepartment of Mental Health The Missouri Department of Mental Health serves residents of the state by working to prevent mental disorders, developmental disabilities, and substance abuse and by treating, habilitating and rehabilitation of individuals with those conditions. The treatment and habilitation of persons with medical diagnosis of mental retardation and other developmental disabilities is the responsibility of the department’s Division of Developmental Disabilities.\nCommunity Resources and Supports Assessment Support Coordinator Individual and Family Supports Autism Navigator Employment/Youth Transition Coordinator Advocacy Specialist Behavior Resource Team Provider Relations Utilization Review The Regional Offices throughout the state has Community Resource Leads to include five core functions, as recommended by state Stakeholders, relating to consumer relations, business administration, resource administration, assuring the quality and availability of clinical services.\nThe assessment process involves the individual and family member completing assessment tools: Vineland-assessment tool completed for persons under the age of 18. MOCABI-assessment tool completed for persons over the age of 18. How should I prepare for this meeting? Bring any and all source documentation of disability diagnosis, IEP, behavior assessments, and legal documentation. Having come prepared for the assessment process with these resources will reduce the likeliness of RO assessment delay your determination of eligibility for RO services.\nSLRO Support Coordinator Each eligible individual is assigned a Support Coordinator who helps put together an Individual Service Plan (ISP) and then monitors the persons treatment and habilitation to assure that the plan is followed and that the individual’s needs are met. How can my Support Coordinator assist me and my family? Support Coordinator are available to assist and provide guidance related to IEP advocacy and supports, Community Resources (Funded/Non Funded), Government Resources (Funded/Non Funded). However, all needs must be documented in the eligible individuals PCP.\nProvides information specific to the region in regards to local community resources. Assist with developing educational materials for families and individuals through approved trainings, brochures, etc. which can be used universally throughout all regions. Support with Peer Support matching, family focused educational opportunities; Alternatives to Guardianship, Charting the Life Courses, webinar trainings, and family leadership workshops and trainings. St. Louis County and St. Louis Tri-County expertise.\nProvides training and technical support to service coordinators and other stakeholders regarding autism and autism services Connects individuals and families to autism related supports and/or activities Attends monthly regional Parent Advisory Committee (PAC) meetings and shares updates and information locally Acts as an advocate for individuals and families affected by Autism\nEmployment and Youth Transition Provides training and technical support to service coordinators and other stakeholders regarding employment and employment resources Moves the system towards creating a culture which focuses on employment – Emphasizes the importance of employment as a priority at the individual and systems level Multi-system Collaboration –Building relationships with school administrators and educators, community members and service delivery partners providing expertise regarding developmental disabilities and our services system Training and Technical Assistance –Track complex and changing employment policies and provide tools and resources to service coordinators, individuals and other advocates who might not have this knowledge or time to do necessary research. Provides resources to service coordinators regarding employment Promotes expansion of effective employment alternatives such as entrepreneurships & competitive employment in community settings\nResponsible for developing, implementing, evaluating and enhancing speakers bureau Provides training and mentoring to individuals within the region Assists with development of self-advocacy groups and training projects Participates in regional and statewide committees and workgroups, as well as Project Stir.\nProvides information, training and consultation and technical support to service coordinators and other stakeholders on Positive Behavioral Supports (PBS), Tools of Choice, and other topics to assist in developing proactive, positive supports and preventative strategies for challenging behaviors Provides environmental evaluation to assist with prevention of challenging behaviors and consultation on positive, teaching based support strategies for individuals in their natural home and in residential placement Provides crisis support in the region via phone and/or direct services Assists the support team (staff, providers, families, etc) in developing positive support strategies and safety crisis plans for individuals identified as having high intensity needs\nResponsible for provider development that enhances existing supports and services Explores new services and works to increase provider capacity to address unmet needs Develops, executes and reviews approved provider contracts Provides technical assistance and training to contracted service providers Works with providers in regard to Improvement and Critical Status Plans Completes budgets with contracted service providers and allocates resources Supervises Vendor Services Coordinator and Community Living Coordinators\nDD Waiver Services DD administers 5 Home and Community Based Medicaid Waiver programs Missouri Children With Developmental Disabilities Waiver (MOCDD or Sara Jian Lopez Waiver) Community Support Waiver Comprehensive Waiver Autism Waiver Partnership for Hope Waiver\nCommunity Based Waivers Comprehensive For all ages - must meet eligibility guidelines for Medicaid as determined by DSS(DFS) Community Support Waiver For all ages - must meet eligibility guidelines for Medicaid as determined by DSS(DFS) Services cannot exceed $28,000 annually and the participant must not require residential services\nCommunity Based Waivers Lopez Waiver Under the age of 18 Ineligible for Medicaid due to the deeming of parental income and resources Live with family Have a permanent and total disability Require habilitative (waiver) services Autism Waiver Participants must be between the ages of 3-18, have a diagnosis of Autism Spectrum. Have a functional limitation in 3 or more areas, and service needs cannot exceed $28,000.00 annually.\nCommunity Based Waivers Partnership for Hope Waiver Implementation: October 1, 2010 95 counties across the state Includes 20 services; available in the 95 counties participating, plus City of St. Louis Annual Cost Cap of $12,000\nParticipation Limitations Individuals receiving services under the waiver have been determined to have the most critical needs. The criteria used to determine needs includes PCP, and source documentation, that is reviewed by the Utilization Review Committee (UR). Number is pre-approved by the Centers for Medicare and Medicaid Services (CMS), based on the state’s request No additional individuals may be enrolled Number the state requests is limited by funding available for the state portion of the match\nFAQ’s Regarding Waver Services Missouri’s Medicaid Waivers for Persons who have Development (Fact Sheet) http://www.dmh.mo.gov/mrdd/new/manual/wvrfacts heet9-09.pdf http://www.dmh.mo.gov/mrdd/new/manual/wvrfacts heet9-09.pdf Missouri Family to Family Resource Center mofamilytofamily.org']"	['<urn:uuid:7e578a91-fb2e-402f-8b32-aef16d0edbe2>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T18:05:20.075489	14	98	1077
86	Can I use Palo Santo and tea tree oil for my house's smell and pest control?	Both Palo Santo and tea tree oil can be dangerous for pets. Tea tree oil can be toxic if pets ingest it or if it's used improperly, even in small doses. Similarly, while Palo Santo is a natural insect repellent, its smoke can irritate pets' lungs and its essential oil contains d-limonene which is toxic to cats. If you must use either, do so in well-ventilated areas when pets aren't present, though safer commercial alternatives are recommended.	['By Hanie Elfenbein, DVM\nFleas are itchy and uncomfortable for our pets, but they are easily preventable. Options to protect your pet from fleas include collars, topical treatments, and chewable tablets. Nowadays, some of these preventatives last as long as 12 weeks.\nSome pet parents opt to use natural topical flea treatments, such as tea tree oil. However, tea tree oil can be toxic for pets if ingested or used improperly. Your veterinarian can help you decide what’s right for you and your pet.\nIs tea tree oil safe as a topical flea treatment?\nHome remedies for fleas are generally inadvisable for both safety and efficacy reasons. Commercially available flea treatments from your veterinarian are safety tested under strict conditions. In addition, they are proven to be effective at preventing and killing fleas. Essential oils and extracts like tea tree, eucalyptus, and citronella do not require testing for safety or efficacy, nor are the contents of the bottle regulated. That means you may not get what you are paying for, and have no guarantee that it will work.\nBut can tea tree oil kill and repel fleas?\nWhen diluted and used safely, tea tree oil can kill and repel fleas. But it is among the herbal treatments listed as “especially risky” in the Merck Veterinary Manual. This is because tea tree oil is difficult to dilute properly in home kitchens.\nCommercially available formulas for pets contain only 0.1 to 1 percent tea tree oil. Even if you measure everything precisely and shake the bottle before spraying it on your dog’s coat, it is easy to apply more than intended. If your cat or dog licks the oil off, which pets tend to do as they groom, your pet could get very sick.\nStore-bought shampoos containing tea tree oil have very small amounts of tea tree oil. The product is formulated so that the oil is evenly distributed, reducing the toxic risk to pets.\nAren’t topical flea products more toxic than tea tree oil?\nMost topical flea products recommended by veterinarians use a type of chemical that targets molecules found in the body of insects that are not found in mammals. That’s what makes them safe to use on our pets and good at killing fleas quickly in the small doses applied to skin or ingested.\nThere is another category of chemicals that are toxic to some mammals but not others. For example, some tick products contain a class of chemical called permethrins that are safe to use on dogs but are toxic to cats. Tea tree oil falls into this category: it can be toxic to our pets and fleas, even though it is likely safe for most humans when used on the skin. The dose required to kill all of the fleas might be fatal to your cat.\nAnother reason commercially available flea treatments are preferred over essential oils like tea tree is that some contain chemicals that actually stop the development of the next generation of fleas. That means any eggs a female lays before she is killed by the flea treatment will not be able to hatch. No adults + no eggs = no more fleas.\nWhat are some safer alternatives for repelling fleas?\nOne home remedy that may be safer than tea tree oil is diatomaceous earth. This is a long name for fossilized algae. It works by creating holes in bugs like cockroaches, snails, fleas, and even some kinds of worms. Diatomaceous earth has long been used around vegetable gardens to keep bugs out and is often fed to horses and livestock to help treat intestinal worms. Since it is safe to consume in specific quantities (consult a veterinarian or physician), it is much safer to use on your pet than something like tea tree oil, which is toxic when ingested even in small quantities.\nIf you choose to administer a home treatment like diluted tea tree oil, please consult your veterinarian first. Your doctor not only knows the science but also is familiar with your pet and your lifestyle, and can help you make the best choice for your family.', 'Most of us like the feeling that a bubble bath or a glass of wine while sitting on the couch, eventually combined with aromatherapy, gives us when we are stressed. This form of relaxation that can help us relieve stress consists of burning incense sticks or scented candles, using essential oils or bath salts, and more.\nA typical scented stick consists of a bamboo wood to which a fragrant powder of plants adheres, but there are also ready-to-burn, such as Palo Santo or “holy stick” (Bursera graveolens).The scent of this wood has notes of mint, citrus, and pine and is used for relaxation and purifying our space.\nFor cats, raw Palo Santo wood is not toxic if it is nibbled. Exposing them to the smoke, when is burning, can irritate their lungs. The only advantage Palo Santo has is that it is a good natural repellent for insects.\nRegarding essential oils, it is advisable to use them when the cat is not around because they are concentrated extracts from plants, and most are harmful for our fluffs, including Palo Santo.\nThe majority of essential oils are bad for your cat, but there are some that are considered safe like lavender, copaiba, helichrysum, and frankincense oil.\nShort description of Palo Santo\nA Palo Santo tree is native to South America and belongs to the citrus family. The wood of the Palo Santo is available in the form of sticks, chips, or powder. It usually comes from the branches of the tree, so it is 100% natural. It also contains a very fragrant resin, which can be extracted and used separately. An analysis of Palo Santo essential oil showed that the main active constituents are limonene 89.33%, α-terpineol 11%, mentofuran 6.6%, and carvone 2%. (1)\nThe wood of the saints was used by indigenous people for spiritual ceremonies and healing – to relieve the symptoms of colds and flu, depression, emotional pain, and other ailments. It is said that its aroma induces relaxation and leads to meditation, but it also brings creativity and luck.\nThese benefits only exist when a Palo Santo tree dies naturally, and then it is allowed to decompose in five to eight years. In these years, the oil in the wood matures completely before being harvested and processed (this is why it is so expensive).\nPalo Santo – Risks and Benefits for cats\nAlthough Palo Santo is not poisonous to cats if they nibble it a bit, it contains d-limonene (a terpene found in most citrus peels), which is toxic. Depending on the concentration at which the pet is exposed (if it ingests wood or essential oil), according to the researchers, d-limonene poisoning shows the following clinical signs:\n- short or long-term hypersalivation,\n- digestive disorders,\n- shiver-like muscle tremors,\n- severe hypothermia,\n- irritation of the mouth,\n- and even death. (2)\nThis toxicity is due to the lack of the liver enzyme glucuronyl transferase – a compound that helps to eliminate the substances found in many essential oils, such as phenols, eugenols, and d-limonene, that cats do not have.\nAlso, wood consumption can lead to intestinal blockages once it reaches the digestive tract.\nExposure to smoke from burning Palo Santo wood (and smoke in general) can cause respiratory problems. Furthermore, its inhalation may result in irritation of the mouth and gums.\nIf cats with skin allergies come in contact with Palo Santo by ingesting the wood or essential oil, rub against the tree bark, or are in a room full of smoke, it can exacerbate their allergy.\nPalo Santo essential oils are obtained by steam distillation of the fallen branches and dead trees. This unique chemistry does not occur when the trees are cut down. In other words, Palo Santo essential oil is a concentrated wood essence; therefore, it will have considerably higher amounts of d-limonene, which makes it unsafe for cats. Exposure to essential oils can irritate the whole body.\nIt is recommended to keep Palo Santo wood away from your cat’s prying eyes and sharp teeth. If you burn a stick from time to time, it is best to do it in a ventilated room.\nAlso, if you think your cat has ingested wood or has been exposed to the essential oil or smoke, go to the vet’s office immediately with your pet.\nFor some cats, Palo Santo wood stimulates play. They will nibble on it, purr, meow, and rub their head against it.\nIt is a good idea not to let your cat ingest it due to the medical complications that may occur later.\nDo cats like the scent of Palo Santo?\nWhile some cats like the smell of Palo Santo wood and love to stick their teeth in it, others do not react very well in its presence due to the content in d-limonene. But due to this compound toxicity, it is best to keep your Palo Santo sticks away from your cat, as well as when you are burning it because cats are very sensitive to smoke.\nIs burning Palo santo sticks Safe for my cat?\nBurning Palo Santo wood won’t kill your cat, unless you set your house on fire! On the other hand, the smoke can cause asthma attacks and irritate the oral cavity and lungs.\nLighting a stick once in a while in a ventilated room will probably won’t affect your cat in any way.\nTo be sure your cat will be fine, burn the Palo Santo sticks away from your beloved pet!\nIs Palo Santo spray/diffuser ok for my cat?\nIt is recommended not to use Palo Santo spray or a diffuser when your cat is nearby. The best you can do is to spray it when the cat is not around and let it sit for a while.\nWhat About other Aroma Sticks and Oils?\nWhether you use burning aroma sticks, essential oils, or diffusers, it is best to do this when the cat is in another room as it can cause your pet all sorts of reactions.\nIf your feline suffers from respiratory problems or skin allergies, it is recommended not to use any essential oils or aroma sticks at all because its health will deteriorate.\nAre cats allergic to Palo Santo?\nLike any other ingredients, you will know if your pet is allergic to something only when it comes in contact with it, and Palo Santo makes no exception!\nIf your cat already has a skin allergy it is better not to use incense sticks in its presence, as it can exacerbate its skin condition.Custom stuffed animals of your pet\nIf your pet has asthma, try not to burn Palo Santo sticks or expose it to any other smoke because it can cause an asthma attack.\nWhich cat breeds are most affected by the incense smoke?\nIn general, short-nosed cat breeds are the most affected by smoke. They have shorter facial bones, and the soft tissues are modified- for this reason, the Persian, Himalayan, and Burmese breeds are the best known to have respiratory problems.\nWhat should I do if my cat came in contact with Palo Santo?\nIf your cat inhaled too much smoke from your burning incense, ventilate the room, move your cat to a smoke-free area, open the windows or take it outside for fresh air. If you see any unusual behavior (sneezing too much, drooling, or showing signs of asthma attack) take your pet to the veterinarian as soon as possible.\nIf your cat ate too much wood or ingested essential oil and you see it trembling, vomiting, hypersalivating, or it does not coordinate its movements, take it to the veterinary clinic for the proper treatment and care.\nIs Palo Santo similar to catnip?\nThese two plants are not related, but some cats react to Palo Santo as they do to catnip.\nCats will bite the wood, purr, meow, rub their face against it, and they will want to play – this is why it is said that Palo Santo stimulates play.\nPalo Santo Vs. Sage\nUnlike Palo Santo, sage is safe for cats (along with basil) – cats can ingest it without any toxic effects to them. However, be careful not to expose your kitty to the essential oils (either directly or through a diffuser). As we mentioned at the beginning of the article, cats lack the liver enzyme that breaks down chemicals found in essential oils, so even a low dose can be considered harmful.\nIs Peppermint or Patchouli Incense Safe For my Fluffy Pets?\nPeppermint plant is considered poisonous to cats – skin exposure and ingestion can be toxic for our felines. It is poisonous whether applied directly to the skin, used in diffusers, or a drop was licked from the ground. Therefore, do not use peppermint incense sticks or essential oils as they are very harmful to your cat.\nPatchouli plant is related to mint (like catnip), but is not safe for cats. Mint-based essential oils and aromas contain chemicals that are too strong for our little cats, making them difficult for their bodies to process.\nUnlike peppermint, patchouli is not considered toxic, but the likelihood of strong side effects on your cat’s health is high.\nCan I use air fresheners for my cat?\nIn general, air fresheners are composed of over 100 chemicals, including volatile organic compounds (such as limonene) – that is why it is recommended to use them when your cat is not in the room. Do not apply it directly on your pet or allow your cat to lick the product!\nIt is best to use pet-friendly air fresheners, such as Feliway, Enviroscent Plug-in Air Freshener Kit, Mrs. Meyer’s Room Freshener and other because they contain natural fragrances.\nAre scented candles safe for my cat?\nCats have a very sharp sense of smell, and the strong scents bother most of them; therefore, they may avoid the rooms where fragranced candles are used. However, if your cat is nearby, make sure the room is well ventilated so that the fumes and smell of the candle do not cause problems to your pet.\nRegardless of the candle type (natural or artificial, scented or not), there is always the risk that your cat will flip it and burn itself or even set the house on fire. So, be careful!\nBest and Worst essential oils and incense sticks for Cats\nNo matter what essential oil you choose to use, all of them have the potential to be dangerous to our cats, but there are some that are considered safer, such as:\n- and frankincense oil.\nIt is best to use them in a diffuser, as they are very dilute this way.\nPeppermint – raw peppermint and essential oil are poisonous to cats because it contains phenols, which are very harmful to our little felines.\nLemongrass – raw lemongrass is safe for cats, but in small quantities; Lemongrass essential oils can be fatal to cats.\nTea tree essential oil can be poisonous to cats if it is not diluted or if the dose is too high (before it is applied to the skin). Although tea tree oil was once considered a safe remedy for fleas, it is now recommended not to use it in cats.\nLavender plants and essential oils are toxic to cats if ingested and can cause nausea and vomiting.\nChamomile, in small doses, is considered quite safe for cats, but if ingested in large quantities or long-term it can be harmful to your cat. It contains a wide range of toxic substances for our little felines, such as tannic acid, anthemic acid, chamazulene, bisabolol, and volatile oils. (3)\nRosemary – although fresh rosemary leaves can’t cause any damage for cats, the essential oil is harmful to them.\nThyme – like peppermint, it contains phenols, which are dangerous to your cat and can make it sick.\nOther harmful essential oils for cats include vanilla, lemon, orange, grapefruit, rose, thieves, oregano, bergamot, and others.\nCinnamon and clove are considered both as toxic as peppermint.\nPlease Pin and Share our Quick Overview!\nWhether we are talking about Palo Santo essential oil or anything other oil, keep in mind that most of them are harmful to your cat because our pets lack a liver enzyme that helps eliminate these harmful substances.\nPalo Santo contains d-limonene, a terpene that is toxic to your cat. Therefore, it is a good idea to keep Palo Santo wood away from your cat’s curious eyes.\nAs for incense sticks, the smoke they emit can cause your cat to have respiratory problems and even asthma attacks.\nCats with skin allergies should be kept away from essential oils as they may amplify their medical conditions.\nHigh concentrated essential oils or continued exposure to smoke can be dangerous for cats. It is best to avoid using them around your fluffy pets and/or use them in well-ventilated rooms.\nI hope our article answered all your questions about your curious cat, incense sticks, and essential oils.\nPlease Share Your Thoughts In The Comment Section Below!\nHappy Cat Care!']	['<urn:uuid:1b652293-c6f4-4cbc-9550-5ce986cff120>', '<urn:uuid:31a95c4f-8858-4a42-ac9a-a4c6b43193b4>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T18:05:20.075489	16	77	2856
87	Which transport system focuses more on underground construction, HCMC or Mumbai?	Mumbai Metro has a more extensive underground component, with 25% of its 200 stations being underground and Line 3 being built almost entirely underground. In contrast, Ho Chi Minh City's Line 1 only begins below-grade in District 1 CBD before elevating through other districts.	['With a growing population and economy, Ho Chi Minh City faces important decisions regarding transportation infrastructure. Robert Marshall calls for a contextual approach.\nBy 2035, it is estimated that another 15 cities around the world will have populations exceeding 10 million. Ho Chi Minh City (HCMC) will be one of these new global megacities. In addition to a complex economy and world-class infrastructure and developments, a major component of its evolution relates to how HCMC will define its transportation strategy moving into the future.\nIf you’ve never been to Ho Chi Minh City, there’s almost no way to describe the volume, the flow, the dance, and the threatening crush of motorbike traffic that has come to define its streets. It’s a continuous movement of metal, noise and exhaust fumes that’s dynamic, unique and that I’ve come to love. In a way, motorbikes have been the perfect solution for this city. They’re affordable, compact, agile, weather-appropriate, and efficient – but can they support the city’s progress?\nWhat Could Replace the Beloved Motorbike?\nWhile off to a slow start, HCMC is building a metro system with Line 1 beginning below-grade in the District 1 CBD, elevating through District 2 and connecting to District 9, slated for 2020. The long-term plan calls for seven metro lines running in every direction to create a backbone for a new transportation system for the city. This future vision bypasses the historic urban model of streets clogged by private automobiles and transit ridership generated by attrition from a congested and failing road system. The hope is that motorbike ridership will shift to transit based on convenience. That’s the best-case scenario, and of course there’s a flipside to the coin.\nAt last count, HCMC registers 750 new motorbikes a day and over 8.5 million hit the road daily. Imagine how traffic would change with only a small percentage of motorbike ridership transitioning to automobile use in a city of over 10 million people? Private automobile ownership is growing quickly in HCMC and the impacts can already be seen (and felt). The actual area of city streets is fixed, and each new car occupies the space of six or more motorbikes, with less manoeuvrability and flexibility.\nRather than taking the typical route of transitioning from motorbike culture to private automobile culture, I’d argue that HCMC would be better off to transition directly from motorbikes to transit. In fact, the city would be well off to actively resist private automobile ownership and use.\nPrivate car ownership is increasing alongside personal economic growth. According to a report by Vietnam Investment Review, “by 2035, more than half the country’s population will join the ranks of the global middle class, with a spending of $15 or more per day.” The typical next step in the ‘traditional’ economic development story is a move to the private car. But is that the right move for this city – a city that doesn’t have the road capacity to accommodate further growth in automobile use? Is a ‘traditional approach’ the right move for any city in today’s era?\nHow Do We Design For an Unknowable Future?\nIf there’s any industry within which we’re seeing massive change, it’s the automotive industry. The use and ownership of private automobiles is entering a state of radical transition, particularly in urban settings. With autonomous, self-driving vehicles on the horizon and the testing of flying cars slated for 2020, there will be a fundamental change in how automobiles and motorbikes make their way through our cities.\nIs it smart economic sense to invest in private car infrastructure – freeways, parking lots, bridges – when the fundamental transportation mode as we know it is in radical transition? How do we design infrastructure that’s flexible enough to accommodate future methods of transportation – methods we can’t necessarily predict, i.e. we can’t know with certainty which technology will ‘stick’?\nWithin HCMC’s Central Districts, there’s potential for a system of “priority” streets, where traffic lanes or complete corridors would be dedicated to a particular transportation mode. Imagine a grid of streets where one corridor is dedicated to, for example, the through-traffic of autonomous vehicles, buses and transit, while another street is dedicated to motorbikes and pedestrians. This may very well be the future of the city where limited road space must accommodate maximum people movement, safely and efficiently. This strategy is well-utilised in cities with a strong and regular urban grid such as Barcelona, where each corridor has a well-defined priority. In a city like HCMC, with a large block pattern and a less regular urban grid, it may be more difficult to implement, but there is great value in organising streets with a hierarchy of users.\nHow Do We Connect People to Places in a Future Vision For HCMC?\nThe other key to the success of transit implementation will be the development of the public realm to a higher quality of pedestrian-oriented design using strategic connections. Transit users are pedestrians at one end of their transit journey or the other and are dependent on accessible and easy connections from stations into the surrounding urban fabric. The redevelopment of Saigon Central Park in HCMC’s District 1 is an example of the type of regeneration that will propel HCMC into the future.\nThe park connects people to places and extends to a flow of pedestrian and vehicular traffic to the larger city context. © B+H Architects\nThe motivation for the regeneration of the park relates partly to the new metro system. The surrounding area features a vibrant and dense mix of uses and the park itself is designed as a main transit hub for the city and to create a central commercial and retail centre that will feed the metro station with pedestrian traffic. HCMC will begin to transform into a walkable city with an improved public realm, replete with sidewalks, parks and linkages connecting through higher density neighbourhoods to metro stations – complementing and supporting a growing and thriving metro system for the people.\nLarge open outdoor spaces encourage group and solo gathering. © B+H Architects\nA bright future for HCMC depends heavily on how we move people through the city as the population multiplies. Considering the limitations of the existing infrastructure and the inherent unknowability of the future, it makes sense to look at HCMC within the lens of its unique context – particularly focusing on what’s working and what’s not. Applying a one-size-fits-all approach rooted in traditional development stories simply doesn’t make sense. By developing the local transit system and discouraging car use, the motorbike doesn’t need to disappear. In fact, it can continue to live as an efficient, agile, and sustainable mode of transportation in this future megacity.', 'The Mumbai Metro is a rapid transit system serving Mumbai city, Maharashtra, and the wider metropolitan area. The system is designed to reduce traffic congestion in the city, and complement the congested Mumbai Suburban Railway (normally called local trains) network. Mumbai Metro is being built in three phases over 15 years, expected to be completed in 2025. Upon completion, the core system will have eight high-capacity metro railway lines, spanning a total of 235 Kilometers (146 mi) and serviced by 200 stations.\n|Number of lines||Operational -1 | Under-construction-3 | Approved-3 | Planned-3|\n|Number of stations||12 Stations|\n|Began operation||08- Jun- 2014|\n|Number of vehicles||16 Trains|\n|Train length||4 coach trains|\n|Average speed||33 km/h (21 mph)|\n|Top speed||80 km/h (50 mph)|\nMumbai Metro Map\nMumbai Metro Route Lines\nThere will be a Total of 10 Lines in MMRD Metro Named As Line 1, Line 2, Line 3, and so on Till Line 10. The Lines are also color-coded as a follows-\n|Line Name||Line Color||Total Stations||Status|\n|Line 1||Blue Line||12 Stations||Operational|\n|Line 2||Yellow Line||39 Stations||Under Constuction|\n|Line 3||Aqua Line||27 Stations||Under Constuction|\n|Line 4||Green Line||32 Stations||Under Constuction|\n|Line 5||Orange Line||17 Stations||Under Constuction|\n|Line 6||Pink Line||13 Stations||Under Constuction|\n|Line 7||Red Line||13 Stations||Under Constuction|\n|Line 8||Gold Line||08 Stations||Planned|\n|Line 9||Purple Line||No Data Yet||Planned|\n|Line 10||Magenta Line||No Data Yet||Planned|\nMumbai Metro Stations\nMMRD Metro will have Around 200 Station covering about 235 Kilometres once it is fully constructed. It’s 25% of the stations will be underground and is being built in 3 Phases in 15 years expected to be completed by 2025.\nPhase 1 of Mumbai Metro Line ( 2006-2011)\n|Line Name||Corridor Name||Distance (KM)|\n|Line 1||Versova – Andheri – Ghatkopar||11.07|\n|Line 2||Bandra – Kurla – Mankhurd||13.37|\n|Line 3||Colaba – Bandra – Seepz||38.24|\nPhase 2 of Mumbai Metro Line (2011-2016)\n|Line Name||Corridor Name||Distance(KM)|\n|Line 4||Charkop – Dahisar||7.5|\n|Line 5||Ghatkopar – Mulund||12.4|\nPhase 3 of Mumbai Metro Line (2016-2021)\n|Line Name||Corridor Name||Distance(KM)|\n|Line 6||BKC – Kanjurmarg via Airport||19.5|\n|Line 7||Bandra (E) – Dahisar (E)||16.5|\n|Line 8||Hutatma Chowk – Ghatkopar||21.8|\n|Line 9||Sewri – Prabhadevi||3.5|\nMumbai Metro Current Working Line-\n|Line||Line Color||From||To||Avg Frequency|\n|1||Blue||Versova||Ghatkopar||3 Min At peak Hour|\n8 Min at Non-Peak Hour\nMumbai Metro Line\nLine 1 or BLUE Line: Line 1 connects Versova in the western suburbs with Ghatkopar in the central suburbs, covering a distance of 11.4 kilometers (7.1 mi). It is fully elevated and has 12 stations. Work on the Versova-Andheri-Ghatkopar corridor, a part of Phase I, began on 8 February 2008. An important bridge of the project was completed in late 2012. The line opened for service on 8 June 2014.\nLine 2 or YELLOW Line: Line 2 Yellow Line This corridor is being executed in two phases i.e. 2A and 2B.The 18.589 km (11.551 mi) long 2A corridor is being executed by DMRC from MMRDA. The corridor has 17 stations (from Dahisar (West) to D.N.Nagar) and it costs Rs 64.1 billion (equivalent to Rs 72 billion or the US $ 1.01 billion).\nLine 3 or AQUA Line: This Line corridor is built almost entirely underground, and is 33.50 km (20.82 mi) long with 27 stations. The metro line will connect the Cuffe Parade business district in the south of Mumbai with SEEPZ and Aarey in the north. It will also pass through Mumbai’s domestic and international airports, for which the airport operator (GVK) has committed an equity investment of Rs 7.77 billion (equivalent to ? 9.4 billion or US $ 132.29 million in 2019).\nLine 4 or GREEN Line: Line 4 of the Mumbai Metro envisages a 32.32 km (20.08 mi) long elevated corridor, covering 32 stations from Kasaravadavali (near Thane) in the north. It costs estimated to be 165.96 billion (equivalent to Rs. 180 billion or 2.30 billion US dollars in 2019). The project will help connect the Thane city of Mumbai to an alternative mode of public transport.\nLine 5 or ORANGE Line: The 24.9 km long Thane-Bhiwandi-Kalyan. MMRD Metro-5th corridor will have 17 stations and Rs. 8,416 crores Rs. It will be a fully elevated corridor. It connects Thane to Bhiwandi and Kalyan in the eastern suburbs, a 12th line will be made a further extension to Taloja in Navi Mumbai.\nThane (West) includes Kapurbawadi, Balkum Naka, Kasheli, Kalhar, Poorna, Anjur Phata, Dhamankar Naka, Bhiwandi, Gopal Nagar, Temghar, Rajanuli Village, Govegaon MIDC, Kongaon, Durgadi Fort, Sahajanand Chowk, Kalyan Railway Station. Kalyan APMC.\nLine 6 or PINK Line: The 14.47-km-long Lokhandwala-Jogeshwari-Vikhroli-Kanjurmarg Metro-6th corridor will have 13 stations and costs Rs. 6,672 crores. It will be an elevated corridor. It will connect Lokhandwala Complex in Andheri with Vikhroli in the western suburbs and Kanjurmarg in the eastern suburbs.\nThe stations include Lokhandwala Complex, Adarsh Nagar, Momin Nagar, JVLR, Shyam Nagar, Mahakali Caves, SEEPZ Village, Saki Vihar Road, Ram Bagh, Powai Lake, IIT Powai, Kanjurmarg (W), Vikhroli-Eastern Express Highway.\nLine 7 or RED Line: The corridor is 16.475 km (10.237 mi) long, and runs from Dahisar (East) in the north to Andheri (East) in the south, with another extension to Bhayandar in the north, and Mumbai Airport is in Terminal 2 South. The line is partially elevated (under construction, with completion for 2019), and partially underground (approved, with construction planned to begin in 2018).\nLine 8 or GOLD Line: It is a proposed metro line between Chhatrapati Shivaji Maharaj Airport to Navi Mumbai International Airport. It will connect Mumbai Airport with the upcoming Navi Mumbai Airport and will have a length of about 32 km.\nBelow 2 project is now on the DPR stage.\nLine 9 or PURPLE Line: This is a proposed metro project to connect Mira Road to Virar. The length of the project is 23 km and the estimated cost of the project is 6900 crores.\nLine 10 or MAGENTA Line: This is a proposed metro project connecting Vikhroli to Kanjurmarg and further to Ambarnath-Badlapur. It will have an intersection at Kanjurmarg with line 6 i.e. pink line. The project is now on the DPR stage. The length of the project is 45 km and the estimated cost of the project is 13,500 crores.\nMMRD Metro Fare Slab\n|Kilometers (Km)||Fares (Rs.)|\n|3 to 8||10|\n|8 to 12||20|\n|12 to 15||20|\n|15 to 20||30|\n|20 to 25||30|\n|25 to 30||30|\n|30 and above||40|\nMumbai Metro Fare\nMumbai Metro Fare Starts from Rs 10 and Accroding to Stations Traveled the maximum amount is Rs 40\n|From||Versova||DN Nagar||Azad Nagar||Andheri||Western Express Highway||Chakala/JB Nagar||Airport Road||Marol Naka||Saki Naka||Asalpha||Jagruti Nagar||Ghatkopar|\n|Western Express Highway||20||20||20||10||10||10||20||20||20||20||30||30|\nMumbai Metro Running Time\nThe headway between each train on Weekdays from Versova to Ghatkopar is (From Monday to Friday) will be 3 and Half minutes at peak hours, and 8 minutes at non-peak hours. The headway between each train on Saturdays during peak hours will be around 5 to 6 minutes. The headway between each train on Sundays or Public Holidays will be 8 minutes.\n**If you have Smart Card of MMRD Mumbai Metro Rail, Then You can Recharge it from here. – RECHARGE MUMBAI METRO CARD\nMumbai Metro Time First Train –\n|First Train From||Time|\nMumbai Metro Time Last Train –\n|Last Train From||Time|\nPassengers can also buy tickets by their mobiles and enter the gates through QR code payment. The service is provided by Paytm and a QR code is generated on the travellers phone which is read by the scanners at the gate upon entry and exit.\nALSO READ –\n- New Mumbai Metro Vacancy 2020: MMRCL/HR-Rect./ 2020-01\n- Metro Recruitment:Mumbai Metro|MMRDA|110 Non-Executive Posts']	['<urn:uuid:7e6205f6-30d6-48fb-b37d-83b064377b20>', '<urn:uuid:38c18950-1e48-4fac-98ad-329bf9bc2d8c>']	factoid	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T18:05:20.075489	11	44	2345
88	How does Major League Soccer's star system for recognizing championship victories work, particularly after the 2016 changes?	In Major League Soccer's current system, implemented in 2016, teams display stars differently based on their number of championships. Reigning champions wear a large gold star with the year of their league win above any other stars. Teams with five championships (currently only the LA Galaxy) display one gold star, while teams with one to four MLS Cup victories wear one silver star for each championship win.	"['Add your company website/link\nto this blog page for only $40 Purchase now!Continue\nFutureStarrA Future Stars of Sports Petersburg\nThe NBA G League is similar to the NHL’s “American Hockey League” or the MLB’s “Class AA” league. It operates much like the minor leagues do, except that the G League is essentially a recognition of affiliations from the NBA franchised clubs. To make a quick comparison with the sports mentioned, the G League is the minor league of the NBA.\nThe WTA Future Stars platform was established in Singapore in 2014 and began with just 12 markets and 24 players in its first year. Each market sends two participants – one each in the U14 and U16 categories, with host nation, China, being represented by a total of eight players. Four Chinese players in each age group will be selected through the Chinese Tennis Association qualification process. One player in each age group will come from the Gemdale Junior Tour, which is run by Gemdale Sports, the official event promoter and presenting sponsor of the Shiseido WTA Finals Shenzhen.\nAjax currently have the right to wear three stars since they have won the league more than 30 times. PSV have the right to wear two stars with their 21 league titles, while Feyenoord and HVV Den Haag can add one for their 15 and 10 titles, respectively. HVV won all titles before 1915 and is currently playing in the fourth level of the Dutch league system. In Malta, Floriana, Valletta and Sliema Wanderers boast 2 golden stars on their badges having won 25, 20 and 26 titles, respectively, while Hibernians have one star thanks to their 10 titles. No other team in the Maltese League has the golden star on their badge. (Source: en.wikipedia.org)\nIn the equipment regulations for FIFA competitions, section 16.1 states, ""Those Member Associations that have won one or more of the previous editions of the FIFA World Cup or the FIFA Women\'s World Cup may display on the Playing Equipment used by their first men\'s or women\'s representative teams a five-pointed star, or other symbol as instructed by FIFA, per edition of the FIFA World Cup (men\'s shirt) or FIFA Women\'s World Cup (women\'s shirt) won by the Member Association.""\nMajor League Soccer\'s previously informal system, one star per MLS Cup title, was standardised in 2006, with the defending champions wearing the MLS Scudetto, like the Serie A system, for one season before adding a new star. Starting in 2012, the Scudetto was replaced with a single gold star worn by reigning champions above any other silver championship stars. In 2016, this system changed again in recognition of the LA Galaxy\'s fifth championship title: champion clubs during their title defence will wear a large gold star (featuring the year of the league win) above other smaller stars set in silver; clubs with five championships (presently only the Galaxy) will wear one gold star; and teams with one-to-four MLS Cup wins will wear one silver star for each victory. (Source: en.wikipedia.org)']"	['<urn:uuid:a56fbcce-68be-4553-ad81-57baac0b9cd8>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T18:05:20.075489	17	67	504
89	How are solar panels used in educational mathematics, and what are the economic implications of implementing green power systems?	Solar panels are used in mathematics education to teach various concepts including measurement from scale drawings, calculation of composite shape areas, and understanding of ratio and proportion. Students work with real-world examples like the INEOS TEAM UK's solar panel array to learn these mathematical concepts. Regarding economic aspects, green power currently has higher costs because it's a relatively new technology with more expensive construction and operation costs. However, these costs are expected to decrease as technology and usage increase. Green power is typically paid for through additional monthly premiums, with residential customers paying $3.50 for 300 kWh blocks and commercial customers paying $17.50-$35.00 for larger blocks.	['Solar panels: Modelling and data analysis – Lesson 1\nLearning about the effectiveness of renewable energy – solar panels:\n- Demonstrate how to take measurements from scale drawings.\n- Solve a problem using measurements from a scale drawing.\n- Calculate areas of complex shapes.\n- Analyse your solution to understand its impact.\nThe lesson is aimed at the knowledge and skills requirements for KS3 and KS4 Mathematics.\nThis maths topic uses the real-life context of the British America’s Cup team INEOS TEAM UK. Students will explore the modelling and data analysis behind INEOS TEAM UK’s team’s solar panel array during the 35th America’s Cup. Over two lessons, they will use the blueprint of the base’s roof to design the array, calculate area and analyse real data to evaluate the energy produced.\nUse the film to discuss the motivations behind the team’s decision to install solar panels, the impact it has and the ways they monitor its efficiency and effectiveness.\nThe class-facing presentations provide clear lessons with differentiated worksheets to reinforce the curriculum-linked learning objectives.\nThis resource can be used either in sequence or flexibly as an introduction to a topic, or for a quick activity.\nThis film introduces the use of solar panels as a renewable, sustainable way of producing electricity. Using Team INEOS UK’s America’s Cup base in Portsmouth, it explores the factors surrounding, choosing and installing solar panels. This includes the modelling required to maximise the area covered by solar panels and the data analysis that is used to understand the technology’s effectiveness and impact.\nKS3 NC Maths:\nRatio, proportion and rates of change.\nModel situations mathematically.\nChange freely between related standard units [for example time, length, area, volume/capacity, mass].\nUse scale factors, scale diagrams and maps.\nDraw, measure and interpret scale drawings.\nCalculate areas of composite shapes.\nUnderstand that a multiplicative relationship between two quantities can be expressed as a ratio or a fraction.\nRelate the language of ratios and the associated calculations to the arithmetic of fractions and to linear functions.\nGeometry and measures.\nCalculate and solve problems involving: perimeters of 2-D shapes and composite shapes.\nSolve problems: begin to model situations mathematically and express the results using a range of formal mathematical representations.\nKS4 NC Maths:\nCompare lengths, areas and volumes using ratio notation and/or scale factors; make links to similarity.\nCalculate and solve problems involving areas of composite shapes.\nConvert between related compound units.\nInterpret and use fractional (and negative) scale factors for enlargements.\nCalculate surface areas.\nR1 Change freely between related standard units.\nR2 Use scale factors, scale diagrams and maps.\nR1 Change freely between related standard units (e.g. time, length) and compound units (e.g. speed) in numerical and algebraic contexts.\nR2 use scale factors, scale diagrams and maps.\nG14 use standard units of measure and related concepts (length, area, volume/capacity, mass, time, money, etc.).\n10.01c Use the scale of a map, and work with bearings. Construct and interpret scale drawings.\nUse ratio and proportion to calculate quantities, including cases where the ‘total’ is not given.\nThird Level Maths – Curriculum for Excellence:\nScale drawings: word problems.\nUse a formula to calculate the area of a shape.\nRecognise the need for scales and when to use them.\nIdentify suitable scales and scale ratios for plans and maps.\nUse scales and scale ratios to read a map and calculate actual distances.\nUnderstand terms such as plan, scale and ratio.', 'Learn About Green Power\nWhat is Green Power?\nGreen Power is energy produced by renewable sources, like wind and sun, that are replenished naturally. Renewable energy is a clean source of energy that has a much lower environmental impact than conventional energy technologies.\nWhy is Green Power important?\nBy choosing Green Power, you can support development of renewable energy sources, which can reduce the burning of fossil fuels, such as coal, oil, and natural gas. Greater reliance on renewable sources also provides economic benefits and can improve our national energy security.\nWhere does Green Power come from?\nLower Valley Energy obtains most of its green power from the Horse Butte Wind farm, just south of Idaho Falls, Idaho. In addition, Lower Valley Energy also produces green power from its two low-impact hydro facilities at Strawberry and Swift Creek projects (located in Bedford, WY and Afton, WY respectively). Energy produced from these projects is certified as Environmentally Preferred Power, a rating endorsed by national and northwestern region environmental organizations.\nHow will this affect my electric bill?\nGreen Power is paid for by an extra monthly premium, which shows up on your bill in addition to regular monthly charges. As a residential customer you can purchase blocks of 300 kilowatt hours (kWhs) for $3.50 extra per month. As a commercial customer you can choose between 1,500 kWhs/$17.50 per month blocks (Partner Level) or 3,000 kWhs/$35.00 per month blocks (Champion Level).\n100% Green Power is a special rate of 6.8 cents/kWh (versus 5.6 cents/kWh for normal residential electricity).\nWhy does Green Power cost more?\nGreen Power, or renewable energy, is a relatively new concept. Companies have just recently begun to create power from these sources of energy. As a modern technology, these types of facilities are more expensive to construct and operate. As technology increases and usage increases, it is likely that overall costs will decrease.\nHow do I know I’m Getting Green Power?\nPower generators put power into the energy pool and power grid as energy customers take it out. Wind-generated Green Power enters the power pool that is already being fed by other power resources like natural gas, oil and coal. By supporting Green Power you are investing in the further development of a clean power resource.\nHow does Green Power benefit our local communities?\nChoosing renewable power reduces the impact on our water sources, the air we breathe, wildlife, and the land around us. Also, the more customers participating in Green Power will help ensure that our communities will benefit from renewable energy sources long into the future.\nWhat effects could wind turbines have on migratory birds?\nThe technology associated with modern wind turbines has improved greatly, significantly reducing bird mortality at wind plants. Today’s wind facilities are specifically located in areas where there is not an updraft. Tubular towers of modern wind turbines ensure that birds are unable to perch on the towers. Smaller turbines that may still have lattice towers have diagonal supports to discourage perching.\nHow can I sign up for green power?\nThank you for your interest! Visit our signup page to get started.']	['<urn:uuid:043adf69-d821-48a5-8cfe-28c19a8cae89>', '<urn:uuid:5612567e-08e7-4eee-8ea7-3dd4c76e21e2>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T18:05:20.075489	19	106	1083
90	how do they make fake earthquakes demonstration	The LAVA Centre simulates earthquakes using two M-Force Moving Magnet Linear Motor transducers powered by M-Drive amplifiers. These devices are capable of 'moving' a floor measuring 150cm by 300cm in both X/Y axes to imitate earthquakes of 3.7 Richter scale or bigger. The simulation also includes a Danley DTS10 subwoofer under the quake floor and four Audac PX110MK2 speakers in the ceiling to create a realistic earthquake experience.	['A haul of AV equipment including amplifiers from Italian manufacturer Powersoft has been selected for the new LAVA Centre in the Icelandic town of Hvolsvöllur, 106km east of Reykjavik.\nThe LAVA Centre is an interactive, high-tech educational exhibition depicting volcanic activity and earthquakes, and tracing the creation of Iceland over millions of years. Its location was chosen because it sits at the heart of some of the major Icelandic volcanos and allows five of the country’s most iconic volcanoes to be seen.\nThe equipment itself includes 14 Powersoft amplifiers with DSP onboard to drive the 72ch of zonal ceiling and surface mount loudspeakers, all in one of the four racks in the centralised technical room. Models comprise Powersoft Ottocanali 1204 DSP+ETH, Quattrocanali 4804 DSP+D (for Dante) and Quattrocanali 2404 DSP+D, while the company’s M-Force Magnetic Linear Motor Transducers were selected to create the sonic realism of an earthquake.\nThe design, preparation, integration and finalisation of all AV equipment for the nine main show items took almost a year. Feris ehf was responsible for audio, video, network, CCTV, UPS, system, as well as for the RGB Neopixel LED integration, AV lighting and LED equipment, and entire show control. Powersoft was chosen for its green credentials and reduction in CO2 emissions while Feris realised that in specifying Powersoft multi-channel amplifiers to drive the complex array of loudspeakers, they would achieve a radical reduction in real estate and power.\nFeris’ project designers Jakob Kristinsson and Bergthor Leifsson selected Powersoft’s sonic M-Force to provide the capability of simulating a volcano erupting, with two M-Drive amplifiers powering two 15kW M-Force Moving Magnet Linear Motor transducers to imitate a 3.7 Richter scale earthquake, or bigger. Their purpose is to ‘move’ a floor measuring 150cm by 300cm in both X/Y axes, the first time M-Force has been used for mechanical movement.\nThere are 15 multi-channel audio zones in use at LAVA Centre, located in 10 rooms/spaces, including an outdoor one — variously triggered by MediaMatrix and Presonus sound cards. This includes 80 x Audac speakers, 13 x Audac subwoofers, two Danley subwoofers, three Panphonics flat panels in the ceiling for spot audio, eight OnePointAudio speakers, a 632 x 360cm Stewart Filmscreen Phantom HALR screen coupled with a 7.1 surround system from PC Asus 7.1 sound card, via Peavey MediaMatrix NION n3, for the movie theatre.\nIn the Movie theatre audio is controlled by Peavey MediaMatrix NION n3 via Powersoft DSP +ETH amplification to the 7.1 speaker setup, using OnePointAudio speakers and a Danley TH212I subwoofer.\n“There were several challenges in the design and specifying amplifiers and selecting the right speakers,” said Kristinsson. “Powersoft amplifiers had already been installed when the audio files started to be available. This dilemma we solved by choosing carefully ‘big enough’ speakers and amplifiers. Mostly audio is played out from ‘content’ computers, the same ones that deliver the image signal or the programs for the Neopixel LEDs. The DSP in each amplifier enables the set-up of an audio path directly from source to amplifier to speaker over all audio zones, thus preventing possible single point of failure.\n“And of course the other special aspect of the audio design is for Room 10, Earthquakes,” added Kristinsson. “Under the quake floor is a Danley DTS10 subwoofer, measuring 140cm x 100cm x 45cm, with frequency response capability down to below 10Hz. At the same time, in the ceiling in the corridor are four Audac PX110MK2 speakers. The combined effect of the Powersoft M-Force, the Danley subs and the ceiling speakers give a ‘true’ representation of an earthquake. However, care must be taken not to scare people.”']	['<urn:uuid:470fe168-e1f2-451d-b0df-196ae80a1e91>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T18:05:20.075489	7	68	600
91	st johns river fish population change cold effects	In the St. Johns River, exotic fish populations like sailfin catfish, brown hoplos, and tilapia have fully recovered from the 2010 cold kill. However, severe changes in stream flows can affect fish populations more broadly through direct mortality, reduced spawning success, and habitat loss. Changes can lead to the replacement of unique regional fauna by fish species better adapted to regulated stream environments.	"['Upper St. Johns River and Lakes\nThis 60-mile stretch of the St. Johns River is the southernmost headwaters where the tannin-stained river originates out of the vast marshes, swamps, water management areas and canal systems. The river flows through a number of lakes ranging from 350 to 4,500 acres in size (Lake Hell\'n Blazes, Lake Sawgrass, Lake Washington, Lake Winder and Lake Poinsett). The river is not marked for navigation, flowing within a single channel downstream to Cocoa at which point the river becomes braided into multiple channels as the river flows across the floodplain downstream to Puzzle Lake. Water levels fluctuate around six feet annually between the dry (winter-spring) and wet season (late summer-early fall). These extreme annual water level changes can radically affect the physical dimension of the river, causing a less than 150 foot wide river to expand out over the flood plain and become several miles wide, changing the navigability of the river and the distribution of the fish. Most of the upper St. Johns River and lakes are surrounded by State-owned lands which makes it a very scenic environment for all users.\nThe ""River Returns"" is a high definition television show about the St. Johns River that will air in October on PBS. Meanwhile their web site provides some excellent information about the river.\nWater levels in the upper St. Johns River are pretty low to start the summer. As long as the water is low, the fish are concentrated in deeper bends in the river and navigation is difficult in traditional motorized vessels. The water should gradually increase as the wet season continues. As the flow increases the outlet end of lakes, such as the western end of Lake Poinsett, generally have the best water quality and are the best bet to find fish willing to bite. Traditional methods for taking all species will work. Swimming plastic worms/jerk baits and twitching shallow-running minnow imitations are preferred tactics among many bass anglers here. Fly-fishing with a surface popper is also a very popular and productive way to catch both bass and panfish in the river section between Poinsett and SR 528. One more note on the river; the populations of exotic sailfin catfish, brown hoplos, and tilapia have fully recovered from the cold kill in 2010. These fish are all very abundant if you have any interest in them. Tilapia have occasionally been taken on small spinners and jigs and are fun to catch when they do bite. But tilapia may also be taken by bowfishing and cast netting if you are looking to put some tasty fish in the box by means other than angling. Just remember that you cannot keep game fish that are caught with these alternative fishing gears.\nWork the banks and emergent vegetation in the high water conditions for bass and panfish. Look for bass to be schooling where the river flows into the lakes and near the mouth of drainage canals. Crappie anglers should take fair numbers of fish by slow-trolling artificials or by drifting with live minnows in the deeper, open water areas of Lake Washington and Lake Poinsett and wider part of the river north of Lake Winder.\nTrophyCatch is FWC\'s citizen-science program that rewards anglers for documenting and releasing trophy bass 8 pounds or larger. The following TrophyCatch bass have been submitted from the St. Johns River system:\nLunker Club (8 – 9.9 pounds): 360\nTrophy Club (10 - 12.9 pounds): 69', 'Effects of Altered Stream Flows on Fishery Resources\nA. Issue Definition\nAlteration of the quantity and timing of river or stream flow can significantly affect fisheries resources. The American Fisheries Society (AFS) is alarmed at the loss of natural streams in North America, and greatly concerned with management of fisheries in streams that have been altered. The AFS believes that public trust rights need more recognition and stronger consideration of the management of stream resources.\nSevere, long-term adverse impacts and irreplaceable loss of stream fishes have occurred throughout North America. Some river basins, particularly in the more arid American Southwest, have been so radically altered by dams and diversions that an almost total replacement of native fish faunas has occurred. Many of our historic and valuable riverine fishes are declining in distribution and abundance, and some are threatened with extinction. In addition, adverse affects of water development have resulted in international problems in water supplies, and loss in estuarine productivity. Degradation of stream water quality has, in some instances, made the remaining water questionable for further use.\nThere are few river basins in North America where natural flow regimens have not been altered. Dams built by public and private agencies regulate impounded water supplies by storing and releasing water for power, flood control, irrigation, navigation, municipal and industrial uses, and others. Water diversions further reduce the amount of water available to stream resources, and also return the remainder to a point distant to the intake. Diversions may degrade water quality parameters such as sediment load, temperature, pesticides, herbicides, and others. Regardless of environmental impacts of water diversions or impoundments, these developments are considered “beneficial” by state and provincial law. Conversely, instream flow use of water for fishes is not universally recognized.\nMost agencies accept fishery protection as conditional to awarding water rights. However, quantification of stream flow needs of fishes, and operational and legal protection are not presently adequate. The importance of protecting stream fishery resources from a water rights perspective is little understood by the public at large, and fishing as a recreational use of larger streams and rivers has been increasingly de- emphasized by agencies in deference to reservoirs.\nThe AFS believes that governmental and private sectors do not place sufficient emphasis on protection of instream water for management of stream fisheries resources. In this statement, we stress a need for more careful consideration in planning future stream alterations, discuss potential impacts of alterations, and provide guidelines for future management of stream fisheries.\nB. Impacts of Altered Stream Flows\nChanges in stream flow can affect fishes directly and indirectly. Direct effects of flow alterations are certainly important if migrations are blocked, fish are trapped in dewatered sections, or reproduction is disrupted. However, insidious effects may be far more detrimental, and include alteration and loss of stream habitat, introduction of competing non-native fishes, degradation of water quality, and other effects. As an example, it is difficult to assess fishery impacts associated with a reduction in stream flooding; however, changed nutrient cycles and disruption of food webs may have serious ecosystem consequences.\nDecreased stream flow can contribute to direct mortality if fish eggs are exposed, covered with silt, or left without sufficient, oxygenated water. Reduction in usable habitat can result in decreased abundance, size, and condition of fishes. Water velocities and the amount of appropriate substrate can be so changed that spawning sites become limited, and in some species, an increase in interspecific hybridization may occur. Anadromous or resident species may not move to appropriate spawning sites if attractant flows are lost or stream passage is inadequate.\nReduction in spawning or nursery habitats by stream regulation or diversions can concentrate eggs and young, encouraging increased predation by resident or introduced fishes. Production of essential food organisms, or their availability in occupied habitats, may be reduced for all life stages of fishes. Productivity of riverine systems may be reduced by storage projects that trap nutrients or release water at unfavorable temperatures.\nWater resource developments and operations may affect stream resources both beneficially and adversely. Return flows from irrigation projects may be warmer, sediment laden, and contaminated with chemicals, including biocides and fertilizers. Conversely, return flows into river channels during droughts can provide some beneficial effects. Flows from hydroelectric plants may fluctuate greatly and create unstable environments. Water degraded in temperature or chemical composition can displace, or limit growth in fish populations. However, projects providing planned flows can mitigate and potentially enhance fish populations in some environments.\nEstuaries are also sensitive to alteration and reduction of stream flow. Eggs and larvae of some estuarine fishes cannot tolerate high salinity of the marine environment that may result from reduced freshwater flows. In this case, the quantity and seasonal timing of freshwater inputs are particularly critical to these sensitive stages. Productivity of estuarine food organisms partially depends on the alochthonous material transported by rivers. Reservoirs may act as nutrient traps thereby reducing estuarine productivity.\nInstream flows are a public trust, and stream ecosystems must be protected as irreplaceable resources. Natural stream systems, if properly managed, can provide sport, subsistence, and commercial fisheries at little cost. However, unless stream flows are established, implemented, and protected, the following impacts can be expected to accelerate:\n- Replacement of unique regional fauna by fishes adapted to the more regulated stream environment. This extirpation will result in more listings of endangered species. Stream fishes currently considered as endangered will continue to disappear in nature.\n- Reductions in localized stream flooding will continue to degrade bottomlands and reduce stream productivity, adversely affecting stream fishes.\n- Riparian habitat will continue to be degraded, and degradation will adversely affect stream quality.\n- Reductions of stream flows will reduce and degrade stream habitat, increase summer water temperatures, reduce oxygen, and concentrate pollutants.\n- Fluctuating flows associated with power generation will reduce stream resources by promoting unstable channels. Such flows will alternately scour, and then promote downstream siltation of stream habitats.\n- Loss of spring peak flows below dams will result in perennial armoring of stream bottoms, with downstream effects of wider, shallower channels due to loss of stream power to move sediments. Alteration of natural hydrographs will result in changed species composition.\nC. Needed Actions\n- AFS encourages state and provincial governments to legally identify stream resources, water needs, and to give formal recognition of instream fishery needs as a beneficial use in their water resource program\nDifferent legal doctrines-riparian and appropriation- govern allocation of water and water rights in North America. Riparian rights insure land-owners whose properties are bounded or traversed by streams, the right to certain uses of water, usually including fishing and recreation. However, the appropriation doctrine gives exclusive right to the senior water user in a stream, permitting (and in some instances demanding) complete diversion of water to obtain a water right. Under both the riparian and appropriation doctrines, fisheries resources are often not included as beneficial use. In many locations, it may be difficult to obtain recognition of the needs of instream fishery resources without further legislative action.\nAFS therefore encourages its membership at all levels to become involved in the process of obtaining legal protection of instream flows, and to provide decision makers with compelling reasons why stream fish habitats should receive more consideration. It is up to AFS members and other interested groups to convince legislators that legal recognition is necessary for instream fisheries habitats.\n- AFS Members should promote the identification of fishery opportunities at new and existing water storage projects, and promote an equal consideration of fishery resources. Opportunities for use of a portion of the unallocated stored water in existing projects for enhanced instream fishery flows should be vigorously pursued. New water projects should be planned and operated with instream flow needs for fisheries as one of the project purposes. Past conflicts over water development can be reduced if the fishery profession demands and is included as a full partner in project planning and operation.\n- AFS will promote development of better habitat evaluation procedures, and support research efforts to obtain more comprehensive information about stream habitat requirements of fishes.\nProfessional fishery workers must continue to provide technically sound and feasible options and alternatives for protection and management of the resource. To date, verification or validation of stream flow recommendations has been inadequate. Without follow-up evaluation of stream flow recommendations, fisheries management with respect to instream flows is only a guessing game. AFS members must become involved with determination of instream flows that work to meet an intended purpose, i.e., result in sustaining or increasing the stream resources.\nThe concept of “minimum flows” and other low flow standards based on statistical records instead of biology (whereby it is assumed that needs of stream fishes can be met as long as some water remains) are seriously outdated. Seasonal life history needs of stream fishes must be understood if we are to provide sufficient water at the correct location and at the proper time to provide for reproduction, recruitment, growth, and other life needs. Long-term monitoring of fish populations may be the only viable approach to understanding stochastic and density-dependent factors that influence carrying capacity in dynamic stream environments. Ways of empirically relating instream flows to fish populations must be developed.\n- AFS will promote the formation of national and international programs to evaluate streams ecosystems, with emphasis on conservation of fisheries resources.\nStream ecosystems, particularly riverine and warmwater systems, are complex in biological/physical/chemical interactions, and may traverse many political boundaries. Such systems are difficult to study. Financial support is generally available only for studies of economically important species in restricted areas. Information received from such studies is usually site specific and extremely limited in scope. More emphasis is needed to evaluate present baseline conditions, and to determine future effects of stream alteration. Research and management for conservation of stream resources must occur at the regional river basin level.\n- AFS will encourage governments to adopt a “no net loss” policy for conserving remaining stream ecosystems in North America. It is time to recognize the importance of stream and riverine ecosystems as national assets,\nbringing these systems in perspective as links between wetlands, estuaries, and other environments. Replacement of stream channels with aqueducts, ditches, and pipes, should be discouraged. The concept of “no net loss” or “net gain” has merit, but further discussions are needed to reach a workable definition. One approach would be to place the burden of proof on those proposing development to show that alterations would not be detrimental in the extreme, and to provide for follow-up evaluations and means for mitigation. Better understanding of functional relationships between stream habitats and fish life histories would provide for further refinement of management alternatives.\nO.E. Maughan, C.B. Stalnaker, and members of the Instream Habitats Subcommittee improved an earlier draft of the manuscript.']"	['<urn:uuid:4765c1a7-5ba1-4400-a755-cc6bd96c13be>', '<urn:uuid:100529d6-9d29-41fb-8185-ff3c8ace7a6f>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T18:05:20.075489	8	63	2374
92	I research energy transitions. How do Ottoman-era reforms and EV charging policies affect social inequality?	The Ottoman-era reforms under the Young Turks resulted in impetuous policies that weakened the empire, similar to how current EV and charging policies have created social inequities. While wealthy early adopters benefit from EV incentives and home charging capabilities, lower-income groups are often excluded - surveys show 95% of EV adopters belong to high or mid-high income households. This creates a divide where wealthy users can access cost savings from smart charging and V2G technologies, while disadvantaged communities face increased transportation burdens.	['Iraq and the Pathologies of Primacy\nThe Flawed Logic That Produced the War Is Alive and Well\nAhmet Davutoglu, Turkey’s new prime minister, started his career as a professor of internattional relations in the 1990s. By 2003, he had worked his way into becoming an influential -- yet still relatively unknown -- advisor to Turkish Foreign Minister Abdullah Gul. When I met him in his small office in an old government building in downtown Ankara in 2005, he struck me as a scholar with deep knowledge of Ottoman history and a strong desire to transform Turkey into a regional powerhouse. If handed power, it seemed, Davutoglu would turn Turkey’s traditional Western-oriented and inward-looking foreign policy upside down. Eventually, as advisor to Recep Tayyip Erdogan, the prime minister of Turkey at the time, and then as foreign minister, Davutoglu did exactly that. Now, as prime minister, he will need to figure out how to contain the damaging effects of his policies.\nWhen we met in 2005, Davutoglu and I discussed a variety of foreign policy issues, including the role of Islam in Turkish politics and the legacy of Ottoman rule, including the responsibility it entailed regarding the people formally under its rule. I told him about my work in the 1990s, when I had organized international conferences in Ankara to publicize the suffering of the Bosnians. Davutoglu, for his part, emphasized the Middle East, suggesting that Turkey had a responsibility to actively cooperate with the Muslims states in the area. He added that only by reaching out to these Muslims nations and others in the Muslim world could Turkey become a great power. Davutoglu, it dawned on me, was an Ottoman revivalist, keen on eliminating the Kemalist legacy in Turkish foreign policy.\nTurkey’s first president, Mustafa Kemal Ataturk had a mantra: “Go West.” He and his successors, the Kemalists, wanted to turn Turkey into a European country, thinking that doing so would make it a great nation. To accomplish this goal, they needed to redefine the whole of Turkish civilization -- to jettison the Ottoman legacy in the Middle East and disavow the country’s Muslim heritage. In its place, Turkey would embrace a new secular national identity and an inward-looking foreign policy rooted in “non-interference” -- that is, avoiding intimate ties with the region’s states, especially Arab nations. They hoped that, one day, Europe would fully embrace their country.\nDavutoglu is no Kemalist. He is a loyal member of the Justice and Development Party (AKP), the backbone of conservative and Islamist politics in Turkey, which has defined itself in opposition to Kemalism, the movement intent on modernizing Turkey from the top down. Often characterized as authoritarian in nature, Kemalism has in fact built Turkey’s democratic institutions.\nAnd it is this opposition to Kemalism that has made Turkey’s Islamists different from other Islamists in the region. For one, Turkish Islamists dismiss violence because they have grown inside a democratic polity. In addition, other Islamist movements have to look deep into the annals of history for models of Islamic governments in their territories. As a result, they often pursue visions of austerity and obduracy. For instance, the region’s Salafist movements harken back to the seventh century in their medieval values. In Turkey, however, Islamists need only look back to pre-Ataturk times, in other words, to the late nineteenth and early twentieth century Ottoman Empire. At home, this means idealizing the late Ottoman Empire. Turkish Islamists envision the late Ottoman sultans as pious and conservative Muslims; in reality, these statesmen were bon vivants and modern. Abdulmecid Efendi, the last Ottoman caliph, was a master artist who painted nudes. Today, his paintings sell for roughly $1 million at auctions. And abroad, this means idealization of Ottoman statecraft, re-engaging with the Middle East and rebuilding power -- the two tenets of Davutoglu’s foreign policy.\nIn the early years of Davutoglu’s tenure as foreign minister, Turkey did pivot toward the Middle East. He sought rapprochement with Turkey’s Muslim neighbors, including Iran, Iraq, and Syria. He also reached out to the Gulf monarchies, and built good ties with countries as far away as Sudan. He believed that these policies, which he dubbed “Zero Problems with Neighbors,” built Turkish influence in regional capitals and helped establish Turkey as a Middle Eastern power.\nThe Arab Spring, however, soon proved Davutoglu wrong. As protests began to heat up in Syria in 2011, Davutoglu flew to Damascus to advise President Bashar al-Assad to refrain from using violence against the crowds. Only hours after Davutoglu’s departure, however, Assad sent tanks into Syrian cities for the first time, snubbing the Zero Problems policy and Davutoglu. Appalled by the slight and by Assad’s treatment of civilians, the Turkish leader decided to back the uprising, and opened the country’s borders to Syrian refugees and anti-regime rebels, including what would eventually become the Islamic State of Iraq and al-Sham (ISIS), which has declared a caliphate in parts of Iraq and Syria. This decision cost Turkey dearly, and its relations with Damascus and its regional patron, Iran, began to crumble. Things went poorly in Iraq, too, where Davutoglu supported the Kurds because he wanted to import their oil and because he saw them as a potential intermediary with Turkey’s own Kurds and as proxy against the Shia-majority government in Baghdad. This rapprochement irritated the Iraqi government and Iran, also Iraq’s patron.\nThe second tenet of Davutoglu’s doctrine, power revival, draws even more directly from the late Ottoman Empire. Davutoglu’s reference here is to the foreign policy pursued first by Sultan Abdulhamid II in the late nineteenth century and then by his successors, the Young Turks, until 1918, just before the dawn of Kemalism. Although Abdulhamid and the Young Turks opposed each other in power, their foreign policies revolved around a common goal: reviving Ottoman greatness. In other words, Davutoglu’s revivalism is itself rooted in a period of revivalism.\nBy the late nineteenth century, when Abdulhamid came to the throne, the Ottoman Empire was weak. In an attempt to restore its past eminence, Abdulhamid, the opera-watching and rum-drinking caliph, pragmatically employed Islam. He sent emissaries throughout Central Asia and the Indian sub-continent, hoping to inspire their local Muslim populations to rebel against Russian and British rule. His ultimate goal was to build up proxies overseas. This strategy worked up to a point: At the end of World War I, when British forces occupied Istanbul, sub-continent’s Muslims organized a massive fundraising campaign to support the faltering Ottoman Empire and help the caliph. Ironically, the funds ended up in Ataturk’s hands. He used them to buy Soviet weapons to defeat the Allies and abolish the caliphate.\nLikewise, Davutoglu envisions running a country that is powerful not just in the Middle East, but also throughout the Muslim world. Early on as foreign minister, he tried to assume the mantle of the protector of Muslims, from the Philippines and Somalia to Myanmar and Bosnia. Over the past decade, Turkey has emerged as a staunch supporter of aid programs for Muslims everywhere, establishing organizations such as the Turkey International Aid Agency (TIKA), the Turkish version of USAID. This agency, a small outfit that had only 12 offices overseas before the AKP came to power, ballooned under Davutoglu. Today it has 33, at least 22 of which are in Muslim-majority countries, including the Palestinian Authority, Pakistan, and Somalia. Davutoglu cares deeply for Muslims around the world and sees Turkey as their advocate. Turkey also took over the presidency of the Organization of Islamic Cooperation in 2004 for a ten-year period. This was an unusual step for a country with a secular constitution, but it fit well into Davutoglu’s vision of building influence among Muslims in order to revive Turkey’s status a great power.\nFor better or worse, Davutoglu has also borrowed a page from the Young Turks’ book. This group was composed of idealist Ottoman soldiers and bureaucrats who in 1908 overthrew Abdulhamid and declared the Empire a constitutional monarchy. The Young Turks were far more impetuous in their pursuit of Ottoman greatness than their predecessor. Thanks to his cunning, Abdulhamid had avoided drawing Turkey into a war for three decades. But the Young Turks first debilitated their empire in the Balkan Wars in 1912–13, in which the Ottomans suffered an embarrassing defeat at the hands of their former subjects, including in Montenegro and Serbia. And then, in 1914, when Germany offered the Young Turks vast territories in the Russian Empire in return for allying with the Central Powers, the Young Turks eagerly accepted, thus dragging the Empire into battle once again, and on multiple fronts. As a result, the Ottoman Empire collapsed like a house of cards. At one point, the sophomoric Young Turk pashas even deployed ill-equipped Arab recruits, many of whom had never seen snow before, to battle against the Russians on the snow-laden Caucasus plateau: tens of thousands of Syrian troops died from the cold before the Russians could get to them.\nDavutoglu’s policy in Syria bears an eerie resemblance to that of the Young Turks’. Turkey was gracious to open its border to Syrian refugees. To date, the country provides shelter to nearly 1.5 million Syrians, with little international assistance. Yet Davutoglu’s policy in Syria proved feckless. The Turkish leader called for Assad’s ouster before securing military backing from NATO or Arab allies. Apparently, he hoped that providing assistance to the rebels would be enough to trigger Assad’s downfall. It wasn’t, of course. And so as the war dragged on, Turkey started allowing foreign fighters to cross into Syria in the hope that they may be able to do the job. These better-funded and better-armed radicals soon became a dominant faction in the country.\nDavutoglu never intended to assist the radicals. The Turkish leader believed that allowing foreign fighters to cross into Syria was a price worth paying for Assad’s fall. And even if a few bad guys got into Syria, he thought, the good guys would clean them up. But three years later, Assad has not fallen, the good guys are not taking over, and bad guys are building a Taliban-style state that stretches across Turkey’s 800-mile border with Syria and Iraq. They have already targeted Turkey; on June 10, after capturing Mosul, the militant group attacked the Turkish diplomatic mission in the Iraqi city, taking 49 Turkish citizens, including children, hostage. From day one, Turkey has lacked the hard power to back its Syria policy. Just as the Young Turks could not fight on multiple fronts, modern Turkey could not oust Assad nor can it contain the ISIS terrorists spilling out of the fray.\nFOREVER FOREIGN POLICY\nDavutoglu’s Ottoman revivalism has dangerously exposed Turkey to regional threats, which will propbably preoccupy him as he takes over the prime ministership. Indeed, it is likely that because Erdogan knew that foreign policy -- specifically managing the Syria crisis -- would figure heavily in his legacy that he picked Davutoglu as his successor as prime minister. Erdogan and his AKP have won seven elections since 2002 primarily because they have delivered phenomenal economic growth in Turkey. Erdogan has more than doubled average Turkish incomes in a decade. And this economic success has been fuelled by record amounts of international investment -- nearly $50 billion annually. Investors prefer Turkey to its neighbors because it is more stable. But the spillover from the Syrian war -- sectarian conflict and ISIS -- could take all that away. The new Turkish president hopes that his prime minister, who catapulted Turkey into the Middle East to begin with, can now find ways to keep it safe.\nAt this point, it is nearly impossible for Turkey to seal its borders or abandon the Middle East. And it is too late for Ankara to turn back inward in its foreign policy. That leaves Davutoglu with one choice. Turkey is now a part of the Middle East reality and all its turmoil. The only way out of it would be to re-embrace the “Go West” mantra of Ataturk and the Kemalists. Ankara can then work with its real allies, such as Europe, NATO, and the United States, to contain the regional threats and thrive together.\nHow It Will Shape Turkish Foreign Policy\nWhat It Is Like to Oppose Erdogan’s Constitutional Reforms', 'Vehicle-grid integration (VGI) refers to the use of smart technologies to coordinate the charging operations of electric vehicles (EVs) to avoid negative impacts on the electric grid associated with additional electricity demand from EV charging. VGI can also help bring a smoother transition of the power sector toward net zero by leveraging EV batteries as distributed storage assets. Finally, VGI can enable cost savings to drivers and fleets. At the same time, there is a risk that VGI might exacerbate the divide between well-off early EV adopters and lower-income communities. Redressing this imbalance is particularly important because such communities have historically already shouldered a larger transportation cost burden and have been disproportionately affected by the negative externalities of transportation.\nIn this article, the authors explain why—despite government incentives—disadvantaged socioeconomic groups are excluded from VGI’s direct cost savings and other benefits. They go on to discuss the critical need for researchers and analysts to model the distributional impacts of VGI on different socioeconomic groups to help inform policies for advancing energy and transport justice.\nVGI technologies have been developed to overcome major grid impacts from EV deployment by exploiting the load flexibility and storage potential of EV batteries when EVs are parked. In particular, “smart charging” refers to systems that control EV charging in response to price signals and other information about the state of the grid. The term “vehicle-to-grid” (V2G) is used when both charger and EV have bidirectional capabilities and control systems, allowing them not only to import electricity from the grid but also to export (i.e., sell) it back. Often “V2G” is also used more generally to indicate any situation in which electricity is not only exported to the grid but used to partially cover electricity demand from specific local sources such as a home or a building.\nV2G and smart charging could offer whole systems benefits to power systems, e.g., mitigation of the reinforcement cost of power systems infrastructure, reduction of the cost and carbon footprint of keeping electricity demand and supply always in balance, and easier integration of renewable energy sources. At the same time, V2G and smart charging can provide multiple benefits to EV drivers and fleets. These include revenue generation, charging cost reductions, home energy cost reductions, reduction of personal carbon footprint, resilience during power outages, minimization of peak import capacity requirements at fleet depots, reduced battery degradation, and the symbolic value attached to contribution to societal benefits.\nThe International Energy Agency has described the rapid and continuous transition of road transportation toward EVs as a “bright spot of the clean energy transition.” This success will come down to policies that include purchase incentives (e.g., grants, tax credits, or rebates) and recurrent incentives (e.g., toll waivers and parking discounts), as well as fuel economy and pollutant standards, zero-emission vehicle mandates, taxation regimes, and bans on internal combustion engines.\nHowever, by and large, the transition kickstarted by these policies has not been an equitable one, as mostly high-income early adopters have benefitted from EV adoption incentives. A survey of California EV adopters between 2012 and 2017, for example, showed that 95 percent of EV adopters belong to high- or mid-high-income households. In Europe, a 2022 study on EV ownership in Ireland demonstrated a correlation between affluent areas and EV ownership. EVs symbolize luxury in geographical contexts as different as China and the Nordic regions in Europe. The uptake of EVs mostly among higher-income households shows that demand-side policies for EV adoption have benefited mostly the well off.\nPurchase incentives, often not accessible to or insufficient for lower-income families, are sometimes mirrored by stick approaches punishing the use of internal combustion (IC) cars (e.g., zero-emissions zones). As a result, the experiences of high- and low-income groups can be very different. Higher-income groups are able to tap both on purchase incentives and reduced operational costs of EVs compared to IC vehicles. Conversely, lower-income groups that are unable to afford expensive EVs are likely to be further penalized for using polluting vehicles when options to use other transportation modes are unavailable.\nEven when low-income consumers might be able to access EVs, there are no clear pathways for them to exploit their EVs as VGI assets, unlike homeowners with the ability to install chargers at their homes. Consider, for example, V2G-enabled energy arbitrage—charging an EV when the price of electricity is low and selling it back to the grid when the price is high. A recent UK study has shown that this could reduce by 28–67 percent the average price paid for charging by EV owners who own a bidirectional charger under wholesale-following time of day tariffs, compared to drivers subject to a flat tariff. However, such price reductions can only benefit those who not only own an EV but also have access to home charging—typically wealthier consumers with garages or driveways where they park their cars at home.\nHome charging station. Photo by Mario Roberto Durán Ortiz via Wikimedia Commons.\nIn principle, some cost reductions could be attained by EV drivers without access to home charging if shares of the cost reduction from smart charging at the workplace or public charging facilities are passed onto customers. However, most smart charging and V2G demonstrators’ projects have focused on residential customers and fleets with charging facilities at their premises. Business models for VGI at public charging stations or at the workplace require more research and development, as the benefit sharing between stakeholders is less straightforward.\nMoreover, as the direct economic benefits to drivers from VGI come from the energy arbitrage described above and are attained under time-dependent electricity prices, groups with more flexible activity-travel patterns, typically high-income earners, are likely to have more opportunities to choose when and where to charge. In contrast, socially disadvantaged groups, with less flexibility, may get locked into more costly charging arrangements.\nThe clean energy transition in road transport is in full swing thanks to government stimuli that have so far given greater benefits to wealthier drivers. It is critical to redress this initial imbalance as well as address the risk of VGI further exacerbating the inequality. Governments are adjusting their strategies, as exemplified by the US National Blueprint for Transportation Decarbonization, which puts equity and access among the guiding principles for the clean energy transition. Building out the evidence base could help translate these guiding principles into policy and practice, and may require addressing some specific questions, including:\nQuantitative approaches can help address such questions. While the energy justice literature has discussed the potential social implications of VGI, it has also singled out “social justice concerns” as a “notable gap” in VGI research. Current research, rooted in the transportation planning and energy planning disciplines, has focused on quantitative analyses of purely techno-economic aspects. In fact, the lack of agreed-upon quantitative metrics for equity impacts may hinder wider application of energy and transport justice analyses. On the other hand, development of quantitative metrics may be criticized for being reductionist and ignoring complex multidimensional interactions. A more effective integration between quantitative engineering approaches and qualitative approaches dominating the energy justice literature may help to devise quantitative metrics that address core energy and transport justice issues associated with transportation electrification and VGI.\n The VGI literature also refers to these systems as “vehicle-to-home” (V2H) and “vehicle-to-building” (V2B).\n Marko Aunedi and Goran Strbac, “Whole-System Benefits of Vehicle-To-Grid Services from Electric Vehicle Fleets,” Fifteenth International Conference on Ecological Vehicles and Renewable Energies, 2020, https://ieeexplore.ieee.org/document/9243032.\n Cenex, “A Fresh Look at V2G Value Propositions,” 2020, https://www.cenex.co.uk/app/uploads/2020/06/Fresh-Look-at-V2G-Value-Propositions.pdf.\n Scott Hardman et al., “Driving the Market for Plug-in Vehicles: Understanding Financial Purchase Incentives,” Plug-In Hybrid & Electric Vehicle Research Center, Institute of Transportation Studies, UC Davis, March 2018, https://ucdavis.app.box.com/v/purchase-incentives-guide.; Scott Hardman et al., “Driving the Market for Plug-in Vehicles: Understanding Reoccurring Incentives,” Plug-In Hybrid & Electric Vehicle Research Center, Institute of Transportation Studies, UC Davis, March 2018, https://ucdavis.app.box.com/v/reoccurring-incentives-guide; International Energy Agency, Global EV Outlook 2023, April 23, 2023, https://www.iea.org/reports/global-ev-outlook-2023.\n Jae Hyun Lee, Scott J. Hardman, and Gil Tal, “Who Is Buying Electric Vehicles in California? Characterising Early Adopter Heterogeneity and Forecasting Market Diffusion,” Energy Research & Social Science 55 (September 2019): 218–226, https://doi.org/10.1016/j.erss.2019.05.011.\n Brian Caulfield, Dylan Furszyfer, Agnieszka Stefaniec, and Aoife Foley, “Measuring the Equity Impacts of Government Subsidies for Electric Vehicles, Energy 248 (June 2022): 123588, https://doi.org/10.1016/j.energy.2022.123588.\n David Tyfield, Dennis Zuev, Ping Li, and John Urry, “Low Carbon Innovation in Chinese Urban Mobility: Prospects, Politics and Practices,” China Low Carbon Reports, November 2014, http://dx.doi.org/10.13140/RG.2.2.22865.48489; Lance Noel, Benjamin K. Sovacool, Johannes Kester, and Gerardo Zarazua de Rubens, “Conspicuous Diffusion: Theorizing how Status Drives Innovation in eCectric Mobility,” Environmental Innovation and Societal Transitions 31 (June 2019): 154–169, https://doi.org/10.1016/j.eist.2018.11.007.\n Tamara L. Sheldon, “Evaluating Electric Vehicle Policy Effectiveness and Equity,” Annual Review of Resource Economics 14 (October 2022), https://doi.org/10.1146/annurev-resource-111820-022834.\n James Dixon, Waqquas Bukhsh, Keith Bell, and Christian Brand, “Vehicle to Grid: Driver Plug-In Patterns, their Impact on the Cost and Carbon of Charging, and Implications for System Flexibility,” ETransportation 13 (August 2022): 100180, https://doi.org/10.1016/j.etran.2022.100180.\n Nathaniel Tucker and Mahnoosh Alizadeh, “Online Pricing Mechanisms for Electric Vehicle Management at Workplace Charging Facilities,” 2018 56th Annual Allerton Conference on Communication, Control, and Computing, 2018, 351–358, https://ieeexplore.ieee.org/document/8635950.\n Filipe Gonzalez Venegas, Marc Petit, and Yannick Perez, “Active Integration of Electric Vehicles into Distribution Grids: Barriers and Frameworks for Flexibility Services,” Renewable and Sustainable Energy Reviews 145 (July 2021): 111060, https://doi.org/10.1016/j.rser.2021.111060.\n US Department of Energy, “The US National Blueprint for Transportation Decarbonization,” https://www.energy.gov/sites/default/files/2023-01/the-us-national-blueprint-for-transportation-decarbonization.pdf.\n Benjamin Sovacool, Lance Noel, Jonn Axsen, and Willett Kempton, “The Neglected Social Dimensions to a Vehicle-to-Grid (V2G) Transition: A Critical and Systematic Review,” Environmental Research Letters 13 (January 2018): 013001, http://dx.doi.org/10.1088/1748-9326/aa9c6d.']	['<urn:uuid:dd9a9601-d706-4950-9424-472fbfbb17e5>', '<urn:uuid:dff66135-0945-47c2-967f-ee1f8f574983>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T18:05:20.075489	15	82	3632
93	How does the capacity of new-generation portable electrical equipment compare to standard models when it comes to powering household items during emergencies?	Inverter generators are less powerful than conventional generators for major power needs. While conventional generators can power an entire house during a power outage, inverter generators can only power one large appliance or a few sensitive electronics. However, inverter generators excel at safely powering and charging sensitive devices like smartphones, tablets, and laptops due to their stable power output. They're ideal for camping trips, caravanning, and emergency situations where maintaining phone power is crucial, but they're not designed to handle major jobs like running power tools or industrial appliances that conventional generators can manage.	['Generally speaking, the generator is a useful machine that provides a powerful source of energy. These machines have been around for more than one hundred years. Fortunately, the generator has gone through its own evolution of sorts, now available in a lighter, compact design that still packs the type of power the generator is known for.\nModern generators are also much quieter than they were just five or ten years ago. These newer models of generators are described as whisper quiet and can be used where loud, conventional generators cannot. But what is a quiet generator DB levels? Most inverter generators already have a reputation as the leading choice for RV’ing and camping, mainly because they’re much easier to haul and transport compared to conventional generators, and they’re more fuel efficient. But aside from the fact that these generators are cheaper to run and easier to use, they’re also considered the quietest generators around. If you plan on using a generator in a public space, then you need a model that’s designed for whisper-quiet operation. That’s where the camp generator comes in.\nWhat is quiet generator DB levels? Forty-seven up to seventy decibels. The inverter generator is known as the quietest generator on the market, known widely for its quiet operation and its ability to safely charge and power sensitive electronics including laptops and smartphones. Of course, the noise level of the inverter generator can depend on engine speed and what you’re powering or charging. But, ultimately, the inverter generator will provide a much quieter operation compared to conventional generators, which is why they’re such a popular choice for camping and RV’ing.\nA generator’s noise level can depend on many factors, which is why you have to pay close attention to a spec sheet to learn what a machine’s noise level rating is, based on power load. Most models of camp generators still have a much lower than average power rating, even with a full load.\nNow that you know what type of noise rating to look for when you’re shopping for a new generator, read on to learn more about why this generator offers a whisper-quiet operation, what makes the inverter generator so different than conventional generators, and what they’re commonly used for.\nHow Quiet is Quiet?\nWhat is a quiet generator? It’s a generator that you can switch on and still carry a conversation. It’s a model that won’t disrupt your environment, and a model you can use in public and not violate a noise ordinance. If you’ve ever camped at a federal campground, then you’ll notice they have strict rules regarding decibel ratings, which is why you can’t use a conventional generator in this type of environment.\nWith this in mind, many campers have turned to inverter generators as the perfect option for use in public places. But how powerful are these generators really, and what can they be used for?\nGenerators can be used as a back-up power source in the event of an emergency, they can supply power when no other power source is available, and they can be invaluable when you’re traveling across country or camping. These portable generators are often relied upon when disaster strikes. When an inverter generator is on hand, you can have electricity, anywhere, anytime, when it’s needed the most.\nFor years, the biggest complaint surrounding generators was how loud they are. The sound of the loud engine was enough to cause most types of generators to be prohibited from use in certain public spaces such as federal parks.\nFinding a quiet running generator is now easier than ever before, thanks to the rise of the inverter generator.\nA More Reliable Power Source\nHow does the inverter generator measure up to the conventional generator? Are inverter generators better?\nConventional generators have a bad rap for their fluctuating levels of power and power surges. Power surges can be harmful to certain types of devices, such as sensitive electronics. They can actually damage these devices beyond repair. Sensitive devices include smartphones, tablets, and laptops.\nOut of any type of generator, the camp generator provides the most stable power output. This is due to the generator’s inverter technology. Unlike a large, traditional generator, the camp generator is able to create more than three hundred sine waves per engine rotation. The AC power is then converted into DC power, then inverted back to AC power.\nSo, what does that mean exactly?\nThis process means you’ll enjoy consistent, pure power with no power surges. This is why it’s safe to charge and run sensitive electronics such as smartphones and laptops.\nThe quiet-running operation is one of the biggest benefits this type of generator offers. Because of the smaller engine combined with the generator’s compact design, it’s able to run at a much lower volume compared to any other type of generator on the market. The ability to run at varying output levels is another noise reduction feature. When you’re powering up a larger appliance, the power level can be increased. For smaller devices, the power level and engine speed can be decreased. This decrease in power results in a quieter operation. Most inverter generators are able to recognize when more power is needed and can actually increase or even decrease engine speed, automatically.\nThe inverter’s housing should take most of the credit for the generator’s quiet running operation. These generators are made out of sound attenuating, insulated materials that work to absorb most of the noise that’s produced by the generator’s engine. This is why the camp generator is the best power source for use in residential areas and campsites.\nAny generator with automatic voltage regulation technology will run at a low noise level. Most models with this feature will run at the seventy-decibel mark, which is why they’re a popular option for the workplace, or backup power, when running sensitive equipment is required.\nIf you’re familiar with inverter generators, then you’ve heard they’re the quietest type of generator on the market. Some of the leading models will have a decibel rating of just fifty-eight when at the lowest power setting and up to sixty-two to sixty-seven decibels at full rated output. Both decibel ratings are much quieter than the volume at a grocery store or a loud conversation. As an example, speech at a normal volume is around sixty decibels, while the average vacuum has a decibel rating of seventy. On a busy city street, the decibel rating is around eighty to one hundred.\nSo, even at full output, the inverter generator is incredibly quiet.\nThe decibel rating for generators can vary from model to model. You may come across a generator that only emits forty-seven decibels on the lowest setting and another model that has a seventy-five-decibel rating. The amount of noise a generator produces is heavily dependent on its housing design, engine power and even the conditions it’s used in.\nThese generators are currently the first choice for caravanning and camping. In fact, they’re often referred to as camp generators. Along with the fact that they’re compact and lightweight, they allow you to easily power all of your appliances, without causing a disruption on your quiet getaway.\nWhat About Conventional Generators?\nAs you can see, inverter generators are a better option if the noise level is more of a priority than overall power. That’s not to say the inverter generator isn’t powerful, but they’re not designed to handle major jobs like a conventional generator is.\nConventional style generators definitely still serve a purpose and work for a wide range of applications such as running power tools on a construction site or for the use of other types of industrial appliances. However, these generators tend to easily produce over eighty decibels of sound. Yet, they’re cost effective and low maintenance if you need a generator you can rely on to power up an entire house in the event of a power outage, while the inverter generator will only be able to power one large appliance or a few sensitive electronics.\nPower or Noise Rating\nIf you need a generator you can use for major jobs, such as running large equipment or power tools, or as a source for emergency power, then a conventional generator is the best choice.\nThe inverter generator shines in terms of portability and quiet running operation. These are the generators you can use in public places, campsites, federal campgrounds, and so on because they will not cause a disturbance during use like a conventional generator can. However, they aren’t powerful enough to rely on for big jobs. Instead, they’re a better option if you need a generator for use during a camping trip, caravanning, as an emergency power source that can keep your phones charged, or for use for public events when you’re far away from traditional power sources.\nThe Pulsar 2,000W Portable Gas-Powered Quiet Inverter Generator has a noise rating of just fifty-nine decibels. It also weighs just under forty-seven pounds and features a compact suitcase design that makes it easy to transport.\nWhy We Love it:\n- Low noise rating\n- Two thousand peak watts with sixteen hundred running watts\n- Compact suitcase design\n- Eight hours of runtime\n- Parallel capable\nWhat We Didn’t Love:\n- Difficult assembly\nTo learn more about this generator, click the button below.\nWhat is One of the Leading Quietest Inverter Generators on the Market?\nThe Wen PowerPro 56101 Inverter Generator features a sixty-decibel rating at a moderate power level, which is better than average. If you’re looking for a particularly quiet inverter generator, then paying close attention to each model’s DB rating is essential. Of course, you’ll want to learn about the generator’s runtime in addition to the decibel rating at different engine speeds.\nTo learn how to pick out the perfect camp generator for your next trip, click here to read our buyer’s guide.\nHow Noisy is 60 Decibels?\nSixty decibels is pretty quiet and comparable to the noise of an air conditioner, a conversation in a public place, and the noise level in a retail store. The top-selling inverter generators typically have a sixty-decibel noise rating, when they’re running a fifty to eighty percent load. At the lowest setting, many models have a decibel rating of just forty-seven to fifty decibels.\nWhich is Better, Solar Panels or Inverter Generators?\nThe HQST 100 Watts 12 Volts Polycrystalline Solar Panel Off-Grid RV and Boat Kit is one of the best-selling solar panel kits for RVs and produces enough power to run most of your appliances unless you run into two or three consecutive cloudy days. Camp generators, on the other hand, can produce power anytime, anywhere, because their operation isn’t based on weather conditions like solar panels are. However, you cannot run a camp generator in the rain. With this in mind, you may want to consider purchasing both a solar panel kit and an inverter generator for your next RV road trip.\nTo learn more about RV solar panel kits, click here to read our buyer’s guide.\nCan Inverter Generators be Used at Campsites?\nYes, they can! However, make sure you check out the rules of any campsite before you bring your generator along. Most campsites have strict noise rules in place, such as noise should not emit beyond an immediate campsite. Each campsite will also have specific quiet hours, usually ranging from ten at night until six the next morning. The strictest campsites will have specific hours of operation in place for inverter generators, although this time restriction will vary from site to site.\nNow you know why inverter generators are the go-to style of generators for most outdoor events, especially in public places. The generator’s lightweight design allows you to easily transport, set up, and haul the generator, but these machines really shine in terms of noise level.\nWhat is quiet generator DB levels? An inverter generator that has a noise rating ranging from forty-seven to seventy decibels. A generator’s noise level rating can vary from model to model and can also be dependent on what you’re using the generator for. But even the loudest inverter generator is much quieter than a conventional generator. These models utilize inverter technology for cleaner energy, consistent power, and quiet operation.']	['<urn:uuid:c98a5da5-a98a-488d-a582-89244a7da112>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T18:05:20.075489	22	94	2040
94	How did gumboot dancing evolve from a means of communication in South African mines, and what role does it play in current arts development initiatives in Cape Town?	Gumboot dancing originated in South African gold mines where workers, unable to communicate verbally, developed a code by slapping their wellington boots. They transformed this into an expressive art form, incorporating rhythmic movements, boot slapping, and traditional songs in native languages. Today, organizations like iKapa Dance Theatre in Cape Town use this and other dance forms as vehicles for social transformation and youth development, particularly in disadvantaged communities like Gugulethu, Khayelitsha, Philippi, and Crossroads.	"['Edited by Azizi Powell\nThis post presents general informaton about South African gumboot dancing (Isicathulo) and a 2001 review of a stage show entitled Gumboots. The international tour of that show debuted in in 1999 and was directed by Zenzi Mbuli and starred the Rishile Gumboot Dancers of Soweto. Two videos of Gumboots are also included in this post.\nThe content of this post is presented for folkloric, cultural, entertainment and aesthetic purposes.\nAll copyrights remain with their owners.\nThanks to all of the early gumboot dancers. Thanks to all those associated with the Gumboot show that is showcased in this post. Thanks also to all those who are quoted in this post and thanks to the publishers of these videos on YouTube.\nGENERAL INFORMATION ABOUT SOUTH AFRICAN GUMBOOT DANCING\n""The gumboot dance (or Isicathulo) is an African dance that is performed by dancers wearing wellington boots. In South Africa these are more commonly called gumboots....\nGumboot dancers are commonly sighted on the streets and plazas of tourist areas in South Africa, such as the Victoria & Alfred Waterfront in Cape Town... The dance likely originated among South African gold miners, and especially in their tough working conditions—including poor lighting and dampness. Many of the steps and routines are parodies of the officers and guards who controlled the mines and workers\' barracks. Like other forms of African dance, Gumboot utilizes the concepts of polyrhythm and total body articulation, drawing from the cultural dances of the African workers that manned the mines. It is a percussive dance made by idiophones or autophones (objects of the everyday life vibrating by themselves), and is similar in execution and style to forms of ""Stepping"" done by African-American fraternities and sororities""...\nFrom http://www.capetownmagazine.com/whats-the-deal-with/gumboot-dancing/125_22_17289 ""...Gumboot Dancing?..."" By John Scharges\n""A Humble beginning\nGumboot (also known as Wellington boot) dancing originates in the gold mines of South Africa, at the height of the oppressive apartheid pass laws. Due to the extremely poor conditions, mine managers saw the easiest solution to be the outfitting of workers with a uniform consisting of no shirt; a bandana to keep the sweat off the brow, and in order to combat the damp – Gumboots.\nWorkers were often not allowed to communicate with one another, which led to them developing their own sort of Morse code through slapping their gumboot covered feet with their hands. With little or no other freedoms allowed to them, it was not long before the workers developed this into the full fledged expressive art of Gumboot Dancing.\nThe Humble Dance\nLike many African dances, the Gumboot dancer articulates his whole body in performing the moves, often in syncopation with the other members of one’s group. A rhythmical, percussive, almost... ‘stomp’ is the end result – nowadays bells are often attached to the boots for added impact. ..\nThe songs that accompanied the flurried frenetic adaption’s of traditional dances (traditional dances, as with traditional dress, were outlawed) were sung in the workers’ native languages and spoke of the trials present in their work life. Some of the moves were even developed in mock imitation of the way the mine operators themselves moved. Contemporary gumboot dancing has more varied themes, but follow similar paths, if only due to origin.""...\nREVIEWS OF GUMBOOT!\nan SFX Back Row Presentation, Pantages Theatre, Toronto, March 6-18, 2001.\nA Stage Door Guest Review by Christopher Hoile\n...The show, created by director Zenzi Mbuli and the Rishile Gumboot Dancers of Soweto, highlights South African songs and the unusual dance form that developed among black miners in that country. Rather than pumping out the standing water in the gold mines miles below the surface, the mine owners found it cheaper simply to issue the miners with gumboots. Working in the darkness, the miners, sent to the Johannesburg area from their native villages, developed a way of communicating with each other by a system of slaps on their boots. In their ""free"" time, they entertained each other with dances in these boots where this slapping and the jingling of the chain rings added to the rhythm. To these rhythms are added the beautiful harmonies of South African song.\nThe general structure Mbuli has given the show is a progress from the simple to the complex. It starts with nothing visible on stage but two Wellies in a pool of light. Then six men enter in the back of the auditorium and begin an a capella song. By the end of the show the rhythm of gumboot stomping and slapping is augmented by two drummers and a keyboardist--first unseen, then visible--and by four more singer/dancers. The music is organized so as to show a typical day in the lives of mine workers in Johannesburg. It begins with a comic scene of the principal performer, Vincent Ncabashe, teaching the other miners to sing. They then move into songs about work and the ""City of Gold"" they are slaving for to a long central section about life after a day\'s work--thoughts of the women left behind, love songs, an hilarious courting song ""I\'m Too Sexy,"" party songs and drinking songs. Just when the tone seems to have lightened perhaps too much, there\'s a brilliantly evocative onomatopoetic song about the trains that bring the miners to Johannesburg, which in many ways encapsulates the whole show, moving, as it does, from a series of isolated rhythmic sounds to the integration of song and dance in the physical imitation of a train. This is succeeded by the most moving and equivocal scene of the show. Ncabashe tells us that ""for every drop of water a man has lived and died in the mines."" A water-filled square is opened in the floor and Ncabashe sings while dancing in it as water splashes with every step high in the air all over the stage. It amazingly transforms the joyful image from ""Singin\' in the Rain"" into one of pain and tragedy.\nThe 90-minute show covers not just a typical day, but also the history of the mines themselves. The show includes the irony of a song lamenting the closing of the deadly mines because they have now become the displaced miners only source of income for their families. The conclusion is a prayer: ""Keep me strong, give me long life, let me see the sunshine."" We finally come to realize the truth of what Ncabashe had said near the beginning of the show, ""The man who takes the gold took away the sun."" The miners have been exiled not just from their villages but, in working two to three miles underground, also from the light. ""Rishile"" in the group\'s name means ""sunrise.""...\n©2001 Christopher Hoile\nFEATURED VIDEOS OF GUMBOOT!\nVideo #1: gumboots dance story\nCrepusculeindien, Uploaded on Jul 11, 2007\nartist : gumboots dancers, south africa\nplace : the playhouse theatre, london, uk\nVideo #2: GUM BOOTS - Northampton -South African - Dance-Singers\nGary Mabee Uploaded on Dec 3, 2009\nGum Boots South African - Dance - Singers 15-11-99 Filmed at The Derngate Northampton.This dance group were on tour from South Africa for the first time. Camera Gary Mabee\nThanks for visiting pancocojams.\nVisitor comments are welcome', 'iKapa Dance is a Cape Town-based, Non-Profit, contemporary dance company that focuses on artistic excellence as a vehicle for social transformation, youth development & job creation.\niKapa Dance Theatre was founded in 2007 as a Non-Profit organisation by Theo Ndindwa and Tanya Arshamian –two young, passionate dancers who identified a lack of black representation in South Africa’s dance industry. The company was established as a vehicle through which social change in local communities could be facilitated through the upliftment of mental, emotional and physical wellbeing of the youth. iKapa Dance’s broad vision was built around empowerment through dance. This ambition has steered the company into creating productions that are socially relevant and communicate issues affecting all South Africans, with a focus on youth impact.GOALS and OBJECTIVES OF IKAPA’S YOUTH DANCE DEVELOPMENT & DANCE SPACE\n- Transformation: Uses the arts as a vehicle for social transformation via educational empowerment of South Africa’s disadvantaged youth.\n- Youth Development: Promotes the physical, mental and emotional wellbeing through dedicated dance training programmes taught by industry professionals.\n- Equity & Redress: Provides a platform for culturally diverse high-quality artistic creation and skills-development.\n- Artistic Innovation and Arts Promotion: Fosters and enhances an appreciation of the performing arts through accessible, affordable and relevant locally produced productions for a wide-ranging audience.\n- Preservation and Development in the Arts: Develops cross-cultural learning through relationships with national and international organisations.\n- Job Creation: Training for talented South African youth from all social and racial backgrounds that groom its students for careers in professional dance and arts administration.\niKAPA’S YOUTH DANCE DEVELOPMENT & DANCE SPACE OVERVIEW\niKapa utilises artistic excellence in dance as a mechanism to create socially relevant, publicly engaging, audience centered performance’s that utilise social trends around current issues, in ways that are entertaining, inspiring and installs appreciation for innovative artistic creations and performances.\niKapa uses Dance to positively contribute to the development, redress and upliftment of youths in the communities of Cape Town though artistic training and life skills development that is accessible and affordable to all.\nTo achieve the goals of the project the following activities have been designed:\n- Professional company: creates and stages unique, educational and world-class dance theatre productions. Exposing existing and new audiences throughout South Africa\n- Youth Dance Theatre and Apprenticeship program: platforms for cultural integration and refining technical, performance and vocational employment skills for aspiring young artists.\n- Training and Outreach School: develops and empowers the youth in the previously disadvantaged communities of Gugulethu, Khayelitsha, Philippi and Crossroads.\n- OnPointe Dance Studio: provides commercial adult open dance classes to the wider public. The studio generates income for the organisation and develops a support base for audience development.\n- Transformation of Arts Administration and Arts Management: a platform for employee & organisational development, through skills transfer and training support.\n- iKapa “Out-House sessions”: a platform for a monthly informal gathering at the iKapa Dance Space, inviting the public to a creative and interactive evening. IKapa Dance Theatre and local artists present short performances. These “Out-House” sessions stabilize and consolidate grassroots organizations and take diverse artistic creations to public spaces in communities across Cape Town.\n- iKapa Dance Space: provides a space for artists, choreographies, residencies, collaborations, productions and exhibitions to be created /showcased. The home aims to celebrate new work, innovative art and allow artists from all over the Western Cape to have a platform to strengthen and grow the arts, culture and heritage sector of South Africa. It is the base for iKapa Youth Dance Development activities.\n- Youth Development Centre: an area for all Arts & Culture activities to take place under one umbrella, providing skills and teaching to all involved. This is a space of safety, learning, empowerment and education.']"	['<urn:uuid:f6e6c2b3-40fe-45bb-aba7-9bc4096dfb8c>', '<urn:uuid:00ad0de9-a784-4f14-afd2-ec27cbfc7cec>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T18:05:20.075489	28	74	1814
95	What materials can you use to make stamps for printing artwork?	A variety of materials can be used to create printing plates. Any sturdy materials can be used as a base, and low relief materials can be glued onto it. Some effective options include dried plant materials, carborundum grit mixed with glue, acrylic gesso, tile adhesives, and thick impasto acrylic paint. However, materials should not be thicker than mountboard to ensure even printing, and sharp objects should be avoided as they can damage printing blankets.	['What is a Collagraph?\nCollagraphy (sometimes spelled collography) is a printmaking process in which materials are applied to a rigid substrate (such as cardboard, paperboard, wood or metal). The word is derived from the Greek word koll or kolla, meaning glue, and graph, meaning the activity of drawing. Collagraph plates can be cut into shapes and combined with other techniques like masking and intaglio. In this way different line qualities, tonal and textural effects, embossing and colour mixes can be achieved\nCollagraph techniques were used in:\n- Project 2.1 Abstract Landscapes\n- Assignment 2: Human Condition\n- Assignment 3: Mushrooms\n- Project 5.2 Arcadia Recycled\n- Assignment 5: The Dreaming\nFor ways in which other artists and printmakers have used and innovated see:\nBibliography of sources consulted\n- Grabowski, B. & Flick, B., (2009) Printmaking: A Complete Guide to Materials and processes, London, Lawrence King Publishing.\n- Hartill, B. & Clarke, R., (2005) Collagraphs and mixed media printmaking, London, A&C Black.\n- OCA Course Guide\nMy own notes from the projects\nThe relief plate can be printed to make complex images through combining:\n- relief: the ink sitting on the higher parts of the plate can be textured through use of different materials, inking using monotype techniques and using different rollers\n- intaglio: inking the lower parts of the plate using a poupee and drypoint techniques, burnishing the relief. The relief parts of the plate can also be incised and drawn into with different tools.\n- combination: using different textures, tones or colours. This often also takes advantages of differences in viscosity of ink that can be obtained through using different amounts of extender.\nMany layers can be superimposed using registration techniques. Plates can be cut. Masks and chine colle can be added.\n1) Making the Plate\nThe plate can be made from any sturdy materials, cut to any size. Or different size plates can be combined as long as allowance is made for some degree of embossing around the plate. Sometimes it may be necessary to seal the plate before gluing on objects.\nAny low relief materials can be glued onto the plate. Collagraphs can pick up fine detail so it is not necessary to have high relief. Thin materials like tissue or glassene paper print very well and these can transfer surface texture and shape when printing. Different surfaces absorb in different ways and these differences can be exploited to the full. Smooth surfaces can be burnished to remove ink and give subtle tones.\nShapes and images can be implied through creative use of contrasting textures and surfaces, creating complex narratives.\nSimple bold arrangements of shapes and textures also work well – abstract qualities creating a surface of great interest and visual effects.\n- plant materials will last longer if the leaves etc are dried out and no longer contain moisture.\n- carborundim grit mixed with glue can give a dense mark.\n- glue, acrylic gesso and tile adhesives and even thick impasto acrylic paint can be used and sculpted or drawn into on the plate in low relief.\n- the surface of the plate can be cut or scored into as long as the marks are not too deep from the top parts of the print.\n- no one material should stand too proud or it will stop the area around it from printing. Try not to use anything thicker than a piece of mountboard to give a good even printing across the plate.\n- Do not use anything sharp or it can cut the blankets on the press.\nWhen the glue has dried thoroughly, give 2 coats of varnish or shellac to stop paper sticking to the plate and enable it to be cleaned.\n2) Inking the plate\nCareful inking can produce very delicate and varied texturing and tonal effects using the different plate textures and edges, as in the series of mushroom collagraphs below.\nBefore inking for the first time, to help the plate separate from the paper, rub a small quantity of oil into the surface with cotton wool.\nBecause different materials hold onto ink in different ways, it is useful to mix etching ink with plate oil or extender to loosen the ink, making it easier to wipe and thin the saturated ink colour. 50/50 is good to start with and can then be adjusted after proofing. Different materials will require different proportions and each plate is unique. Different inks and percentages will also give different effects.\nWipe off with tissue/scrim.\nApplying the ink can be done with different implements and stiff relief ink :\n- brushes of different types and sizes can paint and texture the plate much as with monoprint.\n- rollers – soft rollers will push ink further into the plate. If you want to just cover the highest parts use a hard roller. Small rollers can get ink into different areas of colour. Big rollers give a consistent sweep.\n- dollies made up of rolled J-cloths bound with masking tape.\n- cotton buds, particularly for tight corners\n- wipe off/burnish areas with tissue paper, newspaper, scrim or j-cloths give different effects.\n- fingers protected by latex gloves can also be used for painterly effects.\nAfter inking solvents and other texturing media can be used. Lines can also be drawn into the ink and the ink can be smeared and mixed.\n3) Printing the plate\nTypes of paper\nPaper can be of varying types depending on the effect required eg the relative importance and amount of detail of relief and intaglio, extent of embossing and inking techniques used.\n- 250-300 gms printing or watercolour paper is best used damp. This is good for embossing techniques and smearing marks as in Untitled Red 4 and Untitled Cyan 4.\n- Hosho paper works well but do not dampen it – though a dampened cloth can be used next to the paper to activate the ink. The mushroom series above used thin Hosho paper for high detail and delicacy. Untitled Brown uses thick hosho for more embossed and texturing/markmaking in the ink.\n- thinner paper can be used, including tissue paper or newsprint if more vague images are required.\nPreparation of the paper varies depending on the effect desired:\n- for intaglio use damp paper or a damp cloth next to the paper to activate the ink.\n- for relief dry or damp.\nIt is quite difficult to get good handpulled collagraph prints if intaglio is important as the paper will not go down into the incised marks.\nBest results are obtained with an etching press. Experiment with different amounts and types of packing paper. The thicker and softer the packing eg blotting paper, the more it will push the print into the recesses of the plate – and the quicker the plate will flatten and potentially degrade. You can also reverse the blankets to put the softer blanket next to the paper.\n- Putting the paper on top of the plate pushes down for intaglio.\n- Putting the plate on top of the paper gives a clear relief.\nClean the plate\nUse cooking oil but this will always leave an (often interesting) shadow. Different effects can be built up in this way.\nAbstract Embossed collagraph prints\nOCA Course Guide for Project 2.1: Formal Abstract Prints\nChoose your subject can be chosen from any figurative source, a sketch, painting or photograph but it must have a good underlying compositional structure. Look at your subject. In your learning log write down why you chose it and what attracts you to it. This is important information to establish as it may help you when you come to abstract your design from your subject. You may have been drawn to the colours, or the shapes or the formal arrangement of elements.\nSimplify the formal geometry of the image you have chosen so that the proportions and way the shapes interlock are based on the original image. work through a formal process gradually simplifying a theme until you reach a balance of geometric shapes and proportions. You will have to make several studies until you are happy with the final arrangement.\n2 Preparing the Plate\nOnce the geometric shapes have been organised and put together they can be transferred to the card used for the relief collotype block. Each shape should be cut out with as much accuracy as possible so that they will perfectly fit together.\nWhen you have cut out enough pieces begin to construct your block. Starting with the base glue your shapes down using a thin, even layer of glue such as PVA wood glue. Continue to build up your block keeping each shape at a different level. Don’t use more than four layers as the block will be too deep and there is the risk of the paper tearing when you print from it.\nWhen you have finished, sandwich the block between two sheets of blotting paper and some newspaper, place a heavy weight on top and leave it on a hard surface overnight or until dry. It is important the block dries as evenly and flat as possible so don’t be tempted to take it out of the paper until it is totally dry.\nOnce it is dry, coat the bottom and edges with shellac varnish (or similar waterproof varnish). Leave to dry and then varnish the top surface. If the card absorbs the varnish repeat the process until all the surfaces are fully coated with a hard layer of varnish.\nI printed out the final image on A4 paper and stuck it to 3 pieces of back card. And cut out 3 versions on different thicknesses of card. From these pieces I then made 3 alternate plates with different proportions of card on different parts of the image, sticking these onto the backplate paper with PVA glue. I used 3 coast of diamond hard varnish, and pressed the 3 plates for a couple of days.\n3) Preparing the paper\nThe effect of a relief collotype or collagraph print is to combine colour with an embossed image. To achieve this successfully you need a strong, rag based watercolour paper (200gms). Your supplier will be able to help you with more information. Cut your paper to size and immerse it completely in warm water. It will need to soak for at least ten minutes so it is soaked right through and its fibres are softened and pliable. When ready place your paper between two sheets of blotting paper which will absorb the excess surface water without drying out the paper. You need the paper to be evenly damp all the\nway through but not soaking wet on the outside. Several sheets of paper can be prepared at once and sandwiched between blotting paper in readiness for your printing. If they dry out you will need to repeat the soaking process.\nI used Bockingford following the above instructions. But actually found that Japanese Hosho paper with a damp J-cloth on the back also got a good emboss.\n4) Inking the block\nOnce your relief collotype block is dry you are ready to ink it up. You can use water or oil based block printing ink. The oil based ink will give you a longer working time as it will dry much slower than water-based ink. Water-based ink, on the other hand doesn’t need solvents to clean it up, but will dry fairly fast.\nUsing an old toothbrush apply the printing ink in small smooth strokes to cover your block. Alternatively you might like to add the ink with a dabber.\nWhen the block is covered in ink you can wipe away some of the colour to vary its intensity in places and reveal some of the varnished areas. Wiping the ink away will allow the block surface to emboss the paper without printing any colour.\nI used Hawthorne oil-based inks, sometimes with transparent medium. And used toothbrush, small roller and poupee to get different effects. Some of the ink was quite stiff as it had been left for a while. Experimenting with different viscosities, overlaying smooth ink with thinker ink on a roller, and making brushmarks achieved some quite interesting effects. But it was sometimes difficult to predict exactly how they would print, depending also on the printing pressure and dampness of the paper.\n5) Printing the block\nYou can take your print in one impression as all your colours have been applied to the block. As you are aiming to get the colours of your design printed with an embossed impression of the block as well you will need to apply a strong, firm pressure from a soft roller. If you have a small, flat bed press you can use that. Place your inked relief collotype block ink side up on a sheet of blotting paper and lay your dampened printing paper over the top. On this, place another sheet of blotting paper. Secure the corners of the blotting paper to your table top with masking tape to reduce the risk of the papers slipping. Using a firm pressure by putting your weight behind the covered rolling pin roll it across the paper. You will need to achieve this in one go. Alternatively you may hand emboss the print through a clean towel to avoid tearing the paper.\nThe damp paper is forced onto your relief block and will pick up the ink as well as be pushed into the relief of your block. Once rolled, remove the tape from the blotting paper and lift your print.\nIt should be stored flat between two sheets of blotting paper but not weighted as this will flatten your relief.\nOn an etching press I did not need tape. But I found it necessary to swap around the blankets on my etching press to put the soft blanket at the bottom to get the best emboss.\n6) Cleaning the block for the next print\nClean your relief block and carefully dry it. You may repeat your printing process in the same colours or try a new colour scheme. In each case make the printed surface as perfect as you can taking into account the texture of the paper you are using, the quality of the ink and the effect of the relief collage block.\nI made several prints just re-inking the plate. I really like the way the different ink colours and textures mix, and the coloured lines from left-over ink.\nD’arcy Hughes, A. & Vernon-Morris, H., (2008) The Printmaking Bible: the complete guide to materials and techniques, San Francisco: Chronicle Books.\nGrabowski, B. & Flick, B., (2009) Printmaking: A Complete Guide to Materials and processes, London: Lawrence King Publishing.\nHartill, B. & Clarke, R., (2005) Collagraphs and mixed media printmaking, London: A&C Black.\nMajor-George, K., (2011) Collagraph: a journey through texture, UK: Major Impact.\nMartin, J., (1993) The Encyclopedia of Printmaking Techniques, London: Quarto Publishing.\nStobart, J., (2001) Printmaking for Beginners, London: A&C Black.\nWoods, L., (2011) The Printmaking Handbook: Simple techniques and step-be-step projects, London: Search Press.']	['<urn:uuid:d2f943da-f950-4341-99e9-645ac01ae4df>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T18:05:20.075489	11	74	2504
96	early horror film techniques innovations	Early horror films showcased innovative techniques, with directors like Segundo de Chomón using stop-motion and double exposure photography in 'The Invisible Thief' (1909). Musicians enhanced these visual innovations by providing emotional context through music, using specific melodies to underscore different scenes - from thunderous bass for storm clouds to ominous scores for villains.	"['Wednesday, 26 October 2016, 7:30 pm\nCommissioned by Abertoir, this collection of silent horror shorts celebrates the inventive and imaginative in early cinema with a selection of rarely seen films on the lighter side of horror and, forming the programme’s centrepiece, one of the truly dark but little known masterpieces of the avant-garde in silent cinema.\nAll the films will be introduced and accompanied on the piano by Paul Shallcross, presenting his own brand new scores.\nThose Awful Hats\n1909, USA, 3 minutes, (NR)\nDirector – D.W. Griffith\nPossibly the earliest example of a public announcement film and from none other than the director later famous for Birth of a Nation, a film which features some very unexpected happenings in a cinema audience.\nIMDb – Those Awful Hats\nThe Invisible Thief (Le voleur invisible)\n1909, France, 5 minutes, (NR)\nDirector – Segundo de Chomón\nThe darker undertones of H. G. Wells’ story are transformed into a delightful skit on crime with Chomón’s camera techniques exploring every possibility of stop-motion and double exposure photography. This is the earliest recorded film to make use of the renowned author as source material.\nIMDb – The Invisible Thief (Le voleur invisible)\nAn Over-Incubated Baby\n1901, UK, 1 minutes, (NR)\nDirector – Robert W. Paul\nRobert W. Paul was a pioneering engineer of movie cameras and a filmmaker with a keen eye for the fantastic and the bizarre. His films, such as this one and Undressing Extraordinary, are often cited as being some of the earliest manifestations of horror in film. A cautionary tale of what happens if you leave a baby cooking for too long…..\nIMDb – An Over-Incubated Baby\nThe Man with a Rubber Head\n1901, France, 3 minutes, (NR)\nDirector – Georges Méliès\nThe inventor of inventive cinema applies his teeming imagination to his own remarkably distensible skull.\nIMDb – The Man with a Rubber Head\nThe Fall of the House of Usher\n1928, USA, 13 minutes, (NR)\nDirector – James Sibley Watson & Melville Webber\nThis is unusual in being the work of a group of amateur filmakers with both financial and artistic clout. The use of prism photography suffuses the whole film with an eeriness and claustrophobia which lead inexorably to the truly horrific catharsis. In many ways the definitive cinematic version of the Edgar Allan Poe short story.\nIMDb – The Fall of the House of Usher\nThere It Is\n1928, USA, 22 minutes, (Unrated)\nDirector – Charley Bowers\nOne of the now almost forgotten comedians of the silent era Charley Bowers is both actor, animator and inventor in this film. A thinly veiled ghost-house story is the vehicle for some extraordinarily surreal humour, for elaborate sequences of stunning camera effects rarely if ever seen in silent comedy, and for an eye-boggling array of Heath-Robinson-like contraptions which continually thwart the efforts of the two detectives (Charley and his midget assistant) to apprehend the malevolent spirit in the Daffydil Sanatorium.\nIMDb – There It Is\nAUDIENCE SCORE: 95.71%\nAnd the comments as follows\n- Magical. Perfect piano. Timeless and delightful. I felt delighted, amused and entertained\n- Another wonderful selection of silent era films\n- Loved the piano accompaniment\n- A rare treat excellently delivered. Will be back next year. Paul’s enthusiasm makes the evening\n- A real treat to see the films and to hear Paul play along in just magic\n- Very enjoyable. Never knew that there were such interesting early films. Excellently presented\n- Brilliant as usual\n- As always very entertaining, and also informative. Music is inspired. Paul is obviously very passionate about what he does\n- Charming, quirky and uniquely entertaining\n- The music suited the films very well. Very entertaining\n- An atmospheric session of cinematic treats. Live piano music extremely effective.\n- Entertaining Shallcross’s music great and his selection of knowledge of the films is rare\n- Erudite, informative and entertaining introductions. Please let us have more of the genius that is Paul Shallcross\n- I really enjoyed the evening. Info, films and music were fantastic. I’m going home to read some Poe\n- Quirky as ever and very entertaining\n- Wonderfully entertaining and well presented\n- Very informative and some hilarious moments\n- Wonderful glimpse at history, and the music and talk were fantastic. As a film novice and first timer at film club I was very impressed and will definitely be returning\n- Both educational and entertaining. A most enjoyable evening\n- Really enjoyed The Fall of the House of Usher. Inspirationally shot\n- Fabulous. Great music, entertaining presentation, educational and interesting and fun.', 'Silent Film With an Emphasis on Sound\nAugust 29, 2013\nOn August 31st, acclaimed composer and pianist Andrew E. Simpson will perform the world premiere of his original score for William Wellman\'s riveting story of love and tragedy, Wings (144 minutes, silent; 1927). The program is jointly presented by American Art and the National Portrait Gallery. Free tickets available in the G Street lobby at 2:30 p.m.; limit two per person. Alli Jessing, Auditorium Program Coordinator for both museums fills us in on the history of music and silent films.\nSilent films are enjoying a resurgence of popularity in culture today, as advances in preservation and restoration technology have enabled film conservators to restore decaying film stock. Silent films, like other art forms, are often interpreted through symbolism and visual cues in addition to the narrative. These clues can be an expression, a featured object in the frame, a gesture, or through close-up, detailed shots of things the director wanted to make sure we noticed, or even in how a scene is lit. These clues can also come in the form of music.\nBetween the 1890s and the 1920s, silent films were traditionally accompanied by a pianist, organist, or sometimes an entire orchestra right in the theater. They played something called photoplay music: a mix of original, popular, classical and repertory music that matched the tones and events in the film. Musicians also improvised. A theater would have a library of photoplay music, and base their music choices on cue sheets provided by the film studios. This provided the pianist or orchestra with what something like a mix-tape: a compilation of music that suited the emotions and narrative of the story unfolding before us. While some films did have scores written specifically for them, more often than not the scores varied by theater, their accompanists, and their music library. This less-than-consistent approach to film screenings made each one a unique experience. You might see the same film twice, but you\'d never hear it the same way.\nMusic was the essential thread that tied the visuals to the appropriate emotional context: a thunderous bass section might have underscored the significance of dark storm clouds rolling in on the horizon, while a lighter, fanciful melody would set up a laugh when someone took a pie to the face. The introduction of a hero would be signaled with a rising tide of chimes, while the arrival of a villain (who may or may not be there to tie a wide eyed maiden to the railroad tracks) would be accompanied with a darker, ominous score. With the advent of synchronous sound and the development of amplification equipment, these live ""cinema-concerts"" soon became a thing of the past. But they live again at the Smithsonian American Art Museum and the National Portrait Gallery!\nThis Saturday, August 31st, the Smithsonian American Art Museum and National Portrait Gallery will be hosting a special cineconcert screening of Wings, the silent film classic starring Clara Bow that also features an early appearance by silver screen legend Gary Cooper. This story of two friends who fall in love with the same woman, but must leave her behind when they enlist as combat pilots during World War I, won the very first Academy Award® for Best Picture in 1927 (the only other silent film to achieve this honor was The Artist in 2011). Despite this honor, the film was considered lost for a number of years. When a complete print was finally discovered, it was duplicated and rereleased. The film has enjoyed a resurgence of interest of late, and has recently been restored.\nThe original version of Wings featured an orchestral score by J. S. Zamecnik. At our special screening, however, we are pleased to feature acclaimed pianist and composer Andrew E. Simpson, who will premier his new original score for the film. Simpson is co-founder of the Snark Ensemble, a group devoted to creating and performing new scores for silent film, theater, and dance. He is also the Resident Film Accompanist for the National Gallery of Art and House Accompanist for the Library of Congress\' Mt. Pony Theater. He has performed his acclaimed original film scores locally at the Kennedy Center in Washington, D.C. and the AFI Silver Theater in Silver Spring, nationally at the New York Public Library at Lincoln Center and the J. Paul Getty Villa in Los Angeles, and internationally at the Giornate del Cinema Muto in Pordenone, Italy and the Sala Cecelia Meireles in Rio de Janeiro, Brazil. Simpons is professor and head of the division of Theory and Composition at the Benjamin T. Rome School of Music of The Catholic University of America in Washington, D.C.\nThe comments to this entry are closed.']"	['<urn:uuid:38395ee6-ad97-48af-8cf7-9f469cb1dfc0>', '<urn:uuid:9173b8d0-0f81-4e16-bce0-cd29ea9f363c>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T18:05:20.075489	5	53	1551
97	How are lawmakers trying to increase access to Career and Technical Education opportunities through the recently passed HB 1661?	Legislators passed HB 1661 which aims to promote access to Career and Technical Education through calendar alignment, embedded credits, and transportation.	['Faith Lanzillo was not surprised by the shrugs and blank stares she received from her middle school audience when she asked what they knew about CTE (career and technical education). At their age, her reaction would have been the same.\n“In middle school, I didn’t know there was CTE,” said Lanzillo, one of six career and technical education “ambassadors” who shared their stories with students at the Daisy Bronson Middle School in Littleton in May.\nNow an emergency medical technician and college student, Lanzillo credits her high school’s CTE program with setting her on a solid career path. “It was a big game changer for me,” she said. “I feel like it’s important for kids to know it exists.”\nThe event was the first in a series of grant-funded CTE alumni events designed to introduce young people to CTE at an earlier age, expanding awareness and affording them more time to explore all options as they prepare for high school and life thereafter. A second event was held at the Sugar River Valley Regional Technical Center in Claremont in July, and a final event is scheduled for September in Hudson. The events provide a blueprint for other CTE centers to follow as well.\n“It’s just giving students a little bit of, ‘Hey, this is what we’re doing. This is what we’re about,” said Stephanie Gray-Lemay, an Education Consultant for the Bureau of Career Development at the NH Department of Education.\nAs small groups of middle schoolers rotated among them in five-minute intervals, the alumni described their varying experiences in CTE programs.\nIn contrast with Lanzillo, whose career pathway proceeded in a straight line from a CTE health science program to a career as an EMT, Kuba Kubkowski took more of a zig-zagging path. After graduating from the health science program at the Hugh J. Gallen Career and Technical Center in Littleton, he went on to nursing school. But partway through, he discovered a talent for sales and ended up getting a degree in business management instead. Starting on a career pathway while in high school helped him home in on his real passion earlier, saving him time and money. But, he told the students, thinking about career options as early as middle school can give you even more time to explore, without locking you in.\n“I really encourage you guys to try as many things as you can,” said Kubkowski, who now owns two businesses in town. “You never know which path you’re going down or what opportunity might come up.”\nParticipation in CTE programs around the state has been steadily climbing for the past five years, according to data provided by the Department of Education. In 2022, approximately 10,000 students have participated in CTE programs in New Hampshire.\nProviding greater access to the opportunities available through career and technical education has become a priority for lawmakers and educational leaders of late. Legislators recently passed HB 1661, which aims to promote access through calendar alignment, embedded credits, and transportation.\nThe legislation is part of a broader movement to construct a career pathway network for students. Experts say these pathway systems need to include guidance from mentors such as these recent alums to help students build their career navigation skills. Research also shows a strong link between middle school career exploration and career outcomes.\nGarrick Berry, who attended the White Mountains Regional Culinary Program in Whitefield, was in middle school when he discovered his passion. “When I was your age, I realized I loved to cook. I loved to make people happy,” Berry, who now works as a line cook at the Littleton Freehouse, told the students. “As soon as I became a freshman I took as many culinary classes as I could.”\nA lot of students miss out on such experiences because CTE simply isn’t on their radar, said Gray-Lemay, a Littleton native and former CTE teacher.\n“Even the kids in our own facility didn’t realize there were programs there,” Gray-Lemay said. “If we can get the word out to more students, that will be amazing.”\nFollow us on Facebook and Twitter and sign up for our newsletter to stay informed about current updates. And, join the New Hampshire Education Network (NHEN), our network of New Hampshire parents, educators, business leaders, and community members who are interested in education policy!']	['<urn:uuid:24d9720a-ffb4-4431-93b6-952a16a524b3>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T18:05:20.075489	19	21	720
98	Are patient rights at hospitals different from health centers?	No, patient rights are largely the same across healthcare facilities. Both hospitals and health centers guarantee patients the rights to privacy, respect, dignity, and participation in healthcare decisions. Both types of facilities ensure patients' rights to receive treatment without discrimination, to have complete information about their diagnosis and treatment, and to express grievances. They also protect patients' rights under HIPAA privacy rules, including the right to inspect and request copies of medical information.	"['Your successful medical care requires on ongoing partnership between you and your healthcare providers. All parties must take an active role. Your providers have the responsibility to provide health care services to the best of their abilities. Patients have responsibilities too.\nYou, the Patient, Have the Responsibility\n- To provide a complete medical history including past illnesses, hospitalizations, family history of illnesses, allergies, medications, including herbals and over-the-counter medications and any matters related to your present state of health.\n- To communicate openly with your health care team including your physicians and nurses, to express your concerns clearly and participate in decisions about your diagnostic and treatment options and recommendations\n- To report whether or not you understand your health status and treatment options, ask questions, request information and comply with the agreed upon treatment protocol.\n- To help your health care team develop a pain management plan, including discussion of available options for your pain relief, asking what you should expect, help in the measurement of your pain, and tell when your pain first begins and if it does not subside.\n- For your actions if you refuse treatment or do not follow your provider’s instructions.\n- To refrain from taking any medications, drugs, or alcoholic beverages unless ordered by your physician. Smoking is prohibited in the facility.\n- To report promptly to your providers and nurses any unexpected problems or changes in your medical condition.\n- Discuss end-of-life decisions with your physicians and make your wishes known; possibly writing an advance directive.\n- You may provide for a duly authorized family member or friend to make medical decisions on your behalf in the event you become unable to communicate.\n- To discuss organ donation with your physicians and, if donation is desired, make applicable provisions.\n- To meet your financial obligations with regard to your medical care or to discuss financial hardships with your physicians.\n- To be committed to health maintenance through health-enhancing behavior. Illness can often be prevented by a healthy lifestyle, and you should take personal responsibility when you are able to avert the development of disease.\n- To respect the rights of fellow patients, and hospital personnel, follow rules and regulations affecting patient care and treatment. This includes the responsibility of respecting the privacy of other patients and treating information concerning them as confidential.\n- To understand the effects of your conduct on others and refrain from behavior that unreasonably places the health of others at risk. You should inquire as to the means and likelihood of infectious disease transmission and act upon that information which can best prevent further transmission.\n- To respect the property of CRMC, fellow patients and other facility staff, to assist in noise control and number of visitors.\n- To report on time for scheduled procedures and to keep all healthcare appointments. If you are unable to do so for any reason, notify your provider to cancel and reschedule the appointment.\n- To not initiate or participate in fraudulent health care and to report illegal or unethical behavior by physicians and other providers to the appropriate medical societies, licensing boards, or law enforcement authorities.\nPARENTS OF PEDIATRIC PATIENTS\nWe encourage someone to stay with your child (there may be some restrictions due to the size of the room and space needed to care for your child). However you will always be able to have one parent or designee stay with your child. Your involvement in your child’s care is very important to your child’s comfort and security. If you can’t be here, try to have a family and/or friends spend time with your child. While your child is hospitalized, we encourage you to participate with meal planning so that your child is given food to meet his wants and needs. Please know that we will encourage you to take an active part in discharge and follow up care. It is important for your child’s safety that the crib rails remain elevated at all times. If you are with your child and are leaving, you MUST inform the nursing staff prior to leaving the unit. We encourage parents to make sure that they have a rest period and time to themselves.\nPlease know that:\n- Children at different developmental levels react differently to hospitalization. Knowing the behaviors that are typical for your child at various ages makes it easier for you to address them. It’s important for you to know how the stresses of illness and hospitalization affect your child’s feelings and behaviors.\n- It is important to be honest with children, especially if something is going to hurt.\n- Children 3 years of age and younger generally cannot understand their illness, the new change in their familiar environment and are more concerned with being away from family and home. Toddlers may have difficulty coping with these changes in their routines, especially related to eating, sleeping and toileting. You can help your child by being with them in the hospital. If you can’t be here, try to have family or friends spend time with your child.\n- Children 3 – 6 years of age usually view the hospital and procedures as punishment. Reassure your child that he/she hasn’t done anything wrong. They can also fear a loss of control and fear of the unknown. It will be reassuring to your child to give simple, honest, and developmentally appropriate explanations. Bring a favorite toy; we encourage you to bring blankets or clothes from home which may also be comforting. If your child has a ‘security’ blanket or ‘lovey’ that will serve to calm them as well please bring it along. Please label or mark personal items so staff can assist retrieving if an item is misplaced. Encourage walking around or playing outside his/her room.\n- Children 7 – 12 years old are often worried about painful procedures and may also believe that hospitalization and procedures are punishment. This age group fears a loss of control and independence. It is very import to provide information to this age group. Whenever possible, ask staff to inform your child in advance of what is going to happen (procedures, changes in environment, etc.) Giving your child choices when able is important to regain some control.\n- Adolescents age 12 and up are very concerned about loss of control, fear separation from their friends, and may be very self-conscious. It is important to respect their privacy whenever possible. Encourage adolescent to ask questions and include him/her in discussions and decision making. Support interactions with friends via phone, email and visits.\n- Parents and caregivers will find this a stressful time not only for the child, but also the family. It is important for parents and caregivers to make sure to meet their own needs. Switching off time with the child, taking a break from the room or unit, and communicating stresses or frustrations with your child’s team of healthcare providers will assist you in best supporting your child.\nPatients’ Bill of Rights & Responsbilities\nIt is the policy of Catskill Regional Medical Center to admit, retain and care for all patients without regard to race, color, religion, sex, national origin, cultural differences, spiritual beliefs, disability, psychosocial, personal values, translation needs, gender identity or expression, sexual orientation and/or preferences, source of payment, or age.\nAll hospital employees and Medical staff are responsible for ensuring that the patient’s rights are respected and enforced. The patient has the right to have cultural, psychosocial, spiritual, personal values, beliefs, translation needs, gender identity or expression and sexual orientation and/or preferences respected.\nAll patients have the right to receive treatment without discrimination as to race, color, religion, sex, national origin, cultural differences, spiritual beliefs, disability, psychosocial, personal values, translation needs, gender identity or expression, sexual orientation and/or preferences, source of payment, or age.\nHEALTHCARE PROVIDERS WILL…\n- Have written policies on patient rights.\n- Inform the patient of his or her rights.\n- Treat the patient in a dignified and respectful manner.\n- Respect the patient’s right to, and need for, effective communication.\n- Respect the patient’s cultural and personal values, beliefs, and preferences.\n- Respect the patient’s right to; equal access and consideration regardless of sexual orientation, and/or gender identity or expression.\n- Accommodate the patient’s right to religious and other spiritual services.\n- Respect the patient’s right to privacy.\n- Allow the patient access, request amendment to, and obtain information on disclosures of his or her health information, in accordance with law and regulation.\n- Respect the patient’s right to pain management.\n- Provide information about pain and pain relief measures at the time of initial evaluation.\n- Ask patients on initial evaluation and as part of regular assessments about the presence, quality, and intensity of pain.\n- Review and modify the plan of care for patients who have unrelieved pain.\n- Use the patient’s self report as the primary indicator of pain.\n- Be concerned, compassionate and believe all reports of pain.', ""Patient's Rights and Responsibilities\nAs a patient of the UH Health Center or any other healthcare facility, you have certain rights, including the right to privacy, respect, professionalism, and competent medical care. You also carry certain responsibilities to help optimize the care you receive.\nThe goal of the UH Health Center is to provide all patients with high quality health care in a manner that clearly recognizes individuals' needs and rights. We also recognize that in order to effectively accomplish this goal, the patient and the health care provider must work together to develop and maintain optimum health. As a result, the above patient rights and responsibilities were written.\n- Patients have the right to be treated with respect, consideration, and dignity.\n- Patients have the right to be provided with appropriate privacy.\n- Patients have the right to the degree known, to have complete information concerning their diagnosis, evaluation, treatment, and prognosis.\n- Patients have the right to participate in decisions involving their health care, except when such participation is contraindicated for medical reasons.\n- Patients have the right to refuse to participate in experimental research.\n- Patients have the right to change healthcare providers.\n- Patients have the responsibility to provide accurate and complete information about current and past illnesses, medications, and other matters pertaining to their health.\n- Patients have the responsibility to follow the treatment plan recommended by their healthcare provider or to express concerns regarding their ability to comply.\n- Patients are responsible for their actions if they refuse treatment or do not follow the healthcare provider’s instructions.\n- Patients have the responsibility to arrive as scheduled for appointments and to cancel in advance appointments they cannot keep.\n- Patients have the responsibility to become informed of the scope of basic services offered, the costs, and the necessity for medical insurance, and to actively seek clarification of any aspect of participation in UH Health Center services and programs (including cost) that is not understood.\n- Patients have the responsibility to accept financial responsibility for all services rendered at the UH Health Center.\n- Patients have the right to make suggestions and express grievances; to receive a personal response to the same, if so requested; and to have continued access to care without intimidation, threat, discrimination, or other retaliatory action. No patient will be asked to waive his or her rights, including the right to file a complaint regarding privacy with the UH Health Center Director (713-743-5151), the Division of Student Affairs Office (713-743-5390) or the Secretary of the United States Department of Health and Human Services, as a condition of treatment.\n- Patients have the right under HIPAA Privacy Rules to the following: to respect limitations on their medical information, to confidential communications, to inspect and request a copy of their medical information, to request amendment for their medical information, to request accounting disclosures, and to a copy of the Notice of Privacy Practices.\n- Patients have the right to information on the following: services available at the UH Health Center, provisions for after-hours and emergency care, fees for services, payment policies, provider credentialing, and accurate information regarding the competence and capabilities of the organization.""]"	['<urn:uuid:be08939f-4b89-4b35-8985-72c8901ccf9c>', '<urn:uuid:209d601c-ad8b-4a83-a8d7-14a00cbc9008>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T18:05:20.075489	9	73	2023
99	As a mental health professional working with aerospace professionals, I'm curious about how successful engineers maintain work-life balance and manage stress - what are some effective practices for maintaining mental wellness while pursuing a demanding technical career?	Successful professionals maintain wellness through two key approaches: pursuing meaningful activities outside work and self-motivation. For instance, aerospace engineer Dean Chai volunteers as an EMT at Bethesda-Chevy Chase Rescue Squad, working 24-hour shifts despite his demanding technical role. Additionally, research shows that maintaining an identity separate from one's career and having activities that provide personal satisfaction are crucial for managing occupational stress and promoting resilience. This includes having friends outside of one's professional field and engaging in community activities that allow mental decompression.	"[""Aerospace Engineer Dean Chai Attributes Self-Motivation to His Success\nSupervisory Aerospace Engineer believes that everything has to align for good things to happen and that each individual is responsible for making things align.\nAssociate Branch Head\nFormal Job Classification:\nSupervisory Aerospace Engineer\nOrganization He Works For:\nCode 591, Attitude Control Systems Engineering Branch, Mission Engineering & Systems Analysis Division, Applied Engineering and Technology Directorate\nWhat is most interesting about your role here at Goddard?\nAs Associate Branch Head, I help manage the branch but I also do technical work including attitude control analysis for the Magnetospheric Multiscale (MMS) Mission. Attitude Control is the pointing control of a spacecraft. Anytime you observe a planetary object, you need to maintain accurate and steady pointing in order to capture images scientists want. We developed high fidelity dynamical simulation aided by high-resolution graphics so that everything comes alive.\nWhat qualities do you have that helped you become a supervisor before the age of thirty?\nI’d like to think doing good work, being dedicated, and taking pride in the work contributes to my becoming a\n|› Larger image\nPhoto of Dean Chai. Credit: NASA\nsupervisor at a young age. But equally important are being able to communicate and work well with a variety of personalities. While having a supportive boss and mentors help, I strongly believe that everything has to align for good things to happen and they don’t happen by chance. The individual is responsible for making things align and it starts with oneself.\nWhat makes Goddard a great place to work?\nWe have a lot of talented, enthusiastic people who love what they do. This makes all the difference; everyone is self-motivated.\nWhat was your best day or the best thing that ever happened to you at Goddard?\nIt is difficult to pinpoint. One recent experience is that we have been trying to elevate a particular area of interest within the branch, but our team has been overloaded with existing work. One day, a relatively senior engineer whom I have been attempting to engage stopped by my office and offered to take the initial step. Motivating that person to take one small step actually made my day! Confucius had a relevant saying: “If one hopes to attain order in the state, one must first bring his own family and personal life into order through self-cultivation and the expansion of one's knowledge and the investigation of things.” The translation is a bit rough, but my take-away from this popular quote is that things always start small, but, through good intention and persistence, they will happen.\nHow would you describe success?\nSuccess is when a person lives a happy and self-fulfilled life. As Buddhist teachings say, in order to reach eternal happiness you have to rid yourself of attachments to things. The moment you detach, you attain happiness and thereby success. It is important to seek success in things that you try to do yourself rather than constantly hoping for something great to somehow just happen. The key is self-motivation.\nIs there something surprising about you, your hobbies, interests, activities outside of work that people do not generally know?\nI am a volunteer Emergency Medical Technician for the Bethesda-Chevy Chase Rescue Squad. I go in every Wednesday night at 7:00 p.m. and leave every Thursday morning at 7:00 p.m., so I’m extra sleep-deprived on Thursdays! I’ve been volunteering for about a year and a half. I have met many, very different people at the squad. There are a lot of older medical patients and veterans in our immediate service area. Our station has a Rescue Squad vehicle that extricates people from vehicles, so we assist with a lot of beltway personal injury collision calls between Connecticut Ave. and the Clara Barton Parkway.\nIs there someplace in the world that you want to visit, or someplace you have been and want to go back?\nMy wife and I were both born in Taiwan and immigrated here at an early age. We both speak Chinese and Chinese culture is a very integral part of our life. Also, we both have family including our parents in Taiwan whom we visit every year.\n› More Conversations With Goddard\nElizabeth M. Jarrell\nNASA's Goddard Space Flight Center, Greenbelt, Md."", 'Why your off-duty life is important for stress management\nOver time, if occupational stress is not properly managed, officers may experience life-threatening consequences\nLaw enforcement is one of the most stressful occupations in existence. Over time, if occupational stress is not properly managed, officers who are currently employed, as well as those who are retired, may experience life-threatening consequences. These consequences could be physiological, psychological, or behavioral (Patterson et al., 2014).\nPolice officers face many threats to both their physical safety and mental health that the general population does not. Officers are exposed to traumatic events and must regularly respond to crime scenes and domestic disputes. They also experience stress from things like leadership and management challenges as well as the difficulties associated with long hours and shift work. These different factors often contribute to officers suffering from chronic stress.\nThe Consequences of Unmanaged Stress\nDue to the acute stress that officers experience while on-duty, combined with the chronic stress that occurs over time, officers can suffer from both physical or mental problems. Officers are at a greater risk of suffering from mental disorders. In one study, 24 percent of the officers examined reported symptoms of post-traumatic stress disorder (PTSD), 9 percent had depression symptoms, and 19 percent reported alcohol abuse (Fox et al., 2012). Other research by Austin-Ketch et al. (2012) and June-Hee et al. (2016) found prevalence rates of PTSD among police officers at 35 percent and 41 percent respectively.\nIn addition to concerns involving mental health, they are at higher risk of physical problems. Police officers are at risk of developing cardiovascular disease due to high-levels of stress (Thayyil, Jayakrishnan, Raja, & Cherumanalil, 2012). They are also more likely to have higher alcohol consumption caused by exposure to work-related violence as they continue to accumulate months or years of experience in policing (Leino et al., 2011). Alcohol abuse can then lead to many additional problems for officers including isolation, psychological, physical, and personal problems (Chopko et al., 2013; Leino, Eskelinen, Summala, & Virtanen, 2011).\nTo promote resilience against the psychological, physiological, and behavioral problems resulting from stress, officers need to apply effective stress management strategies. Ways that officers spend their time off-duty is an important part of this.\nWhile on-duty, officers see the worst side of society and are regularly exposed to traumatic events. The stress that this causes can be countered by engaging in activities and maintaining a lifestyle that is unrelated to policing. All officers need to deliberately work to establish a personal identity that’s distinct from their career as an officer. This enables officers to truly separate themselves from the uniform while off-duty.\nTo minimize the risks associated with police stress and to promote resilience from the stress experienced on-duty, officers should:\n- Maintain plans and goals outside of being a police officer. This should include long-term goals following retirement and other aspirations that increase an officer’s personal satisfaction. Giving thought to the future, post-policing, is important because it requires officers to reflect on topics that are not associated with policing.\n- Associate with society outside of law enforcement. It is common for police officers to congregate with other officers, even while off-duty. However, having friends outside of policing is important because it can provide insight into how people in other occupations solve problems. Having civilian friends can also help officers socialize and be part of their community in a role that is outside of law enforcement.\n- Have activities outside of work that provide satisfaction. Engaging in activities outside of policing is also important in maintaining a proper work-life balance. These activities can include regular participation in things like coaching, community groups, sports teams or anything else that officers personally enjoy. These activities also provide opportunities for officers to take their mind off the stresses of policing.\n- Accept the advice of senior officers who have effectively managed stress throughout their police career. Regardless of career stage, officers should take advantage of the excellent resource found in the experience and support of other officers. Take the time to have discussions with fellow officers about how they cope with stress and what works for them. This can be especially insightful if veteran officers have developed coping strategies that are unique to the stressors officers face in specific regions, within certain agencies due to politics or processes, or other factors that may be unique to that area.\nPolice stress is a major concern and can have serious psychological and physiological consequences if it is not properly managed. It is important that steps are taken to promote officer resiliency even while off-duty. Maintaining an identity aside from the badge enables an officer to decompress through personal interests and gives them an opportunity to gain experiences and perspectives that are not police-related. Officers should strive to maintain a network of friends outside of work who are not associated with law enforcement and can provide problem-solving strategies from the perspective of other occupations. In addition, officers should also engage in off-duty activities that promote relaxation. Lastly, officers should talk with other officers at various career stages to gain insight about coping mechanisms and what they do off-duty to mitigate the stress associated with policing.\nAbout the author\nDr. Jarrod Sadulski is an adjunct professor with American Military University. He has spent more than two years studying police stress and its influence on the lives of police officers. Sadulski has conducted a review of approximately 300 peer-reviewed scholarly articles that focused on topics associated with police stress and officer wellness. In addition, he conducted a two-year qualitative study on how successful police officers effectively manage stress throughout their law enforcement career. Each participant in Sadulski’s study reflected upon the value of life and having an identity outside of policing as an effective stress management strategy. With a Ph.D. in Criminal Justice, he continues to research effective stress management strategies for police officers to promote resiliency. In addition, Sadulski has 20 years of policing experience between both federal and local law enforcement. Sadulski is available for speaking engagements and assisting agencies with the development of peer-support and stress management implementation programs. You can contact him at firstname.lastname@example.org.\nAustin-Ketch, T. L., Violanti, J., Fekedulegn, D., Andrew, M. E., Burchfield, C. M., & Hartley, T. A. (2012). Addictions and the criminal justice system, what happens on the other side?? Post-traumatic stress symptoms and cortisol measures in a police cohort. Journal of Addictions Nursing (Taylor & Francis Ltd), 23(1), 22-29 8p. doi:10.3109/10884602.2011.645255\nChopko, B. A., Palmieri, P. A., & Adams, R. E. (2013). Associations between police stress and alcohol use: Implications for practice. Journal of Loss & Trauma,18(5), 482-497. doi:10.1080/15325024.2012.719340\nFox, J., Desai, M. M., Britten, K., Lucas, G., Luneau, R., & Rosenthal, M. S. (2012). Mental-health conditions, barriers to care, and productivity loss among officers in an urban police department. Connecticut Medicine,76(9), 525-531. http://csms.org/publications/connecticut-medicine/\nJune-Hee, L., Inah, K., Jong-Uk, W., & Jaehoon, R. (2016). Post-traumatic stress disorder and occupational characteristics of police officers in Republic of Korea: a crosssectional study. BMJ Open, 6(3), 1. doi:10.1136/bmjopen-2015-009937\nLeino, T., Eskelinen, K., Summala, H., & Virtanen, M. (2011). Work-related violence, debriefing and increased alcohol consumption among police officers. International Journal of Police Science & Management,13(2), 149. doi:10.1350/ijps.2011.13.2.229\nPatterson, G. T., Chung, I. W., & Swan, P. W. (2014). Stress management interventions for police officers and recruits: A meta-analysis. Journal of Experimental Criminology, 10(4), 487-513. doi:10.1007/s11292-014-9214-7\nThayyil, J., Jayakrishnan, T. T., Raja, M., & Cherumanalil, J. M. (2012). Metabolic syndrome and other cardiovascular risk factors among police officers. North American Journal of Medical Sciences,4(12), 630. doi:10.4103/1947-2714.104313']"	['<urn:uuid:d0b4b7ac-512b-4d23-a7a9-519408e4b855>', '<urn:uuid:ed607d6c-791f-4caf-810d-ac3a1bac1198>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T18:05:20.075489	37	83	1969
100	I'm working on automating our web accessibility testing process. What are the main limitations of automated tools like Lighthouse compared to the comprehensive requirements of WCAG guidelines?	Lighthouse and other automated tools have significant limitations when compared to full WCAG requirements. According to analyses, Lighthouse can miss issues like small pointer input targets and duplicate ID attributes. It cannot evaluate the quality or accuracy of alt text - it only checks for presence, not appropriateness. The tool also cannot determine if color is being used as the only means of conveying information, assess page title relevance, or evaluate content quality. WCAG compliance requires meeting specific success criteria across different levels (A, AA, AAA), covering aspects like keyboard navigation, text alternatives, contrast ratios, and timing limitations. For full WCAG conformance, both automated and manual testing are necessary, as many accessibility requirements need human judgment to evaluate properly.	"['Google’s Chrome DevTools is an exceptionally useful resource for developers. With a few clicks, users can troubleshoot issues, inspect network activity, and find opportunities for on-page enhancements — including accessibility improvements.\nLighthouse is an open-source utility within DevTools that creates automated reports on site performance, search engine optimization (SEO), and accessibility. Given that it’s completely free, Lighthouse has become a go-to option for basic website audits.\nHowever, it’s important to understand the limitations of automated testing. In this article, we’ll outline a few types of accessibility barriers that Lighthouse might miss, along with a few tips for using the tool effectively.\nHow does Google Lighthouse analyze accessibility?\nLike most accessibility tools, Google Lighthouse compares web content against the Web Content Accessibility Guidelines (WCAG). Published by the World Wide Web Consortium (W3C), WCAG is the consensus standard for digital accessibility.\nLighthouse covers many WCAG 2.1 Level AA requirements. It presents users with a score, which is a weighted average of all of the tests that Lighthouse performs (in some cases, it’s possible to receive different Lighthouse accessibility scores on a page without making changes).\nTo run Lighthouse, you’ll simply open Google Chrome and take the following steps:\n- Navigate to the web page you’d like to analyze.\n- Right-click on the screen and select “Inspect Element.”\n- Select the “Lighthouse"" tab, then select “Generate Report.”\nLighthouse will generate a report identifying common WCAG violations like missing image alternative text (alt text), inconsistent header structures, and insufficient color contrast ratios. By addressing these issues, you can provide a better on-page experience for many users with disabilities, including people who use screen readers and other assistive technologies.\nGoogle Lighthouse offers a quick overview of accessibility issues\nLighthouse provides a fairly accurate way to find common accessibility issues, and it doesn’t require much technical knowledge to operate. However, no automated tool can guarantee conformance with WCAG — and Lighthouse is not an exception.\nIn an analysis performed by web design and development firm Sparkbox, Lighthouse failed to identify small pointer input targets, ID attributes with the same names, and other technical issues that can be identified by other automated tools. Additionally, all automated tests have limitations when analyzing certain types of content.\nGoogle Lighthouse is not effective for finding WCAG conformance failures such as:\nInaccurate or Unhelpful Alt Text\nAlt text needs to provide a concise, accurate description of non-text content. The description helps people who use screen readers understand the function that an image (or other non-text content) performs on a page.\nArtificial intelligence isn’t capable of judging the quality of your alt text. In other words, if Lighthouse determines that your images have alt tags, it will give you credit. However, it can’t tell you if your alt text is confusing to real-life users.\nUnnecessary Alt-Text on Decorative Images\nAccessibility tools like Lighthouse may recommend adding alt text to every image — but that’s not always the best practice. WCAG 2.1 Success Criterion (SC) 1.1.1, “Non-text Content,” requires text alternatives in most circumstances, but if content is purely decorative, it should not include alt text.\nConsider the user’s perspective: If every decorative element on a page included alt text, the user would be overwhelmed with unnecessary information. Decorative non-text content should be implemented in a way that can be ignored by screen readers and other assistive technologies.\nIrrelevant Page Titles and Low-Quality Content\nLighthouse can’t determine whether your page titles accurately describe your content; you’ll need a real human being to make that determination. Likewise, automated tests can provide an overview of your content’s reading level, but they can’t provide detailed feedback on the overall clarity of your writing.\nUsing Color Alone to Convey Meaning\nWCAG 2.1 SC 1.4.1, “Use of Color,” prohibits websites from using color as the only visual means of conveying information. For example, if a form uses text like “errors shown in red,” it would fail this guideline. Again, Lighthouse (and every other automated tool) doesn’t have the necessary context to determine whether color usage creates barriers for human users.\nFor WCAG conformance, combine automated accessibility tools with manual testing\nLighthouse may miss other WCAG success criteria, and automated Lighthouse audits don’t provide much guidance for fixing issues. In other words: Designers and developers can certainly use Lighthouse to check for a limited number of accessibility barriers, but the tool doesn’t guarantee conformance with WCAG.\nAutomated accessibility tests can be helpful during the first phases of an accessibility initiative. To ensure that your content reaches the widest possible audience — and to improve the on-page experience for all users — you’ll need to use both automated and manual tests throughout your website’s development.\nFor more information, read about the Bureau of Internet Accessibility’s four-point hybrid testing methodology, which we believe provides the best path to achieving and maintaining digital compliance.', 'The Web Content Accessibility Guidelines (WCAG)\nWhat are the Web Content Accessibility Guidelines (WCAG)?\nThe guidelines offer technical recommendations on how to make website content accessible. The guidelines are also the standard reference for most website accessibility-related legislation like the Americans with Disability Act (ADA) in the US, and the European Web Accessibility Directive.\nWCAG Versions - 2.0, 2.1, 2.2 and 3.0\n- WCAG 2.0 - published 11 December 2008.\n- WCAG 2.1 - published on 5 June 2018 and is now the W3C recommended version\n- WCAG 2.2 - not yet in effect, scheduled to be published in 2021\n- WCAG 2.0 had 61 success criteria\n- WCAG 2.1 introduced 17 more success criteria to address mobile accessibility, people with low vision, and people with cognitive and learning disabilities.\n- WCAG 2.2 will be expanding on 2.1 with nine new success criteria, plus an update to one, with the goal of making content more accessible to a wider range of users.\nThe current standing WCAG versions 2.0 and 2.1 are categorized according to four principles, perceivable, operable, understandable, and robust (POUR).\nElements that convey information or components of a website’s user interface must be presented in a way that users are able to find, process, and understand.\nAll functionality and navigation on the website should be usable.\nInformation and the operation of the user interface must be clear and understandable to users of all abilities.\nThe website should be capable of adapting and developing itself to support a variety of current and potential future user agents, including assistive technologies.\nUnder each principle are testable success criteria that provide recommendations on how to make digital content more accessible. The success criteria are classified by three levels A, AA, and AAA, with A being the most basic level of WCAG compliance and AAA being the hardest.\nLearn more about the elements of WCAG and how to comply with its success criteria.\nOn the 21st of January, 2021, the WAI released the first working draft of the WCAG 3.0. WCAG 3.0 is planned to be a major revision with the intention to make the guidelines more user-friendly than the WCAG 2 iterations, and more flexible, covering even more content, apps, and tools, as well as organizations and disabilities. WCAG 3.0 is still in development and is not expected to be finalized for the next few years. Learn more about WCAG 3.0.\nThe different WCAG compliance levels\nThe WCAG categorizes its conformance based on three levels: A, AA, and AAA. To conform to the guidelines, it is a requirement that one of these levels should be fully met.\nMinimal WCAG compliance (level A)\nExamples of Level A success criteria:\n- All non-text content like images or videos must have a text alternative, like alt text or captions, that serves the equivalent purpose.\n- Users can navigate the website effectively using only a keyboard\n- Avoid using color as the only visual means of conveying information or prompting an action, like having green buttons with no text on them to suggest that it is meant to be selected as a ‘yes’ response.\n- If there is audio that auto-plays on your website for more than 3 seconds, ensure that that you provide means of adjusting the volume, stopping, or pausing it.\nAcceptable WCAG compliance (level AA)\nExamples of Level AA success criteria:\n- Ensuring that text on a webpage can be resized without assistive technology up to 200 % without loss of content or functionality.\n- Provide descriptive headings and labels in content\n- Navigational elements on the site, like menus, should be in a consistent, repeated position across the website.\n- When executing an action on the site, like filling in forms or clicking on buttons, errors can occur on the user’s part. If an error should occur, suggestions for correction should be provided.\nOptimal WCAG compliance (level AAA)\nExamples of Level AAA success criteria:\n- The visual presentation of text and images of text must have a contrast ratio of at least 7:1.\n- Removing timing limitations from all content, unless it is for non-interactive synchronized media and real-time events.\n- When a user has to submit information on a webpage, the submissions must be reversible, checked for input errors (and offer suggestions for correction if errors do occur), and there is a confirmation mechanism in place to allow the user to review the submission and edit if needed.\n- Images of text should be avoided or only used for decoration.\nWho should comply with the WCAG?\n- Web content developers (page authors, site designers, etc.)\n- Web authoring tool developers\n- Web accessibility evaluation tool developers\nHow to check your website’s WCAG compliance level\nHow Monsido can help your website meet WCAG standards\nEach audit scans your site for machine-testable issues, provides detailed reports so you can review any errors that may arise, gives you targeted recommendations on how to address these errors based on the guidelines, and shows you your compliance based on levels A, AA, and AAA. You can track and prove your accessibility compliance progress via reports in the History Center. We also offer accessibility training to customers and support, all-inclusive, to ensure that you are well-versed in both automated and manual remediation methods, and can efficiently and consistently improve your website’s accessibility.\nMonsido also offers free tools to complement your web accessibility efforts, including a color contrast checker for web teams to test out compliant color combinations for their web design, and an accessibility statement generator, which helps you generate a public statement declaring your commitment to web accessibility and helps make your web accessibility policy transparent to all your users.']"	['<urn:uuid:48163b07-4dd4-4ebf-b175-7e7966c4fce0>', '<urn:uuid:a2ef5797-959c-4ba6-9b52-ee60633ac0ca>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T18:05:20.075489	27	119	1746
