qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	Do the Smithsonian Archives and Einstein share approaches to combining ideas?	Yes, both demonstrate commitment to combining diverse perspectives. The Smithsonian Archives of American Art actively combines different viewpoints by collecting materials from diverse groups including BIPOC, LGBTQ+, disabled, and women artists, creating an inclusive historical record. This parallels Einstein's approach to knowledge generation, where he emphasized making novel combinations and viewing problems from multiple perspectives. Both institutions recognize the value of bringing together different viewpoints - the Archives through its diverse collections spanning over 200 years of American art, and Einstein through his emphasis on combining different ways of thinking about problems to generate new insights.	['Research Center Website\nAnne Helmreich, Director\nThe Smithsonian’s Archives of American Art (AAA) is the world’s preeminent and most widely used resource for the study of the visual arts in the United States. It serves scholars, students, journalists, biographers, and the interested public from its headquarters in Washington, DC, its research center in New York City, and through its vast online resources available worldwide.\nFounded in 1954, the AAA fosters advanced research through gathering, preserving, and providing access to primary source material documenting more than two hundred years of our nation’s artists and art communities. Unequaled in historical depth and breadth, the AAA is a catalyst for scholarship through its collecting, exhibition, and publication programs, including the peer-reviewed Archives of American Art Journal, the longest-running scholarly journal in the field of American art. Our vast holdings of 6,300+ collections consist of more than 30 million letters, diaries, scrapbooks, preliminary artworks, manuscripts, financial records, photographs, films, and audiovisual recordings of artists, dealers, collectors, critics, scholars, museums, galleries, associations, and other art-world actors. An international leader in the digitizing of archival collections, the AAA makes nearly 3.2 million digital images freely available online. The AAA’s oral history collection includes more than 2,500 audio interviews, the largest accumulation of in-depth, first-person accounts of the American art world.\nFrom its founding, the AAA’s broad approach laid the groundwork for inclusive collecting. We aim to represent a broad chronological and geographic scope, and we actively seek the papers of BIPOC, LGBTQ+, disabled, women, and other historically underrepresented groups significant to the history of art in the US. Thematic areas of concentration include the lives of artists, research and writing about art, arts organizations, the art market, patronage, and art instruction and services. Some of the notable twentieth-century collections available at the AAA are the records of the American Federation of Arts, Leo Castelli Gallery, André Emmerich Gallery, Holly Solomon Gallery, Los Angeles Institute of Contemporary Art, Macbeth Gallery, Downtown Gallery, Woman’s Building, Jacques Seligmann & Co., Cinque Gallery, and Betty Parsons Gallery; the Walt Kuhn papers, which include records of the 1913 Armory Show; the Edward Bruce and Holger Cahill papers, with documentation on New Deal art programs; and personal papers of artists Joseph Cornell, Jeff Donaldson, Arthur Dove, Sam Gilliam, Rockwell Kent, Gyorgy Kepes, Yasuo Kuniyoshi, Louise Nevelson, Violet Oakley, Jackson Pollock, Robert Smithson and Nancy Holt, Alma Thomas, and Charles White. The AAA has a strong concentration of the papers and research materials of critics and art historians including Dore Ashton, Gregory Battcock, Clement Greenberg, Lucy R. Lippard, Dorothy C. Miller, Linda Nochlin, Esther McCoy, and Tomás Ybarro-Frausto, among others.\nSignificant nineteenth-century material includes papers of James Carroll Beckwith, Charles Caffin, George Catlin, Kenyon Cox, Jervis McEntee, William Page, Hiram Powers, and the Weir family of artists, and extensive microfilmed collections of the correspondence of Mary Cassatt, Thomas Cole, Asher B. Durand, Winslow Homer, John Kensett, and James Abbott McNeill Whistler.\nThe AAA’s holdings are described on its website at www.aaa.si.edu, including summaries of all archival collections, more than 1,400 searchable finding aids, more than 1,700 oral history transcripts, and more than 260 collections that have been digitized in their entirety. Microfilm copies of many of the collections are also available.\nIn addition to its primary research center at the Victor Building, 750 9th Street, NW (at H), Suite 2200, Washington, DC 20001 the AAA maintains a New York Research Center at 300 Park Avenue South, Suite 300, New York, New York 10010. (212) 399-5015.\nFranco, Josh T., National collector. B.A. (2007) Southwestern University; M.A. (2010) Binghamton University, SUNY; Ph.D. (2016) Binghamton University, SUNY. Research specialties: archives, contemporary art, minimalism, American art of the 1960s, Chicana/o art, Latinx art, decolonial studies, feminisms, artists as researchers. Contact: email@example.com\nGillespie, Ben, Arlene and Robert Kogod Secretarial Scholar for Oral History. B.A. (2009) University of Georgia; M.A. (2011) University of Chicago; Ph.D. (2019) Johns Hopkins University. Research specialties: oral history, art and literature, global print culture in the 19th and 20th centuries, feminist and queer activist art, aesthetic theory. Contact: firstname.lastname@example.org\nProctor, Jacob, Gilbert and Ann Kinney New York Collector. B.A., University of Colorado, Boulder; M.A., Harvard University. Research specialties: international contemporary art, conceptual art; Fluxus and intermedia practices of the 1960s and 1970s, experimental film and video; art theory, criticism, and historiography. Contact: email@example.com\nShapiro, Emily Dana, Managing editor, Archives of American Art Journal. B.A. (1995) Kenyon College; M.A. (2003), Ph.D. (2005) Stanford University. Research specialties: archives, arts and humanities publishing, pre-1945 American art and visual culture, genre painting, still life painting, art and labor. Contact: firstname.lastname@example.org.', 'Quoted material with my inserted links\n1. Look at problems in many different ways, and find new perspectives\nLeonardo da Vinci believed that, to gain knowledge about the form of a problem, you begin by learning how to restructure it in many different ways. He felt that the first way he looked at a problem was too biased. Often, the problem itself is reconstructed and becomes a new one.\nWhen Einstein thought through a problem, he always found it necessary to formulate his subject in as many different ways as possible, including using diagrams. He visualized solutions, and believed that words and numbers as such did not play a significant role in his thinking process.\n3. Produce! A distinguishing characteristic of genius is productivity.\nThomas Edison held 1,093 patents. He guaranteed productivity by giving himself and his assistants idea quotas. In a study of 2,036 scientists throughout history, Dean Keith Simonton of the University of California at Davis found that the most respected scientists produced not only great works, but also many “bad” ones. They weren’t afraid to fail, or to produce mediocre in order to arrive at excellence.\n4. Make novel combinations. Combine, and recombine, ideas, images, and thoughts into different combinations no matter how unusual.\nThe laws of heredity on which the modern science of genetics is based came from the Austrian monk Grego Mendel, who combined mathematics and biology to create a new science.\n5. Form relationships; make connections between dissimilar subjects.\nDa Vinci forced a relationship between the sound of a bell and a stone hitting water. This enabled him to make the connection that sound travels in waves. Samuel Morse invented relay stations for telegraphic signals when observing relay stations for horses.\n6. Think in opposites.\nPhysicist Niels Bohr believed, that if you held opposites together, then you suspend your thought, and your mind moves to a new level. His ability to imagine light as both a particle and a wave led to his conception of the principle of complementarity. Suspending thought (logic) may allow your mind to create a new form.\n7. Think metaphorically.\nAristotle considered metaphor a sign of genius, and believed that the individual who had the capacity to perceive resemblances between two separate areas of existence and link them together was a person of special gifts.\n8. Prepare yourself for chance.\nWhenever we attempt to do something and fail, we end up doing something else. That is the first principle of creative accident. Failure can be productive only if we do not focus on it as an unproductive result. Instead: analyze the process, its components, and how you can change them, to arrive at other results. Do not ask the question “Why have I failed?”, but rather “What have I done?”\nAddendum: The items below are characteristics I have gathered from his biographical resources.\n9. Study philosophy.\n- Einstein studied philosophy and it influenced the way he thought.\n10. Remain skeptical of your professors and other experts\n- Einstein sometimes showed a high degree of skepticism towards processed knowledge\n11. Slow down your thinking process.\n- Einstein said that he was not smarter but that he stayed with problems longer. He has been described by himself and others as a slow thinker.\n12. Imagine yourself as being part the problem you want to solve.\n- Einstein sometimes imagined himself being part of the dynamics he was trying to understand. He came to some great insights about time by imagining that he was riding a beam of light through space.\n13. Not all innovative ideas are necessarily 100% “good”. Any idea or innovation that can be used for the benefit of people may also be used against them.\n- Einstein’s breakthroughs in energy could be used to power cities and benefit society. But as we learned, the same ideas could also be used to kill thousands of people. It’s important to understand multiple possible applications of your ideas before you make them public. Einstein understood this. But it’s unclear if he understood this when he first proposed his theories or later, when others began talking about how to create a bomb.\n14. Discuss ideas with other bright people to gain a more robust insight.\n- Einstein would discuss his ideas with colleagues and friends who were also experts in the subjects he cared about. His first wife studied along with Einstein early on and she was well-versed in the same subjects. She is very likely to have contributed insights that moved his ideas forward.\n15. Immerse yourself in the newest ideas from others.\n- Einstein worked as a patent clerk. He was one of the fist people to read many of the newest ideas submitted for patent protection by the brightest minds of his time.\nOriginal Source of items 1-8 (I contributed items 9-12 on this blog).\nThe Getty >>']	['<urn:uuid:43ecf453-dae0-44cf-936d-2c1fd33bdee3>', '<urn:uuid:3baa0efe-cf79-4cfb-8813-caed65092001>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T19:47:23.148022	11	96	1570
2	conservation professional seeking information about zebra mussels contamination in aquarium moss balls nationwide distribution status threat level	Federal and state wildlife agencies have confirmed reports of living zebra mussels in 'moss ball' aquarium plants being sold nationwide. This is a critical invasive species threat, as zebra mussels are impossible to eradicate once established. They cause irreversible ecosystem damage, destroy habitats, and kill native species. Authorities are urging for the proper disposal and halted sale of these live aquarium plants.	['Statement from the Association of Fish and Wildlife Agencies Supporting the Introduction of the Abandoned Mine Land Reauthorization Act\nMar 10, 2021, 2:09 PM\nThe Association of Fish & Wildlife Agencies strongly supports the introduction of the Abandoned Mine Land Reauthorization Act. This bipartisan legislation would reauthorize the Abandoned Mine Land Trust Fund, which is set to expire in September 2021. Representative Matt Cartwright (D-PA-08) and Representative Glenn Thompson (R-PA-15) introduced this bill today in the House of Representatives in Washington, D.C.\nInvasive Zebra Mussels Found Nation-Wide in Aquarium Plant Product\nMar 8, 2021, 11:20 AM\nThe Association of Fish & Wildlife Agencies joins federal and state wildlife agencies across the country urging the proper disposal and halted sale of live “moss ball” aquarium plants due to confirmed reports of products containing living zebra mussels. This aquatic invasive species has proven impossible to eradicate once a colony is established, causing irreversible ecosystem damage, destroying habitats, and killing native species.\nAdvancing the National Fish, Wildlife, and Plants Climate Adaptation Strategy into a New Decade Report Now Available\nJan 26, 2021, 9:47 AM\nThe Association of Fish and Wildlife Agencies is pleased to announce the release of a new report “Advancing the National Fish, Wildlife, and Plants Climate Adaptation Strategy into a New Decade”. This is an update on the 2012 Strategy which was designed to enable natural resource managers to take effective steps towards climate adaptation. The new report presents an update of the science and practice, with a special section on Indigenous Knowledge systems, demonstrates implementation progress, and provides recommendations for the future.\nWildlife Monograph Publication on Best Management Practices for Trapping- Largest Trapping Study Ever Conducted\nDec 29, 2020, 3:30 PM\nThe Association of Fish & Wildlife Agencies (AFWA) is excited to announce the recent publication titled “Best Management Practices for Trapping Furbearers in the United States” in Wildlife Monographs. This new publication is a culmination of over two decades of scientific research - by far the largest trap testing study ever conducted. The article is a product of AFWA’s Furbearer Management and Best Management Practices (BMPs) for Trapping Program, a program created to improve animal welfare in U.S. trapping programs by identifying traps that achieve internationally acceptable injury thresholds while remaining efficient and safe, and promoting their use among trappers, researchers, and managers who use traps to capture wildlife.\nThe Association Applauds Congress and the Administration for Enacting the Historic America’s Conservation Enhancement Act (S.3051)\nOct 30, 2020, 3:08 PM\nToday, the Association, along with the nation’s hunters, anglers, conservationists, and outdoor enthusiasts of every kind, celebrated the President’s signing of the most important fish and wildlife legislation enacted this congressional session, S. 3051, the America’s Conservation Enhancement Act (ACE Act). The bill ensures continuation of two of the most successful conservation programs in the nation’s history, with reauthorization of the North American Wetlands Conservation Act and codification of the National Fish Habitat Partnership. It provides measures to combat the most pressing disease currently threatening deer species, Chronic Wasting Disease (CWD), through formation of a state and federal Task Force and the direction of research on CWD transmission and pathways. Among other vital provisions, the Chesapeake Bay program is reauthorized, and states are ensured the necessary authority to continue regulating types of fishing tackle based on localized effects for the next five years.\nFish and Wildlife Conservationists Praise Congress for Passing the America’s Conservation Enhancement Act (S.3051)\nOct 1, 2020, 3:33 PM\nToday, fish and wildlife conservation organizations applauded the historic passage of bipartisan legislation to conserve our nation’s natural resources, from fish and wildlife and their habitats to the vast multitude of species and ecosystems that will benefit from these robust restoration efforts made possible through S. 3051, The America’s Conservation Enhancement Act (ACE Act).\nAFWA Recommends 38 Priority Projects for the 2021 Multistate Conservation Grant Cycle\nSep 25, 2020, 9:08 AM\nDuring its 110th Annual Meeting, held virtually September 9th – 15th, the Association of Fish & Wildlife Agency’s members recommended the approval of 38 projects to be funded through the Multistate Conservation Grant Program (MSCGP).\nAFWA Honors its 2020 Annual Awards Recipients\nSep 15, 2020, 11:07 AM\nThe Association of Fish & Wildlife Agencies recognized eleven individuals, three state agencies and one private landowner for their dedication to advancing fish and wildlife conservation at the Association’s Annual Awards Ceremony held virtually on September 14, 2020.\nSara Parker Pauley Elected 2020-2021 President of the Association of Fish & Wildlife Agencies\nSep 15, 2020, 9:52 AM\nThe membership of the Association of Fish & Wildlife Agencies (AFWA) elected Sara Parker Pauley, Director of the Missouri Department of Conservation, as its new president during its 110th Virtual Annual Meeting. Director Pauley will serve as AFWA President through September 2021.\nSix Honored for Safeguarding America’s Valuable Natural Resources\nSep 10, 2020, 3:00 PM\nToday at the Association of Fish and Wildlife Agencies Annual Meeting, six organizations and individuals were honored by their peers as recipients of the Climate Adaptation Leadership Award for Natural Resources. The Climate Adaptation Leadership Award was established in 2016 to recognize exemplary leadership in the field of climate adaptation and the protection of the Nation’s vital natural resources and the people who depend on them in a changing world.']	['<urn:uuid:9780014e-e2eb-4c1a-b47c-62393c4d77fa>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T19:47:23.148022	17	62	874
3	clean window air conditioner increase efficiency	To increase the efficiency of a window air conditioner, there are several key maintenance tasks. The most important is cleaning the filter once a month, as it traps airborne particles daily which can affect efficiency. The coils should also be kept clean to allow effective heat exchange - use a damp cloth for visible debris and a soft toothbrush for less-reachable parts. Additionally, it's important to give the unit rest periods by not leaving it running when away from home for extended periods.	['Not to scare you out but there are numerous studies that link extreme heat with breathing problems, exhaustion, or even heart problems. While most people are using central air conditioning systems to tackle this problem, this isn’t always the most affordable option. This is why we want to draw attention to other functional yet overlooked coolers — window air conditioners.\nBut how does a window air conditioner work and why consider buying one are the questions that you are probably asking yourself now. Let’s discover.\nParts of a Window Air Conditioner That Cool Your Living Space\nIn order for you to understand the way window air conditioners work, it is crucial to know the parts inside them.\nKnown as the heart of an air conditioning system, the compressor serves as a pump that basically moves the refrigerant between the evaporator coil and condenser coil.\nFan & Blower\nThese two highly important parts of a window air conditioner perform two different functions. While the main function of the fan is to suck in warm air from your room and release it outside, the blower discharges cool air into your living space by circulating hot air over the evaporator.\nThese coils are typically located in the back of the unit and their main function is to collect your indoors heat that the fan has sucked in and reject it outside.\nLocated near the air handler, the evaporated coil absorbs indoor heat and humidity from the air and removes it using a refrigerant.\nThe main function of an air conditioning filter is to trap pollutants such as mold, dust, fungal spores, and pet dander that the air in your living space contains. If the filter isn’t cleaned or replaced in a timely manner, it might hinder the airflow — which could harm your indoor air or increase your power bill.\nThermostats control the temperature of your room, turning on the window air conditioner to cool down a hot room or vice versa.\nWorking on the throttling principle, the main function of the expansion valve is to bring down your air temperature. This part of your window air conditioner is located between the evaporator coils and condenser coils.\nIn a nutshell, refrigerant is a substance that absorbs heat from your living space, transitioning from a low-pressure gas to a high-pressure liquid.\nThe science behind this is as follows: the compressor moves the refrigerant as a low-pressure gas while pushing the gas molecules together and, as the pressure rises, it heats them up. Then it pumps this gas into the condenser which uses fans to cool down the gas, pulling the molecules closer together. This allows the refrigerant to condensate into a low-pressure liquid that cools down the temperature in your home as it moves through the evaporator.\nRoom Air Cycle\nThe room air cycle showcases the way these parts work together to create a cool and airy atmosphere in your home, using the air from your living space.\nThe thermostat in your window unit detects that the temperature levels increase during hot summer days which triggers the following actions:\n- The process begins as the blower sucks in warm air and circulates it over the evaporator.\n- The second thing that happens is the compressor turning on automatically, enabling the refrigerant to move through the evaporating coil.\n- Before the air goes any further, it moves through the filter which traps all the airborne particles.\n- Once the air is free of any pollutants, it passes over the cooling coil, which, thanks to the refrigerant flowing through it, is naturally at a low temperature. This allows for heat absorption, lowering the temperature in your living space.\n- This happens as the unit quickly blows the now-cold air back into your room, which mixes with the warm, humid air and cools it down. This significantly lowers the humidity in your room as colder air can not hold as much moisture as warm air does.\nHot Air Cycle Important Parts & the Way It Works\nThe expansion valve allows phase transition from a liquid to vapor in the evaporator by removing pressure from the liquid refrigerant.\nIt is part of the window air conditioner which rests outside, containing a propeller fan that sucks in warm air and releases it outside through the condenser itself.\nRunning completely different from the condenser, the evaporator converts refrigerant liquid into a gas by absorbing heat from the air in the compartment.\nThe Way It Works\nThe hot air cycle works opposite from the room air cycle as it uses external air to cool a part of the window air conditioner located on a unit section that rests outside — the condenser. This is done as the propeller fan in the condenser sucks in hot air and moves its molecules over the surface of the condensing coil.\nThe condenser contains a high-temperature refrigerant that needs cooling before it begins to bring down the room temperature. As it reaches the unit, this refrigerant is quite hot, with the condenser coil making it even hotter.\nFun fact: this hot air is how the entire cycle gets its name.\nAfter passing through the condenser coils, the refrigerant (which is now in a gas state) penetrates the expansion valve which removes some of the pressure and heat. As a consequence, the air causes it to become liquid at a cooler temperature, automatically cooling the condenser.\nThe refrigerant then continues towards the evaporator which completely removes the heat, enabling this cooled atmospheric air to be discharged outside. This is how a completely new atmospheric air enters the window air conditioner, thus continuing the cycle.\nHow to Increase the Efficiency of Your Window Air Conditioner?\nThere are a few simple things you can do to increase the efficiency of your window air conditioner which include cleaning the filters and coils and giving it a rest.\nClean the Filters\nCleaning the filter in your window air conditioner is the most important thing you can do to boost its efficiency. This is mostly because it traps airborne particles on a daily basis which might affect its efficiency if not cleaned regularly. The recommended time to clean this highly important part of your unit is once a month.\nClean the Coils\nIt’s quite important to keep the window air conditioning coils clean to allow an effective exchange of heat without dirt and debris impairing or blocking the passages. Use a damp cloth to remove visible debris and get to the less-reachable parts with a soft toothbrush.\nGive It a Rest\nDo not leave your window air conditioner to run if you are not home for a longer period of time. As with everything else, these machines tend to wear out quicker with frequent usage. However, this shouldn’t always worry you as giving it proper maintenance is much more important.\nHopefully, we provided comprehensive answers on how does a window air conditioner works in order to make your living space actually livable during the hottest of summer days. Nonetheless, the most important thing to understand is that it uses two main air cycles for these purposes — the room and the hot air cycle.\nCan you use a window air conditioner without a window?\nAs their name suggests, most window air conditioners are designed to run mounted in a window, and placing one such in the middle of your living room might leave you disappointed. This is mostly because their main function is to push the heat and humidity outdoors and there is no other way to redirect these if putting the unit in an enclosed area. You can use a portable air conditioner and run the hose out through a door.\nHow to vent a window air conditioner without a window?\nIn order to vent a window AC unit without a window properly, you need to know that the parts that usually sit outside will expel heat while running. As this heat is way stronger than the cooling air the unit generates, you might end up with a steaming hot room if not placed properly. This is why you need to invent some other creative way such as cutting a hole in the wall.\nHow much is a window air conditioner?\nDepending on the quality and size of the area they are designed to call, window air conditioners cost somewhere around $150 to $600.']	['<urn:uuid:a5d1eb24-e7d6-4aa2-aa04-39639c0094ae>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T19:47:23.148022	6	83	1394
4	How do retailers track customer movement inside stores?	Retailers can track customer movement in stores using Wi-Fi sensors that detect phones' media access control (MAC) addresses. They can also use iBeacons and antennas, which are small wireless devices that communicate with mobile devices through radio signals.	['A rise in the number of devices has contributed to increasing amounts of generated data.\nTable of Contents\nWhat is Personal Data?\nPersonal data is information about a person that refers to an individual.\nSome types of data may be directly personal:\n- Email address\n- Home address\n- Phone number\n- Work history\nPersonal indirect Data is a description of someone which still enables identification. Personal Data may define race/ethnic origin, religious beliefs, political views, sexual orientation, and health conditions, and may demand more strict regulatory requirements. In some situations, consumers provide personal information for free. In others – data is found by giving consumer’s search history, location data, or buyer’s profile. Artificial intelligence and machine learning will drive inferred data.\nData on its own may not stand out as personal data, but when combined with other data it may be private if it identifies information about an individual. The ability to de-anonymise data has increased due to numerous facts. Marketers have become too liberal in the collection of consumer data. It describes radioactive data like customer data, e.g., personally identifiable, which could violate a business agreement if lost. There are about 15 data types, 6 of them are considered radioactive and 7 are considered toxic.\nWhat are the Purposes of Data Collection?\nThere are eight purposes of data collection.\nOne reason for collecting user data is providing the ability to deliver more targeted advertising, based on consumer’s features and demographics. Programmatic advertising became a result of buyer’s tracking. The rise of cookie leaking/syncing enhances the ability to target. According to a report, 5% digital marketers of 83% consider that people-based campaigns perform better than cookie-based.\nPersonalise and improve products/services\nUser’s Data is utilised by site hosts to improve website performance, as well as to personalise content and products, offering to individual consumers.\nAnalysis of data is used to detect fraud and to identify suspicious activity. Both fraud and the incidence of “false positives,’’ i.e., the rejection of a valid transaction can be a significant problem for financial institutions and merchants. Allowing a fraudulent transaction can require the merchant financially, but it can cost the financial institution as well, depending on the nature of an operation. Machine learning improves fraud detection as well as reduces the incidence of false positives.\nBusiness Efficiencies and Processes\nBy retaining customer data, an online retailer can pre-fill forms, e.g., delivery and card details. Data can also be used to decide what products to stock by location.\nThe remaining four reasons relate to a combination of data selling, leakage, sharing, and discrimination.\nSeveral companies sell anonymised data to other parties, e.g., telecom companies may sell data for geolocation tracking, and credit card companies sell these data to advertising companies. First-party data may also give their data to other parties to complete transactions, conduct surveys, and prevent fraud/for security, or marketing.\nHow are consumers tracked online?\nThere are many data trails that we leave as consumers, online and offline. They rise as the use of technology becomes more established in everyday life. Technological progress has resulted in a significant increase in online tracking.\nCookies are the most spreading way of tracking/collecting data which usually contain a string of text as a “unique identifier.” Storing relevant details about a user’s interaction with a site and preferences in this way helps facilitate a more user-friendly experience. When a member returns to a particular website, cookies allow the website to recognise a user’s web browser and remember information about him.\nThere are different types of cookies:\n- Session cookies: Enable websites to link the actions of a user during the browser session\n- Persistent cookies: Allow to check user information and settings/preferences\n- First- and third-party cookies: Refer to the website or domain placing the cookie\nIt is usually the persistent, third-party and tracking cookies which are of most concern from a privacy perspective. These cookies are hard to delete and they have expiration dates that can extend with the website.\nFlash cookies have also become popular. This information can be kept on a computer which is designed to save data, e.g., scores on games. It is difficult to delete them in the same way as other cookies, which means some companies use them to reload other cookies back onto a computer.\nWe can also distinguish ‘Super Cookie’ and ‘Zombie Cookie.’ A super cookie is designed to be permanently stored on a user’s computer and is harder to find out and remove than regular cookies. A Zombie Cookie is projected to return to life after being deleted.\nThere are some newly developed forms of tracking:\nFingerprinting involves collecting unique identifying patterns of information to define a specific device or application. It often consists of gathering unique identifying patterns of information\n- Web Beacons / Pixel Tags are small objects embedded into a webpage or email which are not visible to the user. When a page with one of these objects loads, it will make a call to the server for the object, and this allows the company to know that someone has loaded the page\n- Mobile Tracking\nA new age of connectivity has influenced on such openings.\nThere are several ways of consumer’s mobile tracking:\n- Advertising ID: Apps downloaded on a phone show advertising based on advertising IDs. These include Apple’s Advertising Identifier (IDFA), Android’s Advertising ID and Facebook App User IDs\n- Wi-Fi: When a phone is linked up to a Wi-Fi network, sensors can use the phones media access control (MAC) address to track movement, e.g., around a store\n- Carrier: The mobile carriers may provide de-identified data to third parties for advertising and other purposes\n- GPS: Geolocation tracking (via GPS satellites), e.g., if you pass a particular restaurant on a regular basis, the restaurant could use the information to offer a coupon\n- iBeacons/Antennas: Small wireless devices that use radio signals to communicate with mobiles/tablets\n- Facial Recognition uses biometric software which identifies individuals in a digital image\n- Cross-Device Tracking pulls together disparate datasets to create a picture of consumer behavior/usage as device proliferation has increased\nData protection is not only a dynamic space in Europe. Technological trends are driving changes on a global scale. The implication has been growth, and rising heterogeneity, in the data protection landscape.\nThe United States has taken a more principle and sector-specific based approach than the European Union. U.S. regulators have been willing to work collaboratively with industry operators, encouraging self-regulation. Legislation has been used to address specific risks.\nFair Practice Principles\nModern Data Protection in the United States remains based on traditions stemming from ‘fair practice’ principles first enunciated in 1973.\nCalifornia Has Been the Pioneer at a State Level\nSome states have led the way in privacy regulation, pushing ahead of the Federal Government.\nIn some cases, state provisions can converge to a considerable degree. For example, a notification requirement exists in 47 U.S. States as well as the District of Columbia and Puerto Rico. Despite this, there are significant divergences in areas where states have specific concentrations regarding industry exposure, such as New York for financial services.\nIndustry-Specific Approaches; From Self-Regulation to Legislation\nThere is sector-specific legislation that includes requirements around Data Protection. The framework applies to all businesses that use consumer data.\nThe framework has three key components:\n- Privacy by Design\n- Simplified Consumer Choice\nAsia-Pacific Economic Cooperation (APEC) Privacy Approach Economically Driven\nAPEC’s aims in the area of data protection are exclusively economic. The framework builds public confidence in the safety and security of data flows and to realise the potential of electronic commerce. It is a contrast to Europe where the critical aims of data protection revolve around the protection of fundamental rights and freedoms.\nWhat are the Main Issues for Corporates and Investors in Data Protection?\n- Although the opportunities from data are significant, a lot of speakers have underplayed and underestimated the challenges associated with ensuring ePrivacy and adequately protecting personal data.\n- The implementation of the GDPR in May 2018 represents one of the most excellent events in ePrivacy/data protection regulatory history. The regulation is a game changer regarding not only its scope and ambition but also the significant penalties for non-compliance.\n- The purpose of the developing data protection regulation in Europe is the transferring power to consumers, and in the end, the increase of transparency and trust in how companies use consumer’s data. Best case, consumers become more trusting. The outcome being that data quality improves, and data as an asset increases in value.\n- The very same companies that the GDPR was potentially designed to constrain will probably end up being least affected. Of course, there will be increased regulatory costs associated with compliance, but those larger companies that already have consumer trust and the necessary resources to remain compliant with much stricter and more complex EU rules will likely be less disrupted than smaller enterprises. For all companies, it will probably require a cultural change.\n- The asymmetric approach toward regulation between different regions could lead to a similar level of asymmetry regarding access to investment and 21st century consumer services – artificial intelligence, machine learning, and the Internet of Things.\nIt is in the interests of companies to provide comprehensive data privacy assurances. The World Economic Forum estimates that capturing the share of data privacy-conscious consumers should put about $330 billion at stake in 2015-25. The number of buyers moving to companies with strong data privacy measures is estimated to grow from 5% in 2015 to 25% in 2025.\n- Personal data is any information relating to an identified or a living person\n- The purposes of data collection are diverse\n- Cookies are the most spreading way of tracking/collecting data\n- There are five key issues for Corporates and Investors in Data Protection']	['<urn:uuid:412b27f0-b14c-48dc-95fc-1e76dc3de4eb>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:47:23.148022	8	38	1634
5	I've been wondering about how our minds work - what makes us so good at imagining different scenarios, and why do we sometimes get stuck thinking the worst will happen?	Humans have unique simulation capabilities that allow us to create mental states about the past and future, and even imagine others' intentions and feelings. This simulation ability is more complex and powerful than in other species. However, we can also experience cognitive distortions like catastrophizing, where we jump to the worst possible conclusions. This is a negative thinking pattern where we blow situations out of reasonable proportions, often making minor problems seem much worse than they actually are. Such distortions can lead to racing thoughts and excessive worry, requiring conscious effort to pause and realistically assess situations.	['Human: The Science Behind What Makes Your Brain Unique by Michael Gazzaniga\nHumans are animals. But we are particular interesting due to our niche on the evolutionary tree. While our biological systems function just as many living organism do, we have set of mental faculties that make us unique. Neuroscientist Michael Gazzaniga’s book Human is a tour de force, exploring how certain mental tools available to only humans help explain why our thinking, emotions, perceptions, and memory are different from our predecessors. For instance, consider how we can create states that simulate the past, and the future, or simulate what animals and even objects intentions and feelings are; our simulation capabilities are outmatched in complexity and power. When sensory information fails to provide enough incoming information, we as humans fill in the gaps with what has been and what can be. Simulations is only one of the many psychological mechanism reviewed by Gazzaniga has he explores a number of issues related to human consciousness, language, aesthetics, and emotions.\nHuman Journal Entry Notes:\nThis is my book summary of Human. My notes are a reflection of the journal write up above. Written informally, the notes contain a mesh and mix of quotes and my own thoughts on the book. Sometimes, to my own fault, quotes are interlaced with my own words. Though rest assured, I am not attempting to take any credit for the main ideas below. The Journal write up includes important messages and crucial passages from the book.\n- Consciousness is an emergent property and not a process in and of itself.\n- The human brain is a bizarre device, set in place through natural selection for one main purpose-to make decisions that enhance reproductive success.\n- The ability to simulate emotions from language and imagination, to alter our simulations by using perspective, and project ourselves into the future and past enrich our social world and make our simulations more powerful and complex than those of other species.\n- Much information that we use from our memories or past experiences is highly colored by our non-reflective intuitive beliefs, and some of it can be wrong. It is a challenge to separate the intuitive from the verifiable.\n- Evolutionary Psychology: Attempts to explain mental traits, such as memory, perception, or language, as adaptations-products of natural or sexual selection.\nDistinct systems may have evolved for these negative emotions (e.g. fear, disgust, anger) in order to detect and coordinate flexible responses to different ecological threats.\nOur world has changed too fast for evolution to keep up with it. More types of information are going in, but the mental modules are still triggered in the same old ways.\n- Cognition has a functional structure that has a genetic bases.\n- We have genes that code for certain adaptions, but in order to realize their full potential certain exterior conditions need to be met. “Innate ideas are incomplete ideas… Out evolved inheritance is very rich compared to a blank slate, but impoverished compared to a fully realized person.”\n- Modular brain theory: The concept of a brain with specialized circuits for specific for specific problems.\n- Our social behavior has biological origins. To survive and prosper, we had to become social.\n- Reciprocal Altruism: Selfless favors that are returned at a later date. Hunting and gathering become more efficient, providing protection from predators… larger brains meant we band together in social groups.\n- Gossip: THE social grooming tool. It fosters relationships, elicits information, maintains and reforms facial norms, and builds reputation.\n- Social behaviors correlated with neocortex size in primates:\nGrooming clique size\nTactical deception (influence without force)\nSocial skills in male mating strategy\nGroup size. (Dunbar’s Number)\n- Agency taking: We reflexively build models in our minds about intentions, feelings, and goals of others, including animals and objects. We can’t help it.\n- Many of the systems that function nonconsciously in the human brain function similarly in the brain of other animals, there is considerable overlap among species in the nonconscious aspects of the self.\n- Everyone is a hypocrite. It apparently is just easier to see from outside than the inside.\n- Most people use anecdotal evidence.\n- People don’t use information in an analytical manner.\n- Reappraise>Suppress: Reappraisal can change your physiological response, while suppression does not, and suppression decreases the experience of positive behavior.\n- Imagination: Crucial for social learning. Helps us reappraise a situation, it allows us to simulate our past emotions and learn from those experiences.\n- Our self-centered perception can lead to errors in social judgement. We need to be able to separate ourselves form others. Inhibition of our own perspective will help take others perspective.\n- Children may be born believing in essences; not taught. Does this mean our innate inference systems are made to judge?\n- The left-hemisphere: Makes sense out of all the other processes it takes in all the input and puts together a story that ‘makes sense’ (though it may be wrong).\n- Self is a byproduct. The self is a knowledge structure, not a mystical entity. Information is integrated in a unique coherent way that creates our habitualized belief of an entity known as self.\n- The ability to imitate must be innate.\n- Our agency-detection device combined with our need to explain and teleological thinking is the basis of creationism.\n- Negativity Bias: Through Natural selection, humans have been optimized for identifying the negatives of life over the positives. This is optimal for a few reason:\nNegative events are complex.\nNegative events can be contagious.\nNegative events can happen suddenly.\nNegative events are potent.\n- Our ability to use contingently true information is unique. This allowed us to be very flexible and adapt to various environments. We break out of the rigid behavioral patterns that other animals are subject to.\n- Humans are natural taxonomist.\n- Reason made the list of options, but emotion made the choice. Pure reason is not enough to make decisions. Scientist have found somatic markers for tasks that are commonly correlated with rational decision making.\n- Theory of Mind: The ability to observe behavior and then infer the unobservable mental states causing it.\n- We sit on a branch of an evolutionary tree, not on the top of a ladder.\nIf you liked what you saw. Here are 3 titles that I recommend based on what was discussed in Humans.\n- The Emotional Brain: The Mysterious Underpinnings of Emotional Life by\n- How Emotions Are Made by\n- The Blank Slate: The Modern Denial of Human Nature by\nFind the book on Amazon: Print\nNote: This page contains affiliate links. This means that if you decide to buy a product through them, I will receive a small commission. This has no additional cost to you. If you would like to support Forces of Habit, please use these links. If you do use them, thank you for the support.', 'Our thoughts are powerful. How we think influences how we feel, and those feelings directly impact how we perceive the world around us.\nFor example, under this cognitive distortion, making a minor mistake at work may lead us to jump to the conclusion that we’re about to lose our job, when in reality that outcome would be unlikely. This is when a mental health first aid course may be particularly beneficial for employees, since it can teach them to recognise their own cognitive distortions and prevent them from causing detrimental effects.\nThese interpretations may be a result of us experiencing cognitive distortions. These distortions are negative thinking patterns.\nWhat are cognitive distortions?\nCognitive distortions are negative patterns of thinking that lead to inaccurate, and often pessimistic, perceptions of ourselves and the world around us.\nThey are built from irrational beliefs and patterns of thought that become subconsciously and unknowingly reinforced over a period of time.\nIt’s often because these beliefs and thoughts are so ingrained that we find them difficult to identify and change, which is what forms the basis of the distortion. However, we are able to take control and change our patterns of thinking, as well as our self-talk, to try to see situations more objectively.\nWhat are the most common cognitive distortions?\n- Mental Filtering\nMental Filtering is a cognitive distortion where we filter anything – except the negative things – out of our perceptions.\nAn example of mental filtering would be scoring 95% out of 100% on a test or project. Under a cognitive distortion of mental filtering we may fixate on that few missed percent and let that overrule our thoughts and feelings about our achievement.\nTo combat mental filtering, we can get ourselves into the habit of identifying three positives from it instead. We can address the negative “perhaps I could have obtained that extra %”, but we can also use three positives to minimise the negative so that when we looked at it objectively, we realise it isn’t worth fixating on.\n- Jumping to Conclusions\nIt’s easy to jump to conclusions, and this cognitive distortion usually plays on our insecurities. We either assume something will happen (predictive thinking) or we presume we know what someone is thinking (mind reading).\nFor example, under this cognitive distortion, making a minor mistake at work may lead us to jump to the conclusion For example, under this cognitive distortion, making a minor mistake at work may lead us to jump to the conclusion that we’re about to lose our job, when in reality that outcome would be unlikely. This is when a mental health first aid course may be particularly beneficial for employees, since it can teach them to recognise their own cognitive distortions and prevent them from causing detrimental effects.\nRead more about the benefits of employee wellbeing in the workplace.\nBecause our conclusions are often based on personal worries and opinions, as opposed to facts or concrete evidence, they can have the severest impact on our mental wellbeing. To change this, we can instead mentally take a step back from a situation and assess other possible explanations or outcomes.\nPersonalisation is another cognitive distortion where we take the entirety of the blame in a difficult situation when in reality it may not rest solely on our shoulders.\nFor example, a relationship break down becomes entirely our fault and we blame ourselves for its failings because we see ourselves as being inadequate.\nWhile on one hand being able to identify our shortcomings and take responsibility for our actions and our circumstances is valuable self reflection, this can work against us if it leads us to believe self-fulfilling prophecies, and this can also harm our stress container. To better handle personalisation, we should once again step back from a situation to identify what was our responsibility, and identify the factors that were either out of our control or not our responsibility.\n- Black and White Thinking\nBlack and white thinking is a cognitive distortion where we see everything as all or nothing. There is no grey area or middle ground, things are just one way or the other.\nFor example, viewing a convicted criminal as an inherently bad person, despite the person’s extenuating circumstances or recent patterns of behaviour showing differently.\nBlack and white thinking is an unhelpful cognitive distortion because it forces us to develop fixed views which prevent us from seeing balanced and unbiased perspectives. These narrow views could damage relationships and lead other people to view us in a negative way. To challenge black and white thinking we must challenge our thoughts by forcing ourselves to take into account different viewpoints and interpretations of situations so that we see things more objectively.\nCatastrophizing is a cognitive distortion which is similar to jumping to conclusions. When we catastrophize we usually jump to the very worst conclusion and blow situations out of all reasonable or logical explanations.\nCatastrophizing is commonly associated with racing thoughts, which is where a person suffers from frequent worry and feels restless and agitated because they feel unable to cope with their thoughts – leading them down a negative path of catastrophizing.\nCatastrophizing can mean that we make minor or insignificant problems harder to overcome than they need to be, which takes us more mental energy and effort to then surpass. To counteract cognitive distortion, we must pause and take a moment to assess whether or not the situation is as dire as it seems.\nOvergeneralization is a cognitive distortion where broad assumptions and generalisations are made based on past experiences and minimal evidence.\nFor example, someone overgeneralising may say something like “I don’t want to sing because everyone always laughs at me”. This generalisation bases the current situation on past behaviours and experiences, where they were once laughed at, which won’t align with the present scenario.\nAs this cognitive distortion is based on ingrained past experiences, it can be difficult to overcome but the situation needs to be evaluated without fear of the past. Using our example, the person reluctant to sing should assess the types of people around them, and compare the likelihood of a past scenario reoccurring with this new audience.\nLabelling is a cognitive distortion that is similar again to overgeneralising. In this distortion we make general statements about ourselves or others based on prior behaviour.\nFor example, believing there is little to no point in trying to achieve a promotion because we have never achieved one in the past.\nLabelling is a detrimental cognitive distortion because it can lead us to believing false self-fulfilling prophecies. To work through labelling, we must objectively assess our past experiences and personal opinion and identify the evidence that disproves the label we’ve set ourselves. Using our example, someone pushing for a promotion could produce positive and recognised facts about their past yearly performances to contradict their label they have given themselves regarding never getting promotions.\n- Shoulding and Musting\nShoulding and Musting is a cognitive distortion that sets us up for failure because it involves expecting unreasonable and unrealistic demands from ourselves and other people – similar to the phrase putting someone on a pedestal.\nFor example, expecting that someone else should put the same amount of thought or effort into something because we have.\nThis is particularly unhelpful because it automatically places the other person in a negative light, and also prevents us from seeing the positive.\nWhen we reflect this cognitive distortion on ourselves, we also place added pressure on ourselves which increases our stress and harms our mindset. To counteract this distortion we can view the other person objectively and consider their actions and positives, and for ourselves we can question why we believe things should be this way.\n- Emotional Reasoning\nEmotional reasoning is a cognitive distortion where we view the world entirely based on how we’re feeling emotionally.\nFor example, waking up in an optimistic mood and viewing the world as great, or waking up in a low mood and viewing the world as depressing.\nThis cognitive perspective results in us reacting and interpreting things entirely based on our moods, which can be confusing for other people and detrimental for us because our reactions may be unpredictable. To combat this, we can evaluate whether our emotions are preventing us from seeing the situation clearly.\n- Magnification and Minimization\nMagnifying and minimising is a cognitive distortion similar to filtering. Through the gaze of magnifying and minimising we exaggerate the positive aspects or achievements of another person and minimise our own, leading to feelings of inadequacy, jealousy and even resentment.\nSocial media is a catalyst for magnifying and minimizing cognitive distortions because it alters personal perceptions of success.\nTo overcome magnification and minimisation we can turn our gaze inwards to identify and celebrate our own achievements and use those as supporting reasons as to why we are just as capable and deserving as other people.\nTake control of Cognitive Distortions and enroll in Workplace Mental Health Training from Great Minds At Work.']	['<urn:uuid:2029adfa-0c92-490d-95eb-aa9c32c03f1c>', '<urn:uuid:4932b837-12b7-4e47-a0ea-dacbe65d8caa>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	30	97	2657
6	How do Tibetan monks create sand mandalas, and what tools and materials do they use in this traditional art form?	Tibetan monks create sand mandalas through a meticulous process using specific tools and materials. They use a metal funnel called a chakpu, which is rubbed with a piece of horn to release an even flow of colored sand. They also use a wooden scraper (shinga) to straighten edges and clean up stray grains. The sand itself is made from crushed limestone dyed with pigments, which are now purchased from India. The sand is washed, dried, and sifted into three grades: fine, medium, and coarse. Before applying the sand, monks create a blueprint with chalk, starting from a central dot and drawing four lines outward. Typically, four monks work together, each taking one quadrant of the mandala and working from the center outward. The process requires extensive training - monks must study for about three years before they can create a mandala, and they specialize in only one or two different kinds after two years of intensive training.	['What is a Sand Mandala?\nA sand mandala is a two dimensional representation of an enlightened being’s place of residence and everything that is contained within it. When the Buddha taught the tantras (an advanced meditation practice), he took the form of the enlightened being whose Tantra he was teaching and emanated the complete mandala of that enlightened being. The mandala is an expression of the state of complete enlightenment and is used as an aid to meditation.\nJust seeing a mandala creates a great store of positive energy and makes one’s mind peaceful and clear. Understanding it fully means understanding the whole path to enlightenment. Each part of the mandala is rich in symbolism and reminds the meditator of the insights, states of mind and feelings he or she is trying to accomplish.\nChenrezig (the Buddha of Compassion.) embodies enlightened compassion. If each of us could become more compassionate there would be greater harmony and less conflict in our world. The kind of compassion embodied by Chenrezig is unbiased and wishes to free all living beings, without exception, from the suffering they experience. To develop compassion two factors are essential. First we must fully acknowledge our own suffering because only then will we understand that all other ordinary living beings are suffering too. The other factor is the ability to see all living beings as near, dear and lovable. Compassion will then arise quite spontaneously.\nMany people who have seen and heard His Holiness the Dalai Lama speak have been moved by the universality and compassionate nature of his words. Tibetan people consider His Holiness the Dalai Lama as an emanation of Chenrezig, the Buddha of Compassion.\nThe exact proportions and all the details are laid down in ancient Buddhist texts on the creation of mandalas. The monks carefully follow the canonical iconography because every part of the mandalas symbolises different aspects of the teachings and the realisations of the enlightened being whose mandala it is.\nOnly monks who show a special interest and aptitude are chosen by senior monks with expertise in construction sand mandalas are taught to carry on the tradition. Since each mandala is extremely complex, monks specialise in the construction of only one or two different kinds. Each chosen monk carries out two years of intensive training during which they have constructed the mandala again an again until they know the whole design by heart. When making a mandala the design is initially set down in chalk on the base before the sand is applied. If a mistake is discovered, the monk will cover the wide end of the chakpu (funnel) with a piece of cloth and suck the offending sand up into it. That part is then redrawn.\nSignificance of the Colours\nJust as the exact portions and detail are laid out in the design so are the colours. Each symbol has its exact place and colour. The colours blue, white, yellow,red, green, black, brown, orange, light blue, light yellow, light red and light green are used. In the practice of tantra, white, yellow,red and blue-black are associated respectively with peaceful, increasing, powerful and fierce activity. Such activity is not used for personal purposes but exclusively in order to help other living beings.\nTools and Materials\nThe metal funnel used is called a chakpu. When this ridged funnel is rubbed with a piece of horn, the coloured sand inside trickles out in an even flow. A wooden scraper, or shinga, is used to straighten the edges and tidy up stray grains of sand. The fact that both the funnel and the horn are needed in the process reminds us that nothing has independent existence but that everything arises in dependence on a multitude of factors.\nThe sand used in the mandala is made of crushed limestone dyed with pigments. In Tibet natural pigments were used in this process but the pigments used now are purchased from India. The monks themselves collected the stones and crushed them to form sand, which is then sifted with the use of screens in order to obtain three grades: fine, medium and coarse. The monks carefully wash and dry the sand before colouring it.\nIt is essential for the monks to remain constantly mindful and attentive throughout the whole process of construction. As they work, they try to arouse feelings of love, compassion and altruism and as they create the different parts of the mandala they contemplate their symbolism. Their wish is to give happiness to those who see it, the kind of happiness which creates positive energy.\nOften people ask how do the monks sit so still for so long, Years of practice! Day to day activities in the monastery involve long periods of sitting while studying, chanting and meditating. They often begin this at a very early age. Because they are fully absorbed in what they are doing, they are able to maintain their physical posture for a long time. It is, however, good for them to take a break periodically, during which a massage is often greatly appreciated!\nDissolving of the Mandala\nA core teaching of Buddhism is that all produced things, namely whatever comes into existence through causes and conditions, are impermanent. The dissolution of the beautiful and fragile mandala, which is the result of many hours of careful work, is meant to awaken in the mind the understanding of impermanence and non-attachment. After the mandala has been completed and its purpose fulfilled, the monks send back to their abodes the deities who had been invited to enter it. The monks run their vajras through the mandala and ‘destroy’ it. Many westerners find the destruction of the mandala almost shocking, a direct contradiction to the western ideal of having a material result at the end of a process.\nThe rituals accompanying the dissolution of the mandala include a washing ceremony to cleanse the living beings and their environment. Traditionally, the sand used for creating the mandala is poured into a river or the sea and offered to the nagas ( water spirits): non-human beings who inhabit water and who are said to possess fabulous wealth. Polluting earth or water angers these creatures, who may then cause skin diseases and other illnesses, whereas this kind of pure offering pleases them. The offering is made with the prayer that the place where the mandala was created and it’s surroundings, as well as all beings who live there may enjoy peace, prosperity and good health.\nSee more photos of the Medicine Mandala created by our monks Geshe Jamyang Sherab and Karma Gyasey, Whakatane 2014, in Our Gallery.', 'Demolishing sand castles can be great fun. But what if you had spent weeks creating it painstakingly, only to have it destroyed at the end? Heart breaking, isn’t it? But for the monks of Tibet who create exquisite sand paintings, dismantling their work is the only way. This is said to signify the impermanence of life.\nSand Mandala, the art of creating intricate artworks using colored sand, is practiced by Tibetan monks as a part of tantric tradition. In the Tibetan language, the art is called dul-tson-kyil-khor (mandala of colored powders). As a part of the sand mandala, millions of sand grains are laid painstakingly into place on a flat platform. Several monks work on a single piece, which can take days or weeks to complete. The word Mandala means ‘circle’ in Sanskrit and is said to represent the cosmogram of a Buddha or bodhisattva. This could be the monk’s own, or of the one he wishes to appease. The art includes geometric figures and several Buddhist spiritual symbols. A sand-painted mandala is used as a tool for blessing the earth and its inhabitants. It also provides the monk who practices the art a visual representation of the enlightened mind of the Buddha.\nPhoto: San Jose Library\nA typical sand mandala consists of an outer ring, inside which is a smaller square representing the ‘celestial palace’, a dwelling place of the deity. This square has four gateways representing each of the four directions. The circle as well as the square is made with several intricate layers. The square contains yet another circle, divided into 9 sectors. This circle might just repeat the pattern outside, or the central sector may contain the deity itself. If this is the case, the deity’s manifestations are represented in the surrounding sectors. Sometimes, the whole pattern may be contained within a square, with each of its corners repeating the pattern in smaller mandalas.\nPhoto: San Jose Library\nThe Tibetan mandalas are deceptively simple. They might look like they’re made up of basic patterns, but are extremely complex and might take weeks to complete. Buddhist monks undergo years of training before they can make a mandala. Since the ritual is considered to be very sacred, it cannot be done on a lighter vein. So before a mandala is made, a monk will spend time in philosophical and artistic study. Once a level of understanding has been reached, the mandala is created. In the personal monastery of the Dalai Lama, the Nyamgal monastery, monks spend about three years studying before making the mandala.\nPhoto: Josh Simerman\nTraditionally, four monks work on a single mandala, each taking one of the four quadrants. Each of these four monks has an assistant to fill in their detailed outlines with colors. All the monks begin work on the mandala from the center, moving outwards. Balance is maintained by waiting for all the monks to complete their sections, before moving on to the next. But even before they can start their sand work, the monks perform an opening ceremony. Then, a chalk is used to make a blueprint of the mandala, starting with a single dot at the very center. Four lines are drawn from this dot, after which each monk will work in his own quadrant. The blueprint is completed, the outlines filled with colored sand, made from crushed white stone and dyed. A serrated, funnel-like device is used for the application of the sand, known as chakpu. Two chakpus are scraped against each other, producing a vibration that releases sand onto the blueprint.\nBut what is most unique about the Tibetan sand mandalas is that they are destroyed after completion. The monks work methodically on this as well, brushing off the sand, pushing it all towards the center of the platform. The entire process is undone, and the sand is poured into the nearest body of water. This action is meant to teach us not to get attached to earthly objects, and symbolizes the impermanence of all things material.\nPhoto: Zlatko Unger']	['<urn:uuid:35dd81cb-ae8e-4ae6-bb2a-2c172c887318>', '<urn:uuid:c9c75d9a-0847-4958-a862-7def7dfb1e9f>']	open-ended	direct	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T19:47:23.148022	20	157	1771
7	women risk factors comparison pseudotumor cerebri thyroid nodules	Pseudotumor cerebri occurs most often in obese women of childbearing age, with the risk being almost 20 times higher in obese women under 44. For thyroid nodules, more than half of women over 50 have them, and they are two to three times more frequent in women than in men.	['What is pseudotumor cerebri?\nPseudotumor cerebri is a condition in which the pressure around your brain increases, causing headaches and vision problems. The name means “false brain tumor” because its symptoms are similar to those caused by brain tumors. It’s also known as idiopathic intracranial hypertension. This condition is treatable, but it can return in some cases.\nThe exact cause of this condition is unknown, but it may be associated with having too much cerebrospinal fluid in your skull. This fluid, which protects your brain and spinal cord, is normally absorbed into your bloodstream. Pseudotumor cerebri may occur when this fluid isn’t fully absorbed, which causes it to build up. This leads to increased pressure in your skull. This condition can affect children, men, and older adults but occurs most often in obese women of childbearing age.\nObesity is one of the leading factors that can increase your risk of developing pseudotumor cerebri. According to the Mayo Clinic, the risk is almost 20 times higher in obese women who are under 44 years old than in the general population. Children are also at risk. In fact, the U.S. Center for Disease Control reports that\nCertain medications may make you more susceptible to this condition. These include:\n- birth control pills\n- excessive amounts of vitamin A\n- tetracycline, an antibiotic\n- steroids (when you stop using them)\nOther health conditions\nHealth conditions associated with pseudotumor cerebri include:\n- kidney disease\n- sleep apnea, which is abnormal breathing during sleep marked by phases of paused breathing\n- Addison’s disease, which is a disorder in which your adrenal glands don’t produce enough hormones\n- Lyme disease, which is a chronic flu-like disease caused by a bacterium carried by ticks\nA birth defect\nSome conditions can lead to narrowing of the blood vessels in your brain. This may make you more likely to develop pseudotumor cerebri. The narrowed veins make it more difficult for fluid to move through your brain.\nA common symptom of this condition is a dull headache that starts behind your eyes. These headaches can become worse at night, when you move your eyes, or when you first wake up.\nYou may also have vision problems, such as seeing flashes of light or having brief episodes of blindness or blurred vision. These problems can become worse as the pressure keeps increasing. If left untreated, this can lead to double vision or permanent vision loss.\nOther symptoms include:\n- ringing in your ears\n- pain in your neck, back, or shoulders\nYour doctor will check for papilledema, which is swelling of the optic nerve at the back of your eye. The increased pressure in the skull will be transmitted to the back of the eye. Your vision will also be tested to see if you have abnormal blind spots.\nYour doctor may perform a CT or MRI scan of your brain to look for signs of spinal fluid pressure. These scans can also be used to check for other conditions that could be causing your symptoms, such as tumors or blood clots.\nA CT scan combines several X-rays to make a cross-sectional image of your brain. An MRI scan uses magnetic waves to produce a highly detailed image of your brain.\nYour doctor may also perform a spinal tap, or lumbar puncture, to measure the pressure of your spinal fluid. This involves placing a needle between two bones, or vertebrae, in your back and drawing a fluid sample for testing.\nMedications can help control or reduce the symptoms of pseudotumor cerebri. Your doctor might prescribe the following:\n- Migraine medications can provide headache relief. These can include triptans like sumatriptan (Imitrex) and naratriptan (Amerge)\n- Glaucoma drugs, such as acetazolamide (Diamox), cause your brain to produce less cerebrospinal fluid. These drugs can cause fatigue, kidney stones, nausea, and a tingling sensation in your mouth, toes, or fingers.\n- Diuretics, such as furosemide (Lasix), make you urinate more often. This causes you to retain less fluid in your body, which helps ease the pressure in your skull. These may be used in combination with glaucoma drugs to make them more effective.\nYour doctor may recommend surgery if your vision becomes worse or if they need to drain excess cerebrospinal fluid.\n- Optic nerve sheath fenestration: Optic nerve sheath fenestration involves cutting the membrane around your optic nerve to let extra fluid out. According to the Mayo Clinic, it’s successful at relieving symptoms more than 85 percent of the time.\n- Spinal fluid shunt placement: A spinal fluid shunt procedure involves placing a thin tube in your brain or lower spine to drain extra fluid. The excess fluid is shunted away, typically to the abdominal cavity. This procedure is usually done only in severe cases. According to the Mayo Clinic, it has a success rate of more than 80 percent.\nOther forms of treatment\nOther treatment methods include losing weight and having multiple spinal taps performed to relieve pressure.\nYou’ll need to see your eye doctor regularly to have your vision checked once the pseudotumor cerebri is gone. Your eye doctor will watch you closely to make sure that you don’t continue to have vision changes that could result in permanent vision loss.\nYou should also let your primary care doctor know if you start having symptoms of this condition again.\nGaining weight puts you at a higher risk of having a pseudotumor cerebri. You can help prevent this condition by losing excess body weight and keeping it off. Switching to a healthy diet and getting regular exercise can help you drop the extra weight.\nYour diet should include plenty of fruits, vegetables, and whole grains. You should also choose lean meats and dairy products that are low in fat. Limit or avoid eating foods that are high in:\n- added sugars\n- saturated fat\n- trans fat\nAdopt a regular exercise routine, which can be as simple as walking. You can follow a more vigorous workout routine if your doctor says it’s safe to do so.', 'What should I know about the thyroid?\nThe thyroid is a butterfly-shaped endocrine gland located in the middle of the neck, in front of the trachea, or windpipe. It has two lobes, one on either side of the trachea, below the larynx. The role of the thyroid gland is to produce hormones that are released into the bloodstream to regulate the function of several organs.\nFour small and very fragile glands located behind the thyroid – the parathyroid glands – regulate calcium levels in the body.\nTwo small nerves that control vocal cord movement are attached to either side of the back of the thyroid gland. In some cases, damage to these nerves can result in a modification of the voice.\nRegulation of hormone production\nThyroid hormones influence the stimulation of cellular activity, regulating, for instance, heart rate, weight, body temperature, sleep patterns and energy levels. They are essential for the healthy functioning of many vital organs, at all stages of life.\nT3, T4, TSH\nThe two main hormones produced by the thyroid are T3 (triiodothyronine ) and T4 (thyroxine). Two structures in the brain regulate synthesis of these two hormones: the hypothalamus and the pituitary gland. The hypothalamus influences the pituitary gland, which in turn influences the thyroid by secreting TSH, a crucial hormone in the regulation of T3 and T4 hormone secretion. This mechanism ensures a very fine-tuned regulation of thyroid hormones in the blood, thereby enabling the body to function properly.\nWHAT ARE THE DIFFERENT THYROID DISORDERS?\nHyperthyroidism is an abnormal increase in thyroid hormone production, resulting in high T3 and T4 levels and a low TSH level. When the thyroid is overactive, it produces too many hormones which in turn cause the organs to become overactive, leading to certain functional disorders. Hyperthyroidism occurs more frequently in women than in men, and the intensity of its many symptoms varies from one individual to the next.\n- An autoimmune disease with antibodies that attack the thyroid, causing inflammation (thyroiditis) and hyperactivity of the thyroid (Graves’ disease, and sometimes Hashimoto’s thyroiditis in its early stages)\n- Toxic hyperfunctioning thyroid nodule\n- Thyroid goiter\n- Graves’ disease\nThis autoimmune disease usually develops in young women between the ages of 20 and 30. The body produces antibodies that stimulate the thyroid gland, causing an overproduction of hormones. The diagnosis is based on a clinical analysis and the presence of hyperthyroidism symptoms such as weight loss, anxiety and an increased heart rate. An ultrasound reveals a diffuse enlargement of the thyroid gland (goiter), while blood tests reveal specific thyrotropin receptor autoantibodies (TRAb) in the blood.\nThe disorder usually requires medical treatment. Antithyroid drugs are used to block hormone production. These drugs are contraindicated for women hoping to conceive; surgical ablation is proposed as an alternative. In some cases, radioactive iodine therapy (in tablet form) is prescribed.\nIn 50 percent of cases, the disorder disappears on its own after several months, but the recurrence rate remains high. When treatment is no longer effective, the patient must undergo a thyroidectomy and begin synthetic thyroid hormone therapy.\n- Early stages of Hashimoto’s thyroiditis\nThis illness manifests after antithyroid antibodies are detected. In its early stages, the illness is characterized by hyperthyroidism, after which the gland gradually becomes smaller (hypothyroidism). It is diagnosed using the same method as for hyperthyroidism, and treatment is usually medical.\n- Toxic hyperfunctioning thyroid nodule\nThis is when a thyroid gland nodule increases thyroid hormone production without regulation by the pituitary gland. Often discovered accidentally during a neck palpation, it is diagnosed using a thyroid scan, also known as scintigraphy. The proposed treatment is oral radioactive iodine therapy. The iodine binds to the thyroid gland, or to the adenoma only, and destroys it. Normal thyroid function, known as euthyroidism, gradually returns. Because iodine-131 is a radioactive isotope, it emits radiation and decays spontaneously. Iodine-131 is the firstline treatment in the United States and Canada. In France, it is most frequenly used to treat hyperfunctioning nodules. In the event of recurrence and/or if the patient is trying to conceive, surgical ablation is proposed first.\nHypothyroidism is when secretion of the T3 / T4 thyroid hormones decreases while a negative feedback loop causes TSH production to increase.\nWhen the thyroid is underactive, it produces fewer hormones, meaning the organs are less stimulated and in turn become underactive as well. This disorder usually develops in women after the age of 50.\n- Hashimoto’s thyroiditis is the most frequent cause of secondary hypothyroidism\n- Certain drugs designed to treat heart disease\nHypothyroidism is diagnosed with an ultrasound and blood tests confirming the presence of antithyroid antibodies.\nTreatment consists in taking thyroid hormones. Regular monitoring is necessary to adjust the levels.\n97 percent of the time, hypothyroidism (insufficient secretion by the thyroid gland) is treated with a synthetic version of the thyroid hormone thyroxine. Only 3 to 5 percent of patients are treated with a different synthetic hormone of which T4 accounts for 80 percent T3 accounts for 20 percent.\nThyroid nodules are a very common disorder. An estimated 50 percent of the population has a thyroid nodule larger than one centimeter. Several types of nodules exist, and in 90 to 95 percent of cases they are benign.\nMicro-nodules: less than one centimeter. They only require monitoring, as they have little chance of being cancerous.\n- Hyperfunctioning “toxic” nodules (see above): generally benign.\n- Non-functioning nodules: benign tumors that do not produce hormones and are therefore harmless.\n- Cancerous nodules: rarer but require surgical removal. Nearly 25 percent of the time, a nodule larger than four centimeters is malignant and needs to be removed without delay. When there is a high number of nodules, the challenge is to detect and eliminate any cancerous cells.\nA goiter is a diffuse enlargement of the thyroid gland which may appear as unattractive swelling at the base of the neck.\nThere are different types of goiter:\n- Single goiter: affects four to five percent of the population, is often hereditary and relatively common during adolescence.\n- Nodular goiter: increased thyroid volume due to multiple nodules. The risk of cancer rises with the number of nodules. Monitoring becomes increasingly difficult as nodule number and size increase.\nWhat are the risk factors of hyperthyroidism and hypothyroidism?\n- The risk factors of hyperthyroidism are a family history of this disorder, iodine deficiency and overexposure to pesticides and chemical pollutants.\n- The risk factors of hypothyroidism are a family history of this disorder, iodine deficiency and certain medications.\n- Thyroid disease affects more than 15 percent of the French population.\n- More than half of all women over 50 have thyroid nodules, which are two to three times more frequent in women than in men.\n- Close to five percent of nodules are cancerous.\n- Around 4,000 thyroid cancers are detected annually in France. This figure increases continuously, by approximately six percent per year. 40,000 patients undergo thyroid surgery annually.']	['<urn:uuid:be12aae8-d039-4ad6-b73b-aacc273ee2f3>', '<urn:uuid:302e2924-de00-4126-a1a0-df23426b2fe5>']	factoid	direct	long-search-query	similar-to-document	comparison	novice	2025-05-12T19:47:23.148022	8	50	2160
8	How can heart rate monitoring help swimming coaches?	Heart rate monitoring can help determine if a swimmer is sick, stressed, over worked, needs more rest or is out of shape. It can be used to assess aerobic conditioning, fatigue during holiday training, and amount of resting needed for race pace or sprinting, particularly during taper and resting before meets.	['1. Racing – The drive to win close races to recover from mistakes and overtake your competition, the desire to win!!!! Some swimmers have that desire and others must be taught. You must add racing sets in your workout. Each swimmer must have the ability to start and stop speed (variable speed) thru out the season no matter what phase of training you are currently involved. Drafting then passing, stagger starts and racing different abilities of swimmers in practice must be some part of a weekly routine.\n2. Race Pace – This isn’t sprinting to exhaustion but creating the speed that will be needed to achieve goal times for each event. Let’s take the 100 free for our example: John’s goal is to swim a 48.00 in the 100. In order to achieve this swim we must create and instill muscle memory in john to help him achieve this goal. John will need to maintain 12.00 while swimming 25’s and 24.00 speed while doing 50’s. You can eventually work up to 75’s and broken 100’s (breaking them at different distances) and finally a 100 from the block before he actually swims his big race. This will give John the confidence he needs for his big race. Start the season with enough rest at each desired distance to achieve race pace speed and as the season continues change the rest interval and achieve the same result. If John is tired on a given day that you want him to do race pace then you must give him enough rest to achieve race pace. This doesn’t have to be the main set but just the last 10 minutes of a desired workout. Please remember to do race pace during the aerobic phase of the season and during holiday training. Race pace develops muscle memory and helps create speed and power. Remember that your dryland program is important and must coincide with this type of training. You will do more race pace as the taper progresses. Recovery and over-speed sets are as important and must be incorporated in workouts with race pace. Training with speed and power in the water and during dryland will enhance everything you are trying to achieve in your program.\n3. Over-speed – Creating and enhancing muscle memory with the possible speed of a time not yet achieved in a race. Creating speed in short spurts helps train the fast twitch muscle make-up of every swimmer. Cords is wide spread example usually incorporated during the taper or resting phase of a season and should be used thru out. Over-speed can be achieved off starts and walls and during very short distances or with correct Tarzan swimming.\n4. Tarzan for speed purposes – Swimmers that do water polo use Tarzan to see the ball. They are strong, have arm speed, upper body strength and usually are great at kicking. Wow, everything you need for sprinting!!!!! Sprinting doesn’t always mean short distances. 200’s are now in the sprint category. If you have ever seen Diana Munz swim she has great kicking skills that were evident in her swims off each wall and at the end of distance events. She shows variable speed and power in the distance events with her upper body and legs…………..\n5. Recovery and dryland – These two categories make most coaches nervous. I understand the thinking of overtraining, as I have to constantly trust what I believe and not slip into the way I was trained to over train. Proper recovery must be part of each workout phase and the dryland program must match recovery and must constantly change body part emphasis to ensure recovery. Hard work should alternate legs, core and upper body. That doesn’t mean if you are recovering the legs you can’t work the arms, etc…. You can even alternate upper and lower body between dryland and swimming as well as in each set! You can alternate within each set, from set to set from work –out to workout, week to week. Add a true recovery workout once during the week and see how your swimmers respond the next day. Maybe recover for an hour and surprise them by sending them home early, you have now added MENTAL RECOVERY.\n6. Each set should include distances as well as the repetitions, mechanics emphasis, and what to do on each part of the swim. For example: 6 X 400’s on 5:00 free with 4 fly kicks off each wall breathing to one side of the pool (to insure breathing n both sides and even shoulder rotation) and make-sure the swimmers know why. Odd swims are variable speed 75% – 95% by 50, with numbers 2 and 4 pace holding 1:02 and number 6 being timed with sprint kicking each wall and last 200. Write it down for them to read and repeat it to them verbally as you know it’ll take many times to get it thru their heads.\n7. Make sure your swimmers can read the clock and understand negative, even, ascending, and pace terminology for splits in races and practice. Swimmers should constantly be using the clock even during warm-up and warm-downs so times and speed can be inherent. They must understand when you tell them they need to go out in a certain speed for a race strategy or tell them next time to go out faster or slower. Most swimmers shake their head OK but actually have no idea how to actually physically do it. They understand the concept mentally so you must incorporate this in your training sets. Simple example during warm-up 4 X 200’s with descending send-off tell swimmers to just make the send off as it descends with a goal time on the last 200 that is easy to achieve with some effort in order to reinforce feel of that speed in the water. 4 X 200’s 2:40, 2:30, 2:20 and the last one go a 2:15.\n8. Just FYI and my opinion about certain types of equipment: Paddles – I like paddles but it does take away the feel of the water temporarily from the swimmers. Please incorporate a longer warm-down after a paddle set to give the swimmer time to regain feel. Pull-boys – They are made with different styles that fit each person differently and usually result in bad body position during a set. Please watch for this, as pull-boys don’t promote streamline body position. Kickboards – again different styles and sizes. Please don’t do all your kick sets with boards as this takes the body out of proper streamline position. Kickboards are good for isolating the legs. Cords for dryland and over-speed – Good for cross training as talked about previously. Please check the cords as chemicals do eventually cause the cords to snap and that’ll hurt!\n9. Coaches flexibility: Stay flexible and evaluate if the swimmers are getting what you wanted out of each set. Don’t force the issue if motivation isn’t the issue. Change the set to achieve your goal, scratch the set if needed, adjust it or use it later in the season. If you change the set explain why and try to get them to understand the reason. If you can’t explain it you’ll never be able to teach it. If too much info is written for the set slowly increase the stimulus over time. Flexibility is hard as a coach feels the time constraint to get it all in. Fight that urge and back up, as that’ll help the swimmers more in the short and long term.\n10. IM (Individual medley) and the importance of doing sets in IM order. Training the muscle memory of going from one stroke to the other and breathing patterns. Breathing patterns change from one stroke to the other, as does the timing of each stroke. If the swimmer doesn’t understand that controlling the breathing of each stroke will help the success of the IM swim then breathing will control the swim detrimentally. When switching each stroke the swimmer must gain control of the breathing pattern before settling into the race strategy of each stroke. When control of the breathing pattern is achieved then the swimmer can work on what the coach desires for each stroke in the IM based on each swimmers strengths and weaknesses. Each swimmer should be able to negative split each part of the IM. For example in the 400 IM splits should be fly – 28-28 as the start affects the first split, back – 30-29, breast 34-33, free 27-26. The 200 IM would be a controlled sprint, as race pace should be incorporated in practice for this event. Share splits with your swimmers before and after each race. Each swimmer should have practiced what you want in the meet at practice.\n11. Old school breathing – Have you ever been told to only breath 3 times each lap in the 200, hopefully not. Breathing is your friend in events longer than a 50 and helps the body alleviate the pain you feel in your body by exhaling and eliminating the lactate acid your muscles are producing as a by-product of the work you are demanding of them. Breathing too much or poorly (mechanically incorrect) in the 50 can slow you down. Breathing is important in the timing or the breast and fly and essential to the backstroke. Breathing is covered in mechanics of each stroke. You can’t expect a swimmer to change or do anything different is a meet that you don’t train at practice. Please keep that in mind ……… Breathing should never compromise streamline position especially before and after each wall or flags to flags.\n12. Heart rate is a great tool to see if your swimmer is sick, stressed, over worked, needs more rest or is out of shape. You can measure this many ways by creating a set that helps the swimmer with maxing heart rate and measuring how long it takes to recover. Remember that I am not a doctor nor should you diagnose a swimmer from this, it is only a tool and can be used to help you at each phase of training. This tells you about aerobic conditioning, fatigue during the holiday training and the amount of resting needed to create race pace or sprinting and definitely during taper and resting before meets. Consult a doctor or read up about heart rate, as there are plenty of studies and info on the subject. This will help you with flexibility and changing your workouts when needed.']	['<urn:uuid:12388727-f0c9-43a8-8bbc-99ef97e3c6ae>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:47:23.148022	8	51	1754
9	What is the main difference in height between a comfort height toilet and an ADA-compliant water closet - are they actually different things or the same?	Comfort height toilets and ADA-compliant water closets are actually the same thing. They both measure between 17-19 inches in height with the toilet seat included. These toilets are also known by different names across manufacturers, such as Universal Height (Toto), Right Height (American Standard), or chair height. The requirements specify that the rim should be at 16-1/2 inches, ensuring the seat is at least 17 inches above the floor - which is the ADA minimum.	['Years ago, I learned a lesson that you can’t have a flush valve tailpiece trap primer on an ADA water closet because it raises the height of the flush valve and causes a conflict with the ADA grab bar. This was back in the day when manual flush valves were the norm.\nToday, of course, sensor operated flush valves are the norm for commercial installations – either battery operated or hard wired. Meanwhile, the criteria for ADA water closets has changed – or at least the enforcement of it has.\nThe height requirement for ADA grab bars is 33 to 36 inches measured from the floor to the top of the grab bar, not the centerline. The grab bar is 2 inches in diameter and has a requirement of 1-1/2-inch clearance all around. This is where the fun begins.\nFor some reason I have been specifying (and my company has been installing) ADA sensor operated water closet flush valves and no problems have arisen - unless there was a physical conflict with the grab bar. As of late, projects have ADA specialists who check clearances for all the ADA elements before project closeout – another niche industry adding complexity, cost, and frustration to the construction process. On one recent project all of the ADA flush valves had a fractional interference with the grab bar clearance ranging between 1/8 and ¾ inch. All of the ADA flush valves had to be replaced with a shorter model to resolve the issue. It was a complete waste of money.\nIf you add up the dimensions, the typical wall mounted ADA water closet is installed with the rim at 16-1/2 inches, and floor mounted ADA water closets are also 16-1/2 inches tall to ensure that the seat is at least 17 inches above the floor – the ADA minimum.\nOf the sensor operated flush valves I have researched, Sloan’s product is 16-1/2 inches tall, American Standard’s is 16-1/8 inches tall, and Zurn’s is 16-3/8 inches tall. The Toto product lists no dimension to the top of the valve, but it has an installation note about mounting the grab bar at the maximum 36 inches or using a split grab bar for ADA installations. I would bet my paycheck that the valve interferes with the 1-1/2-inch clearance requirement at the grab bar.\nEven with the shortest valve of those mentioned, adding the 16-1/8 inches plus the 16-1/2 inches fixture height gives you 32-5/8 inches. Looking at the grab bar, 36 inches less the 2-inch diameter and 1-1/2-inch clearance gives you 32-1/2 inches. So in a perfect world, even the shortest sensor flush valve I could find has a 1/8-inch clearance conflict with the ADA.\nThere is a solution to this issue. If you specify a retrofit style flush valve such as the Sloan Optima Plus or the Zurn AquaSense the valve will be no taller than a manual flush valve. But, it is frustrating having to specify a retrofit style flush valve for a new installation.\nI am not naming particular manufacturers to slander any one of them – I am merely pointing out that this is an industry wide issue. In today’s age with barrier busters gone wild, all of the new sensor operated (non retrofit style) flush valves need to be less than 16 inches tall to meet ADA requirements. Realistically, they need to be about 15-1/2 inches tall to allow for field tolerances, and none of them are so designed.\nThe ADA does have an allowance for interruption of the grab bar – creating a two-piece grab bar – for installation of the flush valve controls. If you ask the manufacturer that is the solution: to interrupt the grab bar. But of course the architect and general contractor will not want to do this for both aesthetic and financial reasons. Plus, it’s just plain silly, to interrupt a grab bar where no physical conflict exists. That would only make sense for a bedpan washer flush valve installation that is tall and creates a physical conflict.\nI have appealed to local ADA specialists, asking the question of the difference between the interruption of the clearance and the interruption of the grab bar itself. What’s the difference? If the ADA allows for interruption of the grab bar, why is the interruption of the clearance any different? But of course my logic falls upon deaf ears. Rules is rules. And there is no elegant solution.\nTo make matters worse, if your project has LEED point requirements that weigh heavily on water conservation, the use of 1.1 GPF water closets is of great benefit. The only manufacturer of 1.1 GPF toilets and flush valves that I am aware of is American Standard. However, that flush valve does not meet the clearance requirements dictated by the grab bar – it is 1/8 inch too tall - so you cannot use it for ADA installations. This is the very same flush valve we had to remove on a recent project due to clearance conflicts. None of the retrofit style flush valves that meet the ADA clearance requirements have a 1.1 GPF option, so the dilemma gets further complicated.\nI am hoping that the industry will soon respond by producing flush valves that are 15-1/2 inches tall to resolve this issue. I realize that product redesign is costly, but so too is the price of removing and replacing new product to satisfy the petty nuances of the ADA.\nTimothy Allinson is vice president of Engineering at Murray Co., Mechanical Contractors, in Long Beach, Calif. He holds a BSME from Tufts University and an MBA from New York University. He is a professional engineer licensed in both mechanical and fire protection engineering in various states, and is a LEED accredited professional. Allinson is a past-president of ASPE, both the New York and Orange County chapters. He can be reached at firstname.lastname@example.org.', 'A toilet is a handy fixture that should be bought after thorough considerations. Toilets come in different sizes, types, and prices. A toilet that is good for a short person may not be ideal for a tall person. Also, a toilet that is good for a kid may not be comfortable for a disabled person. As a result, it is important to find the best toilet for your needs as they vary from one person to another. Standard Height toilet and Comfort Height toilet has some basic differences. Read this Comfort Height toilet VS Standard Height toilet to find out the answer.\nIn this comparison review, we will help you find out which toilet is best suited for your needs. Other factors to consider when selecting a toilet are the type of bowl, one-piece or two-piece, flushing system, and ease of cleaning.\nFeatures Comparison: Comfort Height Toilet VS Standard Height\n|Toilet Height Without Seat:||16-18 inches||14-15 inches|\n|More Comfortable For:||All People Including Tall adults, elderly and disabled||Short people, Children|\n|Toilet Height With Seat:||17-19 inches||15-16 inches|\n|Known As:||Universal Height(Toto), Right Height(American Standard), ADA Compliant Height, Chair height||Regular Height|\n|Pricing:||Less Expensive||Slightly Expensive|\nWithout wasting any more time, let’s find out which toilet height is suitable for your needs.\nDetails of the Comfort Height Toilets\nComfort height toilets are also called chair height, ADA height, or universal height. They usually measure between 17-inches and 19-inches with toilet seat included. These toilets can be found in restrooms, offices, and at home.\n- It has ADA height\n- It is affordable when compared to standard height toilet of the same model and brand\n- Best suited for the elderly and the disabled\n- Not ideal for kids and shorter individuals\n- The feet can swing leading to loss of circulation\n- It can increase the probability of constipation\nDetails of the Standard Height Toilets\nStandard height toilets are also referred to as regular height toilets. They are usually shorter than comfort height and measure 15-inches. These toilets are installed at home and are not allowed in restrooms.\n- It is suitable for kids\n- Short will have an easy time sitting down or standing up from the toilet\n- It features a more natural position that prevents constipation\n- It can be challenging to sit down or stand up for most adults\n- It is usually slightly expensive\nSimilarities and differences between Comfort Height and Standard Height toilet\nSummary of content\n- 1 Features Comparison: Comfort Height Toilet VS Standard Height\n- 2 Details of the Comfort Height Toilets\n- 3 Details of the Standard Height Toilets\n- 4 Similarities and differences between Comfort Height and Standard Height toilet\n- 5 Which is better? Comfort Height Toilet or Standard Height Toilet?\nAs their name suggests, the key difference between comfort and standard height toilets is their height. A comfort height toilet measures somewhere between 17-inches and 19-inches, while a standard or regular height toilet measures 15-inches. Also, there is a tall height toilet for tall people, and it measures 20 to 21-inches.\nWhen choosing a toilet for your home, always consider your family members. If you have a person with mobility issues, you should get a toilet that he or she is comfortable with. And if you’ve two or more bathrooms in the house, this can be convenient as you can install different toilets to suit different people in the house.\nEase of installation\nBoth toilets are typically installed in the same way. So, it will depend on the type of toilet that you’re installing. If it’s a one-piece toilet, mounting will be much easier when compared to a two-piece toilet. This is because the tank and bowl are incorporated into one unit. So, you don’t have to go around bolting the tank to the bowl.\nThe height of the toilet doesn’t play any role in how they look. Some toilets come in a one-piece design while others in a two-piece design. Also, some features an elongated bowl while others have a round bowl. One-piece toilets typically look lovelier than two-piece toilets. Besides, different brands offer different colors. However, white is the most notable, and it blends with modern bathrooms easily.\nIf you don’t want to have challenges when relieving yourself, then you’re better off with a standard height toilet. These toilets help in eliminating waste as the user sits in a position that makes evacuation simple. When seated on the toilet, the hips are slightly below the height of the knees in a squat position. In this position, the anorectal angle, the posture of the rectum in connection to the anus, is at the optimum position for natural defection. This is not the case with ADA height.\nStandard height toilets are slightly expensive than comfort height toilets. Try to compare toilets from one brand but with the same features but different height. When buying a toilet, always consider the cost of installation. Some units are more costly than others.\nWhich is better? Comfort Height Toilet or Standard Height Toilet?\nWith this Comfort Height VS Standard Height toilet review, you shouldn’t have a problem choosing the best toilet for your bathroom. A toilet with a standard height is ideal for kids and short people. On the other hand, a toilet with a comfort height is suitable for tall people and those with disabilities.\nIt is also crucial to note that comfort height toilets are ADA compliant, while standard height toilets help to prevent constipation. Apart from considering height, it would help if you also looked at the comfort that the toilet offers. Some feature an elongated bowl while others round bowl. This will help to make your toilet even more comfortable.\nOverall, both the comfort and standard height toilets are explicitly made for certain people. So, choosing the right toilet will ensure that you’ve comfort and ease of standing up or sitting down on the toilet. Make the right choice!\nStill, have questions after reading this Comfort Height Toilet VS Standard Height? Comment below.']	['<urn:uuid:d7de7193-4a8f-46ed-9c84-b30986949507>', '<urn:uuid:b7947955-3511-4b89-8532-31e2ab7453a7>']	open-ended	direct	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T19:47:23.148022	26	75	1979
10	What's the key difference between how oxygen is handled in a P0171 problem versus a catalytic converter issue?	In a P0171 condition, the engine is receiving too much oxygen (running lean) with a ratio exceeding the ideal 14.7 air to 1 fuel ratio, which the ECU tries to compensate for. In contrast, with catalyst system issues (P0420), oxygen is actively managed through a specialized oxygen storage bed coated with cerium that stores and releases oxygen for use in the converter's oxidizing bed, which is essential for converting harmful emissions into safer compounds.	"['Meaning of P0171 Generic OBD2 Trouble Code\nSkip to the good stuff:\nThe technical definition of P0171 code is: System Too Lean in Bank 1\nThis means the engine – specifically in bank 1 – is running lean (similar to code P0174). It is either the engine is receiving too little fuel or possibly too much air, hence resulting in an overly lean running condition.\nThe P0171 generic OBD2 DTC is usually triggered by the front O2 sensor, or oxygen sensor #1 (similar to P0135). Once the ECU detects the sensor is detecting too much oxygen in the exhaust, it will try to compensate by maintaining the ideal burn ratio of 14.7 Air to 1 Fuel. If the ECU is unable to maintain this ration, it will trigger a check engine light (MIL) and display a P0171 code on your OBD2 scanner.\nWhat are the causes of P0171 OBD-II code?\nThe possible causes of a P0171 generic fault code are many! Here’s the most common causes of the P0171 DTC that we’ve encountered:\n- Dirty or faulty Mass Airflow Sensor (MAF)\n- Dirty or faulty Oxygen O2 sensor\n- Clogged fuel filter\n- Vacuum leak\nThe first thing to check is the air filter or air cleaner element. If the filter is clogged or excessively dirty, this is enough to impede or restrict the air flow, hence resulting in a lean running condition.\nBesides a dirty air filter, the P0171 code can also be blamed on a dirty or faulty MAF sensor and a clogged fuel filter. Cleaning the MAF with some brake cleaner should help isolate or rule this one out.\nThe code can also be attributed to a faulty or stuck PCV valve. In some cases, the trouble code is also triggered by a sticky or failing fuel injector. If the motor is not getting enough fuel, the trouble code can also be blamed on a plugged or dirty fuel filter.\nBut in some cases, it can also mean the existence of a vacuum leak in the MAF sensor or PCV connections. The bad news is diagnosing a vacuum leak is tricky without using specialized tools like a Smoke Tester. There are also times when the fault is caused by a faulty O2 sensor as well.\nWhat are the symptoms of P0171 DTC?\nThe most obvious symptom is the presence of a check engine light in the dash. This is followed by engine problems such as engine knock during hard acceleration, rough or problematic idling, hesitation or surging, and a general lack of performance.\n- Check engine light (MIL)\n- Slow throttle response\n- Rough idle\n- Many others\nHow much does it cost to fix P0171 Fault?\nThe cost will depend on the source of the problem. And unfortunately because of the nature of this code, you’ll spend some time isolating the problem.\nThe first thing to do is to replace or clean the air filter and fuel filter. While you’re at it, you should also check the MAF sensor. If the sensor is dirty, clean it using a sensor cleaner. In most cases, cleaning the air filter and MAF sensor is enough to solve the problem. But if the MAF sensor if faulty or damaged, you need to spend around $40 to $80 for the sensor alone, after labor costs you could be looking at $150.\nIf that still doesn’t solve the problem, you want to check for vacuum or exhaust leaks. This is the part that sucks.\nSince the P0171 DTC can be caused by a variety of faults, it is important to follow the proper diagnostic procedures. Replacing the fuel filter and air filter will set you back $30 to $50 each plus labor. If the problem is caused by a vacuum leak (due to a broken or cracked rubber hose), the repair bill could be lower.', 'Catalyst System Efficiency Below Threshold\nOur emissions expert has put together the following information about the P0420 fault code. We have also included diagnostic procedures you can take to your repair shop if the mechanic is having difficulty analyzing the code.\nOBD II Fault Code\n- OBD II P0420\nFault Code Definition\n- Catalyst System Efficiency Below Threshold\n- Check Engine Light will illuminate\n- In most cases, there are no adverse conditions noticed by the driver\n- In some cases, there may be some performance problems noticed by the driver such as a lack of power from a restricted and/or damaged Catalytic Converter\nCommon Problems That Trigger the P0420 Code\n- Inefficient Catalytic Converter(s)\n- Defective Front or Rear Oxygen Sensor(s)\n- Misfiring engines\n- Oxygen Sensors\nPolluting Gases Expelled\n- HCs (Hydrocarbons): Unburned droplets of raw fuel that smell, affect breathing, and contribute to smog\n- CO (Carbon Monoxide): Partially burned fuel that is an odorless and deadly poisonous gas\n- NOX (Oxides of Nitrogen): One of the two ingredients that, when exposed to sunlight, cause smog\n- Volkswagen , Audi , Dodge and Toyota have extended their catalytic converter coverage on many of their vehicles. To find out more about this information on RepairPal.com, enter P0420 followed by your vehicle make in the search bar above, for example, P0420 Audi.\n- Most Toyota vehicles need an OEM Catalyst, especially if there is a Catalyst-related software update required. Toyota vehicles usually re-set the P0420 code unless an OEM catalyst is used as a replacement. It is common for mechanics to replace all the Oxygen sensors only to find that the Toyota vehicle required an OEM Catalyst.\n- Most of the aftermarket Converter companies are re-working their designs so they can be certified for use in OBD-II vehicles in California\nThe Catalytic Converter looks like a muffler. It is typically a stainless steel housing over a ceramic honeycomb core. The catalyst itself is made of platinum, palladium, or rhodium, all rare metals, which is why Catalytic Converters are so expensive. These elements reduce the toxicity of harmful exhaust gases that are expelled from the tail pipe. Catalytic converters are quite efficient, but if engine maintenance is neglected or an engine is allowed to ""run rough,"" damage could occur, resulting in costly repairs. To replace the Catalytic Converter, the vehicle is raised to gain access to its underside. The converter is removed from the exhaust system and the new Catalytic Converter is installed.\nWant to Learn More?\nThe Catalytic Converter is a sophisticated after-burning device designed to complete combustion of the exhaust gases that pass through it. It is a stainless steel container with an inlet and outlet pipe that looks similar to a muffler. Inside, the Catalytic Converter is a ceramic monolithic structure that has honeycomb-like passages running though it. This structure has several sections called beds that are thinly coated with rare metals, which react with the compounds in the exhaust gases to complete the combustion process, thereby cleaning the exhaust of harmful emissions.\n- The first section of the Catalytic Converter is called the reduction bed and is coated with rhodium. It is called the reduction bed because its purpose is to reduce the NOx gases back into harmless nitrogen and oxygen.\n- The next section of the Catalytic Converter is the oxygen storage bed, which is coated with cerium. Its purpose is to maintain an ideal level of oxygen for use by the rear of the converter. It does this by storing and releasing the oxygen that gets released from the reduction of NOx in the previous reduction bed.\n- The oxygen is then available for use in the final oxidizing bed, which is coated with platinum and palladium. The purpose of the oxidizing bed is to complete the combustion of CO by adding oxygen. The oxidizing bed also uses oxygen to burn any of the raw HCs that still remain in the exhaust gases.\nP0420 Diagnostic Theory for Shops and Technicians\nThe P0420 code is set when the Catalyst monitor sees a decrease in voltage from the rear Monitoring Oxygen Sensor(s) and an increase in switching activity—from rich to lean to rich, etc.—that closely resembles the front Oxygen Sensor(s) during the time the computer is activating the Catalytic Converter monitor test. The voltage threshold is usually a minimum of 650 millivolts, which indicates a low level of oxygen. When the voltage goes too far below the 650 millivolt minimum, it indicates a higher level of oxygen. This means that not all of the oxygen is being consumed by the combustion process or by the afterburning effect of the Catalytic Converter. When the oxygen level gets too high, it means that the Cerium or Oxygen storage bed has degraded to the point where it is no longer able to store oxygen created by the reduction of NOx (nitrogen and oxygen). This oxygen is essential for the rear Oxidation bed to complete the conversion of CO into CO2 and HCs into H20 and CO2.\nCommon Tests for Diagnosis of the P0420 Code\n- Retrieve the code and write down the freeze frame information to be used as a baseline to test and verify any repair.\n- If there are any misfires, ignition, fuel and/or intake problems, these must be repaired before the Catalyst code is addressed. Any misfire, ignition, and/or fuel system problem will quickly ruin a Catalyst. They are often the cause for code in the first place.\n- Test drive the vehicle at or near the freeze frame conditions to verify that the rear Catalyst Monitoring Oxygen sensor is either mirroring the front Oxygen sensor and/or is not reaching the 650 millivolt threshold during 55–60 MPH cruise conditions. If either of theses conditions can be easily verified, then the Catalytic Converter is defective.\n- If there is any doubt about the condition of the front and/or rear Oxygen sensors, then check the Mode 6 data for all the Oxygen sensor monitor tests. If any of the front or rear Oxygen sensors barely pass their Mode 6 tests, then clear all codes and perform a drive cycle to see how well the front and rear Oxygen sensors pass their Mode 6 tests. They need to pass the Mode 6 tests with flying colors or they will confuse the OBD-II diagnostic software and possibly trigger a false code P0420. This is an important concept because if the front Oxygen sensor is slow and barely passes its monitor tests, it can fool the computer into thinking that the Catalyst has failed because the computer just watches how closely the switching speed of the front Oxygen sensor compares to the switching speed of the rear Monitoring Oxygen sensor. If the rear Monitoring Oxygen sensor is losing bandwidth and not able to easily reach the 650 millivolt threshold—but still barely passes its monitor tests—then it, too, can fool the computer into setting a P0420 code.\n- If the Catalyst has failed, be sure to check for any software updates for the powertrain computer. Many of the OBD II-equipped vehicles require software updates whenever the Catalyst is replaced.']"	['<urn:uuid:9a4b8990-5dd8-4319-8c93-75cf2b91cae5>', '<urn:uuid:66e81247-26f3-4579-8895-14bbf0bd7d18>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T19:47:23.148022	18	74	1830
11	parkinson disease alexander technique therapy duration frequency sessions	The Alexander Technique therapy for Parkinson's consists of 24 lessons, delivered as two lessons per week over a period of 12 weeks.	['I have given an hour’s presentation ( including working with some volunteers) about the Alexander Technique and Parkinson’s to 30 Physiotherapists and Occupational therapists from the Durham and Darlington NHS Trust at Bishop Auckland Hospital. Research has been done and the Alexander Technique is recommended by NICE ( National Institute of health and Clinical Research) to help people with Parkinson’s. It was well received with many people asking questions at the end. The Senior Occupational therapist, Emma Carr wants to set up 6 week courses for people with Parkinson’s and this would include an Alexander workshop.\nThe Society of Teachers of the Alexander Technique (STAT) Randomised controlled trial of Alexander Technique lessons (AT), for management of disability by people with Parkinson’s\nEvidence for the efficacy of Alexander Technique lessons for patients with Parkinson’s\nA randomised, controlled clinical trial was funded by the Foundation for Integrated Medicine, the Parkinson’s Disease Society, REMEDI and the University of Westminster. It showed that the skills learnt in Alexander Technique (AT) lessons, when applied in daily life, lead to sustained benefit for people with Parkinson’s.1,2 The trial was designed and led by Dr Chloe Stallibrass.\nClinical trial details in brief\nAll contact with the trial participants, apart from the AT lessons and the massage sessions, was with the Research Manager, Peta Sissons. The two AT teachers were both members of STAT.\nA total of 93 people with clinically diagnosed idiopathic Parkinson’s were recruited into the trial, mainly through publicity in the national press. They were randomly allocated to three groups:\n- a control group (no intervention)\n- a group who received 24 lessons in the AT (two lessons a week for 12 weeks)\n- a group who received 24 sessions of therapeutic massage (two sessions a week for 12 weeks) to control for the likely effects of touch and personal attention in AT lessons.\nThe groups were balanced for age, gender, and duration and severity of illness using a randomising computer program. All participants continued their pharmacological treatment for Parkinson’s throughout the trial and received usual care.\nThe main outcome measure was the Self-rated Parkinson’s Disease Disability Scale. Participants rated their performance of everyday activities both at best and at worst times of day: the ‘worst’ times excluded periods of freezing. There were five secondary outcome measures, including one for depression.\nResults: One-to-one Alexander Technique lessons provide significant and sustained benefits for people with Parkinson’s\nThe results of the main measure clearly showed:\nOf the approaches tested, lessons in the AT provided the most benefit. Following 24 AT lessons, participants performed everyday activities with less difficulty than the control group, at both best (p=0.04) and at worst times of day (p=0.0004).\nAt 6-months’ follow-up the comparative improvement was maintained both at best times (p=0.03 and worst times of day (p=0.01).\nThe improvement in the massage group was not statistically significant. This indicated that the benefits from the AT lessons were due to learning and applying skills over and above any improvement due to touch and personal attention.\nPost-intervention, ie during the follow-up period, the AT group was significantly less likely to have adjusted their Parkinson’s medication to cope with worsening symptoms during the trial than were the other two groups (p=0.001). This intriguing finding merits further research.\nAt 6-months’ follow-up, of the participants who had not changed their medication for whatever reason, a smaller proportion of the AT group than the other two groups, reported worsening symptoms, (p=0.045).\nThe secondary measures showed the AT group to be less depressed after 24 lessons compared with the control group (p=0.03) on the pre-determined questions in the Beck Depression Inventory.\nIn an open-ended questionnaire, 41% of the AT group said that they felt more positive/hopeful as a result of the AT lessons; 35% said they felt less stress/panic, and 28% said they had improved self-confidence. When asked to list activities that had improved for them personally, 59% mentioned improved balance/posture, 48% mentioned improved walking, 38% improved speech and 28% reduced tremor. These answers were provided spontaneously rather than being elicited via specific questions\nWhat is the Alexander Technique?\nThe AT is a thoughtful self-help method for the life-long enhancement of an individual’s way of functioning and wellbeing. Learning and applying the AT leads to improvements in balance and mobility, postural tonus3, coordination and functioning. This is because one-to-one AT lessons enable an individual to recognise, understand and avoid poor postural habits and ways of moving that can interfere with the working of movement control systems.\nAlexander Technique lessons\nTeachers use sensitive hands-on contact and spoken explanation to help people attend to head poise and lengthening of the spine in a way that facilitates improvements in postural tone, coordination and control of movement. Lessons are tailored to individual needs and capabilities and applied to daily activities. The aim is that people develop sufficient skill and understanding to begin applying the AT in daily life on their own, in order to manage their disability better and benefit their health and wellbeing.\nThe benefits demonstrated in regard to Parkinson’s might also apply in other chronic conditions where a person’s manner of standing, sitting, moving and speaking is thought to contribute to their problems. Learning and applying the AT is a means for improving general mal-coordination and has been shown to help patients with non-specific low back pain (ATEAM trial, BMJ, 20084).\n- Stallibrass C, Sissons P, Chalmers C Randomised Controlled Trial of the Alexander Technique for Idiopathic Parkinson’s Disease. Clinical Rehabilitation 2002 Vol. 16: 705-718\n- Stallibrass C, Frank C, Wentworth K Retention of skills learnt in Alexander Technique lessons: 28 people with idiopathic Parkinson’s disease. Journal of Bodywork and Movement Therapies 20005 Vol. 9; no. 2: 68-75\n- Cacciatore, TW, Gurfinkel, VS, Horak, FB, Cordo, PJ and Ames, K (2007). Alteration of muscle tone through conscious intervention: increased adaptability of axial and proximal tone through the Alexander Technique. Proceedings of the International Society for Posture and Gait Research, 18.\n- Little P, Lewith G, Webley F, et al. Randomised controlled trial of Alexander Technique lessons, exercise and massage (ATEAM) for chronic and recurrent back pain. British Medical Journal 2008;337:a884. http://www.bmj.com/cgi/content/full/337/aug19_2/a884\nContacts for further information\nQuestions about the Parkinson’s clinical trial: Dr C Stallibrass,10A Greencroft Gdns, London NW6 3LS. Tel: 0207 2093625 Email: email@example.com\nFor further information about the Alexander Technique: Website: www.stat.org.uk\nSTAT, Grove Business Centre, Unit W48, 560-568 High Road, London, N17 9TA, Tel.: 020 8885 6524']	['<urn:uuid:bcb0516b-6ed1-4b14-82e6-af3d161f02f1>']	factoid	direct	long-search-query	distant-from-document	single-doc	novice	2025-05-12T19:47:23.148022	8	22	1068
12	temple bell incense purpose spiritual effects	Temple bells and incense serve distinct spiritual purposes. The bell's sound represents the sacred Om sound that both invites divine energies and removes negative ones. Meanwhile, incense burning serves multiple functions - it eliminates negativity, brings purification with refinement, and different fragrances purify different aspects of one's physiology. The fragrance helps awaken both mind and heart to connect with the divine. In temple rituals, the hand bell must be continuously rung while performing certain ceremonies like waving incense (dhupa) at the deity's navel level and lamps (deepa) at eye-level, with specific mantras being recited simultaneously.	"[""Qualifications of a Pujari\n1. He must have studied Thanthra under a qualified Acharya\n2. He must have experienced vibrations (spandana) of Kundalini\n3. He must have received mulamanthram (incantation for that particular deity) from the Thanthri (puja supervisor) of the temple.\n1. A wooden board in the shape of a Tortoise (Avana palaka)\n2. The board should be large enough so that the poojari can sit over it without any part of his body touching the ground.\n3. Poojari has to sit cross legged or in Padmasana.\n1. Vandanam (prayer) of guru and ganesa. The first manthra is based on poojari's particular Veda to which he belongs. Then he has to salute his Guru adding his name to it. He has to imagine his Guru on the left and Ganesa on the right. Prayer to guru will be through idanadi and Ganesa through pingala Nadi. (Idanadi and pingala nadi are considered to run parallel to the spinal cord, or sushumna nadi)\n2. Cleaning of hand: The asthamanthra (special incantation) of the Deva is to be used for cleaning the hands and massaging those parts of the hands. (Elbow to Wrist) to make them devathamayam (full of godliness).\n3. Thala thrayam (Clap three times) : With Asthamanthra and by clapping the hands three times Agnighandam (fire), Somakhandam (moon’s rays), and Sourakhandam (ether) are to be joined, as described in the Gayatri Mantra.\n4. Dik Bandhanam (close the sides): Using Asthamanthram all 10 sides around the Poojari are to be closed (eight around him, North, North-east, East,… etc, one above him and one below). Now the Poojari gets complete isolation.\n5. Pranayamam (Control of breath): [Using Pranavam (Om) and Mulamanthram (special prayer of that particular deity)]. The Kriyas (actions) of pranayamam are Poorakam (to fill), Kumbhakam (to sustain) and Rechakam (to let out). Ida, Sushumna and Pingala nadis are affected by this. These nadis are to be given Manthra Vibrations. When the controlled Mind and Prana interact with Muladhara (the lowermost chakra) the Kundalini sakthi awakens.(This process requires plenty of training under a great teacher).\n6. Deha Sudhi. (Purification of the body. This is not done for a small pooja)\na. Kundalini shakthi has to reach Sahsraram (Topmost point of head)\nb. All the five elements (Water, earth, air, ether and fire), ego and mind are to be merged into the almighty (Paramathma).\nc. The physical body (Karmasareeram) is to be reduced in size by using air (Vayu); and fire (Agni) is used to burn off (Bhasmikaranam) and using water purify (Amruthaplavanam) and recreate the body. Then the Kundalini Shakti is again brought down to Mooladharam. The above is a fine imagination of Punarjanma (re-birth).\n7. Nyasam:- (bringing divinity to)\na. Lipinyasam (The alphabet)\nb. Shadanga nyasam (The six parts of Manthra chaithanya)\ne. Nyasa (establishment) of Devathas on head, lips and heart.\n8. To concentrate on the shape of Manthra Devatha (Dhyana swaroopam) in the mind.\n9. Manasapooja (worship without an idol): Recite Mulamanthram several times in the mind till the Poojari's body attains Manthra Chaithanya.\n10. Shankha (conch) Pooranam (To purify the puja instruments and puja material).\na. Two Kindis (vessels shaped like kamandalus, with outlet pipes upwords). The left one is to have pure water. The right to contain thirtha (sanctified water) kept over an imaginary Pidham (seat), ie a circle over a square.\nb. Shankhu (conch). Thirtha is to be made inside the conch. The Pidha (seat) below this is shaped like a triangle. Now Pidha puja is to be done. Imagine God inside the Shankhu and do Panchopachara pooja to it (water, aroma, flower, incense, and lamp) representing (water, earth, air, ether and fire).\nc.After this pooja hold the shankhu in the hand and recite Moolamanthra.\nd. Using the water from the conch sprinkle it on the Puja materials and puja instruments. The remaining water is to be poured into the Kindi on the right side. This converts the water in it to Pure Thirtha.and as and when required water from this can be used as thirtha to purify any material.\n11. Athmaradhana (worship the god within oneself): The pujari puts sandal paste and flowers on his body and with hand gestures does pooja on himself. At this time the Sankhu is to be directed towards him. (The pujari becomes equal to God).\n12. Pidha pooja: This is to make God's seat pure. Six pidhas one over the other are to be worshipped separately using Panchopachara manthras. Then imagine God installed on the Pidha and worship Him using flowers (Moorthi Kalpana).\n13. Avahanam: (Bringing Godliness into the puja materials)\na. Bring up Kundalini to Sahasrara Padmam (the top chakra)\nb. Imagine the Deity at Sahasrara Padmam and pray\nc. Then bring down the Kundalini to Muladhara, shift it slightly to the right and bring it up through the Pingala nadi up to the right nostril. The air in the nostril is passed on into the Jala (water), Gandha Pushpa (aromatic flowers), Akshatha (other puja materials) held in the hand of the Pujari. ie the divinity gets into these puja materials.\nd. These materials are used to worship the Deity and then the pujari has to do the Avahanadi Mudras and do Nyasas to the Deity.\n14. Shodasopacharangal [16 Upacharas (obeisances) as given in Sankaracharya’s Prapancha Sarathanthram]: In the beginning give Shankha jalam to Padhya (feet), Arghya (head) and Achamaneeya (obeisance to the spirit within). Then give Upachararpanam to the Deity by giving Padyam at the feet, Arghyam at the head and Achamaneeyam (Madhuparkam) at the Sukshma Deha (the spirit within).\nThe Upacharam with the Madhuparkam (a mix of honey, milk and curd) is similar to welcoming a guest. Here it is like offering Soma, Surya, Agni portions to God. While offering these the Panchopachara manthras (Jala, Gandha, Pushpa, Deepa, Dhupa Nivedya) are to be recited which is like merging all these with God. It is said that the Pujari gets Amrutha Varshana (Bliss) at the end of Shodasopachara Kriya (16 actions). Then the Pujari takes a bath. At the time of the bath (Snana) he has to recite three Riks (mantras) and sprinkle water on his head.\n15. Vasthralamkaram or Murthi puja (adorning the deity): Normally God is invisible. Only when God puts on clothes and ornaments the ordinary person is able to see Him. After this the pujari imagines different parts of God including the ornaments and dresses and does puja to each.\nAccording to Kerala custom, after this, the Pujari stops the main pooja and using different Manthras does the following pujas:\n* Panchopachara puja to Pranavam (water, aroma, flower, incense, and lamp) representing (water, earth, air, ether and fire).\n* Shakthi upacharamanthram invoking the strength of God\n* Mulamanthram (special prayer of that particular deity)\n* Mulaksharangal, (the vowels in the alphabet)\n* Angam, (parts of the body)\n* Ayudham, (weapons)\n* Bhushanangal, (ornaments)\n* Parivarangal (relatives), and their instruments, and\n* Nirmalyadhari. (the minor deity who receives the uchchishtam, or the remainder after the main deity partakes of the offerings)\nThe manthra simply states Sarvangebhyo nama etc but the Panchopachara puja is to be given to each of these separately. Dhupa (incense) is to be waved at Nabhisthal (in front of the deity) and Deepa (lamps) are to be waved at eye-level. While doing this, manthras are to be recited and the hand bell is to be continuously rung.\n16. Nivedyam: Food offered at the temple is purified by Bhootha Sudhi (ceremony to prevent the touch of evil and unclean spirits. Then ghee is poured into it, while reciting Gayathri manthra. The moolamantra is chanted, and the various items are touched in turn. The food is considered to merge with the Deity. Then the Naivedya is covered by Manthras.\nThe Devatha for the remainder (uchishtam) is Nirmalyadhari. For Vishnu the Nirmalyadhari is Vishwaksenan and for Siva it is Chandeswaran. Then the door of the sanctum sanctorum is closed and the Naivedya is offered to Nirmalyadhari with a puja called Pancha pranahuthi. The devotees receive the remaining naivedyam from the Nirmalyadhari. Along with this, a little naivedyam is also offered to the deities of the Balikal (minor deities around the sanctum sanctorium).\nAfter this, the Pujari has to climb the Mandapa and recite Mulamanthra. Then he does sashtanga pranam (prostration) there. Then he has to wash his feet and enter the sanctum sanctorum. .Then he does the pranahuthi puja to the Nirmalyadhari. In big temples during nivedya devotees are not allowed inside.\n17. Prasanna pooja: This is like a Durbar of a king. In the olden days the deity was probably considered the king of the village. This pooja is done with Nritha (dance), Vadya (instruments) and Sangeetha (music). For this pooja, only flowers are offered with Sahasranama to the deity as well as Pidhadevathas (minor deities).\n18. Pooja samarpanam: Here the Pujari offers apologies for any errors committed in the rituals and showers the remaining flowers on the deity, a process called Poorna Pushpanjali. Then he gives away all the Punyas (good karma) acquired by him to the deity. Then the performs the puja that converts the water offered to the deity to thirtha (sanctified water). Then he takes the purified water in the conch and keeps it near the feet of God. Then he prays in his mind to his Guru and showers the offered flowers on all (imagined) Devas and Asuras. Then he showers them on the devotees standing in front of the sanctum sanctorum.\nThen he gives Arghya (offering at the feet) to God and again does Thala thraya (three claps) and Dik bandhanam (closing the sides). Then he does Pranam at God's feet and takes one flower from there, which brings Chaithanyam (sanctification) to the flower. He receives only 1/16 part of the Chaithanyam. The balance is left to accumulate there. This increases the Chaithanya of the temple."", 'What are temple ceremonies?\nA: Read What is a Vedic Ceremony? on DrMichaelMamas.com.\nQ: What are the 16 parts of a puja?\nA: Invoking God and offering Shodasopacharapuja (16 types of offerings to God) should be performed for each puja. In a puja, there are many reasons for doing each action and each action has many effects.\n- Avahanam (inviting)\n- Aasanam (providing a seat)\n- Padyam (washing feet)\n- Arghyam (washing hands)\n- Achmaneeyam (offer water to drink)\n- Snanam (offering a bath)\n- Vastram (offering clothes)\n- Kumkum, haridram akshataan (offering turmeric, kumkum, and sacred rice)\n- Gandham (offering sandalwood powder)\n- Yagnopaveetham (offering sacred thread)\n- Pushpam (offering flowers)\n- Dhoopam (offering sandalwood stick)\n- Deepam (offering light)\n- Naivedhyam (offering prasadam or food)\n- Neerajanam (offering camphor)\n- Manthrapushpam and tambulam (offering flowers and betel leaves/nuts with coins)\nAll foods and spices used in a puja are equal to medicine. After the puja, they are given to people to eat. Nothing is wasted.\nQ: Why do we put kumkum just above and between the eyebrows?\nA: Kumkum is made by grinding together turmeric horn with kumkum stone. The mixture is used to paint the bindi point on the forehead. Kumkum can be applied as a dot or a vertical line between the eyebrows. It symbolizes the third eye of wisdom or enlightenment which opens when the spiritual seeker has purified his mind.\nOne’s power is held in the space between the eyes. This is because it is where the three parts of the nervous system meet – the ida (solar force), the pingala (lunar force) and the sushumna (the spinal cord). Placing kumkum on the bindi point assists the integration of the nervous system and also helps protect a person from having negative energies coming in.\nQ: Why do we apply vibhuti to the forehead?\nA: Three horizontal lines are applied across the forehead and symbolize various triads that the seeker has to transcend. They include the syllables of Om or Aum which represent the Trinity, the planes of waking dream, and deep sleep. The horizontal lines can also symbolize the three gunas of sattwa, raja, tamas. In addition, they can symbolize the instruments of the gross body, subtle body, and causal body.\nVibhuti is a reminder of Shiva, that all that exists is Consciousness. It invites us to go beyond our world of illusion and live from the depth of our being.\nWearing kumkum or vibhuti serves as a constant reminder to the seeker of his ultimate goal of self-realization.\nQ: What is the significance of bell ringing in the temple?\nA: The sound is the Om sound to invite the Divine energies on the outside and inside. It also removes the negative energies.\nQ: Why do we burn incense?\nA: Incense burns negativity and brings purification with refinement. Different fragrances clear different parts of the physiology and allow the physiology to bloom. The freshness invites an awakening of the mind and heart to God.\nQ: Why do we primarily use the fruits coconut and banana in pujas?\nA: In nature, the coconut and the banana are the fruits that are in and of themselves the seed as well as the fruit.\nQ: What does “Veda” mean? How many Vedas are there and what are they? What are their Upavedas?\nA: “Veda” comes from the word “Vid” which means to know. “Veda” literally means the book of knowledge. The Veda, the book of knowledge, is divided into four parts:\n- Rig Veda – Hymns of praise believed to be the oldest book known to humanity and one of the most precious collections of knowledge.\nThe Upaveda is Ayurveda – The science of medicine and health\n- Yajur Veda – Special directions and formulas for the preparation and performance of rituals and ceremonies.\nThe Upaveda is Dhanurveda – Military signs\n- Sama Veda – Melodious songs with precise intonations and modulations to be chanted at rituals, the most voluminous of the four Vedas.\nThe Upaveda is Gandharvaveda – The art and science of music\n- Atharva Veda – Mystical formulas, tantric and other forms of esoteric knowledge which paved the way for modern science in India.\nThe Upaveda is Sthapatyaveda – The mechanics of construction']"	['<urn:uuid:190316a4-799e-4e1b-9d49-c416c6aa68b6>', '<urn:uuid:75e21f65-0ceb-4a03-8053-aa150b60275b>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	6	95	2349
13	What are the natural ways to regulate blood pressure through lifestyle changes, and what specific blood pressure readings are considered high for different age groups?	Natural ways to regulate blood pressure include maintaining a healthy diet, adequate exercise, and regular stress relief. Additionally, reducing salt intake, cutting back on alcohol, losing weight if overweight, exercising regularly, and cutting down on caffeine can help. As for blood pressure readings, high blood pressure is generally considered to be 140/90mmHg or higher for people under 80, while for those over 80, it's 150/90mmHg or higher. The ideal blood pressure range is between 90/60mmHg and 120/80mmHg.	['Blood Pressure Regulators\nThere are natural ways to keep hypertension,\na major cardiovascular risk factor, in check\nIt is called “the silent killer” for a good reason. High blood pressure (hypertension) often damages artery walls without symptoms until something drastic—such as a heart attack or stroke—occurs.\nThat’s why keeping your pressure at normal levels, even if you feel fine, is crucial to maintaining your health. One recent study found that treating prehypertension, a reading that falls between 120 to 139 mg/Hg systolic (the top number) and 80 to 89 diastolic, can reduce stroke risk by 22%. Researchers have also discovered that controlling pressure in middle age can reduce your lifetime risk of developing heart disease or stroke.\nAs with other cardiac risk factors, proper blood pressure control starts with healthy diet, adequate exercise and regular stress relief. In addition, natural remedies exist that can help keep your pressure at normal levels.\n|Nutrient/Herb||What It Is||How It Helps Arterial Health|\n|Arginine||An amino acid (protein building block) known to facilitate wound healing and hormone function||Helps the cells that line blood vessels create nitric oxide, which helps improve blood flow by dilating vessel walls; may improve exercise tolerance among people with angina; also appears to inhibit plaque buildup within arteries|\n||Vaccinium myrtillus, a relative of the blueberry best known for supporting healthy vision||Contains anthocyanins, which help facilitate vascular relaxation and may inhibit angiotensin-converting enzyme (ACE), a substance that constricts blood vessels|\n||Coleus forskohlii, in the same family as ornamental coleus; often used to support weight loss||Contains forskolin, which helps to lower blood pressure and relax muscles; also appears to help increase the force of cardiac contractions, which improves heart function|\n||A vitamin-like compound found throughout the body but concentrated in heart muscle cells||Helps lower blood pressure (may be especially helpful in people who have diabetes); has contributed to improved cardiac function in people with congestive heart failure|\n||Allium sativum, used for centuries as a kitchen herb and medicinal remedy||Acts as an antioxidant, helping to protect blood vessel walls from free radical attack, and may help fight inflammation, a contributing factor to cardiovascular disease; also helps reduce cholesterol and triglyceride levels|\n||Members of the Crataegus family, found throughout western Asia, North America and Europe||Has lowered both systolic and diastolic readings in studies and may improve outcomes after heart attack or stroke; used for centuries in traditional medicine as a cardiac tonic|\n|Magnesium||A mineral that helps maintain normal muscle, nerve and immune function, and helps keep bones healthy||Helps relax the muscles that control blood vessel dilation, allowing blood to flow more freely; also helps coordinate heart muscle activity and prevent spasms of the coronary arteries|\n|Nattokinase||An enzyme derived from natto, a Japanese fermented soyfood||Has helped reduce blood pressure in studies; may also help keep blood from becoming too thick and clot-prone|\n|Niacin||A member of the B-complex of vitamins (vitamin B3) best known for its ability to lower cholesterol levels||Supports blood vessel dilation and may fight vascular inflammation; in addition to lowering LDL (“bad”) cholesterol levels has shown an ability to raise HDL (“good”) cholesterol|\n|Potassium||A mineral that helps regulate water balance and neuromuscular function||Helps maintain normal pressure through its role as an electrolyte; studies have found an association between increased potassium intake and reduced stroke risk|\n|Reishi||Ganoderma lucidum, a mushroom native to China that has shown anti- cancer and anti-microbial properties||May help keep smooth muscle cells, the kind found in blood vessel walls, from becoming overly prolific; also helps fight inflammation, lower cholesterol levels and reduce blood stickiness|\n|Resveratrol||Best known as the healthful component in red wine; Japanese knotweed (Polygonum cuspidatum) contains greater amounts||Improves function of the blood vessel lining and helps protect lining cells against damage; may support the action of nitric oxide on vessel walls; also appears to have anti-aging properties|\nNOTE: Always consult with your healthcare practitioner for help in designing\na supplementation program, especially if you have a pre-existing condition.\nDo not stop taking prescription medications of any kind without your practitioner’s assistance.', 'Hypertension Dubai or a high blood pressure is a very common condition that is easily diagnosed. The old maxim of “100 plus your age” is not a safe way to measure what your blood pressure should be. Hypertension exists when the blood\nHigh blood pressure, or hypertension, rarely has noticeable symptoms. But if untreated, it increases your risk of serious problems such as heart attacks and strokes. The only way to find out if your blood pressure is high is to have your blood pressure checked.\nWHAT IS HIGH BLOOD PRESSURE?\nBlood pressure is recorded with 2 numbers. The systolic pressure (higher number) is the force at which your heart pumps blood around your body.\nThe diastolic pressure (lower number) is the resistance to the blood flow in the blood vessels.\nThey’re both measured in millimetres of mercury (mmHg).\nAs a general guide:\n– high blood pressure is considered to be 140/90mmHg or higher (or 150/90mmHg or higher if you’re over the age of 80)\n– ideal blood pressure is usually considered to be between 90/60mmHg and 120/80mmHg\nBlood pressure readings between 120/80mmHg and 140/90mmHg could mean you’re at risk of developing high blood pressure if you do not take steps to keep your blood pressure under control.\nEveryone’s blood pressure will be slightly different. What’s considered low or high for you may be normal for someone else.\nRISKS OF HIGH BLOOD PRESSURE\nIf your blood pressure is too high, it puts extra strain on your blood vessels, heart and other organs, such as the brain, kidneys and eyes.\nPersistent high blood pressure can increase your risk of a number of serious and potentially life-threatening health conditions, such as:\n– heart disease\n– heart attacks\n– heart failure\n– peripheral arterial disease\n– aortic aneurysms\n– kidney disease\n– vascular dementia\nIf you have high blood pressure, reducing it even a small amount can help lower your risk of these health conditions.\nTHINGS THAT CAN INCREASE YOUR RISK OF GETTING HIGH BLOOD PRESSURE\nIt’s not always clear what causes high blood pressure, but there are things that can increase your risk.\nYou might be more at risk if you:\n– are overweight\n– eat too much salt and do not eat enough fruit and vegetables\n– do not do enough exercise\n– drink too much alcohol or coffee (or other caffeine-based drinks)\n– do not get much sleep or have disturbed sleep\n– are over 65\n– have a relative with high blood pressure\n– are of black African or black Caribbean descent\nMaking healthy lifestyle changes can sometimes help reduce your chances of getting high blood pressure and help lower your blood pressure if it’s already high.\nTREATMENT FOR HIGH BLOOD PRESSURE\nDr Barbara Karin Vela can help you keep your blood pressure to a safe level using:\n– lifestyle changes\nWhat works best is different for each person. Talk to her to help you decide about treatment.\nLIFESTYLE CHANGES TO REDUCE BLOOD PRESSURE\nThese lifestyle changes can help prevent and lower high blood pressure:\n– reduce the amount of salt you eat and have a generally healthy diet\n– cut back on alcohol\n– lose weight if you’re overweight\n– exercise regularly\n– cut down on caffeine\n– stop smoking\nSome people with high blood pressure may also need to take 1 or more medicines to stop their blood pressure getting too high.\nMEDICINES FOR HIGH BLOOD PRESSURE\nIf you’re diagnosed with high blood pressure, Dr Barbara Karin Vela may recommend taking 1 or more medicines to keep it under control.\nThese come as tablets and usually need to be taken once a day.\nCommon blood pressure medicines include:\nACE inhibitors – such as enalapril, lisinopril, perindopril and ramipril\nangiotensin-2 receptor blockers (ARBs) – such as candesartan, irbesartan, losartan, valsartan and olmesartan\ncalcium channel blockers – such as amlodipine, felodipine and nifedipine or diltiazem and verapamil\ndiuretics – such as indapamide and bendroflumethiazide\nbeta blockers – such as atenolol and bisoprolol\nalpha blockers – such as doxazosin\nother diuretics – such as amiloride and spironolactone\nThe medicine recommended for you will depend on things like how high your blood pressure is, your age and your ethnicity.\nKNOWN CAUSES OF HIGH BLOOD PRESSURE\nIn about 1 in 20 cases, high blood pressure happens as the result of an underlying health condition or taking a certain medicine.\nHealth conditions that can cause high blood pressure include:\n– kidney disease\n– long-term kidney infections\n– obstructive sleep apnoea – where the walls of the throat relax and narrow during sleep, interrupting normal breathing\n– glomerulonephritis – damage to the tiny filters inside the kidneys\nnarrowing of the arteries supplying the kidneys\n– hormone problems – such as an underactive thyroid, an overactive thyroid, – Cushing’s syndrome, acromegaly, increased levels of the hormone aldosterone (hyperaldosteronism), and phaeochromocytoma\n– lupus – a condition in which the immune system attacks parts of the body, such as the skin, joints and organs\n– scleroderma – a condition that causes thickened skin, and sometimes problems with organs and blood vessels\nBeing active and taking regular exercise lowers blood pressure by keeping your heart and blood vessels in good condition.\nRegular exercise can also help you lose weight, which will also help lower your blood pressure.\nAdults should do at least 150 minutes (2 hours and 30 minutes) of moderate-intensity aerobic activity, such as cycling or fast walking, every week.\nPhysical activity can include anything from sport to walking and gardening.\nSOME HYPERTENSION TERMS WORTH KNOWING\nEssential (primary) hypertension occurs when you have abnormally high blood pressure that’s not the result of a medical condition. This form of high blood pressure is often due to obesity, family history and an unhealthy diet. The condition is reversible with medications and lifestyle changes.\nGestational hypertension is a form of high blood pressure in pregnancy. It occurs in about 6 percent of all pregnancies. Another type of high blood pressure is chronic hypertension–high blood pressure that is present before pregnancy begins. Gestational hypertension can develop into preeclampsia.\nHBP is simply short for High Blood Pressure.\nHigh Blood Pressure\nHigh blood pressure is considered to be 140/90mmHg or higher (or 150/90mmHg or higher if you’re over the age of 80) ideal blood pressure is usually considered to be between 90/60mmHg and 120/80mmHg.\nIdiopathic Intracranial Hypertension\nIdiopathic intracranial hypertension (IIH) happens when high pressure around the brain causes symptoms like vision changes and headaches. “Idiopathic” means the cause isn’t known, “intracranial” means in the skull, and “hypertension” means high pressure.\nIntracranial hypertension (IH) is a build-up of pressure around the brain. It can happen suddenly, for example, as the result of a severe head injury, stroke or brain abscess. This is known as acute IH. It can also be a persistent, long-lasting problem, known as chronic IH.\nLow Blood Pressure\nLow blood pressure is a reading of less than 90/60mmHg. It does not always cause symptoms, but you may need treatment if it does.\nMalignant hypertension is very high blood pressure that comes on suddenly and quickly. The kidneys filter wastes and excrete fluid when the pressure of blood in the bloodstream forces blood through the internal structures of the kidney.\nPortal hypertension is elevated pressure in your portal venous system. The portal vein is a major vein that leads to the liver. The most common cause of portal hypertension is cirrhosis (scarring) of the liver.\nPulmonary hypertension is high blood pressure in the blood vessels that supply the lungs (pulmonary arteries). It’s a serious condition that can damage the right side of the heart. The walls of the pulmonary arteries become thick and stiff, and cannot expand as well to allow blood through.']	['<urn:uuid:f1a33b34-fb5b-4b6f-96ff-f47913caaf7e>', '<urn:uuid:338f04fd-55d2-4e9f-812e-484948fc82cb>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:47:23.148022	25	77	1943
14	volcano monitoring techniques early warning	Volcano monitoring employs multiple techniques including dilatometer networks that can detect strain signals during intrusion phases before typical geophysical signals appear, making them valuable for early-warning. Additionally, satellite monitoring systems track volcanic gases and plumes, as demonstrated in cases like Taal volcano where NASA satellites monitored SO2 emissions and plume movement, providing crucial data for hazard assessment.	"[""Editorial: Multidisciplinary Geophysical Imaging of Volcanoes\n- 1Dipartimento di Scienze della Terra, Ambiente e Risorse, Università di Naples Federico II, Naples, Italy\n- 2Dipartimento di Fisica Ettore Pancini, Università di Naples Federico II, Naples, Italy\n- 3Univ. Grenoble Alpes, Univ. Savoie Mont Blanc, CNRS, IRD, IFSTTAR, ISTerre, Grenoble, France\nEditorial on the Research Topic\nMultidisciplinary Geophysical Imaging of Volcanoes\nGreat strides have been made in the geophysical imaging of volcanic systems on a wide range of spatial scales. Progress has included data and instrumentation advances, application of new techniques, and improvement in computational capabilities. The goal of this Research Topic is to highlight the imaging capabilities of geophysical methods in characterizing the internal structure and the dynamics of volcanoes, toward an understanding of ongoing and potential eruptions, and to improve time-dependent estimation of related hazards.\nThe majority of the contributions focused on Solfatara Volcano, the most active area of Campi Flegrei, which is experiencing an unrest in the recent years and that is becoming a natural laboratory for testing new methods and applications. Indeed, three contributions analyse active time-lapse seismic data recorded in a 2D and 3D geometry during the first Repeated Induced Earthquake and Noise experiment (RICEN: Festa et al., 2015), conducted from September 2013 to November 2014. De Landro et al., for instance describe a methodology based on the estimation of the quality factor Q from the RICEN active seismic data to obtain a high-resolution attenuation model of the very shallow subsurface. They found low P-wave quality factor (QP 5–40 in the near-surface layer 30 m thick) in shallowest subsoil of Solfatara, which globally increases with depth in the explored volume and shows a strong lateral heterogeneity. Within the well-resolved central portion of the explored volume the QP model shows features consistent with the hydrothermal fluid distribution within Solfatara.\nScala et al., using the 3D active seismic data from the RICEN experiment analyzed instead the scattering properties of the Solfatara crater to evaluate the ratio between coherent and incoherent intensities. The scatterers are interpreted as regions richer in water with respect to the background and eventually due to the condensed steam running below the investigated area. The connection between the scattering mean free path and the type, and content of fluids retrieved here is of fundamental importance to image the volcanic structure: upscaling this technique to a kilometric size area could allow to provide constraints about the magma chamber and related feeding mechanism.\nFinally, Bernardinetti and Bruno, using two machine learning algorithms merged active 2D seismic data from the RICEN experiment (Bruno et al., 2017) with some geophysical anomalies recorded at the surface of the crater (Bruno et al., 2007). The use of unsupervised learning techniques can reduce interpretation uncertainties while interpreting multivariate data, therefore improving the understanding of the complex dynamics occurring in volcanoes. This allowed to associate surface and subsurface anomalies to different hydrothermal features such as shallow gas-saturated and water-saturated zones and their underlying fractures/faults feeding system.\nCarbonari et al., presented a sensitivity study aimed at estimating the effectiveness of continuous magnetotelluric observations for monitoring Solfatara hydrothermal system: in particular, two possible evolution scenarios of a hydrothermal system have been assumed, corresponding to an increase in fluid flow rate from the system source and permeability of the rocks hosting the fluid upwelling zone, respectively. The study has shown that continuous MT measurements are not sensitive enough to detect a significant increase in the source fluid flow rate over time intervals less than 10 years. On the contrary, if the permeability of the upwelling zone increases, a measurable change in the MT response occurs over a time interval ranging from 6 months to 3 years, depending on the extent of the permeability increase.\nThe remaining papers describe geophysical experiments targeting other volcanic areas: Northern Andes, Indonesia and another Italian volcano: Mt. Etna. Vargas et al., analyzed the thermal structure of the Volcanic Arc in Northern Andes, using anelastic tomography. Seismic records from 1993 to 2018 have been processed to extract coda wave attenuation parameters, that have been further converted into thermal profiles, using rock physics relationships, constrained by surface and well observations. The authors show how inferred temperature profiles can provide clues in the structure, dynamics and history of a volcanic system. The study specifically revealed that the temperature is relatively low below the volcanic arc indicating a large presence of water and hydrous minerals. The isotherms become shallower toward the North; they are interrupted by a cold body, which likely indicates a lateral change in viscosity and their shape has been also influenced by regional tectonics. Finally, thermal images suggest that the thickening of the lithosphere at the northern boundary of the region prevents from further development of the volcanic arc.\nDue to the presence of many active volcanoes, the volcanic risk is particularly high in Indonesia. Monitoring of these volcanoes requires precise structural studies to recognize the potential propagation paths of magma. The area of the Agung-Batur Volcanic Complex (ABVC), part of the Sunda volcanic arc, has produced numerous historical eruptions and is the subject of a seismic tomography study in this volume by Zulfakriza et al.. The authors obtained the first local-scale study of Rayleigh wave group velocity structure and the seismic velocity structure using ambient seismic noise tomography. Seismic tomograms clearly display a low-velocity structure between Mt. Agur and Mt. Batur, corresponding to a seismicity cluster, that could be a pressurized hydrothermal system in an area of buried volcanic deposits.\nFinally, the Italian volcano Etna has always been the field of original and innovative experimentation in volcano geophysics, particularly in the study of ground deformations. Bonaccorso et al., using a network of dilatometers were able to determine the strain signature of a wide variety of magmatic processes (lava fountains, explosive sequences, effusive activity) occurred between 2011 and 2018. For instance, they were able to highlight the presence of strain signals during an intrusion phase, occurring well-before the typically monitored geophysical signals. This is particularly useful for the early-warning recognition and interpretation of the initial phase of the intrusion propagation. This study showed that a network of dilatometers effectively reinforces the methods of forecasting volcanic eruptions but also the fine understanding of magmatic processes.\nAll authors listed have made a substantial, direct and intellectual contribution to the work, and approved it for publication.\nSome papers published in this Research Topic used data acquired under the Med-Suv project. MED-SUV has received funding from the European Union's Seventh Program for research, technological development and demonstration under the grant agreement No. 308665.\nConflict of Interest\nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\nWe thank the Editorial Board and Editorial Office of Frontiers in Earth Sciences for the kind invitation to organize and their help in managing this Research Topic.\nBruno, P. P. G., Maraio, S., and Festa, G. (2017). The shallow structure of Solfatara Volcano, Italy, revealed by dense, wide-aperture seismic profiling. Sci. Rep. 7:17386. doi: 10.1038/s41598-017-17589-3\nBruno, P. P., Ricciardi, G. P., Petrillo, Z., Di Fiore, V., Troiano, A., and Chiodini, G. (2007). Geophysical and hydrogeological experiments from a shallow hydrothermal system at Solfatara Volcano, Campi Flegrei, Italy: response to caldera unrest. J. Geophys. Res. 112, 1–17. doi: 10.1029/2006JB004383\nKeywords: geophysical imaging, geophysical observations, volcanic structure, volcano dynamics, novel techniques\nCitation: Bruno PP, Festa G and Vandemeulebrouck J (2020) Editorial: Multidisciplinary Geophysical Imaging of Volcanoes. Front. Earth Sci. 8:214. doi: 10.3389/feart.2020.00214\nReceived: 08 May 2020; Accepted: 22 May 2020;\nPublished: 23 June 2020.\nEdited and reviewed by: Greig A. Paterson, University of Liverpool, United Kingdom\nCopyright © 2020 Bruno, Festa and Vandemeulebrouck. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.\n*Correspondence: Pier Paolo Bruno, email@example.com"", 'Lava, smoke and volcanic ash were seen belching from Taal volcano after two eruptions tore through the fiery mountain since the weekend. NASA satellites over the Pacific are now tracking another source of worry – clouds of volcanic sulphur dioxide (SO2) released by the Philippines volcano.\nOn January 12, a steam-driven phreatic eruption woke the deadly volcano from a 43-year-long slumber.\nThe following morning, on January 13, a fountain of lava shot out of the volcano for more than an hour-and-a-half.\nThe Taal eruption blotted out the skies over Philippines island of Luzon with a thick layer of ash and volcanic gases.\nThe gases are now seen in these NASA satellite scans, carried on winds past Taiwan, China and towards Japan.\n- Black holes: Astronomers to ‘buckle up’ for black hole ‘revolution’\nJust 100 parts per million of SO2 are enough to pose an immediate risk to life on top of breathing problems and irritation.\nAlthough the levels of SO2 released by Taal have so far been tolerable, Michigan volcanologist Simon Carn fears more of the toxic gas could be still be released.\nHe said: “However, it is possible that the Taal eruption could continue and produce more SO2 in the coming days.”\nNASA’s satellite scans show SO2 concentrations of up to 40 Dobson units just to the northeast of the erupting volcano.\nFurther out from Taal, concentrations range from the high 20s down to zero units.\nPhilippines authorities have warned not to breathe in the hazardous air.\nTaal is the second most active volcano on the Philippines, located about 37 miles (60km) south of the capital Manila.\nIt is possible that the Taal eruption could continue and produce more SO2\nSimon Carn, Michigan Tech\nThe volcano sits in what is known as the Pacific Ring of Fire – a chain of volcanoes and earthquake hotspots running along the rim of the Pacific Ocean.\nAfter the initial eruption on Sunday, the Philippine Seismic Network has tracked at least 144 earthquakes associated with Taal.\nStrong seismic activity in the volcano’s vicinity suggests magma is moving deep underground.\nSuch intense magmatic activity could serve as a precursor to another eruption.\nPhilippines volcano could see more eruptions for YEARS [INSIGHT]\nYellowstone volcano hit by 1217 earthquakes – Is it awakening? [ANALYSIS]\nWhich Ring of Fire volcanoes could BLOW in 2020? [ANALYSIS]\n- Philippines volcano eruption triggers explosive ‘WALL OF FIRE’\nPhilippines authorities have ordered nearly 500,000 people to evacuate from what has been dubbed a potential “hazardous explosive eruption”.\nLocal residents have been ordered to leave an 8.6-mile-wide (14km) radius around the volcano.\nAs of Tuesday, January 14, some 20,000 people from the provinces of Batangas and Cavite have sought refuge from the eruption in 188 shelters.\nTaal volcano last erupted between 1965 and 1977, across a number of incidents.\nThis time around, the volcanic was seen spewing a plume of smoke up to nine miles (14km) into the air.\nThe eruption was accompanied by terrifying thunders and lighting hitting the volcano’s summit.\nMr Carn said: “The plume is probably comprised mostly of water droplets.\n“There may be some volcanic ash but in such ‘wet’ plumes volcanic ash can be difficult to detect as the ash particles are often covered by liquid water.”\nSatellite data collected by the Japanese Himawari-8 satellite shows the volcanic plume spreading to the north.\nSource: Read Full Article']"	['<urn:uuid:f0b5a79d-1bd0-4f99-95c8-5496c11535f5>', '<urn:uuid:f902200f-f764-4332-93c0-900b6e6c116c>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T19:47:23.148022	5	57	1909
15	I'm studying microorganisms and I'm curious: what makes yeast cells produce alcohol during beer brewing, and what temperature conditions do they need to thrive and grow?	Yeast cells produce alcohol (ethanol) as a safety mechanism when they hit their energy limit - it's like a safety valve that prevents the cell from being damaged by too much energy release when breaking down glucose. Regarding temperature, since yeast belongs to mesophiles, they thrive best in temperatures between 15-45°C with an optimal temperature around 20-45°C.	"['Alcohol prevents destruction\nRaise your glass to yeast\nIn the Netherlands alone, people drink approximately 1.1 billion litres of beer every year. Beer, the first alcoholic beverage, was invented approximately five thousand years ago. It’s still the most popular drink in the world.\nBut RUG systems biologist Matthias Heinemann says we are lucky that Leffe Blond, Grolsch, and Kilkenny exist at all. They wouldn’t if it weren’t for a little-known mechanism in the yeast cell: a kind of safety valve. Heinemann discovered how this valve works and published his findings in the prestigious journal Nature Metabolism.\n‘It actually doesn’t make any sense for yeast cells to produce ethanol’, Heinemann explains. The yeast cell Saccharomyces cerevisiae, used to brew beer, ferments the sugar in malt by breaking down glucose into ethanol.\nSo the yeast cell leaves part of its energy unused\nBut it would do better to turn the glucose into carbon dioxide. The yeast cell takes a molecule made up of six carbon atoms (glucose) and turns it into a molecule made up of only two of those, turning it into ethanol. But carbon dioxide has only one carbon atom. ‘So the yeast cell leaves part of its energy unused’, says Heinemann.\nThis would make sense if there is no oxygen available, like during the beer brewing process. It can’t make carbon dioxide without oxygen atoms. But yeast turns the glucose into ethanol even when there is oxygen available.\nHeinemann thinks that’s amazing, and so do many other scientists. One would expect that evolution would have done away with this inefficient use of energy. ‘But it hasn’t.’\nOther cells also exhibit this strange behaviour. The E. coli bacterium that lives in your bowels and helps you digest food produces acetate instead of alcohol. E. coli doesn’t fully break down glucose, either. And cancer cells produce lactate, which is another product of insufficiently broken down glucose.\nMaybe there is a story to tell about all of this, Heinemann thought. ‘If all these organisms behave the same, maybe I should look for the answer to that behaviour somewhere else: in how the cell itself works.’\nIt’s been eight years since Heinemann asked himself this question. Around that same time, he received an e-mail from the RUG asking if he was interested in working in Groningen. Because he’s not just a systems biologist: ‘I trained as an engineer’, he says. ‘After that, I did a post doc and became a biologist. But I’m still really technical in my approach to things.’\nThe RUG offered Heinemann a ‘very good package’ to continue his research here, he says. ‘They gave me funding for PhD students without me having to write so much as a proposal. That’s the only reason I’m even able to do this research.’\nAnd so Heinemann left Zurich for Groningen to start a study that had no guarantee of succeeding. He says he just had a gut feeling that the answer to the ethanol problem might turn out to be very important.\nI was just at a complete loss sometimes\nHe decided to approach the yeast cell as though it was a machine. First he created a model for the approximately thousand different chemical reactions taking place in a cell. Then he added thermodynamics.\n‘I looked at the speed with which the cell releases Gibbs energy’, he explains, which is released during a chemical reaction. He suspected that there was a limit; a maximum speed at which the cell reactions are able to release this Gibbs energy.\nFiguring this out wasn’t easy. He worked alongside his PhD students, Bastian Niebel and Simon Leupold, creating endless computational models. These required the computing power of the CIT; there are thousands of reactions in a cell and millions of possible outcomes.\nAfter three years of their research, the team had nothing. ‘I was just at a complete loss sometimes’, Heinemann confesses. ‘When one of my PhD students had no results and nothing published after three years I couldn’t help but wonder if the research was worth continuing.’\nBut they stuck with it. And in the end, it paid off.\nWhen more glucose is introduced into the yeast cell, the cell releases more Gibbs energy. But that energy isn’t limitless. When it reaches the limit, ‘that’s when the cell starts producing ethanol’, says Heinemann. The same goes for the E. coli bacterium: the cell hits the energy limit, and then it secretes acetate.\nHonestly, I’m surprised no one else tested this process before\nIn other words, the cell has a kind of safety valve that prevents it from fully breaking down the glucose, which limits the energy that is released. Too much energy would damage the cell itself.\nNow that he’s got it all down on paper, it makes a lot of sense, he says. ‘Honestly, I’m surprised no one else tested this process before.’ As his results became more reliable and his research neared its end, he became increasingly fearful that a competitor would publish on the topic before him. ‘I couldn’t believe that no one else had figured this out.’\nSince then, he’s already made other advancements. Now that he’s proved the principle of the safety valve, Heinemann wants to know what takes place in the yeast cells that requires this valve in the first place.\nHe’s looking at movements within the cell itself. ‘What if part of the energy that’s released during a reaction actually displaces the protein, causing another reaction?’ he says. ‘Like a little push, almost. If it’s just one reaction, that’s not that big of a deal. But if it leads to too many reactions, that might lead to chaos within the cell. It might even prevent the cell from functioning.’\nIt’s just a hypothesis so far, but his gut says it’s a good one. He is currently working on a proposal to get a grant from the Dutch National Research Agenda so he can test it.\nExplaining a fundamental mechanism within cells could have many implications. Yeast cells are very important for manufacturing various chemical substances. Potentially, the pharmaceutical industry could also use the knowledge gained to destroy cancer cells. ‘If you know the valve releases energy to stop the cell from being destroyed, why not close it off and see what happens?’', ""Unformatted text preview: Chapter 6: Chapter 6: Microbial Growth Environmental Requirements Environmental Requirements Physical and Chemical Factors Temperature pH Osmotic pressure Oxygen availability Hydrostatic pressure Radiation Temperature Requirements Temperature Requirements\nType Psychrophiles Mesophiles Thermophiles Range 020°C Optimum 15°C 1545°C 2045°C 4570°C 60°C Hyperthermophiles 70120°C 90°C Temperature Requirements Temperature Requirements Temperature Requirements Temperature Requirements Psychrophiles: Cause spoilage of foods while refrigerated Include most human pathogens (body is 37°C) Found in compost heaps Hydrothermal vents, hot springs (above 104°C) Mesophiles: Thermophiles Hyperthermophiles: pH Requirements pH Requirements\nType Acidophiles Neutrophiles Alkalophiles Range pH 1.0 – 5.5 pH 5.5 – 8.0 pH 8.5 – 11.5 Acidotolerant and alkalotolerant microbes can persist for short periods under these conditions, but are unable to reproduce pH Values of Some Environments pH Values of Some Environments\nAcidic 1 2.5 3.54.5 6 8 9 10 Basic 11 ammonia gastric juices vinegar peaches, tomatoes peas, corn, shrimp seawater alkaline lakes/soils soap solutions household pH Requirements pH Requirements Large changes in [H+] can: Disrupt the cytoplasmic membrane Inhibit enzymes and transport proteins Most bacteria are neutrophiles Even in acido or alkalophiles, cytoplasmic pH remains neutral Keeping the Cytoplasmic pH Neutral Keeping the Cytoplasmic pH Neutral Antiport exchange of K+ for H+ in neutrophiles (Na+ for H+ in alkalophiles) Synthesize proteins under acidic conditions Acid shock proteins use ATP to actively transport H+ out of the cell Fermentation produces acids Putrefaction produces ammonias Export wastes to the environment Osmotic Pressure Osmotic Pressure Moderate halophile Require NaCl; found in marine habitats Require NaCl; found in hypersaline habitats like the Great Salt Lake Can grow in salty situations, but grow better without the NaCl; skin bacteria Requires sugars; yeasts & molds Extreme halophiles Osmotolerant Saccharophiles Osmotic Pressure Osmotic Pressure Effect of Osmotic Pressure Effect of Osmotic Pressure Osmotic pressure and water activity are inverse Solution has low water activity = high osmotic pressure Low water activity draws water out of cells via osmosis to dilute external [solute] Water is essential to macromolecule breakdown High water activity may lyse cells by drawing too much water into cell cytoplasm via osmosis Cell wall offers great protection from this Mechanosensitive channels in plasma membrane open Regulating Osmotic Pressure Regulating Osmotic Pressure To prevent growth, reduce water availability inside the cytoplasm Add solutes (sugars, salts) Lyophilization (freezedrying) Dessication (drying) Oxygen Availability Oxygen Availability Major groups based on O2 use and tolerance: Obligate aerobes Facultative anaerobes Obligate anaerobes Aerotolerant anaerobes Microaerophiles Obligate Aerobes Obligate Aerobes Undergo aerobic respiration: O2 required as a terminal electron acceptor No other respiratory paths available In a broth medium: Growth only near top of liquid Limited by the penetration of dissolved O2 from the atmosphere Facultative Anaerobes Facultative Anaerobes In the presence of O2: Undergo aerobic respiration using O2 as terminal electron acceptor Producesmore ATP = more growth capable Undergo anaerobic respiration or fermentation paths less ATP produced, and hence less growth than aerobic In the absence of O2: Facultative Anaerobes Facultative Anaerobes In a broth medium: Growth will be densest at top where O2 is available Growth will occur throughout the depth of the medium via anaerobic respiration or fermentation, or both Obligate Anaerobes Obligate Anaerobes Undergo anaerobic respiration: Ions such as NO3 are required as a terminal electron acceptor No other respiratory paths available In a broth medium: Limited by the penetration of dissolved O from the atmosphere very toxic Growth only near bottom of tube 2 Obligate Anaerobes Obligate Anaerobes O2 presence causes the formation of toxic compounds (superoxides, free radicals) Disrupt cytoplasmic membranes & other cell components Do not have enzymes capable of converting toxic compounds to harmless ones: Superoxide dismutase (SOD): converts superoxides to peroxides Catalase: breaks down hydrogen peroxide to H2O and O2 Peroxidase: converts peroxides to water and NAD+ Aerotolerant Anaerobes Aerotolerant Anaerobes Ignore O2 No toxic effects of O2 due to SOD presence Use fermentation or anaerobic respiration for ATP production In a broth medium: Grow throughout the depth of the medium no denser region at surface Microaerophiles Microaerophiles Too high of O2 content is damaging Low enzymes = inability to adequately prevent damage from toxic species Require 210% O2 concentration Normal atmosphere is 20% Microenvironments with aerobically respiring consortia reduce [O2] to tolerable range Many respiratory pathogens are microaerophiles Oxygen Use/Tolerance Groups Oxygen Use/Tolerance Groups Hydrostatic Pressure Hydrostatic Pressure Barophiles (piezophiles): optimal growth rate where pressure > atmospheric pressure Pressureadapted microbes growing at higher temperatures are mostly Archaea Pressureadapted microbes growing at moderate and cold temperatures are mostly Bacteria Adaptation to pressure is not too extreme Slight genomic differences between pressure adapted vs. normal atmosphere isolates Radiation Ultraviolet Rays Radiation Ultraviolet Rays Damages DNA base pair bindings to produce mutations like thymine dimers Mutations will result indirectly in cell death Inability to replicate chromosome Inability to correctly transcribe mRNA Radiation Ultraviolet Rays Radiation Ultraviolet Rays UV light DNA damage can be repaired: Photoreactivation Blue light energizes a specific enzyme which breaks the thymine dimers Allows normal crosshelix basepairing by hydrogen bonding Dark reactivation Thymine dimers are excised by endonucleases Missing bases in the DNA sequence are replaced by other endonucleases Ionizing Radiation Ionizing Radiation Cause atoms to lose electrons Include Xray and gamma radiation Low levels cause mutation and can indirectly cause cell death High level exposure causes direct cell death Breaks HH bonds, oxidizes double bonds, breaks ring structures, polymerize some molecules Often used as a sterilizing treatment What is Microbial Growth? What is Microbial Growth? Defined as an increase in number Achieved by: Budding Binary Fission Cell duplicates its components, then shares them between 2 daughter cells Daughter cells independent when septum forms betweencell ‘halves’ Bacterial Cellular Growth Cycle Bacterial Cellular Growth Cycle C phase = chromosome replication D phase = delay period Nucleoid partitioning Septation begins Cytokinesis = septation complete Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Log vs. Arithmetic Scales Log vs. Arithmetic Scales Conversion to log scale compresses the distance between data points evenly. Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Culture Growth Phases: Lag Log (Exponential) Stationary Death (Decline) Easily measured during growth in liquid media by spectroscopy or densitometry Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Lag phase: Metabolically active but NO increase in number Adaptation: induce enzymes needed; synthesize new ribosomes, ATP, and cofactors; replicate chromosome Repair cellular components, increase in cell size Unbalanced growth rates of synthesis of cell components varies with one another Length of entire phase varies w/ species & environmental conditions Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Log (Exponential) phase: Population doubles each generation Generation (doubling) time ranges from 7 min to 20 hr – average is 20 min Growth is asynchronous not all cells divide at exact same time Growth rates are saturable; limited to [celluar enzyme] Balanced growth all cellular constituents made at constant rates to one another Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Log (Exponential) phase: Rapid expansion with 20min generation time: also called doubling time Population doubles in number every 20 minutes)\n0 m in 2 0 m in 4 0 m in 1 h r 2 h r 3 h r 4 h r 5 h r 6 h r 1 0 ce lls 2 0 ce lls 4 0 ce lls 8 0 ce lls 6 4 0 ce lls 5 , 1 2 0 ce lls 4 0 , 9 6 0 ce lls 3 2 7 , 6 8 0 ce lls 2 , 6 2 1 , 4 4 0 ce lls Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Stationary phase: Curve horizontal: population growth ceases New cells made at same rate as old cells die (growth rate = death rate) Reasons for stationary phase: Nutrient limitation or O2 limitation Accumulation of toxic wastes Cell density Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Stationary phase: Very common in nature (oligotrophic) Not simply a time when things run out and cell enters a stasis we see changes in: Gene expression: starvation proteins and other proteins, as well as antibiotics, are produced Peptidoglycan crosslinking Nuceloid condensation Endospores formed by certain species Changes make them more resistant to unfavorable conditions Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle Death (Decline) phase: Number of viable cells decreases exponentially Constant number of cells die per hour Usually a logarithmic, but not always so clear Bacterial cell death is defined by the inability to grow (reproduce) Death (Decline) phase: Bacterial Culture Growth Cycle Bacterial Culture Growth Cycle\nWhat causes cell death or loss of viability?\nBuild up wastes/toxins and poor environmental conditions for survival Survival of the fittest to reproduce Viable But Not Culturable (VNBC) Temporarily unable to grow under lab conditions but resuscitate upon entry into different environment Programmed Cell Death Certain % of cells that commit suicide to provide nutrients to survivors Bacterial Death/Lossof Viability Bacterial Death/Loss of Viability Continuous Culture Systems Continuous Culture Systems Continuous growth = a constant state of the growth curve; often exponential phase Achieved in a chemostat chamber: Fresh growth medium added at same rate as spent medium and cells are removed Removes pressure of limiting nutrient Also removes toxic waste buildups Growth rate is adjusted by exchange rate Model Chemostat Model Chemostat Population level and generation time are controlled by dilution rate Increase dilution = increase generation time because less of limiting nutrient is available; density kept low most energy used for maintenance, not reproduction Decrease dilution = decrease generation time because little limitation of nutrient; density also increase Measuring Microbial Growth Measuring Microbial Growth By cell number By cell mass Viable vs. Total Direct Microscopic Counts Coulter Cell Counters Viable Counts Calculations and conversions Total cell weight, or by individual chemical (carbon, protein, etc.) Turbidity Direct Microscopic Counts Direct Microscopic Counts PetroffHauser Counting Chamber Accepts fixed volume (0.1 ml) Count number of cells per volume Cannot distinguish live from dead cells Calculations done to determine original cells/ml Coulter Counter Coulter Counter Automated counting device Microbial suspension directed through a small hole the size of an individual cell Change in electrical resistance when cell passes through the hole = 1 cell counted Can’t distinguish live vs. dead cells or cells from small particles of debris Viable Counts Viable Counts Measures colonyforming units rather than cells (due to possible clumping) Seek statistically “countable” plate having 30 300 colonies Serial dilution, plate count, membrane filters Does NOT necessarily count all living bacteria present in the sample just those able to grow under certain conditions given Used with spread and pour plates, also membrane filtration Viable Counts Via Membrane Filtration Viable Counts Via Membrane Filtration Turbidity (Cell Mass) Turbidity (Cell Mass) Done by measuring the amount of light scattered by a cell More mass = more scatter (proportional) Uses a spectrophotometer to measure optical density (OD) of the cells Create standard curves to determine population density based on turbidity Turbidity (Cell Mass) Turbidity (Cell Mass) Low population of cells = low scatter = low OD High population of cells = high scatter = high OD Top scale = % transmittance Bottom scale = optical density Turbidity (Cell Mass) Turbidity (Cell Mass) Create standard curves to determine population density based on turbidity Done in conjunction with viable plate counts initially OD700nm is plotted against the number of viable cell counts taken at the same time points to produce the standard curve Future growth can be estimated from this established relationship Species and environmental conditions must be identical as when the original curve was produced Standard Curve Standard Curve ...\nView Full Document\n- Fall '09\n- Bacterial Culture Growth, Culture Growth Cycle""]"	['<urn:uuid:6b0ed404-0c33-40e4-a7f6-0814ff2dd3f4>', '<urn:uuid:0ef80001-466f-4e36-95aa-42bee82f8185>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	26	57	2969
16	How can we tell what's actually in packaged foods versus what marketers want us to believe, and what strategies can help us make healthier choices at the supermarket?	The front of food packages contains marketing claims that can be misleading, like 'light,' 'made with whole grains,' or 'natural' - these are unregulated sales propaganda. To know what's really in food, you need to check the FDA-required Nutrition Label and Ingredients List. The Nutrition Label shows nutrients per serving and %DV (Daily Value), while the Ingredients List reveals components in order by weight. For healthier shopping, make a list before going to avoid impulse buys, eat before shopping to prevent unhealthy cravings, and focus on fresh foods found on the store's outer perimeter that don't require labels. When buying packaged foods, look for items with whole grains listed in the first three ingredients and fewer ingredients overall.	"['Reading food labels can be tricky.\nToday, more than ever before, the general public is health conscious and scrutinizing the food they purchase.\nFood manufacturers and marketers know this.\nSo you come across a snack that says “light” or “multigrain,” and you assume the product in it is healthy.\nWhy would they lie?\nThere’s nothing worse than finding out your favorite protein bar is actually just a sugar filled candy bar with deceiving packaging.\nThat’s why today we are dedicating an article to seeing through this deceptive marketing.\nWe’ll do this by showing you how to read a nutrition information panel, as well as what packaging claims you can dismiss as bogus.\nWe’ll also give you some tips on shifting your diet to a more healthy way of eating in general.\nIf you are trying to watch what you eat to lose weight, make sure you grab our healthy eating guide: 10 Levels to Change Your Life.\nIt’s a level up system, with each step shifting you towards improved nutrition strategies. We take it slow and work with your own pace, so the changes you make will be sustainable and permanent.\nYou can download our 10-Level Nutrition Guide when you sign up in the box right here:\n- Follow our 10-level nutrition system at your own pace\n- What you need to know about weight loss and healthy eating\n- 3 Simple rules we follow every day to stay on target\nIGNORE THE FRONT OF THE PACKAGE\nFood manufacturers are not required to place any nutritional information on the front of the package.\nBecause of this lack of regulation, what’s included on the front of the package is what the manufacturers want you to see.\nIn other words, sales propaganda.\nLet’s take a look:\n“Real Fruit Bites” is the product and the company is Nothing But The Fruit.\nFrom the looks of this, I would assume these “bites” are mostly made of strawberry? Because what else would you put in them?\nThe verdict: the number one ingredient in this is “apple puree concentrate.”\nI’ll devote an entire section to bogus health terms marketers use to fool us shortly. I bring up the front of the package now, to tell you it’s not where the real info is.\nIf you want to know exactly what you are eating, you need to look at the Nutrition Label and Ingredients List.\nThat’s where food marketers have to tell the truth (sort of).\nWHERE THE REAL HEALTH CLAIMS OF A FOOD LABEL CAN BE READ\nIn the United States, the Food and Drug Administration (FDA) requires most packaged food to include a label.\nMany other countries around the world have a similar agency regulating food.\nThis is key for us nerds because organizations like the FDA force companies to disclose exactly what’s in the food they’re selling.\nThey do this with a Nutrition Label and an Ingredients List.\nA Nutrition Label identifies the nutrients found in a portion of food, like calories, protein, and vitamins. It’ll look like something like this:\nAn Ingredients list tells you what they placed into the product to get those nutrients.\nIt might look like this:\nBoth are important to analyze.\nYou want to know how many calories you’re getting out of your meal, which can be found in the Nutrition Label.\nYou also want to know what foods they are using to make up those calories, which can be found in the Ingredients List.\nHOW DO I READ A NUTRITION LABEL\nLet’s look at another Nutrition Label:\nThis is taken from a Spiced Pumpkin Pie Cliff Bar.\nServing Size. This is somewhat self-explanatory: serving size is the amount of food, by volume or weight, that is considered one “serving.” New FDA regulations require this size should be typically what someone would eat in a single serving. Which matches what we see here, with “1 Bar.”\nHowever, double check this. Your serving might not match what the manufacturer has in mind. For example, a bag of Doritos has about 10 servings in it.\nDo you really only eat a tenth of bag of Doritos at a time? Look at the Nutrition Label.\nCalories. This is the energy you’ll get from food.\nThis will be one of the most critical pieces of information on the food package. Which is why it’s right under the serving size on the label.\nGoing back to our Cliff Bar example, we can see that one bar is roughly 250 calories.\nI want to talk about that word “roughly.” The FDA allows a 20% leeway in either direction for food manufacturers. That means, legally, that Cliff Bar might have 200 calories…or 300 calories.\n%DV. Have you ever wondered what that “%DV” stands for? It means “Daily Value” and the percent will range from 0 to 100.\n%DV is an estimate of how much of a particular nutrient you are eating with one serving of food. For example, if you ate that Spice Pumpkin Pie protein bar, you would be getting around 8% of the potassium required for a healthy diet.\nHowever, we need to explain come caveats: it’s based on a 2,000 calorie a day diet, whereas the average person needs more than this (calculate your calorie needs here).\nAlso, this is the FDA’s estimate of how many vitamins and minerals you need. Do you really trust Big Brother? Use %DV as a starting point only.\nHere’s some rough guidelines:\n–Less than 5% DV isn’t that much.\n–More than 20% DV is a lot.\nYou’ll notice that some things, like Trans Fat, Protein, and Sugar don’t have %DV. The FDA doesn’t require this information, so most food sellers don’t include them.\nDon’t overlook the Nutrition Label. Yes, they are estimates. But an estimate of the number of calories in a meal is better than no estimate, especially if your goal is weight loss.\nA message I really strike home in our “Why Can’t I Lose Weight?” article is having a good estimate of calories in and calories out. While there is plenty of nuance, weight loss really does come down to calories eaten and expended.\nThat’s why reading the Nutrition Label can be so critical – it helps you with the first part of this equation.\nIf you have no idea how many calories you need to eat in a day to reach your goals, I have a couple of resources for you.\nThe first is our article “How Many Calories Should I Eat Every Day?” It includes a calculator to give you an estimate of your individual calorie requirements, plus tips on what to do with this info.\nAnd the second is for people who are looking for more specific, guided accountability, our uber-popular 1-on-1 Coaching Program. We take all the guesswork out of a weight loss plan and help you start to make better food choices.\nWe’ll tell you exactly what you need to do to get in shape, by changing your nutrition strategy, creating an exercise program, and offering personal accountability. Many Rebels have turned their lives around by becoming a client, and we’d be happy to talk with you to see if we could be a good fit for each other.\nClick on the big image below to learn more:\nWHAT NUTRIENTS DO I WANT MORE OF? WHAT NUTRIENTS DO I WANT LESS OF?\nLet’s chat quicky about some of the nutrients identified on the label.\nWe’ll first talk about dietary fat: the good, the bad, and the ugly.\nThe terms “saturated” or “unsaturated” refers to the bonding of hydrogen atoms to carbon atoms in a fat molecule. When every carbon molecule is bound to the maximum number of hydrogen atoms, the fat is “saturated.” If some carbon molecules are double-bonded to each other (and bonded to fewer hydrogen atoms), the fat is “unsaturated.”\nLet’s chat about why this matters.\nThe good: unsaturated fat. This is the “healthy” type of fat. There are two types, monounsaturated and polyunsaturated (don’t worry too much about the difference, it has to do with the number of carbon double-bonds in the molecule). Unsaturated fat is typically liquid at room temperature, and you find it in plant-based products (like nuts, avocados, coconut, etc). When people talk about “healthy fats,” unsaturated is generally what they are referring to.\nWhat about saturated fat? There’s a lot of conflicting studies out there on saturated fat, and it’s been vilified by marketing companies for years. So what’s the truth? Personally, I believe that saturated fat is an important nutrient that we need in our system. You can check out this article for an interesting debate on the subject.\nThe bad: trans fat. There has been a good deal of hype about trans fat in recent years. So what’s the deal? Trans fat is technically an unsaturated fat, but the fat molecule has a different type of carbon double-bond that gets created when oil is “partially hydrogenated.” Hydrogenation is a process used in food production to increase shelf life, which is why it’s often found in processed food like frozen pizza and doughnuts. Why does all this matter? The “trans” type double bond causes the fat to behave differently in the body, and NOT in a good way. Trans fat can contribute to atherosclerosis, which could lead to a heart attack. Keep your intake of trans fat as low as possible.\nIt should also be noted, that if food has less than 0.5g of trans fat per serving, the FDA allows food manufacturers to round down – meaning “0.” So even if the packaging says “Zero Trans Fat,” it might actually still have some.\nWe’ll end this article by talking about ways to avoid this concern, by sticking to “real food.”\nCholesterol is the next big player on the list. Cholesterol is an important building block in cell membranes. We get some from food, and some is made in our liver. Cholesterol is mainly found in animal products, just like saturated fat. In my opinion, cholesterol is not the villain it’s made out to be!\nSodium, and often Potassium, are also listed in this part of the Nutrition Facts label. These are electrolytes. Should you avoid sodium, because of a possible increase in blood pressure? Eh, the findings on this are mixed. A little bit of salt on your food (or in your food) should be fine.\nCarbohydrates include simple carbs (sugars, like glucose, fructose, dextrose) and complex carbs (fiber and polysaccharides).\nDietary fiber is great stuff! There are two types: Soluble fiber is digestible and gets absorbed by your body. This type of fiber helps keep your blood sugar levels stable.\nInsoluble fiber is not digestible, so it stays in your GI tract and helps “keep things moving.” Trust me, this is a good thing.\nPrioritize fiber in the foods you eat.\nSugar is a simple carbohydrate. It can be absorbed by your gut very quickly because it doesn’t need to be broken down by enzymes first. Sugar is not necessarily bad for you, but consuming large amounts of sugar can cause your blood sugar levels to become unstable.\nOther Carbohydrates generally refers to complex (large) carbohydrates in food, such as starch. These large molecules are broken down by enzymes into sugar, but the process takes a while. Complex carbohydrates keep your blood sugar levels more stable than do simple carbohydrates.\nProtein is one of the main building blocks for muscle and other body tissues. There is normally not a %DV for protein because the amount of protein that each person needs is quite variable. You can check out this article for an idea of how much protein you should be eating.\nAt the bottom of the Nutrition Label, you’ll also see a breakdown of micronutrients.\nLet’s go over some of these key players:\nCalcium: needed to build bones. This is especially important for women, because bone mass decreases after menopause, so build your bones strong while you are young!\nVitamin D: needed to aid the absorption of calcium. Some of this comes from food, and some is made by your body when you are exposed to sunlight.\nIron: needed to make hemoglobin, the oxygen-carrying substance in blood cells. Women are more at-risk for being iron deficient, while men are more at-risk for being iron overloaded.\nYou may notice that some labels list more micronutrients than others. If a micronutrient is missing from a label, that means there is not any of it in the food product.\nLet’s look at the Nutrition Facts for a can of soda.\nSee any micronutrients on there? That’s why soda is referred to as “empty calories.”\nHOW DO I READ AN INGREDIENTS LIST?\nLet’s take another look at our Cliff Bar, but this time, looking at the Ingredients List:\nAll the ingredients are listed in order of quantity by weight, so the first few ingredients make up most of the product.\nFor example, the Spice Pumpkin Pie Cliff Bar is mostly brown rice syrup (sugar), oats, and cane syrup (more sugar), in that order. The 4th ingredient, “soy protein isolate” is where this protein bar starts to get some of its protein.\nI know. Perhaps “Carbohydrate Bar” would be more appropriate than “Protein Bar.”\nThe first few ingredients will give you an idea of what you’re actually eating. So this Cliff Bar is sugar, oats, and protein from soy.\nPay attention to the ingredients list. This tells you exactly what you’re eating.\nBecause again, the front of the package can’t be trusted.\nBOGUS CLAIMS ON FOOD LABELS TO WATCH OUT FOR\nMarketers love the front of food packages.\nIt’s where they get to use all their fun terms that sound healthy.\nLet’s go over some:\n“Light.” When you see “light” or “lite” on a package, it means it’s been processed to reduce calories or fat. However, when fat is reduced sugar is often increased to compensate. Check the ingredients list to find out for sure.\n“Made with whole grains.” What does “made” mean? It means it contains “some.” Check the ingredients list to see just how much. Is a whole grain in the top three? If not, it’s mostly not whole grains.\n“Natural.” This has to be good right? Natural, i.e nature, means good for you?\nEh, it just means that at some point, the ingredients came from a natural source like apples. Going back to our example above, you could argue that the protein bar contained “natural” ingredients because brown rice syrup is derived from rice.\nHowever, a protein bar doesn’t resemble anything grown from a plant. And brown rice syrup really doesn’t have much in common with actual brown rice.\nFor the most part, the more ingredients on a list, the less natural it is.\n“No added sugar.” This sounds good, until you realize that some foods like dried fruit are just naturally high in sugar. Double check the ingredients to find out.\nWant some other clever food labels to be on the lookout for? We have a whole article right here diving into the subject.\nFOOD WITHOUT LABELS\nWant to know how to avoid worrying about Nutrition Labels and Ingredients Lists?\nYou’ll find a lot of the following there:\nDo you know what most of this doesn’t have? Food labels.\nA banana won’t have manufacturer disclosed information, because no one manufactured it.\nIt’s a banana.\nIt’s what we here at Nerd Fitness call “real food.”\nWe discuss this in great detail in our Healthy Eating Guide.\nFood that grew in the ground, on a tree, ran on the land, swam in the sea or flew through the air, would fall into this category. Meat, fish, eggs, vegetables, fruits, nuts are all great examples of REAL food.\nMost of this food is found on the outer rim of the grocery store. And there aren’t too many food labels here, besides a barcode.\nIf you want to start eating better, buy whole foods that don’t have food labels on them. They’ll generally have more nutrients and fewer calories, which is an ideal recipe for weight loss.\nThen you don’t have to worry about food manufacturers deceiving or trying to hide ingredients.\nThe banana is a banana.\nHowever, while eating “real food” is a key component of a successful weight loss strategy, it can still be useful to have an idea on the amount of calories in foods such as bananas.\nCalories in and calories out is the central component of weight loss (thermodynamics for ya), so having a rough idea of calories you’re consuming is key. Even in “real food.”\nMaybe one day we’ll get robot vision to analyze the calories in everything we look at.\nFor now, you can Google ‘[food] + calories” or check MyFitnessPal to educate yourself!\nAlright, I think that about does it for today’s article on reading a Nutrition Label.\nDid I miss anything?\nDo you have any tips or tricks on deciphering a Nutrition Label or Ingredients List?\nDo you have any other examples of foods that claim to be one thing, but whose ingredient list proves it’s another?\nLet me know in the comments!\nPS: If you’re reading a Nutrition Label to try and lose weight, you’re on the right track! As we state in our Rules of the Rebellion, “You can’t outrun your fork!” Keep at it and keep asking questions!\nPPS: If you want more 1-on-1 help to lose weight or shift your nutrition strategy, I’ll again remind you of our Online Coaching Program! We work with busy people just like you to develop and execute plans to level up their lives! Click right here to learn if we are a good fit for each other.\nAll photo sources can be read right here.', ""With a few simple tricks and tips, you can make your next trip to the supermarket a healthy one. Here are must-have, must-know tips and tricks.\nBefore you go\nWhen and where you shop can make a difference in what you bring home. Be smart with the following suggestions:\n- Make a list. You'll avoid unhealthy impulse buys, save money and won't forget the must-have ingredient for tonight's dinner. But try to be a little flexible, especially if the store is out of the item you want or if a similar item looks fresher.\n- Pick the right supermarket. Whether it's convenient or offers what you like (vegetarian, ethnic cuisine, seafood or butcher counter), stick to a store that makes it easy for you to get home with foods you enjoy.\n- Pick the right time. Avoid crowds (and long waits at check out) by shopping early in the morning or on weekend evenings. Or ask your cashier about the best times to shop there.\n- Eat first. A healthy snack or meal will ensure your grumbling tummy won't let unhealthy cravings decide what goes in your cart.\nIn the store\nYour list probably includes a combination of both fresh and packaged foods. Packaged foods will contain a nutrition labels that includes Nutrition Facts (which gives info on serving size, calories and 13 nutrients) and a list of ingredients. Here's an example, what it all means and some helpful recommendations.\nThe number of calories listed is per amount of food specified — not for the entire package and not for a Canada’s Food Guide serving. This is important to know because the amount of calories can be for a much smaller amount of food than we think.\nSo, in this example, if the entire package is 250 ml and you eat it all, you just had 160 calories.\nHere is the amount and percentage of the daily value of nutrients per amount of food specified. The % daily value is useful when you’re watching what you eat and when comparing one packaged food to another.\nFor example, the Dietitians of Canada and the Canadian Diabetes Association recommend that we eat less fat, saturated fat and trans fat, cholesterol and sodium; but we should get more carbohydrate, fibre, vitamins A and C, calcium and iron.A good rule of thumb to follow:\n- Less than 5% daily value means that serving size contains a little bit of that nutrient.\n- More than 15% daily value means a serving contains a lot of it.\nAnd the ingredients list will tell you a lot about what you’re buying.\nIngredients are listed from most to least according to weight. This means the product contains more of the ingredients listed at the beginning of the list and less of those at the end.\nUnfortunately, some of the things you’re trying to avoid are listed under a different name. For example, these common items go by the following “codenames”:\n|If you’re trying to avoid…||…Watch for these common ingredients|\n|Saturated fat||Bacon |\nHydrogenated fats and oils\nPalm or palm kernel oil\nPowdered whole milk solids\n|Trans fat||Hard margarine |\nHydrogenated fats and oils\nPartially hydrogenated fats and oils\n|Salt (sodium)||Baking powder / soda |\nMonosodium glutamate (MSG)\nSodium alginate / benzoate / bisulfate / proprionate\n|Sugars||Note: Words that end in “ose” indicate a sugar; “ol” indicates a sugar alcohol.|\n|Cane juice extract |\nHigh fructose corn syrup\nGuide to smarter choices\nNow that you understand food labels, % daily values and ingredient lists better, use your skills to make healthier choices. Here are some helpful tips and tricks for each of the four food groups:\n|Food group||Go with||Pass on|\n|Fruit and vegetables|| || |\n|Grain products|| || |\n|Milk and alternatives|| || |\n|Meat and alternatives|| || |""]"	['<urn:uuid:b783aa67-101e-486e-91b0-b1ae5bcb12ec>', '<urn:uuid:ec409998-7f27-4f6e-8cea-f10ebb5c5e0d>']	open-ended	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T19:47:23.148022	28	118	3580
17	researching protein digestion mechanism pancreas small intestine trypsin protease function process	Trypsin is a protease enzyme that breaks down proteins in the digestive system. It's initially produced in the pancreas as inactive trypsinogen, then carried to the small intestine where it becomes activated. Once active, it breaks down proteins into smaller peptides through hydrolysis of peptide bonds in the duodenum. These peptides are further broken down by other proteases into amino acids that can be absorbed by the bloodstream. This process is essential because proteins are too large to be absorbed directly through the small intestine lining.	"[""Being a part of our digestive system, trypsin works to break down proteins, making it an enzyme called protease. Initially, protease is created in the pancreas in the form of a bigger inactive molecule and subsequently carried to the small intestine. Inside the intestine, this enzyme gets activated and begins digesting the ingested food molecules. In addition, protease also regulates several other digestive processes. Absence of protease production in our pancreas results in a digestive disorder known as cystic fibrosis.\nInside the stomach, proteases partially break down foods composed of different amounts of proteins and the smaller particles of food are then carried to the small intestine. Basically, protease is also a type of enzyme that accelerates the process of breaking down proteins in ingested foods. As we are all aware, amino acids form the basis of proteins and these are connected together by means of peptide bonds. The final objective of digesting proteins is to break them down into amino acids. Subsequently, the amino acids are utilized in the metabolic process in the cells.\nInitially, trypsin is produced in the pancreas as trypsinogen, which is a zymogen (an enzyme precursor molecule). Trypsinogen is a relatively larger molecule that remains inactive till it gets into the small intestine and enteropeptidase acts on it. Enteropeptidases are also a type of protease. When enteropeptidase has acted on trypsinogen, it can activate itself on its own. Aside from breaking down proteins during the digestive process, trypsinogen also works to activate several other zymogens. Since trypsinogen activates several proteases, it plays an important role in the digestive process. Pancreatic juice encloses an inhibitor that interferes with the activities of trypsin all the while during it is in the pancreas.\nTrypsin, together with several other proteases like chymotrypsin, belongs to a class called serine proteases. Proteases belonging to this class comprise endopeptidases. When they are inside a protein molecule, these proteases slash rather than gnaw at the ends.\nIn many ways, trypsin and chymotrypsin are akin to one another in many ways, such as chemical composition and structure. However, they are different too. Their preference for substrates is entirely different. While the amino acids lysine and arginine cleaves trypsin, chymotrypsin has a preference for tyrosine, tryptophan and phenylalanine. Both these proteases are in their best when they are in a neutral pH environment, such as that in the small intestine.\nThe absence of trypsin as well as other digestive enzymes secreted by the pancreas is known to be a key factor for developing a digestive disorder known as cystic fibrosis. When this happens it results in interferences in the intestines to gather as well as take up nutrients from the ingested foods. Usually, all newborns are examined to find the presence of trypsin in their blood stream. An elevated level of this enzyme in the blood stream is an indication of the fact that the baby may develop cystic fibrosis. For other people, cystic fibrosis can be diagnosed by examining their stool. In case the results of the test are negative, more tests need to be undertaken.\nTrypsin works as a catalyst in the hydrolysis process of peptide bonds inside the duodenum by degrading proteins in much smaller peptides. Subsequently, these peptides are hydrolyzed further by other proteases to form amino acids so that they can be taken up by the blood stream. In fact, tryptic digestion is an essential step in the absorption of proteins, because generally proteins are very large and, hence the blood stream cannot absorb them via the linings inside the small intestine.\nOur pancreas produces trypsin in the form of an inert zymogen trypsinogen. Following stimulation by cholecystokinin, the pancreas secretes this enzyme along with other pancreatic juices through the pancreatic duct into the duodenum, which is the opening of the small intestine. When trypsin is introduced into the small intestine, another enzyme called enteropeptidase converts the inactive trypsinogen into an active trypsin via proteolytic cleavage. Aside from this, trypsin can also auto catalyze when trypsinogen is used in the form of a substrate. This means of activation is very common for nearly all serine proteases. When this happens it helps to put off any type of activation and protects the pancreas from auto degradation.\nTrypsin is an enzyme that digests or breaks down proteins. This enzyme is secreted into the small intestine along with pancreatic juices when one is eating a meal. It is secreted by the pancreas in the form of a dormant proenzyme known as trypsinogen. When this enzyme is released in the intestine, another enzyme known as enteropeptidase is released from the cells present in the intestine. Enteropeptidase separates a little portion of trypsinogen to activate the trypsin enzyme. When trypsin is activated, it aids in breaking down proteins in the ingested foods. In addition, trypsin also makes other trypsinogen molecules active. At the same time, a number of other enzymes that help to break down proteins are secreted into the small intestine along with the pancreatic juice in the form proenzymes. As a result, trypsin is extremely necessary for the normal functioning of our digestive processes, which are responsible for converting proteins in ingested foods into amino acids. Eventually, these amino acids are taken up by the body.\nTrypsin inhibitors can be described as compounds having the aptitude to firmly bind to trypsin and interfere with the enzyme's ability to digest or break down proteins. Bowman-Birk inhibitors present in soybeans as well as a number of other cereal grain seeds and legumes are an example of common organic dietary substances which interfere with the activities of trypsin. Consuming excessive amounts of foods that contain trypsin inhibitors may reduce the nutritional value of the proteins in the foods you eat. However, when you cook such foods, it basically makes the dietary trypsin inhibitors inactive. Moreover, our pancreatic cells also produce another form of trypsin inhibitor which also obstructs the activation of premature trypsin as well as other enzymes that break down proteins. This occurs when these enzymes are in the pancreas and have not been released into the duodenum. When this occurs, it puts of the self-digesting ability of the pancreas.\nThere are a number of individuals who are born with some kind of mutation in parts of their genetic matter, which codes production of trypsinogen in the pancreas. Such mutations are inherited disorders and they can change the structure of trypsinogen, thereby creating a condition wherein the activation of trypsinogen to trypsin is not regulated properly any more. As a result, the trypsin inhibitor tissues also do not function properly. However, some parts of the pancreas may still be able to self-digest, resulting in pancreatitis, a condition wherein the pancreas is inflamed. Sometimes consumption of excessive alcohol or a gallstone blocking the duct of the pancreas secreting its juices and enzymes may also lead to improper activation of trypsin as well as pancreatic inflammation. It is worth mentioning here that acute pancreatitis is a health disorder which may result in irreversible damage to the tissues and even put off the secretions necessary to carry out the regular digestive processes.\nAside from the pancreas, trypsin is also found in other tissues, albeit in smaller amounts. These include the skin, digestive organs, liver, kidneys, immune system and the brain cells. In these tissues, trypsin may be engaged in a number of common cell processes.\nWhile the precise role of trypsin in combating cancer is yet to be ascertained, trypsin as well as tumour associated trypsin inhibitors can be manufactured at high levels in the carcinogenic tissues. Enhanced production of such inhibitors in carcinogenic tissues may be related with poor cancer prediction.""]"	['<urn:uuid:e100d901-504d-41bf-8e0b-8d50b560f322>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T19:47:23.148022	11	86	1274
18	indemnify employees vs directors tort liability	For employees, indemnification allows managers to recover the full amount of damages from negligent employees to avoid unjust enrichment. For directors, indemnification by the corporation covers legal costs and monetary damages, but only if they acted honestly and in good faith in the corporation's best interests. Directors cannot be indemnified if they breach their fiduciary duty.	"['When are you liable for another’s acts, and how can you protect yourself?\nAs a fitness manager, you recognize that certain acts result in legal liability. Injuries caused by worn flooring surfaces, equipment malfunctions and inappropriate program designs exemplify what acts have legal consequences. However, the critical issue is who may be legally responsible for injuries. If one of your personal fitness trainers designs an inappropriate program for a client and that client is injured, who is liable—you or the trainer?\nThe answer depends on several factors, including vicarious liability and scope of employment.\nVicarious liability imputes the careless or wrongful act (called a tortious act, or tort) of one person (e.g., the trainer) to another (e.g., you, the manager) because of the employer-employee relationship. You may be held liable even if your conduct is blameless.\nVicarious liability is governed by the doctrine of respondeat superior, which translates to “let the person higher up answer” (Emanuel 2001). This doctrine is activated when employees commit torts during the scope of their employment (Emanuel 2001). The rationale is that employers and managers bear the burden of screening, hiring and orienting a fitness staff with stellar knowledge, skills and abilities. Moreover, managers are responsible for auditing certification status, maintaining general liability insurance and sustaining business costs, such as monetary damages for injuries. Note that this doctrine applies to all torts, including intentional ones, provided the wrongful acts were committed within the scope of the staff member’s employment. Acting for personal purposes is outside the scope of employment and does not activate the doctrine.\nGenerally, a tort is within the scope of employment if the trainer/instructor is acting with the intent of furthering the business purposes of the facility (Emanuel 2001), even if the means of doing so might be suspect or forbidden.\nSeveral scenarios prove to be a bit complex with regard to scope of employment. First, employees’ commute to and from work is usually considered outside the scope of employment (Emanuel 2001), since you have no control or apparent authority over your staff at this time. However, the use of cell phones could change this opinion, since you might, in fact, be conversing and doing business with staff members during their commute. For example, trainers frequently use their cell phones to check for canceled appointments, and fitness managers frequently call to check on trainers’ timecards and monthly quotas. Moreover, commutes to professional conferences, continuing education seminars and staff meetings are work-related and should be deemed within the scope of one’s employment.\nA second scenario involves employees who make short digressions for personal reasons while on business trips or commutes. This is called frolic and detour. Examples include stopping for coffee, using a restroom, going to the bank or grabbing some lunch. Modern courts hold that employees are within their scope of employment if the deviation is “reasonably foreseeable,” and courts determine this by looking at the distance of the detour (Emanuel 2001). Lunch and restroom breaks are certainly foreseeable. If the digression is slight in terms of distance/mileage, then the detour is within the scope of employment. If the detour is of a very long distance, it is considered unforeseeable and not within the scope of business. Unforeseeable detours include lengthy social visits or illegal activities. If the tort is committed during this unforeseeable detour, then the manager is not liable for that trainer’s tort under respondeat superior.\nBecause vicarious liability is characteristically applied to employees, not independent contractors, the alleged wrongdoer (a.k.a. “tortfeasor”) must be legally classified as an employee.\nEmployees are subject to the control of the person who hired them (Emanuel 2001). This control applies to the physical details of all work. Trainers or instructors who are employed by commercial health-club chains are classified as employees.\nIndependent contractors are individuals who, although hired by you to perform certain jobs, are not under your immediate or apparent control and may do their work as they see fit (Emanuel 2001). Trainers who solicit and train clients on their own are independent contractors, even if they pay you a fee in exchange for access to equipment. You are usually not liable for the torts of independent contractors, with some exceptions.\nNegligent Hiring and Supervision. The first exception is your own liability. If you hire an independent contractor you know or suspect will not perform work safely, diligently and professionally, you may be liable for your own negligent hiring and/or negligent supervision of that individual (Rest. 2d §411). Likewise, your failure to inspect the work of the hired individual may prove risky. This liability is particularly pertinent for fitness managers who hire trainers without requiring any degrees or certifications. By knowingly hiring such individuals, you may be putting yourself at risk for liability, absent concrete proof of that person’s competency.\nDelegating “Nondelegable” Tasks. A second exception deals with nondelegable duties (Emanuel 2001). Some duties are deemed so important that they should not be entrusted to anyone. If you hire independent contractors to perform such tasks and they perform poorly, you risk liability. An example is the duty of landowners to keep the premises safe for business visitors. As a manager, you will be held liable for the negligent maintenance of club equipment, facilities or premises even if that maintenance is performed by independent contractors.\nProviding the Appearance of Employment. Vicarious liability may be activated via agency by estoppel, or apparent authority, if an independent contractor appears to be an employee to the average person (Connaughton & Eickhoff-Shemek 2003). If your facility’s actions have created the appearance that an independent contractor is an employee, then you and/or the facility may be liable for that independent contractor’s wrongful acts. For example, members of a facility may reasonably assume that an independent contractor who wears a facility uniform is an employee (Connaughton & Eickhoff-Shemek 2003).\nDefining Employment Classification. Finally, usage of the term independent contractor in employment contracts is open to legal interpretation. Various factors must be evaluated to properly classify a trainer/instructor before liability is imputed to you (Connaughton & Eickhoff-Shemek 2003). Factors leading to a classification of employee, rather than independent contractor, include\n- employer control over the worker’s tasks\n- employer direction or supervision\n- employer provision of tools or supplies\n- long-term employment\n- regular payment schedules, as opposed to payment at the completion of the job\nTo enhance professionalism, you must recognize the legalities and consequences of employment classification. Independent contractors will be held personally accountable for their tortious acts, absent any of the aforementioned exceptions. Encourage your independent contractors to carry liability insurance and to use protective documentation (i.e., waivers) to legally safeguard themselves.\nEmployees do not have this same burden. However, your responsibility for employees carries a large burden, since you may be held accountable for their wrongful acts. To manage this risk, keep the following tips in mind (Connaughton & Eickhoff-Shemek 2003):\n- Diligently hire, or contract the services of, competent professionals.\n- Put all information regarding the details of employment (status, pay, burden of purchasing insurance, etc.) in writing.\n- Require independent contractors who work for you to purchase professional liability insurance. (Most employees will be covered by your general policy, but it might be worthwhile to recommend that they, too, purchase their own insurance to minimize the risk to you.)\n- Carefully inspect or supervise all employee activity. u\nVicarious liability seems to work in favor of negligent employees, since they escape responsibility by shifting their legal burdens onto you. This apparently unjust situation carries a remedy: indemnification. Indemnification allows you to recover the full amount of a damages payment from a negligent employee (Rest. 3d §22[a][i]). Therefore, unjust enrichment is avoided, and all parties have a legal remedy for their respective misfortunes.\nBecause the rules on indemnity vary from state to state, it is impossible to assert that every employer will have a claim for indemnity against his or her employees. In general, it is fitting to suggest that if you are only vicariously liable for another’s conduct, you will be indemnified.\nMore often than not, employment contracts contain provisions for indemnification. Under this type of contract, all employed individuals, regardless of classification, are subject to the law if they fail to conduct themselves in a prudent, ethical and professional manner.', 'Protecting Directors from Civil Liability Through Indemnification\nA likely question an entrepreneur may ask themselves early in their venture is “how do I protect the directors of my company?” They may (or perhaps should) think about this because in all likelihood, they will be one of, if not the only director of their business during its early phases following incorporation. Please note: this post assumes that the company in question is incorporated under the Business Corporations Act of Alberta\nIndemnification refers to one party’s agreement to secure another against responsibility for their actions, or to give security for the reimbursement of a person in case of an anticipated loss. In this case, it refers to a corporation’s agreement to make a director whole, should they be subject to legal proceedings as a result of their actions in their capacity as a director of the corporation.\nGenerally speaking, the Business Corporations Act (the Act) allows corporations to indemnify their directors for both legal costs incurred, as well as any monetary damages that arise from a director’s conduct in relation to the business. In order to benefit from such indemnification, a director must have “acted honestly and in good faith with a view to the best interests of the corporation.”\nAn Alberta corporation is not permitted to indemnify its directors for their actions if they have not acted honestly and in good faith with a view to the best interests of the corporation – that is, if they have breached their fiduciary duty to the corporation. If a director has breached his or her fiduciary duties to the corporation, any indemnity the corporation has offered will be void.\nThe scope of conduct that may be indemnified under the Act is very broad. Section 124(1) of the Act states:\n“…a corporation may indemnify a director or officer of the corporation, a former director or officer of the corporation…against all costs, charges and expenses, including an amount paid to settle an action or satisfy a judgment, reasonably incurred by the director or officer in respect of any civil, criminal, or administrative action or proceeding to which the director or officer is made a party by reason of being or having been a director of that corporation or body corporate…”\nWhen Are Directors Entitled to Indemnification?\nIn Alberta, a director is only entitled to indemnification by the corporation for all costs, charges and expenses, including an amount paid to settle an action or satisfy a judgment in a civil context if they (i) were substantially successful in defending the claim; (ii) acted honestly and in good faith with a view to the best interest of the corporation; and (iii) is fairly and reasonably entitled to indemnity.\nA corporation that does not contain indemnity provisions in its by-laws will still be liable for any loss incurred so long as these criteria are met. If indemnification provisions found in either the corporation’s by-laws, or in an agreement between the corporation and a director impose mandatory indemnification, it will of course be liable to do so.\nHow to Indemnify Directors\nIndemnification provisions can be found within a corporation’s by-laws. If a corporation seeks to provide its directors with a wide range of protection, these provisions do not need to be particularly robust. Any attempt to predict the types of conduct or liabilities that the corporation anticipates indemnifying its directors against may simply limit its ability to protect its directors.\nIf the company’s bylaws do not provide indemnification provisions that are acceptable to a potential director, indemnification provisions may be included within a written agreement between the corporation and the director. This method provides the greatest flexibility as each agreement can be tailored to suit the needs of both the corporation and the individual director.\nSome things that indemnification provisions should contemplate include whether the corporation is required, or simply permitted to indemnify its directors (and in which circumstances), the timing of indemnity payments, and out of court settlement. Indemnification provisions that do not require the corporation to indemnify its directors should also consider a mechanism to oblige the corporation to do so such as arbitration.\nCorporations that provide the widest range of indemnity to their directors often simply state in its indemnification provisions that the corporation must indemnify the director to the greatest extent authorized under the relevant law. Where it is desirable to minimize the short-term financial impact of litigation on directors, indemnity provisions may require the corporation to advance defence costs as they are incurred. Such provisions should also contemplate whether the corporation is required to indemnify the director for out of court settlements, as opposed to simply court judgments.\nWhat Indemnification Provisions Do Not Cover\nIndemnification provisions do not cover directors’ actions when they are not made in good faith with a view to the best interests of the corporation.\nIn cases where a director is being sued by the corporation or its shareholders, including in derivative actions, a corporation may only indemnify a director for their legal expenses. This leaves directors exposed to liability for corporate or shareholder damages arising from their action (or inaction as the case may be). Why is this? Most derivative actions against directors include a claim for breach of fiduciary duty. If this claim is successful, and a breach has been found, a director will have been found not to have acted in good faith with a view to the best interests of the corporation, and indemnity would not be available in any event.\nHamish Gray is a member of the BLG Business Venture Clinic and is a third-year law student at the Faculty of Law, University of Calgary.\n Black’s Law Journal; 2nd ed; online, <a href=""https://thelawdictionary.org/indemnify/"" title=""INDEMNIFY"">INDEMNIFY</a>\n Business Corporations Act, RSA 2000 cB-9 s124 [the Act]\n Act supra note 2 s124(3)\nBlog posts are by students at the Business Venture Clinic. Student bios appear under each post.']"	['<urn:uuid:f72fd326-9242-4b42-b527-adcea9114c24>', '<urn:uuid:f9f41655-9427-49e4-83bf-cc532fc552b3>']	factoid	direct	short-search-query	similar-to-document	comparison	expert	2025-05-12T19:47:23.148022	6	56	2355
19	who came up with availability cascade theory and what year was it published	The concept of availability cascade was first developed by professors Cass Sunstein of Harvard and Timur Kuran of Duke University in 1999, publishing their work in the Stanford Law Review.	['We all have seen it happen. A developer brings a proposal to a community. It is leading edge and promising. Local council loves the concept – so the proponent believes the community is a willing host. The developer beavers away on due diligence and approvals for a year or two, then returns to council for a zoning amendment. Council refuses it, due to negative community comments typically received hours before the meeting. The developer, their agent and the various government offices that recommended approval, are all in shock. Why did this happen?\nThe question isn’t a matter of Why, but How? Your elected officials were probably the victims of an availability cascade, a term explaining how misinformation can affect decision-making. It doesn’t matter that the misinformation is right or wrong, true or false. It is just misinformation that recirculates in the community in cascades of conversation, and for no other reason than its availability, it becomes a common belief.\nThe concept of availability cascades should be kept in mind as we hold another municipal election. Is the candidate susceptible to suggestion and whisper campaigns? Does this person quickly make up their mind on complex issues, or do they support deferring decisions until more information is available to them? Do they explore opinions or do they parrot their circle of influence?\nThe idea of the availability cascade was first developed by professors Cass Sunstein of Harvard and Timur Kuran of Duke University in a 1999 paper published in the Stanford Law Review, available on the Social Science Research Network. They describe an availability cascade as “a self-reinforcing process of collective belief formation by which an expressed perception triggers a chain reaction that gives the perception of increasing plausibility through its rising availability in public discourse”.\nSounds like a lot of gobbledygook, but it’s based on William James’ “dual process theory” – the way we reason. One way we think is based on the fast, automatic, unconscious thought patterns of our caveman ancestors, dominated by fear and aversion to risk. The other thought process is the slow, analytical, rational method of reasoning that developed as civilization evolved.\nAs an analogy, our brain’s decision-making process is like a man riding an elephant. The man is the slow thinker and the elephant thinks automatically and quickly. The man may be smarter but he has no control when the elephant sees a mouse.\n(If your brain concluded that elephants are afraid of mice, then you are a victim of an availability cascade. Apparently, there isn’t much evidence to prove that musophobia is general among pachyderms.)\nThe availability cascade is said to have influenced many headline-grabbing stories over the years: vaccination scares (MMR), environmental stories (Love Canal) and overblown food safety concerns (Alar Scare). A website called Retro Report purports to expose “the truth now about the big stories then”.\nEven people who are educated and well read can be more susceptible to availability cascades. In his book, Thinking Fast and Slow, Daniel Kahneman explains how the emotional power of examples and their perceived frequency causes a mental shortcut to conclusions within people’s brains. It explains why people think celebrities have high divorce rates, because the media rarely reports on movie stars that stay married. It can also explain why politicians can rush to kill a project or a policy if they get two or three negative phone calls about it.\nAs the municipal election nears, consider which candidates can be easily influenced by people behind availability cascades. Sunstein and Kuran describe availability entrepreneurs as organizations and individuals who “attempt to trigger availability cascades likely to advance their own agendas.” Availability entrepreneurs are everywhere: gossiping in coffee shops, gabbing on social media, emailing your Councillor.\nDo you plan to vote in the next election? Consider thinking fast and slow before marking an X on your ballot. The future of prudent decision-making in your community depends on it.']	['<urn:uuid:f6f3129c-30b1-456a-a021-f5d2878d7243>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T19:47:23.148022	13	30	651
20	how much money spent supreme court races 2015 2016	In the 2015-16 election cycle, more than $69.3 million was spent on state supreme court races, with spending by outside groups constituting forty percent ($27.8 million) of that total.	['JUDICIAL RECUSAL IN THE POST-CITIZENS UNITED WORLD\nBy: Bryan Mette, Volume 103 Staff Member\nI. CITIZENS UNITED AND CAPERTON\nIn 2010, the U.S. Supreme Court struck down federal limits on independent expenditures by corporations holding that it was unconstitutional under the First Amendment to prohibit political speech based on a speaker’s corporate identity. In doing so, the Court reiterated once more that independent expenditures constituted political speech. As a result of overruling of prior precedent, the court’s decision struck down the federal statutory provision barring the expenditures of corporate funds for express advocacy. Intensely controversial to this day, the decision arouses ardent defense or biting criticism among supporters and opponents, respectively. And the fallout from it has prompted calls among critics to overturn the decision through a constitutional amendment.\nIn a case decided the year prior to Citizens United, the Court held—on a five-to-four vote—that the Due Process Clause required mandatory judicial recusal in certain circumstances. In Caperton v. A.T. Massey Co., the Court held that a litigant’s due process rights were violated when a judge participated in a case despite the opposing party’s expenditure of exorbitant sums of money on behalf of that judge’s campaign. Specifically at issue was the financial support of the judge’s campaign from Don Blankenship, the CEO, president, and chairman of the board of the appellee. Besides donating the maximum allowable contribution to the judge’s campaign, the actions that raised eyebrows were Blankenship’s $2.5 million contribution to a political action committee (PAC) called “And For The Sake Of The Kids” and $500,000 in independent expenditures to solicit donations and run television and newspaper advertisements. Both forms of financial support were on behalf of a candidate for the Supreme Court of Appeals of West Virginia.Blankenship’s preferred candidate won and the state supreme court—over calls for the disqualification of that judge—overturned a substantial verdict leveled against A.T. Massey. On appeal to the Supreme Court, Justice Kennedy, writing for the majority, held that the appellant’s constitutional rights were violated because the Due Process Clause requires recusal when an objective inquiry reveals there is an “unconstitutional potential for bias.” The Court further held that there was:\n[A] serious risk of actual bias—based on objective and reasonable perceptions—when a person with a personal stake in a particular case had a significant and disproportionate influence in placing the judge on the case by raising funds or directing the judge’s election campaign when the case was pending or imminent. The inquiry centers on the contribution’s relative size in comparison to the total amount of money contributed to the campaign, the total amount spent in the election, and the apparent effect such contribution had on the outcome of the election.\nThe court did note, however, that “[n]ot every campaign contribution by a litigant or attorney creates a probability of bias that requires a judge’s recusal.”\nResponding to the Court’s decision in Caperton, many states made changes to judicial recusal rules to satisfy the guarantees of due process. The next section illustrates some common examples of these post-Caperton rules.\nII. STATE JUDICIAL RECUSAL STANDARDS IN RESPONSE TO CAPERTON\nIn light of Caperton, it is useful to review state judicial recusal standards to examine how states responded to the Supreme Court’s views of when due process demands a judge’s recusal. Because the Court did note that not every campaign contribution requires recusal, states have had some latitude in how they structure recusal standards. This freedom has led to recusal standards which run the gamut from strict to lax. As the following three examples illustrate, judicial recusal standards vary widely among the states.\nFor example, New York rules prohibit a judge from sitting on a case if an attorney in the matter, that attorney’s law firm, or a party in the matter has individually contributed more than $2,500 to that judge’s campaign within two years.Notwithstanding this provision, New York also encourages judges “to consider the propriety of recusal in any proceeding in which the judge’s impartiality reasonably might be questioned in consequence of campaign contributions.”\nAlabama has taken a slightly different approach with regards to judicial recusal. In 2014, the legislature passed, and the governor signed into law, updates to Alabama’s judicial recusal standards. Although not as stringent as New York’s absolute prohibition on hearing cases involving campaign contributors, Alabama’s recusal statute did institute a strong objective standard. That law requires recusal when a “substantial campaign contribution or electioneering communication” raised the specter that: “[a] reasonable person would perceive that the justice or judge’s ability to carry out his or her judicial responsibilities with impartiality is impaired” or “[t]here is a serious, objective probability of actual bias by the justice or judge due to his or her acceptance of the campaign contribution.” This law seemingly adopted Caperton’s objective risk of bias test. The law also created a rebuttable presumption of required recusal if a party’s contribution to a judge’s campaign exceeds a certain proportion of that judge’s total campaign contributions and it was reasonably foreseeable that the judge could hear a case involving the contributor. For example, a contribution in excess of ten percent of total campaign contributions in a statewide appellate race triggers the rebuttable presumption of recusal.\nOn the complete opposite end of the spectrum is Wisconsin which took a very different approach to the relationship between campaign contributions and independent expenditures, and a judge’s recusal. In 2010, on a contentious four-to-three vote, the Wisconsin Supreme Court amended the state judicial code to explicitly provide that, “[a] judge shall not be required to recuse himself or herself in a proceeding based solely on any endorsement or the judge’s campaign committee’s receipt of a lawful campaign contribution, including a campaign contribution from an individual or entity involved in the proceeding.” Although this provision is similar to language in Caperton, it what was what the Wisconsin Supreme Court did in regards to independent expenditures that seems to go against the prevailing wisdom for effective judicial recusal standards. The court also amended its judicial code to provide that:\nA judge shall not be required to recuse himself or herself in a proceeding where such recusal would be based solely on the sponsorship of an independent expenditure or issue advocacy communication (collectively, an “independent communication”) by an individual or entity involved in the proceeding or a donation to an organization that sponsors an independent communication by an individual or entity involved in the proceeding.\nThe justices who voted for the provision justified it on the grounds that requiring recusal would burden the right to vote for judges.\nIII. THE EFFECT OF POST-CITIZENS UNITED CAMPAIGN SPENDING ON JUDICIAL RECUSALS\nThe Caperton court noted that its ruling “addressed an extraordinary situation where the Constitution requires recusal” and that the extreme facts of the case (to the best of its knowledge) have not occurred in another judicial race.Troublingly, however, this scenario is susceptible to replication post-Citizens United. With that decision allowing for unlimited independent expenditures in elections, including judicial races, business is booming in races for state supreme courts. According to the Brennan Center for Justice, in the 2015-16 election cycle more than $69.3 million was spent on state supreme court races and spending by outside groups constituted forty percent ($27.8 million) of that total. In supreme court races in North Carolina, West Virginia, Louisiana, Wisconsin, and Michigan, spending by outside groups actually exceeded direct candidate spending. A 2016 race in Wisconsin even saw expenditures by an outside group hit $2.6 million—more than the amount of independent expenditures at issue in Caperton. As the amount of money flooding into judicial elections continues to rise post-Citizens United, it seems more likely that Caperton-like cases will become more, not less, common. Assuming this trend continues, it is imperative for states to strengthen their judicial recusal standards to protect the due process rights of litigants. For example, a simple and easily applied rule would be to set a contribution or expenditure threshold which, if crossed, would automatically mandate recusal. The benefit of such a rule is twofold. One, it would be straightforward and easy to understand. Two, by providing a strict, objective, and bright-line rule, it would address the Caperton dissents’ criticism that the Court failed to provide clear guidance in its decision.\nIf states decline to implement robust recusal standards that comport with Caperton’s requirements, the Court should not hesitate to invalidate state court decisions made in disregard of litigants’ due process rights. For better or worse, the Court’s decision in Citizens United barred limits on the amount of independent expenditures permitted in judicial races by holding that this spending is political speech. But the Court was clear that this decision did not invalidate Caperton’s call for recusal when a party in a matter spends excessively on behalf of a judge’s campaign. States would do well to ensure their judicial recusal rules comport with this demand, lest they run afoul of their duty to ensure due process is satisfied. Failure to do so weakens core constitutional guarantees and undermines confidence in the impartiality of judges.\n- Citizens United v. FEC, 558 U.S. 310, 365 (2010).\n- Id. (overruling Austin v. MI State Chamber of Commerce, 494 U.S. 652 (1990).\n- See e.g., Christopher C. Horner, In Support of Citizens United, Against Targeting Political Speech, The Hill (Feb. 12, 2015), https://thehill.com/blogs/congress-blog/politics/232535-in-support-of-citizens-united-against-targeting-political-speech; The ACLU and Citizens United, https://www.aclu.org/other/aclu-and-citizens-united (last visited Nov. 21, 2018).\n- See generally, Move To Amend, https://movetoamend.org/ (last visited Nov. 21, 2018) (expressing support for amending the Constitution to “firmly establish that money is not speech, and that human beings, not corporations, are persons entitled to constitutional rights”).\n- Cf. Democratic Party, 2016 Democratic Party Platform 2 (2016), https://democrats.org/wp-content/uploads/2018/10/2016_DNC_Platform.pdf (calling for the “overturn [of] the disastrous Citizens United decision”).\n- Blankenship decided to support a candidate challenging a sitting state supreme court justice because his company was appealing an adverse ruling that it had received in a state court case. If A.T. Massey was successful in its appeal before the state supreme court, it stood to avoid a $50 million verdict that a West Virginia jury had awarded against it. The majority of justices on the state supreme court—including Blankenship’s preferred candidate—overturned the verdict on appeal. Caperton v. A.T. Massey Coal Co., 556 U.S. 868, 873 (2009).\n- This is the highest appellate court in West Virginia.\n- Id. at 873–74.\n- Id. at 874.\n- Id. at 881 (internal quotation marks omitted).\n- Caperton, 556 U.S. at 884.\n- N.Y. Comp. Codes R. & Regs. tit. 22, § 151.1 (2018).\n- N.Y. Comp. Codes R. & Regs. tit. 22, § 151.1(D) (2018).\n- H.R. 543, 2014 Leg., Reg. Sess. (Ala. 2014).\n- Caperton v. A.T. Massey Coal Co., 556 U.S. 868, 884 (2009).\n- Wisconsin Supreme Court Rule 60.04(7).\n- Caperton v. A.T. Massey Coal Co., 556 U.S. 868, 884 (2009)\n- Wisconsin Supreme Court Rule 60.04(8).\n- See In the Matter of Amendment of the Code of Judicial Conduct’s Rules on Recusal, 2010 WI 73, ¶ 11 (“We elect judges in Wisconsin; therefore, judicial recusal rules have the potential to impact the effectiveness of citizens’ votes cast for judges. Stated otherwise, when a judge is disqualified from participation, the votes of all who voted to elect that judge are canceled for all issues presented by that case. Accordingly, recusal rules. . .must be narrowly tailored to meet a compelling state interest.”). But see Patrick Marley, Wisconsin Supreme Court Rejects Recusal Changes When Campaign Donors are Litigants, Milwaukee J. Sentinel (Apr. 20, 2017), https://www.jsonline.com/story/news/politics/2017/04/20/wisconsin-supreme-court-weighs-recusal-rules-when-campaign-donors-litigants/100644698/ (quoting Justice Shirley Abrahamson regarding tighter judicial recusal rules) (“I see nothing in the Wisconsin Constitution or the U.S. Constitution that says that this court can’t make reasonable recusal rules.”).\n- Caperton, 556 U.S. at 887.\n- See Citizens United v. FEC, 558 U.S. 310, 360 (2010) (“Caperton’s holding was limited to the rule that the judge must be recused, not that the litigant’s political speech could be banned.”)\n- Alicia Bannon et al., Brennan Ctr. for Justice, Who Pays for Judicial Races? 5 (2017), https://www.brennancenter.org/sites/default/files/publications/Politics_of_Judicial_Elections_Final.pdf.\n- Wisconsin Democracy Campaign, Wisconsin Supreme Court Finance Summaries, https://www.wisdc.org/follow-the-money/nonpartisan-candidates/supreme-court-finance-summaries (last visited November 20, 2018).\n- See supra Section II (describing New York’s judicial recusal rule).\n- See Caperton v. A.T. Massey Coal Co., 556 U.S. 868, 893 (2009) (Roberts, C.J., dissenting) (arguing that the Court “fail[ed] to provide clear, workable guidance for future cases”); Id. at 902–03 (Scalia, J., dissenting) (arguing the Court’s decision would result in uncertainty).\n- Citizens United, 558 U.S. at 360.\n- See id.']	['<urn:uuid:74004737-d093-4700-9150-e27d24e73eb8>']	factoid	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T19:47:23.148022	9	29	2078
21	What are the different ways shoulder surgery can help fix injuries, and what kinds of daily activities become hard when you have shoulder problems?	Minimally invasive shoulder surgery (shoulder arthroscopy) can help fix various injuries including rotator cuff tears, biceps tendon injuries, bone spurs removal, shoulder instability repairs, and labral repairs. For daily activities, shoulder injuries can make washing hair and getting dressed painful and difficult. Specific movements like reaching backward to fasten a seat belt, picking up items from the back seat, or lifting arms overhead to get plates from cupboards become particularly problematic.	['In the course of recent decades, the medicinal field has seen an ascent in negligibly intrusive medical procedure systems overall claims to fame. New advances in insignificantly obtrusive strategies mean various things for patients, including quicker recuperation times and fewer complexities. In the orthopaedic field, this has made an interpretation of into better approaches to supplant joints, repair sports wounds and treat complex musculoskeletal issues.\nShoulder wounds specifically are a common issue that may lead you to an orthopaedic doctor. Here’s an outline of what insignificantly intrusive shoulder medical procedure includes and what wounds and conditions it might be utilized to treat.\nKeeping in mind the end goal to see how insignificantly intrusive shoulder medical procedure works, it’s essential to have an unmistakable comprehension of what’s required with a negligibly obtrusive medical procedure. Though conventional medical procedure frequently requires a substantial entry point and critical injury to delicate tissue encompassing the treatment area, negligibly intrusive orthopaedic medical procedure endeavours to limit both the extent of careful cuts and in addition the injury to encompassing delicate tissue, while achieving the same careful objective. In orthopaedics, this can be the contrast between uncovering a complete joint through a vast, open entry point and utilizing little, directed cuts to find the problem area. This outcome in less hazard amid medical procedure, less post-agent torment and shorter recuperation times for patients.\nHow does minimally invasive shoulder surgery work?\nNegligibly intrusive shoulder surgery, likewise alluded to as shoulder arthroscopy, includes making at least two little entry points in the shoulder and embeddings a camera mounted on a thin, tube (called an arthroscope) keeping in mind the end goal to picture the inside of the joint. Little careful instruments are then embedded through alternate entry points, and the whole activity is communicated to a TV screen. The specialist utilizes this augmented picture to help direct the instruments and make whatever repairs are important. This sort of methodology can be utilized to calm difficult indications caused by harm to the rotator sleeve ligaments, labrum, articular ligament, and other delicate tissues encompassing the shoulder joint.\nAm I a possibility for insignificantly obtrusive shoulder surgery?\nFor some orthopaedic shoulder wounds and conditions, nonoperative administration and conventional medical procedure are as yet the favoured techniques for treatment. Be that as it may, when surgery is essential, you ought to counsel with an accomplished orthopaedic specialist who can clarify your choices and decide if a negligibly intrusive method would be best for your necessities.\nShoulder issues that might be treated with bear arthroscopy incorporate the accompanying.\nRotator Cuff Injury\nAn arthroscopic rotator cuff restore can be wanted when you have suffered from a rotator cuff tear, both as a result of a disturbing harm (acute tear) or due to wear and tear over the years (degenerative tear). it can also be encouraged for a rotator cuff harm that has no longer advanced after numerous months of nonsurgical remedy.\nBiceps Tendon Injury\nThe biceps ligament, in reality, begins in the shoulder joint. at the off threat that it is torn or harmed then it very well may be an important wellspring of torment or inconvenience. Negligibly intrusive shoulder clinical procedure can either repair a torn biceps ligament or virtually pass it out of the shoulder joint so it isn’t always any more a wellspring of anguish.\nOn the factor whilst bone goads create in the shoulder because of ligament degeneration and bone-on-bone joint harm, a method referred to as a bone goad expulsion might be cautioned. this technique may be performed arthroscopically to expel the difficult goads and help calm side outcomes. Without a remedy, bone goads may also set off rotator sleeve tears and tendinitis within the shoulder.\nA shoulder that rather or definitely disjoins because of shoulder insecurity or lousy harm can also require a shoulder precariousness repair. carried out arthroscopically, this kind of technique can be applied to reattach and fix unfastened tendons in the shoulder to assure valid assist and help count on destiny separations.\nAn arthroscopic labral restore or expulsion might be required on the off risk that you have skilled a labral tear. Contingent upon whether or not the shoulder is steady or insecure due to the damage, this restore might also include simply evacuating the torn fold/segment or repairing and reattaching the ligament in conditions where the ligament is altogether withdrawn or at the off hazard that it stretches out into the biceps ligament.\nAt the off chance that you are experiencing terrible shoulder harm or degenerative shoulder condition, the preliminary step to discovering comfort starts off evolved with a council with an orthopaedic authority. Within the event that mild treatment picks neglect to ease your manifestations and medical method is absolutely vital, it merits getting a few records about whether undergo arthroscopy is appropriate for you.', 'Rotator Cuff Conditions\nRotator cuff disorders are one of the most common causes of shoulder pain. Injuries to the rotator cuff tendons are frequently seen in workers with physically demanding jobs, athletes, and physically active people. Some common examples include painters, welders, carpenters, swimmers and tennis players. It can be due to a one time traumatic injury or from repetitive use of the shoulder. Degenerative rotator cuff tears associated are common, are a major cause of shoulder pain, and can be aggravated by work or sports. Rotator cuff tears generally do not heal without treatment, and results in pain and disability. Many daily activities, like washing your hair or getting dressed, may become painful and difficult.\nPatients with a rotator cuff injury often experience pain when lifting or lowering their arm, muscle weakness and atrophy. These symptoms may appear suddenly after the tendon is torn, or develop gradually after long-term wear. The pain associated with a rotator cuff injury is often described as a deep shoulder pain or ache, disturb sleep, limit activities of daily living and cause weakness. Some common rotator cuff symptoms include pain, weakness, restricted motion, catching, locking and a feeling of instability. The symptoms are usually worse in certain positions, such as reaching backward to fasten a seat belt or pick up a briefcase out of the back seat. Symptoms can also be worse when the arm is elevated overhead, especially if there is weight on the arm, such as when picking up a stack of plates out of a cupboard. Overhead activities like pitching, throwing, playing tennis or playing racquet-ball commonly worsen symptoms.\nA patient’s History and Physical examination is the best way to initially evaluate any shoulder condition. It is important to make sure that the pain is actually coming from the shoulder and not from the neck or elbow. After discussing your symptoms and medical history, Dr. Li will examine your shoulder. He will check to see whether it is tender in any area or whether there is a deformity. To measure the range of motion of your shoulder, he will have you move your arm in several different directions. Your shoulder strength will also be evaluated.\nYour doctor will check for other problems with your shoulder joint. He or she may also examine your neck to make sure that the pain is not coming from a “pinched nerve,” and to rule out other conditions, such as arthritis.\nX-rays: Although plain x-rays do not show the rotator cuff tendons, they are helpful to look for calcifications, arthritis or bone spurs that can cause rotator cuff tears.\nMSK Ultrasound: An ultrasound scan can be helpful for diagnosing rotator cuff problems. It allows the soft tissues of the shoulder to be seen, and can detect fluid and damage to tendons and muscles.\nMRI: This is the most common imaging method to diagnose rotator cuff tears. It can be used to look for tears or inflammation of tissues and to help determine the size and character of the tear so proper treatment can be recommended.\nDepending on several factors including patient activity, quality of the remaining rotator cuff tendon and size of the rotator cuff tear; the problem may respond well to therapy, injections and medication or it may require arthroscopic surgery.\nTherapy– The purpose of Physical Therapy or Occupation Therapy is to improve symptoms and restore function. The approach will depend on whether you have a short-term (acute) problem or a long-standing (chronic) condition. The goal of the therapy program is to “functionally heal” the shoulder and could include:\n- exercises to strengthen weakened muscles, improve their co-ordination and restore function\n- advice on improving shoulder, neck and spine posture\n- exercises to ease or prevent stiffness\n- exercises to increase the range of joint movement\n- applying adhesive tape to the skin to reduce the strain on the tissues and to help increase your awareness of the position of the shoulder and shoulder blade\n- manual treatments to the soft tissues and joints.\nSteroid Injections– A steroid injection given along with a local anesthetic can reduce pain very quickly. The injection reduce inflammation and allow you to move your shoulder more comfortably, though you shouldn’t use your shoulder for anything too strenuous the first two days after an injection. The pain relief should allow you to do your therapy exercises more easily. Repeated injections (more than two or three) aren’t usually recommended. If the problem keeps coming back, then other treatments should be considered.\nMedications– NSAIDs (Non-steroidal anti-inflammatory drugs) are frequently used at the onset of rotator cuff injuries to reduce inflammation and control pain. They are available by prescription as well as over-the-counter. Some common over-the-counter NSAIDs include Ibuprofen (Motrin) and Naproxen (Aleve).\nArthroscopic Rotator Cuff Surgery– can be performed to repair torn tendons following a rotator cuff injury. Because it is minimally invasive, Arthroscopic Rotator Cuff Repair results in a much quicker and less painful recovery. Arthroscopic Rotator Cuff Repair has been performed successfully in an outpatient setting for many years to help people of all ages regain maximum use of their shoulder and resume normal routines and physical activities. Arthroscopic repair is superior to the traditional open repair and is successful even with very large tears, so there is no reason to settle for an open repair. Rotator Cuff surgery is performed on an outpatient basis which means going home the same day. A shoulder sling/orthosis will be used to protect your repair when you are in a public place, and may be used to protect your shoulder during sleep.\nPost-Operative Physical/Occupational Therapy– is critical to a fast and strong recovery, so it is typically started just a few days after surgery. The typical length of post-operative therapy is 3 months but can vary depending on the size of the tear, condition of the rotator cuff, patient’s job requirements, and other factors. Initially the therapy will focus on restoring your passive range of motion, and after 4-6 weeks then active range of motion is started. After active range of motion is restored then the focus shifts to strengthening the rotator cuff muscles. It is very important to continue a home exercise program for 12-18 months because it takes that long to regain your strength.']	['<urn:uuid:f90197c6-ea1a-4cb4-ace1-bb58a20d59df>', '<urn:uuid:7b426d4c-0a19-489f-8844-6802482cf02d>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	24	71	1845
22	training frequency stamina building vs muscle building	For building stamina, training involves progressively increasing duration by 10% per week, with at least 1-2 longer rides weekly in Zone 2 and Zone 3 efforts. For muscle building, training each muscle group 2-3 times per week is optimal, with multiple training split options available: 2-3 times weekly for full body workouts, 4 times weekly for upper/lower splits, or 6 times weekly for push/pull/legs routines. Research shows that higher training volumes (10+ sets per week per body part) result in greater muscle growth (9.8% increase) compared to lower volumes.	"[""Stamina in endurance sports is the ability (both physically and mentally) to keep performing for a long period of time. All endurance sports require stamina, although some—such as Ironman distance triathlons, marathon running, and long endurance events—require more stamina than others. Developing peak physical condition results in endurance athletes having a high degree of stamina because their hearts, lungs, and muscles all function at a high level of efficiency.\nWKO4 Stamina Defined\nFor use in WKO4, the metric is defined as follows:\nStamina: a measure of resistance to fatigue during prolonged-duration, moderate-intensity (i.e., sub-FTP) exercise. Units are percent of maximum, i.e., 0-100%, although most individuals will fall in the 75 to 85 percent range.\nStamina represents time periods beyond 1 hour of performance, scoring your resistance to fatigue over such longer efforts. On the power duration curve, this is typically the “flat tail” of the extended curve beyond about one hour.\nThe Basis and Purpose of Stamina in WKO4\nStamina is a power-duration-derived metric that takes into account all points of an athlete’s power data to quantify resistance to fatigue during prolonged exercise. As a simple example, a stamina score of 100 percent would represent complete fatigue resistance, and the power duration curve beyond one hour(ish) would be completely flat, whereas a 50 percent stamina score would show significant decline of the tail. If fatigue is what causes you to slow down, then being more resistance to fatigue means that you won’t have to slow down as soon or as much.\nThe purpose of the metric is to track and analyze an athlete’s endurance fitness over time in an effort to understand:\n- Response to training stimuli in increasing performance in prolonged duration exercise\n- Effectiveness of training regime on increasing stamina\n- Athlete’s readiness to perform\nPhysiological Factors Behind Stamina\nWhile there are many factors that impact our stamina, the most important is our muscular metabolic fitness. Endurance training induces major adaptations in skeletal muscle. These include increases in the mitochondrial content and respiratory capacity of the muscle fibers. As a consequence of the increase in mitochondria, exercise of the same intensity produces less metabolic strain than in untrained muscles. Additionally, endurance training has consistently shown a fast-to-slow conversion in response to such training. That is, highly-fatigable fast-twitch type IIX fibers are converted into the more fatigue-resistant type IIA fibers, and some type IIA fibers may even be converted into the slower contracting, less powerful, but highly fatigue-resistant type slow twitch type I fibers. The major metabolic results of these adaptations are a slower utilization of muscle glycogen and blood glucose, a greater reliance on fat oxidation, and less lactate production during exercise of a given intensity. These adaptations therefore play an important role in the large increase of the ability to perform prolonged strenuous exercise that occurs in response to endurance exercise training.\nThe adaptations described above result in an increase in functional threshold power (FTP), which is the highest exercise intensity at which aerobic ATP production balances ATP demand, such that a steady-state (or quasi-steady-state) of physiological responses (e.g., blood lactate levels) can be maintained. If cardiovascular fitness, i.e., VO2Max, sets the upper limit for aerobic exercise, it is one’s muscular metabolic fitness, as reflected by FTP, that determines how much of that aerobic upper limit can be used. Thus, FTP and stamina are closely related, with the former describing the level at which the power-duration relationship tends to plateau and the latter describing the rate of decline once past that point in time. Even with the same FTP, however, there can be subtle differences between individuals in stamina, depending in part on genetically-influenced fiber type distribution, training specificity, habitual diet, etc.\nBuilding Stamina in the Endurance Athlete\nStamina is a metric that assigns a score to a specific adaptation to training stimuli (or lack thereof). As stamina increases, one could expect an improvement in longer, steady-state endurance events such as triathlons, endurance cycling, and even marathons. Understanding the basis of stamina as described in this article gives us insight into the types of training that can best increase it.\nPROGRESS TRAINING DURATION\nWhatever your present endurance conditioning, build it slowly but steadily to begin to increase your stamina. This increase in duration needs to occur at two levels: first, in total training duration, and second, in the length and pace of your longer rides.\nOverall duration increase is the simplest way to increase your base fitness and increase stamina (assuming it occurs in the appropriate training zones). The strategy of increasing total training duration at a steady 10 percent per week has been around for a long time for a reason: it is a smart way to build stamina. It’s important to note that just building time should not be the focus; building time at the appropriate effort is imperative to success. As duration increases the athlete should ensure the progression of time in Zone 2, Zone 3, and Sweet Spot to maximize aerobic conditioning and base.\nExtending individual ride duration is a highly effective way to not only increase overall duration but also specifically develop stamina by mimicking the stress of longer endurance events. This means extending at least one ride (two is better) a week to a longer duration based on your target events. To further improve stamina results, these rides should be completed in Zone 2 and Zone 3 efforts.\nFOCUS ON BUILDING FUNCTIONING THRESHOLD POWER\nPhysiologically, FTP is most related to stamina. The introduction of at least 3 to 5 workouts per training macrocycle (2 to 5 weeks) focused specifically on raising threshold performance will pay dividends in the development of stamina. These workouts should focus on training intensities in the Coggan iLevels’ Zone 4a, also known as Sweet Spot Training. This allows an athlete to push threshold up from below, increasing such threshold while still focusing on aerobic efforts.\nBUILD AEROBIC CAPACITY\nAs noted, increasing aerobic capacity is measured by VO2Max. Some training focus should be placed on workouts that maintain and build this capacity in the endurance athlete. With the new iLevels, athletes can maximize results by completing specific intervals in the Coggan iLevel Zones 5 and 6. These intervals should be completed when fresh so the athlete can get maximal effort and time in the training zones. Add 2 to 4 of these style of workouts per training cycling to best maximize results.\nStamina is an effective metric for tracking the increase (or decrease) in fatigue resistance to prolonged efforts. This puts another tool in the coaches' and athletes' hands to better track the effectiveness of training strategies and predict performance results.\nThis article was written by Tim Cusick with contributions from Dr. Andrew Coggan. You can read the original blog post here."", 'Cut through all the conflicting BS and find out which numbers you should be paying attention to for the greatest gains.\nWhich rep ranges to build muscle?\n2-6 – reps greatest for strength gains\n20-35 reps greatest for endurance gains\n2-35+ reps greatest for muscle growth\nWhy? When volume load (eg. 3 sets of 10 or 10 sets of 3) is accounted for, studies almost always show that hypertrophy is similar with heavy or light loads.\nWhich loads build muscle?\nof 1 rep max are the loads you can use to induce muscle growth.\n70-85% – SWEET SPOT\nof 1 rep max. Working with these loads in your sets is the sweet spot for your hypertrophy, strength and time efficiency. Doing 10 sets of 3 at 90%+ of your 1RM for each exercise would take an age in the gym.\nThe optimal frequency to train each muscle group per week. The 1 day a week “bro-split” was found to be sub-optimal\nHow many sets per week, per body part to build the most muscle?\n< 5 sets gained 5.4% more muscle\n5-9 sets gained 6.6% more muscle\n10 sets+ gained 9.8% more muscle\nThis data shows the dose-response between more volume (to a point) and muscle growth.\nSource : Journal of Sports Sciences\nWhat training could I do with the time I have based on the science…?\n2-3 x per week : Full Body Workout\n4 x per week : Upper Body/Lower Body\n5 x per week : Power Hypertrophy – Upper Power/Lower Power/Back+Shoulders hypertrophy/Lower body hypertrophy/Chest+Arms hypertrophy\n6 x per week : Push/Pull/Legs/Push/Pull/Legs\nThe amount of strength and resistance training exercises that exist.\n49 = muscles worked simultaneously by the deadlift. The highest of any single exercise (that isn’t a complex movement)\nsource : Strength Training Anatomy\nHow much muscle can you naturally gain? Well, it depends how long you’ve been at it…\nUp to 1 year : 20-25 pounds (2 pounds per month)\n2 years : 10-12 pounds (1 pound per month)\n3 years : 5-6 pounds (.5 pound per month)\n4 years : 2-3 pounds (not worth calculating)\n3 minutes vs 1 minute rest periods\nLeaving the gym dripping in sweat because you’re jumping from one exercise to the next without resting up in between might not be your best bet..\n3 – minute rest periods were shown to increase both strength and muscle thickness in triceps, biceps and legs.\n1 – minute rest periods showed less gains in both strength and mass. This is hypothesized to be due to less volume being performed due to muscle fatigue\nThis doesn’t mean rest for long periods exclusively as compound exercises for larger muscle groups will inevitably require more rest time than isolation exercises for small muscle groups.\n30-90 seconds for small muscle, isolation exercises.\n2-5 minutes for larger muscle groups and compound movements.\nSource: Journal of Strength and Conditioning Research\nIdeal rep speed for maximum gains\n1 second – concentric portion of a lift\n1-3 seconds – eccentric portion of a lift\n28% – more muscle mass when following the guidelines above vs a superslow lift\n36% – less muscle activation when any portion of the lift takes longer than 5 seconds\n3 seconds – the longest the eccentric portion should take.\nSource : Journal of strength and conditioning research\nFiber types of lifters\nThe fiber type make-up of trainees for three disciplines\nType 1 slow twitch vs Type 2 fast twitch\n36% : 64%\n38% : 62%\n57% : 43%\nType 2 fast twitch – more potential for explosive power\nType 1 slow twitch – more potential for endurance\nBoth – potential for muscle growth (More growth potential in type 2)\nSource: Sports Medicine Journal\nPowerlifter vs Bodybuilder\n1993 – An elite bodybuilder, Tom Platz and an elite powerlifter, Fred Hatfield went head to head in various tests\n855 – pounds was what Fred put up on a squat out-lifting Platz’ 765lbs\n23 – reps that Mike Platz performed when they went down to 525lbs, blazing Fred’s 11 reps.\nThis lends credence to bodybuilding have more type 1 fibers for endurance and powerlifters having a higher ratio of type 2 for explosive strength.\nDose response relationship volume and hypertrophy\nEffects of frequency for muscle growth\nRole of training on muscle fibre types']"	['<urn:uuid:6350af33-26b8-4c05-ab94-d32955891f1b>', '<urn:uuid:3ccb0958-2cb9-470f-8f7f-a7b9915a54dd>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T19:47:23.148022	7	89	1839
23	digital accessibility standards automatic manual testing difference	According to the documents, automated accessibility testing scans can only detect around 30% of basic errors such as missing ALT texts and color contrast issues. For WCAG compliance, a more comprehensive approach is needed that includes both automated tools and manual remediation methods. The Monsido system provides detailed reports and targeted recommendations based on the guidelines, while manual audits are recommended for complete Section 508 compliance through VPAT.	['The Web Content Accessibility Guidelines (WCAG)\nWhat are the Web Content Accessibility Guidelines (WCAG)?\nThe guidelines offer technical recommendations on how to make website content accessible. The guidelines are also the standard reference for most website accessibility-related legislation like the Americans with Disability Act (ADA) in the US, and the European Web Accessibility Directive.\nWCAG Versions - 2.0, 2.1, 2.2 and 3.0\n- WCAG 2.0 - published 11 December 2008.\n- WCAG 2.1 - published on 5 June 2018 and is now the W3C recommended version\n- WCAG 2.2 - not yet in effect, scheduled to be published in 2021\n- WCAG 2.0 had 61 success criteria\n- WCAG 2.1 introduced 17 more success criteria to address mobile accessibility, people with low vision, and people with cognitive and learning disabilities.\n- WCAG 2.2 will be expanding on 2.1 with nine new success criteria, plus an update to one, with the goal of making content more accessible to a wider range of users.\nThe current standing WCAG versions 2.0 and 2.1 are categorized according to four principles, perceivable, operable, understandable, and robust (POUR).\nElements that convey information or components of a website’s user interface must be presented in a way that users are able to find, process, and understand.\nAll functionality and navigation on the website should be usable.\nInformation and the operation of the user interface must be clear and understandable to users of all abilities.\nThe website should be capable of adapting and developing itself to support a variety of current and potential future user agents, including assistive technologies.\nUnder each principle are testable success criteria that provide recommendations on how to make digital content more accessible. The success criteria are classified by three levels A, AA, and AAA, with A being the most basic level of WCAG compliance and AAA being the hardest.\nLearn more about the elements of WCAG and how to comply with its success criteria.\nOn the 21st of January, 2021, the WAI released the first working draft of the WCAG 3.0. WCAG 3.0 is planned to be a major revision with the intention to make the guidelines more user-friendly than the WCAG 2 iterations, and more flexible, covering even more content, apps, and tools, as well as organizations and disabilities. WCAG 3.0 is still in development and is not expected to be finalized for the next few years. Learn more about WCAG 3.0.\nThe different WCAG compliance levels\nThe WCAG categorizes its conformance based on three levels: A, AA, and AAA. To conform to the guidelines, it is a requirement that one of these levels should be fully met.\nMinimal WCAG compliance (level A)\nExamples of Level A success criteria:\n- All non-text content like images or videos must have a text alternative, like alt text or captions, that serves the equivalent purpose.\n- Users can navigate the website effectively using only a keyboard\n- Avoid using color as the only visual means of conveying information or prompting an action, like having green buttons with no text on them to suggest that it is meant to be selected as a ‘yes’ response.\n- If there is audio that auto-plays on your website for more than 3 seconds, ensure that that you provide means of adjusting the volume, stopping, or pausing it.\nAcceptable WCAG compliance (level AA)\nExamples of Level AA success criteria:\n- Ensuring that text on a webpage can be resized without assistive technology up to 200 % without loss of content or functionality.\n- Provide descriptive headings and labels in content\n- Navigational elements on the site, like menus, should be in a consistent, repeated position across the website.\n- When executing an action on the site, like filling in forms or clicking on buttons, errors can occur on the user’s part. If an error should occur, suggestions for correction should be provided.\nOptimal WCAG compliance (level AAA)\nExamples of Level AAA success criteria:\n- The visual presentation of text and images of text must have a contrast ratio of at least 7:1.\n- Removing timing limitations from all content, unless it is for non-interactive synchronized media and real-time events.\n- When a user has to submit information on a webpage, the submissions must be reversible, checked for input errors (and offer suggestions for correction if errors do occur), and there is a confirmation mechanism in place to allow the user to review the submission and edit if needed.\n- Images of text should be avoided or only used for decoration.\nWho should comply with the WCAG?\n- Web content developers (page authors, site designers, etc.)\n- Web authoring tool developers\n- Web accessibility evaluation tool developers\nHow to check your website’s WCAG compliance level\nHow Monsido can help your website meet WCAG standards\nEach audit scans your site for machine-testable issues, provides detailed reports so you can review any errors that may arise, gives you targeted recommendations on how to address these errors based on the guidelines, and shows you your compliance based on levels A, AA, and AAA. You can track and prove your accessibility compliance progress via reports in the History Center. We also offer accessibility training to customers and support, all-inclusive, to ensure that you are well-versed in both automated and manual remediation methods, and can efficiently and consistently improve your website’s accessibility.\nMonsido also offers free tools to complement your web accessibility efforts, including a color contrast checker for web teams to test out compliant color combinations for their web design, and an accessibility statement generator, which helps you generate a public statement declaring your commitment to web accessibility and helps make your web accessibility policy transparent to all your users.', 'What Is 508 Compliance And How To Make A Website Accessible\nWith the rising number of digital accessibility lawsuits, Section 508 compliance has become a buzzword.\nAlmost every vendor or manufacturer of a technology product is concerned about the VPAT scoring. VPAT (Voluntary Product Accessibility Template) is a self-evaluation tool to declare your compliance with Section 508. The law basically applies to Federal agencies that develop or procure technology products.\nWhen a technology business organization deals with a federal agency or any entity that receives funding from the federal government, Section 508 compliance is mandatory for them. The vendors must abide by the law and declare their accessibility standards via VPAT. In today’s post, we cover what is VPAT and how to use it to protect your business from lawsuits.\nWhat’s Covered In Section 508?\nSection 508 is part of the Rehabilitation Act which is also connected to certain other digital accessibility laws. The Americans with Disability (ADA) Act is one of the most important laws that has become the #1 reason for digital accessibility lawsuits and expensive penalties.\nSection 508 standards are applicableto information technology and communication products, such as:\nElectronic content like PDFs\nTo put it simply, if a remote learning app has electronic documents, then the developers need to make sure that the products are easily accessible by users who have hearing difficulties, vision loss or cognitive limitations.\nThe updated version for Section 508 needs accessibility compliance for color, images, tables, lists, headings, forms and PDFs.\nMake Your Website Accessible With A VPAT\nVPAT was originally developed only for technology agencies that are dealing with the federal government. However, compliance with Section 508 through regularly auditing your VPAT accessibility standards is a step in the direction of avoiding expensive legal recourse.\nWhat Is VPAT Compliance?\nThere are four different editions of VPAT including WCAG edition, 508 edition, EU edition and INT edition. Depending on your requirements, you can select the edition and test your products to declare your VPAT compliance.\nThe VPAT template can be filled out after a thorough accessibility audit of the products/offerings. Below is a step by step approach to make your website compliant with Section 508.\nCaptions and transcripts: Make sure that the audio and videos are accessible via written text or transcripts. Speech recognition technologies are useful to make your website 508 compliant.\nNavigation: Make sure that website navigation is hassle free if a user doesn’t have access to a mouse and uses only the keyboard.\nScreen readers: Access that the available content is easily readable via a screen reading platform. To attain Section 508 compliance, it is important to make things accessible for visually impaired users.\nALT text: Visually impaired users use a screen reader, they must know when the screen is displaying an image. Alt texts are alternative texts that include descriptions that the users are now looking at an image or a video on the screen.\nColor and contrast: Foreground and background color contrast is also a useful element to make a website compliant with Section 508. When filling out a VPAT, make sure to test the colour and contrast. This detail is helpful for users to distinguish easily accessible products and offerings.\nTimeouts: Timed responses are challenging for users with cognitive limitations. Avoid using them if you aim for Section 508 compliance.\nA VPAT testing tool is useful to access your website. Although automated scans are only 30% accurate and can only detect basic errors like missing ALT texts and color contrast. It is best to invest in manual audits to make a website totally compliant.\nRegularly updating your VPAT is essential to reduce the risk of being on the ever increasing list of law violators. Click on the link given below to learn what is VPAT accessibility procedure and how long it takes.']	['<urn:uuid:a2ef5797-959c-4ba6-9b52-ee60633ac0ca>', '<urn:uuid:dd6af6a2-f4d0-40ba-a1e2-6d97eac08e6d>']	factoid	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T19:47:23.148022	7	68	1580
24	looking for examples of places where I can see winding rivers that move from side to side what parks offer this	Buffalo National River is a broad meandering stream that covers a large plain. You can also observe meandering streams at Canyonlands National Park, although there the streams have cut so deeply into the rock that they no longer change course as easily as they did in the past.	['CHANGING CHANNELS (LANDFORMS) IN MIDSTREAM\nRivers are not all created equal. River channels form by flowing water and differ in their shape. The channel shape depends on a number of factors, which we will explore in this activity.\nInstructional Method: Experiment\nGoal: Students will perform experiments to see how each channel shape is formed.\nObjectives: Students will be able to:\n- Describe three river channel shapes\n- List what forms each channel shape\n- Identify the channels in nature or photos.\nPreparation: 30 min.\nDemonstration: 45 min.\nDiscussion: 15 min.\n- Broad, flat container\n- Bucket or jar\n- Blocks or bricks\nIn nature, many landforms are created and destroyed by stream flow processes. Streams create landforms, but water in a stream is not a landform. The sediment or rock forming a stream channel is the landform. This activity discusses stream channels as landforms.\nNot all stream channels look alike. Different factors affect stream shape:\nChannel bedrock (i.e. solid rock vs. sediment): Harder rock does not erode as easily as soft rock or loose sediment. Water velocity (water speed): As water velocity increases, so does the rate of erosion. Faster water moves abundant sediment and larger sized particles. When fast-moving water slows down, large particles fall out of flow and deposited on the channel bottom. When stream gradient increases, so does water velocity and so does the streams erosiveness. Stream gradient (the amount of downhill angle the river channel has): Steep streams are straight and narrow. Flatter streams wriggle more and are broader. Water flow consistency: If flow is constant, streams continuously incise the rock. If flow is intermittent, downcutting and flow deposition can be the same.\nThree river channels types are:\n- Meandering (wriggling)\nStraight streams have river channels that are relatively straight. Often they form in channels with a steep gradient. Water velocity in these rivers is fast. Most erosive action takes place along the river channel bottom rather than along the sides of the channel. This process is known as downcutting, or incision. An example of a river where downcutting dominates the erosional direction is found at Black Canyon of the Gunnison National Park.\nMeandering streams are characterized by wandering, or wriggling, rivers that cross a valley bottom and form a river plain. The channel can be wide or narrow with valleys that are usually broad and relatively flat. Side to side erosion dominates meandering river systems. That is why these rivers change their course so often. Buffalo National River is a broad meandering stream that covers a large plain. Some meandering streams have cut so deeply into the rock that they no longer change course as easily as they did in the past. This is the case at Canyonlands National Park.\nBraided streams are characterized by a broad channel with multiple subchannels and streams. The small streams weave back and forth, like the meandering streams, connecting randomly as they flow. When viewed from above it appears as if the smaller channels have been braided forming the stream system. Braided patterns tend to form in streams that are carrying and depositing an excessive amount of sediment. As sediment is carried and deposited in other portions of the smaller channel, it changes to form another small channel where it can cut and deposit again. Areas that flow occasionally due to storms, runoff or melt are common places to find braided streams (desert and glacial regions). These channel systems are constantly changing. Braided stream can be seen and Death Valley National Park and at Denali National Park.\nThe following activity allows students to experiment with different amounts of water flow and slope gradient to make the three discussed river channels.\n- Use a large flat container that can be angled and keep its shape.\n- Pour sand in container and spread it out leaving a space void of sand at one end (to catch the water and sediment).\n- Raise the sandy end of container 2 inches with blocks.\n- Pour water on raised end and observe. Keep pouring water until channel pattern is obvious. You may want to experiment with wet vs. dry sand. Repeat a few times and then change the angle by placing a few more blocks beneath it. Draw sketches of the different results.\n- Now smooth out the sand and our water with a lot of sand mixed with it. This will form the braided stream types.\n- Repeat steps 1 - 4 a few times. Remember to use sandy water this time.\n- It may be helpful to smooth out sand between each attempt.\n- Have students note on paper what occurs at each elevation and with the different rates of water flow.\nWhat stream patterns were formed each time? Did the rate at which the water was poured affect the channel? Did the angle change the river channel? Did the compactness of sand affect the river channel?\nTry pouring water at different rates and observing the resulting landforms. Vary the compactness of sand and observe!\nIncluded National Parks and other sites:\nBuffalo National River\nBlack Canyon of the Gunnison National Park\nCanyonlands National Park\nCapitol Reef National Park\nDeath Valley National Park\nDenali National Park\nGrand Canyon National Park\nZion National Park\nUtah Science Core:\n4th Grade Standard 5 Objective 1,2\n5th Grade Standard 2 Objective 1,2,3']	['<urn:uuid:49ae4267-9e57-44da-b139-fc5c2b732ad5>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T19:47:23.148022	21	48	885
25	When adjusting photo brightness, what does 'non-destructive' mean?	In Photoshop, non-destructive editing means you are adding an adjustment without changing the actual pixels of the image. This is achieved through adjustment layers rather than direct modifications to the original image.	['Photographers use vignettes to put a subtle frame around (and draw attention to) the subject of the picture. They draw our eyes to one part of an image and away from the remainder of the image. For me, creating an effective vignette is a balance between getting the effect of the vignette, but still not noticeable to the untrained eye.\nSo, what is a vignette? For the definitive answer, let’s look at the ultimate source of all things true… Wikipedia. According to Wikipedia, a vignette in photography is “a reduction of an image’s brightness or saturation at the periphery compared to the image center.” Couldn’t have said it better myself. Although you can also vignette with a blur. But they got it mostly right. Anyway, in most cases, vignettes are based on a reduction in brightness, which is what we’ll be discussing here.\nBelow is a picture of a cheetah with no vignette. You will notice the image looks fairly evenly lit throughout.\nBelow I have added a subtle vignette to the image. Notice the cheetah’s face is a little brighter as compared to the rest of the image. It is somewhat difficult to see, but it does help focus your attention on the cheetah. And cheetah’s command attention.\nJust in case you can’t see the vignette, below is a version in which I apply too much vignette so the effect is clear.\nAdding a vignette is usually the last thing I do with an image. Although Lightroom has the option of adding a post-crop vignette, I do my vignettes in Photoshop because I do my output sharpening there. In Photoshop, vignettes are easily created non-destructively, meaning you are adding an adjustment and not changing pixels. Here are the steps to create a simple non-destructive vignette:\nTake the elliptical marquis tool and drag it over the photo so that your screen looks something like this:\nMake sure your foreground and background colors are set to black and white, as shown in the image below. Create a curves adjustment layer which will result in a layer mask that looks like the one below.\nDo Command-Shift-I (Mac) or Control-Shift-I (Windows) which will change the mask colors (black & white) to the inverse.\nSelect “refine mask” in the menu (Command-Alt-R for Mac or Control-Alt-R for Windows) and, if you are working with a RAW or large JPG, try an initial feather at around 250. The idea is to get a nice feather around the edges of the image.\nChange the blending mode to Multiply, which will darken the selected area. You image should look like this, which looks pretty horrible.\nLastly, reduce the opacity until the vignette is not really noticeable. I usually reduce to around 20%, as in the case here.\nYou can make further refinements to the vignette with a brush. I sometimes do this if a corner is already dark and the vignette results in the corner being too dark. Just paint black on the mask at perhaps 30% opacity until the effect is painted out.']	['<urn:uuid:99154423-19d9-4727-bc34-9a2611e41f39>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:47:23.148022	8	32	506
26	sast dast comparison open source risks	SAST and DAST are complementary application security testing approaches. SAST uses white box testing to inspect static source code for security weaknesses, including syntax errors and input validation issues. DAST takes a black box approach, executing code at runtime to detect vulnerabilities in areas like query strings and authentication. When using these tools with open source components, it's crucial to address specific risks. Known vulnerabilities in open source code are a major concern, as over 80% of code in new applications can come from existing repositories. Additionally, there's a risk of compromise of legitimate packages, where attackers can inject malicious code by hijacking maintainer accounts or exploiting package repository vulnerabilities.	['What is Application Security Testing\nApplication security testing (AST) is the process of making applications more resistant to security threats, by identifying security weaknesses and vulnerabilities in source code.\nAST started as a manual process. Today, due to the growing modularity of enterprise software, the huge number of open source components, and the large number of known vulnerabilities and threat vectors, AST must be automated. Most organizations use a combination of several application security tools.\nStatic Application Security Testing (SAST)\nSAST tools use a white box testing approach, in which testers inspect the inner workings of an application. SAST inspects static source code and reports on security weaknesses.\nStatic testing tools can be applied to non-compiled code to find issues like syntax errors, math errors, input validation issues, invalid or insecure references. They can also run on compiled code using binary and byte-code analyzers.\nDynamic Application Security Testing (DAST)\nDAST tools take a black box testing approach. They execute code and inspect it in runtime, detecting issues that may represent security vulnerabilities. This can include issues with query strings, requests and responses, the use of scripts, memory leakage, cookie and session handling, authentication, execution of third-party components, data injection, and DOM injection.\nDAST tools can be used to conduct large-scale scans simulating a large number of unexpected or malicious test cases and reporting on the application’s response.\nInteractive Application Security Testing (IAST)\nIAST tools are the evolution of SAST and DAST tools—combining the two approaches to detect a wider range of security weaknesses. Like DAST tools, IAST tools run dynamically and inspect software during runtime. However, they are run from within the application server, allowing them to inspect compiled source code like IAST tools do.\nIAST tools can provide valuable information about the root cause of vulnerabilities and the specific lines of code that are affected, making remediation much easier. They can analyze source code, data flow, configuration and third-party libraries, and are suitable for API testing.\nMobile Application Security Testing (MAST)\nMAST tools combine static analysis, dynamic analysis and investigation of forensic data generated by mobile applications. They can test for security vulnerabilities like SAST, DAST and IAST, and in addition address mobile-specific issues like jailbreaking, malicious wifi networks, and data leakage from mobile devices.\nSoftware Composition Analysis (SCA)\nSCA tools help organizations conduct an inventory of third-party commercial and open source components used within their software. Enterprise applications can use thousands of third-party components, which may contain security vulnerabilities. SCA helps understand which components and versions are actually being used, identify the most severe security vulnerabilities affecting those components, and understand the easiest way to remediate them.\nRuntime Application Self-Protection (RASP)\nRASP tools evolved from SAST, DAST and IAST. They are able to analyze application traffic and user behavior at runtime, to detect and prevent cyber threats.\nLike the previous generation of tools, RASP has visibility into application source code and can analyze weaknesses and vulnerabilities. It goes one step further by identifying that security weaknesses have been exploited, and providing active protection by terminating the session or issuing an alert.\nRASP tools integrate with applications and analyze traffic at runtime, and can not only detect and warn about vulnerabilities, but actually prevent attacks. Having this type of in-depth inspection and protection at runtime makes SAST, DAST and IAST much less important, making it possible to detect and prevent security issues without costly development work.\nApplication Security Testing Best Practices\nShift security testing left\nNew organizational practices like DevSecOps are emphasizing the need to integrate security into every stage of the software development lifecycle. AST tools can:\n- Help developers understand security concerns and enforce security best practices at the development stage.\n- Help testers identify security issues early before software ships to production.\n- Advanced tools like RASP can identify and block vulnerabilities in source code in production.\nTest internal interfaces, not just APIs and UIs\nIt is natural to focus application security testing on external threats, such as user inputs submitted via web forms or public API requests. However, it is even more common to see attackers exploit weak authentication or vulnerabilities on internal systems, once already inside the security perimeter. AST should be leveraged to test that inputs, connections and integrations between internal systems are secure.\nNew vulnerabilities are discovered every day, and enterprise applications use thousands of components, any of which could go end of life (EOL) or require a security update. It is essential to test critical systems as often as possible, prioritize issues focusing on business critical systems and high-impact threats, and allocate resources to remediate them fast.\nThird-party code security\nOrganizations should employ AST practices to any third-party code they use in their applications. Never “trust” that a component from a third party, whether commercial or open source, is secure. Scan third-party code just like you scan your own. If you discover severe issues, apply patches, consult vendors, create your own fix or consider switching components.\nImperva RASP Solutions\nImperva provides RASP capabilities, as part of its application security platform. Imperva RASP keeps applications protected and provides essential feedback for eliminating any additional risks. It requires no changes to code and integrates easily with existing applications and DevOps processes, protecting you from both known and zero-day attacks.\nIn addition, Imperva provides multi-layered protection to make sure websites and applications are available, easily accessible and safe. These application security solutions include:\n- DDoS Protection—maintain uptime in all situations. Prevent any type of DDoS attack, of any size, from preventing access to your website and network infrastructure.\n- CDN—enhance website performance and reduce bandwidth costs with a CDN designed for developers. Cache static resources at the edge while accelerating APIs and dynamic websites.\n- Cloud WAF—permit legitimate traffic and prevent bad traffic. Safeguard your applications at the edge with an enterprise‑class cloud WAF.\n- Gateway WAF—keep applications and APIs inside your network safe with Imperva Gateway WAF.\n- Attack analytics—mitigate and respond to real security threats efficiently and accurately with actionable intelligence across all your layers of defense.\n- Account takeover protection—uses an intent-based detection process to identify and defends against attempts to take over users’ accounts for malicious purposes.\n- API security—protects APIs by ensuring only desired traffic can access your API endpoint, as well as detecting and blocking exploits of vulnerabilities.\n- Advanced bot protection—analyzes your bot traffic to pinpoint anomalies, identifies bad bot behavior and validates it via challenge mechanisms that do not impact user traffic.', 'Endor Labs, a software firm that facilitates the security and maintenance of open-source software, has released a report identifying the top 10 security and operational risks in open-source software in 2023 .\nCarried out by the Endor Labs’ Station 9 team, the report featured contributions from more than 20 industry chief information security officers from notable companies including Adobe, HashiCorp, Discord and Palo Alto Networks.\nAccording to Endor Labs, the over-reliance on open-source software has recorded some known vulnerabilities, captured as Common Vulnerabilities and Exposures; these vulnerabilities are often overlooked and could be exploited by attackers if not fixed.\n“Open-source software represents a goldmine for application developers, but it needs security capabilities that are equally effective,” said Henrik Plate, lead security researcher at Endor Labs. “In an environment where more than 80% of the code in new applications can come from existing repositories, it is clear there are serious risks Involved.”\nTop open-source risks of 2023\nHighlighted below are the key takeaways of Endor Labs’ report about the top 10 open-source risks of 2023.\n1. Known vulnerabilities\nThe report revealed that an open-source component version may contain vulnerable code accidentally introduced by its developers. The vulnerability can be exploited within the downstream software, potentially compromising the confidentiality, integrity or availability of the system and its data.\n2. Compromise of legitimate package\nAccording to Endor’s report, attackers can target legitimate resources from an existing project or distribution infrastructure to inject malicious code into a component. For example, they can hijack the accounts of legitimate project maintainers or exploit vulnerabilities in package repositories. This type of attack can be dangerous since the malicious code can be distributed as part of a legitimate package and can be difficult to detect.\n3. Name confusion attacks\nAttackers can create components with names that resemble those of legitimate open-source or system components. The Endor Labs report revealed that this could be done through:\n- Typo-squatting: The attacker creates a name that is a misspelling of the original component’s name.\n- Brand-jacking: The attacker suggests a trustworthy author.\n- Combo-squatting: The attacker plays with common naming patterns in different languages or ecosystems.\nThese attacks can be used to trick users into downloading and using malicious components they believe are legitimate.\n4. Unmaintained software\nUnmaintained software is an operational issue, according to the Endor Labs report. A component or version of a component may no longer be actively developed, which means patches for functional and non-functional bugs may not be provided promptly or not at all by the original open-source project. This can leave the software vulnerable to exploitation by attackers who target known vulnerabilities.\n5. Outdated software\nFor convenience, some developers use an outdated version of a code base when there are updated versions. This can result in the project missing out on important bug fixes and security patches, leaving it vulnerable to exploitation.\n6. Untracked dependencies\nProject developers may not be aware of a dependency on a component for several reasons:\n- It is not part of an upstream component’s software bill of materials.\n- Software composition analysis tools are not run or do not detect it.\n- The dependency is not established using a package manager, which can lead to security issues, as vulnerabilities in the untracked dependency may go unnoticed.\n7. License and regulatory risk\nA component or project may not have a license or may have one that is incompatible with the intended use or whose requirements are not or cannot be met.\nUsing components in accordance with their license terms is crucial. Failing to do so, such as using a component without a license or not complying with its terms, can result in copyright or license infringements. In such cases, the copyright holder has the right to take legal action.\nAdditionally, violating legal and regulatory requirements can limit or impede the ability to address certain industries or markets.\n8. Immature software\nAn open-source project may not follow development best practices, such as using a standard versioning scheme, having a regression test suite, or having review guidelines or documentation. This can result in an open-source component that does not work reliably or securely, making it vulnerable to exploitation.\nRelying on an immature component or project can pose significant operational risks. For instance, the software that depends on it may not function as intended, leading to runtime reliability issues.\n9. Unapproved changes (mutable)\nWhen using components that are not guaranteed to be identical when downloaded at different times, there is a significant security risk. This is demonstrated by attacks such as the Codecov Bash Uploader, where downloaded scripts are piped directly to bash without verifying their integrity beforehand. The use of mutable components also poses a threat to the stability and reproducibility of software builds.\n10. Under/over-sized dependency\nThe Endor report pointed out that over/under-dependency on components can be an operational risk. For instance, small components, such as those that contain only a few lines of code, are vulnerable to the same risks as larger components. These risks include account takeovers, malicious pull requests, and continuous integration and continuous development pipeline vulnerabilities.\nOn the other hand, huge components may have accumulated many features that are not necessary for standard use cases. These features increase the component’s attack surface and may introduce unused dependencies, resulting in bloated ones.\nSteps to take to mitigate these open-source risks\nHere are tips from Endor Labs on how software developers and IT managers can mitigate these open-source risks.\nRegularly scan code to spot compromised packages\nPreventing compromised packages is a complex issue because there is no one-size-fits-all solution. To address this, organizations can refer to emerging standards and frameworks such as the OpenSSF Secure Supply Chain Consumption Framework (S2C2F).\nThey can select and prioritize the safeguards that best suit their requirements based on their specific security needs and risk tolerance.\nCheck whether a project follows development best practices\nTo assess a project’s quality and currency, check its documentation and release notes for completeness and timeliness. Look for badges that indicate test coverage or the presence of CI/CD pipelines that can detect regressions.\nIn addition, you can evaluate a project by checking the number of active maintainers and contributors, how frequently new releases are made, and the number of issues and pull requests that are opened and closed. It is also crucial to look up information on a project’s maintenance or support strategy — for example, the presence and dates of long-term support versions.\nKeep dependencies up to date and check code characteristics before using them\nTo ensure code security, checking both code and project characteristics is important. Examples of code characteristics to check include pre- and post-installation hooks and encoded payloads. For project characteristics, consider the source code repository, maintainer accounts, release frequency and the number of downstream users.\nOne way to keep dependencies up-to-date is to use tools that generate merge or pull requests with update suggestions. It’s also important to make dependency updates and recurring backlog items a priority.\nEvaluate and compare software composition analysis tools\nSecurity teams should ensure SCA tools are capable of producing accurate bills of materials, both at the coarse-granular level, such as for dependencies declared with the help of package management tools like Maven or npm, and fine-granular level, such as for artifacts like single files included “out of band” without using package managers.\nUse components in compliance with open-source license terms\nIT leaders should ensure their software developers avoid using open-source components without a license, as this could create legal risks. To ensure compliance and avoid potential legal issues, it’s important to identify acceptable licenses for components used in software development.\nFactors to consider include how the component is linked, the deployment model and the intended distribution scheme. Once you’ve identified acceptable licenses, comply with the requirements stated in those open-source licenses.\nRead next: Top cybersecurity threats for 2023 (TechRepublic)']	['<urn:uuid:1f9d94c3-cbca-4673-95bf-b8ee71b3962c>', '<urn:uuid:0a1689a3-8a49-40ba-8749-c2c082eb1034>']	open-ended	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T19:47:23.148022	6	110	2385
27	why earth called water planet	Three-fourth's of earth is covered by water. Hence, it is called the water planet.	['Class 8 Geography Chapter 2 | Land, Soil, Water, Natural Vegetation and Wildlife Resources | Important Questions And Answers\nHere are important questions from NCERT Class 8 Geography chapter 2 Land, Soil, Water, Natural Vegetation And Wildlife Resources.\n1. Why do places and living standards differ from one another?\nPlaces and living standards differ from one another because the quality of soil, access to water, natural vegetation and technology are not the same everywhere. Some places have fertile soil, favourable weather, abundance of water and easy access to technology and they tend to become prosperous. By contrast, several other places lack in these fundamental aspects and tend to be backward.\n2. Why is land one of the most important natural resources?\nLand is one of the most important natural resources because only 30% of the earth’s surface is covered by land. Also, some parts of the land are not habitable. Actually, 90% of the world population occupies only 30% of the total land. The remaining 70% of the land is either uninhabited or sparsely populated. Because of the limited availability of land, it is an important natural resource.\n3. Why is population distributed unevenly?\nThe characteristics of land and climate change from place to place. Places that have rugged topography or that are situated on the steep slopes of mountains are not suitable for habitation. Also, low lying areas susceptible to flooding, deserts and thick forests are also uninhabited or sparsely populated.\n4. Why are plains and river valleys densely populated?\nPlains and river valleys are densely populated because they offer suitable land for agriculture.\n5. What are some uses of land?\nLand is used for various purposes such as agriculture, mining, forestry, building houses, roads and factories.\n6. What are the factors that determine the use of land?\nFactors that determine the use of land are its topography, quality of soil, climate, presence of minerals and availability of water. Other factors that determine the use of land are the size of the human population and their access to technology.\n7. What are common property resources?\nThese are community lands that are owned by a community for common uses like collecting fodder, fruits, nuts and medicinal herbs.\n8. What are the environmental problems caused by expansion of agriculture and construction activities?\nLand degradation, landslides, soil erosion and desertification are some of the major environmental threats caused by construction activities and expansion of agriculture.\n9. What are some common methods used to conserve land resources?\nAfforestation, land reclamation, regulated use of chemical fertilisers and pesticides and checks on overgrazing are some of the methods used to conserve land resources.\n10. What is soil?\nSoil is the thin layer of grainy substance covering the surface of earth.\n11. What are the components of soil?\nSoil is made up of organic matter, minerals and weathered rocks.\n12. What makes soil fertile?\nThe right mix of minerals and organic matter makes soil fertile.\n13. What is weathering?\nWeathering is the breaking up and decay of exposed rocks due to changes in temperature, action of frost and activities of plants, animals and humans.\n14. What are landslides?\nLandslides are the mass movement of rock, debris and earth down a slope.\n15. What are the factors that can trigger a landslide?\nA Landslide can be triggered by earthquakes, floods and volcanoes. A prolonged spell of rainfall can also cause landslides.\n16. What are some measures that can be taken to prevent landslides or mitigate their impact?\nHazard mapping can be done to identify areas that are prone to experience landslides. Such areas should not be used for building human settlements.\nRetention walls can be built to stop land from slipping.\nRoots can hold soil and prevent it from slipping. Hence, increasing vegetation cover is an effective way to prevent landslides.\nSurface drainage control is another measure.\n17. What are the factors affecting soil formation?\nNature of the parent rock and climate are the major factors affecting soil formation. Other factors include topography, role of the organic material and time taken for the composition of soil formation.\nTemperature and rainfall in a particular region affect the rate of weathering and humus formation.\nThe parent rock determines the colour, texture, chemical properties, mineral content and permeability of the soil.\nThe altitude of the place and the slope of the land affect the accumulation of soil.\nThe flora, fauna and micro-organisms affect the rate at which humus is formed.\nIt takes hundreds of years to make just one centimetre of soil.\n18. What are the different kinds of soil found in India?\nAlluvial soil, black soil, red soil, laterite soil, desertic soil and mountain soil.\n19. What are the factors that lead to soil degradation?\nSome of the factors leading to soil degradation are deforestation, overgrazing, overuse of chemical fertilizers or pesticides, rain wash, landslides and floods.\n20. Briefly explain some methods of soil conservation\nMulching involves covering the bare ground between plants with a layer of organic material like straw. This helps to retain soil moisture.\nThis involves building barriers along contours using stones, grass and soil. Trenches are made in front of the barriers to collect water.\nRocks are piled up to slow down the flow of water. This prevents soil loss and gullies.\nBroad flat terraces are made on the steep slopes of mountains to grow crops. This way, mountain slopes can be used for agriculture. It also reduces surface run off and soil erosion.\nIntercropping is the practice of growing different crops in alternate rows. The seeds are sown at different times to protect the soil from rain wash.\nThis is the practice of ploughing parallel to the contours of a hill slop to form a natural barrier for water to flow down the slope.\nIn coastal areas and dry regions, rows of trees are planted to check the wind movement to protect soil cover.\n21. Why is earth called the water planet?\nThree-fourth’s of earth is covered by water. Hence, it is called the water planet.\n22. Where did life originate about 3.5 billion years ago?\nIn the oceans\n23. Why is ocean water not fit for human consumption?\nOcean water is salty and hence it is not fit for human consumption.\n24. Why is water an important natural resource?\nAlthough three-fourth’s of the earth’s surface is covered by water, fresh water accounts for only 2.7% of it. Nearly 70% of the fresh water occurs as ice sheets and glaciers in the Antarctic, Greenland and mountain regions. Due to their remote location, they are not accessible to humans or animals. Only 1% of the fresh water is available and fit for human use. It exists as groundwater, surface water in rivers and lakes and water vapour in the atmosphere. Because of these reasons water is an important renewable natural resource.\n25. Is water a renewable resource?\n26. Total volume of water remains constant. Explain\nThe amount of water present on earth cannot be increased or decreased. Its quantity seems to vary because it is in constant motion cycling through the ocean, the air, land and back again through processes called evaporation, precipitation and run off.\n27. What are the main uses of water?\nWater is used for a variety of purposes such as drinking, washing, cooking and bathing. It is also used for growing crops, to generate electricity and in industries.\n28. What are the reasons for water shortage?\nAs population increases, the demand for food and cash crops also increases. Since water is essential for growing crops, this rise in demand increases the consumption of water. Increasing urbanisation leads to the drying up of water sources. Industrialization often pollutes water bodies. The rising living standards also increase the consumption of water. All of these factors lead to water shortage. Countries located in climatic zones that do not receive much rainfall face greater scarcity of water. Thus, water shortage could be a consequence of changes in seasonal and annual precipitation or it could be caused by overexploitation or contamination of water sources.\n29. Name some countries or parts of the world where water scarcity is a major problem?\nWater scarcity is a problem in most of Africa, West Asia, South Asia, parts of western USA, north-west Mexico, parts of South America and entire Australia.\n30. What are the main causes of water pollution?\nDischarge of untreated or partially treated sewage, agricultural chemicals and industrial effluents into water bodies are the main causes of water pollution.\n31. How can water pollution be controlled?\nThe main cause of water pollution is the discharge of untreated sewage, agricultural chemicals and industrial effluents into water bodies. These effluents contain nitrates, metals and pesticides. By treating these effluents before they are released into the water, water pollution can be controlled to a great extent.\n32. How can water resources be conserved?\nPlanting more trees and increasing vegetation cover help to reduce surface runoff and replenish underground water reserves. Water harvesting is another method to conserve water. Water seepage can be prevented by properly lining canals used for irrigating land. Using sprinklers for irrigating land will help reduce water loss through evaporation. In extremely dry regions, drip irrigation is useful as it prevents water loss through evaporation.\n33. What is rain water harvesting?\nRain water harvesting is the process of collecting rain water from roof tops and directing it to an appropriate place where it can be stored for future use.\n34. What is biosphere?\nBiosphere is the narrow zone of earth where land, water and air interact with one another to support life. Life exists only in the biosphere.\n35. What is ecosystem?\nBiosphere is the zone of earth supporting life. All living beings in the biosphere are interrelated and dependent on one another for survival. It is this life supporting system that is known as the ecosystem.\n36. How are plants useful to us?\nPlants produce the oxygen we breathe, give shelter to animals, protect soil, provide timber, give us food, oil, gum and even medicines.\n37. How is wildlife useful to us?\nWildlife includes animals, birds, insects and aquatic life forms. Birds and animals provide us with milk, meat, eggs, hides and wool. Insects like bees provide us honey. They also help in the pollination of flowers and act as important decomposers in the ecosystem.\n38. How are vultures useful to us?\nVultures feed on dead livestock and act as a vital cleanser of the eco-system.\n39. Why is the population of vultures dwindling in India?\nIndian vultures are on the brink of extinction because their numbers reduced rapidly during the last few decades. Millions of them died of kidney failure after scavenging on livestock treated with the painkiller diclofenac. Now efforts are on to ban this drug for livestock use and breed vultures in captivity.\n40. Name the two factors that mainly affect the growth of vegetation\nTemperature and moisture\n41. What are the four main types of vegetation in the world?\nForests, grasslands, scrubs and tundra\n42. The type of vegetation that is likely to be found in an area receiving abundant rainfall will be ………………………………..\n43. Grasslands are found in areas receiving ……………………………….. rainfall.\n43. Why do thorny shrubs and scrubs grow in dry areas?\nThorny shrubs and scrubs have deep roots. Their thorny leaves have a waxy surface to prevent water loss through evaporation.\n44. Name the kind of vegetation found in the Polar region\n45. Tundra vegetation comprises of …………………………….\nLichens and mosses\n46. Name some factors that lead to the extinction of natural vegetation and wildlife\nDeforestation, soil erosion, constructional activities, forest fires, tsunami and landslides are some of the human and natural factors leading to the extinction of natural vegetation and wildlife.\n47. What is poaching?\nPoaching is the illegal hunting and capturing of wildlife for collection and illegal trade of hides, skins, nails, teeth, horns and feathers.\n48. Name some measures taken by the government to protect our natural vegetation and wildlife.\nThe government has built national parks, wildlife sanctuaries and biosphere reserves to protect natural vegetation and wildlife.\n49. What is a national park?\nA national park is a designated natural area to protect the ecological integrity of ecosystems for the present and future generations.\n50. What is CITES?\nCITES is the Convention on International Trade in Endangered Species of Wilf Fauna and Flora.']	['<urn:uuid:efb3ce0c-5e94-4e04-8d35-8e11a334f377>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T19:47:23.148022	5	14	2056
28	Which facility was established earlier: PVEL's test facility that began in 2010 or the Teide Observatory that was created in 1959?	The Teide Observatory was established earlier, being officially created in 1959, while PV Evolution Labs (PVEL) began doing module evaluation in 2010.	"['PV module test facilities generally begin with procedures certifying that modules meet IEC or UL standards. PV Evolution Labs (PVEL), a small facility that began doing module evaluation in 2010, has been referring manufacturers seeking certification to other labs.\n“We have the capability and will start offering the service soon,” explained PVEL CEO Jenya Meydbray, “but it is not a strategic focus.”\nPVEL’s focus, Meydbray said, is the downstream PV market. “The needs of bankers and developers are different than those of manufacturers. They want more than just raw data test reports. They need higher-level advisory services.”\nAdvisory services are a PVEL differentiator, Meydbray said, setting it apart from more established testing labs.\nPVEL’s testing may be different from the bankability testing described by Renewable Energy Testing Center and Intertek representatives in this series (see previous articles Testing and Ranking Solar Module Quality and What Do Solar Module Test Procedures Prove?).\n“If bankability is the ability to get institutionally financed,” Meydbray said, “that is what we provide. But it is not enough to provide a third-party test report. PVEL services start with the test report to the manufacturer, they don’t end there.”\nWith its diverse relationships on the “buy side,” Meydbray said, “we can help established manufacturers access buyers. If they do well on our extended reliability testing, we can introduce them around, circulate their test reports, and support getting their product allocated.”\nThere are two key sets of PVEL test protocols, Meydbray said. The first set has to do with energy forecasting.\nAfter a developer’s extensive -- and expensive -- effort to make a project happen, Meydbray explained, the majority of the return comes from a “development fee” when the project goes on-line. “That fee is determined by its forecasted lifetime output in kilowatt-hours made by an independent third-party engineering firm like Black & Veatch or BEW on behalf of the bank.”\nA per-kilowatt-hour price determines the potential cash flow, the bank’s loan and the developer’s fee, Meydbray said. “Developers live and die on that fee. A 1 percent higher energy forecast can increase a developer’s profitability by 10 percent.”\nPVEL’s first protocol characterizes panel performance in varying environmental conditions and therefore provides some certainty around the energy forecast. “[Just] 1 percent can mean millions of dollars. We can do these tests for around $15,000, depending on the details. It becomes a no-brainer.”\nThe first of the three tests in the energy forecast protocol is a PAN file.\n“It is a software file used to project energy and cash flow,” Meydbray said. “It contains raw data from twenty-two varying temperatures and irradiances. Black & Veatch, with whom we have a strategic partnership, turns that raw data, using a software platform called PV Syst, into PAN file coefficients that characterize the panel’s output for energy forecasting.”\nThe second test is a third-party report on the light-induced degradation (LID) of a panel. “That can be a half percent to 3 percent of output, and every fraction of a percent really matters. The test is easy and not expensive.”\nA third measurement done by PVEL is called the Incidence Angle Modifier (IAM). “A solar panel’s ratings are done with light at a normal incident angle, but in the field, a panel is hit by light from many angles, depending on the time of day, the season and the installation,” Meydbray explained. “We characterize performance as the light angle changes.”\nThe IAM measures output at ten different incidence angles from zero to 90 degrees, Meydbray said. The method was developed by PVEL’s David King when he was a Director at Sandia Labs. It is performed at PV USA, PVEL’s outdoor solar testing facility. “PV USA was built in 1986,” Meydbray said. “PVEL took it over in early 2011.”\nThe first three tests are focused on increasing the accuracy of the energy forecast and characterizing the product’s out-of-the-box performance, Meydbray said. “But of course, the product does not perform over 25 years in the same way. It degrades. That’s what most labs are looking at: accelerated lifetime testing.”\nProduct certification “does not catch most panel problems seen in the field because it is performed before high volume manufacturing ramps,” Meydbray said. Most field problems are due to manufacturing quality control failures. “Once you start producing 10,000 modules per day, you start to get deviations in the process that impact product quality.”\nCurrent UL and IEC certification requirements are limited and are done before manufacturing ramps up. “This regulatory regime doesn’t provide the protection investors need. That is the gap we are filling.”\nPVEL has developed statistical models for randomly sampling modules, Meydbray explained. “If you want 100 percent certainty that all the modules are defect-free, you have to test every one. Nobody will do that because it is too expensive and takes too long. We have a statistical model that lets project owners decide their risk tolerance.”\nThe decision, Meydbray said, “has implications on cost of capital, insurance premiums, residual value, resale value -- real commercial things that influence the success and profitability of a project.”\nThe PVEL statistical model allows developers to choose a level of certainty and a cost. “The cost for 99 percent certainty,"" Meydbray said, is usually “between $0.005 cents per watt and $0.007 cents per watt.”\nThe cost for the first set of three tests is $15,000 to $25,000, depending on variables, Meydbray added.\nPVEL and PV consultant SolarBuyer are developing an Approved Vendor Program “that puts this all together,” Meydbray said.', 'The Observatories of the Instituto de Astrofísica de Canarias -the Teide Observatory in Izaña (Tenerife), and the Roque de los Muchachos Observatory, in Garafía (La Palma)- are situated at two of the most privileged sites for astronomical observation. The contemporary history of astronomy in the Canaries started with the astronomical expeditions of the XIX century-A description of these campaigns by Prof. F. Sanchez can be found in “Vistas in Astronomy"" (1985).\nPiazzi Smyth 1856\nThe British astronomer Piazzi Smyth showed for the first time that high altitude sites provide clear advantages for astronomical observing. He reached this conclusion after observing from different heights in Tenerife, from sea level to the Guajara mountain (2,717 m) and Altavista (3,250m) close the peak of Teide.\nJean Mascart, 1910\nIn that year the French astronomer Jean Mascart came to the Canaries with the purpose of observing the approach of Halley’s comet. Mascart was so satisfied with the conditions for astronomical observation on the mountains of Tenerife, that he proposed the setting up of an international observatory on Guajara mountain.\nSolar eclipse, 1959\nMany astronomers from around the world visited the islands to observe a solar eclipse. This renewed the interest in establishing a permanent observatory. Scientists from the CSIC (A. Romañá) and from the University of Madrid (J.M. Torroja) pushed the idea in Spain and studies were initiated of the astronomical conditions in the region of Izaña. In that year the Teide Observatory was officially created, and this later came under the responsibility of the Rectorate of the University of La Laguna\nIn 1961 Francisco Sánchez, the founder-director of the IAC, arrived in the Canaries with a clear objective, to study the astronomical quality of the mountain tops of Tenerife. He had previously established the parameters and measurement methods which were needed for this study. The 60’s was the decade of Spanish astronomical site testing, the first time this had been done in the Canary Islands. In 1964 the first professional telescope was installed at the Teide Observatory, a photopolarimetric telescope for night-time observations, from the University of Bordeaux (France), designed to study the zodiacal light (the light scattered by the interplanetary medium and visible from the tropics). With this the first astrophysical research group in the country (“Upper Atmosphere and Interplanetary Medium”) was formed the first doctoral theses were written and the first astrophysical articles and communications in Spain were produced.\nAfter this telescope more were added as the years went by (in 1969 the first solar telescope was installed at the Teide Observatory) until the present major set of telescopes had been installed in the Canary Island Observatories. The installation of each telescope has always been preceded by the corresponding agreement, following the model established with the Bordeaux Telescope: sky in exchange for a telescope. In this way the prejudices of the European Astronomers about the conditions for astronomy in the Canaries were overcome, and the Spanish authorities (both local and national) began to understand that this “natural resource”, the Canary sky, could be exploited, and in addition used to initiate and develop astrophysics in Spain.\nThe decade of the ‘70s is the decade of international site testing. European astronomers were at last persuaded to test the sites in the mountains of Tenerife and La Palma, and the excellent quality of the Canary skies were recognized. In 1970 there took place the first inauguration at the Teide Observatory. Two years later at this observatory the 1,55m infrared telescope (then the largest infrared telescope in the world) came into service at this observatory. It belonged at that time to Imperial College London, and has since been given to the IAC, and is now called the “Carlos Sanchez Telescope”\nIn 1973 the University Institute of Astrophysics was created at the University of La Laguna, and it later took charge of the Teide Observatory. In 1975 the Instituto de Astrofísica de Canarias (IAC) was founded, as the result of an agreement between the University of La Laguna, the Higher Council for Scientific Research (CSIC) and the Joint Council of the Cabildos of the Province of Santa Cruz de Tenerife.\nThe University Institute of Astrophysics is absorbed by the IAC and the CSIC gives it the status of one of its own centres. Where the Faculty of Physics is today the library, the laboratories, the workshops and the services of the IAC are installed, in some prefabricated buildings made of uralite funded by the Cabildo of Tenerife\nAlso in 1975 the 1st National Assembly of Astronomy and Astrophysics was celebrated in Tenerife, and the IAC initiated the first National Programme for the Training of Researchers in Astrophysics. In 1978 the degree in Physics was started in the University of La Laguna via the Speciality in Astrophysics.\nAfter the necessary negotiations with various European scientific institutions interested in installing telescopes in the Canaries the Agreements for Cooperation in Astrophysics were signed, by which the exploitation of the sky of the Canaries was regulated, so that the IAC’s observatories were opened to the most advanced telescopes\nThus on May 26th 1979 Spain signed, with Denmark, Sweden, and the United Kingdom, in Santa Cruz de La Palma the “Agreement and Protocol of Cooperation in Astrophysics” by which the Teide Observatory (Tenerife) and the Roque de los Muchachos Observatory (La Palma) were internationalised\nIn addition the IAC produced its first contract for the transfer of technology, installs the first technical laboratories and workshops at the University of La Laguna and in 1979 transfers a patent ( for a heat exchanger and thermal regulator) to a company which is being set up (today “Energía Solar Española”)\nThe ‘80s is the decade of administrative consolidation of the IAC and its International Observatories. After a legal and administrative study of the IAC and a scientific audit we managed to obtain autonomy and a specific legal status for the Institute. In 1982 the Spanish Parliament unanimously approved the Royal Legal Decree (7/1982 of April 30th) by which the IAC was declared administratively as a “Public Consortium” depending on the National Administration, the Autonomous Community of the Canaries, the University of La Laguna, and the CSIC. IN 1983 the postgraduate school of the IAC began its operation, including its own positions of “resident astrophysicists”\nNew scientific institutions and countries decide to install their best telescopes in the IAC’s Observatories, and agreements were negotiated by which Germany (1983), Finland (1886), and France (1988) become members.\nIn 1985 the Official Inauguration of the Instituto de Astrofísica de Canarias and the Teide and Roque de los Muchachos Observatories took place, with monarchs and members of the Royal Families of five countries (Spain, Denmark, the United Kingdom, The Netherlands, and Sweden) and two other heads of state (Germany and Ireland). In addition twelve ministers from European countries were present, as well as a distinguished representation of the scientific community, headed by five Nobel Laureates.\nIn 1987 the Anglo-Dutch “William Herschel” telescope of 4.2 metre diameter was inaugurated, at the time the largest telescope in the Canaries, and in 1989 the nordic telescope NOT, of 2.56 m, both at the Roque de los Muchachos Observatory.\nThe IAC begins to gain experience with scientific payloads for sounding rockets by producing instruments for upper atmosphere studies following this with satellite instrumentation. We participate with a leading role in the design and construction of ISOPHOT-S, a spectrograph for the ISO (Infrared Space Observatory) satellite of ESA, and start to participate in the VIRGO and GOLF instruments for the SOHÖ satellite.\nThe IAC designs instruments not only for astronomy, but also stimulates the technological development of the surrounding community. In order to spin off and commercialize technological products from the IAC in 1988 the company “GALILEO, Engineering and Services S.A.” was formed.\nAlso in 1988 the “Law for the Protection of the Astronomical Quality of the IAC Observatories” (Law 31/1988 of October 31st) was passed, whereby the mountain tops of the islands of Tenerife and La Palma were turned into a world astronomical reserve. We attracted more and more international meetings and conferences in astrophysics, among which we note the European Meeting of the International Astronomical Union (IAU) celebrated in Tenerife in 1989.\nIn the same year the first “Winter School” of the IAC is celebrated, with the idea of creating as an institution an international school of Astrophysics with the name “Canary Islands Winter School of Astrophysics”. The object is to ensure, during a period of two weeks, that the best experts in a field of major scientific interest (different each year) live together with doctoral students from all over the world who are working in the field.\nDuring the ‘80s the IAC’s outreach effort increases; there are many open days for the public at the observatories, the researchers give large numbers of popular lectures, and even organize astronomical “shows”. A very popular event was the “Star Party” which the IAC organized to celebrate the proximity of Halley’s Comet in 1986, and which brought together on March 15th, on the beach of Las Teresitas in Tenerife, some sixty thousand people. As well as this, in order to help the astrophysical observations of this well known comet, all the public lighting on La Palma was switched off.\nThe decade of the 90’s is the decade of scientific and technical consolidation of the IAC, which is held in higher international esteem, and is increasingly popular in the Canaries and in the rest of Spain. The IAC continues to sign agreements with scientific institutions wanting to install instruments in the Canaries, and their observatories become established as the European Northern Observatory.\nIn 1991 the Consiglio per le Ricerche Astronomiche decides to install the Italian National Telescope “Galileo” of 3.5m in the Roque de los Muchachos Observatory, going back on its previous decision to install it in Hawaii. In 1994 the European Space Agency (ESA) signs an agreement with the IAC to install at the Teide Observatory a telescope dedicated to optical telecommunications with satellites, with the possibility to use it also for astronomical purposes and to keep track of space debris.\nThe achievements of the IAC in cultural diffusion and outreach become more important, with a private foundation which sets up an official course of astronomy for high school and primary school teachers. A small visitors centre is organized at the Teide Observatory. The IAC has a presence in three pavilions of the Universal Expo in Seville in 1992 and in 1993 a modern interactive museum is opened next to the IAC, the Museum of Science and the Cosmos of the Cabildo of Tenerife. In the same year Prince Felipe, a great enthusiast of astronomy, visits us for the first time.\nThe Canaries, gradually becomes a regular meeting place for the international scientific community, stimulated by the astrophysicists. In 1990 the main space agencies (the American agency NASA, the European ESA, the Japanese ISAS, and the Russian INTERCOSMOS) celebrate a meeting in the islands. IN 1992 the OCDE (Organization for Economic Growth and Development) brings its committee of experts, the “Megascience Forum” to meet in Tenerife- The IAC continues to push technological development in the Canaries and sets up an Office of Technology Transfer.\nThe Government of the Canaries and the Spanish national Government became interested in building a large telescope with diameter greater than 8m. In 1994, with public funding, a public company (GRANTECAN S.A.) was set up, to help the IAC define the telescope, to study its viability and its construction. As well as the obvious scientific objective there is a well defined economic and technological plan: to use this major project to stimulate technology transfer to Spanish industry, and to generate business infrastructure in the Canaries stressing advanced technology.\nIn January 1995 the IAC organizes, together with the BBV Foundation, an international meeting lasting one week between major international experts in astrophysics and young researchers: “Key Problems in Astronomy” Some of the unsolved problems which future generations must tackle were discussed. Cambridge University Press publish a book about the meeting, and the IAC publishes the videos of the discussions.\nIn 1995 the ISO and SOHO satellites, in which the IAC led the Spanish participation in the construction of the instruments ISOPHOT-S, VIRGO, and GOLF, are successfully launched by NASA and ESA,\nImportant scientific discoveries in which IAC astrophysicists take part are made at an increasing rate. In 1992 the first stellar mass black hole in our Galaxy was found, in 1994 the anisotropies in the cosmic microwave background radiation were detected and in 1995 the first “brown dwarf” Teide 1, was detected in the Pleiades.\nIn February 1996 the Governing Council of the IAC, with all the reports in favour, and with the agreement of the participation of the Spanish government and of the Autonomous Region of the Canaries, gives the green light to the construction of the “Great Telescope”, with a segmented mirror of 10m diameter, with the guarantee of at least 50% of the funding for the project.\nIn June 1996 the King and Queen of Spain, accompanied by ministers and other authorities and by distinguished scientists, inaugurate new telescopic and instrumental installations in the Roque de los Muchachos and Teide Observatories, among which we pick out the Italian National “Galileo” Telescope (TNG) and the Franco-Italian solar telescope “THEMIS”.\nIn 1997 the first light of the “Dutch Open Telescope” (DOT) at the Roque de los Muchachos Observatory was attended by His Royal Highness Willem Alexander, the Prince of Orange. At the end of the year His Majesty the Prince of Asturias, Astrophysicist of Honour of the IAC, paid a visit to the IAC Headquarters in La Laguna, for the second time.\nIn collaboration with the Faculty of Medicine of the University of La Laguna a patented prototype was developed of a virtual acoustic space for medical application to blind people. In 1998, with the X Canary Islands Winter School of Astrophysics, dedicated to “Globular Clusters” this annual school was consolidated by the celebration of its 10th anniversary.Also in 1998 the IAC organized a further six scientific meetings, among them three Euroconferences, two international conferences, and the III Scientific Meeting of the Spanish Astronomical Society.']"	['<urn:uuid:885e3329-08e9-479f-81ad-01fe4bd06347>', '<urn:uuid:7b48a837-fc6f-4499-a7f7-15beb745b14a>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T19:47:23.148022	21	22	3266
29	ondansetron antinausea medicine safety risk factors dependence addiction comparison tramadol	While Ominil (ondansetron) is generally well-tolerated when used for short durations (6-10 days) with minimal side effects, tramadol carries significant risks of addiction and dependence despite being classified as a Schedule IV drug. Tramadol users commonly develop tolerance, requiring larger doses over time, and experience withdrawal symptoms upon cessation, while ondansetron has no reported addiction potential.	['Medicine Overview of Ominil Injection\nOminil Injection is used in the treatment and prevention of nausea\nFatigue, Headache, Injection site allergic reaction, Flushing (sense of warmth in the face, ears, neck and trunk), Constipation.\nYour doctor or nurse will give you this medicine. Kindly do not self administer.\nOminil Injection is an antiemetic (anti-nausea medicine). It inhibits the action of serotonin, a chemical messenger that can trigger nausea and vomiting.\nIn Depth Information on Ominil Injection\n- Take Ominil Injection 30 minutes before your food.\n- If you vomit within 30 minutes after taking the Ominil Injection, take the same amount again. If vomiting continues, check with your doctor.\n- If Ominil Injection is used for a short duration, for e.g. 6 -10 days, the risk of side-effects is minimal (well-tolerated).\n- You can use oral disintegrating film/strip (medicated strip that dissolves when comes in contact with a wet surface) form of the Ominil Injection if you are nauseous to swallow a tablet or capsule.\n- If you are using Ominil Injection in the form of oral disintegrating film/strip:\n- Make sure your hands are dry.\n- Immediately place the film/strip on the top of the tongue.\n- The film/strip will dissolve in seconds and you can swallow it with your saliva.\n- You do not need to drink water or other liquids to swallow the film/strip.\nOminil Injection related warnings\nInteraction with alcohol is unknown. Please consult your doctor.\nOminil Injection is probably safe to use during pregnancy.\nAnimal studies have shown low or no adverse effect on the foetus, however, there are limited human studies. Please consult your doctor.\nOminil Injection is probably safe to use during lactation.\nLimited human data suggests that the drug does not represent a significant risk to the baby.\nOminil Injection does not usually affect your ability to drive.\nOminil Injection is safe to use in patients with kidney disease. No dose adjustment of Ominil Injection is recommended.\nHowever, inform your doctor if you have an underlying kidney disease. Limited information is available in patients taking this medicine for more than one day.\nThere is limited information available on the use of Ominil Injection in patients with liver disease. Please consult your doctor.\nIf you miss a dose of Ominil Injection, take it as soon as possible. However, if it is almost time for your next dose, skip the missed dose and go back to your regular schedule. Do not double the dose.\nFrequently asked questions about Ominil Injection\nQ. What is Emset/ondem/Vomikind/Zafran/ Ranidom used for?\nEmset/Ondem/Vomikind/Zafran are some of the brand names of ondansetron, which is used to treat nausea and vomiting. Ranidom is a brand containing the active ingredient Esomeprazole, which is used to treat acidity\nQ. Can I take ondansetron for nausea/vomiting/stomach flu/food poisoning/motion sickness/diarrhea/pain?\nOndansetron can be taken for nausea, vomiting or motion sickness if prescribed by your doctor. It is not used to treat food poisoning, pain or stomach flu\nQ. Can I take ondansetron with Tylenol/Pepto bismol/azithromycin/ xanax/ vicodin/ oxycodone/ hydrocodone?\nThere is no evidence of interaction of ondansetron with Tylenol (paracetamol)/Pepto bismol (bismuth salicylate)/azithromycin/ Xanax (alprazolam)/ Vicodin (aspirin and hydrocodone)/oxycodone/hydrocodone. However, please consult your doctor before use\nQ. Can ondansetron make you sleepy?\nOndansetron may cause sleepiness/dizziness. If you notice such symptoms, please consult your doctor\nQ. Does ondansetron contain acetaminophen/aspirin/penicillin?\nNo. Ondansetron is by itself a chemical compound. It does not contain acetaminophen, aspirin or penicillin\nQ. Is ondansetron antibiotic/anti-histaminic/sulfa drug?\nNo. Ondansetron is not an anti-biotic, sulfa drug or an anti-histaminic. It is an anti-emetic drug used to treat nausea and vomiting\nQ. Is Emeset safe for infants/toddlers?\nEmeset should not be used in infants/toddlers unless prescribed by the doctor\nQ. Is Ondansetron an OTC drug?\nNo. Ondansetron is a prescription drug. It is not available over the counter\nQ. Is ondansetron a controlled substance?\nNo. Ondansetron is not a controlled substance.\nQ. Q. Can I take Ominil with omeprazole?\nOminil is an anti-emetic medicine which helps in relieving nausea (feeling sick) or vomiting. No harmful effects have been seen when omeprazole is used with Ominil. So, these two medicines can be taken together.\nSeverely interacts with other drugs like\nTaking Ominil with any of the following medicines can modify the effect of either of them and cause some undesirable side effects\nMedicines with composition Dextropropoxyphene: Parvodex\nMedicines with composition Tramadol: Tramazac, Contramal, Painadol, Trafix, Relidol, Opiwyn, Biotram, Supridol, Dolmundin, Tramazac TC, Zotra, Domadol, Tram, Trazac, Safrodol, Hytram, Trazodac Fast, Victadol, Trasic, Tramacip, Cemadol, Tranzex, Nobligan, Radogesic, Meridol, Ulmadol, Takol, Tramacad, Syntram, Tramazac HP, OC Dol, Trd, Zyrotram, Tramache, Termeg, Intram, Urgendol, Tramore, Centradol, Tramadin, Kamadol, Analtra, Dolmark, Pentagen, Tradol, Tramatas, Tramak, Tramanam, Tramadex, Trd Contin, Tranzat, Tramasafe, Dolotram, Orchidol, Paincure, Tramot, Tdx, Zoftadol, Sayodol, Notil, Tromanil, Tramanec, Stradol, Tramdol, Trambax, Nexdol, Tramapen, Trugesic, Euphodol, Tramad, Tromadene, Zamadol, Tdol, Trazodac, Axidol, Lomadol, M Dol, Talnex, Paff, Afdol, Adamon, Revodol, Sedate, Trydol, Eltram, Servodol, Pantra, Postadol, Dolus, Topdol, Valcom, Tramaflam, Tramasure, Novadol, Rosetra, Sternix, Tramoflex, Strom, Trama, Tramocare, Tramagesic\nMedicines with composition Ethylmorphine: Dionindon\nMedicines with composition Fluoxetine: Flunil, Fludac, Salidep, Tritin, Fadep, Dawnex, Flutee, Acodac, Theoxetin, Arkadep, Floxin, Platin, Barozac, Zexto, Fudis, Nuzac, Fluoxet, Dedep, Oxefil, Serraflam, Prodep, Flutop, Flumod, Vetodep, Flood, Flugen, Fluze, F Tin, Cool, Depnil, Flunat, Flucer, Inaday, Fluzin, S.R.B., Flutin, Nodep, Exidep, Serodep, Floxiwave, Floatin, Fluden, Cozac, Elemod, Cyclotin, Flumusa, Mentol, Flonol, Fluron, Flox, Fluxater, Flutine, Neurozac, Prodep LA, Fluox, Dimrid, Fluz OD, Fludep, Luxet, Flumeg, Zedep, Exiten, Noc, Depzac, Zotin, Flunam, Flucap, Endep, Flame, Lauxine, Persona, Hidac, Fludawn, Depextine, Depcure, Fluxy, Griloc, Normatine, Trizac, Faxtin, F-20, Fastgen, Strutee, Fly\nMedicines with composition Escitalopram: Recita, Cipralex, Articalm, SC Talo, Sri, Serosez, Depitame, Deplam S, ES OK, C-Pram S, Szetalo, Feliz-S, Nexito, Renopram, Sitaram, Citajoy, Cilentra, Esopam, Deptune, Esciram, Escipra, Esitor, Escitlocad, Elorpax, Stalopam, Citapure Forte, Taloprex, Citofast, Estilo, Jopram, Cipam S, Escalate, Censpram, Pridep S, Celor S, Esna, Citopil, S-Celepra, Escer, Estajoy, Astal, Citel, Jovil, Elcita, Joycom, Galop Plus, S-Citapad, Eloprax, Excita, Lexadep, Newcita, Desilam, Escytop, Coxito S, Citapure, Ambulax AD, Ezeepam, Essita, Pracidep S, Citaford S, Espam, Estalo, Lifodep Plus, Rexipra, Escizen, Estomine, Esto, S Vocita, Escigress, Firsito, Dexanil, Geripram, Xefexor, Prasilife, Esciptal, Prasilex, Galop, Topdep S, Ecitel, Delicita, Escitapax, Citapad, Anxiset E, Talo S, Esredep, Escitalent, Esdep, S Citadelm, Cita S, Esfany, Escance, Esopram, Estomine Zee, Dexanil Plus, Ava, Citalop S, Esci, Strutee-Pam, Esipram, Dipwell, Estadep, Esetalo, Esjoy, Depranex, Xitpram, Halotil, Escitus, Esnax, Taloz, Escada, Enzycare, Yellowpam, Escetalax, Maxcito, Questa, Stalopress\nMedicines with composition Paroxetine: Parox, Pirotin, Paropex, Depaxil, Patriot, Xet, Parocen, Paroxee, Pxt, Praxet, Parot, Paxidep, Pxn, Emoxet, Paxiford, XL Paro, Paradise, Pexep, Petin, Patroxta, Mopart, Paxit, Panex, Praxo, Paronet, Paroxet, Xepar, Paroxta, Raxit, Oxitine PR, Paxgem, Stamna P, Parotin, Pari, Depxet, Paxi, Pexpen, Paxotus, Onbixin, Xodep, Paxonil, Fordep, Paxquit, Senoxetin, Parona\nMedicines with composition Citalopram: Vocita, Olarc, Citopam, Cetadep, Topdep, Celica, Citalomine, Citalent, Citola, Cytop, Madam, Citalin, Citara, Lopram, Ciprexa, Citox, C Pram, Celepra, Citalopam\nMedicines with composition Sertraline: Zosert, Setalin, Tralin, Daxid, Lincer, S R T, Serzest, Senta, Serozac, Lindep, Sertacad, Nash, Sertee, Seroflex, Zotral, Sertagress, Serlin, Sertin, Sertop, Zentalin, Serbest, Seronorm, Sertaj, Zanserta, Steel, Zert OD, Serta, Stalin, Sertil, Serenata, Sen, Serne, Serene, Sertima, Sate, Visert, Inosert, Serpan, Sertrax, Sedeptine, Eccite, Retalin, Episod, Actiser, Obzin, Setra, Mentolift, Santa, Sertakem, Sertawin, Setral, Serotra, Xsert, Normaset, Sentral, Misol, Serdep, Serena, Assert, Osmoset, Serline, Ser, Sermind\nMedicines with composition Dapoxetine: Actinex, DA Sutra, Stay Tal, Dapoxy, Dapnex, Duraject, Appynite, Ristor, Dejac, Xydap, Dapox, Ejalong, Prilyxet, Duralast, Justinex, Kutub, Asenven, Ultimale, Dapolast, Sensape, Viglast, EJ, Sustinex, Duragain\nMedicines with composition Fluvoxamine: Voxinix, Uvox, Voxidep, Voxazen, Voxamin, Psyvoxin, Fluxam, Sorest, Fluvoxin, Fluvo, Frext, Fluvator, Fluvoxine, Ocivox, Arkavoxin, Revilife, Fmine, Fluvaris, Voximin, Flumin, Vanvox, Obsace\nOne of the following vendor pharmacies will deliver Ominil Injection: JSL, GHR, OWP, JVO, JVN, DHR, HEX, DPL, VRS, NVL, KHC, HSP, PMA, EQN, BLE, AMR, DYG, BMJ, NSL, WSI, BSN, SHC, TOM, MAX, RPI, EMB, SWT, HZF, EAN, RHW, OHM, BHM, ARD, SVS, SAF, BAL, ATP, AYU, UBL, AAR, NVY, AYM, PEN, DLP, RSS, EVP, ESY, RWP, RSA, PTI, MBP, SPD, KIS, BHA, RHL, ZVP, GNC, 4IT, SWP, AWS, NNH, VJS, PWN, VDH, JHC, MSD, SGC, BLP, PLT, DVH, AVS, HBV, SJP, MMS, LCC, DAO, TSS, TFP, GTK, BHP, HAT, DZY, SHD, CHP, HIP, GBL, MPC, SWA, ENP, HGI, PPR, STA, MAP, HPC, GPT, ABP, RJH, HNP, AGT, BHS, RPP, TRP, SVH, PHN, MAM, SRG, KNH, BNA, ANP, SSA, DFP, SLN, VNS, MLC, NNP, JBT, FGH, IPL, LMS, SDM, OIP, NXG, PRT, DPP, KHH, TNH, ANT, HMP, QTM, THP, WHL, HTS, UMP, KAL, OLT, SBL, PVP, ADT, BBN, NTB, GTC, NDP, PRN, KMC, MKT, IAD, RDH, MDO, QLF, GPP, CLT, MAK, SBA, JIV, UHP, NAS, EPS, SGH, ZEL, ATL, RDR, 9T9, AAY, DHP, DGN, USF, RUS, 9MM, BDN, MDH, LHA, ZPR, BIO, BTM, SMN, SYN, RKS, BAP, PNT, BGS, BLS, SCH, BBS', 'Tramadol Use and Abuse\nTramadol Use and Abuse\nWhat is Tramadol?\nTramadol is an opioid pain reliever (OPR) that belongs to the same family as fentanyl and oxycodone. Tramadol, however, ranks far lower in strength than fentanyl or oxycodone. Apart from reducing pain, it is also known as a mood enhancer. This medication works best in cases where the patient is suffering from moderate to severe pain. It is also considered for use in people who are recuperating after surgery.\nTramadol is categorized as a Schedule IV drug because of a lower threat perception for inducing abuse in users. People commonly underestimate its addictive properties because it is weaker than other opioid medications. Since it is an opioid, however, the danger of addiction is still present and looms large on regular users of the drug.\nIs Tramadol Dangerous?\nTramadol usage is always accompanied by the possible threat of adverse reactions, even when used with caution and under medical supervision. While typical reactions of its usage range from nausea to dizziness, its abuse can lead to more intense reactions. The medications usage, in combination with other habit-forming substances, can sometimes lead to fatal consequences.\nPeople suffering from addiction or misusing the medication usually procure tramadol illegally from the unregulated markets and street vendors. Some common street names of the drug include Chill Pills, Trammies, and Ultras (a take on the generic brand).\nSince Tramadol is among the least potent of opioid painkillers, there is a common misconception that it does not lead to addiction. It is precisely this misinformation that has led many people on the path to addiction and substance abuse.\nIf a person is using this drug without obtaining a prescription, this is considered tramadol abuse. Similarly, those who are steadily increasing their dosage or continue to use it even after they are no longer prescribed it are also signs of a substance addiction. The same notion applies when using it in combination with other habit-forming drugs or substances.\nThat’s why identifying the signs of abuse at the earliest stage is essential to prevent the onset of addiction.\nThe Scope of Tramadol Abuse in America\nIn 2017, 47,600 people in the United States died from drug overdoses involving opioids.1\nIn 2017, 43,036 people in the United States died from unintentional drug overdoses involving opioids.2\nA total of 2,586 people died of drug overdoses of undetermined intent involving opioids.3\nIn 2016, an estimated 91,840 hospitalizations occurred for opioid-related poisonings in the United States.4\nPrescriptions for Tramadol increased 88 percent from 23.3 million in 2008 to 43.8 million in 2013.5\nIn 2011, Tramadol was ranked ninth in the list of narcotic pain relievers secured in law enforcement operations and analyzed by federal, state, and local forensic laboratories.6\nMore than 399,000 people died from overdoses involving any opioid, including prescription and illicit opioids from 1999-2017.7\nUnfortunately, and ironically, many people who faithfully follow their physicians’ prescriptions are at risk of suffering addiction.\nFrequent and prolonged usage of this drug has been known to lead to drug tolerance in many people. As a result, people must take larger and larger dosages to experience the drug’s effects. In addition to developing tolerance, Tramadol users can expect to suffer from withdrawal symptoms when use suddenly stops. Typically, withdrawal is known to cause symptoms such as irritability, depression, and flu-like symptoms.\nIf a person exhibits tolerance to the drug and also suffers from withdrawal symptoms when they stop using, they are most likely struggling with Tramadol dependence and addiction. Some of the telltale signs of substance addiction are frequent cravings, uninhibited use, and relationship problems that stem from drug use.\nDoctors usually prescribe Tramadol because of its relatively lower potential to cause addiction as compared to many of the other opioid painkillers. This point is also underlined by the fact that it is classified as a schedule IV substance, while the majority of opioid painkillers are listed under schedule II of the Controlled Substances Act.\nMore often than not, the feeling of calmness and euphoria that this drug generates in users is the main reason for its abuse. Those who abuse it usually do so in an attempt to feel unnaturally relaxed and carefree. People suffering from severe pain are also likely to take higher doses of the drug, thus exposing them to greater threats of dangerous side effects that could include seizures and respiratory depression.\nTramadol is a central nervous system (CNS) depressant and thus tends to effect lung and heart function negatively. Those who take very large doses of this drug (much higher than what would be prescribed) might stop breathing altogether and may experience a fatal overdose.\nSide Effects of Tramadol Abuse\nSome of the typical signs and side effects of Tramadol abuse are:\nVomiting coupled with nausea\nPinpoint pupils (pupils are very small)\nChanges in appetite\nTramadol abuse can potentially result in negative reactions of extreme nature, such as seizures. Seizures usually occur when larger dosages are consumed (averaging 400 mg or more per day), over extended time frames. Seizures also manifest more often when this medication and antidepressants are taken together.\nOverdose and Withdrawal\nSome of the symptoms associated with Tramadol overdose are:\nAbnormally low blood pressure\nSlow heart rate\nSweaty or clammy skin\nTramadol Withdrawal Symptoms\nOne of the problems linked to extended usage or high doses of Tramadol use is the possibility of suffering from withdrawal symptoms. When a person with tramadol dependence stops taking it, the body struggles to perform normally. As a result, the very effects that the drug was prescribed to heal might reappear.\nPsychological Symptoms of Tramadol Withdrawal include:\nPhysical symptoms of Tramadol Withdrawal include:\nEarly and Late Withdrawal Symptoms\nOpioid withdrawal is characterized by two main phases: early and late withdrawal. The early withdrawal phase starts when the drug leaves the bloodstream while the late withdrawal phase happens sometime later. Depending upon factors such as intensity and duration of addiction or substance abuse, opioid withdrawal symptoms vary according to the phase in which the patient is currently passing through.\nEarly Opioid Withdrawal Symptoms Include:\nLate Opioid Withdrawal Symptoms Include:\nThe onset of opioid withdrawal symptoms typically begins within about 12 hours of taking the last dose. According to figures released by the Drug Enforcement Administration (DEA), approximately 90% of people experience traditional withdrawal symptoms when stopping use. The remaining 10% might experience symptoms such as severe confusion, extreme paranoia, panic attacks, anxiety, hallucinations, and even tingling or numbness in their extremities.\nWhen people experience intense drug cravings, it is usually an indication that a detox or rehabilitation center would be beneficial as a first point of treatment. Tramadol detox gives the body a chance to regain its original healthy condition without suffering the discomforts associated with the withdrawal symptoms. Rehabilitation equips people with the strategies required to tackle chemical cravings and triggers that encourage the use of drugs.\nWhen administered under medical supervision in a safe and conducive environment, there are significantly higher chances of the patient successfully completing addiction therapy than without formal treatment.\n- Scholl L, Seth P, Kariisa M, Wilson N, Baldwin G. Drug and Opioid-Involved Overdose Deaths – United States, 2013-2017. WR Morb Mortal Wkly Rep. ePub: 21 December 2018']	['<urn:uuid:dacefdd2-0f3d-463f-9051-6982918c9f67>', '<urn:uuid:bbefd1de-f178-4e22-95cd-b00fd95e45a0>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T19:47:23.148022	10	56	2668
30	What are the benefits of using email automation for organizational purposes, and how does the LargeMail feature handle file size limitations in business communications?	Email automation helps streamline tasks by setting up automatic sorting rules, scheduling messages, and creating templates for regular communications like acknowledgments and follow-ups. For handling large files, LargeMail allows secure forwarding of attachments up to 1GB directly from email programs, compared to standard 10-25MB limits. The feature automatically activates for large attachments and includes encrypted transmission and proof of download, with files being automatically deleted after 14 days from the server.	"[""How to Keep Your Professional Business Emails OrganizedCreating and managing a continuous flow of emails throughout the workday can quickly become complicated. To keep your professional work emails organized, develop a filing system and naming system, use sorting features, set reminders, and become knowledgeable about email folders. Use this article to learn how to keep a structured inbox and help you optimize communication, increase productivity, and strengthen customer service.\nCreate a Filing SystemCreating a filing system is the key to keeping your professional business emails organized. It is important to regularly reflect on your filing system, making any necessary changes to keep it up-to-date and effective.\nFirst, make a directory tree structure by creating folders for separating emails between projects or clients. Use Subfolders to create more specific categories, such as “action items”, “urgent”, and “archived”. Additional folders could be added for particular topics, e.g. “Invoices”, “Media”, etc.\nLabeling emails is another important step for organizing your inbox. Common labels to start with include “To-Do”, “Follow-Up”, and “Read & Respond”. To ensure you’re able to determine whether an email is personal or professional, create unique labels like “Work” and “Personal”.\nLastly, make sure to delete unnecessary emails in order to keep your inbox uncluttered. This will help be sure your filing system is actually useful and isn’t bogged down with unnecessary emails.\nВам нужен сайт? Хотите создать сайт, но не знаете с чего начать? Наш конструктор сайтов - идеальное решение. Простая в использовании и с возможностью настройки в соответствии с потребностями вашего бизнеса, вы можете быстро получить профессиональный веб-сайт.\nCreate a Naming ConventionCreating a naming convention for your professional business emails is an essential part of keeping them organized. It allows for easy searching and retrieval of emails, without having to guess what the subject line might be.\nWhen creating a naming convention, the key is to keep it simple and consistent. For example, you may use a combination of the recipient name, the abbreviation for the project you are working on, and the date the email was sent. This will help ensure that all emails related to the same project can easily be found.\nYou may also want to include additional identifiers, such as the email type (e.g. proposal/invoice/report) or the project phase (e.g. planning/development/testing). These identifiers can help you quickly and easily find emails related to specific tasks.\nBy taking the time to create a naming convention and consistently applying it to all emails, you can help ensure your professional business emails are organized and easy to find.\nUse FiltersUsing filters is an effective way to keep your professional business emails organized. Filters allow you to automatically sort your emails into different folders or labels, so you can easily prioritize and track the emails you receive. You can apply filters to sort emails by sender, subject, keywords, or any other criteria. This allows you to route messages directly to the right folder, without having to read them first. You can also set up automatic responses or notifications for any emails that match the criteria. With a good set of filters, your emails can be easily organized and remain neat and orderly.\nSet RemindersOne way to make sure your emails don't get lost in the shuffle is to set reminders. This can be especially helpful when following up on someone or checking in on a project. You can set reminders in your calendar to alert you that an email needs to be sent or checked upon. This ensures nothing slips through the cracks and you don't prolong replies to colleagues or clients.\nReminders can also help with hitting deadlines or staying on top of emails that need extra attention. You can even set daily or weekly reminders to check email inboxes or review client requests or emails. Reminders keep your professional emails organized and on schedule.\nBecome Familiar with Email FoldersBeing familiar with email folders is a key to keeping your professional business emails organized. Email folders are useful for categorizing emails into logical groupings for easier access and management.\nYou can create subfolders for each type of email that requires more organization, such as client emails, project emails, and vendor emails. Creating labels and color coding them according to the contents of folders can make it even easier to locate relevant emails.\nBy utilizing email folders, you can also efficiently manage incoming emails by immediately filing them away into the appropriate folder. Doing this helps to reduce the chances of important emails getting lost in the main inbox and helps to ensure that emails are responded to in a timely manner.\nUsing folders to organize emails can make it much easier to access emails that are important and can help alleviate stress while managing multiple emails and tasks simultaneously. A well-organized email inbox can help make the work day go much smoother.\nDevelop the Discipline to Follow ProcessesOrganizing your professional business emails is key to success. Developing the discipline to follow processes is essential to ensure the efficiency, accuracy and timeliness of your communication. First, decide which organizational system works best for your needs and goals. This could include creating a color-coded system, using folders or maintaining a consistent filing system. Additionally, create templates for frequently sent emails to save time. Ensuring that processes are followed in every email from greetings to sign-offs, can help to streamline communication. Finally, make sure to take the time to reply promptly, as timely responses contribute significantly to better communication. Taking these steps will allow you to keep your professional business emails organized and effective.\nUtilizing Automation is one of the best ways to keep your professional business emails organized. Automation can help streamline mundane tasks, such as sorting emails into different folders, by allowing you to set up automatic sorting rules.\nAutomation can also help you save time by scheduling messages to be sent at specific times as well as creating templates for regularly used email formats, such as acknowledgments or follow-up emails.\nFurthermore, automation can help you keep track of conversations and follow-ups, allowing you to quickly assess the status of any email exchanges. Utilizing Automation is an invaluable tool to help maximize the efficiency of your email organization process."", 'Sending Encrypted Emails, electronic contracts and large files\nregistered with one click.\nEmail transmission without uncertainties\nThe delivery of important correspondence by email can have serious consequences in the event of a dispute. After all, how can it be proven that the submitted email has really reached the recipient? The delivery of important documents is today therefore still often done with the postal registered mail.\nRMail offers with its registered email the advantages of registered letters for email. This way, the dispatch, the content and the delivery of a message including attachments can be proven at any time. Furthermore, with RMail each email can be sent encrypted with a single click.\n- Registered E-Mail\n- Email Encryption\n- Email tracking & record\n- Sending large files via email (LargeMail)\n- Side notes for selected recipients (SideNote)\n- Advanced email tools\nTracking and proving email delivery\nThe registered email with Registered Receipt\nA regular email can be manipulated very easily. Neither content nor sending times nor the reception of the message can be definitely proven in the case of a dispute. Important documents should therefore be sent by registered mail in order to prevent uncertainty in advance.\nThe Registered Receipt: Authenticating and restoring the original email\nThe delivery of each registered email is acknowledged with a Registered Receipt. This proof contains all important information such as the complete message, attachments, transmission and reception times and the used encryption methods. With this proof, you can also allow third parties to verify the authenticity of the submitted email message in the event of a dispute. In addition, the original contents of the message can be restored at any time.\nThe attached file ""HTMLReceipt"" contains the actual delivery receipt. This file contains the original email including all attachments in encrypted and digitally signed form, the complete delivery history (audit trail), digital fingerprints and the timestamps of sending and reception. All information is digitally signed and thus protected against manipulation.\nIn a dispute in which sent emails play a role, the original files can be restored. The RMail user can forward his Registered Receipt to the other party at any time for the verification of the originality of the email.\nThe Registered Receipt includes the send status and the delivery times, if available.\nRMail logs the track & trace information for each email. This includes the server logs as well as additional information of data transfer. This will track the entire route the email message took.\nEmail encyption with a single click\nAn unencrypted email is as secure as a postcard. The message can easily be read and manipulated at any time. Even if you do not send sensitive information or documents - personal messages, pictures or files should still be protected from unauthorized access! With RMail you can easily send your messages encrypted with a single click. If required, the email encryption can be set as default. And best of all, your recipients can easily read your encrypted emails without registration, key management or web accounts!\nAutomatic selection of the encryption method\nThe Auto-TLS feature of RMail detects automatically if there is a secure connection between sender and receiver. If a secure TLS connection is possible, the email is sent over it. And if the recipient\'s email server does not support TLS, the message is converted to an encrypted PDF (SecurePDF, 256-bit AES encrypted PDF wrapper) that can be securely delivered to the recipient via a normal connection.\nEmail encryption without intervention of the recipient and with encrypted reply\nThe RMail encryption solution with SecureReply enables recipients to receive encrypted messages without any previous setup effort and also to answer in encrypted form. If no TLS connection is possible, each encrypted email contains a link to securely reply directly from the browser.\nSending large attachments via email with LargeMail file transfer\nSending large files by email is often difficult. Either the files must be compressed before sending - or a third-party provider is used, whereby you can never be sure how and for what purposes your documents are stored and searched. In contrast, RMail allows the direct and secure forwarding of large files up to 1GB directly from your email program and deletes them automatically after 14 days (adjustable) from the RMail server.\nLarge attachments up to 1 GB\nMost email providers only allow the transmission up to a certain file size (standard 10-25 MB). With LargeMail, you can exchange sensitive information and attachments up to a size of 1 GB directly via email. Including encrypted transmission and proof of download.\nThe recipient can download the attachments via a link known only to him (on request with password) within 14 days. After this time window, the files are completely and definitely removed from the e-communications system.\nAutomatic activation for large attachments\nRMail can be configured to automatically activate the LargeMail option for file attachments above a certain size.\nSide notes for maximum comfort and efficiency in communication\nHow often are emails forwarded to an additional recipient in order to provide him with additional information? The answer is: too often. Because this method has several disadvantages: such a conversation is arranged chaotically in the email program, the sender has additional work steps for sending and the recipient of the additional information often has difficulty in recognizing where the note is over and where the actual original message begins. RMail solves this problem with its practical side note feature (SideNote®).\nAdding private notes\nThe SideNote feature allows you to attach a private annotation only to specific recipients. This appears like a yellow sticky note attached to the beginning of the message and is thus clearly visible. Side notes are visible only for CC or BCC recipients. The recipients of the original message do not receive this note.\nFurther advanced functions for email and file attachments\nRMail offers other important features such as the transmission of contracts for Electronic Signature, the automatic conversion of attached documents in PDF files and metadata removal.\nFurther information on the mentioned functions can be found in the RMail product overview.\n- Sending electronic contracts for signing\n- Conversion of attachments into PDF\n- Removal of metadata\n- Conversion of attachments into ZIP files\n- Anti-phishing warnings']"	['<urn:uuid:40adff27-3dcf-4d99-a188-ae00d25673bf>', '<urn:uuid:6a85b773-bd0c-4764-b011-5a2b5ee7ac56>']	factoid	direct	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:47:23.148022	24	71	2065
31	How does checking product quality at the end of production compare to checking it during the manufacturing process in terms of cost and effectiveness?	Checking quality at the end of production (final inspection) is more costly and less effective compared to checking during the manufacturing process (in-process inspection). When defects are found during final inspection, the product may need to be reworked or scrapped, which is expensive since all value-added steps have already been invested in the product. Additionally, the process causing the defect may still be creating defective products. In contrast, in-process inspection involves checking things like machine settings, machine output, and incoming/outgoing materials throughout production, allowing issues to be caught earlier. Quality management should focus on both preventing defects and avoiding them in the first place, rather than just identifying defective products at the end.	['Hello readers and welcome back to our project management career series. This series is geared towards helping you prepare for a change of job or an interview either as a project manager or as a project team member.\nIn our project management career series, we already explored the following areas:\n- Project management career series – Introduction\n- Project management career series – Scope management\n- Project management career series – Time management\nToday, we will be exploring the project management interview questions that could arise from the project quality management knowledge area. As usual, this article will also be providing practical ways to answer these questions.\nProject Quality Management\nProject quality management is a continuous process that runs across the project. It aims to ensure that the products or services of a project organization are consistent, effective and efficient with respect to the objective for which they were set to achieve.\nQuality management is all about meeting and exceeding stakeholders’ expectations while conforming to project requirements.\nWe often look at quality from a myopic view of the product and services rendered alone. Quality management is more than correcting and maintaining mistakes, it is also about preventing and avoiding. Quality management thus focuses on the process involved in achieving the product/services as well as the product of the project itself.\nWhile it is unlikely for you to be directly asked to define quality in an interview, it is very important to know what it is. According to the International Organization for Standardization, quality is defined as “the totality of characteristics of an entity that bear on it is ability to satisfy stated or implied needs” where the stated and implied needs are the objective the project must achieve.\nIt is also defined as “conformance to requirement” and “fit for use,” which means a product or service can be said to be quality when it has met the product objectives and can also serve the function for which it was created.\nBelow are general project quality management related questions that you are likely to come across during your interview as a project manager.\nQ. Differentiate between quality and grade.\nAnswer – This is a very common interview question and you should expect it especially when applying for the role of a quality control manager.\n- Quality – This is the degree to which a product or result meets the requirements of the customer or end user. It is also defined as “conformance for requirement” and “fit for use.”\n- Grade – This is “a category assigned to products that have the same functional use but different technical characteristics.” Grade has to deal with features of a product.\nThe use of examples is also a great way of answering this question. For example, a Nokia 3310 mobile phone (calling and texting alone) might be of low grade but high quality, while an iPhone can be of high grade and high quality.\nWhile it is perfectly acceptable for a product to be low grade and high quality, low quality on any product grade should never be tolerated.\nQ. Based on the agreement at the internal stakeholders meeting, you were asked to purchase 50 pieces of Blackberry mobile phones for your team members. At the store, the sales personnel advised you to purchase iPhones instead because they had better quality. Moreover, the price is within your budget. What would you do?\nAnswer – You should buy the Blackberry smartphones. Quality is all about meeting stakeholders’ and end users’ requirements.\nNote – The sales person is actually confusing quality with grade. This has been explained above. Do not be tempted to call the purchase department to advice them on getting an iPhone. This is a wrong call.\nQ. Explain the cost of quality.\nAnswer – The cost of quality is the cost associated with not doing things right the first time, i.e. NOT creating a quality product or service. Cost of quality has two main components, which are:\n- Cost of conformance – This is the total cost associated with ensuring a product is of good quality. This includes cost of training, process improvement, quality assurance, quality control and ensuring standards, among others.\n- Cost of non-conformance – This is also referred to as the cost of failure. It is the cost associated with not meeting the quality requirement of a project. This cost can also be divided into two, which are the internal and external cost of failure.\n- Internal cost of failure: Examples are scrap, rework, re-inspection, and retesting.\n- External cost of failure: Examples are warranty claims, customer returns, and product recall.\nQ. The Ishikawa diagram and Pareto’s diagram are some of the most important tools in project quality management. How do they work?\n- The Ishikawa diagram – This is also known as the cause and effect or fish bone diagram. It is used in analyzing a problem statement in order to discover the root cause of a problem. It keeps asking the question “why” until an actionable cause can be attached to the problem.\n- Pareto’s Diagram – This is in line with Pareto’s Principle or 80–20 rule. It is the use of charts (bar chart or histogram) to recognize the few (20%) sources that are responsible for causing most of the problems (80%) in the project or organization.\nQ. Explain precision and accuracy in relation to quality management.\nAnswer – Although these terms are often used interchangeably during quality measurement, they are far from being synonymous.\n- Precision – Precision is the measure of exactness. This means the values of repeated measurement are consistent. This does not necessarily mean they are close to the target measurement.\n- Accuracy – This is the measure of correctness. It refers to how close the measured values are to the targeted value.\nThe diagram below can help you understand precision and accuracy better.\nNote – Precise measurements are not necessarily accurate, while accurate measurements are not necessarily precise. It is your work as a project manager to determine the level of precision and accuracy for every project.\nQ. What is benchmarking and why is it used?\nAnswer – Benchmarking is the process of evaluating one’s business processes and performances against recognized industry leaders and best practices. It is used by organizations that need to keep ahead of competition and best industry practices. Since the world is now a global village, it allows a project organization the ability to judge themselves against competitors using a standard scale.\nQ. What is the difference between quality control and quality assurance?\nAnswer – These two terminologies are often mistaken for each other:\n- Quality Assurance – This is focused on the process that produces the product. It is the set of activities that ensures the quality of the process that produces the product with the aim of preventing defects.\n- Quality control – This is a set of activities that ensures that the product meets the specified quality. This focuses on the finished product and identifies defects before the product is released.\nQ. Why is process analysis important in quality management?\nAnswer – Quality management is more than identifying defective products, it is also about preventing and avoiding defects in the first place, hence a reason to focus on the process that produces the product. Process analysis a tool that helps in making a business process more efficient, thus reducing the amount of mistakes in the process.\nQ. Why do you need to be very proactive as a project manager?\nAnswer – This is a very broad question and requires tactics in answering. A project manager should be proactive so that he/she is not caught unaware by the output of the project he/she is managing. A good project manager should always focus on the long-term as opposed to being reactive to the moment because they understand that their actions today can influence the output tomorrow.\nQ. You have just been appointed as the project manager of an organization and you are told the quality of the project is below standard. What is the first thing you would do as a new project manager?\nAnswer – This is a tricky question as there are literarily a million and one things that could be done. The question is specific on the first thing you would do.\nAs a good project manager, before making any change or recommendation, the first thing is to identify the objectives of the project, the quality benchmark and the quality metrics used for the project. All these information are contained in the project management plan (Quality management is a part of the project management plan). Once the information is derived, it is measured vis-à-vis the quality of the product; the project manager can then decide the next step of action.\nThe article above has just identified a list of probable interview questions that you could face regarding quality management when you attend a project management interview. As usual, we would like to highlight that these questions have been framed in a generic form so that it can be adapted to whatever industry you work in. Expect to see industry-related, case-related and abstract questions during an interview. The idea is to analyze and understand the questions before answering.\nAlso, preparing for interviews is serious business and we should always handle it as such. Never go for an interview unprepared. Sometimes while preparing, we often neglect little details such as researching on the organization we applied to, what industry they belong to and what characteristics they look for in their employees. Being technically sound is definitely important but passing an interview is more than just acing its technical aspect.\nOnce again, thank you for reading and do not forget to share and drop a comment below.', 'What is Quality Control?\nWhat is quality control? Quality control are the systems in your\ncompany that detects defects. Quality control prevents customers (internal and external) from receiving defective products. This\nincludes inspection points for receiving inspection, inprocess inspection\nand final inspection. When establishing these systems you need to consider these items\n- The procedure for inspection\n- The sampling plan for inspection\n- The quality record for inspection\n- The accept / reject criteria or specification.\n- The plan for handling and identifying the defective material\n- The review and corrective action of the defective material (MRB)\nReceiving inspection are the controls you use to prevent defective materials from entering your production lines. Your company decides which materials are critical to your process. Critical materials directly affect product quality. These materials must be controlled.\nConsider these questions when creating your receiving inspection procedure.\n- How are the materials inspected?\n- What is the inspection procedure?\n- What is the sampling plan?\n- Where is the inspection documented?\n- How is the accepted material distinguished differently from the rejected material or unqualified material?\n- What happens to rejected material?\n- What is the process of notifying the supplier of rejected material?\n- How do you prevent good and rejected material from being mixed?\n- What is the staging process for unqualified material?\nYou might have inprocess inspection in various departments in your company\nas production is flowing through those departments. This could include\n- Inspecting machine settings,\n- Inspecting machine output\n- Inspecting the incoming material\n- Inspecting the outgoing material\n- Statistical Process Control\nWhen establishing an inprocess inspection stage consider these items\n- Do you need the inspection to ensure the quality of the part?\n- Do you have a operating procedure for the inspection tool?\n- Do you have specifications for the inspection?\n- Have you consider SPC for the inspection?\n- Have you determined the sampling plan?\n- Is there a quality record for the inspection?\n- Is someone reviewing the quality record?\n- Is there a Material Review Board process if issues are found?\n- Who is notified when issues are found?\n- Is production stopped when defects are found?\nFinal inspection should occur on the product prior to shipping to the customer or prior to storing in inventory. When implementing final inspection activities, review the above inprocess inspection information.\nFinal inspection is more dependent on customer requirements. The customer may have a specification or a drawing. Perhaps you are selling to a catalog or web page. Final inspection assures the customer is receiving the items they order.\nFinal inspection occurs on the product. If defects are found, then the product may need to be reworked or scrapped. Finding defects at this time can cost your company serious money because the process that caused the defect may still be making the same defect on fresh product. In addition all value added steps have been placed in the product. All money spent on creating that product will be wasted if the product is scrapped.\nAs a preventive measure, final inspection should review the inprocess inspection documentation to assure that the data is in order.\nDocumentation for final inspection should be formal as the customer may ask for the data. Design The quality record so it meets your customer requirements and formats. If the customer is asking for the data, be sure to get their acceptance to the format prior to sending the initial data to the customer.\nWhat is Quality Control inspection types?\nMechanical - These are dimensional measurements of the item\nPerformance - This is an inspection for the function(s) of the product.\nVisual - Looking for certain undesirable characteristics. These characteristics need to be defined\nWhat is quality control definition?\nThe operational techniques and\nindividual activities that focus on controlling or regulating processes\nand materials to fulfill requirements for quality. The focus is on\npreventing defective products or services from being passed on.\nAfter reviewing What is Quality Control, see here for the difference between Quality Assurance and Quality Control.\nQuality Assurance Solutions']	['<urn:uuid:b8a27439-e2e1-4b8d-9d1a-5a31fe549409>', '<urn:uuid:e06eed6f-c212-4c5b-8212-79b77d3ed478>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T19:47:23.148022	24	113	2308
32	animals living chernobyl reserve today	The Chernobyl Radiation-Ecological Biosphere Reserve hosts over 300 species of vertebrates and 1400 plant species. Common animals include wolves, lynx, bears, elk, deer, and wild boars. The area has Ukraine's highest population of mooses and wolves. Despite initial radiation damage, nature has recovered remarkably due to the absence of human activity.	['The Chernobyl disaster happened 34 years ago. Since then, an area of 30 km around the Chernobyl nuclear power plant has been an exclusion zone. More than 116,000 residents of this territory were evacuated after the accident. Until now, you can get here only by special passes. How did the Chernobyl zone become the largest nature reserve in Ukraine?\nWill people ever be able to return here? What is happening at Chernobyl and around? How could a nuclear disaster take place to become a haven for the development of alternative energy? And why has the exclusion zone become a magnet for foreign tourists? BBC News Ukraine went to the exclusion zone to learn about the future of this territory.\nThe 30-kilometer exclusion zone does not have the shape of a circle, as it is sometimes mistakenly believed. Radiation pollution was stained, and several traces were formed due to wind and weather conditions. Thus, the exclusion zone stretched for more than 60 km from east to west, and from north to south – for 30 km.\nToday, there are neat streets in the center of Chernobyl where people walk and communal services rush about. However, these are mainly only employees of the zone and service enterprises: the staff of the State Agency for the Management of the Exclusion Zone and its divisions, the Eco-center chipboard, which monitors the radioactive background, scientific and research institutes. In total, there are up to 10,000 employees in the exclusion zone. But so far there is no question of returning people to this territory to live.\nOf course, there are settlers – people who, despite the ban, independently returned to the exclusion zone after the accident to their homes. Now, there are about 130 people left, the majority lives in the city of Chernobyl, 18 km from the Chernobyl nuclear power plant, where the infrastructure is most developed.\nPeople do not belong here\nCompletely unsuitable for people to live in is a “dirty” 10-kilometer zone around the Chernobyl nuclear power plant, which occupies about a third of the entire territory of the Chernobyl exclusion zone. This territory is contaminated with transuranic elements: uranium, plutonium, americium. They have a very long half-life: this is 24 thousand years for plutonium 239, and 6500 years – for plutonium 240.\nTo date, a bill has been registered in the Ukrainian parliament, the purpose of which is to declare this territory unsuitable for human habitation for life. In the rest of the territory, which is about 70% of the exclusion zone, the level of pollution is such that even now it is possible to allow cultivation of agricultural land there. But the entire infrastructure, in particular social, is completely destroyed there, so there is no reason to recommend people to return to these territories.\nAs you know, there are many other very good places to stay in Ukraine. Therefore, it is more rational to turn this area into a radioecological reserve, and to establish a special industrial zone on the territory of very high pollution. Today it is important to help the territories around the exclusion zone.\nNot all residents agreed to leave after the accident, so people still live in separate places. These territories are still considered “territories of compulsory resettlement”, therefore, a number of restrictions are imposed, including on their economic use.\nAn industrial site\nDespite the fact that the Chernobyl NPP has not been operating since 2000, it still employs more than two thousand people. However, the staff is gradually being transferred to work in the city of Slavutich, which was built after the accident to replace Pripyat.\nThe destroyed fourth power unit was covered with a new safe confinement two years ago. To date, measurements of the radioactive background show that this background has decreased significantly after the approaching arch.\nBut the arch is not only a covering, it is a very complex technological object, it has many functions for monitoring, support, work inside the Shelter object. A new safe confinement isolated it from the environment.\nBut there is an old building inside it and now the task is to disassemble unstable internal structures in the heart of the new confinement. It is designed for a hundred years. During this time, mankind must figure out how to disassemble and bury 200 tons of radioactive waste remaining from the molten reactor. While there are no such technologies, according to the initial plan, this will be done by robots.\nA storage facility for spent nuclear fuel from the first, second and third power units of the Chernobyl nuclear power plant was also built in the ten-kilometer zone. It was built by the American company Holtec with funds from international donors. Fuel will be transferred here from the old storage facility on the territory of the Chernobyl NPP within ten years, because the validity period for which it was designed has already expired. Testing of the new storage facility is ongoing.\nThe Vector complex is also under construction – it`s a special plant for the management of radioactive waste and materials, where spent nuclear fuel from Ukrainian nuclear power plants is stored.\nGreen energy instead of nuclear one\nAlternative energy sources are still being developed at the site of the largest nuclear disaster in the world. Last year, a solar power plant of the Ukrainian-German project with a capacity of 1 MW was launched on the territory of the Chernobyl nuclear power plant, next to the destroyed reactor. This may seem like a small capacity compared to the Chernobyl nuclear power plant – only one unit of it had a thousand times more power.\nBut the pilot solar station can only be the beginning. According to the plan, the territory of a ten-kilometer zone can be covered by solar power plants. Today, the exclusion zone has every chance of becoming a promising element of the future “green economy of Ukraine”.\nAccording to the latest data, more than 60 applications for land acquisition for the implementation of the “green energy” project from domestic and foreign investors, in particular from Denmark, the USA, China, Germany, France and Belarus, have been received over the past two years.\nIt is offered 25 square kilometers of free land within the exclusion zone for potential investors for the development of “solar” energy. And at the end of last year, a plot was allocated for the construction of the first Ukrainian wind farm, part of which lies in the exclusion zone.\nParadise for nature\nThe Chernobyl Radiation-Ecological Biosphere Reserve began to operate in the exclusion zone in 2017. Its territory occupies two thirds of the exclusion zone. In Belarus, a similar reserve was created two years after the accident, in 1988.\nBut then, the main activity remained in Ukraine: the Chernobyl NPP, electricity production, radioactive waste management. Issues of nature management were considered secondary and were considered by the then leadership on the residual principle.\nToday, the Chernobyl Radiation-Ecological Biosphere Reserve is the largest reserve in Ukraine, with an area of more than 2000 square kilometers. “Simple animals live here, such as wolves, lynx, bears, elk, which are typical symbols of the European forest,” biologists say. In total, the reserve has more than 300 species of vertebrate and more than 1400 plant species.\nRadiation killed almost everything after the accident, but soon nature recovered – the benefits of the absence of people exceeded the harm from radiation. Today, scientists conduct observations of animals on the territory of the “red forest”. The forest turns green around, birds sing, nothing reminds of the past disaster.\n“Mankind did not limit the development of vegetation, shrubs, herbaceous plants, and the forest was restored. It was very impressive, ” researchers in the exclusion zone say. According to them, deer, wild boars, lynx and brown bears appeared here after the accident. Scientists say that in the exclusion zone there is the highest number of mooses and wolves in Ukraine. It sounds paradoxical, but the accident has become very useful for animals, since it removed people from here. Where there is no man, paradise life for animals and plants sets in, ” biologists say.\nDue to the fact that the Reserve was created, nature itself and its researchers have a unique opportunity to restore and preserve the diversity of the fauna and flora of the exclusion zone.\nTourism: interest is growing\nWe meet about eight buses on the day of the visit to the exclusion zone at the entrance, two of them are especially large. Waiting for their turn to enter, foreign tourists take pictures and examine souvenirs at a local kiosk, from which a loud jazz melody is heard. The kiosk has everything from cards and magnets to gas masks and condoms with the sign of radioactivity.\nTourists from Britain say that they came here because they had heard about Chernobyl since childhood and read a lot about the ghost town of Pripyat.\nToday, several tour operators organize excursions to the Chernobyl zone, and most of them deal exclusively with Chernobyl tours.\nAccording to the Ministry of Ecology of Ukraine, 63 thousand people visited the exclusion zone only in 2018. This is almost eight times more than it was five years ago. Guides and tour guides say that there are about a thousand visitors per day. “There used to be a lot of Poles, now the majority are British,” Vitaliy Poyarkov, who accompanies the groups, says.\nHe says that an information center appeared at the entrance to the zone in the fall of 2018. There is a hotel and hostel for tourists in the zone itself. However, the word “tourists” isn`t liked in the exclusion zone. “We usually use the word visitors. We are not engaged in advertising, but we believe that if people have a certain interest to come and see, then why not give them such an opportunity? ”\nThe city of Chernobyl, the name of which became the goal for visiting Ukraine by many tourists, is now actually the administrative center of the exclusion zone. Despite the lack of population, the infrastructure is maintained here.\nThe city has official buildings, a post office, police, fire services, utilities, dormitories and four shops; there is electricity, gas and water. At first glance, the center of Chernobyl is no different from another small town in Ukraine. But the eye quickly grabs the empty abandoned houses, and it becomes obvious that the inhabitants have long left here, leaving their homes to nature itself.\nThis is what attracts visitors – how nature takes its toll after people left the territory. And, as practice shows, such interest is only growing over the years.']	['<urn:uuid:52daec1e-4a59-4c39-ae3b-48b23a4a63b9>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T19:47:23.148022	5	51	1767
33	mississippi rhythm night club fire details casualties natchez 1940	The Rhythm Night Club fire occurred in Natchez, Mississippi on April 23, 1940. The disaster resulted in 209 African-American party goers losing their lives, and many others were severely injured in the incident.	"['American Disasters - Great Mississippi Disasters\nReviewed Resources for Students and Teachers\nGreat American Disasters State by State\nAlabama - Alaska - Arizona - Arkansas - California - Colorado - Connecticut - Delaware - Florida - Georgia - Hawaii - Idaho - Illinois - Indiana - Iowa - Kansas - Kentucky - Louisiana - Maine - Maryland - Massachusetts - Michigan - Minnesota - Mississippi - Missouri - Montana - Nebraska - Nevada - New Hampshire - New Jersey - New Mexico - New York - North Carolina - North Dakota - Ohio - Oklahoma - Oregon - Pennsylvania - Rhode Island - South Carolina - South Dakota - Tennessee - Texas - Utah - Vermont - Virginia - Washington - West Virginia - Wisconsin - Wyoming\n|These pages do not begin to cover all the disasters that have\ntaken place in every state. There is also controversy over\nwhat constitutes and defines a ""great"" disaster from any other kind.\nAll disasters are ""great"" to those directly involved in them.\nSo these pages are a mix of ""great"" and ""not so great"" disasters. The major disasters are all covered in the larger sites I have linked to as resources and the rest are related items of interest.\nFor example, you might not find a particular major disaster on this list, but it may be included in a site such as gendisasters.com, which is one of the resources found here.\nThis is an excellent place to begin your research. The links found here lead to an ever widening amount of information. Good luck in your efforts.\nGo to Mississippi Social Studies - Mississippi History - Capital City Jackson, Mississippi - Mississippi Tourism - Mississippi Disasters - Mississippi Geography - Mississippi Lesson Plans - Mississippi Colleges and Universities\nBig Bay Dam __ ""...the dam broke sending water downstream destroying 48 homes and businesses while many more were damaged."" an encyclopedic entry. - illustrated - From wikipedia - http://en.wikipedia.org/wiki/Big_Bay_Dam\nFEMA: Mississippi Hurricane Katrina __ General information and news from FEMA - From FEMA - http://www.fema.gov/news/event.fema?id=4807\nGreat Natchez Tornado __ ""The Great Natchez Tornado was a tornado that hit Natchez, Mississippi on May 7, 1840. It is the second deadliest single tornado in United States history..."" A brief encyclopedic article. - From wikipedia - http://en.wikipedia.org/wiki/Great_Natchez_Tornado\nMississippi Earthquake Information __ Overview and history of earthquakes in\nMississippi. - From usgs.gov -\nHurricane Katrina __ ""The hurricane caused severe destruction across the entire Mississippi coast and into Alabama, as far as 100 miles (160 km) from the storm\'s center."" An encyclopedic article. - illustrated - From wikipedia - http://en.wikipedia.org/wiki/Hurricane_Katrina\nMississippi Disasters Floods, Fires, Tornadoes, Mine Explosions ... __ MIssissippi Disasters. Events that touched our ancestor\'s lives: Mississippi floods, fires, tornadoes, hurricanes, storms, mining accidents, explosions, ..."" Many click to read articles and resources. - illustrated - From gendisasters.com - http://www.gendisasters.com/ms/\nMississippi Hurricane, Aug 1860 __ ""Three hurricanes hit the southeastern coast of Louisiana in 1860. August 8-16, the first hurricane of the season, number 1, formed in the Gulf of Mexico west of Tampa, Florida. It headed west and after passing the mouth of the Mississippi River, it made a large U turn, crossed the Louisiana coast near the Lafourche, St. Charles and Plaquemines parishes, turned east along the southern shore of Lake Ponchartrain, crossed Mississippi, Alabama, Florida and Georgia then headed out into the Atlantic Ocean. This was a category 3 hurricane."" Original news articles and links to related material. - From gendisasters.com - http://www.gendisasters.com/data1/la/hurricanes/hurricane-aug1860.htm\nMississippi hurricanes __ An index of wikipedia articles about Mississippi tornadoes. - From wikipedia - http://en.wikipedia.org/wiki/Category:Mississippi_hurricanes\nMississippi tornadoes __ An index of wikipedia articles about Mississippi tornadoes. - From wikipedia - http://en.wikipedia.org/wiki/Category:Mississippi_tornadoes\nMississippi Tornadoes __ ""Mississippi tornadoes from 1950 to 1995, listed by county."" - From tornadoproject.com - http://www.tornadoproject.com/alltorns/mstorn.htm\nRhythm Night Club Fire __ ""The Rhythm Night Club fire took place in Natchez, Mississippi, United States on April 23, 1940 and killed 209 African-American party goers, while severely injuring many others."" an encyclopedic article. - From wikipedia - http://en.wikipedia.org/wiki/Rhythm_Night_Club_Fire\nRhythm Club Fire __ An original news article from 1840. - From natchezsites.com - http://www.natchezsites.com/history/rhythmclub.html\nSuper Outbreak __ ""The Super Outbreak is the largest tornado outbreak on record. From April 3 to April 4, 1974, there were 148 tornadoes confirmed in 13 US states and one Canadian province: Ontario, Illinois, Indiana, Michigan, Ohio, Kentucky, Tennessee, Alabama, Mississippi, Georgia, North Carolina, Virginia, West Virginia and New York."" An encyclopedic article. - illustrated - From wikipedia - http://en.wikipedia.org/wiki/Super_Outbreak\nTupelo, MS Tornado, Apr 1936 __ ""Tupelo’s list of identified dead from last night’s one hundred-mile-an-hour tornado passed the 125 mark today as a fleet of ambulances, trucks and motor cars streamed to hospitals and morgues with the victims."" Original news stories and links to related material. - From Jenni Lanham/gendisasters.com - http://www3.gendisasters.com/mississippi/909/tupelo,-ms-tornado,-apr-1936\nVaughn, MS Casey Jones Train Wreck, Apr 1900 __ ""A disasterous collision occurred about 4 o\'clock this morning at Vaughn\'s Station eleven miles north of Canton. The engineer, CASEY JONES, was instantly killed, and Express Messenger MILLER was hurt internally, but not seriously."" Original news articles and links to related material. - From Stu Beitler/gendisasters.com - http://www3.gendisasters.com/mississippi/1486/vaughn,-ms-casey-jones-train-wreck,-apr-1900']"	['<urn:uuid:7a60e450-afe3-478d-9e8b-cd2bf14a86ab>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T19:47:23.148022	9	33	855
34	how does restorative justice help rebuild countries after human rights violations	Restorative justice plays a crucial role in responding to mass human rights violations and reconstructing justice systems in countries recovering from occupation or authoritarian regimes. It operates at the grassroots, community level to promote healing after violence, addressing the fundamental question of how to stop people from hating each other, and is considered as essential as peace accords and the cessation of armed conflict.	['How is RJ linked to human rights?\nAccording to Tony Ward and Robyn Langlands in their article “Restorative justice and the human rights of offenders: Convergences and divergences”\nRestorative justice has gained significant momentum as a justice reform movement within the past three decades, and it is estimated that up to one hundred countries worldwide utilize restorative justice practices. Although claims about the role of restorative justice in protecting human rights are repeatedly made in the restorative justice literature, they are seldom supported by empirical evidence or a thorough analysis of human rights and their justification. In this paper, we discuss how the assumptions underpinning restorative justice practices impact on offenders’ human rights, and their points of convergence and divergence. We argue that while these assumptions can protect certain offender rights, they may violate others. We finish with some suggestions about how to reconcile the tensions between human rights and restorative justice, focusing in particular on the relationship between community needs and individual well-being.\nAggression and Violent Behavior Volume 13, Issue 5, October 2008\nElizabeth Snyder quotes Joanna Shapland’s article ‘Restorative Justice and Criminal Justice: Just Responses to Crime?’ in A. Bottoms, K. Roach, J. Roberts, M. Schiff and A. von Hirsch (eds), Restorative Justice and Criminal Justice: Competing or Reconcilable Paradigms?, Hart Publishing, Oxford and Portland, Oregon, 195-217.\nShapland asserts that the legitimacy of the state ‘is bound up with its espousal of universalistic values’ and the administration, within formal criminal justice procedures, of these values. Among these universal values is the state dispensation of justice that is truly just, that adheres to basic principles of human rights. Human rights legislation … is a means of safeguarding these values and serves as a bulwark between the ‘lone powerless defendant’ and the potentially ‘coercive state’.\nThis focus on protecting defendants from state corruption or miscarriages of justice is seen in international documents outlining basic principles of human rights. The UN Universal Declaration of Human Rights declares that ‘everyone has the right to life, liberty and security of person; no one shall be subjected to torture or to cruel, inhuman or degrading treatment or punishment; no one shall be subjected to arbitrary arrest, detention or exile’ (UN 1948)\nIn recent years, restorative justice has assumed an increasingly prominent position in discussions of human rights and the rule of law…\nAccording to Paul McCold of the International Institute Women, Restorative Justice, and the Pursuit of Human Rights in the Solomon Islands for Restorative Practices, restorative justice principles ‘have direct implications for how the United Nations organizes efforts to respond in the aftermath of mass violations of human rights and in the reconstruction of justice systems in countries recovering from occupation or authoritarian regimes’. McCold underscores the need for healing in the aftermath of violence and suggests that grassroots, community-level restorative justice is as essential as peace accords and the cessation of armed conflict. In short, restorative justice addresses the basic question: ‘How do people stop hating each other?’ (Porter 2005:2).\n“Waging Peace: Women, Restorative Justice, and the Pursuit of Human Rights in the Solomon Islands” by Elizabeth Snyder']	['<urn:uuid:464712ef-02e9-4d67-88b0-baa7d135bd73>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T19:47:23.148022	11	64	516
35	What different types of creative projects can students make using technology tools, and how do these projects help them learn multiple skills at once?	Students can create various types of creative projects using technology tools that help develop multiple skills simultaneously. Using iMovie, they can produce life stories with childhood pictures, create poems with visual elements, develop tutorials explaining math concepts, or craft personal narratives under 90 seconds. They can also create immersive 360° tours using Tour Creator, build websites with Google Sites to present solutions to problems, and make stop-motion animations using Google Photos. While working on these projects, students learn to combine writing skills, reading skills, speaking skills, technology skills, and organization skills. They also develop collaboration abilities by helping each other complete projects and sharing ideas. These projects become more meaningful when students know they'll be sharing their work with an authentic audience through their digital portfolios.	"['Free Online Resources Engage Elementary Kids (Transcript)\nTeacher: As a K-5 teacher, we understand that it\'s really important to integrate the curriculum as much as possible. So if you\'re doing a lesson in the computer lab, if it\'s combining some of the reading skills, writing skills, language art skills, or their knowledge about a certain subject area, I think that it makes learning a lot more meaningful.\nTeacher: A few years ago, the District had to do a lot of cutbacks on tech spending. And I started to see that there was an advantage of using the cloud-based, or free web apps. And anybody who has a computer or internet, they can access them, not only from school, but at home, so my goal is to get them so excited that they go home and continue on with their learning. For example, they can do their digital art, or digital drawings in programs such as Sumo Paint, or Psycho Paint inside of Google Docs, Google Drawing, PhotoSketch, SketchUp. Those are just a few choices that I give them.\nStudent: This is a list of different applications that you can use to like make music or do art. It\'s just a variety of them that Ms. Dalesio put together.\nTeacher: And then we also take advantage of a lot of the free software that comes built into our Apple computers, so we utilize iLife a lot for our photo and video editing inside of iMovie, or in iPhoto. And we also use things like Garage Band and iTunes for them to do their audio recordings or create music.\nTeacher: You know, you get your ingredients together in the folder, and then when you mix them together, this is what you\'re doing in iMovie. And then when you export it out, it\'s sort of like you\'re baking the cake.\nTeacher: I gave them several choices on the assignment that they had to do. And all of them had to use iMovie. The first one was to make a life story with pictures from their childhood under three minutes. And another one was that they could do a poem. And another choice was that they could create a tutorial explaining a concept that they had learned, they knew how to do, such as a math concept. And then the other one was that they could do a personal narrative or other fictional story that they had written, and turn that into video in under 90 seconds.\nTeacher: And what\'s your project about?\nStudent: I did this personal narrative, and I wanted to express [inaudible] Pismo Beach.\nTeacher: Oh, okay, so you did the story about Pismo Beach.\nTeacher: Where are the photos that you\'re going to use for that project?\nStudent: They\'re on the server and in the iMovie folder.\nTeacher: Okay, perfect. So you already transferred them over from home?\nStudent: Well, we have to make like a movie project. So you kind of have a topic, and with your topic you speak into a microphone, and then you draw pictures to go with what you\'re saying.\nStudent: Sad is the color of dark blue. Sad sounds like sobbing and deep misery. Sad feels as heavy as sharp bricks. Sad smells like tears on a damp pillow.\nStudent: When I go in the lab and do like an iMovie project. And I\'m just going to do one of these poems in here. And I\'m going to-- there\'s this thing called ""Kid Pics."" I\'m going to like load it on, like into little like sections. Put it all together, and I\'m going to put music in the background, and I\'m probably going to talk about it, too, a little bit.\nStudent: And then look at your audio, and you put it in iTunes.\nStudent: So go to this. Put all your--\nStudent: They\'re in order.\nStudent: No, but put them all together.\nTeacher: If you create an environment where they can collaborate with each other, then they get better ideas with each other, but also they help each other to finish their projects.\nStudent: My story is about basketball. Right now I\'m working on adjusting how long each photo is going to take, and seeing where I need to place the photos.\nTeacher: You\'ve got the writing skills and the reading skills and the speaking skills and the technology skills collaborating with each other. Organization skills all put together.\nStudent: Right here.\nStudent: We both were doing tutorials. We got the audio first recorded outside. And then we took a screen recording of what we were doing on like this painting website. And we like just filmed what we were going to show on the tutorial. Then on iMovie we put the video and mixed it with the audio. And cut off some portions of audio or clipped some portions of video for it to kind of match up.\nStudent: Cool technique, huh? Okay, now we can try something a little bit harder. How about, I guess, 25 times 25.\nTeacher: How many people got their iMovie projects uploaded and on their digital portfolios?\nTeacher: And then the next thing would be able to share it with other people. I think it makes it all the more meaningful in terms of if they think that somebody else might see it, you know, you have the whole concept of the authentic audience. Another important concept that I want students to know is that it\'s not just me teaching them. We\'re all here to teach each other. And each one of us is, you know, a citizen in this class, and I want them to participate and contribute to their learning. So definitely want to encourage them to be sharing and to collaborate with others and to always be a lifelong learner.', 'March 15th, 2019\nHow can we reduce plastic in our oceans? In today’s classrooms, teachers use project-based learning (or PBL) so that students can come up with potential solutions to real-world problems like this. With PBL, students identify the problem, research a solution and support it with evidence—all while learning valuable skills they’ll use long after graduation. Brainstorming these dynamic solutions can be an exciting and creative challenge for young minds. Technology can help motivate and spark imagination in ways that static textbooks can’t.\nLast week at SXSW EDU, we helped educators experience the power of technology-enhanced PBL first-hand, with a demo on how to create differentiated and personalized learning using technology in the classroom. The interactive demo let people get hands-on with educational tools from G Suite for Education, Chromebooks, Jamboard, Google Expeditions AR and VR and engaging third-party applications.\nEven if you didn’t attend SXSW EDU, you can recreate the lesson on removing plastic from our oceans with your students. Follow this guide to bring the magic of Google tools to your students and facilitate a collaborative, intelligent, connected and creative learning space.\nAssignment 1: Setting the stage\nFirst, you’ll need to introduce the challenge of reducing plastic in our oceans and identify key facts about pollution in our oceans.\nStep 1: Use Google Classroom to introduce the task.\nYou can use Google Classroom to create individual copies of materials for each student, in just one click. As an example, click here to make a copy of this lesson plan. If you’re using a different Learning Management System (LMS), Course Kit lets you integrate that LMS with G Suite.\nStep 2: Use “Explore” to find and cite a key quote.\nOpen the Google Doc provided in Step 1 and click the “Explore” button in the bottom-right of the document. The “Explore” functionality makes it easy to add citations to materials you referenced across the web.\nStep 3: In the Google Sheet, use “Explore” to analyze waste data for Austin, Texas.\nHere, the “Explore” feature leverages the same machine learning technology used by Google Search and Google Assistant.\nStep 4:Takethis quiz in Google Forms to test your knowledge on the topic.\nGoogle Forms automatically grades your students’ work—saving you from having to do it manually—and give them feedback on how they did.\nBonus:Test out the new “locked mode”, only available on managed Chromebooks.\nThis new feature (which is currently in beta) prevents students from navigating away from the Quiz until they submit their answers.\nAssignment 2: Dissect the problem and dig deeper\nNext, students will use research skills to understand the root of the plastic problem and collaborate with experts and peers.\nStep 1:Use Google Earth to explore real data on plastic moving across the oceans.\nYou can also use have your students use Google MyMaps to compare the size of the Pacific garbage patch to several US states.\nStep 2: Use Hangouts Meet to meet experts in the field.\nHangouts Meet is a great tool to connect students with experts and each other though secure video and messaging.\nStep 3:Go on a virtual reality tour of the ocean with Google Expeditions.\nThis tour is just one of more than 150 AR and 900 VR tours you and your students can experience. You can now view and guide tours you’ve created yourself using Tour Creator on both Android and iOS.\nStep 4:Use a Jamboard to work together to discuss what you’ve learned so far.\nNow that your students have dug into the problem, they can collaborate on the Jamboard or Jamboard app to answer key questions about the plastic problem and discuss what they have learned while researching.\nBonus:If you have a Vernier©️ sensor, use the Science Journal Android app to run an experiment testing how oxygen levels are affected by plastic in the ocean. Science Journal transforms devices, like you phone, into a pocket-sized tool for conduct fun science experiments—no fancy equipment required.\nAssignment 3: Generate creative solutions\nFinally, uplevel the lesson even more by generating creative solutions to the plastic problem based on everything we learned during instruction and research. Here’s a guide that suggests specific tools to use.\nStep 1:Create a VR tour with Tour Creator.\nYour students can help increase awareness of the plastic problem by creating their own immersive, 360° tours right from their computers. With this creative challenge, students can sharpen critical thinking and creativity skills, while building something they can add to a digital portfolio.\nStep 2:Create a website using Google Sites to outline possible solutions.\nSites gives you an easy-to-use tool to build websites, host course curriculum and encourage students to build their development skills.\nStep 3:Use Teachable Machine to create your own trash sorter.\nYour students can make it easier to recycle by training their computers to recognize and sort different types of trash using Teachable Machine, an AI experiment that requires no coding.\nStep 4: Make an automatic stop-motion animation with Google Photos.\nWith Google Photos, you can store and edit an unlimited amount of photos to use in your lessons.\nStep 5: If you have a Jamboard, you can use it to collaboratively review and workshop creative solutions to removing plastic from the ocean. No physical Jamboard? No problem, check out the free web-based version.\nWhether your students prefer to learn through video, reading, collaboration, hands-on experimentation or testing, Google tools allow you provide an engaging educational experience for every type of learner.']"	['<urn:uuid:b51d3b0f-d61e-4b57-b1ef-a2043aa66851>', '<urn:uuid:e3ce1128-1c86-4ff1-9dbc-66221dab244e>']	open-ended	direct	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T19:47:23.148022	24	126	1872
36	home electricity power output fluctuation range	The standard 120 volts received from residential wall outlets actually varies slightly, with voltage swings ranging from 118 to 122 volts from second to second.	['Ancillary services by definition are services that support the transmission of electricity from its generation site to the customer or helps maintain its usability throughout the system. Many people may not know that the standard 120 volts we are used to receiving from the wall actually varies a tiny amount from second to second. If you were to monitor the power from the wall, the voltage may swing from 118-122 volts. We do not typically think about the mechanisms that take place to keep our power useful and ready for when we flip the switch.\nOn a larger scale, ancillary services are generators or other service providers that are synchronized to the grid and are able to rapidly increase output in three major categories: contingency, regulation, and flexibility reserves. The contingency reserve requirement is assumed to be constant for all hours of the year and corresponds to a spinning reserve equal to about 3% of peak load and about 4.5% of the average load. Another way to think of “spinning reserves” are the backup or redundancy built into the grid. Basically, we slightly overbuild the total generation needed so the grid can be provided with ancillary services making good quality power possible.\nAdditionally, regulation and flexibility reserve requirements vary by hour based on the net load and impact of variability and uncertainty of wind and solar. The availability and constraints of individual generators to provide reserves are a major source of the cost of providing reserves. Not all generators are capable of providing certain regulation reserves based on operational practice or lack of necessary equipment to follow a regulation signal.\nSo, what does the future of ancillary services hold and how can they be more beneficial?\nAt a residential level, a combination of solar and storage is only worthwhile when specific conditions are met that make the value of storage greater than the cost of installing It. For example, when the renewable energy creates an excess, the extra energy can be stored for later consumption. This would allow the customer to buy less power from the grid and enable them to cut their costs.\nHowever, some customers are now being charged for using power during peak times, which is known as a demand charge. Energy storage can be used to lower peak time energy consumption, or the highest amount of power a customer draws from the grid; therefore, reducing the amount customers spend on demand charges. In North America, the break-even point for most demand charges is $9 per kilowatt. Energy storage can lower that cost to $4 or $5 per kilowatt by as early as 2020. As storage costs decrease, more customers will begin to see economic benefits and existing storage users will see the optimum size of energy storage increase.\nLastly, energy storage will impact electricity grids as a whole because it provides more function than just power on demand. Batteries can provide the grid with ancillary services like frequency regulation and should be compensated to do so. All this is to say, if utilities provide appropriate price signals to the market, customers will respond by installing battery storage where and how they can be compensated.\nCurrently, grids experience a continuous imbalance between the power they produce and its consumption because of the millions of devices that are turned on and off in an unrelated way. The imbalance can cause frequencies to deviate, which can affect equipment and potentially hurt the stability of the grid. Energy storage is well suited for frequency regulation because of its rapid response time and its ability to charge and discharge efficiently. This storage could significantly reduce the amount and cost of the reserves currently needed to provide such services to the grid.\nOne reason for the optimistic outlook on battery storage’s role with providing ancillary services is the progress lithium ion batteries have made in recent years. In 2015, lithium-ion batteries were responsible for 95 percent of energy storage at both the residential and grid levels. The reason for the increase in popularity is due to the price dropping, safety improving, and better performance characteristics. All of these qualities are leading to lithium-ion batteries being suitable for stationary energy storage across the grid; ranging from large-scale installations and transmission infrastructure to individual and residential use, even without being appropriately compensated for ancillary services.\nThe most important aspect is the large-scale deployment of energy storage that could overturn the status quo for many electricity markets. In developed countries, central or bulk generation traditionally has been used to satisfy instantaneous demand, with ancillary services helping to smooth out discrepancies between generation and load; and energy storage is well suited to provide such ancillary services. Eventually, as costs fall, it could move beyond that role, providing more and more power to the grid, displacing plants; however, that time has not yet come although approaching quickly. It is important to recognize that energy storage has the potential to upend the industry structures, both physical and economic, that have defined power markets for the last century or more.']	['<urn:uuid:c591ee05-8c5c-403b-8935-abf80a4decd3>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T19:47:23.148022	6	25	841
37	I'm researching World War II alliances and wondering how Germany's approach to Croatia differed from its relationship with Yugoslavia regarding the Tripartite Pact membership?	Germany's approach to these two nations was markedly different. With Yugoslavia, Germany used coercive diplomacy, with Hitler giving Prince Paul an ultimatum in March 1941 demanding joining the Tripartite Pact within five days or facing invasion. Despite Yugoslavia's eventual signing on March 25, 1941, this led to widespread anti-Axis demonstrations. In contrast, Croatia's entry into the Tripartite Pact on June 14, 1941, came after it had already been established as an Independent State of Croatia under German-Italian quasi-protectorate status. Croatia was already closely aligned with Germany, operating as a puppet state with the extremist Croatian nationalist Ustashas in power, making its joining of the pact a natural extension of existing cooperation.	"[""War and Revolution in Yugoslavia: 1941 - 1945\nThis is the long-awaited second part of the author s meticulously researched and scrupulously impartial study of the complicated and anguished history of Yugoslavia during the years of World War II. The previous volume dealt with the Chetniks, the resistance movement formed by officers of the defeated Yugoslav army who came to regard the Communist-led Partisans as their chief enemy, and who reached accords with the occupying powers first with the Italians and then with the Germans. The present volume deals with the rule of the Axis powers in occupied Yugoslavia, along with the role of the other groups that collaborated with them primarily the extremist Croatian nationalist organization known as the Ustashas.\nThe book begins by briefly describing the establishment of Yugoslavia in 1918 and its internal history during the interwar period. It then discusses the breakup of the state in April 1941, the annexation or occupation of parts of its territory by its neighbors, and the establishment by the Ustashas of the independent state of Croatia as a German-Italian quasi protectorate, focusing on its governmental policies and its problems with the Bosnian Muslims. The book also examines the role of religion during the occupation, the destruction of the Yugoslav Jewish community, and the economic exploitation of Yugoslav territory by the Axis powers. The work concludes by discussing the wartime population losses of the country and the ultimate fate of the collaborationist forces.\nWhat people are saying - Write a review\nWe haven't found any reviews in the usual places.\nYugoslavia Between the Wars\nThe Partition of Yugoslavia\nGermany and Italy Divide Slovenia\nForeign Annexation of Yugoslavia\nThe Puppet Government of Serbia\nThe Independent State of Croatia An ItalianGerman Quasi Protectorate\nThe Independent State of Croatia Under German Dominance\nThe Independent State of Croatia Internal Problems and Policies\nThe Churches During the Occupation and Revolution\nThe Destruction of the Jewish Community in Yugoslavia\nAxis Exploitation of the Yugoslav Economy Part I\nAxis Exploitation of the Yugoslav Economy Part II\nFurther Economic Consequences of War and Exploitation\nAlleged and True Population Losses\nThe End of the Collaborationist Regimes in Yugoslavia\nAccording administration agreement Albania Allied annexed April Archbishop areas August Balkans Banat became Belgrade Bosnia and Herzegovina Bulgarian Catholic Church Chetniks chief civilian collaboration Communist Party concentration camps Corps Croa Croatia Docs Croatian armed forces Croatian army Croatian authorities Croatian government Croatian Peasant Party Croats Dalmatia December Division economic especially established estimates Foreign Affairs German German forces German occupation German troops Glaise groups Hitler Hungarian Hungary Independent interwar Italian Italy January Jewish Jews July June Kasche killed Kosovo Kvaternik later leaders Ljubljana Ljubljana Province Macedonia Macek March memorandum metric tons Micr Mihailovic minister Ministry of Foreign Montenegro Nedic November October officers operations organization Partisans Pavelic Pavelic's percent political population priests refugees Reich Roll September Serbia Serbian Orthodox Church Serbs Slovene Slovenia southeast Europe Srijem Stepinac surrender tasha tion units Ustasha Ustasha government Ustasha militia Ustasha regime Volksdeutsche wartime Wehrmacht Yugoslav army Yugoslavia Zagreb"", 'The Tripartite Pact\nContributor: C. Peter Chen\nww2dbaseOriginally signed by Germany, Italy, and Japan, the Tripartite Pact was a typical European-style mutual defense treaty between the signatory nations. The treaty would call upon all three nations to unite against any aggressor who had violated the sovereignty of any of the three nations, not including nations that were already engaged in war with any of the three nations. It was not difficult to see that, given the international political situation at the time, the Tripartite Pact was signed with the intention to force the United States to remain isolationist and out of WW2 which had already been waging for an extended amount of time in Europe, northern and eastern Africa, and China.\nww2dbaseA summary of the Tripartite Pact is as follows:\nww2dbaseARTICLE 1. Japan recognizes and respects the leadership of Germany and Italy in the establishment of a new order in Europe.\nww2dbaseARTICLE 2. Germany and Italy recognize and respect the leadership of Japan in the establishment of a new order in Greater East Asia.\nww2dbaseARTICLE 3. Japan, Germany, and Italy agree to cooperate in their efforts on aforesaid lines. They further undertake to assist one another with all political, economic and military means if one of the Contracting Powers is attacked by a Power at present not involved in the European War or in the Japanese-Chinese conflict.\nww2dbaseARTICLE 4. With a view to implementing the present pact, joint technical commissions, to be appointed by the respective Governments of Japan, Germany and Italy, will meet without delay.\nww2dbaseARTICLE 5. Japan, Germany and Italy affirm that the above agreement affects in no way the political status existing at present between each of the three Contracting Powers and Soviet Russia.\nww2dbaseARTICLE 6. The present pact shall become valid immediately upon signature and shall remain in force ten years from the date on which it becomes effective.\nww2dbaseThe pact was signed by representatives Führer Adolf Hitler of Germany, Foreign Minister Galeazzo Ciano of Italy, and Ambassador to Germany Saburo Kurusu of Japan.\nLast Major Update: Dec 2005\nThe Tripartite Pact Interactive Map\nThe Tripartite Pact Timeline\n|5 Jul 1940||Romania announced that it had joined the Axis Powers.|\n|25 Sep 1940||Joachim von Ribbentrop alerted the German embassy in the Soviet Union that Japan was likely to join Germany and Italy in an alliance. Should this happen, the ambassador was to alert the Soviet Union of this news, and to ensure the USSR that this alliance was meant to deter the United States from entering the war and in no way was meant to be formed against Soviet interests.|\n|26 Sep 1940||In the late evening, the German ambassador in the Soviet Union shared the news that Japan was about to join Germany and Italy in a military alliance. The Soviet Union immediately complained that, according to the Molotov-Ribbentrop Pact, the text of such an alliance should have been shared with the USSR prior to the pact being signed, including any secret clauses.|\n|27 Sep 1940||Germany, Italy, and Japan signed the Tripartite Pact.|\n|20 Nov 1940||Hungarian Prime Minister Teleki and Foreign Minister Csaky signed a protocol in Vienna, Austria, joining the Tripartite Pact.|\n|23 Nov 1940||Ion Antonescu of Romania signed the Tripartite Pact in Berlin, Germany.|\n|24 Nov 1940||Prime Minister Vojtech Tuka of Slovakia signed the Tripartite Pact.|\n|13 Jan 1941||Bulgaria stalled in response to German demand to join the Tripartite Pact.|\n|14 Feb 1941||Hitler pressured Yugoslavia to join the Tripartite Pact.|\n|1 Mar 1941||Bulgarian Prime Minister Bogdan Filov signed the Tripartite Pact, which gave Germany the option of invading Greece through Bulgaria. Bulgaria was promised territories lost to Yugoslavia and Greece after WW1.|\n|4 Mar 1941||Prince Paul, Regent of Yugoslavia, arrived in Berchtesgaden in Germany where Hitler applied further pressure for Yugoslavia to join Tripartite Pact. Hitler offered to cede Salonika and part of Macedonia to Yugoslavia in return for allowing German troops to transit into Greece.|\n|13 Mar 1941||Germany repeated demands for Yugoslavia to join the Axis alliance.|\n|19 Mar 1941||Adolf Hitler gave Prince Paul of Yugoslavia an ultimatum, asking him to join the Tripartite Pact within five days, or face invasion.|\n|23 Mar 1941||Anti-Axis demonstrations were held in Yugoslavia.|\n|25 Mar 1941||Prime Minister Dragisa Cvetkovic of Yugoslavia signed the Tripartite Pact at Vienna, Austria; in secret, Yugoslavia had also allowed German troops to travel on Yugoslavian railroads for an attack on Greece. Upon hearing of the signing of this document, anti-Axis demonstrations escalated in Belgrade.|\n|26 Mar 1941||Anti-Axis demonstrations continued in Yugoslavia.|\n|28 Mar 1941||Anti-Axis demonstrations continued in Yugoslavia.|\n|14 Jun 1941||Croatia became the newest member of the Tripartite Pact.|\nDid you enjoy this article? Please consider supporting us on Patreon. Even $1 per month will go a long way! Thank you.\nShare this article with your friends:\nStay updated with WW2DB:\nVisitor Submitted Comments\nAll visitor submitted comments are opinions of those making the submissions and do not reflect views of WW2DB.\n» Ciano, Galeazzo\n» Cvetkovic, Dragisa\n» Hitler, Adolf\n» Kurusu, Saburo\n» Matsuoka, Yosuke\n- » 1,062 biographies\n- » 331 events\n- » 36,844 timeline entries\n- » 1,034 ships\n- » 332 aircraft models\n- » 185 vehicle models\n- » 344 weapon models\n- » 104 historical documents\n- » 190 facilities\n- » 461 book reviews\n- » 25,682 photos\n- » 297 maps\nGeorge Patton, 31 May 1944']"	['<urn:uuid:12c15e9b-708e-4e49-9681-f336dd0a2721>', '<urn:uuid:4298b820-1117-4ede-a0fa-f65fc0317af5>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T19:47:23.148022	24	111	1394
38	how much methane leak reduction PSE&G achieved	PSE&G achieved an 83% reduction of methane emissions by replacing one-third fewer miles of gas lines than would have been needed under a business as usual scenario.	"['Using methane leak data developed by EDF to prioritize pipeline replacement efforts, Public Service Electric & Gas (PSE&G), New Jersey\'s largest utility achieved an 83% reduction of methane emissions by replacing one-third fewer miles of gas lines than that needed to achieve the same result under a business as usual scenario.\nThe effort was part of a three year, $905 million program approved by the New Jersey Board of Public Utilities in November 2015.\nMap: Impact of leak flow rate on gas line replacement priority\nEDF quantified the overall methane leak flow rate associated with gas lines in 30 one-square-mile grids that PSE&G had identified for replacement and ranked the grids by that flow rate. The map reflects that PSE&G replaced a relatively small proportion of pipeline miles to achieve big benefits.\nSelect an area to see the ranking before and after incorporating leak flow rates.\nAbout this collaboration\nOver the three year period from 2016 through 2018, PSE&G is planning to replace 270 miles of low-pressure cast iron distribution pipes (""gas lines"") on their system, out of a total 510 miles of pipeline targeted for replacement over this period. How quickly PSE&G reduces the leak flow rate associated with these gas lines depends on the order in which they are prioritized for replacement.\nThis effort draws on a pioneering collaboration among PSE&G, EDF, and its partners, Google Earth Outreach and Colorado State University. Using cutting-edge spatial analytics methods developed by scientists at Colorado State University and methane sensors specially fitted to a Google Street View car, data indicating the location and relative size of methane leaks from underground lines was gathered in select areas of PSE&G\'s service territory. PSE&G provided EDF with locations and types of gas lines in areas it was targeting for replacement, making it possible to gather data relevant to the company\'s gas line replacement efforts.\nFor six months in 2015, the Street View car took millions of individual readings over hundreds of miles of roadway in PSE&G\'s service territory in New Jersey. Our research team used algorithms developed and tested over several years to assess the vast data stream.\nMap: New Jersey methane leaks\nBy using that data to help plan their gas line replacement efforts, PSE&G was able to reduce methane emissions more quickly, by replacing significantly fewer miles of gas lines than would have been necessary to achieve the same emissions savings without the new data.\nPSE&G divided its service territory into one square mile grids that were proposed for gas line replacements, and ranked each grid (displayed as ""original ranking"" in the map above) using an algorithm that considers safety risks associated with gas line segments. EDF quantified the overall leak flow rate associated with gas lines in a subset of 30 grids, and ranked the grids on this basis. Leak flow rate per mile reflects the volume of methane released per minute for a mile of gas line.\nUsing their original ranking algorithm, PSE&G replaced grids with the highest rankings first, and used EDF\'s leak flow rate ranking to determine relative priority among grids with comparable rankings. In other words, where grids had similar original rankings, the grid with the higher leak flow rate ranking was prioritized first, locking in reductions in methane emissions more rapidly than otherwise possible, while keeping safety considerations paramount.\nOur readings indicated an average of about one leak for every mile of gas line within grid areas where leak flow rate was quantified.\nCars with methane sensors took readings from May-November 2015. Our map represents a snapshot in time and may not reflect current leaks. A number of the leaks identified through this collaborative project have already been repaired or eliminated through the replacement of cast iron pipes, including all leaks deemed hazardous by PSE&G\'s existing leak grading system.\nStudies have consistently shown that a few large leaks are often responsible for a substantial portion of emissions from a population of leaks. Here, we observed a similar pattern. The three grids that PSE&G prioritized based on leak flow rate accounted for over 37% of the emissions, but represented only 9% of the gas line miles for which leak flow rates were quantified.\nUsing leak flow rates for prioritization allowed PSE&G to achieve an 83% reduction of methane emissions early on by replacing one-third fewer miles of gas lines than that needed to achieve the same result under a business as usual scenario. This difference is noteworthy considering that the typical cost to replace one mile of gas line on PSE&G\'s system is $1.5 to $2.0 million.\nThe results from PSE&G\'s leak quantification and gas line prioritization program have significant implications for the nation\'s gas utilities:\n- Older pipes made from cast iron and unprotected steel represent the most leak prone pipelines, according to national studies.\n- If PSE&G\'s ratio of emissions to gas line miles (37% to 9%, respectively) were to be found nationally, prioritizing replacements for 9% of the highest-emitting cast iron gas lines in the nation could result in 12,000 tons of methane emission reductions, which is a comparable climate benefit to taking over 200,000 cars off the roads each year.1\n- Significant methane emission reductions through cast iron distribution pipeline replacement programs can be achieved more quickly by using leak flow rate data to prioritize such programs.\n- Based on a 20-year global warming potential for methane of 84.']"	['<urn:uuid:69138da1-dc0a-44e2-bf0a-eef4b8f2d44e>']	factoid	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T19:47:23.148022	7	27	896
39	As an interior designer, I'm curious about wool carpets' maintenance and environmental aspects - what are the cleaning requirements, and how sustainable are they compared to synthetic materials?	Wool carpets are easy to maintain, requiring only vacuuming once or twice weekly and periodic professional deep cleaning. Wool fibers are naturally dirt-resistant and have a protective membrane with lanolin that repels moisture and stains. For cleaning, steam cleaning is effective, but rubbing should be avoided as it causes pilling. Regarding sustainability, wool carpets are made up to 80% from natural resources, are biodegradable, and require only a fraction of the energy used to produce synthetic fibers. When disposed of, natural wool fiber takes just a few years to decompose.	['Just like any other investments, rugs should be taken care of if you want them to last longer. Fortunately enough, your wool rugs are easier to clean because wool rugs are dirt-resistant in composition. Wools fibres are opaque and dirt doesn’t show as much as with some clearer, brighter man-made fibres. It’s true that wools are probably one of the most cost-effective types of rugs because they are easy to maintain. However, that fact doesn’t exclude the need of cleaning them once in a while.\nSome people say that vacuum has the most damaging effect on rugs. High powered vacuums can even pull the threads out of the back of rugs and cause sprouts to appear. But for wool rugs, that is not the case. Vacuuming your wool rugs twice a week will save you the penny to buy new one. However, for other rugs, it can still be advised to use vacuum cleaners for as long as the beater bars in the cleaner are removed.\nAnother way of taking care of your rugs is by becoming sensitive about the dirt that your rugs get. If your kids or pets had soiled your rugs, be sure to clean the dirt rightaway. Or in other cases, if the stain is caused by foods, scoop up the food residues and try to absorb the liquids the moment they spill onto your rug. It’s important to note that you should only use a paper towel or cloth to blot stains. Letting the stain dry on your rug will cause you extra work will only damage your rug -- both causes additional bother to us.\nSteaming can be a very effective way for wool cleaning. By letting the steam get into the fibres of your wool, the residues or dirt will let loose on the fibres of your rug. This can make your rug extra easy to clean. Furthermore, it is strongly discouraged that you rub your rugs because it can cause pilling and matting. It’s not advisable to soak your wool rugs in water. Wool rugs have fibres that can absorb more water than synthetic fibres. Soaking them and then letting them dry would take time. Wool rugs will become evidently heavy as well. Excess soap and water can ruin your rug, too.\nEvery rug owner should know the practice of rotating their rugs 180 degrees every six months to ensure even wear. Exposure to direct sunlight will always be a factor to consider when placing a rug in a room. Rotating the rug will help your rug looks balanced in colour. Besides, all fibres, dyed or un-dyed will normally change over time.\nBut above all these, it’s safe to say that the easiest, safest and effective deep cleaning solution for any type of area rugs is to have them rolled up and taken away for cleaning by experts. Not all wool rugs are the same for they have different fibre thickness. The best way to extend the life of your rugs is by reading the cleaning instructions for your rugs carefully before setting up the piece of art in any area of your room.', 'When choosing carpets for a hospitality project, the choice of material plays an important role. Here you will find information on the benefits of wool carpets\nThe benefits of wool carpets\n• Natural beauty and comfort\n• Fire resistant\n• Easy to clean\n• Environmentally friendly – healthy indoor climate\n• Breathable material\n• Naturally non-allergenic\n• Reduction in energy costs\n• Noise insulation\nNatural beauty and comfort\nBecause quality is the very heart of what we do, we select materials of the finest quality. We use the finest grades of British and New Zealand wool to bring the purest shades of colour. This wool composition is naturally very white and free of black fibres, making it easier for us to produce dense and uniform colours.\nA unique spiral crimp built into the wool fibre means a wool carpet will retain its look for longer because it has natural bounce-back abilities. The good capability to recover also makes walkways seem less worn and furniture marks less visible. A wool carpet remains good looking and provides immense softness and comfort underfoot. Wool carpets provide a sense of welcome and calm – warm, comforting, soft and luxurious.\nThe lifetime of our carpets is 8-12 years under normal conditions and with the right care and maintenance.\nWool is the safest carpet fibre because it is the most difficult to ignite and it is flame retardant up to 600º C. It burns slowly and is easy to extinguish. Burns from cigarettes and sparks from a fire will cause minimal damage to a wool carpet because it will smoulder and self-extinguish. In a major fire, wool carpets contribute much less in terms of smoke and toxic gas formations than most other materials. Wool carpets provide an added insulation against heat and flames for the underlay and underlying floor surface.\nAll Dansk Wilton carpet qualities holds certificates to meet standards required by our customers both on and off shore.\nEasy to clean\nThe structure of wool fibre has a natural protective membrane and a natural content of the fat ‘lanolin’ that provides the ability to repel moisture and acts as a natural dirt and stain resister. Dry dirt is easily vacuumed off. A properly maintained carpet only needs to be vacuumed once or twice weekly and periodic deep cleaning by a professional. The overall maintenance cost of wool carpets is significantly lower than that of hard flooring.\nUp to 80 of the Dansk Wilton, carpets are made from natural resources. Dansk Wilton is committed to provide a quality wool carpet that minimises the impact on the environment.We set the highest standards for our product quality and for safeguarding the natural environment. Wool is a natural material and a totally sustainable, renewable and biodegradable fibre.Wool fibre takes only a fraction of the energy used to produce synthetic fibre. Disposed of, natural wool fibre takes only a few years to decompose. The reason for adding nylon to the wool carpets is to obtain the best possible durability and long life.\nWool has a natural ability to regulate the humidity of an interior – it absorbs moisture (up to app 2/3 of its weight) when the atmosphere is damp and releases it when the atmosphere is dry, creating a more comfortable environment. – This without feeling damp or cold, whereas synthetic fibres can feel quite wet with only a fraction of the moisture content.\nNatural non-allergenic – healthy indoor climate\nWool carpets are naturally non-allergenic. They naturally absorb and filter airborne volatile organic compounds (VOCs) – unwanted toxins and odours that can cause irritation to humans. Wool carpets bind the allergy-causing substances tightly. Tests showed that indoor air in a carpeted room contained less than half the amount of fine particulate dust than the smooth floors. While allergens such as house dust mites do settle in the carpet (far less so than bedding), they can be vacuumed away using a good vacuum cleaner, preferably with a HEPA filter system. However, heavily worn carpet will not clean as effectively.\nDansk Wilton achieves the best possible VOC indoor rating. All Axminster and ColorTec carpets from Dansk Wilton have been tested and awarded the A+ rating for their very low VOC emission levels (Volatile Organic Compounds). Some of the common sources of negative VOC’s in the indoor environments are adhesives, caulking compounds, chemicals and other substances used in the production of carpets.\nBuild-up of VOC’s in indoor environments have often been associated with “sick building syndrome” causing a bad indoor environment.\nReduction in energy costs\nWool carpets help to keep the room warm in winter and cool in summer because of its superb insulation properties and its ability to regulate humidity. The thicker the carpet, the better the level of insulation provided. Studies have shown energy costs can be significantly reduced for both heating and cooling by up to 12 with a wool carpet when there are temperature differences between the indoor air and that under the floor.\nCarpets reduce noise in large areas and eliminate any scraping noises from furniture. They dramatically improve the acoustics of busy spaces, absorbing sound and softening the volume. Also in corridors, this acoustic performance of carpets is of great value to the guests. Carpets reduce noise not only on the specific floor level, but also on the floor below.\nTo evaluate the effect of the noise reducing properties by applying carpets on the floor construction, the airborne and impact sound insulation properties have been investigated in several studies. The measured Impact Sound Pressure levels for the constructions with carpets applied are all decreasing considerably.\nDownload the Dansk Wilton WOOL guide here.']	['<urn:uuid:e5307711-7213-4742-baf6-9e443dfb7153>', '<urn:uuid:96a2b209-b53e-4cfe-a8ac-51a4c058e0f9>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T19:47:23.148022	28	90	1458
40	I am working on my first knitting project and just finished it - should I be doing something special with water to make the yarn look better?	Yes, introducing moisture into your crochet and knit fabrics through soaking, washing, or steaming allows the yarn fibers to 'bloom' and come to life. This is necessary because yarn fibers have been processed through various treatments like dyeing, spinning, and bleaching, plus they accumulate oils from handling. When water enters the fiber pores, it helps wake up and revitalize the fibers, making them look their best.	"['Four Techniques for How To Block Your Crochet and Knit Fabrics\nFour Ways to Block Crochet and Knit Fabrics\nWhy Use It?\nDo you REALLY need to ask?? Okay… if you haven\'t already been back through our posts and read how strongly we feel about blocking… here\'s a summary:\n1. Beginning with swatches: You MUST block your crochet and knit swatches in order to know exactly how your fabric will turn out.\nYes, we know… you\'ve been crocheting/knitting for 120 years and you\'ve never swatched or blocked anything a day in your life and you always get gauge the first time. Fine… you have a magic hook and magic needles and you work with magic yarn making magic stitches… got it.\nFor the rest of us normal mortals, we will swatch and block because unless we do, we don\'t know if we are going to come out with the correct gauge for the pattern we are working from and we don\'t know how this particular (non-magic) yarn is going to work with this particular (non-magic) hook or this particular (non-magic) set of needles with this particular stitch pattern.\n3. Until you block your finished crochet and knit fabrics, the fibers in your chosen yarn will not bloom and come to life. Those yarn fibers have been processed using methods that might include everything from dyeing, spinning, bleaching, heating, twisting, oiling, coating with any number of treatments, and crushing. Then, you\'ve been handling that yarn with your own hands which impart to it your own natural body oils as well as any residue from hand lotions, soaps, food, or pets. All of these act to mush, squish, weigh down, and dull the fibers (even the artificial ones) in your yarn. Introducing moisture into your crochet and knit fabrics via soaking, washing (according to the manufacturer\'s label!), or steaming allows those fibers to wake up and bloom.\nOne way of describing this reaction of fibers to water is the term ""bloom."" For an expert definition of bloom and how it occurs in wool yarns, we turned to Beth Smith, author of The Spinners Book of Fleece. We asked Beth to describe the mechanics of bloom in wool yarns.\nSo there you have it from an expert on fiber. And this magical bloom isn\'t just a concept that applies to wool fibers. In fact, as long as you can get water into the pores of the fibers, a form of blooming will happen.\n(psst… if you\'d like to purchase Beth\'s FAB book, click on the photo or the name of the book above. This book is not just for spinners but for anyone who wants to know more about the wool that makes up yarn. Pick up a copy today and educate yourself!)\nSo… why use it? Why block? Because, if you want to meet the designer\'s gauge, want to make your fabric look like the designer\'s fabric, wake up your fibers and let them bloom, and show your beautiful handmade crochet and knit fabrics in their best possible light, you are going to block.\nFor more on blocking check out these articles here on our site:\nBlocking University – Course Overview\nBlocking Universiy – Session 1: The Right Tools For The Right Job\nA Tangential Post About Swatching\nBreak It Down\nThere are four basic ways to block your finished fabrics and they all depend on the fabric you are using and how that fabric reacts to the introduction of moisture.\n1. Finger Blocking\nThis is by far the quickest and simplest technique for blocking. No pins, wires or strings are used. Simple introduce moisture into the fabric and coax the stitches and dimensions into place with your fingers (thus the clever name… FINGER blocking).\nAfter introducing moisture to your crochet or knit fabric, if the dimensions are pretty close to correct you can probably get away with just finger blocking your piece into shape. This is usually the case when:\na. The stitch you are using is a solid stitch pattern that is stitched pretty close to the blocked gauge dimensions given in the pattern you are making from.\nb. The yarn you are using is made from most or entirely from acrylic fiber. These fibers react really well to steam heat and will soften and stay in place once cooled.\nThat said… make sure your recheck your finished measurements when your fabric is dried. Depending on the fiber content of your yarn and the stitch pattern, even a fabric that SEEMS to react well to finger blocking can shrink up depending on the fiber content (remember what Beth Smith said about wool?) and the elasticity of the stitch pattern you used.\nNo, we\'re not talking about the wildly popular social media outlet. This is a whole different type of pin.\nPinning alone is usually for small pieces of fabric or is used along with Finger Blocking in order to stabilize a fabric that doesn’t need much shaping to reach finished dimensions or that is made using a stitch pattern that doesn\'t need opened up very much at all.\nIf you are soaking or washing your fabric to introduce moisture to the fibers, you will do that first before inserting the blocking wires. Otherwise… well… you\'d be trying to soak or wash a piece of fabric with wires in it. And that\'s just silly. If, however, you are steaming your fabric (our preferred method of blocking 90% of the time), insert your wires first then steam your fabric before moving it into shape.\nWhatever method you use to get moisture into your fabric, now it\'s time to gently pull your work into the desired finished dimensions as dictated by either the pattern you are working from or the stitches you used. Have your pins and tape measure handy here… and GENTLY pull the wires outward from the center of your fabric until your reach your finished dimensions or until your stitches are blocked into their proper shapes. Pin the wires into place as you go using a (gentle) pull and pin, pull and pin method. Place the pins at an angle so they are holding the wires in place otherwise they could slip as the fabric dries and the edges of your fabric will pull in aaaaaand you\'ll have to start all over again.\nIF you encounter any pulling in one particular spot while you are (gently) pulling and pinning, STOP and steam or spray down the offending spot and smooth the fabric outward with your hands away from the pulled spot. We have seen this happen a few times in solid crochet fabrics, in particularly tightly knitted fabrics, or in crochet or knit laces. It\'s just a matter of either not enough moisture or an uneven amount of (not-so gentle) pulling when you are (gently) pulling and pinning. Keep an eye out for this because once those stitches are popped, they are nearly impossible to put back. Then there\'s wailing and throwing things and stomping on… oh… what? Just me?\nIf you are using steam to introduce moisture into your fabric, after you have evenly wired and (gently) pulled and pinned, hit the fabric with another bit of steam to set it and walk away. Otherwise, just walk away. No, really… go… the fabric has to dry now.\nTIP: We have been known to place a fan so it blows directly on a blocking piece of fabric to aid in the drying process. This is helpful if you live in a particularly humid climate or if you have a particularly dense fabric made with thick yarn. Fan enhanced drying is also helpful if you are on a tight deadline and you have three other projects to block out and you already have EVERY surface in your house covered in blocking mats and blocking fabrics and you feel like you\'re living in a factory and… and… what? Just us? Still… in a hurry or just impatient… the fan helps.\nThis is something we stumbled upon when we were trying to block a piece of fabric that was REALLY big and it just wasn\'t working out to block it in sections and we didn\'t even have enough wires to deal with it all. We somehow got it into our heads to run string up the sides of the fabric with a yarn needle and pull the ends TIGHT like a guitar string. Yup… it worked!\nBlocking with string is FAB for blocking BIG pieces of fabric and for blocking curves like necklines, armholes, round afghans, and circular or semi-circular shawls and wraps. Also, if you just don\'t have wires, this will work just fine with a little fiddling.\nAlso, you don\'t want your string or yarn to be a larger gauge than the fabric you are blocking. Weaving a larger gauge string or yarn through the edges of your crochet or knit fabric could result in holes and pulled edge stitches.\nTIP: Years ago, an ingenious stitcher in one of our classes suggested using unwaxed dental floss for blocking very fine gauge crochet or knit fabrics. We love this idea and tried it on a very fine gauge yarn we used to make a particularly airy fabric. The unwaxed floss went through the stitches without leaving a mark and the project came out beautifully.\nAfter the string is tight and the edges of your fabric are tight, place pins just as you would for blocking with wires and you\'re done!\nAs we said, this is a FAB technique for blocking out curves and circles. We use this for blocking the curves in armholes and for blocking out big ol\' circular shawls. Just run the string around the edges of the curve and pin into place. Wires are okay for slight curves over short distances but string works FAB for more dramatic curves over longer pieces of fabric.\nSo there you have it! No matter what the fiber, fabric, or circumstances, you now have a fully loaded toolbox of information on how to beautifully block your finished crochet and knit fabrics.\nWe\'re sure this won\'t be the last you hear from us on the subject of blocking. As long as crocheters crochet and knitters knit, there will always be more fabrics to finish and more blocking to be done.\nOh hey… before you go… Here are a few more pages we thought you might be interested in. Click on any of the photos below… enjoy!\nCome look us up on Facebook, Twitter, and Pinterest by clicking on one of these icons.']"	['<urn:uuid:52e0aae5-4cbc-4423-887e-e22e68067156>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:47:23.148022	27	66	1758
41	As a military strategist interested in modern counterinsurgency, I'm curious about how the U.S. forces' approach to equipment and protection compares to Afghan forces' methods, and what are the lessons learned from each side's tactics?	The U.S. forces tend to heavily rely on armored vehicles (MRAPs) and extensive protective equipment, with troops required to wear at least 40 pounds of gear before accounting for sustainment. While this provides blast protection, it significantly reduces mobility and situational awareness. Afghan forces, in contrast, wear little body armor and use unarmored vehicles, allowing them to greatly outpace ISAF troops both on foot and mounted. Their vehicles can access areas ISAF's cannot and travel at much greater speed. The reduced protection actually provides advantages in some situations, as speed and mobility can serve as force protection by allowing rapid displacement and the ability to stay off predictable routes. This contrast was particularly evident during the 2009 U.S. troop surge in Afghanistan, where American forces were observed using too much 'hard' power and insufficient 'soft' power, leading to civilian casualties and the need to implement more courageous restraint in their operations.	"['In December of 2009, President Barack Obama ordered the build-up of an additional 30,000 U.S. forces to Afghanistan in support of the 8-year-old war officially known as Operation ENDURING FREEDOM. The primary objectives for this large troop surge were to reverse the Taliban momentum that had been gained throughout Afghanistan while simultaneously increasing the size and capability of the Afghanistan National Security Forces; consisting of military, police and intelligence forces. As the surge technique had worked in Iraq under General David Petraeus, it was widely believed it could succeed equally as well under General Stanley McChrystal in Afghanistan.\n“I make this decision because I am convinced that our security is at stake in Afghanistan and Pakistan. This is the epicenter of the violent extremism practiced by al-Qaida. It is from here that we were attacked on 9/11, and it is from here that new attacks are being plotted as I speak,” said President Barack Obama, Afghanistan Build-Up Speech, Washington D.C., 2 December 2009.\nAs the build-up of U.S. forces matured and increased numbers of American Marines and Soldiers undertook the missions of patrolling the streets, markets, villages, highways and rural communities, lethal engagements and operations against the Taliban began to dramatically increase. Almost instantaneously with these engagements and decisive operations, Afghanistan civilian casualties also began to mount. It quickly became evident that these additional U.S. forces had come to Afghanistan a little too focused on lethal operations and with insufficient focus on non-lethal operations. Even within the established U.S. military command headquarters in Afghanistan, it was perceived by many senior officers, civilians and staff non-commissioned officers that the additional forces were using too much “hard” power and not enough “soft” power; they were not using enough courageous restraint during their missions and patrols.\nAfter observing these troublesome trends and actions, Lieutenant General David Rodriquez, Commander ISAF Joint Command, ordered the immediate establishment and deployment of counter-insurgency (COIN) mobile training teams (MTTs) to all six ISAF Joint Command regional commands in order to share, discuss, understand and document local experiences, tactics, techniques, procedures, lessons learned and best practices from the current operations taking place on the battlefield.\nAs a result of the MTT engagements with the regional commands and Afghanistan National Security Forces, it was decided that a COIN shura was needed to bring everyone together, including both military and civilian representatives, in order to openly discuss and exchange these critical observations, lessons learned and best practices.\n“Shura is an Arabic word for consultation and the Qur’an encourages Muslims to decide their affairs in consultation with those who will be affected by that decision. Shuras are vitally important in the Afghanistan culture,” stated ISAF Commander General Stanley McCrystal Commanders Update Brief, ISAF Headquarters, Kabul, Afghanistan, May 2010.\nFrom 13-14 May 2010, the COIN shura took place in Kabul aboard Camp Julien, the COIN Academy in Afghanistan. More than 100 representatives from all the ISAF Joint Command regional commands, ISAF Joint Command headquarters, ISAF headquarters, NATO Training Mission Afghanistan headquarters, provincial reconstruction teams, Afghanistan National Army and Afghanistan National Police participated in the COIN shura. It was the first of its kind hosted by the ISAF Joint Command.\n“We need to change the mindset of our troops to the COIN approach of \'protecting the people.’ We need to make good decisions to help reduce civilian casualties. The MTTs bring back many good lessons of restraint and good decision making from our young soldiers. Approximately seventy-five (75) percent of civilian casualties have occurred when we are not partnered, so it\'s very important to work with our Afghan partners to solve this problem. MTTs have proven successful at capturing these best practices and sharing them with other regional commands,"" said Commander ISAF Joint Command Lieutenant General David Rodriguez, ISAF Joint Command COIN Shura, Camp Julien, Kabul, Afghanistan, 13 May 2010.\nDiscussions and recommendations at the COIN shura included more pre-deployment COIN-centric training, stressing the use of tactical directives and SOPs, and less focus on kinetics and more focus on non-kinetics. Intra-theater training was also a big topic. Leaders from all the ISAF Joint Command regional commands agreed that a better job needs to be done with passing information to units coming in to replace ones that have completed their tour, and everyone needs to learn as much as possible from the Afghanistan National Security Forces.\n""COIN is about leadership down to the lowest level. There has to be more importance put on training before forces arrive, and training after they\'re here. We\'ve all read the SOPs, but you have to believe in them, and that comes from leadership and training,"" stated Sergeant Major ISAF Joint Command Sergeant Major Darrin Bohn, ISAF Joint Command COIN Shura, Camp Julien, Kabul, Afghanistan, 13 May 2010.\nBelow are the critical “Top Ten” observations that were taken from the COIN shura. As you can easily see, they apply equally as well to all operations that the Marine Corps is currently conducting, especially those missions in the range or spectrum of military operations from theater security cooperation to crisis response.\n- Number One mission in COIN is to protect the local (Afghan) people.\n- You must always connect with the local people; get as close to them as you possibly can in order to gain their trust and respect.\n- Do not engage insurgents when you cannot exploit the situation.\n- Courageous restraint saves lives. On many occasions, it has saved\nnumerous innocent civilian lives.\n- Let local Afghans tell you what their needs are; don’t tell them what you think their needs are. You must listen to the locals.\n- Perception is everything in COIN; you must always respect the\nAfghan people and treat them as equals. If you trust and respect\nthem, they will trust and respect you.\n- Conduct shuras with local leaders and elders at every opportunity. When problems come up, most shuras can help resolve them.\n- The best round in COIN is the one that is not fired. If you are\nobliged to fire, respect the golden rule of necessity and\n- You cannot commute to work in COIN; you must live among the people\nto gain their trust and confidence. Get off the FOBs and into the COPs/PBs.\n- Always be first with the truth. Admit your mistakes and\ncommunicate your sorrows.\n“We want to ensure that we are doing everything we can to reduce civilian casualties. We need to change the mindset of our Marines and soldiers to the COIN approach of \'protecting the people,\' and the best way to do that is by sharing best practices and improving overall COIN awareness,” said Deputy Chief of Staff for Joint Operations ISAF Joint Command Major General Michael Regner, ISAF Joint Command COIN Shura, Camp Julien, Kabul, Afghanistan, 14 May 2010.\nThese “Top Ten” COIN observations are absolutely essential in the conduct of small wars and should be trained to, educated and instilled in our Marines on a regular basis as we prepare for future and uncertain conflicts. If one were to read and study the Marine Corps’ Small Wars Manual printed in 1940, they would immediately notice that these observations are consistently weaved throughout the manual.', 'Among advisors to Afghan units, one phrase often heard is “Afghan good enough,” usually as a disparaging comment. It means, ”They’ll never be as good as Americans, but they’re good enough to get by in this country.” It is true that the ANSF will never be able to project land-, sea-, and airpower anywhere in the world. But, in the world the Afghans live in, they bring assets to the table that most American forces do not. People wishing to sound insightful about Afghanistan’s history often remark on how its fighters have made it the “graveyard of empires.” Forgotten is the fact that many of those fighters are on our side, and that there is much they can teach us. While there are wide disparities in the quality and motivation of ANSF, the best are able to achieve more with fewer resources than any Western military could. Applying that same level of resourcefulness and agility to a Western military that already has high quality equipment and training would create a remarkable force.\nIn 1988, Robert Fulghum wrote a book called “Everything I Really Need to Know I Learned in Kindergarten.” The simple, homespun wisdom borne of the basics we teach children was a publishing sensation at the time. Its advice, such as “Play fair” and “Don’t hit people,” is as true at age 55 as it is at 5. Of course, there are some things one needs to know that are not taught in kindergarten—driving and sex education come to mind. The book’s point is that much of what we really need to know to be successful is really just the basics that we learn early on. Just as adults forget the simple lessons of childhood in favor of the complexities of adulthood, often to their detriment, so do militaries. In our sole-superpower, globe-spanning dominance, we have lost much of our former speed, flexibility, and mental agility. Just as adults can learn from children, so Americans can learn from Afghans. Some of what makes Afghans effective are traits that Americans lost in the course of industrializing warfare.\nAn Afghan commander once told my advisor team a story about two frogs. He said that Afghans are like a frog at the bottom of a well. That frog there does not know that there is anything in the world beyond the space from one side of the well to the other. Then another frog falls into the well and tells the incredulous first frog about how big the world really is. That second frog is an American. The Afghan CO meant the story as a compliment to Americans and our broad-ranging experiences. While that CO did not mention it, there is a caveat to the story in regards to the American frog. That frog brings a lot of knowledge to the table, but he does not know the inside of the well like the first frog, and if they are both going to succeed inside the well, they need to learn from each other. The Afghan already knows he has much to learn from the American. Americans are often too intellectually arrogant to admit they have several things that they can learn from Afghans.\n1. There is such a thing as too much planning. Sometimes things really are as simple as they look. Afghans with no formal education and only the most basic military training can execute vehicle checkpoints, airborne interdictions, HVI snatches, and most of the basics of counterinsurgency without a single PowerPoint slide. If Afghans can successfully do these missions on a moment’s notice, why can’t Americans? Americans frequently get sucked down rabbit holes planning to counter every contingency and mitigate every risk. While we pride ourselves on detailed planning, COA development, wargaming, and so on, sometimes we forget that sometimes a simple mission is just that. While there are missions that require lengthy and detailed planning, many do not. Americans often plan operations for so long that the situation that necessitated an operation in the first place has long since passed by the time we are ready to act. In our zeal to answer every question we often paralyze ourselves with inaction.\n2. Risk is part of war. Afghans are consistently willing to put themselves and their troops in danger when the mission requires it. Among other things, they are encouraged to use field expedient methods in order to complete the mission in a timely fashion. While clearly some specific Afghan techniques are unsound, Americans have overcorrected, eliminating risk to the point which the mission becomes secondary to adherence to rules established at higher echelons. In many AORs, “Rules of the Road” and other guidance has been established to the point which small-unit leaders are not allowed to make judgment calls about the composition and conduct of patrols. Lives and equipment are valuable, and should not be spent lightly. However, at least at this point of the war in Afghanistan, it often seems as if our actions are guided by John Kerry’s words of 1971,”How do you ask a man to be the last man to die for a mistake?” It may be someone else’s country, and we may be leaving it soon, but as long as we are in a fight, we need to act with a sense of commitment to fighting, not just to avoiding losses. Running out the clock may work in athletic contests, but is not the way to victory in war.\n3.Speed and mobility can be force protection. Afghan troops wear little body armor, and most of their vehicles are unarmored. While body armor and armored vehicles may be necessary in high-kinetic environments, much of the time they cripple the mobility and agility of the forces using them. On partnered operations, Afghans greatly outpace ISAF troops, both on foot and mounted. Their vehicles can go where ISAF’s cannot and can travel at much greater speed. American MRAP vehicles do only one thing well—survive explosive blasts. In every other regard, they are deficient. They are lumbering, difficult to enter or exit, and the armored cocoon saps the situational awareness of the entire crew. They are largely roadbound—even the MATV, designed for all-terrain use, loses most of its off-road capability when encumbered with a mineroller. In most environments, IEDs are best countered by simply staying off roads and trails, which American vehicles are unable to do. If engaged, the ability to displace rapidly can be more valuable than additional armored protection. The same holds true for PPE. Regardless of the mission, conventional American troops are required to wear a minimum of 40 pounds of gear, even before accounting for sustainment. This makes them unable to move at any pace faster than a quick shuffle, rendering them immobile in terms of covering any significant distance or to maneuver against a threat. This immobility is often more dangerous than any lack of protection would be.\n4.Minimize logistical requirements. Afghans are well-known for having a poor logistics system. While this has continued to hamper ANSF’s development into a more capable force, it has also fostered innovation and required commanders to get by with less. As Americans, we have a tendency to bring everything but the kitchen sink with us, just in case we need it. This cripples our ability to act quickly by increasing equipment requirements, which increases planning time and further decreases tactical mobility. While Americans will never be able to live off the land as Afghans do, drinking from streams and resupplying from bazaars, emphasizing travelling light increases operational tempo. We often find ourselves in an escalating spiral of requirements. We bring extra vehicles in case the primary vehicles fail, which necessitates more recovery assets, which requires more fuel, which requires vehicles to carry the fuel, which requires more vehicles to escort the fuel vehicles, and so forth. The Afghan mindset is often closer to the expeditionary mindset we should be trying to achieve. Keeping things as simple as possible and reducing logistical footprints can reduce risk.\n5.Aggressiveness can make up for a lack of skill. In initial military training, Americans are hammered with truisms about such things as “speed, surprise, and violence of action.” Later in careers, the importance of these in warfare receives only lip service. When one has only his rifle and his comrades to rely on, the only thing that keeps one going towards the sound of battle is his determination to close with and destroy the enemy. Every day, Afghans push through dangers that would stop American units in their tracks. Americans will not drive across an Afghan street without minerollers and ECM equipment. American units stop even administrative movements when air support is unavailable. Afghan troops execute virtually all of their operations operating under what Americans would consider “no-go” criteria. While restraint and discretion certainly have their places in military operations, leaders should never emphasize them to the detriment of aggressiveness and a bias towards action.\n6.Sometimes the appearance of being outgunned is an advantage. Americans roll through the Afghan countryside armed to the teeth and armored like medieval knights, then wonder why the enemy never shows himself. Afghans have no choice but to go out, even when they do not have the advantage, and unsurprisingly, the enemy frequently shows up. One cannot fight the enemy one cannot find. Letting the enemy think he has a chance can be a useful tactic in counterinsurgency. Unlike Afghans, Americans have airpower to bring down destruction upon the enemy once he presents himself. The enemy will never show if he believes himself to be outgunned from the start.\n7.Don’t be afraid to leave a subordinate in charge. While it is true that the Afghans lack a professional NCO corps, and this continues to be a huge weakness, in certain other regards they are more decentralized than some American units. Many leaders in the American military are afraid to allow subordinates to make decisions in their absence. Often, they insist on receiving updates on even routine matters while they are gone. On the other hand, Afghan unit leaders are often gone for days or even weeks at a time, leaving subordinates completely on their own. This is in a land where communication technology is lacking, so subordinates are truly working without nets. While this situation is partially due other systemic problems, the result is frequently the development of trusted subordinates who have the ability to run a unit on their own. Few Americans are willing to delegate more than symbolic authority to subordinates, for fear that those subordinates will make mistakes. In reality, while they may make some errors, the errors are rarely catastrophic, and the subordinates learn in the process. Subordinates usually rise to the challenge and become better leaders themselves.\n8.Uneducated does not mean stupid. Americans value education greatly, and it is indeed very important, especially in a technologically advanced force. We tend to look down on those who do not have formal knowledge. The average Afghan spends his entire life living in conditions that most Americans would consider military-caliber survival training. He continuously has to improvise solutions to problems. Once one realizes the value of this type of learning, it prompts two revelations. One, never underestimate the capability of a man who has grown up in such an environment. There is a reason that some Afghans, having never taken a chemistry class, can turn ingredients found in a typical hardware store into bombs capable of throwing 30,000lb vehicles into the air. Two, it shows us that just because a procedure is nowhere in any manual does not make it invalid—Afghans devise effective solutions that are not found in manuals because they can’t read the manuals. Americans need to know when to use the manual, and when they need to pretend it doesn’t exist.\n9.No better friend. No worse enemy. Reward those who cooperate with you. In counterinsurgency, one’s friends have to know that there are advantages to cooperation. It is often better to live with a little evil if that allows one to defeat the bigger evil. As long as one knows what the ultimate goal is, he can ignore smaller infractions as long as progress continues towards the larger goal. In Afghanistan, one can see this in the government-led eradication programs. While some areas are indeed so corrupt that major traffickers own the entire area, in other areas selective enforcement can be a viable tool. If the government can’t eradicate all the poppy harvest, then at least the farmers who don’t harbor the enemy should be left alone. If someone is not cooperating, make his life hard. In the parts of Afghanistan where law enforcement is more effective, that means known bad actors get searched, get treated roughly, their poppies get destroyed, and their equipment gets confiscated. If enough fence-sitters see the upside of cooperation and the downside of opposing the government, an area can begin to turn.\n10.Always make time for tea. In the end, armed forces are bands of brothers who put their lives in one another’s hands. Afghans always greet every individual warmly and will take the time to sit down and discuss any issue, whether business or social. Personal interaction is what makes everything happen. Personal relationships are often an afterthought in American forces, especially in garrison. Training is done via computer. People e-mail instead of calling or walking down a hall. Whether it is over a cup of Afghan sheen chai, or over an American Budweiser, troops need to have the time and opportunity to form those bonds that let them trust one another when the chips are down. Beyond that, Americans neglect the value of face-to-face interaction. Giving an informal class can be more effective than death-by-PowerPoint. Stopping by someone’s office is usually more effective than sending an e-mail.\nAs individual military leaders progress through their careers, they pick and choose attributes from other leaders that they try to emulate. The same should apply for militaries. While there are certainly many qualities of ANSF not worth copying, there are certain things at which they excel, and Americans need to have the institutional humility to learn from them. The US military will not keep its premier status unless it has the moral courage to examine itself critically, to identify better ways of doing things, and to take risks in implementing those changes.']"	['<urn:uuid:72feba05-6e82-480f-ab10-d932416a44e4>', '<urn:uuid:e4593038-40f5-4fe7-987f-a2a75105792a>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T19:47:23.148022	35	151	3579
42	How quickly are populations getting older in different countries?	The pace of aging varies significantly between countries. While it took France 100 years to double its proportion of people over 60 (from 7% to 14%), developing countries like China, India, and Brazil are expected to undergo the same demographic shift in less than 30 years.	['Celine Herweijer: Demographics is all really about how people change and there’s three core dynamics to that. First of all, there is going to be more of us, second of all, we’re living longer and third of all, we’re having fewer children. So, just some statistics to bring that into picture. In terms of there being more of us, by 2025 there is going to be one billion more people on the planet. One-third will be the over 65 bracket. So, we’re going to be getting older.\nHow that actually plays out really does vary though at a country and regional level. It’s in Africa where we’ll actually see the quickest pace of change.\nIn Nigeria, for example, the median age will be 21 by 2050. Somewhere like Japan, at the other end of the extreme, we’re talking about an age of 53. So, clearly, a big shift.\nIt’s not the case that we can wait 10 or 20 years to think about these challenges. Actually, these issues are already impacting us today.\nJon Andrews: There are quite a few reasons that organisations should care about demographics as a megatrend. One of them, for example, is that their customer base is going to change, so the demographics impact their customers as much as they do their employees.\nSo when organisations look at demographic challenges they need to look at four different areas. One of them is gender diversity. One of them is the impact of the aging workforce and the fact that people are going to be around a lot longer. One of them is around the different ways of working of the new millennial generation and how they need to adapt to that. The fourth one is the importance of global mobility, where the changing and shifting epicentres of the economic power base is going to mean they need to be clear on where they get their talent from, which countries.\nIt’s fantastic that we’re going to create a huge amount more diversity within the working environment and all that can do is fuel innovation across all of our economies, fuel innovation within organisations. If we can tap into the different thinking, if we can tap into a larger spectrum of age within the workforce, if we can get the melting pot to work together effectively and there is a big if in there, then all I can see is a huge amount more advanced technology coming out of it, a huge amount more efficiency in working practices and a greater collective brain from the global perspective to deal with the bigger challenges that we’ve got.\nCeline Herweijer: One of the most fascinating collisions between demographics and another megatrend is that between demographics and climate change and resource scarcity. What we’re seeing here are the regions where we’re seeing the largest population increase in the emerging economies and especially in places like Sub-Saharan Africa are actually the places where climate change is going to have the biggest impact in terms of the risks that it will pose to those economies, in terms of the negative impacts it might have in terms of extreme weather event and changes in climate conditions that will make issues like health and food security and access to water increasingly difficult in the future. So, here we’re seeing a collision between growing population and increasing demand for resources and a physical change in the climate that is going to make it increasingly difficult to supply those resources.\nSo, therefore, what our clients need to be thinking about, both governments and businesses, who are working in these developing and emerging countries is the resilience of their investments going forward in time and how they can best work with their partners in developing and emerging economies to build resilient economies, operations and practices.\nJon Andrews: The PwC CEO survey has shown that two-thirds of CEOs are really struggling with finding talent at the moment and 93% of them are absolutely aware of the challenges of the changes in demographics and you bring those two things together and you create a real issue for the business. To me, the most telling fact is that only a third of CEOs are actually doing anything actively to address those two issues.\nI think what is going to happen anyway as a fact is we’re going to have an older workforce. What is going to happen anyway as a fact is we’re going to have greater ethnic diversity in our workforces. We’re going to work in many more geographic locations. What is also going to happen as a fact is there will be a trend and an understanding that women will be more successful.\nNow, we can either allow that to happen at a pace driven by the facts or we can embrace it earlier and identify how we can improve the way in which we tap into the value that is generated by it more quickly. And the organisations that do that and the economies that do that will be the ones that are more successful in the future.\nThe pace of this change will vary immensely across different regions. Africa’s population is expected to double by 2050, while Europe’s should shrink. The average age in Japan in 2050 is expected to be 54 – and just 21 in Nigeria.\nAll countries will need to implement bold policies to cope with these demographic changes. In Europe, Asia and Latin America, higher labour force participation from women and the elderly alongside higher levels of immigration will be needed to sustain an ageing population. Africa will experience a strong demographic dividend but it will need to develop the right policy conditions to maximise returns from its younger population.\nEmerging economies will need to face up to their demographic challenges over a much shorter period of time. It took France a century to double the share of the over 60’s in the labour force (from 7% to 14%) – China, India and Brazil are expected to do it in less than thirty years.\nBut because demographic trends are often presented as problems, the opportunities can be overlooked. We see two core sources of growth that our clients can tap into:\nWomen already control two thirds of the household budget in G7 countries, and as the wage gap with men continues to narrow, their purchasing power should continue to rise. Aging baby boomers will spur the demand for health care services. Nigeria’s population is expected to exceed America’s by 2045. There is an underlying theme that connects the demographic trends we’re seeing – the world is becoming a more diverse place.\nA growing body of evidence connects workplace diversity to improvements in businesses and economies’ performance. The most innovative companies are already adapting to tap into the growing diversity of their workforce in terms of gender, geography and age - as well as feelings and values.']	['<urn:uuid:a2eb542e-0c40-4c0d-90a9-73e2b62a6e2c>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:47:23.148022	9	46	1152
43	I'm researching feeding habits of piranhas - can you tell me what they eat both in the wild and in captivity, and how dangerous they really are to humans?	Piranhas are actually harmless to humans despite their razor-sharp teeth. Their natural diet consists mainly of small fish in the wild. In captivity, they can be fed on chunky seafood and small frozen fish like whitebait and lancefish. Adult piranhas typically feed at dawn and dusk.	"['6 Must See Animals of South America\nThe Animals of South America have been the inspiration of both writers and directors to produce books and movies telling tales of epic adventures or sheer horror. Meet the protagonists and make up your mind for yourself.\n1. Andean Condor\nThe king of the sky, the Andean Condor (Vultur gryphus) is an absolute stunning sight when spotted. It is the largest flying bird on our planet, with a wingspan of up to 310 cm. (122 inches).\nCondors are the national bird of Bolivia, Colombia, Chile and Peru and play important roles in folklore and mythology of South America.\nThe Andean Condor\'s diet consists mainly of carrion and occasionally sick or young animals. It feeds on dead deer, squirrels, mice, rabbits and other small mammals.\nThe place to view the majestic Andean Condor is from the Cruz del Condor at the Colca Canyon in Southern Peru. The condors start taking to the sky from below the lookout area. Watching a condor soar through the sky takes your breath away.\nThe Piranha (Pygocentrus Piraya) is a carnivorous sweet water fish that lives mainly in the Amazon Basin of South America.\nThey are about 15 to 25 cm. (6 to 10 inches) long. The feature that made them the protagonists of many horror tales are their razor sharp teeth.\nPiranhas are harmless to humans (I have swum in rivers with piranhas while traveling in the Amazon Basin). Their diet exists mainly of small fish.\nThe capybara is a semi-aquatic herbivorous animal and the largest of living rodents. It is native to most of the tropical and temperate parts of South America east of the Andes.\nCapybaras live in big herds and grow between 105 and 135 cm (40-55 inch) in length, and weigh 35 to 65 kg (75-140 lbs).\nThe capybara spend most of their time on the banks of rivers, feeding in the mornings and evenings. Their diet consists of vegetation such as river plants and bark.\n4. Southern Right Whale\nThis magnificent animal comes to the waters of Argentina\'s Península Valdés (Patagonia) to breed. Spotting a Southern Right Whale is a breathtaking sight and you can almost touch them when you set out on a boat from Puerto Pyramides.\nAdults may be between 11-18 meters (36-59 feet) in length and up to 80 metric tons (88 short tons) in weight. Southern Right Whales (Eubaleana Australis) spend the summer months in the Southern Ocean feeding, probably close to Antarctica. Animals migrate north in winter for breeding and can be seen around the coasts of Chile, Argentina, Brazil, South Africa, Australia and New Zealand. The total population is estimated to be 7,000 to 8,000.\nThe Southern Right Whale\'s diet consists mainly of plankton and tiny crustaceans like copepods, krill, and pteropods.\nThe Anaconda is one of the biggest and heaviest snakes of the world. There have been sightings of anacondas at over 10 meters (32.8 feet) in length (although most are considerably smaller) and can weigh up to 250 kg (551 pounds) and have a girth of more than 30 cm (11.8 inches) in diameter.\nThe best two known species are the Green Anaconda (Eunectes murinus) and the Yellow Anaconda (Eunectes notaeus). Their habitat are the swamps and rivers of the dense forests from the Amazon Basin.\nAnacondas are carnivorous. Their diet exists of capybaras and other large rodents, tapirs, deer, peccaries, fish, turtles, birds, sheep, dogs and aquatic reptiles. Anacondas occasionally prey on jaguars.\n6. Giant Tortoise\nThe Giant Tortoise is an animal that can only be found on the Galapagos Islands (Ecuador). It is an impressive sight; a giant tortoise can weigh over 225 kg (500 pounds) and measure 1.8 m. (6 feet) from head to tail.\nThere are 15 recognized species of tortoises, all generally considered to be members of the single species Geochelone Elephantopus.\nThe 15 species of tortoises can be divided into two general morphotypes: domed and saddle-backed. In the domed tortoises, the front edge of the shell forms a low line over the neck while in saddle-backed tortoises, the front edge arches high over the neck. Four species are extinct. The Charles Darwin Research Station in Puerto Ayora (Santa Cruz Island) on the Galapagos Islands, has a breeding program running.\n*© Photograph ""Condor"" by Grigory Kubatyan | Agency: Dreamstime.com', ""TF2YD Stock Alerts:\nEnter your email address below and you will be alerted when a shop adds new TF2YD stock:\n- About TF2YD\n- TF2YD FAQs\n- Refunds & returns\n- Shipping & Delivery\n- Terms & Conditions\n- Acclimatising fish\n- Nitrogen Cycle\n- Buying Fish\nOur TF2YD Rating:\nOur rating is shown below based on TFF user reviews of our shop and ordering service.\nSome of the above images have been provided by Tropicalfishfinder. Please be aware that variations within species mean that the fish you are sent may not be identical to the fish in the photographs.\n© 2003-2018 Tropical Fish Finder | All Rights Reserved | E&OE\nTF2YD Stores > Wildwoods > Predatory - Piranhas> Red-Bellied Piranha Red Piranha Pygocentrus nattereri\nRed-Bellied Piranha Red Piranha Pygocentrus nattereri\nCategory: Predatory - Piranhas\nPrice: £49.95 each\nDiscount: Buy 3 or more Pygocentrus nattereri and get 5% off!!\nStock: Temporarily out of stock\nWater Chemistry: Adaptable\nCare Level: Difficult\nSchooling Fish: No\nDUE TO SENSITIVITY THIS SPECIES IS EXEMPT FROM OUR 24 HOUR LIVE ARRIVAL GUARANTEE.\nPLEASE ALSO NOTE DUE TO SPECIAL PACKAGING PROCEDURES MORE THAN 4 PIRANHA ORDERED MAY INCUR AN EXTRA DELIVERY CHARGE FOR SHIPPING SPACE AND WEIGHT.\nThese are farm bred 'Super Red' red breasted piranhas that will show superb colour when larger. This species can be shoaled together succesfully as long as the tank is big enough and they are in groups of 5 or more.\nFurther information can be found below:\n|Male or Female||Cannot be sexed|\n|Water conditions:||These fish are currently kept in water Ph 7.2 and Neutral|\n|Volume Discount:||Buy 3 or more Pygocentrus nattereri and get 5% off!!|\nPygocentrus nattereri (Red-Bellied Piranha, Red Piranha)\nOne of the most popular piranhas in the hobby, combining moderate size with good colours. It has a distinctive pug-like face with a protruding lower jaw, and an expansive red region running from the lower jaw across the lower part of the cheeks and onto the throat. Body colour is variable but typically dark but metallic grey. Juvenile fish have very red anal fins and a scattering of black spots across their bright silver bodies.\nCompared with the standard red-bellied piranha, Pygocentrus nattereri ‘Super Red’ has a stronger pattern of dark spots on the flanks and the red colouration extends further across the face and belly. It gets to about 30 cm in captivity. A widespread and adaptable species, the ‘Super Red’ is a good piranha for the home aquarium, particularly when kept in a reasonably large group of no less than four specimens and preferably at least six specimens.\nBecause this species has very strong jaws and exceptionally sharp teeth, it must always be handled with extreme care. Aquarium specimens have been reported to bite aquarists.\nFish information (behaviour and breeding):\nCan be kept either singly or in groups of six or more. Smaller groups than six are rarely stable, with the dominant fish attacking the weaker until only one remains. Wild fish are solitary, so there isn't any overwhelming need to keep them in groups.\nRed-bellied piranhas are shy and skittish. They need tanks with subdued lighting, ideally by providing the tank with lots of floating plants. In the wild adults feed mostly at dawn and dusk. Food items can include chunky seafood and small frozen fish such as whitebait and lancefish.\nHas been bred in captivity. Courtship is a vigourous affair with much chasing and sometimes biting. Once the eggs are laid in the nest, the female may guard the nest with the male for a short while but is quickly chased away. The male guards the eggs alone until they hatch. Broods contain several hundred eggs. Initially the fry will eat brine shrimp nauplii.\nFurther fish details are shown below:\n|Distribution||South America: widely distributed, including the Amazon and Paraguay river systems|\n|Size||Over 25 cm in the wild, usually smaller in captivity|\n|Water Parameters||Adaptable, but prefers slightly soft and acidic conditions|""]"	['<urn:uuid:57d137ac-372b-4ede-8613-be4ed2ea29d2>', '<urn:uuid:c6c8aea7-2bdb-42d7-b570-db14e18f66cd>']	factoid	with-premise	verbose-and-natural	similar-to-document	three-doc	expert	2025-05-12T19:47:23.148022	29	46	1374
44	Hey, I'm learning Russian. Why does that Shadow and Bone book not use real Russian words?	The author chose not to use real Russian because Ravka isn't Russia, and Russian would be too opaque for western readers since they don't share the same alphabet. Instead, they created a unique language that belongs to Ravka while still evoking meaning for readers, using techniques like adding Russian suffixes to familiar root words.	['(This was originally published on MochaLatteReads as part of the Shadow and Bone blog tour.)\nOne of the biggest challenges in building the Ravkan world was language. As one glance at the map will tell you, Ravka isn’t Russia, so it didn’t make sense to simply transcribe Russian (though it would have made life easier). And even if I had, for non-speakers, Russian is an incredibly opaque language. Because we don’t share an alphabet, very few words have any resonance for a western reader. Still, the world of Shadow and Bone was clearly inspired by Russia and I didn’t want to violate the reader’s sense of place by throwing some random language into the text. My goal was to use language to reinforce the reader’s experience of the world rather than undermine it.\nOne of the first problems I came across was the Darkling’s title. Because so much fantasy is set in Medieval England, I didn’t want anyone running around saying, “Yes, m’lord” in Ravka. In Russian, “yes, sir” or “yes, m’lord” would be something like moi gospodin—again, meaningless for a non-Russian speaker. I wanted something that belonged wholly to Ravka, but that would evoke something for the reader. I took the root word “sovereign” and added a Russian suffix. The Darkling’s title became moi soverennyi. Generally, I think most readers skim over italics. (I often do!) But you never know what the eye may pick up and if someone does want to look more closely, I’ve given them something to find.\nI used a similar strategy with Poliznaya (the town where Mal and Alina took their military training), starting with the root “polis” for city—which also gives a nice echo of “police.” This was also the way I approached the names of the Grisha Orders: Corporalki, Etherealki, and Materialki.\nNothing to See Here\nThere were moments in the story where I decided it was okay to let the language be opaque. When Alina is studying Grisha theory, she comes across the concepts of odinakovost (thisness) and etovost (thatness)—which are actually based on haecceity and quiddity. She doesn’t understand them at this point in the story, so in that moment I like that the reader is grappling with the word as Alina grapples with the ideas.\nIn a very few instances, I used real Russian words. Sometimes they were simple and obvious, like “tsar,” which is obvious in its meaning to any reader. But some were a bit more complicated.\nThe Grisha: I chose the name Grisha for the magical elite because it is the Russian diminutive of Gregory which means “watchful” and derives from the biblical Grigori (which a lot of paranormal fans will recognize from fallen angel tales). It also evokes the word “geisha,” which reinforces the sense of beauty, secrecy, and the elite.\nKvas: Kvas is, in fact, a semi-fermented beverage popular in Russia, but in Shadow and Bone it’s an obvious stand-in for vodka. For me, “vodka” places us instantly in our world and time. It’s simply too common. Kvas, however, maintains immersion in the world, but has the advantage of visually evoking the word “vodka”—there’s no real leap for the reader to make. (Also, this is YA. I suspect a lot more flags would have been raised if I’d had a bunch of teenagers sitting around drinking Stoli.)\nOtkazat’sya: “The abandoned.” This is the word Grisha use to refer to people unable to practice the Small Science. In Russian, this is a verb form and not a noun. But because I liked the look and sound of the word, I adopted the “‘sya” suffix for other constructions later in the series. The verb itself can mean “to abandon,” but it has other meanings as well, an ambiguity that comes into play in the story.\nMerzost: This word doesn’t actually appear in Shadow and Bone, but it does show up in the next book and my short story, “The Witch of Duva.” This is the Russian word for abomination, but it’s the Ravkan word for both abomination and magic.\nAgain, I don’t know if readers will pick up on things like this or care about them, but it was a way for me to add another layer of meaning to the story for anyone who wants to go looking.\nTruth in Fiction\nLet’s be honest: I chose to use Russia as my inspiration, but my goal was never authenticity. As I mentioned above, it would have been far easier to bring on a Russian translator and simply have him or her transcribe words and dialogue. Deciding not to go this route meant agonizing over much smaller decisions—from how to construct a plural (“Corporalki” over “Corporalniki”) to whether or not Ravkan surnames should be gendered (Starkov” over “Starkova”).\nThe fact that there are guns in the Grisha Trilogy and that its cultural references are not drawn from Medieval Europe seems to cause some confusion, but Shadow and Bone is high fantasy—not historical fantasy or alternate history—and Ravka is not Russia. For me, using Russian wouldn’t have felt true to the world.\nWriting high fantasy, every choice is a bit of a gamble. We take something familiar, put our own spin on it, and hope the risk will reward the reader’s experience. There were days when naming that fifteenth town or figuring out what to call a character who I knew I was going to kill off just a few pages later made me want to down a few shots of very real vodka. But how many people get to build a world and then invite others into it? Sure it’s a gamble, but it does leave a girl feeling lucky.']	['<urn:uuid:4243fc7d-da3d-4b41-9b2c-1d6cecdf2f4a>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:47:23.148022	16	54	943
45	I'm looking to help kids create masks for different occasions - what types of printable mask patterns are typically available for children's craft projects?	There are many styles of printable masks available including paper plate masks, stick-on disguises, headbands, eye masks, eyeglasses, and print-and-cut mask activity sheets. Some masks come as coloring sheets ready for kids to decorate with crayons or markers, while others are full-color paper craft masks that just need to be printed and cut out. The varieties include animal masks, Halloween masks, holiday masks, Chinese masks, Greek masks, and African masks.	"['Kids Crafts: Paper Masks\nPrintable masks and paper mask crafts for children.\nFun and Easy Mask Crafts for Children\nKids love paper mask activities\nOne of the most enjoyable kid crafts I\'ve found is making paper craft masks. Children love to make masks for any reason, whether to mimic characters in a favorite book or video, for party activities, for holiday celebrations such as Halloween and Mardi Gras or for simple costume play.\nKids mask crafts are wonderful creative arts and crafts, and they\'re inexpensive. Often you can make a mask from recycled materials - so you\'re making crafts that are also green.\nIf you want to make a specific type of mask, you can find unique printable masks and mask templates available online or in books, free to print or copy for personal use or to use for classrooms and club activities. We\'ve collected a list of free printable paper masks ... keep on reading for the links and a free mask download.\nRibbit! Frog Face Mask\nFunny green frog mask, ready to print\nPaper Frog Face Mask\nFunny Party Mask Crafts for Kids\nFun to make and fun to wear, printable party masks\nKids love to make funny masks. My daughter\'s favorite paper craft activity for birthday parties or holidays was crafting party hats and funny paper masks.\nNo matter what style party mask you make with kids - paper plate masks, stick on disguises, head bands, eye masks, eye glasses or print and cut mask activity sheets - kids love to make and decorate masks. Enjoy my selection of free childrens paper mask crafts.\nFun to make and fun to wear, printable mask patterns and printables will keep kids busy and creative for hours - making the masks and wearing them!\nDay of Dead Skull Mask\nFancy Sugar Skull Printable Mask, Ready to Decorate\nSugar Skull Mask Craft\nInstant Fun - Paper Masks Crafts for Kids\nSimple, economical paper mask craft activities for children\nI\'ve collected printable mask craft sheets for more than 40 years. I share my favorite paper masks here for you to download and make with your children or students. The paper masks in my free downloads and linked resources are all fun, free printable masks or mask patterns in full size sheets, ready to print and use.\nSome of the paper masks you\'ll find in this collection are coloring masks, ready for kids to decorate with crayons or markers; other free printable masks I\'ve listed in this section are full color paper craft masks. All you need to do with those is print, cut out and wear - fun, free instant masks!\nFree Printable Coloring Mask Craft Collections\n- Funny and Frightful Mask Crafts for Kids - print and color in these free paper masks\n- Make Your Own Masks, Face Paint and Kids Costumes\n- Masquerade Costumes and Masks\n- Phee McFaddell Free Printable Masks\n- 15 Print and Color Masks - free printables at Woman\'s Day\n- Scissorcrafts Free Printable African Masks\n- Woo Jr. Printable Masks Collection has all sorts of animal masks, Halloween masks, holiday masks, even Chinese and Greek masks to print and cut out.\n- Fun and Funny Animal Masks to color in and cut out.\nMardi Gras Mask\nFancy Feather Mask to Print, Cut Out, Decorate\nFancy Mardi Gras Mask - Printable\nFree Printable Mask Crafts and Mask Templates\nNo cost easy to make paper masks for kids\nPaper Masks for Children\nFamily Portal presents a collection of paper masks and mask craft activity sheets for children. Free printable masks for personal and classroom use.\nPaper Toys - Masks\nFree printable kids mask crafts, many different unique paper masks\nShamrock Eyeglasses Mask Craft\nFunny green shamrock shaped eye glasses masks craft activity sheet.\nMardi Gras Masks\nMardi Gras fancy mask craft sheets and patterns\nPrintable Owl Mask\nFull color print, cut and wear owl mask craft activity sheet.\nInstant Masks Blog\nLinks to new and creative printable masks and easy to make masquerade masks.\nFind lots of printable mask crafts for kids\nAdorable Coloring Masks for Kids\nCute kids mask templates, ready to print and color.\nRaccoon Glasses Paper Mask\nCut out and wear funny raccoon ""mask"" eyeglasses paper mask.\nPrintable Pink Pig Face Mask\nWoo! Piglet, Miss Piggy, Olivia, or Charlotte - which pink piggy can you be with this printable pink pig paper mask craft?\nMasks for Kids\nEasy and fun mask crafts for kids. Basic instructions for paper plate and other simple craft masks plus helpful hints for cutting and wearing home made masks for kids.\nDay of the Dead Mask Craft\nKids have fun and learn about Mexico\'s traditional celebration making this paper plate Day of the Dead skull mask.\nPrintable Party Mask Mustaches\nBoys LOVE Wearing a Mustache - Mario, Sheriff, Pirate Mustaches\nMustache mask cut and print sheet\nMask Crafts Materials and Supplies\nUse nontoxic supplies and recycled materials to make craft masks with children\nMask crafts are versatile and non-toxic when you work with kid safe materials. We like to craft masks from clean heavy weight paper, card stock, recycled cereal cartons and paper plates.\nDecorate children\'s mask crafts with fabric or paper scraps, buttons, stickers, markers, paints, craft feathers, sequins, dried pasta, flowers or anything else you have on hand in the craft supply bin or cupboard. Tear up old colorful magazine pages to create paper bits to stick onto a mask. Use bits of yarn or shoelaces to create ear loops.\nKids are creative artists and will enjoy making paper masks fron any clean safe paper source and embellishing materials you provide. Be sure to use non-toxic paints and glues and if you create eye or face masks, attach them to a stick or make ear loops to hold a mask onto a child\'s face.\nGroovy Goggles Paper Masks\nPrint, Cut, Wearable Paper Eye Masks for Kids\nFunny Paper Mask - Groovy Goggles Printables']"	['<urn:uuid:b01318e2-4d97-4a52-a1ff-d28fed4c6605>']	factoid	with-premise	verbose-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:47:23.148022	24	70	987
46	egypt israel sinai peninsula withdrawal terms diplomatic relations camp david	Under the Camp David Accords, Israel agreed to completely withdraw its armed forces from the Sinai Peninsula and restore it to Egypt. In exchange, Egypt would establish normal diplomatic relations with Israel and guarantee freedom of passage through the Suez Canal and nearby waterways. Israel fulfilled this commitment in 1982 by returning the last segment of the Sinai to Egypt.	"['Camp David Accords\nThe Camp David Accords were signed by Egyptian President Anwar Sadat and Israeli Prime Minister Menachem Begin on September 17, 1978, following twelve days of secret negotiations at Camp David. The two agreements were signed at the White House, and were witnessed by United States President Jimmy Carter. This represented the first diplomatic success in the effort to resolve the ongoing problems surrounding the creation of the State of Israel in 1947, following the initial war between Israel and her Arab neighbors, the 1967 six-day war when Israel extended her borders, occupying parts of Jordan, Syria and Egypt; and the Yom Kippur War of 1973. It saw the first Muslim Arab state to recognize Israel and to normalize diplomatic and economic relations.\nCritics say that Camp David was a limited development, since it only involved Egypt. However, it was (at the time) the only diplomatic initiative to succeed, and it gave both impetus and content to the subsequent Oslo Accord of 1993 which led to the establishment of the Palestinian National Authority (anticipated by the Accord\'s reference to the possibility of an autonomous state in the West Bank and Gaza Strip) followed by the Israel-Jordan treaty of 1994.\nEgypt\'s leaders, who enjoyed considerable status within the Arab world, were courageous to take this step towards resolving the complex issues surrounding Israel, her relationship with her neighbors and the demands of displaced Palestinians for justice within the territory occupied by Israel and for a state of their own. This process was made more complex by the desire of some Arab Muslims to destroy Israel, hence Israel\'s reluctance to return territory which served as a security-buffer, enhancing Israel\'s defense system. However, this Agreement also established the principle of land for peace—since Israel agreed to return the Sinai to Egypt, which also resulted in the re-opening of the Suez Canal. Both Egypt and Israel have continued to benefit from US aid linked to the treaty.\nSadat and Begin, the principals to the accord, both took great political risks. Later, Sadat would be assassinated in 1980 by Muslim radicals who objected to peace with Israel. Yet in moving the peace process forward, Camp David is clearly understood as an historical, progressive, and positive event.\nUpon assuming office on January 20, 1977, President Carter moved to rejuvenate the Middle Eastern peace process that had stalled throughout the 1976 presidential campaign in the United States. Following the advice of a Brookings Institution report, Carter opted to replace the incremental, bilateral peace talks which had characterized Henry Kissinger\'s shuttle diplomacy following the 1973 Yom Kippur War with a comprehensive, multilateral approach. This new approach called for the reconvening of the 1973 Geneva Conference, this time with a Palestinian delegation, in hopes of negotiating a final settlement.\nCarter also wasted no time in visiting the heads-of-state on whom he would have to rely to make any peace agreement feasible. By the end of his first year in office, he had already met with Anwar Sadat of Egypt, King Hussein of Jordan, Hafez al-Assad of Syria, and Yitzhak Rabin of Israel. Carter\'s and Cyrus Vance\'s exploratory meetings gave him a basic plan for reinvigorating the peace process based on the Geneva Conference and Israeli withdrawal on all fronts, including the West Bank. The political situation in Israel underwent a dramatic upheaval with a devastating electoral loss of the long-ruling Israeli Labour Party to Menachem Begin\'s Likud Party in May of 1977. While Begin officially favored the reconvention of the conference, perhaps even more than Rabin, and even accepted the Palestinian presence, Israel and eventually also Sadat strongly preferred bilateral talks. Even earlier, Begin had not been opposed to returning the Sinai, but a major future obstacle was his firm refusal to consider relinquishing control over the West Bank.\nThe Sadat Peace Initiative\nPresident Anwar Sadat came to feel that the Geneva track peace process was more show than substance, and was not progressing, partly due to disagreements with Syria. He also lacked confidence in America to pressure Israel after a meeting with Carter. His frustration boiled over, and after meetings with Israelis, secret even to the Americans, in November 1977 he became the first Arab leader to visit Israel, thereby implicitly recognizing Israel. The gesture stemmed from an eagerness to enlist the help of the United States in improving the ailing Egyptian economy, a belief that Egypt should begin to focus more on its own interests than on the interests of the collective Arab world, and a hope that an agreement with Israel would catalyze similar agreements between Israel and her other Arab neighbors and help solve the Palestinian problem. Prime Minister Begin\'s response to Sadat\'s initiative, though not what Sadat or Carter had hoped, demonstrated a willingness to engage the Egyptian leader. Like Sadat, Begin also saw many reasons why bilateral talks would be in his country\'s best interests. It would afford Israel the opportunity to negotiate only with Egypt instead of with a larger Arab delegation that might try to use its size to make unwelcome or unacceptable demands. In addition, the commencement of direct negotiations between leaders—summit diplomacy—would isolate Egypt from her Arab neighbors, a long-standing goal of Israel.\nAccompanied by their capable negotiating teams and with their respective interests in mind, both leaders converged on Camp David for thirteen days of tense and dramatic negotiations from September 5-17, 1978. By all accounts, Carter\'s relentless drive to achieve peace and his reluctance to allow the two men to leave without reaching an agreement are what played the decisive role in the success of the talks. Numerous times both the Egyptian and Israeli leaders wanted to scrap negotiations, only to be lured back into the process by personal appeals from Carter. Begin and Sadat had such mutual antipathy toward one another that they only seldom had direct contact; thus Carter had to conduct his own microcosmic form of shuttle diplomacy by holding one-on-one meetings with either Sadat or Begin in one cabin, then returning to the cabin of the third party to relay the substance of his discussions.\nA particularly difficult situation arose on day ten of the talks. The issues of Israeli settlement withdrawal from the Sinai and the status of the West Bank created what seemed to be an impasse. Begin and Sadat were “literally not on speaking terms,” and “claustrophobia was setting in."" In response, Carter had the choice of trying to salvage the agreement by conceding the issue of the West Bank to Begin, while advocating Sadat’s less controversial position on the removal of all settlements from the Sinai Peninsula. Or he could have refused to continue the talks, reported the reasons for their failure, and allowed Begin to bear the brunt of the blame. Carter chose to continue and for three more days negotiated, arm-twisted, assured, and petitioned until at last an agreement was possible. The result was the Camp David Accords.\nTerms of the agreements\nThere were two 1978 Camp David agreements—A Framework for Peace in the Middle East and A Framework for the Conclusion of a Peace Treaty between Egypt and Israel—the second leading towards the Israel-Egypt Peace Treaty signed in March, 1979. The agreements and the peace treaty were both accompanied by ""side-letters"" of understanding between Egypt and the US and Israel and the US.\nThe first agreement had three parts. The first part was a framework for negotiations to establish an autonomous self-governing authority in the West Bank and the Gaza strip and to fully implement UN Security Council Resolution 242, thus anticipating the Palestinian National Authority established in 1996. It was less clear than the agreements concerning the Sinai, and was later interpreted differently by Israel, Egypt, and the US.\nThe second part dealt with Egyptian-Israeli relations, the real content being in the second agreement. The third part ""Associated Principles"" declared principles that should apply to relations between Israel and all of its Arab neighbors.\nThe second agreement outlined a basis for the peace treaty six months later, in particular deciding the future of the Sinai peninsula. Israel agreed to withdraw its armed forces from the Sinai and restore it to Egypt in return for normal diplomatic relations with Egypt, guarantees of freedom of passage through the Suez Canal and other nearby waterways (such as the Straits of Tiran), and a restriction on the forces Egypt could place on the Sinai peninsula, especially within 20-40km from Israel. Israel also agreed to limit its forces a smaller distance (three km) from the Egyptian border, and to guarantee free passage between Egypt and Jordan.\nThe agreement also resulted in the United States committing to several billion dollars worth of annual subsidies to the governments of both Israel and Egypt, subsidies which continue to this day.\nThe time that has elapsed since the Camp David Accords has left no doubt as to their enormous ramifications on Middle Eastern politics. Most notably, the perception of Egypt within the Arab world changed. With the most powerful of the Arab nations\' military and a history of leadership in Arab world under Gamal Abdel Nasser, Egypt had more leverage than any of the other Arab states to advance Arab interests. Sadat\'s alacrity at concluding a peace treaty without demanding greater concessions for Israeli recognition of the Palestinians\' right to self-determination incited enough hatred in the Arab world to bring about Sadat\'s assassination in 1981. Egypt was also suspended from the Arab League from 1979 until 1989.\nAlso, the Camp David Accords prompted the disintegration of a united Arab front in opposition to Israel. Egypt\'s realignment created a power vacuum that Saddam Hussein of Iraq, at one time only a secondary consideration, hoped to fill. His ambitions became visible in 1980 when he ordered the invasion of neighboring Iran (Iran-Iraq War), starting a chain of events that would later lead to an invasion of Kuwait in 1990 (followed by the Gulf War), then ultimately toppling of his own regime in 2003.\nLastly, the biggest consequence of all may be in the psychology of the participants of the Arab-Israeli conflict. The success of Begin, Sadat, and Carter at Camp David demonstrated to other Arab states and entities that negotiations with Israel were possible—that progress results only from sustained efforts at communication and cooperation. Despite the disappointing conclusion of the 1993 Oslo Accords between the PLO and Israel, and even though the 1994 Israel-Jordan Treaty of Peace has not fully normalized relations with Israel, both of these significant developments had little chance of occurring without the precedent set by Camp David.\nNew World Encyclopedia writers and editors rewrote and completed the Wikipedia article in accordance with New World Encyclopedia standards. This article abides by terms of the Creative Commons CC-by-sa 3.0 License (CC-by-sa), which may be used and disseminated with proper attribution. Credit is due under the terms of this license that can reference both the New World Encyclopedia contributors and the selfless volunteer contributors of the Wikimedia Foundation. To cite this article click here for a list of acceptable citing formats.The history of earlier contributions by wikipedians is accessible to researchers here:\nNote: Some restrictions may apply to use of individual images which are separately licensed.', 'At the White House in Washington, D.C., Egyptian President Anwar el-Sadat and Israeli Prime Minister Menachem Begin sign the Camp David Accords, laying the groundwork for a permanent peace agreement between Egypt and Israel after three decades of hostilities. The accords were negotiated during 12 days of intensive talks at President Jimmy Carter’s Camp David retreat in the Catoctin Mountains of Maryland. The final peace agreement–the first between Israel and one of its Arab neighbors–was signed in March 1979. Sadat and Begin were jointly awarded the 1978 Nobel Peace Prize for their efforts.\nA state of war had existed between Egypt and the State of Israel since the establishment of Israel in 1948. In the first three Arab-Israeli wars, Israel decisively defeated Egypt. As a result of the 1967 war, Israel occupied Egypt’s Sinai Peninsula, the 23,500-square-mile peninsula that links Africa with Asia. When Anwar el-Sadat became Egyptian president in 1970, he found himself leader of an economically troubled nation that could ill afford to continue its endless crusade against Israel. He wanted to make peace and thereby achieve stability and recovery of the Sinai, but after Israel’s stunning victory in the 1967 war it was unlikely that Israel’s peace terms would be favorable to Egypt. So Sadat conceived of a daring plan to attack Israel again, which, even if unsuccessful, might convince the Israelis that peace with Egypt was necessary.\nIn 1972, Sadat expelled 20,000 Soviet advisers from Egypt and opened new diplomatic channels with Washington, which, as Israel’s key ally, would be an essential mediator in any future peace talks. Then, on October 6, 1973, Egyptian and Syrian forces launched a joint attack against Israel. It was Yom Kippur, the holiest day of the year for Jews, and Israeli forces were taken entirely by surprise. It took more than a week for Israel to beat back the impressive Arab advances. A U.S. airlift of arms aided Israel’s cause, but President Richard Nixon delayed the emergency military aid for seven days as a tacit signal of U.S. sympathy for Egypt. In November, an Egyptian-Israeli cease-fire was secured by the United States.\nAlthough Egypt had again suffered military defeat against its Jewish neighbor, the initial Egyptian successes greatly enhanced Sadat’s prestige in the Middle East and provided him with an opportunity to seek peace. In 1974, the first of two Egyptian-Israeli disengagement agreements providing for the return of portions of the Sinai to Egypt were signed, and in 1975 Sadat traveled to the United States to discuss his peace efforts and seek American aid and investment.\nWhen talks with Israel stalled, Sadat made a dramatic journey to Jerusalem in November 1977 and spoke before the Israeli Knesset (Parliament). In September 1978, President Jimmy Carter invited Sadat and Israeli Prime Minister Begin to the presidential retreat at Camp David, Maryland, where dual peace accords were hammered out under the direction of Carter. Signed on September 17, the historic agreements provided for complete Israeli evacuation from the Sinai, laid the groundwork for the signing of a final peace agreement, and outlined a broader framework for achieving peace in the Middle East.\nSadat and Begin received the Nobel Peace Prize, and on March 29, 1979, a permanent peace agreement was signed that closely resembled the Camp David Accords. The treaty ended the state of war between the two countries and provided for the establishment of full diplomatic and commercial relations.\nAlthough Sadat was greatly praised in the West, he was widely condemned in the Arab world. In 1979, Egypt was expelled from the Arab League, and internal opposition to his policies led to domestic crises. On October 6, 1981, Sadat was assassinated by Muslim extremists in Cairo while viewing a military parade commemorating the Yom Kippur War. Despite Sadat’s death, the peace process continued under Egypt’s new president, Hosni Mubarak. In 1982, Israel fulfilled the 1979 peace treaty by returning the last segment of the Sinai Peninsula to Egypt. Egyptian-Israeli peace continues today.']"	['<urn:uuid:93035ac7-013d-4043-b929-c3d3dc667715>', '<urn:uuid:d4ec4632-d112-4ce0-a48b-2b92f657fcef>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T19:47:23.148022	10	60	2523
47	home garden watering system parts maintenance	A drip irrigation system consists of several key components: timer and filter attached to spigot, pressure control valve, distribution lines, control and safety valves, connectors, and drippers/emitters near the plants. For maintenance, it's essential to regularly check all working emitters, inspect for water leakage, and ensure uniform water distribution. Filters must be cleaned weekly, with sand filters requiring backwashing and screen/dice filters needing manual cleaning of trapped debris. The system may require chemical treatment using acid or chlorine if chemical impurities or biological materials cause blockage. Regular inspection for clogs and mineral buildup is necessary, as filters only work so well.	['Lush gardens are a wonder to behold. We all love seeing those rows of plants spring up and watch veggies grow big, not to mention seeing the joyous colors of the season in the delicate flower petals we’re lucky enough to enjoy.\nAll of this takes work, of course, but for many gardening is a toil of love. However, even if one loves tending to their garden, some tasks can be grueling for the gardener and the plants. Watering is one of those landscaping tasks that can take up more time and money than a gardener predicts.\nSprinklers and hand-watering seem to be the irrigation choice of every movie character ever seen. The opening shots of a “typical suburban home in summer” scene tends to include a person watering the outdoor greenery with a hose. And who could forget the ill-timed sprinkling system in Caddyshack? As universal as these solutions may seem on the big screen, they aren’t the only ways to irrigate your landscape, and they definitely are not the most efficient.\nDrip irrigation, sometimes called “micro-irrigation” after the tiny water droplets it disperses, is a pipe or hose system with parts called emitters that distribute the water directly to the soil and roots around a plant. The simplest version is a garden hose manufactured with small holes that allow water to seep or lightly spray out. Drip irrigation can save up to 50% of the water typically spent using hand-watering or sprinklers. Many more benefits exist for the homeowner who installs a micro-irrigation system.\nA drip irrigation system:\n• is ready-to-install by the home gardener\n• can be found at many gardening supply stores\n• maintains a good balance of air and water\n• is easily adapted to odd-shaped landscapes\n• has over a 90% water efficiency rate (sprinklers hit 50-70%)\n• may be exempt from restrictions in a water-restriction zone\n• minimizes water runoff by inserting water into the soil\n• avoids over-watering, which can damage and kill plants\n• does not damage delicate new growth\n• can be used in containers and raised beds\n• can be automated with an inexpensive timer found in supply stores\n• is expandable – can be added onto as the garden gets bigger\nParts of a drip irrigation system (in order from the pressurized water source):\n• Timer and/or filter, usually attached to spigot\n• Pressure control valve\n• Distribution lines (the “pipes”)\n• Control and safety valves along the lines\n• Connectors (if necessary)\n• Drippers or emitters (near the plants)\n• Depending on the type of system installed, the timer, valves and emitters can all be set to work and shut off automatically or they can be controlled manually. Some drip irrigation kits come with controllers for the entire system.\n• A bit of gardening logic is required. Emitters must be placed near the roots of the plants. If the emitters are spaced too widely apart, some of the seeds and roots won’t be watered. Knowing the spacing of the rows will assist in emitter placement.\n• Unlike sprinklers, evidence of a drip irrigation system’s work isn’t always so obvious. It can be hard to know if the system is doing its job well. Occasional checks of the soil may be necessary.\n• Filters only work so well. Minerals in the water may collect at different parts of the system. Maintenance includes checking for clogs and mineral buildup. Filtering the water does help avoid clogs. Look for a system that has self-cleaning emitters.\n• Tubes that sit above ground are always a tripping hazard. Don’t run drip irrigators over any pathway or walkway, and avoid laying tubing over any “desire paths” in the landscape. Burying the length of distribution line that must traverse a path is worth the time and effort.\nTry it this year\nDon’t be intimidated. A drip-irrigation system, while not the stuff of movies, is still just as glamorous. And it will be easier to install than you may think. Most kits sold come with detailed instructions for the homeowner to follow, but a person new to drip-irrigation can find more information online. Saving on your water bill also helps save the environment. And that will give us all many more years of gardening to come.', 'This Guide will teach you everything you need to know about Drip Irrigation.\nComponents Of Drip irrigation system\nCost of installation\nAnd Guidelines For Maintenance and much other valuable information that I’ve never shared anywhere else before.\nLet’s get started…\nThe available water for agriculture decreases day by day due to the increase in population, industrialization, and short rainfall. It has become essential to use modern irrigation technologies like Drip irrigation, sprinkler irrigation in agriculture.\nDrip irrigation means providing the required quantity of water directly to Crop plants’ root zone through a network of small pipes; this is also called micro-irrigation or trickle irrigation.\nThis is the most efficient irrigation technique.\nIn the Drip irrigation system, water supplies to plant roots through a collection of plastic pipes, lateral tubes, and valves. These components, controlled with the help of a dripper and water pump. With the help drip irrigation system, it is easy to provide liquid fertilizer to the plant root system.\nDrip Irrigation Advantages\n- Drip irrigation Saves water around 30% – 60% as compared to flood irrigation.\n- It is observed that crop yield increase up to 230%.\n- Fertilizer use efficiency increase by 30 percent\n- Reduce Weed growth\n- Save Labour and electricity cost is much significantly less than other irrigation methods.\n- For drip irrigation, Field leveling isn’t vital.\n- Easy to maintain Moisture near the root zone\n- Water supply is extremely optional, controlled by way of each nozzle.\n- Help to reduce Soil erosion.\n- Enable the use of saline water for irrigation\nComponents Of Drip irrigation system\nThe drip irrigation system comprises the water pump, filter unit, mainline, sub-mainline, lateral pipes, dripper, and other accessories such as control valves, pressure gauge, Fertilizer tank/venturi, end cap, etc.\n1) Water Pump\nA pump of a suitable capacity Water pump is used to supply water through the drip irrigation system components at a specific level of pressure.\nIf the source of a water supply is a bore well, open well, or a Canal, there is the possibility of organic and inorganic foreign bodies in the water. In this case, use the suction filter to get relatively clean water.\nThe electric motors or diesel engines are the common prime mover of the pump. Recently the solar pump is being used to popularize it for drip irrigation purposes.\n2) Filter unit\nThere should be a good quality filter in the control head installation of a drip system. The filter uses to clean the suspended impurities of water supplied by the pump before it reaches to drippers. Impurities in irrigation water may cause blockage of the holes and passage of drippers.\nThe success of drippers greatly depends on the performance of the filter.\nA filter unit cleans irrigation water impurities that are suspended and prevent blockage of holes. There is various type of filter available in the market.\nThere are Three Types of filters: Hydrocyclone Filter, Media filter, and Screen or Disk filter.\nSuitable filters are installed according to the impurities found in the source of water.\nIf the water source is a bore well with less physical impurities, you can install only a screen filter.\nIf the water source is an open well or a Canal install screen or disc filter along with sand filter and Hydrocyclone Filter.\nTo work the drip irrigation system properly two-stage filter unit is used.\na) Media (Sand/ Garval) Filters\nThese filters come as the primary filter unit, and this is powerful against inorganic suspended solids, biological materials, and other organic substances.\nMedia Filter consists of fine gravel and sand of selected sizes placed in a pressurized tank. It helps to remove organic substances such as algae and other vegetative substances present in the water.\nThe filters are made up of a circular tank filled with layers of coarse sand and different gravel sizes with a provision of valves or flushing the filter assembly in case of clogging.\nThe media filters are available in different sizes ranging from 500 to 900 mm diameter with an output of 15 to 50 Cu.M., respectively.\nA gravel filter or sand filter is vital for the open reservoir, even where algae growth occurs in the water source.\nb) Hydrocyclone Filter\nIf the irrigation water contains more sand particles, hydro-cyclone type filters are removed this sand particle by creating centrifugal force and spins out sand from water.\nThe Hydro-cyclones Filter generates a spinning activity causing the sand particles to separate the water and be trapped in a storage tank at the bottom of this device.\nHydro-cyclone type filters are produced in various sizes for different discharge capacities.\nc) Screen Filter:\nIn general, the screen filter consists of a single or double perforated cylinder placed in a plastic or metallic container\nfor removing the impurities.\nGenerally, 100 to 200 mesh screens are used in this type of filter. It must be cleaned and inspected periodically for satisfactory operation of any drip system.\nScreen filer is installed with or without a gravel filter, depend upon the quality of irrigation water. The screen filter is made up of non-corrosive plastic or metal substance.\nc) Disc Filter:\nDisc filter comes as part of the secondary filter unit. Multiple circular discs filter water. it is made up of high-quality plastic material\nThe mainline transfers the total amount of water for the irrigation system. It connects the different sub-mains to a water source. The main pipes are commonly made of flexible materials such as PVC (polyvinyl chloride) or plastics.\nThe mainline pipe passes water from the filtration unit to the sub-main pipe. This pipe’s diameter depends upon drips irrigation system flow capacity, normally 2.5 – 4 Inch diameter PVC pipe used as main-line.\nMainline & sub-main should be installed in a Telescopic manner; that is, the pipe with a larger diameter should be connected first, followed by pipes with a smaller diameter. This arrangement helps to maintain uniform pressure in the system.\nThe Mainline should be buried at least 45 centimeters to prevent them from getting damaged during cultural operation.\nThe sub-main feed to the laterals on one or both sides. It is made of either medium-density polyethylene (PE) or PVC. There should be a balance between the diameter of the main and sub-mains.\nThese are determined in consideration of the rate of discharge, number of sub-mains, and friction losses in pipes\nLaterals are made up of low-density polyethylene (LDP) or linear low-density polyethylene (LLDPE) material and are available in different sizes, 12 mm, 16mm, and 20 mm.\nBased on the availability of water, crop, and spacing, 12 millimeter 16-millimeter laterals install.\nDrippers are also called emitters. The dripper is discharging water from the lateral pipe to the soil.\nDrippers are generally manufactured from polypropylene materials.\nMainly, two types of drippers are available in the market. Online Dripper and Inline Dripper\na) Online Dripper:\nIn this type of dripper, they are placed on the lateral, called an online dripper. these drippers are fixed on the lateral by punching suitable size holes in the pipe\nThis type of dripper uses mostly for Orchard Crops like coconut, pomegranate, guava, etc., the capacity to discharge water available is 2L/hour, 4L/hour, and 8L /hour.\nb) Inline Dripper:\nIn this type, the dripper is placed inside the lateral pipe. The distance between the two drippers is the same. There are three types of available inline drippers.\nNon-Pressure Compensating Drippers (NPC): This is a very simple dripper, which does not maintain uniform pressure\nPressure Compensating Drippers (PC): This dripper is more advanced; it maintains uniform pressure in all drippers. It mostly uses for the greenhouse crop, the price of this dripper is slightly high.\nNon-Draining Drippers: This dripper is mainly used in a soilless-like mediums like cocopits, Perlite, and Vermiculite.\n7) Fertilizing Unit\nThe direct application of fertilizer through drip irrigation has increased the efficient use of fertilizer and saving in labour and money.\nWith this fertilizing unit’s help, liquid fertilizer is provided to the plant through a drip irrigation system.\nApplication of fertilizer into irrigation system is made by either a by-pass pressure tank or by venture pump or direct\nIt is used to determine water pressure in the drip irrigation system.\n8) Controls Valves\nThis value is used to control water flow. They made up of plastic and iron material\n9) Flush Valve:\nThe flush valve is placed at the end of the sub-Maine pipe it uses to flush out dirt.\n10) Non-Return Valve\nNon -return valve is useful to stop return water toward the water pump.\n11) Air valve\nIt helps prevent the sucking of mud by the drippers and release air into the drip irrigation system.\nThe end cap is used to close one end of the lateral pipe; they remove it at the cleaning time.\nTypes of Drip Irrigation System\nTheir many types are available in drip irrigation here. Only two popular types are explained.\n1) Surface drip system\nIn the surface drip system, the emitter and lateral pipe are placed on the soil surface. This is the most common and popular type of drip system.\nIt is suitable for wide-spaced plants as well as for row crops. It is easy to use a surface drip to observe and inspect, change, clean the emitters, observe surface moisture patterns, and measure individual emitter discharge rates.\n2) Sub-surface drip system\nin a subsurface drip irrigation system, the lateral is placed below the ground and near the plant root zone area. In this system, water is slowly applied below the surface through the emitters.\nSub-surface drip systems have gained wider acceptance due to removing earlier problems of clogging to a large extent.\nDue to the sub-surface drip method, there is less intervention in agriculture or any cultural practices and possibly more operational life.\nSub-surface drip system provides little interference with cultivation or any cultural practices and possibly longer operational life.\nDrip Irrigation System suitable for this crop\n|Vegetables||Tomato, Capsicum, Cabbage, Chilly,|\nCauliflower, Onion, Brinjal, Bitter Gourd,\nRidge Gourd, Peas, Cucumber, Pumpkin,\n|Cash Crops||Sugarcane, Tobacco, Cotton|\n|Poyhouse crop||Gerbera, Dutch rose, Carnation, Anthurium,|\nLily, Orchids, Strawberry, etc\n|Plantation Crop||Coffee, Coconut, Tea, Rubber, etc|\n|Orchard Crops||Banana, Grapes, Citrus, Orange,|\nPomegranate, Mango, Guava,\nPineapple, Cashewnut, Coconut,\nPapaya, Watermelon, Muskmelon,\nLitchi, Lemon, etc\nDrip Irrigation System Cost\nThe Cost of drip irrigation system installation depends upon various factors, like what kind of crop you are sowing, type of terrain, quality of soil, sowing pattern, water quality, quality of drip material, manufacture company of drip irrigation system, and design of the drip irrigation system.\nDrip irrigation system cost per acre For vegetable crop will come around Rs. 50,000–65,000 per acre approximately and For fruits crop, if planted in the 3X3 pattern cost per acre for drip irrigation system come around 35,000- 40,000 approximately.\nGovernment Suggestive cost of drip irrigation system:\nSuppose you are using non-ISI material; your initial cost for one acre comes around 20,000-25,000 rupee for the vegetable crop, but the life of non-ISI material for 2-3 years with high maintenance cost. At the same time, the life of ISI material is 7-10 years with low maintenance.\nGovernment subsidy for drip irrigation\nThe subsidy is available for Drip irrigation in India under Pradhan Mantri Krishi Sinchayee Yojana (PMKSY). The government of India gives subsidies per beneficiary up to 5 hectares. There has been financial assistance under different categories and depending upon States’ categories for dessert, drought, hilly, and other regions of the country. For more detail, check the PMKSY website or contact the nearest agriculture office.\nList of Top Drip irrigation companies\nThese are the Top Drip irrigation manufacturer company in India.\nGuidelines For Maintenance Of Drip irrigation system\nDrip irrigation is a mechanical system, which performs at a specific pressure of water; if the system has to perform well for a long time, the pressure should be properly maintained.\nEven if good quality materials are used and installed scientifically, there is a possibility of failure. If the field maintenance is not proper and regular, therefore, the drip irrigation system’s management is very important.\nFollow these guidelines to maintain drip irrigation system –\nA. General care\n- Check all Working emitters or Dripper, Inspect the water leakage, and make sure that water is uniformly distributing all of the corners of this plot/field.\n- Check the water dispensation area. If their dry patches are found, increase drip irrigation operation time.\n- If a twist, fold, cut, puncher is observed on the lateral, sub-main, valve correct it immediately.\nB. Clean the filters.\nThe filter is the main part of the drip irrigation set. If the filter is not working properly, there is a major chance of complete drip irrigation system Failure.\n1. Sand filter:\nClean sand filter after every week. The backwashing facility available in the sand filter uses this by allowing water to flow through the lid instead of the water valve and stir the sand in the filter. So, the waste which is sitting at the bottom will come and go out of the water\n2. Screen Filters/ dice filter:\nOpen the lid and remove the mud and garbage trapped. Open the filter and remove the filings and rubber seals and clean it on both sides.\n3. Submain pipe and Laterals tubes:\nSometimes a fine particle, dirt passes through the filter and store in the main and sub-main tubs, Laterals; therefore, to clear these tubes, remove the end cap, Flush Valve and let the water flow. Do this until the clean water come.\nC. Chemical Treatment:\nThe drippers are stopping working due to various alkali\nIf the amount of carbonate, bicarbonate, chlorides, sulfur, manganese, and calcium, sulfur sulfate is higher in water, yellowish dots appear, and the reddish dots appear dripper, and if the amount of iron is high red spot appear on the dripper following treatment carried out.\n1. Acid Treatment:\nAcid treatment is useful for though dripper and laterals blocked by various chemical impurities, including fertilizer residue. This impurity can be removed by treating the system either with hydrochloric acid or sulphuric acid or nitric acid. This hydrochloric acid at 25% is best for acid treatment.\nTake the required amount of hydrochloric acid in water. Inject it into the system through the Venturi or fertigation tank. The system is filled with water allows acid solution into the system until the pH of 4. A check pH with litmus paper for both at the starting and the last dripper. Close the system for 24 hours.\nThe acidified water in the system reacts with the salts deposited in the system and dissolves it. After 24 hours, they acidify water along with the dissolved solids coming out of the system by flashing.\n2. Chlorine treatment:\nChlorination is useful to remove the biological impurities collected in the drip irrigation system. Chlorination can be done either using calcium hypochlorite, sodium hypochlorite, chlorine or calcium hydrochloride, or bleaching powder.\nDissolve the required quantity of bleaching powder in water one day before treatment. This solution is let into the system through Venturi or fertigation tank and allows it to stay in the system for 24 hours. after that, open the end caps of the lateral and run the system for about an hour so that the impurities are thrown out of the system\nDrip Irrigation Disadvantages/ Limitation\nDespite observed successes, some problems have been encountered in applying water with drip equipment for some soils, water qualities, and environmental conditions. Some important limitations are described below:\n1) Persistent Maintenance Requirements\nDripper clogging is considered the most serious problem in Drip irrigation unless preventive measures are taken. It is, therefore, necessary that water should be filtered properly.\nBesides this, salts and chemical deposits can be deposited in an emitter or lateral pipe. This leads to Clogging. It will adversely affect the rate and uniformity of water application, increase maintenance costs, and result in crop damage and decreased yield if not detected early and corrected timely.\nOther maintenance problems include pipeline leaks and cracking of the tubes. Rodents, coyotes, rabbits, and dogs can chew and damage dripline, and ants and other insects occasionally have enlarged openings in drip tubings.\n2) Economical – Initial cost\nAs the equipment requirements are numerous with drip irrigation, initial investment and annual costs may be high compared to surface or portable sprinkler irrigation systems.\nThe actual prices of drip irrigation system equipment will vary greatly depending on the types of crops, grades of pipelines, filtration equipment, and fertilization equipment, etc.,\n3) Salinity Hazards\nIf the drip system is used under saline conditions, it should be taken extra care to manage drip irrigation work properly.\n4) Technical knowledge\nHigh skill is required for drip irrigation system design, installation, and subsequent operation.\nTechnical improvements in the designs of emitters, fittings, filters, etc., have been necessary;\nThe development procedures for preventing or correcting emitter clogging and equipment failure have been difficult, and the development of the proper methods for injection of fertilizers and other chemicals has sometimes been a\nA higher level of design management and maintenance is required with drip than other irrigation methods.\nImage source :']	['<urn:uuid:13d4c4f8-c639-4fcf-bf98-09e22a10c28c>', '<urn:uuid:985313b8-574b-4533-abaa-a71d7519e156>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	6	101	3550
48	How did jade crafting evolve through Chinese history?	Jade crafting in China evolved significantly over time. During the Western Zhou period (9th century BC), artisans were already creating sophisticated pieces like the huang pendant with intricate dragon and human head designs using openwork techniques. The craftsmanship continued to advance through various dynasties, with artisans spending increasingly more time creating complex designs. During the Bronze Age, jade was primarily used for practical items like weapons and axes, as well as luxury objects like cups and buttons. By the Han Dynasty (202 BC-220 AD), jade work had become so refined that entire burial suits for emperors were made from jade pieces. The evolution continued through the dynasties, with designs becoming progressively more intricate, eventually reaching the recognizably modern Chinese designs of the Qing Dynasty (1636-1911).	"['Lot 22: HUANG PENDANT WITH DRAGONS AND HUMAN HEADS\nDecember 3, 2016\nVienna, Vienna, AustriaLive Auction\nJade. China, Late Western Zhou period, 9th century BC\nThis huang or arched-shaped pendant offers a good example of the level reached by the jade carving tradition at the end of the Western Zhou period. The ornament is carved from a very good quality of green jade with no major inclusions, so that the colour is even: the only exception are the whitish patches scattered here and there and the ivory-coloured areas near the edges, where the stone has altered during burial. Both sides are incised with the same pattern: two stylized and coiled dragons with big heads carved near the edges, coupled with two human heads engraved as mirror images on the sides of the central axis. The pattern is more clearly seen when the huang is oriented with the extremities pointing up: in this case, the indentations carved on the border and at the edges become consistent element of the dragons\' heads: the short tips look like horns while the v-shaped projections on the short sides recall a stylized crest. The pattern is rendered through double incised lines, which give more emphasis to the motifs: the jade has also been painstakingly abraded around certain details - the eyes of the figures, the scrolls of the dragons\' snouts and the ears of the human faces - so that the motif appear more vivid and as in low relief, especially when the light hits the surface of the jade at certain angles. A number of regularly spaced cut-outs help to visually differentiate the motifs and emphasize the outline of the dragons and human mouths. A fault line running vertically through the head of a dragon is carved on one side of the huang and near one edge: it was probably created by mistake during the initial cutting and shaping of the ornament.\nIt is at the end of the Western Zhou and during the transition to the following ""Spring and Autumn"" period (770-475 BC), as the first half of the Eastern Zhou rule is known, that the openwork technique started to be used with increased frequency on carved jades. Examples from the archaeological records of huang pendants worked in openwork are offered by the jades found in the richly furnished tombs of the marquises of the Jin state and excavated at Tianma-Qucun, Beizhao, Quwo, Shanxi province. Particularly relevant is the large pectoral discovered in tomb M31 of this site: it is formed by six huang alternated with chain of beads and three of the arched pendants are carved in openwork. Further examples have come to the light from the tombs excavated in 2005 and 2006 at Yangshe village, Shicun, still in Quwo county, Shanxi province, which have yielded jades worked in openwork, including huang pendants. The archaeological report, with pictures of some of the excavated objects, can be read at this link:\nThe closest comparable example from private collections is a very similar huang pendant carved with the same pattern of dragons and human heads and cut-outs, in the Aurora Foundation collection (Taiwan and China, with its own museum in the city of Shanghai), image at this link:\n人龙纹玉璜- 西周晚期, 公元前9世紀\n宽 14.3 厘米\nWIDTH 14.3 CM\nFrom a German collection\nAll jades in this catalogue will soon be published in the forthcoming jade book by FILIPPO SALVIATI: ""THE MYSTERIOUS STONE - Archaic and Antique Chinese Jades from Neolithic to Han from Private Collections"".\nProfessor Salviati teaches Chinese Art at the University ""La Sapienza"" in Rome, Faculty of Oriental Studies, and is one of the world\'s most recognized jade experts, specialized in archaic jades until the Song Dynasty. Hi-resolution pictures of all jades are to be found in our web-site, and can be enlarged so that handwork, age-related traces of weathering, as well as colors are very well recognizable.', '- China Tours +\n- Create My Trip\n- Destinations +\n- Travel Guide +\n- China Visas\n- The Great Wall of China\n- China’s Top 10 Attractions\n- Giant Pandas\n- The Terracotta Army\n- Best of China\n- Culture +\n- Asia Tours\n- Day Tours\nJade (Yu in Chinese Pinyin) has always been a favored gemstone in China, considered a lucky charm and full of virtue. It is still the most popular common gemstone and can be bought all over China.\nThere are two types of minerals that are called jade. The kind that was known in ancient China is called nephrite. The other kind of jade mineral is called jadeite. Nephrite is a little tougher against fracture than jadeite, but it is a little softer.\nFor some reason, cultures around the world have thought that both types of jade have special virtues or powers. The Chinese thought so too, and for thousands of years, jade articles had a special value and significance, and skilled artisans carved increasingly intricate designs.\nMaybe because it was so rare in China, yet useful for its toughness, nephrite became the status symbol of the rulers, considered imperial stone.\nJade is still a popular gemstone in China, so jade pieces can be bought everywhere. Many people, especially boys or young men, wear jade pendants.\nMany Chinese believe that jade will protect them and give good health. Jade pendants are also a sort of a Chinese religious symbol.\nJade also holds a deep symbolic meaning, conveying beauty, protection, power, and immortality. Even today it\'s often the choice for a lucky charm for children, in the form of a pendant or bracelet.\nWhen shopping for expensive jade articles in China, be wary, and perhaps take the article to a credible gemologist before buying a piece.\nThese days, fake jade articles can be easily produced from hard plastic, glass, or various kinds of minerals, such as quartz or serpentine. Many mineral species look like nephrite and jadeite.\nJade may also be chemically modified, which makes it worth less than natural jade. Determining whether an article is made of true nephrite or jadeite takes some training and gemologist equipment.\nThere are two types of Jade, nephrite and jadeite. In China, jadeite is now more popular and expensive than the softer type, nephrite.\nNephrite (ruan yu, 軟玉, soft jade) is a kind of amphibole mineral. It has a hardness of 5.5 or 6. This is hard, but not as hard as quartz. Therefore the ancient Chinese craftsmen could use quartz sand (regular sand) or pieces of quartz to carve jade.\nJadeite (ying yu, 硬玉, hard jade) is only found in about 12 places on the Earth. It is a kind of pyroxene mineral.\nIt looks like nephrite, but it is harder, polishes brighter, and comes in a greater range of colors. It has a hardness of about 6.5 or 7, about the same as that of quartz. This means that carving and polishing jadeite is more difficult.\nThe stone-age Indians in Central America made jadeite articles, but it is said that carving even simple objects took a long time. Jadeite was found in Europe, but perhaps very little was imported into China.\nNow the majority of the world\'s jadeite comes from Burma.\nIn the West, diamonds, rubies, and emeralds were the expensive gemstones worn by high-ranking people as status symbols, but in China, nephrite (ruan yu, 軟玉) was regarded as a status symbol, and was used for jewelry and expensive decorations.\nThe character for jade 玉 looks almost like the character for emperor 王, with the addition of the small stroke on the bottom right that might represent a stone. The character for jade may mean ""the emperor\'s stone"".\nDuring the Chinese Bronze Age and before, jade was valuable as a material for making weapons and axes. At the time, durable luxury objects, such as cups and buttons, were also made from jade.\nIts scarcity, because it had to be imported from Xinjiang and other faraway places, and the powers it was thought to possess to bring health and cure diseases, may be why it came to be considered the gemstone of the Emperors.\nIt is interesting that nephrite was imported from as far away as the region of the Taklamakan Desert, in what is now Xinjiang, 3,000 years ago.\nAt that time, Caucasian people lived in the area. It may be that these people traded with the Shang Dynasty (1766 -1122 B.C.) of China, because all the nephrite pieces found in the Shang Dynasty tombs are from that area of Xinjiang.\nDuring the Han Dynasty (202 BC–220 AD), the Emperor\'s burial suits were made entirely of jade pieces that were pieced together.\nAs the various empires came and went, the intricacy of the carvings increased. Craftsmen spent more and more time to carve more intricate designs.\nJadeite wasn\'t mined in China,Mongolia or Xinjiang, so when green jadeite from Burma became available after 1800, during the Qing Dynasty, it became the preferred imperial form of jade.\nLooking at nephrite articles of the various eras, the designs progressively became recognizably more like modern Chinese designs, like the ones from Qing Dynasty (1636–1911).']"	['<urn:uuid:a9279cdc-7a40-433a-ab38-47165695ce6f>', '<urn:uuid:080e8af0-7ee7-4d52-877e-eb70a69e9ce5>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	8	125	1510
49	As a municipal sustainability director, I'm interested in learning about the maintenance requirements - how do the upkeep needs differ between Phoenix's cool pavement initiative and Nashville's urban tree program?	Phoenix's cool pavement requires evaluation of product performance and life cycle through a one-year study, while Nashville's tree program demands more intensive ongoing maintenance, specifically requiring regular watering for the first two years, with young trees needing approximately one inch of water per week and additional pruning services, which costs $90 per tree annually through their maintenance program.	"['Healthy Urban Environments\nSouth Phoenix streets get reflective coating to reduce excessive heat\nThe second phase of Phoenix’s Cool Pavement Program in south Phoenix has kicked off. The Office of Heat Response and Mitigation is part of this effort, with $2.8 million allocated for\nCity of Phoenix adopted the 2021 Climate Action Plan\nPhoenix Mayor Kate Gallego and the City Council have adopted a Climate Action Plan roadmap to cut emissions and build a more resilient city by approving the City’s comprehensive action\nHUE partnership with City of Tempe for mobile cooling trailer looks forward to future\nHUE and the City of Tempe unveiled a new project that will not only serve as a place of shelter from the brutal summer heat in the Valley of the Sun, but\nExtreme heat and public health podcast\nJennifer Vanos, a HUE partner and Assistant Professor at the School of Sustainability, College of Global Futures, and Rachel Braun, a Postdoctoral Research Associate with HUE, have been featured in Come Rain\nSilent killer: The rising problem of extreme heat in the U.S.\nCongratulations to Melissa Guardaro, PhD, a HUE member, Knowledge Exchange for Resilience and Assistant Research Professor at the Global Institute of Sustainability and Innovation, for testifying in front of the\nA double heat and housing crisis in Phoenix\nIn the June 20 edition of The New York Times, writer Jack Healy visits Phoenix to explore how the region is addressing a housing shortage while in the midst of\nDo trees provide the best shade for urban environments?\nShade is a term that residents of arid, hot environments learn to appreciate, especially during scalding summer months. But what makes for the best shade? “Cities have started to plant\n2020 brings record heat and dryness to Arizona\n“The heat in 2020 was not helpful in the least, and the global pandemic was not helpful as well,” said associate professor David Hondula, a partner with the Healthy Urban\nTransforming Phoenix into an ‘urban forest’ to combat extreme heat\nA State Press article detailed how the Healthy Urban Environments Initiative partners are working with Phoenix city officials to reduce temperatures across the Valley after a record-breaking summer. Reducing temperatures\nCool pavement pilot study\nA joint study between Arizona State University researchers — led by Ariane Middel and Jenni Vanos — and the City of Phoenix, and sponsored by the Healthy Urban Environments Initiative, will quantify and evaluate the effectiveness of the CoolSeal product in mitigating urban heat considering various heat metrics (air temperature, surface temperature and radiant temperature). This one-year project will also assess the product performance and life cycle.\nAZ Heat Preparedness and Resilience Workgroup\nThis Workgroup was created in the summer of 2020 to share heat forecasts and warnings with communities; highlight approaches to heat relief, communications strategies and resources; identify opportunities and gaps\nEvent Sept. 3: Killer Heat in COVID Times\nLast month, Phoenix broke its record for the most days at 110-plus degrees, while being the world’s hotspot for coronavirus. This case critical discussion brings together ASU, the City of Phoenix, as well as a local nonprofit and a national NGO, to discuss the compounding crises of extreme heat and COVID-19.', ""Why Are Trees Important?\n- Trees slow down storm flow.\n- Trees reduce storm water runoff.\n- Trees filter pollutants.\n- Trees cool urban heat islands.\n- Trees beautify our communities.\nWhat Can Trees Do For Nashville?\nNashville’s impervious surfaces have increased by 20% over the past 2 decades in urban areas at a cost in excess of $100 billion nationally. Local governments are increasingly looking toward non-built storm water management strategies, including trees to reduce the cost of constructing storm water control infrastructure.\nSome of the intercepted water evaporates back into the atmosphere, and some soaks into the ground reducing the total amount of runoff that must be managed in urban areas. Trees also slow storm flow, reducing the volume of water that a containment facility must store. For example, in the Metropolitan Washington DC region, the existing 46 % tree canopy reduces the need for retention structures by 949 million cubic feet, valued at $4.7 billion per 20-year construction cycle (based on a $5/cubic foot construction cost).\nWhat can I Do For Nashville's Urban Canopy?\n· Help Nashville get more roots in the ground! The Cumberland River Compact, Nashville Tree Foundation, and many other environmental non-profits in Nashville offer free trees from September – April each year.\n· Keep an eye out for Emerald Ash Borer and alert a Metro Tree Advisory member if you spot it! This tiny terror is responsible for wiping out Ash trees all over North America and is expected to cost Tennessee billions.\nContact Root Nashville\nfor more information!\nOUR COMMITMENT TO RESTORING NASHVILLE’S URBAN CANOPY\nTrees are a remarkably effective and low-cost solution to water pollution, so we are working with the Nashville Tree Foundation, the Tennessee Environmental Council, several Metro Nashville Departments, and a conglomerate other citizen groups to collectively restore Nashville’s tree canopy.\nThe plan is to plant 500,000 trees between now and 2050, and we have committed to being responsible for at least 10,000 trees between 2016 and 2021. So far, we have planted approximately 6,000 trees and we plan to meet (and possibly exceed) our target of ten thousand before 2021!\nURBAN CANOPY MAINTENANCE PROGRAM\nThe restoration of our urban canopy is dependent upon regular watering of newly planted trees—especially during the first two summers. A recent survey of park trees planted in 2015 and 2016 showed a 60% survival rate; which indicates that newly planted trees with no maintenance plan in place are at a higher risk of fatality. So, we decided to start a program to do just that!\nWe officially launched the long-awaited Urban Canopy Maintenance Program in December of 2017! This program was established to help care for young trees all over Nashville, and anyone can enroll their trees. Each maintenance package includes one year of watering, pruning, and data collection–and for every paid maintenance package, we are donating a year of maintenance to a tree in the area with greater impermeable surface area, lower canopy coverage, higher average surface temperatures, or socioeconomic disadvantage. There are thousands of trees planted every year in Nashville, and they need a lot of water in their first few years of life to establish a healthy root system. Unfortunately, they often don’t receive the care they require to flourish or even survive. The goal of the Urban Canopy Maintenance Program is to increase their survival rate so that Nashville can continue to restore and expand its urban canopy.\nPROGRAM FEES & BENEFITS\nFor each paid maintenance package, the Cumberland River Compact will donate one year of maintenance to a tree in need.\n$90/year per tree includes:\n- Regular watering\n- Pruning of suckers and shoots\n- Data collection & reporting\nSee if you qualify for\nThe Compact’s Tree Water Maintenance Program!\nEmail Heather Listermann\nwith the following info:\nNumber of Trees\nTree Age (Approx)\nTREE CARE TIPS\nHOW OFTEN DO I NEED TO WATER MY TREE?\nYoung trees need approximately one inch of water per week. During dry times it is important to water your tree at least once a week, sometimes even twice. Always water the tree slowly so the water has time to soak into the soil. As trees age, they are able to withstand a wider range of climate conditions—but it’s good to keep an eye on precipitation patterns, and water when in doubt.\nWHEN IS THE BEST TIME TO PLANT?\nThe best time to plant seedlings in the southern U.S. is between mid-December and mid-March, this will give roots time to settle into their new environment and prepare to supply water to the buds and foliage when warmer temperatures arrive. Temperatures that range between 35–60 degrees F are best for planting. Higher temperatures cause transpiration to increase and will dry the roots. Lower temperatures could freeze the roots, causing mortality.\nHow To Plant A Tree\nDetermine the best planting location, keeping in mind the location of overhead and underground utilities. Select an area large enough to handle tree at maturity.\nDig hole wider than what seems necessary in order to help the roots spread out more easily, but no deeper than the roots. Remove grass in immediate area to reduce competition for water and nutrients.\nDo not plant the tree deeper than its root collar (the bump just above where the roots start). Plant it higher than you think because it will probably settle a little over time. Keeping the root collar from being covered up by dirt helps reduce the chances of root/stem rot. Ensure that all of the roots point down and are not tangled before filling in the hole.\nFill with the soil that came out of the hole. Do not add any other types of soil or fertilizer if possible. If soil amendments are needed, be sure to mix the old and new soil together. Make sure that all air pockets are sealed by applying light pressure to the soil surrounding the seedling. Pack the soil firmly but not tightly.\nConstruct a small ring by mounding soil around the tree about 2-3 feet away from its base. This ring will act as a water basin by and will help capture and direct water towards the roots.\nWater tree thoroughly. Newly planted trees need about an inch of precipitation or 15 gallons of water per week.\nApply a layer of mulch 2 inches deep and 2-3 feet in diameter in a circular shape around tree–and avoid piling mulch up against the trunk. The mulch will keep moisture in the soil and keep lawn mowers and weed eaters away.\nMORE INFO ABOUT TREE PLANTING/MAINTENANCE:\nWould you like to learn more about trees? We have volunteer opportunities available throughout the year to help plant and care for young trees. For more information on how to support us in our quest to restore and protect Nashville’s urban canopy please contact Heather Listermann.""]"	['<urn:uuid:13b8ceec-61b8-4e43-8f00-b4288e7f2a21>', '<urn:uuid:fe58e4b6-2238-4135-99a5-e038c6fe773a>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T19:47:23.148022	30	58	1667
50	What is the minimum temperature requirement difference between a warm greenhouse and a typical greenhouse used for growing lettuce and sweet peas?	A warm greenhouse requires a minimum temperature of 55 degrees, while greenhouses for lettuce and sweet peas typically operate at 40°F (4°C), making a difference of about 15 degrees.	['- Air pollution\nThe term hydroponics denotes soilless culture of plants. The possibilities of this technique have received considerable attention in recent years. In hydroponics, an outgrowth of laboratory techniques long used by scientists, plants are grown with their roots immersed in a water solution containing necessary minerals or rooted in a sand medium kept moistened by such a solution. Soilless culture of plants is similar in principle but larger in scale. A typical hydroponics technique has plants supported in a bed of peat, wood fibre, or similar material, on a wire screen with the roots dipping into the solution below. Aeration of the solution is provided. In another method, the plants are rooted in a medium of sand or gravel contained in a shallow tank into which the solution is pumped at intervals by automatic control. Between pumpings, the solution drains slowly down into a reservoir tank. Hydroponic techniques are practiced on a small scale both out-of-doors and in greenhouses.\nOf the elements known to be necessary for plant growth, carbon, oxygen, and hydrogen are obtained by the plant from atmospheric gases or from soil water. The others are all obtained as mineral salts from the soil. The elements absorbed as salts—iron, manganese, boron, copper, zinc, and molybdenum—are required in minute quantities and are called the micronutrients. The principal elements that must be provided as dissolved salts in hydroponic techniques are nitrogen, phosphorus, sulfur, potassium, calcium, and magnesium. Numerous solutions have been devised to fulfill these requirements.\nCrop yields of some plants can be obtained fully equal to those obtained on fertile soils. Wide-scale crop production by hydroponics, however, would be economic only for certain intensive types of agriculture or under special conditions. Some greenhouse crops, both vegetables and flowers, are grown by this method. In regions having no soil or extemely infertile soil but with favourable climate, hydroponic techniques have been very useful; for example, on some of the coral islands of the Pacific.\nThe greenhouse is typically a structure whose roof and sides are transparent or translucent, permitting a sufficient quality and quantity of solar radiation to enter the structure for photosynthesis (see below Photosynthesis). It allows the growing of crops independently of the outside climate, since its interior temperature and humidity can be controlled. Greenhouses vary in size and complexity from small home or hobby structures to large commercial units covering an acre or more of land. An even smaller greenhouse might be termed the hot bed, a glass-topped box containing fermenting organic matter; the fermentation process yields heat, allowing the gardener to start plants from seed in early spring for later transplanting.\nThe basic construction of a greenhouse consists of a light but sturdy frame capable of resisting winds and other loads. Conventional foundations usually support vertical walls; the roof may be gabled, trussed, or arched. The conventional greenhouse is fitted with glass panes, but plastic-film or fibre-glass panels often supplant glass.\nMaintenance of temperature within the greenhouse is difficult because of fluctuating outside conditions. When the sun shines brightly, little heat is needed, and the heating system must be controlled in some way to prevent injury to the crop. Hot water, steam, electric cable, or warm-air furnaces provide the heat, which is usually controlled by thermostat. Temperatures in greenhouses are regulated to suit the crop. Typical ranges are from 40° F (4° C) for lettuce, violets, carnations, and sweet peas to 70° F (21° C) for cucumbers, tomatoes, and orchids.\nCooling is often required during summer days in warm climates. Ventilation is the simplest technique, reducing inside temperature to near that of the outdoors. Additional cooling by refrigeration may be required; in dry regions, the evaporative cooler is efficient and also increases the relative humidity within the structure. Another form of environmental control consists of adding extra carbon dioxide to the air if the crop requires it for extra photosynthetic efficiency.\nThe commercial-greenhouse operator usually grows vegetables or ornamental plants. Such production makes more demands on the grower, because he must assume many of the tasks normally handled by nature in the open fields. He must regulate the temperature, ventilate, adjust the amount of entering sunlight, provide soil moisture, fertilize, and even facilitate pollination. During the off-season, the structure must be cleaned and fumigated, its soil restructured, and mechanical equipment checked. Mechanization of greenhouse operations has lagged far behind the pattern of agriculture in general. Disease is a particularly serious hazard in greenhouse farming, requiring constant attention and use of chemicals.\nThe factor of weather\nThe interaction of weather and living systems is a basic aspect of agriculture. Although great strides in technology have resulted in massive production increases and improved quality, weather remains an important limiting factor. Though man is not yet able to change the weather, except on a very small scale, he is capable of adjusting agricultural practices to fit the climate. Thus, weather information is of utmost importance when combined with other factors, such as knowledge of crop or livestock response to weather factors; the farmer’s capability to act on alternative decisions based on available weather information; existence of two-way communication by which specific weather forecasts and allied information can be requested and distributed; and the climatic probability of occurrence of influential weather elements and the ability of the meteorologist to predict their occurrence.\nOther weather-research benefits\nApart from the many applications of weather forecasting to current problems, meteorological research may benefit agriculture in at least three other ways: (1) improved planning of widescale land usage depends partly on detailed knowledge of plant-climate interactions; radiation, evapotranspiration, diurnal temperature range, water balance, and other parameters are measured and analyzed before a plan realizing maximum economic benefit for a given area is prepared; (2) agronomic experiments are combined with climatological documentation to obtain the greatest scientific and technological return; (3) problems of irrigation, row spacing, timing of fertilizer application, variety selection, and transplanting can best be solved with the aid of climatic environmental data; cultural practices related to artificial modification of microclimates should be based on research knowledge rather than personal judgment.', 'A greenhouse is a building where plants can be grown. The main purpose of greenhouse is to grow both seasonal and unseasonal crops. Greenhouse can be built using different types covering material. Glass and plastic can be used for covering the greenhouse. Various equipments are required for building greenhouse such as heating, cooling, screening installation and sensor systems. Different varieties of flowers, plants and vegetables can be grown in greenhouse. Pests and disease should be avoided for the plants grown inside the greenhouse. Irrigation is necessary to provide sufficient amount of water to the plants. Temperature and humidity must be constantly monitored inside the greenhouse. High temperature may cause damage to the plants inside the greenhouse.\nBased on cost and space you want greenhouses can be classified. If you consider temperature as a main factor for your greenhouse then based on climatic condition at your place greenhouse can be classified. Three types of greenhouses exist if temperature is taken in to consideration. They are hot greenhouse, warm greenhouse and cold greenhouse.\nA hot greenhouse is generally used for growing tropical and exotic plants. Minimum of 65 degrees of temperature should be maintained inside the greenhouse for growing such type of plants. If you are living in a cold region, then you have to install special lights and heating elements. These equipments help in growing tropical and exotic plants.\nA warm greenhouse should have a minimum temperature of 55 degrees inside the greenhouse. Very large variety of plants can be grown using warm greenhouses. The plants which you can grow in outdoor region can be grown in warm greenhouse. During winter season, few lights and heating elements must be installed inside the greenhouse to provide proper sun light to the plants. Any kind of hybrid flowers and plants can be grown in warm greenhouse.\nA cold greenhouse should have a minimum temperature of 40 to 45 degrees of temperature. A cold greenhouse is perfect to use during summer season as it produces cool environment for the plants. Various kinds of cooling systems and exhaust fans can be used inside the greenhouse for this purpose. Use of light or heating elements is not required for the cold greenhouse.\nBased on the kind of material used to construct the greenhouse, they can be classified as glass greenhouse and plastic greenhouse. Greenhouse should be constructed in such a way that sun light should be radiated inside the greenhouse. Sun light is absorbed by the plants, soil or any other vegetation plant inside the greenhouse. Plastic greenhouse can be used for building portable greenhouses. Small greenhouses can be built using either plastic material or glass material. Plastic greenhouse is generally built by using PE film and multiwall sheet in PC or PMMA. The glass greenhouse contains material such as heating, cooling, lighting and screen installation. These equipments inside the greenhouse can be automatically controlled by a computer.\nThe air inside the greenhouse must be rotated to avoid exhaustion. For this purpose a small window can be opened. Automatic cooling systems can also be used for this purpose.']	['<urn:uuid:8a534cbf-ff00-4bbb-bf5f-586c34dcb224>', '<urn:uuid:acec7d15-2c93-4328-8d76-8025b5fe0b2b>']	factoid	direct	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T19:47:23.148022	22	29	1522
51	workplace wellness program culture health components	A healthy company culture integrates a total health model into every aspect of business practice. This includes career, emotional, financial, physical and social well-being. Specific components include flexible work schedules, worker autonomy in decision-making, reasonable health goals, social support, health-promoting policies, and a healthy physical environment with features like healthy food options, accessible staircases, walking trails, and treadmill workstations. Success stories like Dow Chemical show that promoting such a culture over the long term leads to improved employee health and contained company costs.	['Lately, there’s been some debate about whether workplace health promotion programs, more commonly known as wellness programs, work. To us, it’s similar to asking whether reviews, training programs, employee assistance services, or other company initiatives are effective for both worker performance and the bottom line. The honest answer is that some are successful while others fail. And most of the time this comes down to how they’re designed and executed.\nSo how do you create an evidence-based health promotion program that does work? And what can employers do to avoid common pitfalls that lead to ineffective and, in worse case scenarios, harmful initiatives?\nTo tackle these questions, our respective organizations (the Transamerica Center for Health Studies and the Institute for Health and Productivity Studies at the Johns Hopkins Bloomberg School of Public Health) prepared a report, “From Evidence to Practice: Workplace Wellness that Works.” It offers practical advice to employers, large and small, based on the latest research on workplace programs, expert advice from practitioners and candid interviews with business leaders.\nOne of the biggest lessons we learned in the process of creating the report is one-time events masquerading as health promotion programs – that is, activities not integrated into a comprehensive workplace health promotion strategy – are likely to fail. And there are five common ways these solitary initiatives tend to pop up in companies.\nAdministering health risk assessments only. Health assessments typically involve asking employees questions about modifiable risks, such as smoking behavior, physical inactivity, poor diet, and high stress levels. Oftentimes, these surveys are coupled with biometric screenings of blood pressure, cholesterol, height/weight, and blood glucose levels. But providing feedback reports that remind employees that smoking, not exercising, or being overweight is unhealthy does not motivate change unless workers are given the tools and resources to actually change and track their behaviors.\nUndoing decades of poor health habits won’t be achieved by asking employees to complete a 15-minute questionnaire. And for otherwise healthy employees, frequent biometric screening is often unnecessary, and from a clinical standpoint may do more harm than good because follow-up treatments can be unnecessary and costly.\nPaying people to change their habits. While financial incentive programs are popular, they may not achieve long-term behavior change; instead, they may lead to resentment and even rebellion among workers. This is because many traditional incentive programs are grounded on the assumption that people will behave in certain rational ways if paid to do so. Behavioral economics tells us otherwise: Sometimes people do things that are irrational and even counter to their best interests. Individuals may not focus on long-term benefits of a given action when a short-term reward (for example smoking a cigarette, consuming a large pizza, or spending hours watching television) is more appealing.\nWhile there is some evidence that incentives work in specific instances for a small subset of workers, there is little research on the use of financial incentives in achieving long-term lifestyle changes like losing weight and not regaining it.\nSending people to your health plan’s website. Surprisingly (at least to us), many employers think they’ve offered a wellness program if they direct their employees to a website made available by their insurer. These under-the-radar programs do not improve population health unless they are part of a broader comprehensive health promotion program that offers many ways to become engaged.\nIntroducing short-term campaigns. Biggest Loser-themed events or pedometer challenges are random acts of wellness and are not very effective. In fact, they may even do more harm than good by promoting quick fixes as opposed to long-term progress.\nHiring a vendor to “fix” unhealthy employees. Employers sometimes hire outsiders and call it a day. Worse yet, they’ll sometimes hire different vendors to address different issues – lifestyle coaches, employee assistance counselors, case and disease management vendors, nurse lines, occupational health and safety experts, workers’ compensation specialists, disability managers, organizational development consultants, you name it. When hired independently, these vendors often work in silos, which can result in overlapping or duplicated work. In addition, relying on outside entities to attend to organizational needs may not get at the root of a systematic problem.\nSo what does? We’ve identified five approaches that, while comparatively difficult, can actually change the health and lives of employees for the better.\nLeadership commitment and support. A successful health promotion program starts with a commitment from company leaders, and its continued success depends on ongoing support at all levels of the organization. In particular, leaders at companies with successful programs establish a healthy work environment by integrating health into the organization’s overall vision and purpose. At Lincoln Industries, a manufacturer and distributor of trucking accessories, promoting workers’ health and well-being is embedded in the company’s core mission and values. Senior leaders not only speak of its importance to the organization’s success, they lead by example.\nBuilding a culture of health. A healthy company culture is built intentionally. It is first and foremost about creating a way of life in the workplace that integrates a total health model into every aspect of business practice, from company policies to everyday work activities. By “total health” we mean a culture that’s supportive of career, emotional, financial, physical and social well-being – not just an occasional road race. Examples include offering flexible work schedules, giving workers latitude in decision-making, setting reasonable health goals, providing social support, enforcing health-promoting policies and establishing a healthy physical environment (healthy food offerings, staircases instead of elevators, walking trails in and outside buildings and treadmill workstations).\nThis, of course, takes time and support. A company like Dow Chemical is a success story in this way. The company has promoted a culture of health for more than 30 years, with countless peer-reviewed studies showing that employees’ health has improved and company costs have been contained.\nAsking for help. A workplace health promotion program cannot be imposed on workers as yet another management cost-containment initiative. Boosting engagement in wellness can only be achieved when workers own the program, understand how they and the company benefit, and are given a meaningful voice in its ongoing operation.\nThere are a few simple ways to start doing this. The most common approach is to conduct regular surveys or focus groups to determine which aspects of health and wellness are important to employees, and which initiatives are not a good use of time. Honest Tea discovered that employees were not interested in yoga sessions offered by the company and instead began a series of vigorous workouts that many of its younger workers wanted. Now participation exceeds 50% since this change and has helped workers become more actively engaged in the company’s wellness program.\nAnother approach is creating and supporting wellness committees. These groups of employees can be given a budget to come up with initiatives supported by their co-workers. Lastly, it may also be worth involving spouses or other family members who can help build a broader web of social support.\nSpreading the word. Strategic communication leads to greater engagement in employee wellness programs. This boils down to getting clear messages out to workers: this is what the program entails, here is how it works, here’s what’s in it for you, and here are ways to get involved. This can help overcome some of the top barriers to program participation and success: lack of awareness, lack of interest and suspicions about employers’ motivations.\nThese communications must be frequent, varied in content, multi-channel, and tailored to the target audience so that it doesn’t fade into background noise.\nFor example, USAA describes its communications with workers as relentless and surround sound. Wherever employees turn, they are reminded that the company cares about their health and wants to support their efforts. The messages are clear – this program is there to serve you, your family and our customers, whom rely on you to be positive, healthy and performing at a high level.\nOffering smart incentives. As we’ve already noted, simply paying people to change life-long habits may not work. However, there is strong evidence that proper incentives drive participation rates, keep employees engaged and motivated to begin efforts to achieve self-determined health goals.\nThe challenge is to migrate employees from simply participating for a reward (external incentive) to a place where the new behavior or habit is sufficiently satisfying and worth maintaining (internal incentive), such as taking a walk daily while listening to music or a favorite podcast. At NextJump, teams participate in a weekly Fitness Challenge where virtual cash rewards for the winning teams are coupled with bragging rights, creating camaraderie and social cohesion among workers. The company has found that motivating employees to fit in a workout during the workday gives them more productive energy and is helping drive better performance. Employees feel good, are happier, establish close partnerships with their office mates, and at the end of the day find work fun and personally rewarding.\nMeasuring the right things. Program evaluation is critical to maintaining accountability for a wellness program. To do this well, develop an evaluation plan at the start of a program so that useful baseline data collection can occur and be monitored over time.\nSo what should you measure? There are generally two answers: return on investment (ROI) and value of investment (VOI). ROI in this context is generally limited to examining the tangible benefits of a program, such as a reduction in medical costs or absenteeism. Fortunately, a robust scientific literature review supports the conclusion that well-designed and well-executed programs can produce a positive ROI along with significant improvements in population health.\nJohnson & Johnson, for example, has published dozens of studies in academic journals over the past three decades showing its wellness and prevention programs have improved employees’ health, saved the company millions of dollars and enhanced workers’ productivity – something they could only conclude after the smart collection and analysis of data.\nIn our view, ROI in isolation fails to capture the full benefit of workplace health promotion. VOI calculations, on the other hand, allow employers to examine the broader impact of programs and their impact on core priorities for their organization, which may include improved employee morale, talent attraction and retention, enhanced company loyalty and heightened customer loyalty.\nThere are a lot of misconceptions about wellness programs out there. As a result, many leaders pick and choose options fairly blindly, doing their employees and their company a disservice. In the end, you don’t necessarily need the latest wearable or a new vendor. To achieve very real health improvement at the workplace, employers should first understand what the evidence says about what works, and then weave together individual health promotion programs with organizational change interventions that build on and support a healthy company culture. This isn’t always easy. But the rewards can be huge, both for your company and for your employees for years to come.']	['<urn:uuid:40f23f08-7d25-4ee4-a1fe-dfde90fdca64>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	expert	2025-05-12T19:47:23.148022	6	83	1806
52	How do OCSD and LLCD laser systems compare for space communication?	The OCSD and LLCD laser systems are different in their approach and capabilities. OCSD CubeSats achieved data transmission rates of 100 Mbps from space to ground, using a hard-mounted laser that requires the entire satellite to rotate for pointing. Meanwhile, LLCD demonstrated much faster speeds of 622 megabits per second for download and 19.44 megabits per second for uplink between the Moon and Earth, using a system of four separate telescopes on the ground to compensate for atmospheric disturbances.	['The Optical Communications and Sensor Demonstration (OCSD) mission, funded by NASA, launched two 1.5-unit AeroCubes on Nov. 12, 2017 as secondary payloads on a resupply mission to the International Space Station (ISS). Their goal is to demonstrate high-speed optical (laser) communications and proximity operations.\n“These CubeSats are really breaking ground and we are excited to see where this research will lead in several different technologies and mission areas,” said Dr. Siegfried Janson, one of the principal investigators for OCSD. He and the Aerospace small satellite team designed and built the OCSD spacecraft, and are now taking them through their paces on orbit.\nVideo: OCSD Demos Laser Comms and Proximity Ops\nTo demonstrate the laser comm system, the OCSD CubeSats are transmitting to a telescope in Aerospace’s El Segundo, Calif., laboratories. The team has received data from the satellites at a rate of 100 Mbps, but expects to exceed that soon.\n“Space-to-ground optical communications from a free-flying CubeSat has never been done before, and we’re excited to be involved in this demonstration,” said Todd Rose, the OCSD optical communications lead.\nThe laser is hard-mounted to each OCSD satellite, which means that in order to point the laser, the entire satellite must rotate. This unique design simplifies the laser comm system since beam steering mirrors are not required, but the satellite now has to accurately point while downloading data.\nThe team developed a highly accurate attitude control system, including tiny star trackers, which allow the spacecraft to point to an accuracy of 0.025 degrees, 40 times better than was previously possible for this size satellite.\nThe OCSD satellites also completed a proximity maneuver as part of their technology demonstration. The spacecraft, which are zipping around the Earth at a speed of about 5 miles per second, were able to approach each other until they were only 20 feet apart.\nThe OCSD pair determined where they are in relation to each other by using on-board GPS receivers. Inexpensive cameras, beacons, and laser rangefinders will be used in upcoming proximity demonstrations.\nTo control their movement, they have a novel propulsion system, designed at Aerospace, that uses water as a propellant. The CubeSat has a thermal design that keeps the water from freezing, and then the thrusters spew it out as steam when propulsion is required.\nNormally, secondary spacecraft on an ISS supply mission are not allowed to have a propulsion system, to avoid causing problems for the main mission. OCSD was granted an exception, however, due to the safe nature of the water propellant.\nThe technology flown on OCSD is exciting and innovative. The laser communication demonstration is merely the beginning. Once the concept has been tested and verified on orbit, future missions could make minor improvements to OCSD’s technology, which would allow them to download gigabits of data per second.\nAlso, if a CubeSat can be positioned confidently and accurately near other objects in space, it could be used to inspect or service other satellites. It could also fly in formation with other CubeSats to accomplish larger missions.\n“Providing CubeSats with high-speed laser comms and advanced maneuvering capability creates some interesting opportunities for our next generation missions that wouldn’t have been possible a few years ago,” said OCSD program manager Darren Rowen.\nThe OCSD mission is supported by NASA’s Small Spacecraft Technology Program. Funding for the program comes from the Space Technology Mission Directorate at NASA Headquarters.', 'If future generations were to live and work on the moon or on a distant asteroid, they would probably want a broadband connection to communicate with home bases back on Earth. They may even want to watch their favorite Earth-based TV show. That may now be possible thanks to a team of researchers from the Massachusetts Institute of Technology’s (MIT) Lincoln Laboratory who, working with NASA last fall, demonstrated for the first time that a data communication technology exists that can provide space dwellers with the connectivity we all enjoy here on Earth, enabling large data transfers and even high-definition video streaming.\nAt CLEO: 2014, being held June 8-13 in San Jose, California, USA, the team will present new details and the first comprehensive overview of the on-orbit performance of their record-shattering laser-based communication uplink between the moon and Earth, which beat the previous record transmission speed last fall by a factor of 4,800. Earlier reports have stated what the team accomplished, but have not provided the details of the implementation.\n“This will be the first time that we present both the implementation overview and how well it actually worked,” says Mark Stevens of MIT Lincoln Laboratory. “The on-orbit performance was excellent and close to what we’d predicted, giving us confidence that we have a good understanding of the underlying physics,” Stevens says.\nThe team made history last year when their Lunar Laser Communication Demonstration (LLCD) transmitted data over the 384,633 kilometers between the moon and Earth at a download rate of 622 megabits per second, faster than any radio frequency (RF) system. They also transmitted data from the Earth to the moon at 19.44 megabits per second, a factor of 4,800 times faster than the best RF uplink ever used.\n“Communicating at high data rates from Earth to the moon with laser beams is challenging because of the 400,000-kilometer distance spreading out the light beam,” Stevens says. “It’s doubly difficult going through the atmosphere, because turbulence can bend light—causing rapid fading or dropouts of the signal at the receiver.”\nTo outmaneuver problems with fading of the signal over such a distance, the demonstration uses several techniques to achieve error-free performance over a wide range of optically challenging atmospheric conditions in both darkness and bright sunlight. A ground terminal at White Sands, New Mexico, uses four separate telescopes to send the uplink signal to the moon. Each telescope is about 6 inches in diameter and fed by a laser transmitter that sends information coded as pulses of invisible infrared light. The total transmitter power is the sum of the four separate transmitters, which results in 40 watts of power.\nThe reason for the four telescopes is that each one transmits light through a different column of air that experiences different bending effects from the atmosphere, Stevens says. This increases the chance that at least one of the laser beams will interact with the receiver, which is mounted on a satellite orbiting the moon. This receiver uses a slightly narrower telescope to collect the light, which is then focused into an optical fiber similar to fibers used in terrestrial fiber optic networks.\nFrom there, the signal in the fiber is amplified about 30,000 times. A photodetector converts the pulses of light into electrical pulses that are in turn converted into data bit patterns that carry the transmitted message. Of the 40-watt signals sent by the transmitter, less than a billionth of a watt is received at the satellite—but that’s still about 10 times the signal necessary to achieve error-free communication, Stevens says.\nTheir CLEO: 2014 presentation will also describe how the large margins in received signal level can allow the system to operate through partly transparent thin clouds in the Earth’s atmosphere, which the team views as a big bonus.\n“We demonstrated tolerance to medium-size cloud attenuations, as well as large atmospheric-turbulence-induced signal power variations, or fading, allowing error-free performance even with very small signal margins,” Stevens says.\nWhile the LLCD design is directly relevant for near-Earth missions and those out to Lagrange points—areas where the forces between rotating celestial bodies are balanced, making them a popular destination for satellites—the team predicts that it’s also extendable to deep-space missions to Mars and the outer planets.\nPresentation SM4J.1, titled “Overview and On-orbit Performance of the Lunar Laser Communication Demonstration Uplink,” will take place Monday, June 9, at 4:00 p.m. in Meeting Room 212 A/C of the San Jose Convention Center.\nSource: The Optical Society']	['<urn:uuid:d3716f74-b493-45d1-9523-e6820ff5b396>', '<urn:uuid:f1fbeb74-e2b8-4be8-b881-5332b0c974a4>']	open-ended	with-premise	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T19:47:23.148022	11	79	1306
53	how beta measures investment risk	Beta or beta coefficient indicates whether an investment is more or less volatile than the market as a whole. It measures risk from general market movements rather than specific factors. The market portfolio has a beta of exactly 1, and a beta below 1 indicates a less volatile investment than the market.	['Beta Investopedia Sharper Insight Smarter Investing. Beta is a measure of the volatility, or systematic risk, of a security or a portfolio in comparison to the entire market or a benchmark Beta is used in the capital asset pricing model CAPM Beta Definition of Beta by Merriam Webster How It Works A stock s beta is determined by analyzing how much its return fluctuates in relation to the overall market return A stock with a beta of . will tend to move higher and lower in lockstep with the overall market Stocks with a beta greater than . tend to be volatile than the market, and those with betas below . tend to be less volatile than the underlying index. Beta Beta UK b i t , US b e t uppercase , lowercase , or cursive Ancient Greek , translit b ta or Greek vita is the second letter of the Greek alphabet.In the system of Greek numerals it has a value of In Ancient Greek, beta represented the voiced bilabial plosive b.In Modern Greek, it represents the voiced labiodental fricative v. Beta finance In finance, the beta or beta coefficient of an investment indicates whether the investment is or less volatile than the market as a whole. Beta is a measure of the risk arising from exposure to general market movements as opposed to idiosyncratic factors The market portfolio of all investable assets has a beta of exactly A beta below can indicate either an investment with Beta USA Street Legal Off Road RR S MSRP ,. Beta definition of beta by The Free Dictionary beta b t , b n The second letter of the Greek alphabet See Table at alphabet The second item in a series or system of classification A mathematical measure of the sensitivity of rates of return on a portfolio or a given stock compared with rates of return on the market as a whole A beta of . indicates that an asset closely Beta financial definition of Beta Beta Beta is a measure of an investment s relative volatility The higher the beta, the sharply the value of the investment can be expected to fluctuate in relation to a market index. Beta Nightclub The official website of Beta Nightclub, Denver CO It s a great club It really is It s a wonderful sound system, a great booth. Beta Know the Risk Investopedia Beta is a measure of a stock s volatility in relation to the market By definition, the market has a beta of and individual stocks are ranked according to how much they deviate from the market. Beta Theta Pi Developing men of principle for a SOMETHING TO SMILE ABOUT Shop at Don t forget to select Beta Theta Pi Foundation as your supporting charity at Smile smile. .% of all qualifying purchases benefit young Betas through the Beta Leadership Fund.\n[PDF] Download ✓ Beta | by ☆ Rachel Cohn 164 Rachel Cohn\nTitle: [PDF] Download ✓ Beta | by ☆ Rachel Cohn']	['<urn:uuid:6c39de0b-d6cd-4bb1-b8b1-090614760561>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T19:47:23.148022	5	52	503
54	As someone studying site-specific performance, I'm curious about the relationship between alternative performance spaces and audience engagement. How does the Island Moving Company's approach to site-specific work differ from traditional venues, and how does this connect to proxemics theory in performance spaces?	The Island Moving Company actively embraces site-specific work, with performances taking place in various locations around Newport and Providence, including estates like Gray Craig. This approach allows for unique interactions between performers and audiences, as demonstrated in their Open For Dancing festival where professional dancers perform alongside community participants. From a proxemics perspective, these alternative spaces break away from the traditional fixed-feature space constraints of conventional theaters, allowing for more flexible and interactive relationships between performers and spectators. This approach aligns with the concept that space arrangements can either encourage (sociopetal) or discourage (sociofugal) interaction. While traditional theaters force standardized viewing patterns and rigid performer-audience relationships, site-specific work allows for more varied and intimate spatial relationships, though this often means compromising on audience size and revenue potential.	['Spencer Gavin Hering, 36 | Associate artistic director, Island Moving Co.\nHe first came to Newport as a guest performer to dance the role of Cavalier in Island Moving Companyís (IMC) Newport Nutcracker at Rosecliff in 2012. At the time, Spencer Gavin Hering was artistic director of infinite Movement Ever Evolving (iMEE) in Houston. After iMEEís spring tour to Italy, Gavin Hering accepted IMC Artistic Director Miki Ohlsenís invitation to move to Newport to take on the newly created position of associate artistic director for the contemporary ballet company. Gavin Hering serves as the companyís ballet master and resident choreographer while also dancing in IMC productions, including the recent sixth season of the biannual Open For Dancing festival in which he danced the leading role in guest choreographer John-Mark Owenís ďRhapsody in BlueĒ at the Gray Craig estate in Middletown. He also works on programming, casting and planning.\nThe patio on the Gray Craig estate was a perfect setting for a ballroom dance during Open For Dancing. Who were the dancers?\nThere were only five members of the dance company, including myself. The rest of the cast were participants ranging from age 19 to 50. Some were dancers from the Portsmouth Abbey Music Department, but most were not dancers, just people who appreciate dance. There is a two and a half hour rehearsal on Wednesday, Thursday, and Friday, and then the performance is on the weekend. Itís a wonderful meld of different worlds, educated dancers and people who just want to be part of it.\nThat was a moving and surprising little love story told through dance and the locale. It was thrilling to watch IMC dancer Josť Gonzalez on the balcony looking out to sea at the moment he saw you bounding up the long, sloping lawn from the sea.\nJosť and I are both straight and we both had to act a little bit. We realized the dance didnít have to be blatantly sexual, so we imagined ourselves as brothers that hadnít seen each other for a while.\nWhat is unique about Island Moving Company compared to other dance companies youíve worked with?\nI am a huge fan of site-specific work. I like the idea of being able to float around Newport and Providence and having choreographers set site-specific work. Of course there is another side to that story. The company has been around for 30 years without its own resident theater. We are at 3 Charles Street in Newport, right across from the beautiful vaudeville Opera House on Washington Square. It would require $22 million to make it into an absolutely epic space for dance. The low budget is $12 million, and it would be a bit harder to dance there on that, but we could still do it. In the summer we could do several shows a week. We could coordinate with cruise lines that come to town. It is an incredible revenue-building opportunity for the company and for Newport.\nWhat is your role as associate artistic director?\nI am Miki Ohlsenís right hand man. I am learning from her and I am here to help her as much as possible with the artistic challenges, connecting her with my connections abroad or nationally, and just keeping the ball rolling. We have a wonderful mother-son relationship around artistic issues.\nSpeaking of parents, your parents were both professional ballet dancers. Was it inevitable that you would become a dancer?\nMy parents put me in ballet when I was five and I despised it. I got into sports instead. My freshman year in high school, I got into football, I played basketball, golf and soccer. One day during my sophomore year, I was doing homework in my motherís school and I saw this pretty girl walk in. My mother asked me, ďHey, would you like to take a pas-de-deux class,Ē and I said, ďIíd love to!Ē Long story short, the girl became a hairdresser and Iíve been dancing ever since!\nHave you ever wanted to quit?\nI took two years off when I was 22 because I had a bad knee injury. I could feel bone against bone when I was dancing on stage. I completely retired and went back to school for architecture. I did it for two years and missed dance the whole time. I was fortunate; I got a guest engagement and that led me to a job offer at the Maximum Dance Company in Miami. I met my wife (Andrea Dawn Shelley) there, so a lot of good came of it!\nDance is a very physical art form. Are most dancers, if they dance long enough, eventually in pain?\nAs a dancer, your pain tolerance is probably higher than any other human being. We definitely donít baby our injuries. There is pain that you can deal with and pain that you think you canít, but as dancers we tend to get through both kinds of pain. You have to be smart about it. There are many things you have to do to maintain the instrument, which is your body. You have to do physical therapy, stretch, do yoga and eat right.\nWhat do you do on a typical day?\nI like to watch the sun rise. Coming from the West Coast, Iím not used to seeing that! On a typical work day I teach company class at about 9:30. We rehearse for close to three and a half hours. After that, I go into the office, address emails, connect with festivals. Iíll go home for dinner and I might come back in the evening to teach, or Iíll do work at home editing videos for the company. We might have to pick up a choreographer and take him here or there. The Nutcracker is just around the corner and we have to schedule rehearsals.\nHow do you deal with the demands of juggling dancing, planning and teaching?\nIím a big surfer. I started surfing in Orange County, California, and I never imagined the waves would be as good as they are here. It is one of the purest forms of experiencing nature, the inertia and the energy. You donít get that sensation from walking around in the woods. I donít have to necessarily catch any waves; I might just sit on my board and float, just to find the rhythm of the earth. I donít think about bills or anything else. Just the peace and tranquility of nature.\nDo you think being a dancer gives you a unique perspective on life?\nWell, everyone sees the world differently. As dancers, I guess we gain discipline and self-esteem. Maybe we notice things that others donít. I might go to the Fastnet and see how someone throws darts and Iíll incorporate that into a dance. I might notice a dog doing something funny and Iíll try to copy it. We spend our whole lives studying and looking into a mirror to see if we are perfecting it. I donít think we tend to ever think, ďIíve hit it perfectly. Iíve done it. Iíve won the lottery!Ē Dancers continually strive for perfection. I love it though, Iím so grateful dance found me.\nYour many tattoos worked great for the sailor role you just did but do they keep you from being cast in other roles?\nI got my first tattoo for my mother Cecilia on my chest, when I was 16. She was bummed out and touched at the same time! I have been getting tattoos now for 20 years. They are like the rings on a tree. If I notice them I remember where I was in my life. It doesnít stop me from doing anything. With classical ballet, I would always be completely covered by a tunic. If I do a topless piece, I can cover the tattoos with makeup and respect the choreographerís needs. My body is a canvas. Michael Cruz in Houston is the artist that I always use.\nDo you and the other professionals dancers ever just go out to a club and let loose and dance to house party music?\nYes, we have gone a few times. It is important for all of us to maintain a rapport, and especially for me, since I am in that middle role between the artistic side and dancing. Of course, we love to dance. We have a good time!', 'The way a person uses the surrounding space and his/her distance to objects and other people is not accidental or merely functional but is a result of conscious or subconscious decisions.\nThe person who came up with this idea in 1963 was anthropologist Edward T. Hall. He proposed that the way a person uses the space around him/her should also be taken into account in studies of behavior and communication and he called this field of study “proxemics.” In proxemics it is maintained that people use the space according to the related social codes in their culture and those codes become more and more accepted and established as the usage is repeated.\nHall stated that people “locate” their bodies according to three types of space:\n1. Fixed-feature space (immobile things, such as walls, stairs, windows, sidewalks, etc.).\n2. Semifixed-feature space (movable furniture and objects).\n3. Informal (personal) space (space around the body and the distance to others).\nThe theater semiologist Keir Elam applies this classification to performance spaces:\n1. The building, lobby, auditorium, ceiling, stage, fixed seats, etc. constitute the fixed-feature space.\n2. Set, curtain, lights belong to semifixed-feature space.\n3. Spaces around and between the actors and spectators form the personal space. (Elam, 62-63)\nThe architecture and design of performance spaces are built and designed to serve specific purposes, according to specific rules. Once built, those structures embody, protect and impose those rules.\nLet us look at the theater auditoriums where most performances take place: even now, in the 21st century, they are being built in the bourgeois “playhouse” style developed about two centuries ago, consisting of a stage facing or surrounded by fixed rows of seats. The “stage” (elevated or not) connotes status, hierarchy and immunity as inherited from temples, churches and courts.\nThe center of the stage (altar) is the focal point, it forms the core of the symmetrical architecture of the auditorium. That is the spot where the orchestra’s conductor, the lecturer, the soloist stands. During the curtain call of a theater performance the lead actor stands there, the others line up on two sides in a hierarchy determined by the importance of their roles in the play. The ticket prices are set according to the distance to this center spot: those who pay more (usually dressed elegantly) sit closer and those who pay less end up sitting at a distance. Economical and demographic characteristics reflect the architectural symmetry. Performers enter the stage from some invisible spaces and they basically “disappear” when they exit. In short, the fixed space supremely dominates and controls the semifixed and informal spaces.\nThose who build these spaces are not the ones who perform there: they are either investors who want to make a profit or organizations that have to or want to provide cultural services to society. Naturally, the major concern of these individuals and organizations is to fit in as many spectators as possible. They don’t have any reason to build flexible spaces that can be arranged in different ways for different works or to utilize existing structures to offer alternative spaces. Indeed, most of these builders tend to be unaware of the existence of such ideas.\nOne doesn’t hear demands from the performing arts audiences for different, alternative spaces because they tend to prioritize the comfort and untouchableness of their personal spaces (comfort zone) and, therefore, are content with the existing design and decorum. There seems to be an ongoing reciprocal cause-effect relationship between the builders and the audiences: builders give what the audiences demand and the audiences demand what they were given.\nThose who produce and present the works of art are usually left out of this vicious supply-demand circle. The work is evaluated and accepted for presentation in a given space according to its suitability for the architectural and operational features – basically, its “salability.” If we accept that search for the new is the most essential motive in the arts, we can say that fixed-feature spaces and the relationships stemming from them do not comply with the nature of the arts.\nWe can notice the importance of proxemics when we look at the history of modern theater. Proxemic arrangements may have been the most discussed topic since the beginnings of the 20th century: some didn’t use the backstage, some asked the audience to watch standing and walking, some placed the audience on stage, some made the actors meet the audience at the entrance, some invited the audience to touch the actors, etc. These experiments can be summarized, using two somewhat technical terms, as efforts to move from “sociofugal” to “sociopetal” space arragements.\nSociofugal arrangements are aimed at allowing people to preserve their personal and private spaces in solitude, without having to communicate with each other. The waiting rooms of hospitals, government offices, airports are typical examples of sociofugal spaces (usually arranged in grids). On the other hand, sociopetal spaces allow people to see and hear each other, to communicate and interact with each other. They aim for setups where personal spaces can be mobile. Cafes, cabaret theaters where spectators sit at tables, plazas of small Italian towns are often given as examples to sociopetal space arrangements (usually circular or radial). (Elam, 64-65)\nThe spatial experiments in modern theater gave way to new ideas in performer-spectator relationship but they were not effective enough to start radical changes in fixed-feature spaces. Smaller performance spaces called “black box,” where all walls are painted in black, appeared in the 1960s and became popular. At first glance, these look like a reaction to the limits of the playhouse but in reality they serve the purpose of quick and inexpensive accommodation of smaller productions in the same, traditional manner.\nIn both traditional spaces and black boxes the audience is often seated in sociofugal arrangements, where personal spaces remain separate. Interestingly, the need to look and be seen, to watch and to show, to talk and to listen tend to be satisfied at the sociopetal environment of the lobby. This may be one reason why the sizes of the lobbies have been expanding in recent years, almost reaching the size of the auditorium, particularly in buildings where opera, ballet, orchestral concerts and large theatrical productions take place (Bennett, 130-131).\nSocializing at the sociopetal lobby ends when the spectators enter the sociofugal auditorium: they go quiet, find their seats and sit and wait, as determined by the architecture and the traditional codes generated from it. When the performance begins, every person watches from his/her personal space, as an individual, with minimal interaction with each other despite the fact that they sit elbow to elbow. That is, the event that is supposed to be a collective experience turns into an individual one.\nModern dance may be a good field to look at to understand proxemics. Generally modern dance groups are born at dance studios which do not have fixed seats for an audience. As a result, the choreography does not target one direction, there is no “exit” area for the dancers who step out of the dance, the dancer can enter the action from any point in the space, etc. Dance groups usually present their initial performances in such studios to a small number of spectators who can be seated anywhere in the space which often allows them to see the dancers and each other up-close.\nIf the dance group establishes itself and becomes known, then they will want to perform to more people and will move to a standard building. This will mean that they will be dancing in a three-sided box with entrances and exits mostly from two sides and that they will get viewed from one direction only. That is, the number of choices in the open dance studio will get reduced significantly. As the group “develops,” they move to bigger and more institutional spaces where they get viewed by more people from further distances. In other words, the work and its presenters become “cinematized” and the “human-to-human” aspect diminishes. Standard architecture forces the performance to standardize itself. As the number of viewing eyes increases, the number of choices and the possibilities of doing something new or antithetical decreases.\nIn order to change the space from being “the location of presentation” into “one of the components of the performance” and to enable the practitioners to control the space, I personally think that the artists have to compromise on the number of viewing eyes (therefore, income). When this can be done, then the alternative space choices increase notably.\nThe experiments in the past proved the unavoidability of certain physical requirements by the spectators: they get tired if they stand for a while, they feel uncomfortable if it is cold or hot, they sometimes need to go to the restroom, they don’t like to be touched, they don’t like tiring travels to reach the performance space, etc. As long as these requirements are met, I think every space can accommodate a performance.\nIn the current century all societies have been experiencing the ever increasing dependency on electronic devices. The process of “dehumanization” in communications had begun to devalue the significance of human-to-human interaction. Then, almost like a punishment from the heavens, humans were obliged to physically disconnect from each other as a result of the Covid-19 epidemic of 2020. The performing arts, inevitably, began “going online.”\nIt is still impossible to predict what the world will go through in the near future (as of May 2021, in New York), but I personally believe that the necessity of live interaction among humans has been clearly understood and appreciated. Witnessing the absurd demand for “live” streamings as opposed to recordings, I think the performing arts will go back to live performances as soon as they can and proxemics, sociopetal arrangements and interaction between personal spaces will be considered anew in the light of the lessons learned during the pandemic.']	['<urn:uuid:5403e5a8-f10b-4944-9d85-d95cb9f5d7c6>', '<urn:uuid:34080f2a-96bd-436b-8d84-db0712917f9d>']	open-ended	with-premise	verbose-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:47:23.148022	42	127	3037
55	What leads to elbow injuries in baseball, and what stops them in kids?	Elbow injuries are primarily caused by high values of elbow valgus stress, which occurs when the ulna in the forearm is pulled away from the elbow joint. Research shows four significant variables contribute to elbow valgus. To prevent these injuries in youth baseball, organizations have implemented pitch count limits and mandatory rest periods between games, following USA Baseball's Pitch Smart guidelines. Additionally, proper warm-up routines and review of pitching mechanics are essential preventive measures.	['There are a few writers and people out there who criticize me for talking about pitchers’ mechanics without having specific joint loads (kinetics), stating that it’s akin to “guessing” without any merit. Their claims that without a full biomechanical analysis (presumably using three-dimensional modeling), you can’t make any definitive statements about health and efficiency.\nWell, as readers of this blog know, I am sympathetic to that argument – so much so, that I invested four years of my life (and many dollars) into building my very own biomechanics lab:\nThat’s the first iteration of our control object (which you need to film using multiple high-speed cameras to gain an anchoring position) at our old facility in North Seattle.\nI’m no stranger to calculating the kinematics and kinetics of the pitching delivery, and I think I’ve learned a great deal by putting many pitchers through it. However, the idea that we must do this to make educated guesses about mechanics is simply wrong.\nUnderstanding the Mechanisms of Injury\nAs I stated in my article about elbow injuries in pitchers, we do not yet know the definitive cause of UCL rupture (which requires Tommy John surgery to repair). However, research indicates high values of elbow valgus stress is primarily responsible for tension on the UCL and the prime contributor to joint loads about the elbow. Remember, the primary function of the ulnar collateral ligament is to stabilize the elbow while the ulna in the forearm is pulled away from the elbow joint (medial epicondyle).\nYou can test for this injury by doing an elbow valgus stress test (a video from my alma mater, Baldwin-Wallace College):\nIn the groundbreaking study by Dr. Werner et al (2002 JSES: Relationship between throwing mechanics and elbow valgus in professional baseball pitchers), Dr. Werner concluded that there were four significant variables that contributed to elbow valgus:\nI wrote about these at length in my article about elbow injuries if you’re interested in a more detailed look at these variables and what they actually mean.\nApplying it to Video Analysis\nSince Dr. Werner’s research (and many additional papers that support her theories) indicates those four variables as being highly significant with regards to elbow valgus stress, if you see a pitcher who displays a high amount of shoulder abduction angle at stride foot contact (SFC), then it’s likely that that pitcher has higher-than-normal elbow valgus stress. While correlation does not equal causation, it’s pretty clear that these characteristics are linked to higher “joint loads” on the elbow.\nSimple Physics: Force Application\nHowever, instead of trotting out research papers, let’s think about this from a simple mechanical physics/engineering perspective. I recently posited that Trevor Bauer’s “late launch” (as Ron Wolforth calls it) is inherently more efficient and less stressful on the elbow joint than an “early launch” exhibited by pitchers like Stephen Strasburg, where Trevor’s throwing shoulder is rotated far more into the target before maximum internal rotation angular velocity is reached. Here’s overhead high-speed video to show what I’m talking about:\nA realization hit me a few years ago when I spoke to Dr. Murray Maitland at the University of Washington – instead of talking about complex kinematics and kinetics, he illustrated a simple point from an engineering perspective: Force is best applied in lines parallel to the desired direction of trajectory in the ballistic motion. (Of course, Dr. Marshall has been beating this drum for decades now.) This is obvious once you think of it from an engineering perspective – would you build a machine that throws a projectile in a circuitous path where the distal ends of the joints were held together by a pulley, or would you build it where the lever arm applied force in a straight line?\nFrom a slightly more biomechanical point of view, think of it this way: The UCL stabilizes the elbow from being pulled apart as the forearm separates from the upper arm (humerus). If force is applied in a maximum sidearm position with an early launch, this theoretically maximizes valgus stress, while a more vertical delivery reduces it (like Trevor’s, or Roger Clemens’ delivery).\nWhile a true vertical delivery may not be possible (ask Fritz Outman about that, though), getting closer to that should reduce stress on the elbow and possibly increase efficiency and release velocity of the pitcher.\nResearch tends to back this theory up, as Aguinaldo’s study (Am J Sports, 2009: Correlation of Throwing Mechanics With Elbow Valgus Load in Adult Baseball Pitchers) showed that a later trunk rotation in the pitching delivery and increased elbow flexion near peak valgus/ball release significantly reduced elbow valgus stress.\nWhat You Need to Adequately Study Pitching Mechanics\nIdeally, you get yourself a pair of high-speed cameras and film the delivery from overhead as well as from the side, but they’re not cheap anymore (good thing I bought five of them when I could!).\nYou can still use regular speed video to do some cursory analyses if you get the right angles, but it’s all about setting them up in a proper and repeatable way. I highly recommend getting an overhead shot (something Bill Peterson from RPM Pitching has been telling me to do for years) as well as a view from the side. That will cover all relevant angles that you want to see.\nWe take regular high-speed video to analyze our pitchers’ mechanics in the MaxVelo program to make sure the cueing is properly done, and I recommend you seek out someone locally to do the same for you – any pitching coach worth his salt will have invested in at least one high-speed camera.\nBut if you can’t find anyone, our new facility in SeaTac is just minutes from the airport – so come on by!', 'Most youth baseball organizations today limit pitch counts or require days of rest after a young pitcher’s stint on the mound — or both. And that’s a good thing. Medical research has shown that these rules are a safe way to protect the arms of these young players.\n“Kids who start pitching who are not fully grown put tremendous stress on their shoulders and elbows,” says Gary Calabrese, PT, DPT, Senior Director of Rehabilitation and Sports Therapy. This can lead to a variety of physical problems, says Mr. Calabrese, who is a doctor of physical therapy and specializes in rehabilitation.\nCommon medical issues for young pitchers\nLittle League elbow and Little League shoulder are common problems among adolescent athletes. Other common injuries or issues that affect young pitchers include:\n- Growth-plate fractures — Widening of the important growth plate at the ends of bones in the arms.\n- Shoulder instability — An unstable shoulder bone can destabilize the shoulder and increase the risk of pain with a subluxed shoulder.\n- Rotator cuff problems — Repetitive overhead motion can cause inflammation of the cord-like tendons that enclose the shoulder (tendonitis) or the fluid-filled sacs that lubricate/protect the shoulder (bursitis). The swelling can cause painful shoulder impingement that frays and irritates the rotator cuff. A torn rotator cuff is often called pitcher’s shoulder, but is uncommon in youth pitchers.\n- Ulnar collateral ligament injury — A tear in the ligament on the inside of the elbow.\n“Kids usually experience shoulder or elbow pain for one of three reasons,” Mr. Calabrese says. “Their pitch count is too high, they didn’t rest enough between pitching sessions, or their pitching mechanics are inadequate.”\nGuidelines to safeguard young pitchers\nFortunately, Little League International, Pony Baseball, Babe Ruth/Ripken Baseball, Dixie Youth League, American Legion Baseball and other youth organizations have implemented rules that limit pitching to prevent these kind of overuse injuries. Many of these rules follow USA Baseball’s Pitch Smart guidelines and limit the number of pitches during a game or require several days of rest between appearances on the mound.\n“Injuries to the growth-plate region of the bones in the shoulder and elbow can be prevented with proper adherence to pitching guidelines and review of proper pitch mechanics,” Mr. Calabrese says.\nA pitch count controls the overuse stresses that accumulate over time in young pitchers, Mr. Calabrese says. Mandating rest periods between games allows the body an appropriate recovery period between pitching sessions.\n“If a player wants to get better, the only way to improve, with regard to performance or training, is to have adequate recovery, and these guidelines allow that,” he says.\nOther ways to prevent injury\nThere are other strategies a player can use to avoid injuries aside from limiting pitches and proper intervals between games.\nProper warm up before an outing is critical. A warm-up should begin with a light jog, followed by functional movements such as side shuffles or a high knee run, in addition to trunk rotation and upper extremity stretches.\nWatch that windup\nGeneral soreness is not uncommon in young pitchers. However, “if an athlete experiences more pain than the usual muscle soreness, or if sharp pain occurs while throwing the ball hard, it’s time to see the doctor,” Mr. Calabrese says.\nReviewing the young pitcher’s throwing mechanics also is important, and while the coach can be a good source of advice, professional, medical-based help also is available to improve performance and keep the athlete injury-free.\nA good sports performance program offers customized video analysis of pitching motion, with specific drills that address each athlete’s deficiencies, functional training and conditioning, and should offer injury prevention and rehabilitation for young pitchers in baseball and softball, Mr. Calabrese says.\nEach injury is unique and should be treated as such by a sports medicine professional, he says.\n“The key is addressing each injury individually,” he says. “No two injuries are the same.”']	['<urn:uuid:5c7bfe04-48b7-4226-9e88-5e58ba546636>', '<urn:uuid:8344851c-220c-47b8-abe1-b2ff0eef4205>']	factoid	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	13	74	1609
56	I heard someone had the shortest noble title ever. How long was it?	Frederic Leighton held the shortest-lived peerage in history. His hereditary peerage became extinct after only one day upon his death.	"['Frederic Leighton(Redirected from Frederic Lord Leighton)\nFrederic Leighton, 1st Baron Leighton, PRA (3 December 1830 – 25 January 1896), known as Sir Frederic Leighton between 1878 and 1896, was an English painter and sculptor. His works depicted historical, biblical, and classical subject matter. Leighton was bearer of the shortest-lived peerage in history; after only one day his hereditary peerage became extinct upon his death.\nThe Lord Leighton\nFrederic Leighton, ""Self-portrait"", 1880\n3 December 1830\n|Died||25 January 1896 (aged 65)|\n|Education||Eduard von Steinle and Giovanni Costa|\n|Known for||Painting and sculpture|\n|Movement||Academicism, Neoclassicism and British Aestheticism|\n|Awards||Prix de Rome, Légion d\'honneur|\nLeighton was born in Scarborough to Augusta Susan and Dr. Frederic Septimus Leighton. He had two sisters including Alexandra who was Robert Browning\'s biographer. He was educated at University College School, London. He then received his artistic training on the European continent, first from Eduard von Steinle and then from Giovanni Costa. At age 17, in the summer of 1847, he met the philosopher Arthur Schopenhauer in Frankfurt and painted his portrait, in graphite and gouache on paper—the only known full-length study of Schopenhauer done from life. When he was 24 he was in Florence; he studied at the Accademia di Belle Arti, and painted the procession of the Cimabue Madonna through the Borgo Allegri. From 1855 to 1859 he lived in Paris, where he met Ingres, Delacroix, Corot and Millet.\nIn 1860, he moved to London, where he associated with the Pre-Raphaelites. He designed Elizabeth Barrett Browning\'s tomb for Robert Browning in the English Cemetery, Florence in 1861. In 1864 he became an associate of the Royal Academy and in 1878 he became its President (1878–96). His 1877 sculpture, Athlete Wrestling with a Python, was considered at its time to inaugurate a renaissance in contemporary British sculpture, referred to as the New Sculpture. American art critic Earl Shinn claimed at the time that ""Except Leighton, there is scarce any one capable of putting up a correct frescoed figure in the archway of the Kensington Museum."" His paintings represented Britain at the great 1900 Paris Exhibition.\nLeighton was knighted at Windsor in 1878, and was created a baronet, of Holland Park Road in the Parish of St Mary Abbots, Kensington, in the County of Middlesex, eight years later. He was the first painter to be given a peerage, in the 1896 New Year Honours. The patent creating him Baron Leighton, of Stretton in the County of Shropshire, was issued on 24 January 1896; Leighton died the next day of angina pectoris.\nLeighton remained a bachelor, rumors of him having an illegitimate child with one of his models, in addition to the supposition that Leighton may have been a homosexual, continue to be debated. He certainly enjoyed an intense and romantically tinged relationship with the poet Henry William Greville whom he met in Florence in 1856. The older man showered Leighton in letters, but the romantic affection seems not to have been reciprocated. Enquiry is furthermore hindered by the fact that Leighton left no diaries and his letters are telling in their lack of reference to his personal circumstances. No definite primary evidence has yet come to light that effectively dispels the secrecy that Leighton built up around himself, although it is clear that he did court a circle of younger men around his artistic studio.\nAfter his death his barony was extinguished after existing for only a day; this is a record in the Peerage. His house in Holland Park, London has been turned into a museum, the Leighton House Museum. It contains many of his drawings and paintings, as well as some of his former art collection including works by Old Masters and his contemporaries such as a painting dedicated to Leighton by Sir John Everett Millais. The house also features many of Leighton\'s inspirations, including his collection of Iznik tiles. Its centrepiece is the magnificent Arab Hall. The Hall is featured in issue ten of Cornucopia. A blue plaque commemorates Leighton at Leighton House Museum.\nLeighton was an enthusiastic volunteer soldier, enrolling with the first group to join the 38th Middlesex (Artists\') Rifle Volunteer Corps (later to be known as the Artists Rifles) on 5 October 1860.\nHis qualities of leadership were immediately identified, and he was promoted to command A Company within a few months. On 6 January 1869 Captain Leighton was elected to command the Artists Rifles by a general meeting of the corps. In the same year he was promoted to major and in 1875 to lieutenant colonel. Leighton resigned as commanding officer in 1883. The painter James Whistler famously described the then, Sir Frederic Leighton, the commanding officer of the Artists Rifles, as the: “Colonel of the Royal Academy and the President of the Artists Rifles – aye, and he paints a little!"" At his funeral, on 3 February 1896, his coffin was carried into St Paul\'s Cathedral, past a guard of honour formed by the Artists Rifles.\n- 1864 – Associate of the Royal Academy\n- 1868 – Royal Academy Academician\n- 1878 – President of the Royal Academy\n- 1878 – Légion d\'honneur Officer\n- 1878 – Knight Bachelor\n- 1886 – Created a baronet in the Baronetage of the United Kingdom\n- 1889 – Associate member of the Institute of France\n- 1896 – Created a baron in the Peerage of the United Kingdom\n- Death of Brunelleschi (1852), oil on canvas\n- The Fisherman and the Siren, c. 1856–58 (66.3 x 48.7 cm)\n- Cimabue\'s Celebrated Madonna is carried in Procession through the Streets of Florence (1853–55), oil on canvas.\n- The Discovery of Juliet Apparently Lifeless (c.1858)\n- The Villa Malta, Rome (1860s), oil on canvas\n- The Painter\'s Honeymoon, c. 1864 (83.8 x 77.5 cm)\n- Mother and Child, c. 1865, (48.2 x 82 cm)\n- Actaea, the Nymph of the Shore (1868), oil on canvas, (57.2 x 102.2 cm) National Gallery of Canada, Ottawa.\n- Daedalus and Icarus, c. 1869, (138.2 x 106.5 cm)\n- Hercules Wrestling with Death for the Body of Alcestis (1869–71) (132.4 x 265.4 cm)\n- After Vespers 1871, (111.5 x 71.5 cm), Princeton University Art Museum\n- Greek Girls Picking up Pebbles by the Sea, 1871 (84 x 129.5 cm)\n- Teresina (circa 1874) Christchurch Art Gallery Te Puna o Waiwhetu, Christchurch, New Zealand\n- Music Lesson, c. 1877, (92.8 x 118.1 cm)\n- An Athlete Wrestling with a Python (1877), bronze sculpture\n- Nausicaa, c. 1878 (145 x 67 cm)\n- Winding the Skein, c. 1878, (100.3 x 161.3 cm)\n- Light of the Harem, c. 1880, (152.4 x 83.8 cm)\n- Idyll, c. 1880–81\n- Wedded, (c. 1881–1882) (145.4 x 81 cm)\n- Cymon and Iphigenia (1884) Art Gallery of New South Wales\n- Captive Andromache, c. 1888 (197 x 406.5 cm)\n- The Bath of Psyche, (c. 1889–90) (189.2 x 62.2 cm) Tate Gallery\n- The Garden of the Hesperides, c. 1892, (169 x 169 cm)\n- Flaming June (1895), oil on canvas, Museo de Arte de Ponce, Puerto Rico (120.6 x 120.6 cm)\n- The Parable of the Wise and Foolish Virgins (Fresco)\n- The armlet\n- Phoebe (55.88 x 60.96 cm)\n- A Bather\n- The Leighton Frescoes, The Arts of Industry as Applied to War and The Arts of Industry as Applied to Peace\n- Phoenicians Trading with the Early Britons on the Coast of Cornwall, 1895. Mural at the Royal Exchange, London\n- The Return of Perspephone 1891, oil on canvas, Leeds Art Gallery\nPortrait of May Sartoris at about 15 years of age, Kimbell Art Museum, Fort Worth, TX.\n|Leighton\'s An Athlete Wrestling with a Python, Smarthistory|\n- Peerage records – Leigh Rayment\'s Peerage Page\n- Elizabeth Lee, ‘Orr, Alexandra (1828–1903)’, rev. Katharine Chubbuck, Oxford Dictionary of National Biography, Oxford University Press, 2004 accessed 30 April 2017\n- Crowther, Paul, and Miruna Cuzman. ""A Rediscovered Contemporary Full-Length Sketch-Portrait of Schopenhauer by Frederic, Lord Leighton."" Schopenhauer Jahrbuch, 92 Band, Konigshausen und Neumann, 2011: 301–306.\n- Shinn, Earl (1880). The World\'s Art: From the International Exhibition. Lovering.\n- ""No. 24651"". The London Gazette. 29 November 1878. p. 6695.\n- ""No. 25551"". The London Gazette. 22 January 1886. p. 328.\n- ""No. 26705"". The London Gazette. 31 January 1896. p. 587.\n- Emanuel Cooper, The Sexual Perspective: Homosexuality and Art in the Last 100 Years in the West, 2005\n- Oxford Dictionary of National Biography vol 33\n- Cornucopia 10, Ingres and Lady Mary Montagu, Leighton House, yurts, the Lycians plus elegant eggplant. Cornucopia.net. Retrieved on 20 February 2011.\n- ""LEIGHTON, FREDERICK, LORD LEIGHTON (1830–1896)"". English Heritage. Retrieved 1 July 2012.\n- Barry Gregory. A History of The Artists Rifles 1859–1947. Pen & Sword. 2006.\n- Frederic, Lord Leighton | Cimabue\'s Celebrated Madonna | L275 | The National Gallery, London. Nationalgallery.org.uk. Retrieved on 20 February 2011.\n- Tate Collection | Study for \'The Discovery of Juliet Apparently Lifeless\'. Tate.org.uk. Retrieved on 20 February 2011.\n- Frederic, Lord Leighton | The Villa Malta, Rome | L851 | The National Gallery, London. Nationalgallery.org.uk. Retrieved on 20 February 2011.\n- Artwork Page: Actaea, the Nymph of the Shore. Cybermuse.gallery.ca. Retrieved on 20 February 2011.\n- Tate Collection | An Athlete Wrestling with a Python by Frederic, Lord Leighton. Tate.org.uk. Retrieved on 20 February 2011.\n- Newforestparishes.com Archived 3 July 2007 at the Wayback Machine.\n- Leeds Art Gallery, listings\n|Wikimedia Commons has media related to Frederic Leighton.|\n- Frederic-Leighton.org 114 works by Frederic Leighton\n- Advice to Young Artists by Frederick Lord Leighton high resolution images\n- Scarborough, Birthplace of Lord Frederic Leighton\n- Leighton House Museum\n- Obituary from The Times\n- Leighton Gallery at MuseumSyndicate\n- 171 paintings by or after Frederic Leighton at the Art UK site\nSir Francis Grant\n| President of the Royal Academy\nSir John Everett Millais, Bt']"	['<urn:uuid:81357eca-9b46-4c54-9e52-f56816727144>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:47:23.148022	13	20	1633
57	As a geology enthusiast, I'd like to understand the differences between hiking experiences at Grotto Canyon and Waterton Lake when it comes to exploring unique geological features.	At Grotto Canyon, hikers can explore surreal rock formations, indigenous wall art, and vertical rocks while having a geologist guide explain the geology of the rocky landscape. The experience includes examining Devonian fossils and includes a stop at a waterfall. In contrast, Waterton Lake, situated at over 4,000 feet altitude, offers a different geological experience within a UNESCO World Heritage Site. The lake is surrounded by national park lands known for their biodiversity, with waters reaching depths of nearly 500 feet in some places. While both locations offer hiking opportunities, Grotto Canyon focuses more on detailed geological features and fossil exploration, while Waterton Lake's trails are set in a broader, more diverse ecological context.	"['To limit the spread of the coronavirus, attractions may be closed or have partial closures. Please consult government travel advisories before booking. The WHO is closely monitoring the coronavirus and more information can be found here.\nHike on a frozen creek, and explore surreal rock formations and indigenous wall art on this small-group tour through Grotto Canyon. Your specialist guide explains the unique history and geology of the canyon as you glide past wondrous icefalls in this sublime winter environment. The tour also includes use of ice cleats and a hiking pole, plus a light snack and hot chocolate.\nChoose from among four of Banff National Park’s most iconic trails on this 7-hour guided hiking tour with packed lunch. Pick the perfect walk for your level of ability and experience, varying from 4.5 miles (7km) to 6.6 miles (10.6km) in length, with spectacular mountain scenery no matter which option you select. The trip’s four options include explorations of Larch Valley, Stanley Glacier, Consolation Lakes or the Plain of Six Glaciers.\nExperience the winter wonderland of Banff’s Johnston Canyon on this ice walk tour. Join a professional guide on a hike along the canyon’s steel walkways, to visit the beautiful lower and upper waterfalls, and learn how this majestic canyon was formed. Your excursion includes round-trip transportation, ice cleats, and a hiking pole, plus a light snack and hot chocolate to help warm you up.\nDiscover the scenic beauty of the Icefields Parkway on this full-day tour to the Athabasca Glacier in the Canadian Rockies. Travel from Jasper en route to the famous Columbia Ice Fields. Walk through snow and ice on the glacier while your guide navigates crevasses and keeps you safe. Experience is not necessary on this alpine walk, perfect for beginners. All gear and transportation is included.\nJasper National Park is an amazing place to experience nature in the Canadian Rocky Mountains. Let a guide take you through some of the best spots for lake and mountain vistas, beside a waterfall, and on a lakeside hike on a forest path. This tour will take you to Maligne Canyon and its mountain waterfalls, disappearing Medicine Lake, and on an interpretive hike beside Maligne Lake.\nTake a walk through a real-life ‘postcard’ during this 7-hour hike amidst the sublime scenery of the Canadian Rockies. Join a knowledgeable local guide on the trails near Lake Louise, pausing as you walk to admire the local geology, plants and animals that make up this stunningly beautiful place. Depending on the season you’ll also enjoy majestic snow-capped peaks, blooming wildflowers or rainbow-hued fall colors. Pack some snacks to ensure you stay fueled up during your trek.\nHike through a narrow canyon and see how water in its many forms has shaped the mountains over time. After reaching the back of the canyon, go further and explore the ink pots, a great day out covering 11.6 km with 215 m elevation gain.\nThe Canadian Rockies has some of the wildest, and most spectacular hiking. Let Cloud Nine Guides take you to trails off the beaten path, and away from the crowds. Certified ACMG Hiking guides will provide unique trail interpretations of flora, fauna, geology and history of the area.\nDiscover the beauty of Banff with a guide and learn more about the charming mountain town during this small-group walking tour. Walk within the town of Banff, located within Banff National Park, and learn about the area\'s history, heritage, wildlife, and what life is like in grizzly bear country. Stop along the way for photo opportunities and to admire the beauty of Canada\'s Rocky Mountains.\nThis ridge hike is truly a unique experience - it\'s not well known, even by Banff locals. It\'s also the perfect add-on to the Banff Gondola experience. The views are the best you will experience in the Banff area, with a 360 degree panorama of the surrounding mountains. Our certified guides will pass on unique information about this sensitive alpine environment, the history of Banff, among other interesting topics, while making sure you are safe and comfortable.\nAn ecological wonder hidden in the city. Don’t forget your hiking shoes and prepare yourself for a wonderful 2.5 hours among gurgling natural springs, wild stands of Douglas Fir and epic views of a glacier scoured landscape. Wandering through Calgary\'s past, from the Ice Age to modern times, we will inhabit our imaginations to understand how the past has influenced the picturesque setting of Calgary. Not far from civilization but truly a world apart, this tour will give you a taste for Canada’s natural beauty and its amazing history. Great for families and those with a spirit of curiosity!\nEmbark on an easy to moderate 3.5 hour hiking tour with a real geologist! Guests will experience a fun and interactive tour exploring the awesome forces of nature that led to the building of the Rocky Mountains. Tour includes wildlife viewing, a rest stop at the emerald green waters of Grassi Lakes, a visit to a shallow cave, ancient pictographs and spectacular mountain views. Great for families and those with a spirit of curiosity!\nThere are many travel and tourism companies in Alberta offering fun and amazing hikes. But other companies don\'t tailor their experiences to beginners, or small private groups. Gypsy\'s Caravan is unique because you can book as a solo Adventurer, a couple, a family, a group of friends, or any mix of all of the above - either way, your experience will be intimate and tailored to your skill level(s).\nGroup sizes will never exceed 10 people, so you can cherish the privacy and serenity that is found in the Rocky Mountains without feeling crowded or rushed. You will also be dealing directly with me, Kelsey, at all times before, during, and after your Adventure. No middle men.\nIf a fun, casual, yet rewarding excursion is what your after, Gypsy\'s Caravan is the company for you!\nA shorter version of our Athabasca Glacier Full-Day tour, this half-day tour meets at the Toe of the Athabasca Glacier where you will be gearing up and stepping onto the glacier for a four-hour walk. While on the glacier, you will be travelling through snow, ice and navigate crevasses with an experienced ACMG certified guide.We also are the only company that looks for opportunities to use a few rope tricks to get below the surface of the ice into crevasses and millwells when the conditions and group enthusiasm allows!\nSkip the crowded tours and enjoy a more intimate view of Banff with this private and personalized walking tour. In the company of just your party (up to 10 people) and a guide, explore Banff at your own pace and discover such places as Cascade Gardens and the Bow River Trail.\nThe Mountain Metis and first Nations of the Jasper National Park area were known as the Asiniwuche Winewak, which means the Rocky Mountain People in Cree.\nWe lived off the land as Hunters and gatherers and later became guides and scouts; we depended on the use of trail horses and packhorses to survive.\nIn this program, we share some of the skills that were crucial for our survival, and hope that all our visitors take away useful skills that will serve them in life.\nGet up close and personal with a program that gives you horseback skills and confidence to be in the backcountry\nCreate a memorable experience walking along imposing vertical rocks of a stunning canyon. Guests will explore the geology of this rocky landscape with a geologist guide. This relaxing 3.5 hour tour includes pictographs, hunting for Devonian fossils and a blissful rest stop at a waterfall. An excellent hike for families and those with a spirit of curiosity!\nLearn now to hike and camp safely in bear country. Receive hands on experience with certified local professionals. Get need to know information about wildlife encounters and how to use Bear Spray properly. Learn about mountain survival, basic tracking and survival plant identification. This fun and educational hike will help prepare you for your next adventure in the Rockies. After this intro into hiking you will be ready to take on the mountain trails with confidence.\nExplore the majestic mountain views and turquoise lakes of Lake Louise and Moraine Lake on this all-day small-group guided tour of Banff National Park. The day includes short hikes with a naturalist guide that offer views of the Rockies and the local wildlife. Plus, enjoy a picturesque chairlift ride at Lake Louise.\nAdventure into the Canadian Rockies area during this hiking tour. Along your way, catch sight of Grotto Mountain, see cool rock formations, and learn about the Hopi natives. Your guide knows how to get around even in the harshest snow, and ice-walking crampons are provided for you, so you can easily trek through the snow-covered ground.\nCooking in the back-country is an art. It takes careful planning, cool outdoor gear and light weight ingredients. At Peak Nic, you will experience how it feels to be completely self-sufficient in the great outdoors.\nEquipped with a small backpack full of ingredients, outdoor cooking gear and a hiking pole, you will be hiking a 3.5 KM intermediate trail to a famous local peak Old Fort Point. Once at the top, we will setup camp and I will teach you how to cook delicious food in the outdoors using light and flavor-packed ingredients with a ""leave-no-trace"" approach. We will then sit and enjoy our creations while taking in panoramic views.\nJoin one of our certified guides in the Kananaskis Village, then depart for Canada\'s highest paved pass! Relax and enjoy the views in our spacious van en route to the trailhead. Your hike will begin at 7250 feet above sea level; take in the breathtaking scenery as you hike to an elevation of 8000 feet, the perfect habitat for bighorn sheep, pikas and an incredible array of alpine plants. Follow the creek and waterfalls back on this easy 2-3 hour hike. Suitable for children 8 years and older. Please schedule 4.5 hours for this excursion.\nJoin Kananaskis Outfitters\' expert guide on a beautiful hike through the forest before exploring this picturesque waterfall; take in the views as you hike back along the Kananaskis River. This 2-hour, 4 km easy hike is suitable for your whole family. With return shuttle service provided from Kananaskis Outfitters to the trailhead. Sit back, relax and enjoy the fresh air and amazing mountain views.', 'Canada is a country with more than her fair share of lakes. In reality, eight percent of the country’s landmass is covered by the fresh water bodies. That’s a fact which puts Canada in prime position on the list of countries with the most surface area covered by lakes.\nLakes are beautiful and just as environmentally important as oceans, yet they remain pretty much uninvestigated. Yes, data is available on how big they are, what their depth is and there’s plenty of information on estimations of their volume. We know they play an important part in the hydrologic cycle, but what exactly lies beneath the waters is pretty much a mystery. The world takes lakes for granted. We shouldn’t. Lakes are amazing.\nIf you really want to see lakes in all their true natural glory, Canada should be your number one place to visit. Check out the most incredible fifteen here and you’ll start to wonder why you haven’t been there sooner.\n15. Wedgemount Lake\nWedgemount Lake nestles just below Wedge Mountain in the Garibaldi Mountain Range in British Columbia, Canada. It’s encompassed by two glaciers, the Wedgemount and the Armchair. At its deepest point it’s estimated to be around sixty metres deep and five at its shallowest. The lake can only be reached after a steep, seven-kilometre climb which you’ll need some good boots for and the right equipment. Don’t plan on catching your dinner while you’re up there. Recent studies have shown the lake to have no fish life.\nWhy Go? The incredible turquoise waters of Wedgemount Lake are surrounded by some of the most stunning scenery you’ll ever see. It’s comparable with a glacial moonscape. Because of its geographical position the night skies are light pollution free which makes it a great place for overnight star spotting.\nMap Location: Lake Wedgemount\n14. Lake Superior\nIf you’re planning on visiting Lake Superior, don’t think you’ll be able to take a leisurely stroll all the way around its shores. Lake Superior, with a surface area of almost thirty-two thousand square miles, is as big as some countries. It spreads its watery reach from Ontario in the north of Canada, over the border and through several American states.\nWhen they named it, they named it right, because it really is superior in many ways. Not only is it the largest of the Great Lakes, it’s also the largest freshwater lake in the world. Surprisingly enough, it only comes in third where the volume of water it holds is concerned. You’ll need some good muscles if you’re considering rowing across it though. It measures a staggering one hundred and sixty plus miles at its widest point. Now that’s some lake.\nWhy Go? While you might not be able to walk around Lake Superior, you can drive around it. It’s a one thousand two hundred mile road trip of a lifetime through national parks with amazing canyons, cliffs, and waterfalls. A definite must do.\nMap Location: Lake Superior\n13. Berg Lake\nBerg Lake lies just underneath the highest peak in the Rockies, Mount Robson, which is in the Canadian province of British Columbia. It’s tucked away out of sight at an altitude of just under five and a half thousand feet. The only way to reach it is by hiking through the wilds of Mount Robson Provincial Park. Is it worth the effort? Definitely. The azure waters of Berg Lake are backed by impressive mountains and bordered by pine forests. The landscape and light are a photographers dream.\nWhy Go? Berg Lake is a glacial lake fed by the Berg Glacier. As it is constantly moving, chunks frequently drop off the glacier and into the lake waters to float there like mini icebergs. To get to Berg Lake you need to pass through the incredible Valley of a Thousand Waterfalls which, on its own, is reason enough for making the trip.\nMap Location: Berg Lake\n12. Lake Ontario\nLake Ontario, in the Canadian province of Ontario, is bordered not just by Canada, but by the US state of New York too. It would be easy to compare Lake Ontario to one big faucet as the lake provides drinking water to over nine million people. That’s a lot of water coming out of the smallest of the Great Lakes every day. It might be diminutive, but what it lacks in superficial square footage it makes up for in depth. Being over eight hundred feet at its deepest point means it holds more water than lakes which are bigger in size. An unusual fact which makes it the fourteenth largest lake in the world.\nWhy Go? Lake Ontario is rimmed with beautiful beaches and cliffs. It’s the perfect place for practicing water sports of every kind. From windsurfing, kayaking and paddle boarding. The bonus? If you fall off you don’t have to worry about swallowing the water, it’s fresh not salt. While you’re there, keep your eyes peeled and see if you can spot Lake Ontario’s monster of the deep, Ogopogo.\nMap Location: Lake Ontario\n11. Maligne Lake\nMaligne Lake is a stunning fourteen mile-long stretch of bright blue water at the base of the Maligne Mountains. Part of the Jasper National Park in Alberta, Canada, its glacial waters are on average just over a hundred feet deep in most parts, but in some points plunge down as far as three hundred. It’s forty-five miles of shoreline is bordered with pine forests. It’s also world renowned for the tiny, but much-photographed islet called Spirit Island.\nWhy Go? From on the waters of Lake Maligne it’s actually possible to see all three of the different glaciers which feed the lake. It’s also a great place for sport fishing, with the main catch being rainbow trout, and kayaking and canoeing. So if you have a pioneering spirit, Lake Maligne should be on your bucket list, but keep an eye out for bears, wolves, and caribou.\nMap Location: Maligne Lake\n10. Spotted Lake\nSpotted Lake, in the Similkameen Valley in British Columbia, when full of water isn’t that much to look at. It’s also probably the smallest patch of water to be labeled lake too. It’s just under half a mile long and less than a quarter of a mile wide. Makes you wonder what all the fuss is about, doesn’t it? But when Spotted Lake dries out in the summer months, it becomes one of those weird and wonderful creations of Mother Nature that are just absolutely amazing. The lake’s waters have an exceptionally high mineral content and as they evaporate, they leave behind a strange lunar-like surface full of crystallized pathways and pools.\nWhy go? Spotted Lake was revered as a sacred site by the local indigenous tribes who realized it had amazing medicinal and therapeutic qualities. It also magically changes color as the season progresses and the water evaporation increases. It really is one of those bizarre things which just has to be seen to be believed.\nMap Location: Spotted Lake\n9. Abraham Lake\nAlthough Abraham Lake, in Alberta, Canada, is called a lake it is, in fact, a man-made reservoir. The twenty mile long stretch of water was created with the construction of the Bighorn Dam in the early nineteen seventies. It’s quite narrow measuring just over two miles at its widest point and with a surface area of around twenty-one square miles it doesn’t show up on any list of the world’s largest lakes. Abraham Lake has become famous for creating its own kind of magic.\nWhy go? During the harsh Canadian winter, Abraham Lake freezes over and as the ice forms it traps bubbles rising from the depths inside it. These frozen bubbles are a freak of nature caused by bacteria rotting the underwater vegetation. Getting to Abraham Lake to photograph the bubbles should be on every adventurous photographer’s list of must do’s.\nMap Location: Abraham Lake\n8. Great Slave Lake\nThe Great Slave Lake, in Canada’s Northwest territories, is ranked as the tenth largest lake in the world. What really sets it apart from other lakes though is its depth. At just over two thousand feet in some parts, it’s the deepest in North America. It’s not a lake you can explore in one day either as it measures an incredible ten and a half thousand square miles, is almost three hundred miles long and its width is over a hundred and twenty miles at the widest point. Yes, it is enormous.\nWhy go? The Great Slave Lake’s remoteness means it’s a part of the world which is pretty much untouched by progress. It really is an unspoilt wilderness which is perfect for kayaking and fishing. Parts of the lake can be frozen over for up to eight months of the year and are cut off from civilization, so be careful when you go or you could be there for much longer than expected.\nMap Location: Great Slave Lake\n7. Lake Minnewanka\nLake Minnewanka, tucked away in the mountains near the town of Banff in the Banff National Park, is thirteen miles long which makes it the second longest mountain lake in Canada. It’s surrounded by stunning snow-capped mountains and its chilling, glacial blue waters can reach depths of up to five hundred feet. The lake’s volume has also been increased over the years, with water levels rising almost a hundred feet, after dams were built to provide hydro-power to Banff. What’s so special about it?\nWhy go? After the dams were built and Lake Minnewanka’s water levels rose so dramatically, they flooded a small village, Minnewanka Landing, which was on the shoreline. Now completely submerged, it’s become an amazing scuba diving site with a whole abandoned ghost village to explore underwater. If you’re there in the winter, there’s also a chance to go ice diving if you can handle the cold.\nMap Location: Lake Minnewanka\n6. Waterton Lake\nWaterton Lake is a lake with dual nationality. It is located partly in Canada, near Alberta and partly in the US, near Montana. Although it’s split into two distinct parts, upper and lower, it’s joined by the Bosporus channel so is classed as one lake rather than two. Waterton Lake, at an altitude of over four thousand feet and with a surface area of approximately five square miles, is surrounded by national park lands renowned for their biodiversity. Its waters have an average depth of around two hundred and fifty feet, but this can change to almost five hundred feet in some places. That’s pretty deep.\nWhy go? Waterton Lake is situated in the Waterton Lakes National Park. It’s an environment so uniquely special it’s been classed as a UNESCO World Heritage Site, an International Peace Park, and a Biosphere Reserve. It’s the perfect place to really get back to nature by hiking some of the park’s amazing trails. Though if you do go, make sure you’re bear aware.\nMap Location: Waterton Lake\n5. Peyto Lake\nPeyto Lake is another of those stretches of water which will take you by surprise at the startling azure blue color of its waters. Located high up in the Canadian Rockies at an altitude of over six thousand feet, it is incorporated in the boundaries of the Banff National Park. It’s a small, but easily accessed glacial lake which measures just under two miles at its longest point, only half a mile at its widest and has a surface area of two square miles. As far as Canadian lakes go, it’s quite petit.\nWhy go? Even though Peyto Lake is up in the mountains, it’s is relatively easy to get to. Which is great if you’re not the backpacking, trailblazing, hiking type. It means you don’t have to miss out. Peyto Lake can be reached by driving through the Banff National Park on highway 93 or the Icefields Parkway to call it by its other name. Don’t feel guilty for not hiking, the photographs you take will be just as stunningly impressive as if you’d walked.\nMap Location: Peyto Lake\n4. Emerald Lake\nEmerald Lake can be found in the Yoho National Park which is in British Columbia, Canada. The lake really lives up to its name by being a stunning emerald green in color when it’s in its liquid form. High up at an altitude of over four thousand feet, Emerald Lake can remain frozen for periods as long as seven months of the year so don’t plan on going before at least July. It may be the largest lake in Yoho National Park, but its shoreline only measures just over three miles long and is completely circumvented by a hiking trail which takes around one and a half hours to fully complete.\nWhy go? Emerald Lake is in a quiet and very secluded spot but is easy to get to with a vehicle. It’s a great place for observing eagles and ospreys in their natural habitat while hiking the trail. You can also canoe across the beautiful green and tranquil waters and get a feel of how it might have been for the original indigenous inhabitants of the area.\nMap Location: Emerald Lake\n3. Garibaldi Lake\nGaribaldi Lake is a large stretch of water geographically located in British Columbia, Canada between the townships of Whistler and Squamish. Its surface area spreads for an amazing two thousand five hundred acres through the Garibaldi Provincial Park, which is a lot of lake. It has an average depth of just under four hundred feet which can plummet down as far as almost nine hundred feet in places. Garibaldi Lake can only be reached by hiking the Garibaldi Lake Trail which is around five and a half miles long.\nWhy go? The turquoise waters of Lake Garibaldi are so crystalline, the surface acts like a mirror and reflects super clear images of the surrounding landscapes. Great for photography. If you’re into winter sports like snowshoeing and backcountry skiing then this is a place you really should be aiming to visit.\nMap Location: Garibaldi Lake\n2. Lake Louise\nLake Louise is a glacial lake fed by the Lefroy Glacier and can be found in the Banff National Park, Alberta, Canada. It’s a small, but stunning lake which measures just over a mile in length, a third of a mile in width and has only a third of a square mile of surface area. With a maximum depth of around two hundred and thirty feet, it’s almost as deep as it is wide. Its close proximity to the small town of Lake Louise has made it a popular tourist attraction.\nWhy go? This is the ideal Canadian lake to visit if you’re unsure of being too far away from civilization. Spend the day wandering around the wilderness of Lake Louise’s shores, enjoy some horse riding through the forests or even rock climbing if you’ve got a head for heights. Then, rather than roughing it at a campsite, spend the night at the Château Lake Louise, the huge and very luxurious hotel built right by the water’s edge. No-one said you couldn’t visit Canadian lakes in style, did they?\nMap Location: Lake Louise\n1. Moraine Lake\nMoraine Lake nestles in a landscape which looks like the perfect picture postcard image. Pine forests, symmetrical mountain peaks and the lake’s ice blue waters shimmering between them. This impeccable stretch of glacial water is to be found in the Banff National Park, near the township of Lake Louise, in Alberta, Canada. Moraine Lake covers an area of around one hundred and twenty acres in the Valley of Ten Peaks and reaches almost fifty foot in depth in places.\nWhy go? Moraine Lake is one of the most photographed spots in all of Canada. It’s been replicated on everything from paper money to video games and even used as log-in screens for major technology companies. You’ve probably seen it digitally reproduced, so why not go and see the real thing? Believe it, it’s even better live, so make sure you don’t miss out by not going to see it.\nMap Location: Moraine Lake']"	['<urn:uuid:5d9c2beb-2b2a-4ca5-ab11-1d740efd7828>', '<urn:uuid:42847a9a-736c-4b8a-af6d-7fbf0ecc364b>']	open-ended	with-premise	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T19:47:23.148022	27	114	4417
58	What's the best way to propagate magnolias?	Both layering and air layering are effective methods for propagating magnolias. Air layering can be done in May or early June by selecting a shoot from the previous year's growth, making an incision, applying hormone rooting powder, wrapping the cut in wet sphagnum moss, and covering it with a polythene sleeve. Regular layering can also be done in spring by pegging down a prepared branch into the soil until roots form.	['Propagating Shrubs and Trees – Layering/Air Layering\nPropagating Shrubs and Trees by Layering\nLayering is a good method of and trees that are particularly difficult to raise from cuttings. There are many kinds which lend themselves to this method of increase, including azaleas, rhododendrons, all the viburnums, magnolias, kalmias, camellias, daphnes, lilacs, cotoneasters, clematis and wisteria. Layering actually mimics the process by which some species propagate themselves naturally anyway.\nBranches on many plants come into contact with the ground as they grow or are weighed down by their foliage and from this point, buds can shoot roots into the ground and this new root system will eventually become established enough to support the branch as a plant in its own right.\nThe Method of Layering\nLayering merely involves the rooting of shoots or branches while they are still attached to the parent plant. The best time, I find, is in the spring but it can also be done throughout the summer. Choose a branch or shoot that is near to the, or one that can be brought down comfortably to soil level; and always use a shoot of the previous year’s growth, not an old shoot.\nBefore I peg down the layer I prepare the soil thoroughly by forking it over and incorporating liberal quantities of peat and sand. To prepare the shoot, make a 2 inch long slit with a sharp knife, half way through it, lengthways, and preferably through a joint. This cut should be made 9 to 12 inches from the tip of the shoot.\nThen make a depression in the prepared soil about 3 to 4 inches deep. The part of the stem which has been cut is pegged down into this hollow, ensuring that the cut remains open. Use either a wire or wooden peg. I always tie the shoot to a i cane so that it is held in an upright position. Then the part of the shoot which is pegged down is covered with soil which must be made really firm. After layering, just keep the soil moist until the shoot has rooted. Placing a large, flat stone over the layered part of the branch will help to keep it moist and assist in good root formation.\nThe Rooted Layers\nSome layers will root during the same summer, in which case they can be severed from the parent plant in the autumn and planted in a nursery bed. Others, such as rhododendrons, azaleas and kalmias, usually take two seasons to form their roots. No layers should be severed until a good root system has been formed.\nAir Layering Shrubs and Trees\nAir layering is also an easy way of increasing or propagating shrubs and trees that are particularly difficult to raise from cuttings; for example, azaleas, rhododendrons, acers, and magnolias. It is basically the same as ordinary layering except that the shoot is not rooted in the soil but in its normal position on the plant, the prepared area being enclosed in wet sphagnum moss which is held in place with a polythene sleeve. I do my air layering in May or early June.\nThe Method of Air Layering\nAgain, it is a matter of selecting a shoot of the previous year’s growth and making an incision as I have just described for normal layering. The cut should be kept open with a small piece of wood and then dusted with a hormone rooting powder. There is a rooting powder on the market especially for air layering. The cut is wrapped in a large handful of wet sphagnum moss – some is pushed inside the cut and then covered with a polythene sleeve. This must fit fairly tightly and each end is sealed to retain moisture.\nYou will be able to tell quite easily when the air layer has rooted as the white roots will begin to emerge through the moss. At this stage the layer may be severed from the parent plant and potted. Water it in well and place it in a propagating case or a polythene tent until it becomes established in the pot.\nRead more about air layering roses to see exactly how this propagation method works.\nA Grafting is a more specialised form of propagation and is a quick method of raising plants. It is particularly suitable for hybrid kinds which you want to be true to type.\nGrafting involves uniting living parts of plants so that they form a permanent union. One plant supplies the root system only and is called the ‘stock’. A small part of the plant of the variety required is joined to the stock and is known as the ‘scion’. This eventually produces the shoots and branches.\nIt is essential that the stock and scion should be compatible ie. that they will unite or grow together. In most cases they are of the same genus. Sometimes plants of different genera can be grafted – for example, amelanchier on to Sorbus aucuparia.\nSelection of Stocks\nVarieties ofcan be grafted on to the common holly; Crab Apples on to the common Crab or apple stocks; ornamental peaches, plums and almonds on to Common Mussel plum stock; ornamental cherries on to Prunus avium, the Gean or wild cherry; rhododendrons on to R. ponticum; amelanchier and varieties of Mountain Ash on to Sorbus aucuparia; crataegus varieties on to the common Quickthorn and laburnums on to L. vulgare. You will notice that most plant varieties are grafted on to their common counterpart.\nPreparation of Stocks\nThe stocks are bought from nurseries and planted in rich, well prepared soil during October or November. They are then left to grow until the following March when grafting can take place. During this time it pays to rub off all buds on the stems 9 to 12 inches from the ground.\nSelecting the Scions\nI use one year old shoots for grafting as these are not too thick. They can be cut from the parent plant in February, tied in bundles and heeled in under a north wall until they are required in March. Just before use they must be washed free of soil.\nWhip and Tongue Grafting\nThis is the most popular method and is used for propagating shrubs and trees of many varieties. For this, and any other type of grafting, always use a really sharp knife.\nFirst, the stock is cut down to within 3 or 4 inches of the ground, then a slanting upward cut, 1-1/2 inches long, is made at the top of the stock. This should remove about half the thickness of the stock.\nThen a small downward cut is made in the cut surface near the top, thereby forming a ‘tongue’. To prepare the scion, the selected shoots are cut so that each piece contains three or four buds. Do not use the soft tips of the shoots. The base of the scion is prepared by making a slanting downward cut similar to the one on the stock, but opposite a bud. Then a tongue is cut so that it corresponds with the one on the stock.\nThe tongue of the scion is fitted into the one on the stock. If the cut surfaces are perfectly smooth they will fit closely together. The graft is then bound very tightly with raffia which in turn is covered with grafting wax to render it airtight and watertight. If the widths of stock and scion differ, then it is essential that one side of each should meet perfectly. This is to allow the cambium layers – seen immediately under the bark of stock and scion to unite. Unless the cambium layers meet somewhere a union will not occur.\nOnce the scion is growing vigorously the raffia may be cut away, and shoots or buds which appear on the stock should be rubbed out regularly.']	['<urn:uuid:580226b7-9cc5-40fd-b2d3-180e91a83ffe>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T19:47:23.148022	7	71	1306
59	I'm a boat owner who frequently visits Port Clinton Harbor and I'm wondering why these jetties are so important for the area. What would happen if they failed to maintain them?	The failure of the jetties would have serious consequences for Port Clinton Harbor. It would result in increased shoaling that would choke the mouth of the Portage River and cause increased wave action up the river. This would curtail ferry service to the islands and negatively impact marina operations. Additionally, the public park and beach, which are protected by the jetties from west to northwest winds, would likely suffer erosion.	"['Contract awarded to repair Port Clinton Harbor East Jetty\nPORT CLINTON - The east jetty many residents and visitors use to fish, walk and take sunset pictures from will be closed this summer — but for good reason: $300,000 in renovations.\nThe U.S. Army Corps of Engineers (USACE), Buffalo District has awarded a $375,243 contract to GeoGradel of Toledo, to repair a sections of the Port Clinton Harbor East Jetty that was damaged during Superstorm Sandy.\nGeoGradel will begin work after the Fourth of July holiday and continue through the summer into early October.\nThe proposed project consists of repairs to the jetty head by replacing existing damaged structure with new rubble mound armor stone overlay, a 4-foot wide cast-in-place concrete walkway with new light foundation and returning the structure crest to the original elevation.\nIn October 2012, Superstorm Sandy battered the east coast of the U.S., but damage was felt on the Great Lakes as well as the storm moved inland across the Midwest.\n""The Port Clinton Harbor East Jetty experienced extensive damage during Sandy due to oversized waves and the extreme angle they were hitting the structure,"" said Capt. Kelly Polashenski, USACE project manager. ""Because the damage was caused by Sandy, the jetty was eligible for funding to repair the structure using Superstorm Sandy appropriations.""\nGeoGradel project manager Wayne Moon said the company would be using a crane barge, a tugboat and a material barge to work on the project.\nGeoGradel is also working on portions of the Lorain East Breakwater wall and at Edgewater Park in Cleveland, which were both damaged by Sandy, he said.\n""We\'ll put up a barricade 250 feet around from our work area and other signs detailing the project,"" Moon said Thursday.\nMoon expected to have an average of five workers on-site each day, five days a week. Moon noted the river is especially busy with boaters and ferries during the weekends, so they will avoid them.\nThe river\'s ""No Wake"" sign and ""Power Line"" signs will be temporarily removed, he said, but the laws still apply to those on the water.\n""We ask that boaters please exercise caution when in the area,"" Moon said.\nThe USACE began studying the east jetty in June 2010, with engineers noting that failure of either jetty would result in ""increased shoaling choking the mouth of the Portage River as well as increased wave action propagating up the river.""\nFerry service to the islands would be curtailed and effects on marina operations would be ""detrimental,"" the study noted.\n""The public park and beach are protected by the jetties from wave action due to a west to northwest wind and would likely suffer erosion,"" a release from USACE read.\n""I welcome this step in restoring and improving this vital harbor,"" Congresswoman Marcy Kaptur (D-Toledo) said. ""The Port Clinton Harbor is central to several local private marinas, almost eighty seasonal charter fishing boats and the local recreational boating community.""\n""Port Clinton\'s recreation and tourism industry is a key component of our state\'s economy. This important restoration project will help to protect the shoreline and ensure that the Port Clinton Harbor can continue to support the region\'s boating activity and tourism jobs,"" U.S. Sen. Sherrod Brown said.']"	['<urn:uuid:2ed30fa1-7978-45f0-b481-8ed4e88b103d>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T19:47:23.148022	31	70	537
60	store manager seeking tips customer mailing list management importance regular correspondence with customers	Regular correspondence with customers through a mailing list is crucial for retail success. It's not sufficient to simply have a mailing list - you must actively use and update this resource while providing meaningful information to customers. A good practice is to ask customers at the cash/wrap counter if they're interested in joining the mailing list if they haven't already done so.	"['You need to make it as easy as possible for shoppers to get to your store. If your store is in an odd location, publish directions in your advertising and make sure your staff is prepared to provide clear directions over the phone.\nAdvertising: Do you have an in-store link to your outside advertising?\nIt’s amazing how many stores spend a bundle on advertising, while neglecting to ""connect the dots"" between their ads and their store. One easy way to do this is to display your ads on a sign board at the front of the shop. Use your logo in advertising, and as a consistent presence in your shop. Spotlight the items featured in your ad with special displays or signage. Be consistent with your advertising, so customers will come to recognize your advertising style.\nBasics: Are your shelves stocked with basics?\nBe sure always to stock the basics. Rain checks are no longer an acceptable replacement for out-of-stock items. You want the people who stop in for a quick purchase to return to your store—don’t turn them away with under-stocked shelves.\nBathrooms: Sigh. This really does matter!\nAre your bathrooms clean? Bathrooms are an important part of customer service. Do you keep them clean and well-lit? Are paper towels, soap, etc. always available? Do you have an employee frequently “inspect” them to assure they are well-maintained?\nCash/Wrap: Is your cash/wrap counter organized and free of clutter?\nThe cash/wrap is your customers’ last impression of your store. Keep it neat and representative of your store’s dedication to service and organization. Be mindful of open storage shelves that often become a haven for clutter. Keep office supplies neat and organized.\nCleanliness: Is your store clean?\nCleanliness should start in the parking lot, continue through your entrance and carry throughout your entire store. Remember that tidiness is an important indication of your store pride. Make cleaning and tidying your shop a daily activity.\nCompetitive (But Practical) Prices: Is your merchandise priced appropriately?\nStudies show that 20 percent of customers are true price hounds and will chase down the lowest price. The other 80 percent care about price, but also take into consideration what else the store has to offer. When pricing merchandise, don’t be distracted by the 20% who are price-hounds and lose sight of the 80% who are looking for total value. Keep your prices competitive, but don’t lock yourself into a habit of hasty discounting in order to chase fickle price shoppers.\nCustomer Mailing List: Do you ask customers if they’d like to enter their e-mail address on a mailing list?\nDon’t underestimate the importance of regular correspondence with customers. It’s not enough just to have a mailing list—you must continually use this resource and update it, providing your customers with meaningful information. At the cash/wrap, ask customers if they’re interested in joining your mailing list if they haven’t already.\nDisplays: Are they timely? Are they fun? Are they constantly changing?\nIf a display seems to be turning slowly and not generating a lot of interest, consider making some changes. Use creative signage to add interest, replace it with another display, couple it with faster turning products or move it to a different part of the store. Just make sure it’s eye-catching.\nFixtures: Do the fixtures showcase the merchandise?\nThe best fixtures highlight merchandise in a discreet way. Choose fixtures that are practical and durable, but won’t draw the attention away from the products with gimmicky colors or design.\nIn-Store Graphics and Signage: Are your graphics and signs clear? Are they colorful? Do they have a theme?\nGraphics and signage should never detract from or compete with the merchandise. Their goal should simply be to draw attention to various products, discounts and special offers. Make sure your signage communicates its message immediately. Customers…']"	['<urn:uuid:837f5f2e-87e9-4fe2-bb67-024ddb13ef4d>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T19:47:23.148022	13	62	634
61	small enterprise startup financial strategy importance planning cost evaluation	Business planning is vital for financial stability and opportunity management. A business plan helps deal with day-to-day needs and navigate both predictable and unpredictable situations. For funding purposes, it must demonstrate ability to repay loans with required ROI. When evaluating costs, it typically takes 2-3 years for a new business to become profitable, though this varies by industry. The financial section should include current status, future outlook, and assessment of resources needed to get started. This planning helps determine if you have necessary tools and qualifications to implement the business idea effectively.	['Whether you are a veteran business owner, have recently begun your own venture, or are still in the dreaming stages, you have invariably spent many hours thinking about and envisioning your organization. While it is always exciting to try to realize your dream in your mind and to project that image into your future, the realization that barriers, stumbling blocks, and necessary “to-do’s” exist. These subjects range from broad to very specific, and can include questions such as:\nWho is my ideal client?\nWhat would be a good name for my organization?\nWhat is my unique selling proposition (USP)?\nHow will I balance my work life with my personal life?\nHow big do I want this business to be?\nDo I have the necessary resources? If not, where can I get them? If so, how do I most effectively utilize them?\nThe list goes on for pages and pages; indeed, there are numerous resources that outline these very details, and putting some time into exploring these ideas is always a smart move. For some specific ideas, check out some of the free articles on offer by MEG Enterprises.\nThe reasoning behind developing a business plan for businesses of all sizes can vary, but business plans are most often created for two primary reasons: as a management & planning tool, and to acquire funding for operational business needs.\nManagement & Planning Tool\nIf you are like many other small business owners, you are not only the owner of the company, you are likely also actively involved in the day-to-day operations as the President/CEO, the marketing department, the IT department, the HR department…the list goes on and on! One of the most important things to remember in trying to bring all of this together into a cohesive and efficient package is that planning is vital!\nMany small businesses take a “fly by the seat of your pants” approach to operating their businesses. For example, let’s say that an excellent business opportunity arose for you, an opportunity that would net your business $5,000 over the next 3 months. However, in order to take advantage of this opportunity, you need an initial cash outlay of $1,000. Do you have the resources necessary to take advantage of this opportunity? If your answer is no, you may have been able to easily accomplish this goal by planning for such expenses in advance through a business plan. Even if having cash at the ready is not a viable alternative for you, you may have planned to have a line of credit available for such opportunities, knowing that in your field these opportunities do arise from time to time.\nIn a more broad sense, business planning helps businesses of all sizes to deal with the day-to-day needs of the organization by forcing the owner to weed through the operations of a typical work day. Business planning will assist you in understanding how to effectively market your business, how to understand and plan for financial stability both now and in the future, how to carry out your daily operations with a necessary level of routine, and so forth. Moreover, while unpredictable issues will certainly always arise, effective business planning will not only help you to navigate the predictable operations, but will also take these unpredictable situations into account. This will help you to deal with these issues with a level of comfort and ease, knowing that you have thought through and planned for such events.\nIn essence, here is a great way to think of superior business planning. Imagine you are taking a road trip from California to New York. No planning (“flying by the seat of your pants”) would involve you getting in the car and driving “East” on every freeway you come across. Adequate planning would involve mapping out your course, planning where to stay overnight, where to eat, sights to see, and so forth. This is a much better plan indeed. However, a superior plan would take all of these ideas to the next level by planning for “what if”: what will I do if I get a flat tire, if I run out of gas, if someone gets sick, or if I lose my wallet? You can see how the superior plan is clearly the best in most situations in that it allows for flexibility, plans for the expected and the unexpected, and allows you to spend more time enjoying the trip, knowing that you have all of your bases covered.\nHELP…I Need Cash! (AKA Creating a Business Plan to Acquire Funding)\nAnother reason to create a business plan is to acquire funding. In today’s struggling economy, having access to cash as a small business is vital. In developing plans for this reason, a much more specific approach is taken. Here, the plan is created with a specific reader in mind: the lender. Whether seeking funds from a bank, an angel investor, or so forth, knowing your audience is vital.\nHow do you create an effective business plan in this situation? Well, simply stated, place yourself in the shoes of the person lending the money. What would you as the lender want to read in a plan? First and foremost, these individuals want to see that you have demonstrated the ability to repay the loan with the required level of return on investment (ROI) and within the required time frame.\nThese areas require that you present a strong case for your proposed financial expectations, grounded firmly in the supporting information of your plan, including marketing, market analysis, business operations, and so forth. Having confidence in your business and in yourself will assist you in demonstrating the potential for your company and in being able to deliver what your investor is looking for. Doing your due diligence and knowing the facts surrounding your business and your market will prove to be of great benefit when selling your business case, both in writing and verbally, to the lender you are seeking funding from.\nSo, How Do I Create A Business Plan? What Does It All Come Down To?\nAlthough the term “business plan” conjures many negative images in the eyes of some business owners, taking a step-by-step approach will prove that creating a business plan is much less daunting than one might imagine. Although no two plans are exactly the same (the necessary details of the plan can vary between companies), the contents of a typical business plan include the following topics:\nExecutive Summary – Sell your business to your reader!\nBusiness Overview – Giving a general summary of the business.\nMarket & Competitive Analysis – What environment are you competing in?\nMarketing & Sales Strategy – How will you “win” in your market?\nOrganization Plan – How is your organization structured?\nFinancial Projections – Current status and future outlook.\nFunding Sought (if required)\nKey Milestones – What are your specific & achievable goals?\nCritical Risks – What keeps you awake at night?\nDoes this look like a lot to you? Well, believe me, as you truly delve into the details of the plan and your business, you will be wondering why there isn’t more room for details!\nThe most important aspect of business planning (the “What does it all come down to?” part), however, is spending the time to do your research (“due diligence”) and critically thinking about these various aspects of your business. Of course, it is impossible to anticipate every detail simply by spending time thinking and writing; it is for this reason that plans are referred to and viewed as “dynamic”. However, the more issues and scenarios you are able to come up with ahead of time, the more prepared you will be to handle these as they arise.\nSo, in the end, I encourage you to realign whatever preconceived notions you may have of the business planning process and view it not as a daunting task or a necessary evil of running a successful business. Instead, view it as yet another opportunity, the chance to help make your dreams into a reality by mapping out the needs of your business, your customers, your employees, your suppliers, your community…and yourself!', 'Evaluating business ideas can save your time and money. Analyze your business concept and check if it’s worth pursuing. Answering the questions we’ve gathered below should help. Read on.\nBusiness idea – table of contents:\n- Is your business idea unique?\n- Does your business idea solve a relevant problem?\n- Will the customer be happy to pay for your product?\n- Who are your competitors?\n- Do you have access to the tools you need to get started?\n- Are you qualified to implement your business idea?\n- How long will it take your business to break even?\nIs your business idea unique?\nFor starters, it’s worth considering whether your business idea is truly unique. It often turns out that many companies are already working on the same or a similar concept simultaneously. Therefore, it might be more difficult for your company to break into the market.\nSometimes, however, it is possible to find your niche even in a highly competitive industry. You should also check whether any other company has already tried to implement a similar idea in the past. If they have, you need to analyze why it has failed to turn into a profitable business.\nDoes your business idea solve a relevant problem?\nA startup must add value to the market and ought to solve a real problem your potential customers face. It would help if you did not confuse a mere inconvenience with a real issue. An inconvenience can be irritating and uncomfortable. Nevertheless, for most people it is perfectly acceptable.\nIf you’ve already identified the problem and determined that it exists, it’s time to answer the question: does my idea solve it? Make sure that it’s possible to prepare a product that meets your potential customers’ expectations and improves their quality of life.\nWill the customer be happy to pay for your product?\nThe bigger the problem your product or service solves, the higher the profit you can expect from selling it. You can charge more for products and services that provide unique solutions. However, if you are introducing extra functionalities to the existing products or you are addressing the problem from a different angle, you can’t go overboard with the price.\nWho are your competitors?\nWhen evaluating the potential of your business idea, you shouldn’t ignore your competition. Identify potential products that can become a threat to your concept and check how your competitors are doing in the market. Will you be able to keep your business afloat? What are the strengths and weaknesses of your competitor’s product and how does your solution compare to that? What changes should you make to give your company a competetive advantage?\nDo you have access to the tools you need to get started?\nTake time to assess the resources you have at your disposal. Do you have the necessary tools to start a business and develop the product or service you have come up with? If you don’t have all the things you need, will you be able to get them quickly?\nAre you qualified to implement your business idea?\nAre you the right person to implement the idea? How qualified are you to build a business based on this concept? Although you can easily find people with the relevant qualifications to develop your startup, you should have good business skills in order to be able to critically assess the progress of work on the product and understand the difficulties arising from it. Most highly successful startup founders had the right qualifications to implement their ideas.\nHow long will it take your business to break even?\nBecoming profitable quickly is the goal of any company. Assess how long it might take to get your business idea off the ground. It usually takes two or three years before a new business turns a profit, although it might take longer in some industries. However, if you think that you will reach the break-even point in ten years, perhaps you should search for another business idea.\nWant more advice? Check out: What is peer-to-peer feedback?\nLaunch your startup:\n- What is a startup?\n- Pros and cons of creating a startup\n- 8 best industries for startups\n- Top 5 skills every highly successful startup founder needs\n- How to create a startup? 7 simple and easy steps\n- 6 essential startup development stages\n- How to create a startup growth strategy?\n- General startup statistics you need to know\n- Startup vs. corporate job. Which is right for you?\n- 5 incredible companies that started in a garage\n- How to find a business idea?\n- How to check if your startup idea already exists?\n- How to name a startup? Useful tips and strategies\n- How to gain business knowledge quickly? 5 best practices\n- Why do startups fail? 6 startup ideas you should avoid\n- 5 absurd startup ideas that made money\n- Top 6 most profitable small businesses\n- 7 questions to determine if your business idea is worth pursuing\n- What is a buyer persona? 5 benefits of creating a buyer persona\n- How to validate your business idea? 3 easy steps\n- Should you follow your passion? The importance of passion in business\n- What is market reseach and why is it important?\n- Using social media in business\n- What to do when you have too many business ideas?\n- How to write a good problem statement for your startup?\n- How to test your business idea for real?\n- How to create a prototype for a product?\n- How to build an MVP?\n- How to use surveys for testing your business idea?\n- 10 useful tools to validate your business idea\n- What is a business plan? 4 types of business plans\n- What should be included in a business plan?\n- What should a product description include?\n- Competitor analysis\n- Marketing strategy\n- Traditional business plan vs. lean startup plan\n- Implementation plan. What is it and how to create it?\n- Everything you need to know about patents\n- Financial management for startups\n- What permits and licenses does my startup need?\n- What is the average startup founder salary?\n- 4 startup taxes you need to pay\n- Which legal structure is best for your business?\n- Startup costs. How much money will you need?\n- Protection of intellectual property in a startup\n- Family funding vs. self-funding\n- What is a shareholders’ agreement?\n- What should a financial section of a business plan include?']	['<urn:uuid:1e7011c0-e939-43d9-9bf0-fb32220a004d>', '<urn:uuid:933a820a-ee2b-4e5b-b1a6-21e9b5b44689>']	open-ended	with-premise	long-search-query	distant-from-document	multi-aspect	expert	2025-05-12T19:47:23.148022	9	92	2446
62	What are the educational programs offered for communication skills development, and how does this compare to the physical aspects of yoga practice?	The communication skills development is offered through Toastmasters educational programs, specifically the Competent Communication track which includes 10 speech projects from ice breaker to inspiring audiences, and the Advanced Communication Series with 15 manuals covering areas like entertaining speaking and speeches by management. As for yoga's physical practice, it focuses on the third limb called asana, which involves physical postures designed to align the body and open channels for energy flow. The practice connects body movement to breath rhythm, making it more than just physical exercise by incorporating mind-body awareness and inner focus.	['Our Educational Programs\nHave you ever wonder how our members improve their English communication and leadership skills?\nCU Gavel Club is an affiliate of Toastmasters International in the United States of America. We offer two of Toastmasters educational programs for our club members to develop their communication and leadership skills. In more details, the two educational programs are Communication and Leadership track.\nMoreover, we also provide the Basic Organization Leadership program for the members interested in preparing for the future careers by taking parts as our club officers. Let us introduce you all of our development programs we offer at our club!\nCU Gavel Club members build their communication skills begin with Competent Communication.\nThis manual consists of basic 10 speech projects to develop communication skills one step at a time. When a member finishes all of the projects, the member will receive Competent Gavelier Communicator award. Toastmasters International will send a Competent Gavelier Communicator certificate to the member with the signature of Toastmasters International President.\nProject 1: The ice breaker\nProject 2: Organize your speech\nProject 3: Get to the point\nProject 4: How to say it\nProject 5: Your body speaks\nProject 6: Vocal variety\nProject 7: Research your topic\nProject 8: Get comfortable with visual aids\nProject 9: Persuade with power\nProject 10: Inspire your audience\nAdvanced Communication Track\nGaveilers can develop more advanced communication skills through the Advanced Communication Series manuals. There are 15 Advanced Communication manuals. Each contains five speech projects. In addition, many of the manuals are career-oriented. Gaveliers can choose the manuals they want to complete, or the skills they want to learn.\nThe Entertaining Speaker\nSpeaking to inform\nThe Discussion Leader\nSpeeches by Management\nCommunication On Television\nSpecial Occasion Speeches\nCU Gavel Club members interested in building their leadership capabilities begin with Competent Leadership.\nThis is the core of the leadership development program for Gaveliers. The Competent Leadership manual features 10 projects that they complete while serving in various club meeting roles. An evaluator will give feedback on each project, helping them to improve. When they complete the manual, they will be granted the Competent Leader award, and a Competent Leader certificate will be sent from Toastmasters International to the Gavelier with the signature of Toastmasters International President.\nProject 1: Listening\nProject 2: Critical Thinking\nProject 3: Giving Feedback\nProject 4: Time Management\nProject 5: Planning and Implementation\nProject 6: Organization and Delegation\nProject 7: Facilitation\nProject 8: Motivation\nProject 9: Mentoring\nProject 10: Team Building\nBasic Organization Leadership\nGaveliers can also gain the basic leadership experiences as club officers of CU Gavel Club. With a close guidance from the club advisors, the members will learn to operate a small organization starting from planning strategies, executing operations, collaborating with people and creating the best values. Moreover, the responsibilities from being club officers can also be credited for the projects in Competent Leader.\nThe positions where the members can take and learn the basic leadership are as follow:\nVice President Education\nVice President Membership\nVice President Public Relations\nSergeant At Arms\nThink of taking our educational program?\nOur educational programs are exclusively for CU Gavel Club members. Learn more about the benefits of being a Gavelier and start your journey with us!', 'Classes, Pricing, Membership\nHow do I register for class?\nRegister for class by visiting our MindBody site. You will be prompted to login (or create one) with your email address. Click ‘Drop-In Yoga’ to see the schedule and click ‘Sign Up’ from there.\nHow much does a class cost?\nWe have a variety of pricing options available for our weekly classes including single visit (Drop-In Price) and multiple visit packages (10-class pass, or 10-class pass with auto-renew). We also have membership options for unlimited weekly drop-in classes. Please visit our website for most up to date pricing information.\nI have no experience, what is the best class for me to start with?\nIf you are an absolute beginner, the very best class for you is Yoga for Beginners. If those times don’t suit you, then we can also recommend Hatha (All Levels), Gentle Yoga, Eischens Yoga and Yin Yoga classes. Please visit our class schedule for times and descriptions.\nDo you have mats and props at the studio?\nYes – we have mats and all other props that you might need during class.\nWhat should I wear?\nCome to class wearing comfortable, loose-fitting clothing. It is important for your body to be able to move freely, without restriction. No special footwear is required because you will be barefoot.\nDo you have childcare?\nYes! Anyone taking a class at The Yoga Centre, may use the childcare at The Tennis and Fitness Centre, located across the street from the studio. The cost is $3/hour. The childcare hours are M-Sa (8AM–12PM), and M&W (5PM–7PM). In the summer months, the hours are slightly different, please check the Tennis and Fitness Centre website for most up to date information.\nWhat is the relationship between The Tennis and Fitness Centre and The Yoga Centre?\nThe Tennis and Fitness Centre (TFC) owns The Yoga Centre, which started its yoga program more than 25 years ago. As it gained popularity, the program grew out of the space available at TFC. Likewise, in 2011, TFC expanded its operations into space across the street by opening up The Yoga Centre. What’s wonderful about this arrangement is that The Yoga Centre is open to the community and TFC members enjoy unlimited weekly drop-in yoga classes with a Yoga + membership purchased at The Tennis and Fitness Centre. Another option for TFC members is the $10 drop-in fee. If you’ve never been to TFC, come on over and take a tour! Visit our website at www.tenandfit.com or contact our staff with questions at email@example.com.\nDo I have to be a member to attend classes at The Yoga Centre?\nNo – of course not! All of our classes at The Yoga Centre are open to the community. The only classes that are for members-only are held at The Tennis and Fitness Centre, across the street.\nWhat yoga classes are included in my Tennis and Fitness Centre membership?\nYoga, Philosophy, and Guidelines for Practice\nWhat is Yoga?\nThe word yoga, from the Sanskrit word yuj means to yoke or bind and is often interpreted as “union” or a method of discipline. A male who practices yoga is called a yogi, a female practitioner, a yogini.\nThe Indian sage Patanjali is believed to have collated the practice of yoga into the Yoga Sutra an estimated 2,000 years ago. The Sutra is a collection of 195 statements that serves as a philosophical guidebook for most of the yoga that is practiced today. It also outlines eight limbs of yoga: the yamas (restraints), niyamas (observances), asana (postures), pranayama (breathing), pratyahara (withdrawal of senses), dharana (concentration), dhyani (meditation), and samadhi (absorption). As we explore these eight limbs, we begin by refining our behavior in the outer world, and then we focus inwardly until we reach samadhi (liberation, enlightenment).\nToday most people practicing yoga are engaged in the third limb, asana, which is a program of physical postures designed to purify the body and provide the physical strength and stamina required for long periods of meditation.\nWhat does Hatha mean?\nHatha Yoga is the study of the physical postures (“asanas”) that are designed to align your skin, muscles, and bones and open the many channels of the body—especially the main channel, the spine—so that energy can flow freely.\nHatha is also translated as “ha” meaning “sun” and “tha” meaning “moon.” This refers to the balance of masculine aspects—active, hot, sun—and feminine aspects—receptive, cool, moon—within all of us. Hatha yoga is a path toward creating balance and uniting opposites. In our physical bodies, we develop a balance of strength and flexibility. We also learn to balance our effort and surrender in each pose. In this way, Hatha yoga can be a powerful tool for self-transformation. It asks us to bring our attention to our breath, which helps us to still the fluctuations of the mind and be more present in the unfolding of each moment.\nIn a group Hatha Yoga class at our studio, the sequencing and presentation of the postures is open to the creativity of the teacher. You should expect a balanced class with both heating and cooling postures. Some teachers may also incorporate breathing and meditation techniques. Every teacher’s style is their own, making each class a unique experience.\nI’m not flexible—can I do yoga?\nYes! You are a perfect candidate for yoga. Many people think that they need to be flexible to begin yoga, but that’s a little bit like thinking that you need to be able to play tennis in order to take tennis lessons. Come as you are and you will find that yoga practice will help you become more flexible. This newfound agility will be balanced by strength, coordination, and enhanced cardiovascular health, as well as a sense of physical confidence and overall well-being.\nHow many times per week should I practice?\nYoga is amazing—even if you only practice for one hour a week, you will experience the benefits of the practice. If you can do more than that, you will certainly experience more benefits. Likewise, making the commitment to attend one class/week is an excellent first step. You will likely find that after a while, your desire to practice expands naturally. Ideally, we work towards building a personal practice in which we practice every day, any amount.\nI’ve heard it’s better not to eat at least two hours before class, why is that?\nIn yoga practice, we twist from side to side, turn upside down, and bend forward and backward. If you have not fully digested your last meal, it will make itself known to you in ways that are not comfortable. If you are a person with a fast-acting digestive system and are afraid you might get hungry or feel weak during yoga class, experiment with a light snack such as yogurt, a few nuts, or juice about 30 minutes to an hour before class.\nHow is yoga different from stretching or other kinds of fitness?\nUnlike stretching or various forms of fitness, yoga is more than just a physical practice because we connect the movement of the body to the rhythm of our breath. Connecting the body and breath helps us to direct our attention inward and quiet the mind. Through this process of inward attention, we become more aware of our own thoughts and experiences from moment to moment. The awareness that we cultivate is what makes yoga a practice, or a way of being. Your body will most likely become much more flexible by doing yoga, and so will your mind.\nWhat are the Eight Limbs of Yoga?\nThe eight limbs of yoga, as described in The Yoga Sutra, are as follows: the yamas (restraints), niyamas (observances), asana (postures), pranayama (breathing), pratyahara (withdrawal of senses), dharana (concentration), dhyani (meditation), and samadhi (absorption). As we explore these eight limbs, we begin by refining our behavior in the outer world, and then we focus inwardly until we reach samadhi (liberation, enlightenment).\nIs Yoga a Religion?\nYoga is not a religion. It is a philosophy that began in India an estimated 5,000 years ago. The father of classical ashtanga yoga (the eight-limbed path, not to be confused with Sri K. Pattabhi Jois’ Ashtanga yoga) is said to be Patanjali, who wrote the Yoga Sutra. These scriptures provide a framework for spiritual growth and mastery over the physical and mental body. Yoga sometimes interweaves other philosophies such as Hinduism or Buddhism, but it is not necessary to study those paths in order to practice or study yoga.\nIt is also not necessary to surrender your own religious beliefs to practice yoga.']	['<urn:uuid:19e7b996-3aea-4a27-8062-f16940a0f916>', '<urn:uuid:1b8100d9-50bf-4b2a-8b7d-2d0c2f348e76>']	open-ended	direct	verbose-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T19:47:23.148022	22	93	1968
63	As a reinsurer, how can I assess financial losses to my building portfolio?	As a reinsurer, you can calculate the average annual loss and probable maximum loss to your portfolio of buildings using your own input exposure data.	"[""We envisage the following stakeholder groups to benefit from the platform, GEM's research output and other related products and services we are collaboratively working on:\n- the wider scientific community (from geo sciences to social sciences), with a special focus on collaborators in regional programmes\n- governmental agencies, organisations and individuals working on risk financing\n- organisations and individuals working in the construction industry\n- governmental agencies, organisations and individuals working on disaster risk management\n- media and the public at large\nThere are many possible applications of the model and below we provide a number of examples that demonstrate how different stakeholders might use our platform and resources, in some cases together with their own input data.\n- A country’s Ministry of Economy would like to find out how the average household income is affected by a possible earthquake, in the short, medium and long term.\n- A civil protection department would like to see the expected distribution of damage and fatalities within an urban area for a selected scenario earthquake for emergency management planning.\n- An international agency would like to compare relative earthquake risk for the areas they operate in (over 2 continents).\n- A company would like to gain insight in the effect of possible earthquake scenarios on the gross domestic product (GDP) of the countries in which it has operations.\n- A risk manager of a multinational would like to get a global overview of the risk of the various production sites of his/her company.\n- An urban planner would like to calculate risk maps within a given region for a given building typology in order to identify the areas of a large city with higher levels of risk.\n- A reinsurer or global primary insurer would like to calculate the average annual loss and probable maximum loss to a portfolio of buildings (based on their own input exposure data).\n- A geophysicist would like to calculate the expected seismic motion on bedrock for a given location, in order to defi ne the reference motion needed for site effect analysis.\n- A geologist would like to carry out a new tectonic analysis in proximity of a dam, and would like to download data on active faults as a starting point for his/her study.\n- An engineer who is working on the design of a bridge located in a zone with seismic activity, would like to obtain uniform hazard spectra at different return periods for different performance limit states.\n- A university researcher would like to access GEM's platform as an expert user in order to produce his/her own PSHA input model using the Modeller’s Toolkit, and run hazard calculations with OpenQuake.\n- An individual would like to understand how hazardous the area is, where (s)he is planning to buy a house.\nGEM is going through a continual user-needs assessment process, to ensure that the model, the platform and the OpenQuake software will meet the needs of stakeholders worldwide. We are working to support both the needs of academics and others with profound expertise on hazard and risk estimation, and of those working in the fields of risk assessment and mitigation, or those with an interest in deepening their knowledge about earthquake risk.\n- Scientists and others with experience in hazard/loss calculations are trial-using the OpenQuake Engine.\n- The Nexus collaboration website allows all those that work on GEM to share their ideas and output of that work with each other and the wider community, and to jointly discuss the standards, tools and databases that will be integrated into the model.\n- A project was carried out to discover the needs of non-expert users in developing and non-developing areas of the world.""]"	['<urn:uuid:4a90040e-1934-4469-8193-92c20492c9e9>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T19:47:23.148022	13	25	616
64	irrigation system design process steps	The process of designing irrigation systems involves gathering information about the local water table and weather data, analyzing crop water demands to determine reservoir sizes, examining elevation and slope changes for optimal reservoir placement, and calculating water flow rates to determine pipe types and sizes. The system is designed so water flows from canals through pipes into reservoirs, from which farmers can pump water to their fields.	"[""Agricultural engineering is a combination of engineering technology and biological science applied to the field of agriculture. These engineers apply their knowledge and skills to improve sustainable agriculture production. Agricultural engineers are involved in many diverse projects, including the design of machinery and structures, such as housing for large hog operations, and the development of methods to conserve soil and water and improve the processing of agricultural products.\nAt a glance\nImagine you are standing in a hilly field of potatoes watching as a crane slowly lowers a 10-metre section of aluminum pipe into a freshly dug trench. Behind the crane, a crew is busy welding the pipe sections together. You are watching all this activity because this is your design. You're an agricultural engineer who specializes in designing irrigation systems, which is why the local irrigation district has hired you. Several summers of drought conditions have decreased the productivity of dry-land fields, and the irrigation district needed to find a way to extend its system and carry water to the parched areas. You have been brought in to design a system of piping and reservoirs that will give farmers the water they need to grow their crops.\nAs an agricultural engineer, you combine your knowledge of pipeline design and construction with information on the area's geography, climate, and crop rotations in order to design a new system for piping irrigation water from the district's canals to dry-land fields. You gather information on the local water table and historical weather data, particularly the longest periods between rainfall, as well as the water demands of different crops, so you know how large to make the reservoirs and connecting pipes. Then you look at localized elevation and slope changes to determine the optimal placement for the reservoirs. These reservoirs need to be down grade from the canal, so the forces of gravity will keep the water moving. Finally, you use your irrigation schedule and water-flow rates to determine the type and size of pipe needed. When completed, water will flow from the canals through a system of pipes you have designed and into reservoirs from which farmers can pump the water onto their fields.\nDuties vary significantly from job to job, but the following list includes typical job duties one might encounter as an agricultural engineer:\n- Plan, design, and supervise the building of irrigation, drainage, and flood- and water-control systems.\n- Plan, design, and supervise the construction of agricultural buildings and storage facilities, such as livestock structures, greenhouses, silos, and cold-storage facilities for apples.\n- Design and evaluate equipment used for ground preparation, seeding, spraying, harvesting, and transporting agricultural goods.\n- Ensure design is consistent with local codes and all required permits obtained.\n- Supervise the cleaning, milling, grading, mixing, processing, cooling, packaging, and distribution of agricultural products.\n- Prepare and present technical reports, meet with clients, and communicate technical concepts to colleagues and clients with non-technical backgrounds.\n- Conduct research to find new sustainable ways to produce food and fibre for consumers.\n- Conduct research for the design of innovative new structures and systems.\n- Conduct research to develop new technologies and management practices for food production that protect environmental resources.\nAgricultural engineers work in a variety of locations, including:\nIn the office:\n- Doing paperwork, analyzing data, and preparing reports\n- Drafting construction plans and designing computerized management plans\n- Planning projects, administering and managing budgets\n- Communicating on the phone and in meetings with clients, stakeholders, and government departments and presenting to clients designs, ideas, and recommendations\n- Researching and collecting background information and consulting with other engineers and professionals""]"	['<urn:uuid:c5e28022-b2bd-4542-b000-176d8181ba4b>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T19:47:23.148022	5	67	602
65	Do cartels stay more stable than perfect competition in the long term?	No, cartels are inherently unstable, especially with cost differences between firms and free entry of new firms who may not join the cartel, while perfect competition reaches a stable long-run equilibrium through entry and exit until profits are driven to zero.	['Table of Contents\n2. Oligopoly: Some concepts and definitions\nThere are different types of market orientation in different geographies and for different products or verticals. It can be perfect competition or monopolistic or may be a duopoly. But in the reality, probably the most important and common nature of competition and the market structure is “Oligopoly“, which can also be defined as “Competition among the Few”.\nSo, setting prices independently is very rare or almost non-existent in the oligopolistic markets. Some kind of understanding between the firms arises, may be either in the form of a formal agreement or even in a tacit way. A formal agreement is one when the oligopolists agree after discussion to observe certain common rules of conduct in regard to price and output determination. So this kind of an oligopolistic situation is generally termed as Collusive Oligopoly. But more often we find that the agreement between the firms is a tacit one, as in most of the countries a formal or open agreements to form monopolies are illegal.\nUnder oligopoly, a firm can not assume that its rival will keep their prices unchanged when it makes change in its own price. So the demand curve facing an oligopolists looses its definiteness and as well as its own significance. It goes on constantly shifting as the rivals change their prices in reaction to the price changes by that firm.\nSOME CONCEPTS AND DEFINITIONS\nOligopoly: An economic condition in which a small number of sellers exert control over the market price of a commodity.\nCartels: In a Cartel type of collusion, firms jointly fix a price and output policy through agreements. Basically, the term ‘cartel’ was used for the agreement in which there existed a common agency which alone undertook the selling operations of all the firms that were party to the agreement. But now-a-days all types of formal and informal agreements reached among the oligopolistic firms of an industry are known as Cartels.\nNON COLLUSIVE OLIGOPOLY:\n1. Common characteristics of non-collusive oligopoly is that they assume certain pattern of reaction of competitors, in each period and despite the fact that the expected reaction does not in fact materialize , the firm continue to assume that the initial assumption holds.\n2. In other words, Firms are assumed never learn from past experience which makes their behavior at least naïve.\nWorking of oligopoly:\n1. Market Sharing by Non Price Competition: Here only a uniform price is set and the member firms are free to produce and sell amount of outputs which will maximize their individual profits. Though the firms agree not to sell products below a certain floor level, but they are free to vary the style of their products and advertising expenditure and to promote sales in the other ways. 2. Market Sharing by Quota: Here the member firms agree regarding the quota of output to be produced and sold by each of them at the agreed price. It is worth mentioning that the all types of cartels are unstable, when there exists cost difference between the firms. The low cost firms always have the tendency to reduce the price of the product to maximize their profits which ultimately result in the collapse of the collusive agreement. Again, if the entry of the firms in the oligopolistic industry is free, the instability of the cartels is intensified as the new entrants may not join the cartel and may fix a lower price of the product to sell a larger quantity. This means that the stability of the cartel agreement is always in danger.\nOligopoly in Telecom sector (INDIA)\nIndia’s telecom sector has made rapid progress since the announcement of the National Telecom Policy – 1999 (NTP-1999). Since 2000, the telecom sector has been a key contributor...', '8.1 Perfect Competition and Why It Matters\nA perfectly competitive firm is a price taker, which means that it must accept the equilibrium price at which it sells goods. If a perfectly competitive firm attempts to charge even a tiny amount more than the market price, it will be unable to make any sales. In a perfectly competitive market there are thousands of sellers, easy entry, and identical products. A short-run production period is when firms are producing with some fixed inputs. Long-run equilibrium in a perfectly competitive industry occurs after all firms have entered and exited the industry and seller profits are driven to zero.\nPerfect competition means that there are many sellers, there is easy entry and exiting of firms, products are identical from one seller to another, and sellers are price takers.\n8.2 How Perfectly Competitive Firms Make Output Decisions\nAs a perfectly competitive firm produces a greater quantity of output, its total revenue steadily increases at a constant rate determined by the given market price. Profits will be highest (or losses will be smallest) at the quantity of output where total revenues exceed total costs by the greatest amount (or where total revenues fall short of total costs by the smallest amount). Alternatively, profits will be highest where marginal revenue, which is price for a perfectly competitive firm, is equal to marginal cost. If the market price faced by a perfectly competitive firm is above average cost at the profit-maximizing quantity of output, then the firm is making profits. If the market price is below average cost at the profit-maximizing quantity of output, then the firm is making losses.\nIf the market price is equal to average cost at the profit-maximizing level of output, then the firm is making zero profits. We call the point where the marginal cost curve crosses the average cost curve, at the minimum of the average cost curve, the “zero profit point.” If the market price that a perfectly competitive firm faces is below average variable cost at the profit-maximizing quantity of output, then the firm should shut down operations immediately. If the market price that a perfectly competitive firm faces is above average variable cost, but below average cost, then the firm should continue producing in the short run, but exit in the long run. We call the point where the marginal cost curve crosses the average variable cost curve the shutdown point.\n8.3 Entry and Exit Decisions in the Long Run\nIn the long run, firms will respond to profits through a process of entry, where existing firms expand output and new firms enter the market. Conversely, firms will react to losses in the long run through a process of exit, in which existing firms cease production altogether. Through the process of entry in response to profits and exit in response to losses, the price level in a perfectly competitive market will move toward the zero-profit point, where the marginal cost curve crosses the AC curve at the minimum of the average cost curve.\nThe long-run supply curve shows the long-run output supplied by firms in three different types of industries: constant cost, increasing cost, and decreasing cost.\n8.4 Efficiency in Perfectly Competitive Markets\nLong-run equilibrium in perfectly competitive markets meets two important conditions: allocative efficiency and productive efficiency. These two conditions have important implications. First, resources are allocated to their best alternative use. Second, they provide the maximum satisfaction attainable by society.']	['<urn:uuid:1b42d436-5227-489a-b960-c6151b256e39>', '<urn:uuid:742be25f-2f35-4cf2-98be-ac237d6a05c4>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T19:47:23.148022	12	41	1203
66	transport network risks solutions worldwide	Global transport networks face interconnected risks as they function as a 'system of systems' where different modes (seaports, airports, rail, roads, waterways) are interdependent. Key risks include hurricanes, floods, sea-level rise affecting coastal infrastructure, and climate-driven changes in temperature and precipitation. To address these vulnerabilities, several solutions are recommended: implementing whole-of-government approaches, identifying critical infrastructure, including resilience in regulations and planning, improving decision-making through better data and tools, and providing appropriate financing. New international standards like ISO 14090 are being developed to help organizations assess climate impacts and develop adaptation plans across entire transport systems and supply chains.	"[""A new report finds that the extra cost of building resilience into infrastructure systems is only 3% of overall investment needs, and yet would provide an enormous $4.2 trillion in net benefits over the lifetime of new infrastructure.\nThe World Bank’s Lifelines: The Resilient Infrastructure Opportunity report provides a framework for understanding infrastructure resilience by examining four essential infrastructure systems: power, water and sanitation, transport, and telecommunications.\nIf countries continue to build infrastructure that is not resilient to climate change, the additional cost will be $1 billion over the next decade, the report says. Climate change makes action on resilience even more important as, on average, climate change doubles the net benefits from taking resilience measures. In Bangladesh, for example, $1.6 billion has been avoided in damages to power systems thanks to $560 million in flood protection spending.\nReliable infrastructure such as water and sanitation, energy, transport, and telecommunications are essential for raising people's quality of life. It is therefore critical that infrastructure can withstand the impacts of climate change. When powerlines fail, water pipes are damaged, and transport links are wiped out, people’s lives can be put in danger.\nAnalysis in the report shows that one in three Kampalans would not reach a hospital in time because of road disruptions during just moderate floods. In the Democratic Republic of Congo, after one day of water disruption, incidence rates of suspected cholera increased by 155%, compared to the incident rate when there was optimal water provision.\nDisruption to infrastructure costs households and firms in low- and middle-income countries at least $390 billion a year, and indirect costs increase the burden too.\nIn Tanzania, for example, the cost to firms from outages and disruptions in power, water, and transport due to rainfall and flood is $250 million per year. Therefore, ensuring that essential infrastructure systems are resilient to the physical risks climate change presents not only avoids costly repairs, but also minimizes the effect and extent of the consequences of natural disasters on people’s livelihoods and well-being.\nThe trick is to spend better—not necessarily to spend more. By investing in regulations and planning in the early stages of project design, and in maintenance, the costs of repairs or reconstruction after a disaster strikes can be significantly reduced.\nUnwarranted expenses can be minimized by planning based on providing services rather than assets and helping these service users to manage disruptions. Good infrastructure management is key—a one-off resilience intervention will not suffice. Instead, a range of coordinated actions will be required, collaborating across sectors and governance. As such, the report outlines five recommendations on how to unlock the $4.2 trillion opportunity.\nThe trick is to spend better—not necessarily to spend more. By investing in regulations and planning in the early stages of project design, and in maintenance, the costs of repairs or reconstruction after a disaster strikes can be significantly reduced .\nGet the basics right: Poor management and governance largely explain why infrastructure systems underperform. While more money may be spent on infrastructure systems, improvement will be hampered by (i) a lack of enforcement of regulations, construction codes, and procurement rules; (ii) not having a system for appropriate infrastructure operation, maintenance, and post incident response; and (iii) not providing appropriate funding and financing for infrastructure planning, construction, and maintenance.\nBuild institutions for resilience: Governments need to play a coordination role and provide a certain way forward in order to overcome political economy challenges. They should prioritize actions that will (i) implement a whole-of-government approach, facilitating information exchange, and build on existing regulatory systems; (ii) identify critical infrastructure and define intolerable and acceptable risk levels; and (iii) ensure equitable access to resilient infrastructure, by using approaches that also assess well-being loss and socioeconomic resilience for both poor and rich households.\nInclude resilience in regulations and incentives: Too often, decision-makers only consider lower repair costs when deciding on investments in resilience, and rarely take into account other impact costs. Governments can help by including resilience in regulations and financial incentives to align parties’ interests. This includes (i) considering resilience objectives in master plans, standards, and regulations, and adjusting them regularly to account for climate change; (ii) creating financial incentives for service providers to promote resilience infrastructure services; and (iii) ensuring that infrastructure regulations are consistent with risk-informed land use plans and guide development toward safe areas.\nImprove decision-making: Even with financial incentives and regulations, infrastructure providers will not be able to do much without access to data and tools, and the skills to put them to use. Governments therefore need to help stakeholders by (i) investing in freely accessible natural hazard and climate change data; (ii) making robust decisions via stress-testing that will work across a range of uncertain futures, minimizing potential for regret and catastrophic failures; and (iii) developing the skills needed to use data and models to mobilize the practical know-how of the private sector.\nProvide appropriate financing: Affordability and financing constraint will also inhibit action on making infrastructure more resilient. Making resilient changes may well result in decreased repair costs, but may also incur greater design, construction, and maintenance costs. Therefore, there needs to be (i) provision of adequate funding to include risk assessments in master plans and early project design; (ii) a government-wide financial protection strategy and contingency plan in order to raise funds in the aftermath of a disaster; and (iii) a promotion of transparency to better inform investors and decision-makers of the physical risks associated with investments and assets.\nCentral to all five of these recommendations is the emphasis on implementing resilience thinking in the early design and planning stages of infrastructure services. These early stages are where a little investment in resilience thinking can go a long way and significantly improve the overall resilience of infrastructure systems, securing people’s well-being and livelihoods.\nDownload a copy of the Lifelines report here.\n Jeandron, A., J. M. Saidi, A. Kapama, M. Burhole, F. Birembano, T. Vandevelde, A. Gasparrini, B. Armstrong, S. Cairncross, and J. H. J. Ensink. 2015. “Water Supply Interruptions and Sus- pected Cholera Incidence: A Time-Series Regression in the Democratic Republic of the Congo.” PLoS Medicine 12 (10): 1–16. https:// doi.org/10.1371/journal.pmed.1001893."", ""Following an UNCTAD expert meeting on adapting to climate change in the transport sector, held on 16 and 17 April in Geneva, Switzerland, John Dora, member of the International Organization for Standardization's climate change strategy task force, shares his perspectives on the issue.\nWhy is climate change adaptation becoming important for international transport?\nTransport has been traditionally looked upon as a challenge in terms of reducing greenhouse gas emissions, and a lot of effort has been rightly directed at that issue.\nIn recent years, however, with the Sustainable Development Goals and the Paris Climate Agreement, adaptation of transport to the future climate, once a 'poor relation' in terms of climate change response, is increasingly being recognised as vital to the continued success of mobility and global trade and development.\nWhat's new is the growing realization that individual transport modes and their infrastructure - seaports, airports, rail routes, roads, inland waterways - have a collective interdependence on each other.\nTransport is a ‘system of systems’ and resilience of each transport mode to the impact of future weather patterns along the entire network of global supply chains warrants consideration so that impacts, risks and vulnerabilities across transport modes are identified and addressed.\nWhat kind of risks are we thinking about?\nWe can think about extreme weather and events – hurricanes/cyclones and floods - which can disrupt transport routes both inland and on the coasts. Other climate factors that may affect transportation include climate change-driven changes in temperature, humidity and precipitation.\nCoastal transportation infrastructure, in particular seaports, will be impacted by sea-level rise, which will exacerbate coastal flooding during extreme storm events. Road and rail infrastructure, inland waterways and airports that are already prone to flooding may face more frequent inland flood events and deeper floodwaters under climate change.\nThere is a potential for infrastructure damage, as well as for disruption and delay, throughout global supply chains, which can have major economic impacts. In the light of what is at stake, there is an important need for greater focus on transport adaptation, as the discussions at the UNCTAD Ad-hoc Expert Meeting highlighted. Guidance and standards will play an important role in this respect.\nAre any international sets of standards in place or being developed to aid transport and infrastructure providers?\nIn the summer, a new International Standard, ISO 14090 'Adaptation to climate change - Principles, requirements and guidelines' is to be published. I had the privilege of being the convenor of the international expert team that drafted it. ISO 14090 describes international best practice for adaptation that transport and other organisations can use to develop their own adaptation plans. It will be useful for others also with an interest - stakeholders like the finance sector. Investment banks - multilateral development banks, for instance, seek ways to verify that the loans they issue will be funding infrastructure that will be resilient to the future climate. Currently, much infrastructure is built to technical standards that relate to the past climate, not the future one. ISO 14090 will permit users to assess impacts on the infrastructure and the entire transport system and supply chain from the future climate perspective.\nWhat is meant by technical standards and why are they important?\nTechnical standards are those used by engineers, for example, to design vehicles and infrastructure so they are safe - that bridges can carry the weight of vehicles, that drainage is designed to carry floodwaters, that cranes and towers are resistant to wind loads and so on. Generally, these standards relate to weather patterns in the past. For the future, we will need technical standards that factor in climate change.\nSome individual organisations have issued guidelines on designing structures for future weather patterns, and CEN/CENELEC, the European standards body, has been mandated by the European Commission to provide guidance to modify infrastructure standards to cater for future weather and climate vulnerabilities.\nIt is important to note that individual organisations - infrastructure owners and operators - can produce their own specifications for climate resilience and need not wait for formal 'CEN' or 'ISO' standards, although ISO 14090 will provide a useful framework for adaptation.""]"	['<urn:uuid:054933ab-84b3-4c52-b815-6719e8fb008a>', '<urn:uuid:fecb0663-68a9-4db3-801a-6a836066b070>']	open-ended	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	5	98	1709
67	what features define american great novel beloved huckleberry finn differences similarities	The Great American Novel must contain and explain the whole country, and both Beloved and Huckleberry Finn qualify by addressing America's racial divide. They follow the script of 'imagining across or from within' the country's major social divisions, especially between black and white. However, while Huckleberry Finn is more straightforward in its approach, Beloved is described as more 'challenging and refractory,' particularly in its treatment of American history through the horrifying central tragedy of Sethe's infanticide. Both books have become more than mere literature and are now mainstays of high-school and college syllabi, though Beloved has particularly distinguished itself by becoming the subject of numerous academic studies, with only Shakespeare rivaling Morrison in the number of senior theses devoted to their work.	"['Made in the U.S.A.\nFiction and critique of American society\nThe phrase “The Great American Novel” means something more than the sum of its parts. There are plenty of great American novels that are not Great American Novels: Henry James’s Portrait of a Lady doesn’t qualify, and neither does Ernest Hemingway’s The Sun Also Rises, or Willa Cather’s The Lost Lady, even though everyone acknowledges them as classics. No, the Great American Novel—always capitalized, like the United States of America itself—has to be a book that contains and explains the whole country, that makes sense of a place that remains, after 230-odd years, a mystery to itself. If other countries don’t fetishize their novels in quite this way—if the French don’t sit around waiting for someone to write the Great French Novel—it may be because no country is so much in need of explanation.\nHardly anyone talks about the Great American Novel without a tincture of irony these days. But as Lawrence Buell shows in The Dream of the Great American Novel, his comprehensive and illuminating new study, that is nothing new: American writers have always held the phrase at arm’s length, recognizing in it a kind of hubris, if not mere boosterism. Almost as soon as the concept of the Great American Novel was invented, in the nation-building years after the Civil War, Buell finds it being mocked, noting that one observer dryly put it into the same category as “other great American things such as the great American sewing-machine, the great American public school, and the great American sleeping-car.” It was enough of a cliché by 1880 for Henry James to refer to it with the acronym “GAN,” which Buell employs throughout his book.\nYet Buell warns us against taking all this dismissal at face value: “critical pissiness suggests the persistence of some sort of hydrant,” as he puts it. Even today, in our endlessly self-conscious literary era, novelists are still writing candidates for the GAN. What else are Jonathan Franzen’s Freedom, or Philip Roth’s American Pastoral, or Don DeLillo’s Underworld, if not attempts to capture the essence of American modernity between two covers?\nBuell, now Cabot research professor of American literature, does not spend much time theorizing about the Great American Novel. Instead, he seeks to illuminate the concept by analyzing some of the books that have laid claim to the title. Most of these are, by definition, mainstays of high-school and college syllabi, from Nathaniel Hawthorne’s The Scarlet Letter down to Toni Morrison’s Beloved. But alongside these classics, Buell ranges a number of lesser-known works, showing how the basic “scripts” of the Great American Novel are played out by writers like Helen Hunt Jackson in Ramona and Harold Fredric in The Damnation of Theron Ware. And he takes account of contemporary works that respond to, challenge, and rewrite the classics, such as Alice Randall’s The Wind Done Gone, a parody of Gone With the Wind.\nGone With the Wind is not what most people would think of as a Great American Novel—surely it is too middlebrow, not to mention too racist, for that distinction. But as Buell points out, the themes Margaret Mitchell writes about—slavery, the weight of Southern history, “the old-order mystique”—are the same as those of an undoubted GAN, William Faulkner’s Absalom, Absalom. (The difference is that “for every reader of Absalom, fifty had read Gone With the Wind.”) Buell proposes that GAN candidates tend to follow a few major “scripts,” and Faulkner and Mitchell are both using the same one: novels that seek to explain America by “imagining across or from within” the country’s major social divisions, especially the divisions between black and white, and between North and South.\nThe GAN candidates that follow this script manage to remain controversial even as they attain the status of classics. Uncle Tom’s Cabin, by Harriet Beecher Stowe, is Buell’s first example: a huge bestseller on publication in 1852, it was credited by some with helping to hasten the Civil War, thanks to its frontal assault on slavery. Yet as Buell notes, the book tries hard to depict slavery as a national problem, while sparing Southern sensitivities: Stowe “makes her arch villain a New Englander,” while “she makes her most brainy and articulate white character a slaveholder.” Though her depictions of black characters now strike us as deeply racist, “essentializing…Africans as inherently childlike,” Buell urges us to consider the novel as a “white person’s attempt to comprehend nonwhites at a moment when even most white northerners considered them less than fully human.”\nUncle Tom’s Cabin inaugurates a long tradition of GANs that try to bridge the racial divide—though later, more sophisticated works would focus on the ways it remained unbridgeable. Here the key example is Toni Morrison’s Beloved, the subject of one of Buell’s best chapters. Like many GANs, Buell notes, Morrison’s book “undertakes a far-reaching geographical scan of the…United States,” following its characters from Georgia to Delaware to New Jersey to Ohio. But the horrifying tragedy at its center—an escaped slave, Sethe, murders her daughter rather than see her returned to slavery—means that it remains a far more challenging and refractory work than, say, Huckleberry Finn, another example of this GAN “script.” Buell quotes Morrison’s own feeling that, in Beloved, she was treating an aspect of American history that “the characters don’t want to remember, I don’t want to remember, black people don’t want to remember, white people won’t want to remember.”\nYet as Buell insists, the GAN has always thrived by criticizing American society, not by celebrating it. “Great American Novels are not expected to be rituals of self-congratulation like July 4 celebrations or Hollywood melodramas,” he writes. “On the contrary, the historical record suggests that serious contenders are much more likely to insist that national greatness is unproven, that its pretensions are hollow, and that the ship of state is going down.”\nThat is certainly the approach of two other prime candidates for GAN-hood: John Dos Passos’s USA and John Steinbeck’s The Grapes of Wrath. These books, for Buell, follow a second script—the one originated by Moby-Dick, the grandfather of all GANs—which tries to encompass all of American life, in almost sociological fashion, through sheer breadth of vision. They are “sprawling performances of encyclopedic scope with multiple agendas from the ethnographic to the metaphysical.” But where the democratic crew of the Pequod is destroyed by the monomania of Captain Ahab, the cast of USA—12 characters drawn from across the range of socioeconomic types—are dragged down by the mediocrity and money-madness of pre-Depression America.\nAs Buell cannily notes, the language of the characters in USA is not “the speech of the people,” as Dos Passos claims, but a manufactured “slanguage,” showing how Americans’ minds have been colonized by “newsreel argot and the platitudes of professional wordsmiths.” Unlike Steinbeck’s Okies, who are described in a poetic plural of “groupthink, grouptalk,” Dos Passos’s people seem atomized: “social interaction becomes much more diffuse, fleeting, happenstance, compartmentalized, abstract, mediated.” One of the purposes of a book such as The Dream of the Great American Novel is to reintroduce us to forgotten classics, and USA, probably the least read of Buell’s GAN candidates, is perfectly suited for rediscovery in our own Great Recession moment.\nIf these meganovels seek to take in all of American society, a third “script” for the GAN focuses on the representative career of a single character on his or her quest for the American Dream. Buell calls these “up-from fictions,” and sees their archetype in Benjamin Franklin’s Autobiography, that story of successful self-invention. In novels, however, the journey is rarely so straightforward—whether it is the louche career of Augie in Saul Bellow’s The Adventures of Augie March, or the painful education of the nameless narrator in Ralph Ellison’s Invisible Man.\nThe most famous example, however, and one of the first titles to come to mind whenever the Great American Novel is mentioned, is F. Scott Fitzgerald’s The Great Gatsby. Gatsby, who remains a cipher even as he transforms himself into a prince of Long Island society, is the antithesis of Franklin, who made a self by making his fortune. “To read Gatsby as a symbol of the great American dreamer, then,” Buell writes, “is to read the dream itself as already sealed and gone, locked somewhere in the remote inaccessible past.” But of course, if the American Dream weren’t still alive, somewhere in our culture and our minds, it wouldn’t be necessary for the novelists to keep writing Great American Novels. And Buell’s erudite study convinces us that the death of the GAN, despite all predictions, is not coming anytime soon.', 'In 1987, Toni Morrison\'s Beloved won the Pulitzer Prize. In 1992, with Beloved still widely regarded as her masterpiece, Morrison was awarded the Nobel Prize for Literature. Three legs make a stool: This past month, in a New York Times poll of 200 critics, writers, and editors, Beloved was named ""the single best work of American fiction published in the last twenty-five years,"" beating out novels by such luminaries as Roth, DeLillo, and Updike. I participated in this survey and can attest that, from the moment the solicitously hand-typed letter from the Times Book Review arrived in the mail, Beloved was the presumptive winner.\nLike two other American novels devoted to race, The Adventures of Huckleberry Finn and To Kill a Mockingbird, Beloved exists wholly beyond its own artistic merits and demerits. These books have become something more than mere literature; they\'re homework, with an afterlife guaranteed by their place (or in the case of Huck Finn, its embattled absence) on the high-school and college syllabus. (""Only Shakespeare rivals her in the number of senior theses devoted to her work,"" Harvard English professor Barbara Johnson has said.) Were it simply a matter of social redress, we could all go home now, the Dead White Males having been forced to cocktail with a Living Black Woman. But Beloved isn\'t solely a work of protest and advocacy, as Morrison herself has insisted, nor solely a symbol for the progress and virtue of the prestige-granting institutions in American letters. It\'s a serious novel and a work of art, and it deserves to be accorded the highest respect. It deserves, in other words, to be asked, Yes, but are you any good?\nFor those who haven\'t read it, Beloved tells the story of Sethe, an ex-slave who has resettled to the outskirts of Cincinnati with her daughter, Denver. Near the beginning of the book, the two are joined by Paul D, once Sethe\'s fellow slave on a Kentucky plantation called ""Sweet Home."" (After years of thankless yearning, Paul D has at last become Sethe\'s lover.) It\'s 1873, the Civil War has been fought, and though slavery as a legal institution is over, it has only started its haunting of the African-American psyche. This Morrison dramatizes with the actual haunting of Sethe\'s house by Sethe\'s deceased baby daughter. We never learn that baby\'s given name, but in exchange for sex, Sethe has had a headstone carved for her girl, bearing the single word ""Beloved."" Paul D exorcises the house of the ghost, but later, upon returning from a carefree day spent at a carnival, Sethe, Denver, and Paul D discover a young woman sleeping near the front door of their house. The young woman goes by the name Beloved, and from all appearances she is a revenant, the embodied spirit of Sethe\'s dead daughter.\nMorrison presents Sethe\'s turbulent inner life through a process both Morrison and Sethe herself call ""rememory,"" a kind of psychic haunting in which the specifics of a traumatic incident are told and retold, even as the teller tries to block their full emergence into the conscious mind. The central traumatic episode of Beloved, to which the narrative returns again and again, is an infanticide: Twenty years earlier, Sethe beheaded her baby Beloved with a handsaw rather than allow her return to slavery. In Beloved, Morrison perfected a mode of narration, entirely her own but with roots in everything from the African griot to As I Lay Dying, built out of compulsive repetition, in which the onion, as it were, is constantly being both peeled and reconstituted; in which memories are constantly being both exhumed and buried; and in which the mind of the storyteller is both imprisoned and set free in the act of retelling. And so, like the return of Beloved, and the enduring curse of slavery itself, rememory is both a reconciliation and a vexation, both a healing and a wounding.\nNo wonder Beloved has been such a success: It draws up so elegantly on the blackboard. And in some very real sense, the central deranging tragedy of American life has found its finest expression in the oceanic rhythms of Morrison\'s prose, with its deft use of synecdoche, anaphora, and incantatory repetition, with its unique combination of seething plaintiveness and iron triumph. At the same time, though, one notices a secondary effect of these rhythms: The traumas depicted in Beloved don\'t actually feel, to the reader, particularly traumatic. On the contrary, the dominant sentiment of the book is the nobility of sheer endurance, a sentiment that transfers rather too easily to the reader\'s own self-regard. Would this nobility have transferred so easily had the book portrayed its humiliations more graphically? For a novel that will educate many Americans about racism, the grislier facts of the post-bellum world have been strangely muted—there is a single, somewhat oblique mention of the Klan (""Desperately thirsty for black blood, without which it could not live, the dragon swam the Ohio at will""), and there is almost no sense of hunger or deprivation. (""Feeding her is no trouble,"" Sethe says of Beloved. ""I pick up a little extra from the restaurant is all."") Certainly Beloved contains passages meant to invoke intense horror, but none is immediate or documentary. For vividly portrayed action in the novel\'s present tense, the following passage is more representative:\nBeloved took Denver\'s hand and placed another on Denver\'s shoulder. They danced then. Round and round the tiny room and it may have been dizziness, or feeling light and icy at once, that made Denver laugh so hard. A catching laugh that Beloved caught. The two of them, merry as kittens, swung to and fro, to and fro, until exhausted they sat on the floor.\nA Land of Cockaigne quality pervades their lives, with meat stews on the stove and pies in the oven, so that when Sethe fetches pillows for her girls ""from the keeping room,"" I had to work not to picture items out of the Cuddledown catalog. Physical humiliation is downgraded in favor of psychic humiliation, which is then dispersed into ambient waves of spiritual uplift. Sethe\'s mother-in-law was a lay preacher who once worked a spot called simply the Clearing:\nFinally she called the women to her. ""Cry,"" she told them. ""For the living and the dead. Just cry."" And without covering their eyes the women let loose.\nIt started that way: laughing children, dancing men, crying women, and then it got mixed up. Women stopped crying and danced; men sat down and cried; children danced, women laughed, children cried until, exhausted and riven, all and each lay about the Clearing damp and gasping for breath. In the silence that followed, baby Suggs, holy, offered up to them her great big heart.\nIn the universe of Beloved, rational knowledge isn\'t denigrated so much as absent—a notable regression from Uncle Tom\'s Cabin, a book written more than a hundred years earlier. In Stowe\'s novel, an interesting allegoricalconflict had been generated between the characters Uncle Tom and George Harris. As a simple man in the image of his lord and savior, Tom is totally exempt from the world of rational calculation, an innocent when it comes to profit and loss. His worth, Stowe lets us know with her signature operatic bluntness, is infinite. Harris, in the meantime, is a black man who is mentally talented. How do you stand between him and his fullest potential, Stowe asks,between him and his fair market value as a wage earner? Knowing full well that religious zeal, and not Jacksonian free enterprise, would carry the cause of abolition furthest, Stowe made Tom, and not Harris, her symbol for the brutality of racial oppression. Uncle Tom\'s Cabin, according to a bon mot often attributed to Lincoln, started the Civil War. Beloved … well, it has won a black woman the major literary prizes, hasn\'t it?\nUncle Tom\'s Cabin, Black Boy, Invisible Man … Beloved? Perhaps the reduced compass of its ideological ambitions is an expression of the book\'s feminism, in the same way, for example, the Great War has been reduced to a mere parenthetical in To The Lighthouse. And yet, though Virginia Woolf always wrote as a woman, and often as a feminist, nothing she ever wrote feels written for women. A feminist\'s rage against the facts of a man\'s world, and especially a recent tendency to mass violence, runs throughout her work. Beloved, meanwhile, is a historical novel that doesn\'t feel grounded in history. Its historical details, such as they are, feel quickly sketched, and much of its imagery is vague, combining a tendency to faux-folksy countryism with a casual use of modern slang (Paul D is said to have ""beat the shit out of"" the haunting spirit of the baby Beloved).\nWhat Beloved does feel grounded in, and firmly, is a repudiation of everything that exerts a soft but nonetheless unpleasant authority in a young person\'s life. In place of the need to master hard knowledge or brute facts, there is folk wisdom; in place of science, animism; in place of the strict father, the self-sufficient matriarchy, first of Baby Suggs\', and later Sethe\'s, house; and finally, in place of a man\'s world, the hallowed sorority of women, especially women of color—though on this last, Morrison does not insist too heavily. As Sethe, very pregnant fugitive, breaks for the Ohio border, she runs into a young white woman named Amy. At first Sethe is fearful, but Amy is raggedy and a little touched in the head and presents no danger; and eventually she helps Sethe through (a peculiarly uneventful) labor. In a passage Morrison has identified as holding out hope for racial reconciliation, Sethe and Amy swaddle the newborn on the banks of the Ohio. ""A pateroller passing would have sniggered to see two throw-away people, two lawless outlaws—a slave and a barefoot whitewoman with unpinned hair—wrapping a ten-minute-old baby in the rags they wore.""\nIf Beloved expends almost no energy mapping its historical reality—the wage system, like parturition, has been oddly gentle to Sethe—it\'s because Beloved was not designed to conjure with the past as a concrete reality. True, the past, in all its ugliness, breaks into the present, but the landscape of Morrison\'s present is mostly an interior one. Late in the novel we discover that the painfully withdrawn Denver, like George Harris before her, is clever. ""Other people said this child was simple, but Lady Jones [her teacher] never believed it. Having taught her, watched her eat up a page, a rule, a figure, she knew better."" But this is a small flourish, another aside. Beloved is not Denver\'s story, the story of a worldly gift artificially depressed by bigotry. It\'s Sethe\'s story, as Sethe stands poised before the possibility of her own therapeutic self-renewal. The word ""self"" is worth pausing here. If you asked a college student in 1967 what she most believed, she might put a finger in your face, before telling you off about the war; these days, too often she points to her own chest. In a folk ideology, the ardor with which we believe becomes its own truth. Not coincidentally, Morrison has spoken eloquently of ""the precious interior, the loved self, whatever that is, whatever vocabulary you ascribe to it, [that] is suppressed or displaced and put someplace else.""\nThe language knocked around in my head for days after I read it. Where had I heard this before? In the late \'70s, coincident with the emergence of Toni Morrison as a major novelist, a European psychologist named Alice Miller wrote a best seller called The Drama of the Gifted Child and the Search for the True Self. The opening paragraphs reads as follows:\nIn order to become whole we must try, in a long process, to discover our own personal truth, a truth that may cause pain before giving us a new sphere of freedom. If we choose instead to content ourselves with intellectual ""wisdom,"" we will remain in the sphere of illusion and self-deception. … The damage done to us during childhood cannot be undone, since we cannot change anything in our past. We can, however, change ourselves. We can repair ourselves and gain lost integrity by choosing to look more closely at the knowledge that is stored inside our bodies and bringing that knowledge closer to our awareness.\nBeloved is indeed a work of genius. No other American novel of the past 25 years has so elegantly mapped the psychobiography of its ideal reader.']"	['<urn:uuid:710879ac-54b8-4533-bc41-2787dc0595b2>', '<urn:uuid:9b0d5002-1a39-4be3-b556-43bd723f8953>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	11	122	3519
68	What resources exist for mental health support during emergencies?	Several resources exist for mental health support during emergencies. Through CLDR and CDS, mental health professionals can provide support to children and staff during camps, offering referrals for ongoing services. Additionally, in Poland, Ukrainian refugees have access to psychological help and support services at reception points, and organizations like 'Ocalenie' coordinate psychological support (requiring Ukrainian/Russian language skills).	['Official, verified charities in Poland and Ukraine\nBelow we gathered a list of official, verified organisations which are helping in different ways refugees or people who stayed in Ukraine.\nThe official Polish government website https://pomagamukrainie.gov.pl/ with verified charities. The organisations are focused on providing food, accommodation and in general taking care of refugees. The website is in Polish and in Ukrainian. We picked a few charities which have English versions and accept donations by credit/debit cards:\n- Polska Akcja Humanitarna: https://www.siepomaga.pl/en/pah-ukraina. Official website of the organization: https://www.pah.org.pl/en/\n- SOS Wioski Dziecięce (SOS Childrens’ Villages): https://pomagam3.wioskisos.org/en/ . We have known and support this organization for years. They are focused on helping orphans. Purpose of this donation is for children from orphanages in Ukraine which are in dire situation.\n- Official Ukrainian National Bank. link sent by our friend from Lviv https://bank.gov.ua/en/news/all/natsionalniy-bank-vidkriv-spetsrahunok-dlya-zboru-koshtiv-na-potrebi-armiyi\n- United Help Ukraine: https://unitedhelpukraine.org/\n- Ukrainian Government, Ministry of Digital Transformation of Ukraine, donations in cryptocurrencies: https://donate.thedigital.gov.ua/\n- Internationa Red Cross: https://www.icrc.org/en/donate/ukraine.\n- Fundacja Jaś Wędrowniczek: http://fundacjajaswedrowniczek.pl/konta-bankowe/ . Local foundation from our town of Rymanów that we are in touch with personally. A very effective and trusted people. See more: https://fundacjajaswedrowniczek-pl.translate.goog/6736-2/?_x_tr_sch=http&_x_tr_sl=pl&_x_tr_tl=en&_x_tr_hl=en&_x_tr_pto=wapp (Polish article translated by google translate)\n- Zhytomyr Community Foundation http://www.fondgromady.org.ua/en/contact/. Through our friend Alex who writes reports from Zhitomir (published in our newsletter only: https://polishorigins.com/newsletter-sign-up/) we met the head of the foundation Zhana and together try to help community of Zhitomir in their specific needs. See more about foundation and the community needs in one of the newsletter messages https://polishorigins.com/?wysija-page=1&controller=email&action=view&email_id=62&wysijap=subscriptions .\nOther ways to help:\n- Aid collection centers and warehouses in the world where you can donate goods: https://ua-aid-centers.com/ . Lists of aid needed most: https://ua-aid-centers.com/lists\n- Send a package – Polish Red Cross Organization collects food (canned food, long-term food), cosmetics (e.g. shower gels, shampoos, soaps, toothbrushes, toothpaste, diapers for children) and some medical equipment (e.g. sterile gases, sterile compresses, ampoules of physiological saline, disinfectants, disposable gloves, bandages). They also collect new blankets and sleeping bags.\nThey DO NOT collect clothes.\nIf you want to visit Poland to help:\n- Polish Red Cross cooperates with volunteers who don’t speak Polish/Ukrainian/Russian. You can bring the items you want to donate or you can donate your time and work. Just contact any office in the town you plan to visit: https://pck.pl/wp-content/uploads/2022/03/adresy-dla-zbiorek-darow-09.03.pdf\n- PAH (Polska Akcja Humanitarna) cooperates with volunteers from abroad. The only limitation is that you declare to be their volunteer for at least 2 weeks.\n- If you can come to Przemyśl, the city near the Ukrainian border, contact the regional coordination office first. They will advise where to go and how to help. Phone number for English speaking volunteers: +48 16 675 20 91. This is region where the most people cross the border.\nIf you do speak Polish at least on the communicative level you can join many other organizations. Each bigger city has it’s own volunteer organization here are just a few of them:\nOrganization „Ocalenie” coordinates volunteer’s help all over Poland: https://ocalenie.org.pl/ukraina-pomoc . Here you can also volunteer as translator, medical support, law advisors, psychology support (Ukrainian and/or Russian is needed), as well as helping with the transportation to Ukraine and from the Polish-Ukrainian border.\nPoland is helping Ukrainian war refugees. If you know someone who is escaping Ukraine through Poland – pass this information.\nEvery Ukrainian citizen, who is crossing the Ukrainian-Polish border will be allowed to come. The limitations can be on the Ukrainian side, because men between 18 and 60 years old are not allowed to leave the country and are drafted.\nIf you don’t have anyone (friends/family) on the Polish side who could help you with the accommodation, visit the reception point. They will provide temporary accommodation, medical help, warm food, and some clothes if necessary. You will also receive any information you need.\nAlso local Polish people gather every possible thing that may be useful for the refugees. Often these are the local initiatives. Besides clothes, food or cosmetics there are also transportation services. Ask at the reception point how to get to your destination.\nEvery Ukrainian refugee has a right to free medical help. They can visit hospitals or regular health clinics. Just remember – all the NATIONAL (run by the Polish government) medical aid is free. There is still private medical help, but this one may be paid one. However some of the private clinics (Damian, LuxMed) also offer free doctor appointments as well.\nIn bigger cities in Poland there are special points dedicated to the refugees (mostly near the biggest train or bus stations). They can get some food but also receive any required information and help with accommodation.\nPoland also organised help with translation, psychological help, law help.\nThere are huge groups on Facebook, where you can ask questions or ask for help (private initiatives): https://www.facebook.com/groups/683793819641997\nList of reception points can be found here (in Ukrainian): https://www.gov.pl/web/ua\nWebsite with the most important information gathered (in Ukrainian): https://ukraina.grupagranica.pl/', 'Please click on each role to learn more.\n*Roles with CLDR are in orange, roles with CDS are in blue, roles with Camp Noah are in red.\nWe are currently working on a systematic implementation model for child life specialists to respond locally to community emergencies and disasters. Current opportunities to participate locally are listed below. Please click on each role to learn more.\nCLDR Local Responder\n- Provide psychosocial support in the immediate and direct aftermath of disasters and traumatic experiences within your local community\n- Coordinate and respond in the immediate and direct aftermath of disasters and traumatic experiences within your local community\n- Assess CDS needs, advocate for CDS involvement with other relief agencies, and respond to larger disasters in your local area\n- Coordinates or supports CDS rapid responders and certified volunteers during larger deployments in your area\n- The primary role is to support the camp through helping with food, registration, set-up and take-down, etc. Online training is not required.\n- The primary role is to enact the camp curriculum and lead and support campers. Online training is required (free to CCLS, $250 for CL students).\n- The primary role of the mental health professional is to be a resource and support to any children or staff with specific mental health needs during the camp. In addition, should the need for additional or ongoing mental health support be assessed, the mental health professional will assist in providing contacts in the community for these ongoing services. Online training is required (free of charge) and a $500 stipend will be offered for full-week commitment to the camp.\nTo continue coordinated efforts for children and families’ post-disasters, the first step to get involved nationally is to become a certified volunteer with Children’s Disaster Services (CDS); this is accomplished through attending one of their training workshops. These trainings come in two forms: 7-hour child life specific training (offered 1-2/year) and a 27-hour overnight disaster simulation training (offered 5-8/year). After becoming certified with CDS, you then are eligible for deployment. Deployment is typically requested through a mass email. If able to serve, your travel arrangements will be coordinated with CDS. It is important to note that all travel, food, shelter, and supplies are funded in full when deployed with CDS. Please click on each role to learn more.\nCDS Certified Volunteer\n- Respond to disaster deployment requests and provide child-directed play opportunities in disaster stricken areas (shelters, resource centers, etc.)\n- Respond and provide psychosocial support during mass casualty national events (aviation incidents, terrorist attacks, etc.)\nWe have partnered with Dunkirk Refugee Children’s Center. The Children’s Center is open on a daily basis and offers children the opportunity to come and engage in play and socialization. For additional information please contact firstname.lastname@example.org\nWe are interested in gathering information about international organizations and opportunities and finding out ways child life specialists might be able to meet needs specific to children and trauma. If you have information for us in the following areas, please contact us:\n- Are you a child life specialist who has deployed with an organization internationally to serve children and families in need? We want to learn more.\n- Are you an international organization who knows of a specific need a child life specialist could meet within your organization? We want to learn more.\nWant to support and be part of CLDR but cannot work directly at the scene of those affected by disasters? Here are opportunities for you!\n- Engage in CLDR research processes as guided by CLDR team (submit relevant research, participate in research projects underway, etc.)\n- Work as a team with others to seek funding opportunities and write grants with the goal of supporting CLDR local, national, and international tasks and initiatives']	['<urn:uuid:de7f80af-f65c-4399-95d8-23b25001982d>', '<urn:uuid:7e6c7819-0a36-421b-9bb2-598a60e298f0>']	factoid	direct	concise-and-natural	distant-from-document	three-doc	expert	2025-05-12T19:47:23.148022	9	57	1453
69	looking evening entertainment vienna what shows available on cruise	"In Vienna, there is an evening concert called ""Sound of Vienna"" which provides an atmospheric conclusion to the day. This concert costs $67.00 when booked individually."	"['Nuremberg → Budapest\nThis cruise includes a trip along the Rhine-Main-Danube canal (a masterpiece of engineering), and along the Danube River.\nPass through four of Europe’s most important countries (Germany, Austria, Hungary and Slovakia), three world-famous capital cities (Vienna, Budapest and Bratislava), and through diverse landscapes such as the “Schlögener Schlinge” below Passau, the picturesque, myth-enshrouded Wachau region, and the Danube-knee at Visegrad (the Hungarian Wachau), with imposing architectural buildings along the banks of the rivers, such as Melk Abbey and the Esztergom Basilica. The cruise can be booked either from the Bavarian town of Nuremberg to Budapest – the “Queen of the Danube”, or in the opposite direction.\nYour journey along the beautiful blue Danube begins in the Bavarian town of Nuremberg, where you check into your AMADEUS ship during the afternoon. Get to know the cruise director at a Welcome Cocktail and enjoy your first evening with a Welcome Dinner on board, while your ship sets course for Regensburg.\nYou arrive in Regensburg at lunchtime. In the afternoon you have the opportunity to view Regensburg’s magically beautiful, Medieval old town, either on your own or as part of a guided tour. In the evening, you can enjoy a typical Bavarian evening on board, after which the ship continues to Passau.\nYour ship arrives in the Bavarian city of Passau, also known as the “City of Three Rivers”, where the mighty Danube converges with the green Inn River and the Ilz River. You can visit the romantic old town in the afternoon on a sightseeing walk, allowing yourself to be inspired by Passau’s winding lanes and historical buildings. In the late afternoon, you return to your ship and continue your journey to the Austrian Wachau region.\nYour cruise ship stops at numerous memorable places today, starting in Melk/Emmersdorf, where your ship docks in the early morning. You can visit the Baroque Melk Abbey and learn more about its eventful 900-year history. Your journey then continues along the Danube, through the UNESCO World Natural Heritage region of the Wachau. Your ship docks for lunch in the well-known wine village of Dürnstein, and in the afternoon you can take a leisurely stroll through the town, ending with wine tasting. Your ship will then weigh anchor and take you to the Austrian capital city of Vienna, where the evening concert, “Sound of Vienna”, provides a fittingly atmospheric conclusion to this eventful day.\nYou have the whole day at your disposal to explore this majestic metropolis on the Danube – Vienna. In the morning, we organise an extensive city sightseeing tour, which takes you to the most beautiful landmarks and majestic buildings of this former residence of the Habsburg family. After lunch, you can join a guided tour to the magnificent Schönbrunn Palace, which will draw both art-lovers and the general public into its spell. Your ship leaves Vienna shortly before midnight and heads for Bratislava.\nEarly in the morning, your AMADEUS ship arrives in the Slovakian capital of Bratislava. In the morning, we recommend that you take a city walk or a guided sightseeing tour in order to discover this magnificent city with its Gothic and Baroque architecture. After lunch, you have the opportunity to take a drive along the “Marchfelder Schlösserstraße” and visit the imperial Festschloss Hof, where the Austrian Empress Maria Theresia once resided. In the evening, a festive Captain’s Gala Dinner awaits you on board and your ship will depart for the last stage of the journey.\nAt breakfast time, your ship arrives in the majestic metropolis of Budapest, where you can enjoy a comfortable morning city sightseeing tour, which takes you to its beautiful buildings and landmarks. After a generous lunch on board, you can take part in an afternoon excursion to Gödöllo Palace, a former residence of the Austrian Empress “Sissi” (Elizabeth), where you will enjoy a comprehensive tour through this sumptuous 18th century palace. The highlight of the day is a romantic evening cruise through Budapest, which is festively lit up at night, together with a typical Hungarian folklore show on board your floating luxury hotel.\nEnjoy a generous breakfast on board before you disembark and take leave of the AMADEUS crew. We wish you a pleasant journey home and hope to welcome you again soon!\nOwing to the variety of available excursions it‘s often difficult to choose between them - so on this cruise we offer you tailor-made and carefully composed sets of excursions that you can book in advance with 15% discount. All excursions can of course also be booked individually at regular rates on board.\n|Cruise Highlights||Unit Price||A||B||C|\n|Regensburg walking tour||$ 20.00|\n|Passau walking tour||$ 20.00|\n|Excursion to Melk Abbey||$ 54.00|\n|Dürnstein walking tour and wine tasting||$ 29.00|\n|Concert ""Sound of Vienna""||$ 67.00|\n|City tour of Vienna||$ 41.00|\n|Guided tour of Schönbrunn Palace||$ 55.00|\n|City tour of Bratislava||$ 40.00|\n|Guided tour of the Imperial Schlosshof Palace||$ 61.00|\n|City tour of Budapest||$ 40.00|\n|Excursion to the Royal Palace of Gödöllö||$ 54.00|\n|Onboard Booking Price||$ 175.00||$ 256.00||$ 481.00|\n|Prices for packages booked in advance||$ 148.00||$ 217.00||$ 409.00|']"	['<urn:uuid:f9e8f683-acdd-4618-ad00-50a7e03f6b02>']	factoid	with-premise	long-search-query	distant-from-document	single-doc	novice	2025-05-12T19:47:23.148022	9	26	842
70	What's the focal ratio of the Meade 6-inch astrograph reflector?	The Meade 6-inch astrograph reflector has an f/4.1 focal ratio.	"['6 Reflector Telescope Short Tube Ota. The Meade 6"" f/4.1 astrograph reflector optical tube works equally well visually or for astrophotography. F/4.1 means very short exposures, and a long back focus means. ·\nA 6 Reflector Telescope Short Tube Ota (TSO) is a type of telescope that uses a lens to capture a clear image of distant objects. It is an ideal choice for amateur astronomers and hobbyists alike, as it is relatively affordable and easy to use. The 6 TSO is best suited for viewing planets, stars and other celestial bodies, although it can also be used to observe land-based objects such as mountains and buildings.\nHow it Works\nThe 6 TSO consists of a telescope tube, an eyepiece, and a lens. The lens is the primary component of the telescope, as it is responsible for gathering and focusing light from distant objects. The light is then directed through the eyepiece, which magnifies the image and makes it easier to observe. The 6 TSO’s short tube design makes it easy to transport and store, and also reduces the amount of light distortion caused by the long tube design used in other types of telescopes.\nThe 6 TSO is a relatively simple telescope to operate. To use the telescope, the lens must first be focused on the desired object. This is done by adjusting the focus knob located on the eyepiece. Once the focus is set, the user can then use the telescope to observe the desired object. The telescope can also be used to observe objects in the night sky, such as stars and planets. To do this, the user must adjust the telescope’s alignment so that it is pointing at the desired object.\nWhen using the 6 TSO, it is important to remember to adjust the focus and alignment of the telescope before each use, as this will ensure that the image is clear and distortion-free. Additionally, it is important to remember to store the telescope in a safe, dry place when not in use, as exposure to moisture can cause rust and other damage to the telescope’s components.\nThe 6 TSO is a great choice for amateur astronomers and hobbyists alike. Its short tube design makes it easy to transport and store, and its relatively simple design makes it easy to use. Additionally, its affordable price makes it a great investment for those who are just starting out in astronomy. With a little practice, anyone can use a 6 TSO telescope to observe distant objects in the night sky.\nHow To Collimate Short-Tube (6) Newtonian Reflector Telescopes\nHere you can read more about the effect of offseting secondary mirror on collimation of a short-tube Newtonian Reflector: fpi-protostar.com/bgreer/collim.htm\nThe 6"" Imaging Newtonian can be attached to a wide variety of equatorial mounts by either using the Vixen-sized dovetail that is included or adding your own Losmandy-style. Because this 6"" optical tube is compact and weighs just 10.2 pounds, it will ride easily on even fairly lightweight mounts. Accessories include a 6 x 30 finder with bracket, a 2"" to., 6 Reflector Telescope Short Tube Ota.']"	['<urn:uuid:46a63315-7eaf-4bd6-965a-6417a523e945>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T19:47:23.148022	10	10	517
71	bodhisattva precepts samurai warrior code compare	Both the samurai's Bushido code and the Bodhisattva Precepts emphasized ethical behavior and moral conduct. The samurai code of Bushido required warriors to be honest, fair, and fearless, valuing loyalty and honor above life itself. Similarly, the Bodhisattva Precepts provided ethical guidelines including vows not to kill but cherish life, not to steal but respect others' property, not to lie but speak truth, and not to unleash anger but seek its source. Both systems of principles governed daily life and behavior, though Bushido was specifically for warriors while the Bodhisattva Precepts were part of general Buddhist practice.	"['Rise of the Military Society\nWhen Minamoto Yorimoto came to power to Japan in 1192, instead of taking place as emperor, he took place as shogun and set up a military government with a capital of Kamakura, which started a new era that marked a beginning in Japan history. Samurai, professional warriors, became Japan\'s ruling class.\nThe military government was led by a shogun, who ruled with help of warrior-lords called daimyos. Which in turn, they were supported with large numbers of samurai. The samurai pledged to protect the shogun and in the 14th century Japan resembled the lord-vassal system of Europe. Because of the samurai\'s loyalty to their leaders, they were rewarded with appointments to office and grants of land.\nThe samurai were expected to be honest, brave, and intensely loyal to his lord. The word samurai means ""those who serve."" For the samurai\'s it was more important to die an honorable death than to live a long life. The samurai were fearless fighters and devote Buddhists, which helped them prepare to face death and their duties.\nSamurai Armor, Weapons and Fighting\nThe samurai went into battle dressed in heavy armor, since it was needed for them to be protected. The first thing on was a robe called a kimono and baggy trousers, and to protect their legs they wore leather or cloth shin guards. But, the actual armor itself was unique and different, lined with small metal plates coated with lacquer and was laced with silk cords. To protect their chest and back, they wore boxlike panels of armor. The samurai\'s would wear metal sleeves to protect their arms and broad shoulder guards and panels that hung over his hips to provide extra protection. To look fierce to their opponents, they wore a mask that was mean looking, yet still protected his face. Last but not least was the helmet, which they burned incense in it before putting it on.\nMilitary Training and Fighting Skills\nLearning the skills of the samurai to extensive training. They had to learn archery, fencing and/or sword fighting. The samurai\'s were apprenticed by archery masters that taught the mental and physical techniques. They also learned how to breathe and shoot properly by riding on a galloping horse. In s wording Fighting they learned how to make their opponent make the first move, how to stay out of range of their enemy\'s sword, and how to fight fight against their enemy in tight spaces or against more than one opponent. When they would either lose or break their swords in battle, they learned how to fight using other object such as a metal fan or wooden staffs. Another option was martial arts.\nThe samurai had a unique style in battle.\n1. Messages from opposing sides met to decide the time and place if combat?\n2. The two armies faced each other a few hundred yards apart.\n3. The samurai shouted their names , ancestors, heroic deeds, and reason for fighting.\n4. Then the armies charged with samurai firing arrows as they urged forward.\n5. After battle they would clean and mount their Oppenheimer heads and show to the warlord that they really killed their foes.\nSamurai Training and the Warrior Code\nMetal and Physical Training:\nThe samurai\'s education of war also included mental training. This was key to their training, especially since they had to learn self control to overcome their emotions, Like the fear of death. To learn how to handle pain, they went days without eating, walked barefoot in the snow, and had stiff posture for over long periods of time. The samurai were trained to develop a ""sixth sense"" about danger and came from tough training.\nThe code of Bushido:\nThe samurai code developed over servearl centuries, and in the 17th century it took for as Bushido, "" Way of the Warrior."" This governed the samurai\'s life. It required the samurai to be honest, fair, and fearless in the face of death. They valued loyalty and honor even more than their life. Their job was to be loyal to his master, and if his lord died he would avenge his death.\nIf one did not live up to Bushido, the price of pay was ritual suicidqe, or seppuku. The would also do this if they did a shameful deed, committed a crime, or insulted a person of higher rank. When seppuku was performed guests were invited. To prepare they would take a bath, unbide their hair, and put on white clothes, and was served his favorite foods. When finished eating he took a sword and plunged into his stomach trying to make a circle. To put him out of his agony, a swordsman would quickly cut off his head.\nMost samurai studied Buddhism, the two popular forms were Amida and Zen. The samurai were drawn to this, and especially zen.\nIn the 12th century a monk named Honen founded Amida Buddhism. They believed that all people could reach paradise. They taught that believers could reach paradise by relying on Amida Buddhism. Amida was an Indian Prince and when he became Buddha he set up a paradise named Pure Land. To enter, one must prayerfully repeat Amida\'s name over and over again. When the believer died he would go to the Pure Land.\nZen was another form of Buddhism, the samurai liked it because of its emphasis on effort and discipline. To achieve enlightenment and self-reliance, you meditated. Zen Buddhists would meditate for hours to reach enlightenment. To become enlightened it required giving logical thinking everyday. Zen masters used puzzling questions called koans. They also created zen gardens to aid their meditation . It symbolized nature, like rocks on the sand could represent islands in the ocean. Zen helped samurai learn discipline, focus on their minds, and conquer the fear of death.', 'About zen practice\nThe fundamental practices of Zen Buddhism are zazen meditation and the Bodhisattva Precepts.\nIn essence, zazen is simply existing without grasping or aversion to our immediate experience.\nZazen is not a means to an end, but the ongoing practice of freedom from suffering. By keeping a spacious mind through whatever physical sensations, thoughts, and emotions arise, we are awake to their transparent, transforming quality and at the same time come into a direct feeling- intimacy with the texture of ourselves, Thus, we become increasingly able to flow in connection with the inevitable changes and difficulties that life brings.\nThe Bodhisattva Precepts are guidelines for ethical practice that embody a clear mind and connected heart animated by the intention to live a life of compassion and deep respect for all beings. The 16 Bodhisattva Precepts are:\nThe Three Pure Precepts\nI vow not to commit evil.\nI vow to cultivate goodness.\nI vow to help others.\nThe Three Refuges (Three Treasures)\nI take refuge in the Buddha.\nI take refuge in the Dharma.\nI take refuge in the Sangha.\nThe Ten Grave Precepts\nI vow not to kill, but to cherish all life.\nI vow not to steal, but to respect that which belongs to others.\nI vow not to misuse sexual energy, but to be honest and respectful.\nI vow not to lie, but to speak the truth.\nI vow not to misuse drugs or alcohol, but to keep the mind clear.\nI vow not to gossip about others’ faults, but to be understanding and sympathetic.\nI vow not to praise myself by criticizing others, but to overcome my own shortcomings.\nI vow not to withhold spiritual or material aid, but to give freely when needed.\nI vow not to unleash anger, but to seek its source.\nI vow not to speak ill of the Three Treasures, but to cherish and uphold them.\nJukai is the process and public ceremony of taking the 16 precepts, committing to practicing them in everyday life. People take jukai after some years of practicing with the community, establishing a relationship with one of the teachers, and taking part in sesshin.\nPractice at Home\nThere are three essential elements for successfully practicing at home:\nspace, time, and community.\nSet Aside a Space\nHome practice is greatly enhanced if the meditation space is dedicated to meditation. Ideally in this space no other activities take place, and it remains undisturbed when not in use. Of course this isn’t always possible, but it is very powerful. Another important element is cleanliness. The act of cleaning the meditation space is a reminder of the importance of meditation, and the ever-returning dust speaks of the necessity of returning to meditation again and again.\nAn altar can bring power and dignity to a meditation space. The various items on the altar are placed there because they have meaning, and serve as reminders of spiritual aspiration. Common items for an altar are a small bowl of fresh water, a small bowl with rice or some other food offering, fresh flowers, an incense bowl, a photo of a teacher or inspiring person, and a spiritual image. There are no rules for a home altar, but it should be kept clean and tidy. Fresh flowers are a wonderful addition to an altar; be sure to change them when they begin to wilt.\nDecide How Much Time to Meditate\nHome meditation practice is well served by consistency and it is helpful to meditate at the same time every day. Choose a meditation time that is reasonable, preferably when interruptions are unlikely. Many people wake a little early to meditate first thing in the morning, another common time is just before bed. Some people meditate on lunch break, or at other quiet moments in the day.\nIt is important to meditate for an amount of time that can be done daily. Choose a length of time that is unquestionably possible, rather than one that feels like a stretch. It is better to meditate for five minutes per day than for one hour every week. Of course when the meditation period ends it’s fine to continue meditating. This should be done with a mind of enjoyment and appreciation, rather than a sense of “should” or obligation. Once a time of day and length of time have been decided and established, commit to that schedule. Making a firm commitment allows meditation practice to become a habit, rather than something that is done only when it’s convenient.\nMeditate with a Community (Sangha)\nSitting with a sangha is a great addition to a home practice. Sangha is a reminder that there are many, many people in the world who also believe that meditation is important, and are willing to commit time and life energy to the practice. The experience of meditating with a sangha is different than sitting alone, and this difference is very interesting and revealing. A sangha is also an environment to connect with other people, some who are inspiring and some who are challenging.\nThe ZCO sangha includes teachers, and interacting with a teacher can be a great catalyst for spiritual practice. A sangha also offers an opportunity to benefit others with the fruits of meditation. Listening attentively or supporting someone in need are great gifts to share, and enhanced by meditation practice.']"	['<urn:uuid:b18f8611-e1ef-43f0-b0df-79179fead026>', '<urn:uuid:42aacf48-0494-4673-9d27-5d077980852e>']	open-ended	with-premise	short-search-query	similar-to-document	three-doc	expert	2025-05-12T19:47:23.148022	6	97	1859
72	How are scientists studying the impact of environmental changes and diseases on stream communities and amphibian populations across different regions?	Scientists are conducting comprehensive research examining multiple factors affecting stream communities and amphibians. They study how stream productivity is controlled by various physical and biological factors that often operate simultaneously in complex ways. Research has revealed that disease, particularly the amphibian chytrid fungus, is having significant impacts on amphibian populations globally. Studies also investigate how environmental changes affect stream communities, including factors like atmospheric nitrogen deposition, channel drying, and urbanization. This research spans diverse locations from Arctic streams to Neotropical ecosystems, providing crucial data about how these environmental changes and diseases are impacting aquatic biodiversity and ecosystem function.	"['- (205) 348-4136\n- 2107-A Bevill Building\n- Postdoctoral research, The University of Alabama, 1991\n- PhD, Entomology, University of Georgia, 1986\nI am a freshwater ecologist with a strong interest in how local, landscape, and regional factors affect the structure and productivity of stream communities. These factors may be physical, such as stream channel form, water temperature or periods of wetting, drying and freezing, or biological, such as differences in food sources or food-web structure. I have found that stream productivity is usually controlled by several factors that operate simultaneously, often in complex and surprising ways. A predictive understanding of such factors and their interactions is critical for making sound decisions concerning the management and restoration of stream communities. In addition to my academic interests, I am also a committed field naturalist who has been lucky enough to have worked extensively in some truly spectacular streams in the caves of northern Alabama, the Smoky Mountains of North Carolina, Maine, Arctic Alaska, Iceland, New Zealand, and Panama.\n- Parker, S.M.& A.D. Huryn. 2013. Disturbance and productivity as codeterminants of stream food web complexity in the Arctic. Limnology & Oceanography 58:2158-2170.\n- Whiles, M.R., R.O. Hall, W.K. Dodds, P. Verburg, A.D. Huryn, A. Rugenski, C.M. Pringle, K.R. Lips, S.S. Kilham, S. Peterson, C. Colon-Gaud & S. Connelly. 2013. Disease-driven amphibian declines alter biogeochemical cycling and energy flow in streams. Ecosystems 16:146-147.\n- Venarsky, M.P., A.D. Huryn & J.P. Benstead. 2012. Re-examining extreme longevity of the cave crayfish Orconectes australis using new mark-recapture data: a lesson on the limitations of iterative size-at-age models. Freshwater Biology 57:1471-1481.\n- Huryn, A.D. & J.E. Hobbie. 2012. Land of extremes: a natural history of the arctic North Slope of Alaska. University of Alaska Press, Fairbanks. xvi+311 pp.\n- Benstead, J.P. & A.D. Huryn. 2011. Extreme seasonality of litter breakdown in an arctic spring-fed stream is driven by shredder phenology, not temperature. Freshwater Biology 56:2034-2044.\n- Huntsman, B., M.P. Venarsky, J.P. Benstead & A.D. Huryn. 2011. Effects of organic matter availability on the life history and production of a top vertebrate predator (Plethodontidae: Gyrinophilus palleucus) in two cave streams. Freshwater Biology 56:1746-1760.\n- Parker, S.M. & A.D. Huryn. 2011. Effects of natural disturbance on stream communities: a habitat template analysis of Arctic headwater streams. Freshwater Biology 56:1342-1357.\n- Benke, A.C. & A.D. Huryn. 2010. Benthic invertebrate production–facilitating answers to ecological riddles in freshwater ecosystems. Journal of the North American Benthological Society 29:264-285.\n- Gaenzle Schilling, E., C. Smith Loftin & A.D. Huryn 2009. Effects of introduced fish on native macroinvertebrates in two types of historically fishless lakes. Conservation Biology 142:3030-3038.\n- Whiles, M.R., A.D. Huryn, B.W. Taylor & J.D. Reeve. 2009. The influence of handling stress and fasting on estimates of ammonium excretion by tadpoles and fish: recommendations for designing excretion experiments. Limnology & Oceanography Methods 7:1-7.\n- Gaenzle Schilling, E., C. Smith Loftin & A.D. Huryn 2009. Macroinvertebrates as indicators of fish absence in naturally fishless lakes in Maine. Freshwater Biology 54:181-202.\n- Connelly, S., C.M. Pringle, R.J. Bixby, R. Brenes, M.R. Whiles, K.R. Lips, S. Kilham & A.D. Huryn 2008. Changes in stream primary producer communities resulting from large-scale catastrophic amphibian declines: can small-scale experiments predict effects of tadpole loss? Ecosystems 11:1262-1276.\n- Gaenzle Schilling, E., C. Smith Loftin, K.E. DeGoosh, A.D. Huryn & K.E. Webster. 2008. Predicting the locations of naturally fishless lakes. Freshwater Biology 53:1021-1035.\n- Chadwick, M.A. & A.D. Huryn 2007 Role of habitat in determining macroinvertebrate production in an intermittent-stream system. Freshwater Biology. 52:240-251.\n- Chadwick, M.A., D.R. Dobberfuhl, A.C. Benke, A.D. Huryn, K. Suberkropp & J.E. Thiele2006 Urbanization regulates stream ecosystem function via changes to flow regimes, invertebrate richness, and water chemistry. Ecological Applications 16:1796–1807.\n- Parker, S.M. & A.D. Huryn 2006 Food web structure and function in two Arctic streams with contrasting disturbance regimes. Freshwater Biology 51:1249-1263.\n- Whiles, M.R., K. Lips, C. Pringle, S. Kilham, R. Brenes, S. Connelly, J.C. Colon Gaud, M. Hunt, A.D. Huryn, C. Montgomery & S. Peterson 2006. The effects of amphibian population declines to the structure and function of Neotropical stream ecosystems. Frontiers in Ecology 4:27-34.\n- Huryn, A.D. K.A. Slavik, R.L. Lowe, S.M. Parker, D.S. Anderson & B.J. Peterson 2005. Landscape heterogeneity and the biodiversity of Arctic stream communities: a habitat template analysis. Canadian Journal of Fisheries and Aquatic Sciences 62:1905-1919.\n- Chadwick, M. & A.D. Huryn. 2005. Response of stream macroinvertebrate production to atmospheric nitrogen deposition and channel drying. Limnology & Oceanography 50:228-236.', 'Deanna (""Dede"") H. Olson\nResearch Ecologist, Team Leader\n3200 SW Jefferson Way\n3200 SW Jefferson Way\nContact Deanna (""Dede"") H. Olson\nMy research focuses on three themes: the ecology of amphibians and fishes in headwater stream basins in managed forests; the taxonomic and distribution patterns of the amphibian chytrid fungus; and the conservation biology of amphibians, reptiles, and turtles.\nConservation biology, behavioral ecology, population and community ecology. Effects of forest management practices, landscape designs, and policies on aquatic-dependent organisms. Interaction of climate variation with species\' ecology and land management practices. Development of new research paradigms engaging multiple partners.\nMy past studies have explored the reproductive behavioral ecology of frogs and toads.\nWhy This Research is Important\nOver one-third of amphibians and almost 50 percent of turtles are threatened with extinction worldwide, yet we know very little about their general ecology and the main threats that are affecting them. Both taxonomic groups are considered ideal indicators of environmental degradation owing to their reliance on both aquatic and terrestrial habitats and their central position in food webs. Ecological and conservation biology research can aid management decisions regarding these iconic faunal groups, preserving them for generations to come.\n- Oregon State University, Department of Zoology, Ph.D. Breeding ecology of amphibians in the Cascade Range of Oregon 1988\n- University of California, San Diego, B.A. Major: Biology; Concentration Area: Population Ecology; Minor: Philosophy 1980\n- PNW representative, Intermediate Management Group\n2012 - Current\nSurvey and Manage Program advisory group, Northwest Forest Plan\n- Lead, Aquatic Biodivesity in Forests, Division8.02.06, International Union for Forest Research Organizations\n2010 - Current\nOrganized sessions in Seoul at the IUFRO World Congress, and Cork at the Biodiversity subdivision conference.\n- Associate Editor, Herpetological Review, Society for the Study of Amphibians and Reptiles\n2000 - Current\nIn 2008, I initiated the Amphibian Diseases section of the journal.\n- Courtesy Faculty, Oregon State University\n1990 - Current\nI have held positions in the Departments of Zoology, Fisheries and Wildlife, Forest Science, Forest Ecosystems and Society, and the Environmental Science Program, and the program for BioResource Research. Primarily, I co-advise graduate students and mentor undergraduate students who are collaborating with me on research projects.\n- National Co-chair, Partners in Amphibian and Reptile Conservation\n2009 - 2012\nI wrote the 10-years of PARC report, 2011-State of the Turtle report, and 2012-State of the Lizard report. I helped initiate PARC\'s annual report process (2 new brochures: 2010 and 2011), two new annual awards (Alison Haskell Award, Visionary Leader Award), and PARC\'s ""year of"" campaigns.\n- Northwest Regional Co-chair, Partners in Amphibian and Reptile Conservation\n1999 - 2009\n- President, Society for Northwestern Vertebrate Biology\n2003 - 2005\n- Vice-President, Oregon, Society for Northwestern Vertebrate Biology\n2000 - 2003\n- Associate Editor, Northwestern Naturalist, Society for Northwestern Vertebrate Biology\n1993 - 2000\nAwards & Recognition\n- Alison Haskell Award for Excellence in Herpetological Conservation, 2012\nThis is a national award given by Partners in Amphibian and Reptile Conservation to an individual who has been an ""unsung hero"" for herpetological conservation\n- USFS Senior Leader Program graduate, 2011\nThis is a national program designed to develop leaders in the US Forest Service.\n- PNW Station Director’s Award for Excellence: Distinguished Science Award, 2010\nFor sustained excellence in the fundamental study of amphibian ecology and the effective delivery of that knowledge to address practical management and policy issues.\n- Top 10 Most Cited Papers, 2010\nFor recent papers published by Elsevier, my paper (Olson et al. 2007) was in the Top 10 for the journal Forest Ecology and Management.\nFeatured Publications & Products\n- Xie, Gisselle Yang; Olson, Deanna H.; Blaustein, Andrew R. 2016. Projecting the global distribution of the emerging amphibian fungal pathogen, batrachochytrium dendrobatidis, based on IPCC climate futures.\n- Olson, Deanna H.; Aanensen, David M.; Ronnenberg, Kathryn L.; Powell, Christopher I.; Walker, Susan F.; Bielby, Jon; Garner, Trenton W.J.; Weaver, George; Fisher, Matthew C. 2013. Mapping the global emergence of Batrachochytrium dendrobatidis, the amphibian chytrid fungus.\n- Searle, C.L.; Gervasi, S.S.; Hua, J.; Hammond, J.I.; Relyea, R.A.; Olson, D.H.; Blaustein, A.R. 2011. Differential host susceptibility to Batrachochytrium dendrobatidis, an emerging amphibian pathogen.\n- Grant, Evan H Campbell; Muths, Erin; Katz, Rachel A; Canessa, Stefano; Adams, Michael J; Ballard, Jennifer R; Berger, Lee; Briggs, Cheryl J; Coleman, Jeremy TH; Gray, Matthew J; Harris, M Camille; Harris, Reid N; Hossack, Blake; Huyvaert, Kathryn P; Kolby, Jonathan; Lips, Karen R; Lovich, Robert E; McCallum, Hamish I; Mendelson, Joseph R; Nanjappa, Priya; Olson, Deanna H; Powers, Jenny G; Richgels, Katherine LD; Russell, Robin E; Schmidt, Benedikt R; Spitzen-van der Sluijs, Annemarieke; Watry, Mary Kay; Woodhams, Douglas C; White, C LeAnn. 2017. Using decision analysis to support proactive management of emerging infectious wildlife diseases.\n- Olson, Deanna H.; Burton, Julia I. 2014. Near-term effects of repeated-thinning with riparian buffers on headwater stream vertebrates and habitats in Oregon, USA.\n- Kluber, Matthew R.; Olson, Deanna H.; Puettmann, Klaus J. 2009. Downed wood microclimates and their potential impact on plethodontid salamander habitat in the Oregon Coast Range.\n- Olson, Deanna H.; Burnett, Kelly M. 2009. Design and management of linkage areas across headwater drainages to conserve biodiversity in forest ecosystems.\n- Shoo, Luke P.; Olson, Deanna H.; McMenamin, Sarah K.; Murray, Kris A.; VanSluys, Monique; Donnelly, Maureen A.; Stratford, Danial; Terhivuo, Juhani; Merino-Viteri, Andres; Herbert, Sarah M.; Bishop, Phillip J.; Corn, Paul Stephen; Dovey, Liz; Griffiths, Richard A.; Lowe, Katrin; Mahony, Michael; McCallum, Hamish; Shuker, Jonathan D.; Simpkins, Clay; Skerratt, Lee F.; Williams, Stephen E.; Hero, Jean-Marc. 2011. Engineering a future for amphibians under climate change.\n- Kiester, A. Ross; Olson, Deanna H. 2011. Prime time for turtle conservation.\n- Olson, Deanna . 2019. A decade of herpetological disease papers: puzzle pieces of a bigger picture.\n- Emel, Sarah L.; Olson, Deanna H.; Knowles, L. Lacey; Storfer, Andrew . 2019. Comparative landscape genetics of two endemic torrent salamander species, Rhyacotriton kezeri and R. variegatus: implications for forest management and species conservation.\n- Mims, Meryl C.; Olson, Deanna H.; Pilliod, David S.; Dunham, Jason B. 2019. Considering the hidden threats of climate change: A new approach puts overlooked species on the conservation radar.\n- Watts, Andrea ; Olson, Dede ; Harris, Reid ; Mandica, Mark . 2019. The deadly amphibian bsal disease: How science-management partnerships are forestalling amphibian biodiversity losses.\n- Reeves, Gordon H.; Olson, Deanna H.; Wondzell, Steven M.; Bisson, Peter A.; Gordon, Sean ; Miller, Stephanie A.; Long, Jonathan W.; Furniss, Michael J. 2018. Chapter 7: The aquatic conservation strategy of the northwest forest plan A review of the relevant science after 23 years.\n- Works, Aaron J.; Olson, Deanna H. 2018. Diets of two nonnative freshwater turtle species (Trachemys scripta and Pelodiscus sinensis) in Kawai Nui Marsh, Hawaii.\n- Blaustein, Andrew ; Urbina, Jenny ; Snyder, Paul ; Reynolds, Emily ; Dang, Trang ; Hoverman, Jason ; Han, Barbara ; Olson, Deanna ; Searle, Catherine ; Hambalek, Natalie . 2018. Effects of emerging infectious diseases on amphibians: A review of experimental studies.\n- Gray, Matthew J.; Allender, Matthew C.; Haman, Katherine H.; Harris, Reid N.; Olson, Deanna H. 2018. Facilitating early detection and rapid response: An alert system to combat emerging herpetofaunal diseases.\n- Mims, Meryl C.; Olson, Deanna H.; Pilliod, David S.; Dunham, Jason B. 2018. Functional and geographic components of risk for climate sensitive vertebrates in the Pacific Northwest, USA.\n- Adams, Michael J.; Harris, Reid N.; Grant, Evan H. C.; Gray, Matthew J.; Camille Hopkins, M. ; Iverson, Samuel A.; Likens, Robert ; Mandica, Mark ; Olson, Deanna H.; Shepack, Alex ; Waddle, Hardin . 2018. Prepublication communication of research results.\n- Penaluna, Brooke E.; Olson, Deanna H.; Flitcroft, Rebecca L.; Weber, Matthew A.; Bellmore, J. Ryan; Wondzell, Steven M.; Dunham, Jason B.; Johnson, Sherri L.; Reeves, Gordon H. 2017. Aquatic biodiversity in forests: a weak link in ecosystem services resilience.\n- Roon, David; Dunham, Jason; Harvey, Bret; Bellmore, Ryan, J.; Olson, Deanna; Reeves, Gordon. 2017. Evaluating the ecological trade-offs of riparian thinning for headwater stream ecosystems in second-growth redwood forests.\n- Gervasi, Stephanie S.; Stephens, Patrick R.; Hua, Jessica; Searle, Catherine L.; Xie, Gisselle Yang; Urbina, Jenny; Olson, Deanna H.; Bancroft, Betsy A.; Weis, Virginia; Hammond, John I.; Relyea, Rick A.; Blaustein, Andrew R.; Lötters, Stefan. 2017. Linking ecology and epidemiology to understand predictors of multi-host responses to an emerging pathogen, the amphibian chytrid fungus.\n- Iwanowicz, Deborah D.; Schill, William B.; Olson, Deanna H.; Adams, Michael J.; Densmore, Christine; Cornman, R. Scott; Adams, Cynthia; Figiel, Jr., Chester.; Anderson, Chauncey W.; Blaustein, Andrew R.; Chestnut, Tara. 2017. Potential concerns with analytical methods used for the detection of Batrachochytrium salamandrivorans from archived DNA of amphibian swab samples, Oregon, USA.\n- Leach, J. A.; Olson, D. H.; Anderson, P. D.; Eskelson, B. N. I. 2017. Spatial and seasonal variability of forested headwater stream temperatures in western Oregon, USA.\n- Burton, Julia I.; Olson, Deanna H.; Puettmann, Klaus J. 2016. Effects of riparian buffer width on wood loading in headwater streams after repeated forest thinning.\n- Foster, Alex D.; Olson, Deanna H.; Jones, Lawrence L.C. 2015. A review of the biology and conservation of the Cope\'s giant salamander Dicamptodon copei Nussbaum, 1970 (Amphibia: Caudata: Dicamptodontidae) in the Pacific northwestern region of the USA.\n- Gray, Matthew J.; Lewis, James P.; Nanjappa, Priya; Klocke, Blake; Pasmans, Frank; Martel, An; Stephen, Craig; Parra Olea, Gabriela; Smith, Scott A.; Sacerdote-Velat, Allison; Christman, Michelle R.; Williams, Jennifer M.; Olson, Deanna H.; Hogan, Deborah A. 2015. Batrachochytrium salamandrivorans: The North American response and a call for action.\n- Olson, Deanna. 2015. Contributor to chapter 1, Amphibian Conservation. In: What works in conservation: 2015.\n- Bradley, Paul W.; Gervasi, Stephanie S.; Hua, Jessica; Cothran, Rickey D.; Relyea, Rick A.; Olson, Deanna H.; Blaustein, Andrew R. 2015. Differences in sensitivity to the fungal pathogen Batrachochytrium dendrobatidis among amphibian populations.\n- Mazza, Rhonda; Olson, Deanna (Dede). 2015. Heed the head: buffer benefits along headwater streams.\n- Olson, Deanna; Crisafulli, Charlie. 2014. Conservation Assessment for the Van Dyke\'s Salamander (Plethodon vandykei) Version 1.0.\n- Foster, Alex ; Olson, Deanna H. 2014. Conservation assessment for the Cope s Giant Salamander (Dicamptodon copei) Version 1.0.\n- Chestnut, Tara; Anderson, Chauncey; Popa, Radu; Blaustein, Andrew R.; Voytek, Mary; Olson, Deanna H.; Kirshtein, Julie. 2014. Heterogeneous occupancy and density estimates of the pathogenic fungus Batrachochytrium dendrobatidis in waters of North America.\n- Ruzicka, Kenneth J., Jr..; Puettmann, Klaus J.; Olson, Deanna H. 2014. Management of riparian buffers: upslope thinning with downslope impacts.\n- Barela, Kimberly L.; Olson, Deanna H. 2014. Mapping the western pond turtle (Actinemys marmorata) and painted turtle (Chrysemys picta) in western North America.\n- Olson, Deanna H.; Kluber, Matthew R. 2014. Plethodontid salamander distributions in managed forest headwaters in western Oregon.\n- Olson, Deanna H.; Leirness, Jeffery B.; Cunningham, Patrick G.; Ashley Steel, E. 2014. Riparian buffers and forest thinning: Effects on headwater vertebrates 10 years after thinning.\n- Olson, Deanna H.; Chestnut, Tara. 2014. Trouble in the aquatic world: how wildlife professionals are battling amphibian declines.\n- Burton, Julia I.; Ares, Adrian; Mulford, Sara E.; Olson, Deanna H.; Puettmann, Klaus J. 2013. Above-ground carbon storage, downed wood, and understory plant species richness after thinning in western Oregon.\n- Wessell-Kelly, Stephanie J.; Olson, Deanna H. 2013. Community structure of vascular plants, arthropods, amphibians, and mollusks in managed forests of the Pacific Northwest (USA).\n- Olson, Deanna H.; Burnett, Kelly M. 2013. Geometry of forest landscape connectivity: pathways for persistence.\n- Burton, Julia I.; Olson, Deanna H.; Puettmann, Klaus J. 2013. Headwater stream flow, climate variation, and riparian buffers with thinning in western Oregon.\n- Gervasi, Stephanie; Gondhalekar, Carmen; Olson, Deanna H.; Blaustein, Andrew R. 2013. Host identity matters in the amphibian-Batrachochytrium dendrobatidis system: fine-scale patterns of variation in responses to a multi-host pathogen.\n- Anderson, Paul D.; Olson, Deanna H.; Ares, Adrian. 2013. Initial riparian down wood dynamics in relation to thinning and buffer width.\n- Wessell-Kelly, Stephanie J.; Olson, Deanna H. 2013. Leave islands as refugia for low-mobility species in managed forest mosaics.\n- Kluber, Matthew R.; Olson, Deanna H.; Puettmann, Klaus J. 2013. Log and soil temperature profiles in managed headwater sub-basins in the Oregon coast range: implications for wildlife habitat.\n- Burton, Julia I.; Ares, Adrian; Olson, Deanna H.; Puettmann, Klaus J. 2013. Management trade-off between aboveground carbon storage and understory plant species richness in temperate forests.\n- Kluber, Matthew R.; Olson, Deanna H.; Puettmann, Klaus J. 2013. No effects of thinning with riparian buffers on terrestrial salamanders in headwater forests 5 to 6 years post-harvest in western Oregon.\n- Kluber, Matthew R.; Olson, Deanna H. 2013. Plethodontid salamander population ecology in managed forest headwaters in the Oregon coast range.\n- Olson, Deanna H. 2013. Riparian buffers and thinning in headwater drainages in western Oregon: aquatic vertebrates and habitats.\n- Ruzicka, Kenneth J., Jr..; Olson, Deanna H.; Puettmann, Klaus J. 2013. The intertwining paths of the density managment and riparian buffer study and the Northwest Forest Plan.\n- Ares, Adrian; Olson, Deanna H.; Puettmann, Klaus J. 2013. Thinning and riparian buffer configuration effects on down wood abundance in headwater streams in coniferous forests.\n- Parks, Noreen; Olson, Deanna H. 2011. Engineering a future for amphibians under a changing climate.\n- Paoletti, David J.; Olson, Deanna H.; Blaustein, Andrew R. 2011. Responses of foothill yellow-legged frog (Rana boylii) larvae to an introduced predator.\n- Bancroft, Betsy A.; Han, Barbara A.; Searle, Catherine L.; Biga, Lindsay M.; Olson, Deanna H.; Kats, Lee B.; Lawler, Joshua J.; Blaustein, Andrew R. 2011. Species-level correlates of susceptibility to the pathogenic amphibian fungus Batrachochytrium dendrobatidis in the United States.\n- Oliver, Marie; Burnett, Kelly; Olson, Deanna. 2010. Linked in: connecting riparian areas to support forest biodiversity.\n- Thompson, Jonathan; Raphael, Martin; Molina, Randy; Marcot, Bruce; Olson, Deanna. 2008. Conservation of biological diversity: all things considered.\n|Conservation priorities identified for Northwest amphibians and reptiles|\nAt a reptile conservation conference, scientists and natural resource managers synthesized conservation concerns and priorities for 105 species ...\n|People, Forests, and Change: Lessons from the Pacific Northwest|\nA new book explores the past four decades of change in forest science and management in the Pacific Northwest\n|Response to Emerging Infectious Amphibian Diseases Forges New Alliances Between Science, Management, and Policy|\nThe newly described salamander chytrid fungus Batrachochytrium salamandrivorans (Bsal) is rapidly spreading in Europe, killing salamanders as it ...\n|Scientists Determine Ideal Buffer Width to Sustain Aquatic and Riparian Resources Along Headwater Streams|\nHow wide does a riparian buffer need to be to maintain aquatic and riparian habitat in and along forested headwater streams when upland forest t ...\n|Scientists Link Amphibian Fungus to Increasing Temperature Range|\nScientists find the odds of fungus occurrence decreased with increasing temperature range at a site, linking disease emergence to climatic consi ...\n|Strategically linking headwater habitats across ridgelines benefits amphibians and management|\nFederal biologists, land managers, and watershed stewardship councils are interested in this new design that maintains amphibian habitat and whi ...']"	['<urn:uuid:61b16cb1-e62a-4f1f-b5eb-0ebe758fd93f>', '<urn:uuid:6cd29691-2c1b-4acd-b272-629317204b85>']	open-ended	direct	verbose-and-natural	similar-to-document	three-doc	novice	2025-05-12T19:47:23.148022	20	98	3225
73	incense altar dimensions materials construction	The incense altar was made of acacia wood, square in shape, measuring 1 cubit long by 1 cubit wide and 2 cubits high, including its horns. It was covered with a layer of pure gold on its top, walls, and horns, with a gold rim around it.	"[""The Incense Altar\n||Make an altar to burn incense out of acacia wood.\n||It shall be square, a cubit long and a cubit wide, and 2 cubits high, including its horns.\n||Cover it with a layer of pure gold, on its top, its walls all around, and its horns. Make a gold rim all around it.\n||Place two gold rings under [the altar's] rim on its two opposite sides as receptacles to hold the poles with which it is carried.\n||Make the carrying poles out of acacia wood and cover them with a layer of gold.\n||Place [this altar] in front of the cloth partition concealing the Testimony Ark - before the cloth partition concealing the testimony area where I commune with you.\n||Aaron shall burn incense on [this altar] each morning when he cleans out the lamps.\n||He shall [also] burn [incense] before evening when he lights the lamps. Thus, for all generations, there will be incense before God at all times.\n||Do not burn any unauthorized incense on it. Furthermore, do not offer any animal sacrifice, meal offering, or libation on it.\n||[Furthermore,] once each year Aaron shall make atonement on the horns of [this altar]. For all generations, he shall make atonement with the blood of the atonement sacrifice once each year. [This altar] shall be a holy of holies to God.\nInstructions for a Census\n||God spoke to Moses saying:\n||When you take a census of the Israelites to determine their numbers, each one shall be counted by giving an atonement offering for his life. In this manner, they will not be stricken by the plague when they are counted.\n||Everyone included in the census must give a half shekel. This shall be by the sanctuary standard, where a shekel is 20 gerahs. It is half of such a shekel that must be given as an offering to God.\n||Every man over 20 years old shall be included in this census and give this offering to God.\n||The rich may not give more, and the poor may not give less than this half shekel. It is an offering to God to atone for your lives.\n||You will take this atonement money from the Israelites and use it for making the Communion Tent. It will thus be a remembrance for the Israelites before God to atone for your lives.\n||God spoke to Moses saying:\n||Make a copper washstand along with a copper base for it. Place it between the altar and the Communion Tent, and fill it with water for washing.\n||Aaron and his sons must was their hands and feet from [this washstand].\n||If they are not to die, they must wash with the water [of this washstand] before entering the Communion Tent or approaching the altar to perform the divine service, presenting a fire offering to God.\n||If they are not to deserve death, they must first wash their hands and feet. This shall be for [Aaron] and his descendants a law for all time, for all generations.\nThe Anointing Oil\n||God spoke to Moses, saying:\n||You must take the finest fragrances, 500 [shekels] of distilled myrrh, [two] half portions, each consisting of 250 [shekels] of fragrant cinnamon and 250 [shekels] of fragrant cane,\n||and 500 shekels of cassia, all measured by the sanctuary standard, along with a gallon of olive oil.\n||Make it into sacred anointing oil. It shall be a blended compound, as made by a skilled perfumer, [made especially for] the sacred anointing oil.\n||Then use it to anoint the Communion Tent, the Ark of Testimony,\n||the table and all its utensils, the menorah and its utensils, the incense altar,\n||the sacrificial altar and all its utensils, the washstand and its base.\n||You will thus sanctify them, making them holy of holies, so that anything touching them becomes sanctified.\n||You must also anoint Aaron and his sons, sanctifying them as priests to Me.\n||Speak to the Israelites and tell them, 'This shall be the sacred anointing oil to Me for all generations.\n||Do not pour it on the skin of any [unauthorized] person, and do not duplicate it with a similar formula. It is holy, and it must remain sacred to you.\n||If a person blends a similar formula, or places it on an unauthorized person, he shall be cut off [spiritually] from his people.\n||God said to Moses: Take fragrances such as balsam, onycha, galbanum, and pure frankincense, all of the same weight, as well as [other specified] fragrances.\n||Make [the mixture] into incense, as compounded by a master perfumer, well-blended, pure and holy.\n||Grind it very finely, and place it before the [Ark of] Testimony in the Communion Tent where I commune with you. It shall be holy of holies to you.\n||Do not duplicate the formula of the incense that you are making for personal use, since it must remain sacred to God.\n||If a person makes it to enjoy its fragrance, he shall be cut off [spiritually] from his people.\n|a cubit long...|\nIt was therefore 18' x 18' x 36'. Some say that it was 15' x 15' x 30' (Eruvin 4a; Maaseh Choshev 7:1). It appears that it was made of solid wood (cf. Exodus 27:8), although some say that it was like an inverted box (Maaseh Choshev 7:1).\nOr 'protrusions' (see Exodus 27:2). Some say that these protrusions were small cubes, three fingerbreadths (2 1/4') on each side (Maaseh Choshev 7:1). Others, however, maintain that they were horn-like protrusions. See note on Exodus 27:2.\nThe Talmud notes that this layer was as thick as a dinar (Eruvin 19a). See note on Exodus 38:24.\n|two gold rings...|\nSome say that they were on opposite corners of the altar (Maaseh Choshev 7:1). Others say that it had four rings, one on each corner (Abarbanel; cf. Baaley Tosafoth).\n|in front of the cloth partition|\nBetween the table and the lamp (see Exodus 26:34; Yoma 33b; Yad, Beth HaBechirah 3:17).\nEduth in Hebrew. Or, 'communion place.'\nDonated by an individual and not made special for this purpose (Rashi). Or, made with unauthorized ingredients (Ibn Ezra; Ramban).\nLiterally, 'burnt offering.'\n|once each year|\nOn Yom Kippur (Rashi; Yoma 61a). See Leviticus 16:18. See Leviticus 4:7.\nBy placing blood (Rashi).\nSome say that this was after the sin of the Golden Calf (Yerushalmi, Shekalim 2:3).\n|take a census|\n(Targum; Saadia). Literally, 'when you lift the head.' See Genesis 40:13,20.\n|to determine their numbers|\nOr, 'to count each one separately' (Saadia). Pakad in Hebrew. See Genesis 21:1.\nLiterally, 'sanctuary shekel' or 'holy shekel.'\n|a shekel is 20 gerahs|\nA gerah is the same as a ma'ah (Targum), a weight equivalent to 1.14 grams. Hence, a shekel is 22.8 grams or 0.8 ounces. Josephus notes that it is around 4 drachmas (Antiquities 3:8:2). A half shekel was a silver coin around the size of a half dollar.\nKiyyor in Hebrew. It looked like a large kettle with two spigots for washing (Rashi; Maaseh Choshev 6:8; Yoma 37a). According to others, it was apparently like a basin with spigots (Radak, Sherashim, who says it was like a cuenca, Spanish for basin). It was later made with 12 spigots (Yoma 37a). See 1 Samuel 2:14.\n(Targum; Rashi; Ibn Janach; Radak, Sherashim). Kan in Hebrew. See 1 Kings 7:38, 2 Chronicles 4:14 (cf. Ibn Ezra). In shape, the base was somewhat like an inverted pot (Tosafoth, Zevachim 22a, s.v. Kal VeChomer; Maaseh Choshev 6:9), or like an open box (Tosafoth). Some say that it was 3 handbreadths (9') high (Rabbenu Meyuchas).\nLater, this base was made to include a mechanism to fill the washstand with water (Yad, Beth HaBechirah 3:18), as well as machinery to lower the entire washstand into a well (Raavad ibid.; Yoma 37a; Zevachim 20a; Yad, Biyyath HaMikdash 5:14). A similar mechanism may have existed in the Tabernacle (see Torah Sh'lemah 38:6).\nAround 25 pounds.\n(Septuagint). Deror in Hebrew. Or, 'free of impurities' (Ibn Janach; Radak, Sherashim); or 'wild' (Ramban; Bachya). On the basis of Semitic cognates, some suggest 'flowing' or 'congealed into pearls.'\n(Raavad, Kley HaMikdash 1:3; Ramban; Bachya; Septuagint). Mor in Hebrew. Myrrh is a gum resin produced by trees and shrubs of the family Burseracea, most notably Commiphora myrrha, Commiphora abysinica, and Commiphora schimperi. The resin is obtained from Arabia and adjacent Africa, and is taken from the small, prickly gray-barked trees. Pearls of myrrh are brown, red or yellow, with an oily texture, becoming hard and brittle with age. It has a pleasing fragrance, very much like balsam, and a lasting, bitter, aromatic taste, hence the name mor, which signifies bitterness.\nAccording to many authorities, however, the mur here is not myrrh but musk (Saadia; Yad, Kley HaMikdash 1:3; Abarbanel; cf. Radak, Sherashim; Ibn Janach). This is an extract taken from the musk deer (Moschus moschiferus) which lives in Nepal and Tibet (see Ibn Ezra).\n|two half portions|\n(Rashi, Kerithoth 5a; Bachya; Ralbag; cf. Yad, Kley HaMikdash 1:2). According to Josephus, however, it would be translated, 'a half portion...' (Antiquities 3:8:3).\n(Rashi; Septuagint; Abarbanel, Canela in Spanish; Ibn Janach; Rambam on Kerithoth 1:1, but see Ramban on Exodus 30:34). Kinman in Hebrew. This is the dried bark of the cinnamon tree, Cinnamomum zeylanicum, a species of laurel cultivated mainly in Ceylon (cf. Yad, Kley HaMikdash 1:3; cf. Theophrastus, Plants 9:7; Herodatus 3:111).\nAccording to others, however, the kinman of the Bible is aloeswood or lignum aloes (Radak, Sherashim; cf. Saadia; see Ramban on Exodus 30:34). This is the resinous hartwood, Aquilaria agallocha of the family Thymalaeaceae, which grows in the East Indies and tropical Southeast Asia, and is still used for incense and perfumes.\nAccording to other ancient sources, the 'cinnamon' of antiquity was not the Ceylonese product, but an herb coming from Arabia (Theophrastus, History of Plants 9:4; Strabo 16:778; Diodorus Sicculus 2:49, 3:46) or Ethiopia (Pliny 12:42). Some identify it with 'Mecca Straw' (paja de Mecca in old Spanish; Ramban; Abarbanel), which was used as fodder for camels (Shir HaShirim Rabbah on Exodus 4:4). There are also indications that the 'cinnamon' of antiquity grew in the Holy Land (Yerushalmi, Peah 7:4; Bereshith Rabbah 65:17; see Kaftor Va Pherach 10, 31a).\nKeneh bosem in Hebrew. Ancient sources identify this with the sweet calmus (Septuagint; Rambam on Kerithoth 1:1; Saadia; Ibn Janach). This is the sweetflag or flag-root, Acoras calamus which grows in Europe. It appears that a similar species grew in the Holy Land, in the Hula region in ancient times (Theophrastus, History of Plants 9:7). Other sources apparently indicate that it was the Indian plant, Cympopogan martini, which has the form of red straw (Yad, Kley HaMikdash 1:3). On the basis of cognate pronunciation and Septuagint readings, some identify Keneh bosem with the English and Greek cannabis, the hemp plant.\nThere are, however, some authorities who identify the 'sweet cane' with cinnamon bark (Radak, Sherashim). Some say that kinman is the wood, and keneh bosem is the bark (Abarbanel).\n(Radak, Sherashim; Peshita; Vulgate). Kidah in Hebrew; ketzia in Aramaic (Targum; Rambam on Kelayim 1:8). Cassia is the common name for the bark of the tree Cinnamomum cassia or Cassia lignea belonging to the laurel family, which grows in China. (Pachad Yitzchak, s.v. Ketoreth; cf. Pliny 12:43; Theophrastus, History of Plants 9:7; Diodorus Siculus 3:46; Herodatus 3:110).\nThere are some, however, who identify the 'cassia' of the ancients, and hence kidah here, with costus, known as kosh't in the Talmud (Yad, Kley HaMikdash 1:3; Saadia; Ibn Janach; cf. Rashi). Costus is the root of the annual herb, Sausurea lappa, which grows on the mountain slopes of Kashmir, and is used for incense and perfume.\nThe Septuagint translates kidah here as iris, possibly Castus speciosus. Others suggest that it is kitto or mosylon, a plant very much like cassia, coming from Meuzel on the African coast (cf. Dioscorides, De Materia Medica 1:13).\nHin in Hebrew. Actually 0.97 gallon, or 3.6 liter.\nThe anointing oil was made by soaking the aromatic substances in water until the essential essences are extracted. The oil is then placed over the water, and the water slowly cooked away, allowing the essences to mix with the oil (Yad, Kley HaMikdash 1:2; from Kerithoth 5a). According to another opinion, the oil was cooked with the aromatic herbs, and then filtered out (Ibid.).\nSee note on Genesis 17:14.\n(Yad, Kley HaMikdash 2:4; cf. Kerithoth 6a; Rashi; Radak, Sherashim; Saadia; Rambam on Kerithoth 1:1). Nataf in Hebrew. Some say that nataf denotes the wood of the balsam, rather than the sap (Ramban). Balsam is also known as tzori in Hebrew (see Genesis 37:25). It is derived from the balsam tree, Commiphora opobalsamum, known as kataf in the Talmud, which grows wild in Yemen and around Mecca.\nThe Septuagint translates nataf here as stacte, which some identify as storax gum (Pachad Yitzchak, s.v. Ketoreth; cf. Dioscorides, De Materia Medica 1:79). However, the Greek word can also mean 'an oozing substance', very much like the basic connotation of nataf, and hence, it can denote any gum (thus, stacte can also denote myrrh; Pliny 12:75). It can thus denote balsam gum as well.\nShecheleth in Hebrew. The Targum translates this as tufra, the Talmud as tziporen (Kerithoth 6a), and the Septuagint as onyx, all denoting 'fingernail.' Some maintain that this is a spice actually prepared from human fingernails (cf. Arukh s.v. Tziporen), but most authorities see it as coming from an aquatic animal (Mossef HeArukh ibid.; Ramban). It is therefore usually identified as onycha (Hirsch; King James) or blatta byzantia (Abarbanel; Shiltey Gibborim 85), the fingernail-like operculum or closing flap of certain snails of the murex family, such as the Onyx marinus, Strombus lentiginosus, or Unguis Odaratus (Tifereth Yisrael, Chomer Bakodesh 2:67; Cf. Ben Sirah 24:15, Dioscorides, De Materia Medica 2:10). This emits a very pleasant smell when burned.\nOther sources, however, state that shecheleth is a kind of root (Rashi). The Talmud also appears to indicate that it came from an annual plant (Kerithoth 6b). Some identify this plant with a species of rock-rose, Cistus ladaniferus, which has fingernail-like petals.\nThe onycha was rubbed with an alkali solution prepared from the bitter vetch (Vicia sativa) (cf. Rambam on Maaser Sheni 2:4) to remove all impurities. It was then soaked in the fermented juice of the caper berry (Caparis spinosa) (see Tosafoth, Betza 25b, s.v. VeTzalaf) or strong white wine to enhance its fragrance (Kerithoth 6a; Yad, Kley HaMikdash 2:5).\n(Rashi; Septuagint). Chelbanah in Hebrew. It is a yellow-brown gum resin obtained from the Persian plant, Ferula galbaniflua (Pliny 12:56,24,13; Dioscorides, De Materia Medica 3:97). Alone it had a pungent, almost unpleasant odor (Kerithoth 6b).\nAccording to some, the chelbanah here is the gum of the common storax tree (Rambam on Kerithoth 1:1).\nOr oliban. (Septuagint). Levonah in Hebrew. This is a gum resin from trees of the genus boswellia, most notably Boswellia carterii and Boswellia frereana from Arabia, and Boswellia serratae from India. The gum is yellowish and semi-transparent, with a bitter nauseous taste. It is hard and capable of being pulverized, producing a strong aromatic odor when burned.\n|all of the same weight|\n(Targum; Kerithoth 5a; Rashi; Saadia). Or, 'each ground separately' (Ibn Ezra; cf. Yad, Kley HaMikdash 2:5).\n|other specified fragrances|\nBy tradition, an additional 7 fragrances were added, besides the four mentioned here, to give a total of eleven.\nThe formula for the incense was given in terms of the maneh which was 100 shekels or 5 pounds. It was\n70 maneh 350 lb. Balsam\n70 maneh 350 lb. Onycha\n70 maneh 350 lb. Galbanum\n70 maneh 350 lb. Frankincense\n16 maneh 80 lb. Myrrh\n16 maneh 80 lb. Cassia\n16 maneh 80 lb. Spikanard (shiboleth nard)\n16 maneh 80 lb. Saffron (karkom)\n12 maneh 60 lb. Costus (kosh't)\n9 maneh 45 lb. Cinnamon\n3 maneh 15 lb. Cinnamon bark\nThe total amount was 365 maneh, so that one maneh (5 lb) could be burned each day of the solar year.\nBesides these ingredients, 1/4 kav (1 cup) of Sodom salt (nitrate) and small amounts of maaleh ashan (probably Leptadenia pyrotechnica, which contains nitric acid) and kippath ha-yardan (probably cyclamen) were added. Besides this, 9 quarts (kab) of vetch lye (borith karshina) and 21 quarts (3 saah and 3 kab) of caper wine were used to prepare the onycha.\n(Targum; Rashi; Radak, Sherashim; Saadia). Or, 'salted' (Ibn Ezra; cf. Ibn Janach), referring to the Sodom salt that was added, Or, 'finely ground' (Ramban).""]"	['<urn:uuid:64655719-69a1-4f05-b351-b42d80ca99d2>']	factoid	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T19:47:23.148022	5	47	2736
74	What are the advantages of ozone over chlorine for water treatment?	Ozone has several advantages compared to chlorine in water treatment: it does not remain present in the water, it does not cause the formation of potentially carcinogenic organochlorinated products, and it does not leave an unpleasant taste that consumers dislike. However, it does leave a minimal blue tint to the water.	['RODO® – A UNIQUE AND PATENTED PROCESS\nIn order to control the proliferation of the ferrous-based ochre leading to expensive problems for our homes, “Solution Ocre Ferreux®” developed a unique and patented process called RODO®\nThis process consists of 4 stages :\n- Rinsing: We proceed with a very high-pressure rinsing of the drainage conduit, the backwater valve and the conduit connecting the home to the municipality. The water used is enriched with ozone in order to begin, from the first stage, the destruction of the bacteria.\n- Oxidation: We inject a solution made of ultra-pure concentrated ozone, reactive and very powerful in the drainage conduit in order to treat the conduit up to the connection to the municipality conduit. Unlike the rinsing stage, this stage is not performed under high pressure. In order to oxidize the bacteria in the drain and especially in the ground surrounding the drain, the oxidation stage rather works with the volume of water and ozone injected in the drains through cleaning vents.\n- Disinfection: We ensure the total and complete disinfection of the drainage system. The drainage system consists of the ground surrounding the drain, the internal and external walls of the drain, the backwater valve as well as the evacuation conduit up to the connection to the municipality. This stage is important and is focused on the contact time between the oxidizing solution and the bacteria.\n- Ozoflotation: The ozoflotation is in fact the conglomeration in non-harmful flakes of the bacteria in suspension rendered inert by the oxidation and disinfection stages. The conglomerated flakes are evacuated from the circulation of water in the ground surrounding the drain and inside the drain, up to the evacuation conduit connected to the municipality. This last stage completes the cleaning following the previous stages of oxidation and disinfection.\nHOW DO WE GENERATE THE OZONE?\nWHAT IS THE OZONE?\nThe ozone is a chemical compound containing 3 atoms of oxygen (O3). Its structure is a resonance between three states. Metastable to ambiant conditions, tends to decompose naturally into dioxygen. At room temperature, it is a pale blue gaz. It liquefies at -111,9 °C in the form of a particularly unstable dark blue liquid and solidifies at -192,5 °C.\nContrary to the odourless dioxygen, the ozone is preceived by the human sense of smell; its smell is very characteristic, slightly pungent. It is toxic when inhaled in large quantities.\nThe ozone was first discovered in 1781 by Martin Von Marum, then the molecule was isolated in 1839 by the Swiss chemist Christian Friedrich Shonbein. He so called it by referring to the Greek root ozein (exhale a smell, to smell).\nTHE VARIOUS INDUSTRIAL USES OF THE OZONE\nAt the industrial scale, the ozone is produced using short wavelengths ultraviolet radiations emitted by a mercury vapour lamp or by a process of cold discharge (corona discharge) which uses a high electrical field. The discharge device is constituted by two metal plates separated by a layer of air and an electrical insulator of high dielectric constant. A high bias voltage is applied between the two plates, which entails the dissociation of the molecules of oxygen from the layer of air and their recombination in ozone. The ozone is an oxidizer and a powerful disinfectant. It presents certain advantages in comparison to other oxidizers usually used in the industry.\nPURIFICATION OF DRINKING WATER\nThe ozone presents a series of advantages with respects to the chlorine used for water treatment, which explains its use in spite of the often significant cost of a water ozonization installation. These advantages are as follows:\n- It does not remain present in the water;\n- It does not provoke the appearance of organochlorinated products, which can be carcinogenic;\n- It does not leave a bad taste unappreciated from consumers of drinking water\nBut it leaves a minimal blue tint to the water.\nThe ozone is used in the treatment of water for several functions :\n- Iron oxidation;\n- Performance improvement of sand filters;\n- Improvement of the flocculation (called ozoflotation);\n- Disinfection of the water;\n- Elimination of harmful organic compound, in particular pesticides and herbicides. For this application the ozone is generally injected upstream of an active coal-based filter.\nThis application is highly prevalent in Anglo-Saxon countries and less in continental Europe. The ozone is essentially used for the washing of white linen in commercial installations.\nThe ozone is used in the food-processing industry for two very different applications :\n- The ozonization of cold rooms;\n- The washing of food products in ozonated water.\nThe wafers intended for the manufacturing of integrated circuits are washed with ozonated water with the objective to eliminate potential traces of organic matter.']	['<urn:uuid:5ba44cc5-3834-4c0a-9d00-3a81c94fc962>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T19:47:23.148022	11	51	784
75	meaning real vs abstract words examples	The words 'cat', 'water', and 'teacher' are concrete words because they represent actual substances or things, while words like 'truth', 'excellence', and 'adulthood' are abstract because they represent abstract qualities rather than tangible objects.	"['CONCRETEcon•crete (kon′krēt, kong′-, kon krēt′, kong- for 1–10, 11, 14, 15; kon krēt′, kong- for 12, 13),USA pronunciation adj., n., v., -cret•ed, -cret•ing.\n- constituting an actual thing or instance;\nreal: a concrete proof of his sincerity.\n- pertaining to or concerned with realities or actual instances rather than abstractions;\nparticular (opposed to general): concrete ideas.\n- representing or applied to an actual substance or thing, as opposed to an abstract quality: The words ""cat,\'\' ""water,\'\' and ""teacher\'\' are concrete, whereas the words ""truth,\'\' ""excellence,\'\' and ""adulthood\'\' are abstract.\n- made of concrete: a concrete pavement.\n- formed by coalescence of separate particles into a mass;\nunited in a coagulated, condensed, or solid mass or state.\n- an artificial, stonelike material used for various structural purposes, made by mixing cement and various aggregates, as sand, pebbles, gravel, or shale, with water and allowing the mixture to harden. Cf. reinforced concrete.\n- any of various other artificial building or paving materials, as those containing tar.\n- a concrete idea or term;\na word or notion having an actual or existent thing or instance as its referent.\n- a mass formed by coalescence or concretion of particles of matter.\n- set or cast in concrete, to put (something) in final form;\nfinalize so as to prevent change or reversal: The basic agreement sets in concrete certain policies.\n- to treat or lay with concrete: to concrete a sidewalk.\n- to form into a mass by coalescence of particles;\n- to make real, tangible, or particular.\n- to coalesce into a mass;\n- to use or apply concrete.\nINin (in),USA pronunciation prep., adv., adj., n., v., inned, in•ning.\n- (used to indicate inclusion within space, a place, or limits): walking in the park.\n- (used to indicate inclusion within something abstract or immaterial): in politics; in the autumn.\n- (used to indicate inclusion within or occurrence during a period or limit of time): in ancient times; a task done in ten minutes.\n- (used to indicate limitation or qualification, as of situation, condition, relation, manner, action, etc.): to speak in a whisper; to be similar in appearance.\n- (used to indicate means): sketched in ink; spoken in French.\n- (used to indicate motion or direction from outside to a point within) into: Let\'s go in the house.\n- (used to indicate transition from one state to another): to break in half.\n- (used to indicate object or purpose): speaking in honor of the event.\n- in that, because;\ninasmuch as: In that you won\'t have time for supper, let me give you something now.\n- in or into some place, position, state, relation, etc.: Please come in.\n- on the inside;\n- in one\'s house or office.\n- in office or power.\n- in possession or occupancy.\n- having the turn to play, as in a game.\n- [Baseball.](of an infielder or outfielder) in a position closer to home plate than usual;\nshort: The third baseman played in, expecting a bunt.\n- on good terms;\nin favor: He\'s in with his boss, but he doubts it will last.\n- in vogue;\nin style: He says straw hats will be in this year.\n- in season: Watermelons will soon be in.\n- be in for, to be bound to undergo something, esp. a disagreeable experience: We are in for a long speech.\n- in for it, [Slang.]about to suffer chastisement or unpleasant consequences, esp. of one\'s own actions or omissions: I forgot our anniversary again, and I\'ll be in for it now.Also,[Brit.,] for it.\n- in with, on friendly terms with;\nfamiliar or associating with: They are in with all the important people.\n- located or situated within;\ninternal: the in part of a mechanism.\n- in favor with advanced or sophisticated people;\nstylish: the in place to dine; Her new novel is the in book to read this summer.\n- comprehensible only to a special or ultrasophisticated group: an in joke.\nincluded in a favored group.\ninbound: an in train.\n- being in power, authority, control, etc.: a member of the in party.\n- playing the last nine holes of an eighteen-hole golf course (opposed to out): His in score on the second round was 34.\n- Usually, ins. persons in office or political power (distinguished from outs).\n- a member of the political party in power: The election made him an in.\n- pull or influence;\na social advantage or connection: He\'s got an in with the senator.\n- (in tennis, squash, handball, etc.) a return or service that lands within the in-bounds limits of a court or section of a court (opposed to out).\nv.t. Brit. [Dial.]\n- to enclose.\nMODERNmod•ern (mod′ərn),USA pronunciation adj.\n- of or pertaining to present and recent time;\nnot ancient or remote: modern city life.\n- characteristic of present and recent time;\nnot antiquated or obsolete: modern viewpoints.\n- of or pertaining to the historical period following the Middle Ages: modern European history.\n- of, pertaining to, or characteristic of contemporary styles of art, literature, music, etc., that reject traditionally accepted or sanctioned forms and emphasize individual experimentation and sensibility.\n- (cap.) new (def. 12).\n- [Typography.]noting or descriptive of a font of numerals in which the body aligns on the baseline, as 1234567890. Cf. old style (def. 3).\n- a person of modern times.\n- a person whose views and tastes are modern.\n- [Print.]a type style differentiated from old style by heavy vertical strokes and straight serifs.\nHello peoples, this photo is about ROBATA 72 CONCRETE IN NOUGAT – RAVINE MODERN (awesome Linear Fire Pit #12). This picture is a image/jpeg and the resolution of this photo is 983 x 656. This picture\'s file size is just 91 KB. Wether You decided to download This image to Your PC, you might Click here. You may too download more pictures by clicking the image below or see more at this post: Linear Fire Pit.\nThe house frequently has its figure. Also with cottages or the bungalow can be found in britain. Don\'t want to modify the building\'s framework is toomuch, ROBATA 72 CONCRETE IN NOUGAT – RAVINE MODERN (awesome Linear Fire Pit #12) types compete with classic pad.\nNever asked a result, lovely! So that you can take care of the identity of the building, Kitchen\'s artist Alex Saint Structure incorporating a kitchen layout in addition to the key building. The result? Stunning! Yes, Chelshire was located in by a cottage, the united kingdom may be the building involved.\nIf you just like the setting of the home that is cozy as well as peaceful with a minor antique sense with possibly a great choice for you. To obtain this style you utilize a wooden ground and can make cheap kitchen units an election that have pattern includes a sample. Using pastel shades brown with variations of lumber and bright hues is likely to make supper within the kitchen together with your household may experience warmer.\nThe bungalow was built-in the 18th-century and is now past the stage of renovation. As opposed to looking to imitate the cottage\'s design, Alex Saint decided to develop one more home design that can lessen the entire lodge\'s structural change and protect the type with this residence.\nDesire to deliver the setting is comfortable and hot, the furniture has a delicate bright color as his finishing. Storage that is much and contemporary equipment can be beautiful home design suits this one. Similarly with up lighting to illuminate the area through the night.\nA cube\'s kitchen style in the type. The usage of glass here is supposed to have the ability to control the heat. While summer occurs, glass can be exposed to provide fresh-air into the area. Surfaces utilising the same product with the outside patio for there to be always a frequent bond involving the Linear Fire Pit with new home.']"	['<urn:uuid:249d9c2c-3a4d-4393-970a-eacbbc299d5d>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T19:47:23.148022	6	34	1313
76	singapore botanic gardens orchid program success conservation efforts extinction rates native plant species	The Singapore Botanic Gardens has a successful orchid program that includes hybridization since the 1920s and conservation efforts through propagating endangered orchids and replanting them across Singapore's parks. However, despite these conservation initiatives, Singapore has lost 30-38% of its native plant species since 1822, with an estimated 464 recorded extinctions plus 213-534 additional unrecorded species extinctions.	['• Rubber ‘n’ Spice: Economic Botany power house\n• Orchids: Exotica – science meets commerce\n• Dipterocarps: rainforest ark in a City-State\n• Dynamism: great team – catalysing regional capacity\n• High wire act: balancing rapid change / newness with history / richness\nWhen I visited, the man master-minding ‘Gardens by the Bay’ was lured from retirement after having successfully re-wired Singapore Botanic Gardens, setting a cracking course, just capped off by that Botanic Gardens’ welcome World Heritage Listing. The man now running the Botanic Gardens, Dr. Nigel Taylor and his dynamic team continue great work improving, renewing, conserving and promoting these wonderful gardens.\nWhy listed? As an exemplary tropical colonial ‘empire’ botanic garden from the 1860s.\n• Rubber & Spice: this was the place whose 1880s-90s trials in rubber cultivation and tapping-techniques made Singapore and South-East Asia ‘rubber capital’ of the world, snatching that crown from Brazil. Spices, fibres and other crops were tested here for colonies and settlements around the Straights, Malay Peninsula (now Malaysia) and neighbours. Importation, trials, supplying plantation managers led to vital industries that continue to nourish regional economies: ‘economic botany’ of yore when empires were built on such – think tea, coffee, chocolate, palm oil…\n• Orchids: commerce-led horticulture and cropping morphed into more ornamental and horticultural pursuits – and Singapore’s colonial botanic garden went with it, re-purposing into collecting native flora and experimenting with the rich orchid life of South East Asia. Cataloguing and understanding what is endemic or local, testing and breeding are key pursuits. Hybridising orchids since the 1920s – crossing different genera and back-crossing, has led to the explosion of colours and forms that are orchids today. This garden remains high on any ranking of global orchid centers. Past Directors such as Eric Holttum and Dr. Kiat W. Tan did much to put it there, collecting, hybridising, displaying, promoting. Better still the interested public today, from school children to grandfathers, tourists, whatever, can look straight into laboratories with tissue culture flasks, benches with scientists working on orchid hybridisation. This ‘window into the science’ and back-room tours help demystify these wonderful plants, and some of the whiz-bang tools by which they can be brought into being.\nA programme of naming orchid hybrids after visiting dignitaries or honouring celebrated locals means regular visitors to the gardens and ongoing interest. A smart move. Propagating endangered and rare orchids and re-planting these on trees or in parks across Singapore’s system of greenway roads and parks has meant a safer future for some species, practical conservation and education.\nHorticultural staff lived in cottages on site until 1974, learning their trade from generation to generation. Moving staff off site led to a School of Horticulture (1972-99), continuing to train and develop skills. This today is the Centre for Urban Greenery and Ecology (CUGE), a joint venture of National Parks and the Singapore Workforce Development Agency. It provides professional training in urban greening, ecology, landscape management and horticulture, offering certificates to up skill this sector and certify professionals. CUGE also undertake research, job placements and career advice to this growing industry.\n• Dipterocarps are canopy emergent rainforest trees whose seedling leaves (cotyledons) are paired and winged. These distinctive forms dominate a type of closed forest peculiar to this region.\nSadly extensive and ongoing logging means that many dipterocarp species today are endangered. Certainly you are extremely lucky to come across giant veterans, upright.\nThat a 6 hectare chunk of pre-colonial forest and revegetating fringes survive in the middle of madly-urban Singapore is a gob-smacker. The adjacent Tyersall Learning Forest area, with c.1905 secondary lowland rainforest values has potential as an additional research and educational resource. That the Gardens’ fore-runners had the foresight to protect it, amid or alongside cleared areas for crop trials originally laid out in the 1860s for pleasure and recreation, and from 1874 taking on a more scientific (‘nursery of empire’) role is a wonder.\nThis forest today is a green lung in the midst of fast and extensive urban development. And all the more welcome as contrast and relief. Not just for humans either. Its offspring are marching down Singapore’s streets and greenways – the Gardens took a leading role in helping ‘Garden City’ (now ‘City in a Garden’) effectively re-greening the city-state. This was a Prime Ministerial policy from the late 1960s, advising on trees and other plant species for its streets, parks and green links. While much of this is now devolved to NParks, the Gardens continues to advise and assist – e.g. propagating and re-introducing endangered orchid and ginger species into the open space system. NParks’ vision is ‘Let’s make Singapore our Garden’. One of its six thrusts is ‘Establish world class gardens’ (of which SBG is a vital part), along with ‘Rejuvenate urban Parks and enliven our streetscape’, ‘Enrich biodiversity in our urban environment’, ‘Engage and inspire communities to co-create a greener Singapore’ and ‘Enhance competencies of our landscape and horticulture industry’.\n• Dynamism is one good word for the staff of this institution who continue in a proud tradition of leadership in this part of the world. For many decades led by Directors sent from and trained at Royal Botanic Garden, Kew (UK), a custom of educating, training and working to up-skill local staff led to native-born Directors and management. It today means training workshops, joint collecting expeditions, funding and support for a range of other botanic gardens across this biodiverse region. Borneo, Sarawak, Indonesia, Vietnam, Burma and more are near-neighbours, all with rich floras, little-studied or understood, let alone used. A programme of cooperative partnerships with nearby countries in botanical surveys, research and publication, in addition to training on site and remotely, strengthens the gardens’ scientific and educational functions.\nRetention and building not only living but conserved (library, bibliographic and visual reference) collections, herbarium specimens (including fungi, micro-plantlets in vitro and specimens preserved in spirits/alcohol) maintain the Gardens’ historic role as one of the leading centres for research and support in identifying and classifying the super-rich biodiversity of the region, particularly peninsular Malaysia. This allows reference comparison of material by researchers, logistical and other support and training opportunities.\n• Managing Change: as in any heritage place, particularly a living one such as a garden, change is inevitable and welcome. Managing it while conserving and sustaining the very things that make it ‘heritage’ is a challenge, a high-wire act. More so in a public open space with ready access in a big city. Even more so in a tropical climate. A series of over 40 significant heritage trees have lightning conductors on them, in case of strikes, which Singapore gets. High standards of maintenance by teams dedicated to particular sections mean that damage by animals like squirrels, monkeys and large birds, storms or weather (erosion, broken or fallen branches), humans (large events, wear and tear, rubbish) are swiftly reported and repaired. Retention, propagation, replanting long-term replacements and active maintenance of a range of landscape features such as Swan Lake (1866), the Palm Valley (1879), Potting Yard area (1880s), The Dell (1882), sealing wax palm avenue (1905), frangipani, palm and orchid collections, Sundial Garden (1929), Bandstand (1930, replacing a c.1860 one), brick steps constructed during World War II, Symphony Lake (1974, evoking lost Cluny Lake of 1891) help conserve the cultural landscape as a rich and layered one, expressive of its age and changing curators’ foci. Continuing displays and development of the National Orchid Garden remind visitors of vital work done on classification, hybridising and releases.\nBotany today has shifted focus from medicinal and economic crop-led-export income to one of conservation, ecology and education. Singapore too has evolved from a colony to an independent nation with distinct priorities and aspirations. The gardens have been used as a site fostering national integration and unity – by Lee Kwan Yew, e.g. multi-cultural celebrations held here in 1959. It has a high number of visitors, c.4.4million per annum. And growing numbers of school children, who understandably love its diversity and frankly wonderful children’s gardens. The Jacob Ballas Children’s Garden was Asia’s first, and a huge success. It is being expanded in area, with ambitions to broaden its target audience to include teenagers.\nThe broader public is increasingly well-catered for: the Holttum Hall Museum has displays on the history and evolution of the Gardens, a new CDL exhibition Gallery currently has an exhibition on medicinal and cultural uses of herbs and other plants in the Malay or Peranakan community.\nAlongside is an ongoing parallel programme of guided heritage and other walks and rollout of new signage highlighting and explaining heritage trees and other features. And three new subway stations are open or being built (2015-20) allowing far better access for all Singaporeans to their gardens: bravo!\nMake sure you visit Singapore Botanic Gardens next time you’re in Singapore – it rewards closer scrutiny.', 'Have you ever wondered how many species were lost before we had the chance to discover them? In a paper now out in Conservation Biology, we estimated just that, for plant species in Singapore. This paper follows from the Chisholm Lab’s related work on Singapore birds and butterflies.\nAll over the world, many species remain undiscovered while both known and unknown species continue to go extinct. This is particularly true in the tropics, where biodiversity is high and development continues apace. Singapore provides an invaluable case study of tropical biodiversity loss. Since British colonialisation, most of Singapore’s forest cover has been replaced with urban landscape. However, it also has one of the best-documented floras in the world, both in terms of taxonomic and temporal coverage, with historical collections beginning only a few years after colonialisation.\nWe collated a high-quality database of Singapore plant collections, and we the “SEUX” model to estimate extinction rates and total numbers over time. The SEUX model is based on a fairly straightforward idea: if we assume that the per-year extinction rates were the same for discovered and undiscovered species, then we have a basis for working backwards in time to estimate the number undiscovered species and the proportion that went extinct. The technically challenging aspect was to turn this idea into a method for obtaining interval estimates.\nWe estimated that 30 - 38% of Singapore plant species have gone extinct since 1822. The central estimate using classical methods was 32% and that using Bayesian methods was 35%. Crucially, these numbers are much higher than the 22% extinction rate that one obtains from the naïve method of simply dividing the number of known extinctions by the total number of discovered species, demonstrating the importance of calculating extinction rates in a way that accounts for unknown species.\nIn addition to the 464 extinctions recorded in our database, we estimated that between 213-534 additional species (95% CI total range) went locally extinct. As we explain in the paper, these estimates should be treated as lower bounds. If undiscovered species are more likely to go extinct than discovered ones (likely), then both the total extinction rate and the number of undiscovered extinctions is in fact higher. The number of undiscovered extinctions will also be higher if many undiscovered species remain extant today (though the total extinction rate estimates are unaffected by this assumption).\nThe Singapore plant collection is a valuable resource: it is an unusually rich database, situated in a tropical biodiversity hot-spot, and temporally coinciding with extensive forest clearing and urbanisation. Our colleagues at Singapore Botanic Gardens did a lot of work compiling the database, including filtering for native species in wild localities, resolving taxonomic issues, and checking the correctness of records. The resulting database contains 34,224 specimens from 2,076 species and 174 families, from collections around the world. We hope that future researchers will find this dataset useful as a test-bed for their work.\nKristensen, N.P., Seah, W.W., Chong, K.Y., Yeoh, Y.S., Fung, T., Berman, L.M., Tan, H.Z., Chisholm, R.A. (2020) Extinction rate of discovered and undiscovered plants in Singapore, Conservation Biology\n- the new paper\nTheng, M., Jusoh, W.F.A., Jain, A., Huertas, B., Tan, D.J.X., Tan, H.Z., Kristensen, N.P., Meier, R., Chisholm, R.A. (2020) A comprehensive assessment of diversity loss in a well-documented tropical insect fauna: Almost half of Singapore’s butterfly species extirpated in 160 years, Biological Conservation, 242\n- the SEUX model applied to butterflies in Singapore\nChisholm, R. A., Giam, X., Sadanandan, K. R., Fung, T., & Rheindt, F. E. (2016) A robust nonparametric method for quantifying undetected extinctions. Conservation Biology, 30(3), 610-617.\n- the paper introducing the SEUX model\nKristensen, N.P. SEUX for R on Github\n- a repository of an R package (including tutorials) so you can apply the SEUX model to your own data\nPrevious blog post\n- a 5 minute tutorial to get started using the SEUX package']	['<urn:uuid:7456282a-7d39-4c55-b4fb-cd9150572341>', '<urn:uuid:3e7f617b-8ca4-4529-90b3-2dc0245f4282>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T19:47:23.148022	13	56	2118
77	what happens if cervix not removed during hysterectomy procedure risks	If the cervix is not removed during hysterectomy (called supracervical hysterectomy), the woman remains at risk for cervical cancer and must continue to have routine Pap smears. Some women may also continue to experience menstrual periods if the retained cervix is attached to the lower uterine segment and its endometrium.	"[""The definition of a hysterectomy is removal of the uterus.\nThe uterus is a hollow, pear-shaped muscular organ located in the lower abdomen or pelvis. One end of each fallopian tube is contiguous with and opens into the side of the uterus, at the upper end, and the other end lies next to an ovary. At its lower end, the uterus narrows and opens into the vagina. The lower end of the uterus is called the cervix. The ovaries lie lateral to and slightly behind the uterus.\nA total hysterectomy includes removal of the uterus, cervix and sometimes, but not always, the ovaries. A partial hysterectomy means the cervix was not removed. A separate term, oophorectomy, is used for removal of the ovaries. If the fallopian tubes are removed with the ovaries, the term is salpingoophorectomy.\nThere are several ways to perform a hysterectomy, all dependent on the patient’s history, the indications for the procedure and the size of the organ itself.\nAbdominal hysterectomy is a surgical procedure in which the uterus is removed through an incision in the lower abdomen. One or both ovaries and fallopian tubes may also be removed during the procedure. Abdominal hysterectomy is a safe, routine procedure, but still a major operation. The patient typically stays in the hospital for two days and should plan a period of several weeks to recover at home, during which normal activities can gradually be resumed.\nVaginal hysterectomy is a surgical procedure in which the uterus is removed through the vagina. One or both ovaries and fallopian tubes may be removed during the procedure as well. A vaginal approach may be chosen if the uterus is not greatly enlarged, and if the condition prompting the surgery is benign and limited to the uterus. Studies have shown that vaginal hysterectomy is associated with fewer complications, a shorter hospital stay and faster recovery than an abdominal hysterectomy.\nAlthough infrequent, your surgeon may find conditions, such as extensive scar tissue, during a vaginal hysterectomy that make abdominal hysterectomy the better choice. Sometimes these conditions are not apparent before surgery. When this happens, the surgeon stops the vaginal procedure and changes to an abdominal approach.\nLaparoscopically Assisted Vaginal Hysterectomy\nSome surgeons use a laparoscope in conjunction with the vaginal hysterectomy procedure. A laparoscope is a surgical instrument inserted through a small incision in the abdomen. Using the scope, the surgeon can see structures within the abdomen and, if needed, perform procedures such as removing scar tissue to aid the vaginal surgery. It is generally believed that a woman can return to normal activities sooner after a vaginal hysterectomy than after an abdominal hysterectomy. As a result, sometimes patients who are candidates for abdominal hysterectomy may have laparoscopy done to mobilize the uterus and allow a vaginal hysterectomy to be done instead.\nSupracervical or Subtotal Hysterectomy\nThere are situations in which the entire uterus is not removed. A supracervical, subtotal or partial hysterectomy refers to a procedure in which the cervix is left intact. Sometimes during hysterectomy difficulties arise that make the surgeon decide that leaving the cervix in place is safer. Also, some women prefer keeping the cervix, as they feel it will contribute to sexual satisfaction after hysterectomy. Prior to planned supracervical hysterectomy, tests should be performed to exclude any cervical abnormalities. In addition, the woman should discuss with her doctor the risks and benefits of leaving the cervix intact.\nIn rare cases of emergency hysterectomy to control bleeding following childbirth, the supracervical procedure may be performed because, in this setting, it can be difficult to identify the boundary of the cervix.\nIf the cervix is not removed, the woman remains at risk for cervical cancer and must continue to have routine Pap smears. In some women, the retained cervix is attached to the lower uterine segment and its endometrium, meaning they will continue to experience menstrual periods.\nA hysterectomy treats a number of conditions, including:\nFibroids: Noncancerous tumors of uterine muscle, which may occur in up to a third of all women and typically shrink after menopause\nPelvic relaxation: Stretching and weakening of the pelvic muscles and ligaments that allow the uterus to sag (or prolapse) into the vagina, often associated with vaginal childbirth\nAbnormal uterine bleeding: Irregular or heavy uterine bleeding unresponsive to D&C or medical therapy\nAdnexal mass: A mass involving the ovaries or, rarely, the fallopian tubes\nCervical abnormalities: Precancerous abnormalities or cancer of the cervix unresponsive to lesser procedures such as cone biopsy or treatments using laser or cryosurgery\nEndometrial hyperplasia: Excessive growth of the endometrium, or the tissue that lines the uterus. Some types of endometrial hyperplasia may be precursors of endometrial cancer.\nMalignancy: Cancer affecting the uterus or other reproductive organs\nTubo-ovarian abscess: A collection of pus and inflamed tissue involving the ovaries and fallopian tubes\nChronic pelvic pain: Chronic pelvic pain may be due to the effects of endometriosis or scarring (adhesions) in the pelvis and between pelvic organs\nOther Treatment Options\nIn some cases, medication or limited surgery can be used to treat an underlying condition, and hysterectomy can be postponed or avoided. The decision to proceed with surgery should be made mutually by the patient and her doctor after careful consideration of the woman's particular medical problem, all available treatment options, and the risks and benefits of each type of treatment.\nComplications associated with hysterectomy, most of which can be easily managed, include:\nHemorrhage: Excessive bleeding occurs in a small number of cases and may require transfusion and/or a return to the operating room to identify and correct the cause.\nInfection: Fever after hysterectomy is not uncommon. In some women, the fever may be due to an infection that can be treated with intravenous antibiotics. Rarely, serious infections that are not eliminated by antibiotics require further surgery.\nDamage to other abdominal structures or organs: The urinary bladder, ureters (small tubes leading from the kidneys to the bladder), and the large and small intestine are located in the lower abdomen and can be damaged during hysterectomy. If injury occurs, it may be detected and corrected at the time of surgery, usually without subsequent problems.\nThromboembolism: Formation of a blood clot in a blood vessel is a rare complication after abdominal hysterectomy. Preventive treatment and postoperative ambulation help minimize this risk. Women taking oral contraceptives should discontinue them one month prior to planned surgery, and use alternative methods of birth control.\nOther more rare complications include protrusion of the small intestine or prolapse of the fallopian tube into the vagina. These can be corrected surgically. As with any abdominal operation, there is a possibility of developing a hernia in the incision site. Complications related to anesthesia can also occur.\nIf the decision is made to proceed with a hysterectomy, there are additional aspects that need to be considered.\nOophorectomy is surgical removal of the ovaries. It is sometimes done in conjunction with hysterectomy. Some women have a condition that requires that the ovaries be removed. Others, however, may be asked to make a choice between keeping the ovaries or having them removed.\nIf the patient has not gone through menopause, keeping the ovaries allows for continued natural production of estrogen and progesterone and allows the woman to avoid the need for hormone replacement therapy immediately after surgery. Removal of the ovaries, however, essentially eliminates the risk of needing additional operations for future ovarian problems, and may be desired in some women.\nIn women who have completed menopause, the ovaries are frequently removed as part of the surgery because they are not producing large amounts of hormones, it is easily accomplished as part of the surgery, and avoids the need for future surgeries should ovary problems arise.""]"	['<urn:uuid:53a6171f-70ab-4c00-80e5-0bbccc0a9f17>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T19:47:23.148022	10	50	1282
78	What are stem cells' benefits and testing safety concerns?	Stem cells offer remarkable benefits as they can regenerate damaged organ tissues and potentially enable growing new organs for cell-based therapies. They could provide treatments for cancer, diabetes, Parkinson's disease, Alzheimer's disease, and various brain and spinal cord disorders. However, testing these therapies raises serious safety concerns, particularly in first-in-human trials. The challenges include minimizing harm to subjects, dealing with unreliable animal models, protecting vulnerable patient groups, and managing uncertain risks. These issues are especially critical given the seriousness of potential adverse effects and the high-stakes nature of such publicly visible research.	"[""Most of the cells in our bodies have a short life span and a specific job to do. Stem cells, found in many organs from skin to bone marrow, are different. The “ancestors” of ordinary cells, they can replenish themselves indefinitely. Given the right biochemical signals, these cells can divide and transform themselves into a range of different cell types as and when the need arises.\nSuch versatility means that stem cells have the potential to regenerate damaged organ tissues and provide amazing new treatments for a range of now devastating diseases. Indeed, stem cells may perhaps make it possible for humans to “grow new organs” to replace diseased ones, thereby providing for the possibility of new forms of cell-based therapies.\nOn the basis of research with comparable cells from mice, scientists envision that stem cells could be used to replace bone marrow in patients suffering from cancer, produce pancreatic cells for alleviating diabetes, or neuronal cells for treating Parkinson's disease, Alzheimer's disease, and various brain and spinal cord disorders.\nStem cells used in this research are best derived from very early stage embryos created by in vitro fertilization, but this leads to ethical dilemmas. The idea that human embryos could be created with the express purpose of “harvesting” them for medical purposes incites fear of a “brave new world” revisited. Even if that extreme is unlikely, the possibility of abuse of any sort generates strong opposition.\nMost of those who oppose stem cell research do not question its scientific and medical value, but object to the use of human embryos. These critics, especially in Germany, where stem cell research evokes nightmares of Nazi era eugenics, argue that research with stem cells derived from embryos is unnecessary because multi-potential stem cells occur naturally in adult tissues as well. While guardedly encouraged by recent reports of the remarkable plasticity of stem cells obtained from adult tissues, scientists know little about their potential for prolonged maintenance outside the body, their capacity for differentiation, and whether they can be obtained in the quantities needed to explore their utility for clinical use.\nIf embryonic stem cell research were to be halted based on that hope, it is possible that years may pass before scientists could determine if adult-derived stem cells are equivalent to embryonic stem cells in their potential to treat disease. We cannot afford the luxury of proceeding with these promising technologies sequentially. We owe it to those who are in need to explore all possible avenues that could lead to medical cures. While research on adult-derived stem cells should be pursued aggressively, it should not be at the expense of an aggressive program to explore the full potential of stem cells for use in human therapies.\nAnother restriction has been recently adopted in the United States, where the issue was explored by the National Biomedical Ethics Advisory Commission in extensive hearings and discussions with ethicists, religious leaders, scientists and physicians. They concluded that it is ethically permissible to prepare stem cell lines from frozen embryos, but only from those obtained in the course of in vitro fertilization procedures and deemed by donors and their physician to be in excess of clinical requirements. (More eggs are usually fertilized than are actually implanted in women who want to become pregnant.)\nFollowing these discussions, the US National Institute of Health (NIH) promulgated guidelines that allow the use of government funds for stem-cell research but, in their present form, prohibit federally-funded investigators from creating new stem cell lines. That restriction requires that stem cell lines must be obtained only from privately-funded or commercial sources that follow the protocols mandated by the guidelines.\nI believe that restricting the use of government funds for the creation of stem cells, although politically prudent, imposes substantial scientific handicaps. We know that a variety of poorly understood factors cause embryonic stem cells to lose their capacity to differentiate into all possible cell types. This loss of capacity to differentiate may be caused by the particular mode in which the stem cells are derived, the conditions surrounding their growth, and other variables of handling.\nEnabling individual government-funded investigators to derive stem cell lines using a variety of conditions in their own laboratories, as permitted recently in Britain and France, is the best route to determining what conditions are critical for generating cells for specific therapeutic purposes. Embryonic stem cells prepared in a scientist’s own laboratory are most likely to have been derived, stored and maintained in ways that maximize their potency for particular uses. Cells obtained from commercial sources are likely to be of unknown genetic background and history and therefore be less useful for some important studies.\nWhile various remote contingencies are sometimes invoked, the fundamental ethical choice posed by stem cell research is more circumscribed: whether human embryos, which will ultimately be legally discarded, should be used to create potentially life-saving therapies for those in need. Those who oppose abortion irrespective of its purpose argue that sacrificing a human embryo to obtain stem cells is akin to abortion and therefore immoral. But harvesting stem cells from very early stage embryos that will in the end be discarded is, in my view, ethically distinct from abortion. For we must also consider the moral consequences of failing to pursue this line of research if it has the potential to save and improve human lives. What greater morality exists than doing all we can do to help individuals whose lives are blighted by disease and disability?"", ""Uncertain translation, uncertain benefit and uncertain risk: Ethical challenges facing first-in-human trials of induced pluripotent stem (ips) cells\nDavid Bourget (Western Ontario)\nDavid Chalmers (ANU, NYU)\nRafael De Clercq\nJack Alan Reynolds\nLearn more about PhilPapers\nBioethics 27 (2):89-96 (2013)\nThe discovery of induced pluripotent stem (iPS) cells in 2006 was heralded as a major breakthrough in stem cell research. Since then, progress in iPS cell technology has paved the way towards clinical application, particularly cell replacement therapy, which has refueled debate on the ethics of stem cell research. However, much of the discourse has focused on questions of moral status and potentiality, overlooking the ethical issues which are introduced by the clinical testing of iPS cell replacement therapy. First-in-human trials, in particular, raise a number of ethical concerns including informed consent, subject recruitment and harm minimisation as well as the inherent uncertainty and risks which are involved in testing medical procedures on humans for the first time. These issues, while a feature of any human research, become more complex in the case of iPS cell therapy, given the seriousness of the potential risks, the unreliability of available animal models, the vulnerability of the target patient group, and the high stakes of such an intensely public area of science. Our paper will present a detailed case study of iPS cell replacement therapy for Parkinson's disease to highlight these broader ethical and epistemological concerns. If we accept that iPS cell technology is fraught with challenges which go far beyond merely refuting the potentiality of the stem cell line, we conclude that iPS cell research should not replace, but proceed alongside embryonic and adult somatic stem cell research to promote cross-fertilisation of knowledge and better clinical outcomes\n|Keywords||stem cells Parkinson's disease research ethics first‐in‐human trials risk|\n|Categories||categorize this paper)|\nSetup an account with your affiliations in order to access resources via your University's proxy server\nConfigure custom proxy (use this if your affiliation does not provide a proxy)\n|Through your library|\nReferences found in this work BETA\nNo references found.\nCitations of this work BETA\nNo citations found.\nSimilar books and articles\nMark T. Brown (2009). Moral Complicity in Induced Pluripotent Stem Cell Research. Kennedy Institute of Ethics Journal 19 (1):pp. 1-22.\nMark Brown (2013). No Ethical Bypass of Moral Status in Stem Cell Research. Bioethics 27 (1):12-19.\nDavid Magnus (2010). Translating Stem Cell Research: Challenges at the Research Frontier. Journal of Law, Medicine and Ethics 38 (2):267-276.\nNikolaus Knoepffler (2004). Stem Cell Research: An Ethical Evaluation of Policy Options. Kennedy Institute of Ethics Journal 14 (1):55-74.\nZubin Master & G. K. D. Crozier (2012). The Ethics of Moral Compromise for Stem Cell Research Policy. Health Care Analysis 20 (1):50-65.\nLiza Dawson, Alison S. Bateman-House, Dawn Mueller Agnew, Hilary Bok, Dan W. Brock, Aravinda Chakravarti, Mark Greene, Patricia King, Stephen J. O'Brien, David H. Sachs, Kathryn E. Schill, Andrew Siegel & Davor Solter (2003). Safety Issues In Cell-Based Intervention Trials. Fertility and Sterility 80 (5):1077-1085.\nGlenn McGee & Arthur L. Caplan (1999). The Ethics and Politics of Small Sacrifices in Stem Cell Research. Kennedy Institute of Ethics Journal 9 (2):151-158.\nBernard Dickens, International Society for Stem Cell Research (ISSCR) Guidelines for the Conduct of Human Embryonic Stem Cell Research (December 2006).\nHossam E. Fadel (2012). Developments in Stem Cell Research and Therapeutic Cloning: Islamic Ethical Positions, a Review. Bioethics 26 (3):128-135.\nInsoo Hyun (2010). Allowing Innovative Stem Cell-Based Therapies Outside of Clinical Trials: Ethical and Policy Challenges. Journal of Law, Medicine and Ethics 38 (2):277-285.\nRebecca Dresser (2010). Stem Cell Research as Innovation: Expanding the Ethical and Policy Conversation. Journal of Law, Medicine and Ethics 38 (2):332-341.\nPhilip J. Nickel (2008). Ethical Issues in Human Embryonic Stem Cell Research. In Kristen Renwick Monroe, Ronald B. Miller & Jerome Tobis (eds.), Fundamentals of the Stem Cell Debate: The Scientific, Religious, Ethical & Political Issues. University of California Press.\nDemetrio Neri (2011). The Race Toward 'Ethically Universally Acceptable' Human Pluripotent (Embryonic-Like) Stem Cells: Only a Problem of Sources? Bioethics 25 (5):260-266.\nTamra Lysaght & Alastair V. Campbell (2013). Broadening the Scope of Debates Around Stem Cell Research. Bioethics 27 (5):251-256.\nBernard Lo & Lindsay Parham (2010). Resolving Ethical Issues in Stem Cell Clinical Trials: The Example of Parkinson Disease. Journal of Law, Medicine and Ethics 38 (2):257-266.\nAdded to index2011-07-05\nTotal downloads15 ( #126,527 of 1,692,217 )\nRecent downloads (6 months)3 ( #78,120 of 1,692,217 )\nHow can I increase my downloads?""]"	['<urn:uuid:6e251ceb-a8d1-4f41-a9d8-165b9f205064>', '<urn:uuid:0058a241-8e87-441b-8e5f-f65adfba6d4e>']	open-ended	direct	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	9	92	1651
79	boron fusion vs magnetic confinement fusion timescale durations how compare	The timescale durations of the two fusion processes are vastly different. Boron fusion reactions using laser technology last only about one-billionth of a second for the boron plasma, with the proton pulse lasting one-trillionth of a second. In contrast, magnetic confinement fusion in tokamaks can maintain plasma for much longer periods, while the pressures achieved are lower than in laser-based fusion.	['Boron and Nuclear Fusion: Is Boron the key to Unlocking Energy via Nuclear Fusion? For the past eighty years, scientists have been exploring ways to capture energy to provide an inexhaustible supply of quasi-free, green energy. Boron via laser technologies, may finally provide the key to moving beyond the laboratory to commercialisation.\nBoron and Nuclear Fusion\nProducing large-scale sustainable energy by nuclear fusion of hydrogen and other very light nuclei similar to the energy source of the galaxy is the holy grail for many scientists.\nSixty years of worldwide research for the ignition of the heavy hydrogen isotopes deuterium (D) and tritium (T) have provided near breakthroughs for ignition. However, DT fusion produces radioactive waste.\nHowever, one clean fusion process – without neutron production – is the fusion of hydrogen (H) with the boron isotope 11B11 (B11). Boron 11 plays a vital role in creating the conditions necessary to release energy in fusion experiments.\nBorates Today has chosen three research projects which show different approaches to using Boron in the quest for nuclear fusion.\nFusing Protons and Boron11 Nuclei using Lasers (2013)\nIn 2013, physicists at the CNRS laboratory succeeded in producing fusion at an accelerated rate in the laboratory led by Christine Labaune, research director of the CNRS Laboratory for the Use of Intense Lasers in France.\nPrevious laser experiments generated boron fusion by aiming the laser at a boron target to initiate the reaction, which provided low levels of energy but the infrastructure needed to provide energy meant that any commercalisation would require more energy to create the energy than that produced.\nLabaune claims the laser-generated proton beam in her setup produces a tenfold increase of boron fusion because protons and boron nuclei are smashed together directly.The two-laser system fuses protons and boron-11 nuclei.\nOne laser creates a short-lived plasma, or highly ionized gas of boron nuclei, by heating boron atoms; the other laser generates a beam of protons that smash into the boron nuclei, releasing slow-moving helium particles but no neutrons. The researchers describe their work in Nature Communications today.\nLasers have previously been used to crush a tiny pellet of two hydrogen isotopes — deuterium and tritium, containing two and three neutrons, respectively — to the point of initiating fusion. But in addition to producing neutron radiation, crushing the pellet evenly for the reaction requires a large array of lasers — there are nearly 200 at the world’s largest laser, the National Ignition Facility (NIF) at the Lawrence Livermore National Laboratory in California.\nIf the boron-fusion method were successfully scaled up, “we expect that it will reduce significantly the total laser energy” needed compared with that used at NIF, says Labaune.\nTiming was crucial for the success of the experiment, says study co-author Johann Rafelski, a theoretical physicist at the University of Arizona in Tucson.\nThe boron plasma generated by the laser lasts only about one-billionth of a second, and so the pulse of protons, which lasts one-trillionth of a second, must be precisely synchronized to slam into the boron target.\nThe proton beam is preceded by a beam of electrons, generated by the same laser, that pushes away electrons in the boron plasma, allowing the protons more of a chance to collide with the boron nuclei and initiate fusion.\nMourou’s study is not targeting ignition, the holy grail, but trying to make the study practical so it can be commercialized as a future power source.\nHB11 fusion for a reactor may be used instead of the DT option. Several key research studies summarised below show how scientists are making significant progress in accelerating the ability to harness power through hydrogen boron fusion.\nStudies into Laser Beam Ignition of Boron-hydrogen Fusion (2017)\nAccording to studies done and published here, the road map to clean energy using laser beam ignition of boron-hydrogen fusion follows this process: process is as follows:\nAcceleration of a plasma block with a laser beam with the power and time duration of the order of 10 petawatts and one picosecond accordingly;\nA plasma confinement by a magnetic field of the order of a few kiloteslas created by a second laser beam with a pulse duration of a few nanoseconds (ns);\nThe highly increased fusion of HB11 relative to present DT fusion is possible due to the alphas avalanche created in this process;\nThe conversion of the output charged alpha particles directly to electricity;\nTo prove the above ideas, simulations in the lab show that 14 milligram HB11 can produce 300 kWh energy if all achieved results are combined for the design of an absolutely clean power reactor producing low-cost energy.\n2022 Generating Power with Boron and Giant Lasers\nHB11 Energy raises US$20m to develop laser fusion process using boron\nHB11 Energy, an Australian start-up, has completed ground-breaking laser experiments which can potentially provide limitless electrical power generation via nuclear fusion.\nSydney start-up HB11 Energy, which is launching its own $US20 million capital raise to help develop its fusion process, is working with Japanese researchers in Japan who give access to the petawatt-class laser needed for fusion experiments. There are only a few lasers available right now which can provide this level of energy needed to carry out the experiment.\nHB11 Energy’s approach uses the laser beam to smash hydrogen atoms into boron, which produced a nuclear fusion reaction. The resulting energy released, albeit in small amounts today, may be stabilised and converted to electricity.\nWhat’s more, this is a safer approach than current nuclear energy procedures. Traditional nuclear reactors, or fission reactors, split larger elements such as uranium, into smaller elements. This releases energy together with radiation and plutonium, which takes a very long to tie to degrade. None of these negatives are found in the fission process using laser with boron and hydrogen.\nIn the case of HB11, the smaller elements of hydrogen and an isotope of boron – boron-11 -, are smashed together under extreme pressure. The new element formed releases energy but any waste created has a very short afterlife. Also, there is no radiation released.\nAccording to an expert in the field who has been working on laser-ignited fusion since the 1960s, Professor Henirich Hora, (also involved with HB11 Energy), the positive charge is harvested to produce electricity and helium gas, which makes it an ideal technology for de-carbonising the electricity grid.\nUsing just 3.4kg of boron, the fusion process with hydrogen, when commercialised, has the potential to meet the power needs of an individual for their entire lifetime. The boron reserves of the world’s largest producer, Eti Maden, estimated to be over 1 billion tonnes, could power the planet for about 3,000 years.\nDespite the promise of a new source of safe and clean energy, research into fusion has been going on for decades and some scientists believe it is as far away from commercialisation as ever.\nDr Daniele Margarone, a laser acceleration physicist at the Queen’s University Belfast who collaborated with HB11 on its latest fusion experiment, described the results as a “big step forward, not a breakthrough”.\nResults of the experiment were published in January, in the peer-reviewed journal Applied Sciences.\n“For the first time, a large amount of proton-boron fusion reactions were demonstrated using a short pulse laser . . . that can be potentially scaled up at large laser fusion facilities using an approach known as fast ignition,” Dr Margarone said.\n“There was no similar experiment done earlier, so this new approach could open a new research field for future net energy production.”\nNet energy production, where the energy gathered from the fusion reaction is more than the energy required to ignite the reaction in the first place, is one of stumbling blocks for the fusion industry.\nCommercialising laser facilities which require petawatts of power can be as large as a football stadium, which in itself will require more power than that generated from the reaction\nBefore we can look at real progress to net energy gain, Dr McKenzie estimates HB11 needs to produce around 10,000 times more power from hydrogen-boron fusion than current experiments.\nPodcast - With new applications demanding boron’s use in decarbonisation and green energy, as well as the traditional applications related to agriculture feeds and commercial applications for glass and ceramic ware, boron is becoming increasingly valuable as a strategic mineral.\nThe world market for boron si facing a shortage due to rising demand. Boron is a strategic material globally, and demand is projected to grow significantly due to its use in multiple industries and applications, from advanced energy to decarbonization and food security.', 'Last week, politicians and leaders met in Glasgow for COP26 to iron out policies that might alleviate the climate crisis. Because the need for energy is fueling that crisis, it might be tempting to look at the burning sun and wonder whether we could do what it does—that is, nuclear fusion.\nScientists have been trying for decades to achieve fusion. Actually doing so, though, requires overcoming an immense number of logistical challenges. Although fusion is a science that moves slowly, scientists are inching ever closer to achieving the dream.\nThey’re splitting their efforts between two types of fusion reactors. One approach goes big and tries to trigger fusion in a room-size chamber. The other goes small and tries to do the same in a pin-size pellet. But both, ultimately, try to mimic what happens in the sun.\nOur star produces its immense heat and blinding light by fusing hydrogen atoms, which crunch together to create helium and an incredible amount of energy. That’s what fusion researchers ultimately want to do: If we could create even a pale shadow of a star on Earth, that would open the door to incredible amounts of clean energy.\nThese are the two most promising approaches.\nThe sun can fuse hydrogen atoms with ease because of the infernal conditions in its center. At temperatures of tens of millions of degrees, atoms overcome the electromagnetic forces naturally keeping them apart. They fuse. That reaction produces no greenhouse gases.\nAt such temperatures, the atoms are so hot that they lose their electrons and become a scalding soup of electrically charged particles, called plasma. By applying electric and magnetic fields, scientists can manipulate and stir this soup.\nCreating plasma on Earth is doable. But that’s only the first step. Next, physicists must compress the plasma into sufficiently high densities. One way to do this is to put the plasma in a harsh magnetic cage. That’s called magnetic confinement fusion.\nThe best-known vessel for this method is the tokamak: a doughnut-shaped chamber typically about as large as a medium-size room. The walls of the chamber are home to powerful magnets, which help corral the plasma until it reaches the densities high enough for fusion to launch.\n[Related: Humans just generated nuclear energy akin to a star]\nFusion’s long-sought goal is a threshold called “ignition,” when the reactor puts out more energy than required to start it—a necessary benchmark to make a fusion power plant viable. But even though magnetic confinement fusion has been around since the 1950s, so far, no such reactor has come close to that mark.\nBut that date, scientists hope, may be in sight. Under construction, nestled in the hills of the south of France, is the largest and most powerful tokamak the world has ever seen: the International Thermonuclear Experimental Reactor, or ITER. Its tokamak will be ten times roomier than today’s largest. Having been in the works for more than a decade, ITER hopes to begin operations in 2025. It’s been called the most expensive science experiment ever.\nPartway around the world from France, in August 2021, another sort of reaction was underway. At the National Ignition Facility (NIF) at Lawrence Livermore National Laboratory in California, scientists announced they’d conducted fusion at an efficiency so high it nearly reached ignition.\nThere is no tokamak in NIF’s successful experiment. Instead, NIF uses a type of reaction called inertial confinement fusion. This relies on taking a tiny pellet of hydrogen fuel, typically the size of a pinhead, and jolting it with powerful shockwaves. As those shockwaves wash over the pellet, they compress and broil the hydrogen inside to pressures and temperatures high enough for fusion to launch.\n[Related: We still don’t really know what’s inside the sun—but that could change very soon]\nPhysicists can create those shockwaves in multiple ways, but nearly all of them rely on aiming high-energy lasers at the pellet. Some facilities directly blast the pellet with those lasers. NIF, instead, converts the laser’s energy into X-rays, which in turn strike the pellet.\nThat takes more energy, but it also makes the setup less delicate and more manageable by giving scientists just a little more leeway to work with. “The process of getting one of these things to compress…is very finicky. Everything has to be really, extremely precise,” says Doug Larson, NIF director.\nNIF and its counterparts can create immense pressures, more than is possible in a tokamak—but the pressures in a tokamak last much longer.\n“On NIF, we can create pressures up to something like a hundred billion atmospheres—really, really, crazy extreme pressures—but these things exist in our target chamber for trillionths of a second,” says Larson.\nNIF is the only inertial confinement fusion facility of its size, although other such facilities, such as the Laser Mégajoule in Bordeaux, France, and the proposed Shenguang IV in China, may one day match it.\nA sunnier future for fusion?\nPhysicists say it’s an exciting time in the world of fusion. “Over the past year, a lot of understanding has kind of come together in a way that the progress has really accelerated” at NIF, says Larson.\nFusion reactors offer the promise of near-unlimited power–but they are far from an immediate remedy to the climate crisis. The August experiment at NIF relied on a laser that can pulse every few hours, for instance. But for an inertial confinement fusion power plant to be commercially viable, says Larson, that laser needs to fire every few seconds. NIF plans to upgrade that laser.\nAs for ITER, the giant tokamak under construction in France, it’s only an intermediate step toward making fusion power feasible. Its builders hope the knowledge from ITER will improve the next generation fusion reactors, called the Demonstration Fusion Power Plant, or DEMO. DEMO, they hope, might be what brings fusion to the people. Those power plants won’t start construction until the 2030s.']	['<urn:uuid:9a2756bb-81f5-4376-808e-d485a502966a>', '<urn:uuid:16df7f09-ea5a-439a-acfa-30e0694ca11b>']	factoid	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T19:47:23.148022	10	61	2402
80	What are the main differences between regional A2/AD capabilities as discussed in NATO context versus China's military modernization goals by 2020?	Regional A2/AD in the NATO context involves modern IADS with hundreds of miles range supplemented by long-range surface-to-surface weapons and non-traditional weapons like cyber-attacks, creating large 'bubbles' that current long-range weapons cannot effectively counter. In contrast, China's 2020 goals focus on developing a regional blue water navy with extended A2/AD capabilities specifically designed for denying access throughout China's periphery and approaches, including the ability to conduct marine interception operations and high-level noncombatant evacuation operations in the western Pacific and Indian Ocean.	['Joint Air & Space Power Conference 2016\nAre We as an Alliance Prepared to Operate in a Degraded Environment?\nBy Lieutenant General Joachim Wundrak, DEU AF\nSince we have been involved in Afghanistan, we have unintentionally let certain aspects of our overall mission capability degrade as we’ve focussed on developing those skillsets we needed to be successful in that environment. Specifically, I do not think NATO is training as hard as we used to for situations that will require us as Airmen to be flexible and innovative to counter the challenges of a near-peer adversary.\nWhen the JAPCC team briefed me about the topic for the upcoming Air and Space Power Conference, I was very supportive of the proposal. If we do not address our ability to operate in degraded environments through equipment procurement and by training at the most realistic levels we can manage, we may find future conflicts do not end favourably for our way of life.\nYou could ask why are we addressing preparing for degraded environments instead of contested environments or why not look more specifically at the concept of preparing to defeat an Anti-Access/Area Denial (A2/AD) environment. While both these are essential conversations, from my perspective, A2/AD is a subset of contested, which itself is a subset of degraded. So, an environment could be degraded as a result of an adversary contesting that environment. That adversary could use an A2/AD strategy to contest the environment. The use of degraded as opposed to contested opens the discussion to situations potentially caused by a wide variety of factors, not just those caused by enemy actions. However, as a senior NATO air commander, I am personally focussed on the impact that A2/AD might have on our ability to operate and think we need to pay particular attention to the A2/AD challenge.\nThe term A2/AD itself is relatively recent, but the concept of controlling access to a battlespace or controlling an enemy’s freedom of movement within battlespaces is as old as human warfare. I won’t go into a long history of A2/AD here, but I do want to briefly address why today’s A2/AD is not what our predecessors dealt with. From an air perspective, the modern A2/AD area is built on the foundation of an Integrated Air Defence System, or IADS, and uses associated systems to extend their range. Modern IADS have ranges in the hundreds of miles and are supplemented by long-range surface-to-surface weapons and non-traditional weapons (such as cyber-attack) that can reach well beyond this distance. Thus, the adversary now has the ability to hold at risk our assets deep inside our territory without ever leaving their territory. This is Regional A2/AD, or A2/RD – an ability to deny access and control action across an entire region, which is a greater problem by several orders of magnitude.\nWhat concerns me about A2/RD is that we may not have superior technology available to us – stealth and precision weapons may not be enough, even when coupled with the most modern TTPs we can employ. The A2/RD ‘bubble’ is so large that our current long-range weapons are simply not long-range enough or are too easily defeated by modern systems. Of particular note, the threat is not static – it is persistently adapting to match our capabilities. We cannot remain complacent and assume that our capabilities will remain effective against the changing threat. Regional A2/RD is here to stay and, as those charged with the collective security of our Alliance, we must be ready to counter its effects.\nWhat is the way forward for the Alliance? How do we ensure we are ready to counter the effects of A2/RD? We must start now to prepare to operate in an environment that features A2/RD as the centrepiece of our adversary’s posture. Preparation, in my mind, has two key facets – personnel and equipment. Of course, these two are intrinsically linked – you must have the right equipment for the personnel to use and they must be capable of using that equipment to create the desired effects.\nAt the core of the problem is NATO’s failure to continue to develop and acquire the technology required to counter modern IADS and related systems. Of course, industry is primarily charged with the development of such systems, but we as NATO’s militaries have not asked them to give us solutions to this problem in recent years. Without a demand signal, industry will not invest in the necessary systems development. Without investment in research and development, the technologies we need will not be available to us. Of course, the question then becomes ‘What technologies will defeat the A2/RD systems?’ However, there are many other questions that must be addressed: Manned or unmanned platforms? Cyber or real-world weapons? Large numbers or highly technological? Or a combination of the two and, if so, in what ratio? How do we handle increasing levels of automation in weapons systems? How do we make systems resilient?\nThe other side of preparation is the personnel. I believe that we in NATO have the most capable and motivated airmen in the world. Given the proper equipment, resources, and sufficient training, they will not let us down. It goes beyond buying the enough of the right equipment – our personnel need to have the right kinds of training to be effective with that equipment. The questions that have to be answered in terms of\npersonnel are also significant: How many personnel and in what kinds of organisations? What ration of combat versus support forces? Whose responsibility is supporting deployed forces in the event of a conflict? How much live versus how much virtual training? How much training versus how much exercising? How do we make personnel resilient?\nOf course, some of the questions I have raised here are not specific to the A2/RD environment but they are all questions that must be answered if we are to succeed in countering the A2/RD threat. As NATO’s air leaders, we must debate these and other questions and determine what the right answers are to ensure our collective security while facing an A2/RD world. I don’t even know all the right questions, but I know if we don’t take the opportunity this 2016 JAPCC Conference brings to start this conversation, we may not be ready when the moment comes.', 'China’s Modernization of Its Naval and Air Power Capabilities\nAndrew S. Erickson\nThis chapter assesses China’s modernization of its naval and air power capabilities and draws implications for U.S. interests in the Asia-Pacific.\nAt the strategic and tactical levels, China’s naval and air forces can now achieve a variety of effects unattainable a decade or two ago. Although these capabilities are concentrated on operations in the near seas close to mainland China, with layers radiating outward, the PLA is also conducting increasing, albeit nonlethal, activities farther from China’s periphery, including in the Indian Ocean. Over the next decade and beyond, China’s naval and air power forces could assume a range of postures and trajectories. At a minimum, a greater diversity of out-of-area missions will depend on strengthening and broadening anti-access/area-denial (A2/AD) capabilities. While China is likely to develop and acquire the necessary hardware should it elect to expend sufficient resources, “software” will be harder to accrue.\nThe PLA will continue to focus on high-end A2/AD capabilities to secure China’s maritime periphery, along with its growing but low-intensity capabilities farther abroad.\nU.S. policymakers should seek ways to resist Chinese pressure in the near seas and cooperate with China in areas of mutual interest farther afield.\nThe U.S. must demonstrate the ability to persist amid A2/AD threats, in a manner that is convincing to China, allies, and the general public.\nThe U.S. must demonstrate a commitment to sustaining a properly resourced and continually effective presence in the Asia-Pacific. Rebalancing by redirecting resources from elsewhere will be essential and determine the success of these initiatives.\nThe People’s Republic of China (PRC) entered the second decade of the 21st century as a global economic and political power. The country is now in its third decade of rapid military modernization and boasts growing regional capabilities. Poverty in its vast interior, ethnic unrest in its western regions, and ongoing territorial and maritime disputes continue to necessitate that China prioritize military development and focus high-end military capabilities on its homeland and immediate periphery. Specifically, China’s naval and air power modernization has been concerned largely with developing a variant of regional anti-access/area-denial (A2/AD)—or “active defense” and “counter-intervention” from Beijing’s perspective—to deter Taiwan from declaring independence. An important part of this strategy is to demonstrate China’s ability to hold U.S. forces at risk should Washington elect to intervene in a cross-strait crisis or other disputes in the near seas.\nOperationally, asymmetric capabilities represent the core of the high-end development of the People’s Liberation Army (PLA). Based partially on “nonlinear, noncontact, and asymmetric” (sanfei) operations, they match key Chinese strengths against U.S. weaknesses. China systematically targets physics-based limitations in U.S., allied, and friendly military platforms, thereby seeking to place them on the wrong end of physics. By developing the world’s foremost sub-strategic missile force, for instance, the PLA\nexploits the fact that it is generally easier to attack with missiles than to defend against them. This affords China a defensive posture along interior lines and renders U.S. forces inherently vulnerable.\nAt the tactical level, China’s actual approach of employing “active strategic counterattacks on exterior lines” may be more nuanced and change more with specific circumstances than Western depictions of A2/AD imply.  For example, compared with the U.S. and some allied militaries, the PLA continues to face weaknesses in command, control, communications, computers, intelligence, surveillance, and reconnaissance (C4ISR). For high-priority missions on China’s periphery, however, the PLA can compensate for these limitations in complex real-time monitoring and coordination capability by massing forces selectively, maneuvering them specifically, and separating them in time and space. In peacetime, services may not be in perfect alignment and may have other tasks to perform.\nWith cross-strait relations stabilizing and China continuing to grow as a global stakeholder, the PLA Navy (PLAN) is likely to supplement this A2/AD strategy centered on Taiwan and the South China Sea, which China’s current naval platforms and weaponry largely support, with “new but limited requirements for protection of the sea lines of communication (SLOC) beyond China’s own waters, humanitarian assistance/disaster relief (HADR), and expanded naval diplomacy.”  As the world’s second-largest economy, China’s interests increasingly extend beyond its shores to resource-rich areas of the developing world and the trade- and energy-choked SLOCs of the Indian Ocean. The country’s manufacturing industries consume a tremendously high volume of imported resources, with 40% of oil arriving by sea.\nBy 2020, the PLA seeks a “regional [blue water] defensive and offensive-type” navy with extended A2/AD capabilities, limited expeditionary capabilities, and corresponding defensive and offensive air power.  Such a force would be able to deny access by holding opposing forces at risk throughout China’s periphery and the approaches to it (out to and beyond the second island chain and the full extent of the South China Sea). In addition, this force could conduct marine interception operations and high-level noncombatant evacuation operations (NEO), when necessary, in the western Pacific and Indian Ocean... [Free preview ends here. See purchase information above.]\n Anton Lee Wishik II, “An Anti-Access Approximation,” China Security 19 (2011): 37–48.\n Office of Naval Intelligence, The People’s Liberation Army Navy: A Modern Navy with Chinese Characteristics (Suitland, August 2009), 45.\n Nan Li, “The Evolution of China’s Naval Strategy and Capabilities: From ‘Near Coast’ and ‘Near Seas’ to ‘Far Seas,’ ” Asian Security 5, no. 2 (2009): 168.']	['<urn:uuid:cfcd90f6-f980-4fc4-a5fb-61f31db00ce0>', '<urn:uuid:8a416a0d-8133-4ed3-b78c-ea91c811fd1f>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T19:47:23.148022	21	81	1932
81	What conservation efforts exist for California's aquatic species, and how are citizen scientists involved in monitoring?	California employs a five-tiered approach to protect aquatic biodiversity, including strategies for species restoration and habitat protection. The conservation efforts extend to monitoring and management programs, particularly in the Sacramento-San Joaquin River system. Citizen scientists play a crucial role in these efforts, particularly through networks like PlateWatch and Green Crab Watch, which monitor target non-native species along the Pacific Coast. Citizens also participate in tracking Olympia oyster recruitment and studying the movement and predatory effects of introduced species, contributing to both research and conservation initiatives.	['Review of R. Pethiyagoda, Freshwater Fishes of Sri Lanka. The Wildlife Heritage Trust of Sri Lanka, Colombo. . Copeia . 1991, 1166-1177.(1991).\nReview of P. C. Trotter, Cutthroat: Native Trout of the West, and R. E. Greswell, Status and Management of Interior Stocks of Cutthroat Trout. Copeia . 811-812.(1989).\nReview of Freshwater Fishes of Mexico by Robert Rush Miller. (Miller, R. Rush, Ed.).Quarterly Review of Biology . 81,(2006).\nReview of E. K. Balon (Editor), Charrs, salmonid fishes of the Genus Salvelinus. Transactions of the American Fisheries Society . 110, 313-316.(1981).\nReview of D. L. Soltz and R. J. Naiman, The Natural History of Native Fishes in the Death Valley System. Natural History Museum of Los Angeles County Science Series 30. . Environmental Biology of Fishes . 5(1), 96.(1980).\nReview of Community ecology of stream fishes: concepts, approaches, and techniques, edited by K.B. Gido and D. A. Jackson. Quarterly Review of Biology . 86(339),(2011).\nReview of Candiru: life and legend of bloodsucking catfishes by Stephen Spotte. Fisheries . 28(1),(2003).\nReview of C. H. Hocutt and E. O. Wiley, The Zoogeography of North American Freshwater Fishes. Aquaculture . 62, 171-172.(1987).\nReview of: Analysis and interpretation of freshwater fisheries data. (Guy, CS., & Brown ML., Ed.).Quarterly Review of Biology. 83, 203-204.(2008).\nReview of An entirely synthetic fish: how rainbow trout beguiled America and overran the world by Anders Halverson. Quarterly Review of Biology . 87(168),(2012).\nRestoring native fish assemblages to a regulated California stream using the natural flow regime concept. Ecological Applications. 22(5), 10.(2012).\nRestoring aquatic ecosystems is a matter of values. California Agriculture. 54(2), 16–25.(2000).\nResponses of Fish Populations in the North Fork of the Feather River, California, to Treatments with Fish Toxicants. North American Journal of Fisheries Management. 3(1), 48-60.(1983).\nResource partitioning among the fishes of rainforest streams in Sri Lanka. Journal of Zoology. 202(2), 195–223.(1984).\nRegional Applications of an Index of Biotic Integrity for Use in Water Resource Management. Fisheries. 13(5), 12-20.(1988).\nRecent changes in the fish fauna of the San Joaquin River system. Cal-Neva Wildlife. 1973, 60–63.(1973).\nThe Rebirth of California Fish & Game Code Section 5937: Water for Fish. UC Davis Law Review. 45, 105.(2012).\nRapid decline of California’s native inland fishes: A status assessment. Biological Conservation. 144(10),(2011).\nRainforest fishes of Sri Lanka. Loris . 15, 341-344.(1981).\nQuantifying activated floodplains on a lowland regulated river: its application to floodplain restoration in the Sacramento Valley. San Francisco Estuary and Watershed Science. 7(1),(2009).\nProtection of Aquatic Biodiversity in California: A Five-tiered Approach. Fisheries. 19(2), 6-18.(1994).\nProtecting migration routes. Issues in Science and Technology. 24 (summer)(4),(2008).\nProjecting Cumulative Benefits of Multiple River Restoration Projects: An Example from the Sacramento-San Joaquin River System in California. Environmental Management. 42(6), 933-945.(2008).\nA programmable information system for management and analysis of aquatic species range data in California. Environmental Modelling & Software. 53, 13 - 26.(2014).\nPredicting Invasion Success: Freshwater Fishes in California as a Model. BioScience. 56(6), 515-524.(2006).\nPotential factors affecting survival differ by run-timing and location: linear mixed-effects models of Pacific salmonids (Oncorhynchus spp.) in the Klamath River, California.(2014).\nPotential Factors Affecting Survival Differ by Run-Timing and Location: Linear Mixed-Effects Models of Pacific Salmonids (Oncorhynchus spp.) in the Klamath River, California. PLOS One. 9(5),(2014).\nPopulations of peritrichs on the pond snail, Physa gyrina Say. Journal of the Minnesota Academy of Science . 31, 125-130.(1964).\nPink salmon (Oncorhynchus gorbuscha) in the Salinas River, California: a new record and historical perspectives. California Fish and Game. 99(1), 55-59.(2013).\nPhysical controls on the distribution of the submersed aquatic weed Egeria densa in the Sacramento-San Joaquin Delta and implcations for habitat.. San Francisco Estuary and Watershed Science. 14(1),(2016).\nPersistence and Structure of the Fish Assemblage in a Small California Stream. Ecology. 66(1), 1-13.(1985).\nPatterns of Freshwater Species Richness, Endemism, and Vulnerability in California. PLoS ONE. 10(7),(2015).\nPatterns In The Use Of A Restored California Floodplain By Native And Alien Fishes. San Francisco Estuary and Watershed Science. 5(3),(2007).\nPatterns in distribution and abundance of a noncoevolved assemblage of estuarine fishes in California. Fishery Bulletin. 84, 105-117.(1986).\nOccurrence of king (chinook) salmon in the Kings River, Fresno County. California Fish and Game . 56, 314-315.(1970).\nNuisance essay: good report but should go much farther. Fisheries. 19, 22-23.(1994).\nNovel Aquatic Ecosystems: The New Reality For Streams In California And Other Mediterranean Climate Regions. River Research and Applications.(2013).\nNon-native fish in the fresh waters of Portugal, Azores and Madeira Islands: a growing threat to aquatic biodiversity. Fisheries Management and Ecology. 16(4), 255–264.(2009).\nNative and Alien Fishes in a California Estuarine Marsh: Twenty-One Years of Changing Assemblages. Transactions of the American Fisheries Society. 131(5), 797-816.(2002).\nA multidisciplinary approach to vector ecology at Clear Lake, California. Bulletin of the Society of Vector Ecologists . 2, 21-31.(1975).\nMorphometric Analysis of Tule Perch (Hysterocarpus traski) Populations in Three Isolated Drainages. Copeia. 1981(2), 305-311.(1981).\nMore on Nile perch. SFI Bulletin . 281, 7.(1977).\nMontane meadows in the Sierra Nevada: comparing terrestrial and aquatic assessment methods. Environmental Monitoring and Assessment. 184(11), 6967-6986.(2012).\nMolecular Evidence for Multiple Paternity in a Population of the Viviparous Tule Perch Hysterocarpus traski. Journal of Heredity. 104(2), 217-222.(2012).\nMolecular evidence for multiple paternity in a population of the viviparous tule perch, Hysterocarpus traski. Journal of Heredity. 104(2),(2013).\nMississippi Silversides and Logperch In Sacramento San-Joaquin River System. California Fish and Game . 60, 144-149.(1974).\nMicrohabitat Use by an Assemblage of California Stream Fishes: Developing Criteria for Instream Flow Determinations. Transactions of the American Fisheries Society. 114(5), 695-704.(1985).\nMaximizing the ecological contribution of conservation banks. Wildlife Society Bulletin. 38(2), 377–385.(2014).', 'Tiburon Laboratory: Invasions and Community Ecology on the US West Coast\nOur core mission is to track the pulse of biotic change in the estuaries and coastlines along the Pacific Coast of North America. We identify non-native species in San Francisco Bay and surrounding coastlines, from Alaska to Panama, learn how their life cycles work in these new environs, and determine what effects they might be having on resident communities. We seek, in short, to make the consequences of moving species to strange new oceans both known and predictable, including how these species will interact with changing climate regimes.\nSince 2000, our researchers have maintained a laboratory on San Francisco Bay, at San Francisco State University’s marine research and teaching facility, the Estuary & Ocean Science Center (EOS). A 2013 Memorandum of Understanding between the Smithsonian and San Francisco State University expanded our relationship and signified our joint commitment to understand and protect the marine environment. With our colleagues at RTC and the San Francisco Bay National Estuarine Research Reserve, we are excited to be part of a community of researchers engaged in working out the effects of global change, invasions, and many other pressing issues in truly complementary fashion.\nWe have undertaken intensive, long-term studies of marine and estuarine communities across several habitats in California and especially San Francisco Bay, including hard substrates (fouling), soft sediment benthos, and plankton. We have used settlement panels to study fouling community development and invasion impacts since 2000 as part of an international effort to understand and document patterns of the species distributions and invasion in marine coastal waters. Recent studies have paired traditional morphological approaches to species identification with genetic approaches (with our collaborator, Dr. Jonathan Geller) to get a more complete picture of the species in California bays and near-shore environments.\nVessels of all kinds move species within and between bays. Our scientific divers conduct in-water hull surveys of boats and ships in West Coast ports and beyond to assess the role of vessel biofouling in moving organisms from port to port. Read more about our work on cargo and cruise ships in the Smithsonian Magazine.\nWe have studied ways to make eradication of targeted non-native pest species more efficient and effective, and have undertaken numerous eradication efforts, including the Japanese Mudsnail (Batillaria attramentaria) and the colonial tunicate Didemnum vexillum. Most recently, we worked with UC Davis and NOAA to monitor and eradicate a local population of non-native European Green Crabs (Carcinus maenas) near Stinson Beach, CA, and have investigated the possibility of functional eradication of the Atlantic oyster drill (Urosalpinx cinerea) as part of native Olympia oyster restoration efforts.\nNative Olympia Oysters\nWe collaborate with a number of agencies and research facilities to investigate native Olympia oyster (Ostrea lurida) demographics along California’s shorelines. Using a rigorous combination of field surveys, laboratory experiments, and population modeling, we examine the influence of environmental factors, climate change, and non-native species on the distribution and abundance of native oysters. We are also investigating oyster enhancement and restoration methods in San Francisco Bay and Elkhorn Slough.\nCitizen scientists form an essential and dedicated component of our research efforts. We run a citizen science network (PlateWatch and Green Crab Watch) to monitor for target non-native species along the Pacific Coast from Alaska to California. Participants include scientists from local state and federal agencies, concerned citizens, school groups and native groups. Within San Francisco Bay, we run citizen science programs to track Olympia oyster recruitment and the movement and predatory effects of the introduced snail Urosalpinx cinerea. If you wish to participate in any SI Citizen Science program, please contact Allison Cawood for more information at firstname.lastname@example.org.\nWe mentor interns and students in a wide range of capacities, and we usually host several undergraduate interns each summer. Our interns come from STAR and REU programs at San Francisco State University as well as the Smithsonian Internship program, which includes an REU program and numerous other opportunities. Each student participates in major lab projects as well as designs and executes an individual research project. Please see each program for their requirements and how to apply.\nJimenez, H., Keppel, E., Chang, A.L. and Ruiz, G.M. (2018). Erratum to: Invasions in Marine Communities: Contrasting Species Richness and Community Composition Across Habitats and Salinity. Estuaries and Coasts, 41 (2) , 611. http://dx.doi.org/10.1007/s12237-017-0315-1\nJimenez, H., Keppel, E., Chang, A.L. and Ruiz, G.M. (2018). Invasions in Marine Communities: Contrasting Species Richness and Community Composition Across Habitats and Salinity. Estuaries and Coasts, 41 (2) , 484-494. http://dx.doi.org/10.1007/s12237-017-0292-4\nNewcomer, Katherine A., Marraffini, Michelle L., and Chang, Andrew L. (2018) Distribution patterns of an introduced encrusting bryozoan, Conopeum chesapeakensis (Banta, Perez and Santagata, 1995) in an estuarine environment in upper San Francisco Bay. Journal of Experimental Marine Biology and Ecology 504: 20–31. DOI: 10.1016/j.jembe.2018.04.001\nZabin, C. J., Davidson, I.C., Holzer, K.K., Smith, G., Ashton, G.V., Tamburri, M.N., and Ruiz, G.M. 2018. How will vessels be inspected to meet emerging biofouling regulations for the prevention of marine invasions? Management of Biological Invasions (9). (Online: June, 18 2018)\nChang, Andrew.L., Brown, Christopher W., Crooks, Jeffrey A., Ruiz, Gregory M. (2017). Dry and wet periods drive rapid shifts in community assembly in an estuarine ecosystem. Global Change Biology. DOI: 10.1111/gcb.13972\nMarraffini, Michelle L., Brown, Christopher W., Ashton, Gail, Chang, Andrew L., and Ruiz, Gregory M. (2017) Do settlement plates effectively sample established fouling communities for non-indigenous species? Management of Biological Invasions.\nJimenez, Haizea, Keppel, Erica, Chang Andrew L., and Ruiz, Gregory M. (2017) Invasions in marine communities: contrasting patterns of species richness and community composition across habitats and salinity. Estuaries and Coasts. http://dx.doi.org/10.1007/s12237-017-0292-4\nTracy, Brianna, Larson, Kristen, Ashton, Gail, Lambert, Gretchen, Chang, Andrew L., and Ruiz, Gregory M. (2017) Northward range expansion of three non-native ascidians on the west coast of North America. BioInvasions Records. http://dx.doi.org/10.3391/bir.2017.6.3.04\nCheng, Brian S., Chang, Andrew L., Deck, Anna and Ferner, Matthew C. (2016). Atmospheric rivers and the mass mortality of wild oysters: insight into an extreme future? Proceedings of the Royal Society B: Biological Sciences, 283 (1844) http://dx.doi.org/10.1098/rspb.2016.1462\nChang, Andrew L., Deck, Anna K., Sullivan, Lindsay J., Morgan, Steven G., and Ferner, Matthew C. (2016) Upstream – downstream shifts in a recruitment hotspot of the native Olympia oyster in San Francisco Bay during wet and dry years. Estuaries and Coasts. http://dx.doi.org/10.1007/s12237-016-0182-1\nCrooks, Jeffrey A., Chang, Andrew L. and Ruiz, Gregory M. (2016). Decoupling the response of an estuarine shrimp to architectural components of habitat structure. PeerJ, 4 http://dx.doi.org/10.7717/peerj.2244\nJimenez, Haizea, and Ruiz, Gregory M. (2016). Contribution of non-native species to soft-sediment marine community structure of San Francisco Bay, California. Biological Invasions. http://dx.doi.org/10.1007/s10530-016-1147-9\nWasson, Kerstin, Hughes, Brent B., Berriman, John S., Chang, Andrew L., Deck, Anna K., Dinnel, Paul A., Endris, Charlie, Espinoza, Michael, Dudas, Sarah, Ferner, Matthew C., Grosholz, Edwin D., Kimbro, David, Ruesink, Jennifer L., Trimble, Alan C., Vander Schaaf, Dick, Zabin, Chela J. and Zacherl, Danielle C. (2016). Coast-wide recruitment dynamics of Olympia oysters reveal limited synchrony and multiple predictors of failure. Ecology, 97 (12) , 3503-3516. http://dx.doi.org/10.1002/ecy.1602\nZabin, Chela J., Wasson, Kerstin and Fork, Susanne. (2016). Restoration of native oysters in a highly invaded estuary. Biological Conservation, 202, 78-87. http://dx.doi.org/10.1016/j.biocon.2016.08.026\nAshton, Gail V., Davidson, Ian C., Geller, Jonathan, and Ruiz, Gregory M. (2016). Disentangling the biogeography of ship biofouling: barnacles in the Northeast Pacific. Global Ecology and Biogeography, 25, 739–750. http://dx.doi.org/10.1111/geb.12450\n2015 and older\nMcCann, Linda, Keith, Inti., Carlton, James T., Ruiz, Gregory M., Dawson, Terence P. and Collins, Ken. (2015). First record of the non-native bryozoan Amathia (Zoobotryon) verticillata (delle Chiaje, 1822) (Ctenostomata) in the Galapagos Islands. BioInvasions Records, 4 (4) , 255-260.http://dx.doi.org/10.3391/bir.2015.4.4.04\nAshton, Gail, Davidson, Ian C. and Ruiz, Gregory M. (2014). Transient small boats as a long-distance coastal vector for dispersal of biofouling organisms. Estuaries and Coasts, 37 (6) , 1572-1581.http://dx.doi.org/10.1007/s12237-014-9782-9\nCanning-Clode, João, Fofonoff, Paul, McCann, Linda, Carlton, James T. and Ruiz, Gregory M. (2013). Marine invasions on a subtropical island: Fouling studies and new records in a recent marina on Madeira Island (Eastern Atlantic Ocean). Aquatic Invasions, 8 , 1-10.\nMcCann, Linda D., Holzer, Kimberly K., Davidson, Ian C., Ashton, Gail V., Chapman, Marnie D. and Ruiz, Gregory M. (2013). Promoting invasive species control and eradication in the sea: Options for managing the tunicate invader Didemnum vexillum in Sitka, Alaska. Marine pollution bulletin, 77 (1-2) , 165-171. http://dx.doi.org/10.1016/j.marpolbul.2013.10.011']	['<urn:uuid:0d13a609-0a1b-4eea-b40e-96ad41ed45d6>', '<urn:uuid:7d4cc509-41e9-4f77-85aa-f98d711bcf74>']	open-ended	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:47:23.148022	16	85	2295
82	How does wind affect ocean water movement and mixing?	Wind affects ocean water movement and mixing in several ways. In the surface layer (up to 100m deep), wind drives surface currents and produces a well-mixed layer of uniform density called the mixed layer. The wind-driven surface currents are strongest in this upper layer, though some currents like the Gulf Stream can be strong hundreds of meters deep. Strong storm winds can temporarily disturb the ocean's stable stratification by bringing colder water to the surface. The mixing ability of wind-driven turbulence depends on the Richardson number - when this number falls below 1, turbulent diffusion becomes important and can lead to vigorous mixing. Once winds weaken, the ocean returns to its stable layered structure.	"['Advection-Diffusion Equations and Turbulence\nFile last modified 2 November 1998\n15.4 The Numbers Game\nBefore going on to talk about specific oceanographic results, we will digress to discuss various NUMBERS that are of interest in oceanography. Numbers are, by definition, dimensionless quantities which somehow embody physically important characteristics of the systems being studied. More often than not, they indicate the relative importance of various processes, and appear as the ratio of either terms or time scales. The reason for describing your system in terms of these numbers is that often the character of model output tends to depend more on the ratio of terms rather than their absolute value. So, for example, when faced with modeling the behavior of a system over a wide range of velocities and diffusivities, we may find that we need only perform experiments over a range of the ratio of two terms (the Peclet number) rather than doing every conceivable velocity and diffusivity combination. We will list some of the more important, along with typical values and what they mean.\n15.4.1 The Reynolds Number\nThe Reynolds number, often referred to as Re is a measure of the relative importance of intertial to viscous terms. The higher the Reynolds number, the more likely to be turbulent the flow, and the lower the number, the more likely the flow will be laminar. The Reynolds number is defined as\nwhere the denominator is the kinematic viscosity (the greek letter ""nu""). This may also be regarded as the ratio of the (molecular) diffusive time scale to the advective time scale. For most fluids (the atmosphere, oceans, etc.) the Reynolds number is of the order of several thousand or greater. Thus most fluids are in a state of turbulent flow.\n15.4.2 The Peclet Number\nThe Peclet number (Pe) is a measure of the relative importance of advection to diffusion. Diffusion here is turbulent diffusion. The higher the Peclet number, the more important is advection. It is given by\nand can be arrived at by non-dimensionalizing the advection-diffusion equation. This number may also be thought of as the ratio between the diffusive to the advective time scales. A typical open ocean is characterized by velocities of order .01 m/s, lengths of order 2-3000 km (the size of ocean gyres), and turbulent diffusivities of order 1000 m2/s. This gives a Peclet number of order 20-30.\nThe trick, though, is in the seemingly arbitrary choice of the length scale L. Clearly, the bigger L becomes, the higher the Peclet number becomes. This is equivalent to saying that given enough time, advection always wins out over diffusion. This is because while the displacement of a particle increases linearly with time with advection, it only increases as the square root of time with diffusion. This can be seen by thinking about diffusion as a random walk experiment (which is what it mathematically is). But it does boil down to this implicit ambiguity that the Peclet number (and hence the apparent relative role of advection and diffusion) depends on the spatial scale of the system being studied.\nRadioactive tracers, with their built in decay constants can define their own space scales. This can also be seen by non-dimensionalizing the advective-diffusive-decay equations. The characteristic length scale is the velocity divided by the decay constant, or quite simply the distance a fluid parcel would go before the tracer would be reduced to 1/e of its value by decay. Thus the radiotracer Peclet number would be defined as\nNow for a given fluid flow, the length scale will be different for differing radiotracers, so that diffusion and mixing will be more important for one tracer than for another. Consider, for example 7Be, which has a half life of 53.4 days, and thus has a decay probability of 1.51x10-7 s-1. For the subtropical North Atlantic, with velocities of order .01 m/s, and horizontal turbulent diffusivities of order 1000 m2/s, this gives a Peclet number of order 0.7, which says that diffusion and mixing are as/more important than advection. Consider the same situation, however, with tritium (half-life 12.45 years). The same calculation yields a Peclet number of order 50-60, which says that tritium is more affected by advection than diffusion. Now let\'s turn the problem around and say that if you were interested in studying the effects of diffusion, you\'d be more interested in using 7Be than tritium.\n15.4.3 The Richardson Numbers\nThe ocean is in general stably stratified. That is, heavy water is overlain by lighter water. If the reverse were true, then the water column would be gravitationally unstable, and vertical motions (convection) would result that would erase the condition. Now for turbulent displacement to occur vertically in a stratified water column, the fluid particles must overcome the vertical density (buoyancy) gradient. (We\'ll discuss this more in the next section). Thus one would expect that the ability of the water column to resist this vertical turbulence will be related to the vertical density gradient (also referred to as the rate of buoyancy production). Now one model of the origin of the energy required to produce turbulent motions is the vertical shear in the horizontal velocity. That is, if the horizontal velocity is changing with depth, the different layers traveling at different speeds tend to ""rub"" against one another, and there must be an overall dissipation occurring to maintain the velocity gradient. This dissipation scales as the square of the velocity gradient, so that defines a Richardson Flux number as\nwhere is the thermal conductivity, is the kinematic viscosity, g is the gravitational constant and u is the horizontal velocity. This is the ratio of buoyancy production to turbulent kinetic energy. Another important quantity is the gradient Richardson number, which is defined by\nIn situations where Rg decreases much below 1, turbulent diffusion becomes important, and can grow to a point where the system mixes vigorously. Laboratory experiments indicate that a critical Rg of 0.25 is a good approximation for most systems.\n15.4.4 Various other Numbers\nVarious other numbers crop up in different circumstances. Ones that you may hear of are the Prandtl number and the Schmidt number. The former is the ratio of viscosity to thermal diffusion\nand the latter is the ratio of viscosity to molecular diffusion\nThese are often used to compare model or flux calculations between\ndifferent situations or chemical species.\nGoTo Next Section', ""Except at high latitudes, the ocean is divided into three horizontal depth zones based on density: the mixed layer, pycnocline, and deep layer. At high latitudes, the pycnocline and mixed layer are absent.\nWind-driven surface currents are restricted mostly to the ocean's uppermost 100 m (300 ft) layer or so depending upon the depth of the pycnocline. This is because the thickness of the surface mixed layer is typically 100 m or less. The pycnocline acts as a porous boundary that allows some kinetic energy to penetrate into deep water. The strongest currents generally occur in the ocean's surface layer although some surface currents such as boundary currents like the Gulf Stream (discussed later) can be relatively strong to depths of several hundred meters. Surface currents are changeable, continually responding to variations in the wind, precipitation, and heating or cooling. Stirring of surface waters by the wind produces a well-mixed layer of uniform or nearly uniform density. For this reason, the ocean surface is called the mixed layer. We know most about the mixed layer because ships, aircraft, and Earth-orbiting satellites can readily monitor it.\nThe pycnocline, situated between the mixed layer and the deep layer, is where water density increases rapidly with depth because of changes in temperature and/or salinity. Recall that cold water is denser than warm water and salty water is denser than fresh water. Where a decline in temperature with depth is responsible for the increase in density with depth, the pycnocline is also a thermocline. On the other hand, if an increase in salinity is responsible for the increase in density with depth, the pycnocline is also a halocline. Typically, the pycnocline extends to a depth of 500 to 1000 m (1600 to 3300 ft). (However, in middle latitudes seasonal pycnoclines may develop within the mixed layer.) The dark, cold deep layer below the pycnocline accounts for most of the ocean's mass. Within the deep layer, density increases gradually with depth and water moves slowly; in only a few locations (usually near the bottom) are water movements fast enough to be considered currents.\nThe ocean's three-layer structure is an example of how gravity separates a fluid into layers such that the density of each layer is less than the density of the layer below it. More dense fluids sink and less dense fluids rise. The ocean's pycnocline is very stable thus suppressing mixing between the mixed layer and deep layer; that is, the pycnocline acts as a barrier to vertical motion within the ocean. The concept of stability is useful in understanding this property of the pycnocline.\nStability as used here refers to vertical motions of ocean water. A system is described as stable if it tends to persist in its original state without changing. Following a disturbance (i.e., vertical motion), a stable system returns to its initial state or condition. As noted above, the usual stable state of the ocean features a layer of water that is warmest near its interface with the atmosphere (the mixed layer) and the mixed layer overlies water that becomes denser with increasing depth (the pycnocline). Strong storm winds may temporarily disturb this stable stratification bringing colder than usual water to the surface. Once the wind slackens, however, the original layered structure is soon restored.\nAdapted from DataStreme Ocean and""]"	['<urn:uuid:c347db1d-fd68-4060-acff-fe994ed925ad>', '<urn:uuid:1aa2d06e-e6bd-4b0b-b0ef-68c346790eaa>']	open-ended	direct	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T19:47:23.148022	9	114	1609
83	how blackbody radiation helps check temperature tools petrochemical plant	Blackbody radiation serves as the standard for calibrating temperature measurement tools in petrochemical plants. MSL's high-accuracy standard radiation thermometer is calibrated against blackbody furnaces for industrially-relevant temperatures. This is based on the principle that a blackbody is a perfect absorber and emitter of radiation, following the Stefan-Boltzmann Law (Eb=σT4). No other surface can emit or absorb more energy than a blackbody, making it the ideal reference standard for calibration of measurement tools used by plant operators.	['Taking the temperature of industry\nAccurate temperature measurement is vital in countless industries; everything from pharmaceuticals and food production to materials manufacturing and aviation.\nHelping NZ’s petrochemical industry stay ahead\nFor more than twenty years, New Zealand’s petrochemical industry has looked to MSL for its temperature expertise. Under the leadership of Distinguished Scientist, Dr Peter Saunders, the lab has provided consultancy, advice and training to companies like Methanex and Refining NZ, ensuring the safe and efficient operation of their plants.\nHeadquartered in Canada, with sites scattered across the globe, Methanex is the world’s largest producer of methanol. At their site in Taranaki, the company converts natural gas into hydrogen, and then to methanol. Doing this requires furnaces – some as tall as a five-storey building – that can heat the gas to temperatures in excess of 900 °C. At the heart of a furnace’s operation are its 680 process tubes. Made from exotic metals that can withstand extreme heat, each tube costs upwards of $30,000 to replace. “Companies like Methanex want to get the maximum lifetime from their materials,” explains Dr Saunders. “But as temperatures increase, even these materials will suffer. Being just 15 degrees too high will halve the tube’s lifetime.” So it is critical to know exactly what temperature each and every tube operates at.\nMeasuring furnace temperatures to the required level of accuracy is not straightforward. Access is via a number of small ‘portholes’, which can be opened only for short periods of time. MSL scientists use radiation thermometers, devices that look a bit like a camera. As the name suggests, these thermometers can determine the temperature of an object from the thermal (infrared) radiation it emits. But while they might be fast and robust, radiation thermometers are prone to a number of errors, including the radiation that reflects from nearby surfaces. “There’s the infrared signal from the object you’re interested in and a reflective component from other hot sources around it,” says Saunders. “The thermometer can’t distinguish between them, so it tends to give values that are too high.”\nTo tackle this, Saunders developed computer models of the thermal environment inside industrial-scale furnaces. These models could be altered to reflect the geometry and demands of each plant, allowing operators to take a small number of measurements and then calculate the true temperature of each tube.\nThis innovation caught the attention of Quest Integrity, a global company that specialises in advanced inspection and engineering assessment services for the energy industry. “The ability to measure accurate temperatures in a refinery is critical for process reasons, and also for defining the integrity of assets and furnaces,” says Dr Andy Saunders-Tack, chief engineer at Quest Integrity NZ. Quest and MSL now have a licensing agreement, bringing MSL’s work to a world-wide audience. “Peter has developed the algorithms to allow us to develop accurate temperatures from the thermometry applied. We turned that into a software platform to allow us to sell to customers,” says Saunders-Tack.\nThe ability to measure temperature in challenging conditions, combined with a deep knowledge of thermal physics has led Saunders to valuable insights for his industrial partners. If you have a gas heater at home, you might know that burning natural gas produces water vapour. This is also true in petrochemical furnaces, and if not accounted for, it can affect temperature measurements. Operators tune their furnaces by scanning thermometers from tube to tube, making sure that the readings are all fairly uniform. “The issue is that when operators measure the tubes at the centre of the furnace, they may not account for the large error from water vapour,” says Saunders. “It means that while the tube temperatures look to be the same from the edge to the centre, those central tubes are actually much colder.” For Methanex, the temperature difference was significant, so they took MSL’s advice to increase the tube temperature in that region by 15 °C. The result? A profit boost of $1 million a year thanks to increased productivity that didn’t compromise the lifetime of the tubes.\nMSL also supports the measurement tools that plant operators use on-site. We do this via our high-accuracy standard radiation thermometer, calibrated against blackbody furnaces representing a range of industrially-relevant temperatures. This facility has become the go-to option for a large number of Quest Integrity USA’s customers. “Labs in the US could do the calibrations, but the truth is that these industrial customers trust us. They know that we intimately understand their application, so we can give them a fit-for-purpose calibration plus advice based on years of experience,” says Saunders.', 'All objects above 0 K (-273.15°C) emit radiation at the surface, due to the internal energy contained within the object. Radiation is continuously being emitted from surfaces and being absorbed, transmitted, and reflected. Some surfaces emit or absorb, for instance, better than others. This is mainly due to a surface condition called emissivity, which is determined by color, finish, and intrinsic material properties. Emissivity is a ratio of how well a surface performs as an emitter of infrared radiation in comparison to what is commonly called a blackbody, or ideal radiator. A blackbody is described as follows:\n- A blackbody absorbs all incident radiation, regardless of wavelength and direction.\n- For a prescribed temperature and wavelength, no surface can emit more energy than a blackbody.\n- Although the radiation emitted by a blackbody is a function of wavelength and temperature, it is independent of direction. That is, the blackbody is a diffuse emitter .\nGenerally speaking, a blackbody is the perfect absorber and emitter. It is the standard we compare other radiating surfaces against. No other surface absorbs more energy or emits it than a blackbody. In addition, no radiation emitted from a blackbody is reflected from other sources.\nAccording to the Stefan-Boltzmann Law, the emissive power of a blackbody is Eb=σT4, where\nand the temperature is measured in Kelvin. Accordingly, for a non-blackbody, the emissive power is Eb=εσT4, where ε is the emissivity of the surface being considered (0<ε<1), and ε = 1 for a blackbody. The chart below shows emissivities for common materials. Additional values can be found at its source: http://www.engineeringtoolbox.com/emissivity-coefficients-d_447.html.\nFor plant scientists, it might also be interesting to consider the emissivities documented for various plant leaves and diverse vegetation over different wavelengths (http://g.icess.ucsb.edu/modis/EMIS/html/leaf.html).\nIf emissivity isn’t taken into account, the surface temperature of an object may appear to be emitting more radiation, due to the addition of reflective radiation for instance, than it really is and thereby giving us an incorrect perception for the emissive power or temperature of that surface. This can be especially true for an IRR sensor that has been calibrated to a nearly standard blackbody surface. The calibration procedure for Apogee’s IRR sensors ( http://www.apogeeinstruments.com/content/SI-calibrationposter.pdf ) shows just that.\nThe question then is: how can I adapt for such a situation?\nAppropriate correction for target emissivity is required for accurate surface temperature measurements. However, the simple correction that is commonly made, dividing the measured temperature by the target emissivity, is incorrect because it does not account for reflected radiation. The radiation detected by an infrared radiometer includes two components: 1) the radiation directly emitted by the target surface, and 2) reflected radiation from the background. The second component is often neglected. The ratio of the two components in the radiation detected by the radiometer is weighted according to the emissivity (ε) of the target surface:\nWhich after substitution of the Stefan-Boltzmann Law and rearranging, we get:\nIn almost all situations, having a precise IRR sensor, such as Apogee’s ±0.2°C accurate IRR sensors won’t be enough. Emissivity correction is required to minimize error in your data. Whether you are using it in on a plant canopy or monitoring icy road conditions, emissivity correction will help you make better measurements.\n “Fundamentals of Heat and Mass Transfer” Incropera, F.P., Dewitt, D.P., et. al. 6th Ed. 2007, pg 736.\n “A Review of the Physics for Emissivity Correction of Infrared Temperature Measurements” URL: http://www.apogeeinstruments.com/content/SI-emissivitycorrection.pdf.']	['<urn:uuid:1398dafd-3e6a-453a-80b7-3dbacb8d7685>', '<urn:uuid:c56056dc-1286-47be-b66f-3f204d02bade>']	factoid	with-premise	long-search-query	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	9	76	1330
84	immediate versus delayed rewards psychology economics differences	Psychological research shows that people struggle with immediate versus delayed rewards because they tend to focus more on immediate consequences (like getting extra sleep) than later ones (like missing breakfast). However, economic research has challenged traditional assumptions about rational choice, demonstrating that people don't always make purely self-interested decisions. When faced with choices, individuals often exhibit 'limited rationality' and 'limited control' rather than pure economic rationality. This is evident in experiments where people choose to share money rather than keep it all, and in successful interventions like automatic pension enrollment that help people choose long-term benefits over immediate gratification. The key to better decisions often lies in reframing choices to highlight future consequences rather than relying solely on willpower.	"['When faced with a tempting choice, it can be hard to stop and think through the potential consequences, but new research suggests that framing the choice as a sequence of events can help us exercise patience by prompting us to imagine the future. The findings are published in Psychological Science, a journal of the Association for Psychological Science.\n""People often have difficulty forgoing immediate temptations, like hitting the snooze button on the alarm, for the sake of later benefits. One possible reason is that people tend to consider the immediate consequences of a particular action, like getting a few more minutes of sleep, more than the later ones, like not having time for breakfast,"" explains Adrianna Jenkins of the Helen Wills Neuroscience Institute and Haas School of Business at the University of California, Berkeley.\n""Past work has shown that a subtle change in how choices are framed can increase people\'s patience. We found evidence that this change affects patience by increasing imagination and its role in decision-making,"" she adds.\nA considerable amount of research has shown that people who are able to forego immediate temptations in lieu of future rewards experience all sorts of benefits later on, including greater physical, psychological, and financial well-being. But the typical explanation for this ability -- using willpower to tamp down our immediate desires -- does not always seem to underlie increases in patience. For example, research on framing effects indicates that people\'s ability to exercise patience can differ based on small differences on how current and future choices are presented, even without changes in willpower.\nJenkins and UC Berkeley colleague Ming Hsu wondered whether reframing decisions might increase people\'s reliance on a second possible route to patience: imagination.\nIn one experiment, Jenkins and Hsu presented 122 participants with a series of binary choices and the participants had to choose which option they preferred. In some cases, the options were framed as independent - for example, they could choose between receiving $15 tomorrow or $20 in 30 days. In other cases, however, the options were framed as sequential - that is, receive $15 tomorrow and $0 in 30 days versus receive $0 tomorrow and $20 in 30 days.\nReplicating past research, people were more likely to exercise patience when the options were framed as a sequence relative to when they were framed as independent. Importantly, the options were financially equivalent regardless of how they were framed.\nIn a second online experiment, 203 participants were randomly assigned to receive either an independently framed choice or a sequentially framed choice with higher stakes: $100 tomorrow or $120 in 30 days. This time, Jenkins and Hsu also measured participants\' imagination.\nIn the second experiment, people were more likely to imagine the potential outcomes of the options when choices were framed as sequences. This increased reliance on imagination was apparent in both participants\' self-reports and also in the notes they took about what they were thinking while making their decision.\nIn a third experiment, the researchers looked at brain imaging data from another group of participants making the same kinds of decisions. They found that when options were framed as a sequence, choosing the patient option was linked to activity in brain regions associated with imagination. When the options were framed as independent, choosing the patient option was more strongly linked with activity in brain areas associated with willpower.\nThese findings, the researchers say, provide evidence that reframing options as a sequence influenced the route by which participants exercised patience.\n""Our findings suggest that imagination and willpower represent dissociable routes to patience,"" says Jenkins. ""Willpower might enable people to override impatient impulses after they\'re formed, whereas imagining future consequences might affect the formation of the impulses themselves.""\nImagining future consequences may be a particularly useful strategy, the researchers note, when circumstances are less than ideal - say, when you\'re multitasking, distracted, stressed, or just tired - since is it possible for willpower to be compromised in these situations.\nSo when your alarm clock rings tomorrow and you don\'t feel like getting out of bed, try reframing your options. Instead of choosing between ""get out of bed now"" and ""sleep 20 more minutes,"" try thinking of your options more in terms of ""get out of bed now and have time for breakfast"" or ""sleep 20 more minutes and have no time for breakfast."" It may just help you throw off the covers and get ahead on your day.\nThis research was supported by National Institutes of Health Grant R01 MH098023 to M. Hsu.\nAll data and materials have been made publicly available via the Open Science Framework and the complete Open Practices Disclosure for this article is available online. This article has received badges for Open Data and Open Materials.\nFor more information about this study, please contact: Adrianna Jenkins at email@example.com.\nThe article abstract is available online: http://journals.\nThe APS journal Psychological Science is the highest ranked empirical journal in psychology. For a copy of the article ""Dissociable Contributions of Imagination and Willpower to the Malleability of Human Patience"" and access to other Psychological Science research findings, please contact Anna Mikulak at 202-293-9300 or firstname.lastname@example.org.', 'Richard H Thaler—the Charles R Walgreen distinguished service professor of behavioral science and economics at the University of Chicago Booth School of Business—who is engaged in researching behavioral economics and finance and the psychology of decision making has been awarded the 2017 Nobel Prize in Economics “for his contributions to behavioral economics” that built a “bridge between the economic and psychological analyses of individual decision-making”, paving the way for “the new and rapidly expanding field of behavioral economics.”\nReacting to the award of Nobel Prize, Thaler said that his major contribution to economics is “the recognition that economic agents are human, and economic models have to incorporate that [factor]”. And perhaps, as an acknowledgement of his argument about the sometimes-unreasonable behavior of humans, he jocularly said, “I intend to spend the prize money ‘as irrationally as possible’”.\nJokes apart, what Thaler meant when he said “economic agents are human” is: traditionally, economists assuming that each person makes totally rational choice in pursuit of their own self-interest, built elaborate theoretical and mathematical models to explain how markets work to efficiently allocate capital and set prices. But Thaler along with Daniel Kahneman and Amos Tversky challenging this underlying premise of the models, demonstrated how often individuals make illogical choices that sabotage their economic interests, that too, believing that they are totally rational. Citing Brexit as a classic example of behavioral economics in action, Thaler observed that British voters chose an economically irrational route while considering the options offered to them by the elites.\nStudying the underlying reasons for people to behave irrationally, he came up with three propositions: one, ‘limited rationality’; two, ‘social preferences’; and three, ‘limited control’. The concept of limited rationality is explained through ‘endowment effect’ where individuals value an item more when they own it rather than when they do not. To test this hypothesis, he gave coffee mugs at random to half of a group of test subjects asking them to sell them, if they wish, to the other mug-less half of the group. Rationality demands that people from both the groups must, on average, value them the same, and accordingly, half of the mugs should change the hands. Contrary to this expectation, the have-nots valued the mug less while the haves have valued it high and as a result few mugs have changed hands.\nMoving to the role of ‘social preferences’ in economic decisions, Thaler says that individuals do not necessarily make choices driven solely by selfish interests. For instance, in an experiment conducted by him, he asked a randomly selected student to divide a $20 bill between himself and another subject. Rarely had he noticed any student retaining the whole amount with himself, as pure rationality would suggest. Similarly, his studies revealed that people find practices like overcharging for a good under duress as unfair.\nAlong with Hersh Shefrin, Thaler developed the third idea, namely, ‘limited control’ that emanates from the conflict between two competing cognitive forces of an individual: ‘doing-self’ which is more concerned with short-term happiness and ‘planning-self’ that values long-term goals more. To resolve this dichotomy, he says that the planning-self perhaps offers fewer choices to the doing-self that lie in the immediately upcoming future. And this phenomenon runs contrary to the economic theory of rationality, for more the choices, the better would be the decision. It otherwise means, presenting people with a “choice architecture” which favors the planning-self than the doing-self can impact people’s behavior in a big way.\nIt is perhaps taking a cue from this phenomenon that Thaler came up with his now famous “nudging” theory. He co-wrote the global best-selling book, ‘Nudge: Improving Decisions about Health, Wealth and Happiness’ in 2008 with Cass Sunstein that nudged governments to explore his ‘nudging’ theory to achieve better outcomes in the financial behavior of people—of the society at large. For instance, we are all aware that we need to save more for our retired life. But under most of the hitherto operating schemes one has to voluntarily choose: you need to first decide to save, then decide how much to put aside, in which scheme and for how long? It is too much for most of the people and thus many in the US were found under-saving.\nThen came Thaler with a paper, “Save More Tomorrow: Using Behavioral Economics to Increase Employee Saving” jointly with Shlomo Benartzi of UCLA that revolutionized designing pension schemes. Under this new design, employees of the company are automatically enrolled in the plan, that they will contribute an initial percentage of their salary, that this contribution will increase each year by a certain percentage of their salary increase, and if the employees make no further choices, they will be assigned to a target date fund that is designed to automatically control asset allocation along the way towards assumed date of retirement of the respective employee. There is, of course, a provision for the employees to always opt out of any of these provisions. Intriguingly, thus default-enrolment proved to nudge employees to save for their retired life more effectively, for seldom anyone opted out. With the result, people in the automatic plans found saving more than those outside of them. And an obvious improvement is: the quality of life for future retirees.\nHe thus came up with the concept of improving “choice architecture”—an architecture that does a favor to their planning-self over their doing-self, so that it can result in better financial behavior. For instance, suppose a prospective client of credit card is provided with all the charges and fees to be paid under its usage right at the time of his choosing the card, would it not enable him to make a better choice rather than to struggle with loads of fine print later and crib at the decision already taken?\nWorking for over three decades in association with a group of behavioral economists around psychology of decision making, economics and sociology, Thaler has simply nudged even the hardnosed rationalists to look differently at how people think when it comes to money and the decision making thereof and importantly nudged rationalists to appreciate that there exists deviations in human behavior, for human agents act fallibly. And all this happily nudged him to pocket his much deserving Nobel Prize too.']"	['<urn:uuid:1c74f37e-2901-4a7e-9fd9-b9b7a1ccead2>', '<urn:uuid:c29ddd2e-9609-4a06-934c-0f839795a9f2>']	open-ended	with-premise	short-search-query	distant-from-document	comparison	novice	2025-05-12T19:47:23.148022	7	119	1899
85	Which watch offers more detailed weather information?	Both devices provide weather information but through different systems. The Montana 700i offers Active Weather forecasts through the Garmin Connect app using cellular connectivity, while the Fenix 6X Pro Solar uses ABC sensors including a barometer specifically to monitor weather conditions. The Montana 700i also offers detailed inReach weather forecast service via satellite subscription for updates directly to the handheld or paired devices.	"['Stay in touch and on track with Montana 700i. This rugged GPS handheld combines TopoActive and City Navigator mapping for routable on- or off-road navigation with global two-way messaging and interactive SOS via inReach satellite technology (active satellite subscription required; some jurisdictions regulate or prohibit the use of satellite communications devices). The 5” color touchscreen features a full QWERTY keyboard for messaging and a display that’s 50% larger than earlier models. Multi-GNSS support and sensors for the 3-axis compass and barometric altimeter put a world of outdoor navigation capability within your grasp. Plus, wireless pro-connectivity gives you direct-to-device access to BirdsEye Satellite Imagery downloads, location sharing, Connect IQ app support and more. Pair with a compatible smartphone to access Active Weather forecasts. Extend battery life for more than a week with the new ultralow-powered Expedition mode. For hunters and trainers, the IPX7-rated unit syncs with compatible Astro or Alpha series handhelds (sold separately) to help track sporting dogs in the field.\nGO INTO THE WILD WITH A SATELLITE CONNECTION\n• To keep adventurers in touch and on track, the Montana 700i handheld combines the flexibility of on- or off-road navigation with global messaging and interactive\n• SOS via inReach satellite technology\n• Big news. A full QWERTY keyboard lets you easily type messages.\n• Wander wider and still stay in touch — with inReach for messaging and 24/7 SOS.\n• Routable topographic maps? Check. Now you’re free to take it off-road or on.\n• Built like a tank. Meets MIL-STD 810 for thermal, shock, water and vibration.\n• Get the clear perspective of BirdsEye Satellite Imagery right to your device.\n• Power your adventures with 18+ hours of lithium-ion battery life in GPS mode.\n• Link up from way out — with inReach technology.\n• Exchange text messages with loved ones, post to social media, or communicate via inReach device to inReach device in the field (active satellite subscription required ).\n• In case of emergency, trigger an interactive SOS message to GEOS, a professional 24/7 global monitoring and response center (active satellite subscription required ).\n• Instead of relying on cellphone coverage, now your messages, SOS and tracking are transferred via the 100% global Iridium® satellite network (active satellite subscription required ).\n• Available inReach weather forecast service provides detailed updates directly to your Montana handheld or paired devices, so you’ll know what to expect en route.\n• Be prepared. With navigation for any situation.\nRUGGED BY DESIGN\n• This durable, water-resistant handheld is tested to U.S. military standards and features a glove-friendly 5” touchscreen that’s 50% larger than previous Montana models.\nCITY NAVIGATOR MAPS\n• Seamlessly transition from trails to road navigation with preloaded City Navigator street mapping, which guides your drive with easy-to-follow turn-by-turn directions.\n• To navigate your adventures, use preloaded topographical maps for the U.S. and Canada. View terrain contours, elevations, coastlines, rivers, landmarks and more.\nPUBLIC LAND BOUNDARIES\n• Optional display of federal public land boundaries on topographical maps helps you know that you’re venturing where you’re supposed to be.\n• Use direct-to-device downloads of BirdsEye Satellite Imagery (no annual subscription required) to see photo-realistic views and create waypoints based on landmarks.\n• Navigate your next trail with ABC sensors, including an altimeter for elevation data, barometer to monitor weather and 3-axis electronic compass.\n• Use cellular connectivity to access up-to-date Active Weather forecasts through the Garmin Connect™ app .\n• For hunters and trainers, this Montana syncs with compatible Astro ® or Alpha ® series handhelds (sold separately) to help track sporting dogs in the field.\n• Access to both GPS and Galileo satellite networks enables Montana 700i to track in more challenging environments than GPS alone.\n• Using your web-based MapShare™ trip-sharing page, followers can see your progress, ping your device to see your GPS location and exchange messages during\n• Trek far and wide off-grid. Use Expedition mode, an ultralow-power GPS reference, to keep Montana tracking for a week or more between recharges.\nEXTENDED BATTERY LIFE\n• An internal, rechargeable lithium-ion battery pack provides more than 18 hours of battery life in GPS mode.\n• A variety of Montana handheld-compatible mounts (sold separately) lets you adapt your navigator for use on ATVs, boats, cars, motorcycles, RVs or other\n• PHYSICAL DIMENSIONS: 3.6"" x 7.2"" x 1.3"" (9.19 x 18.30 x 3.27 cm)\n• TOUCHSCREEN DISPLAY SIZE 2.55""W x 4.25""H (6.48 x 10.80 cm); 5"" diag (12.70 cm)\n• DISPLAY RESOLUTION 480 x 800 pixels\n• DISPLAY TYPE WVGA transflective, dual orientation\n• WEIGHT 14.5 oz (410 g) with included lithium-ion battery pack\n• BATTERY TYPE rechargeable lithium-ion (included)\n• BATTERY LIFE\n• GPS Mode: up to 18 hours\n• GPS Mode, 10 min Tracking: up to 18 hours\n• Expedition Mode: up to 330 hours\n• Expedition Mode, 30 min Tracking: up to 300 hours\n• WATER RATING IPX7\n0 stars based on 0 reviews\nAdd your review\nReview Garmin Montana 700i Rugged GPS Touchscreen Navigator with inReach Satellite Communicator (010-02347-10)', ""- Team Sports\n- Other Sports\n- Skin Care\n- Beach & Leisure\n- About Escapade\n- Corporate Social Responsibility (CSR)\n- Our Stores\n- Contact us\n- Shop By Brands\n- Gift Vouchers\nHK7s 2023 Official Merchandise\nSpecial Discount Deals\nAll Discounted Items\nGARMIN - Fenix 6X Pro (Solar Edition Titanium Carbon Gray)\nClick here to open the product specification\nStay Charged Up For Any Adventure With Solar Power\nAs our first watch to offer solar charging, the fēnix 6X Pro Solar puts no limits on how far you can go. Its 1.4” Power Glass lens uses sunlight to extend battery life for off-grid adventures.\n- Sunlight gives your watch a free energy boost, extending the time between charges\n- Dynamic PacePro feature helps you run smarter over various types of terrain\n- Preloaded ski maps cover more than 2,000 resorts around the world\n- The power manager feature shows how adjusting settings can extend battery life\n- Wrist-based heart rate¹ and Pulse Ox² sensors add insight to your fitness activities\n- Sync your favorite streaming services, and listen to music without your phone\nQUALITY BY DESIGN\nThe rugged yet sophisticated design features a large 1.4” display that’s 36% larger than previous fēnix models. It is tested to U.S. military standards for thermal, shock, and water resistance.\nA first-of-its-kind PacePro feature helps keep you on pace with grade-adjusted guidance as you run a course.\nTOPO and Ski Maps\nUse topographical maps for North America to navigate your adventures and view run names and difficulty ratings for 2,000 worldwide ski resorts.\nView how various settings and sensors impact your watch’s battery life, so you can make battery-extending changes on the fly.\nView advanced training metrics that include running dynamics¹, heat- and altitude-adjusted VO2 max, recovery advisor, and more.\nUse preloaded activity profiles for trail running, swimming, running, biking, hiking, rowing, backcountry and resort skiing, golfing, and more.\nUse ClimbPro ascent planner to see real-time information on your current and upcoming climbs, including gradient, distance, and elevation gain.\nAccess full-color CourseView maps for more than 41,000 preloaded golf courses around the world, and get button targeting and the PlaysLike Distance feature.\nFollow a route or course with help from turn-by-turn directions, which let you know ahead of time when the next turn is coming.\nEnter a distance you want to travel, and get suggested routes that will bring you back to your starting point. Trendline popularity routing helps you find the best local paths.\nAccess multiple global navigation satellite systems (GPS, GLONASS, and Galileo) to track in more challenging environments than GPS alone.\nNavigate your next trail with ABC sensors, including an altimeter for elevation data, a barometer to monitor weather, and three-axis electronic compass.\nSafety and Tracking Features\nIf you feel unsafe or if your watch senses that an incident occurred, assistance and incident detection features³ send your location to emergency contacts.\nStore up to 2,000 songs, or access streaming apps from your compatible services, including Spotify, Deezer, and Amazon Music, and listen through headphones with BLUETOOTH technology.\nGarmin Pay Service\nBreeze through checkout lines with Garmin Pay contactless payment solution through participating providers.\nReceive emails, texts, and alerts right on your watch when paired with a compatible device.\nWrist-based Heart Rate\nEnhanced optical heart rate1technology gauges the intensity of your fitness activities as well as heart rate variability to calculate your stress level score. It also tracks underwater without a heart rate strap.\nPulse Ox Sensor\nFor altitude acclimation or sleep monitoring, a Pulse Ox2 sensor uses light beams at your wrist to gauge how well your body is absorbing oxygen.\nBody Battery Energy Monitor\nOptimize your body’s energy reserves, using heart rate variability, stress, sleep, and other data to gauge when you’re ready to be active or when you may need to rest.\nConnect IQ Store\nDownload custom watch faces, add data fields and get apps and widgets from the Connect IQ Store.\nTrek far and wide between recharges. Expedition mode, an ultralow-powered GPS reference, lasts for weeks.\nInternal, rechargeable lithium battery provides up to 21 days of battery life in smartwatch mode with an additional 3 days when solar charging 4, and 15 hours in GPS mode with an additional hour when solar charging5.\nWhat's in the box?\n- Fenix 6X Pro Solar Version\n- Charging/data cable""]"	['<urn:uuid:a6457f1c-6a0f-429e-bb5c-639d8f83524e>', '<urn:uuid:ec14aeae-517e-47ef-ae23-20d86f8278f5>']	open-ended	direct	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T19:47:23.148022	7	63	1550
86	How do light sources differ for different wavelengths in spectroscopy?	For different wavelength regions, different light sources are used in spectroscopy. For near infrared, incandescent lamps are suitable but limited to wavelengths below 5 μm due to bulb glass transmissivity. For longer wavelengths, Nernst glowers made of zirconium/yttrium ceramics are used. Silicon carbide rods can be used up to about 40 μm, and mercury vapor lamps are also available. The light source must provide sufficient spectral flux and emit continuously with stable optical properties throughout the interferometer scan.	['Fourier Transform Spectroscopy\nFourier transform spectroscopy is a method where one computes an optical spectrum from raw data by applying a Fourier transform algorithm. The method is applied in various techniques for spectroscopy – most often in the context of infrared spectroscopy.\nThe term time domain spectroscopy is also common, because the measured interference signal is measured in the time domain e.g. in the sense that an optical time delay is varied.\nThe operation principle of Fourier transform spectroscopy in its most common form is fairly simple to understand. The investigated electromagnetic radiation (most frequently, infrared light) is sent to an interferometer, normally in the form of a Michelson interferometer. One then measures the optical power at the output of the interferometer as a function of the arm length difference, using some photodetector. That arm length difference is usually manipulated by mechanically moving a mirror (or more conveniently a retroreflector) over some distance.\nIf the optical input to the interferometer were monochromatic, one would obtain a sinusoidal oscillation of the detected power as a function of arm length difference, and the period of that oscillation would be the optical wavelength. If the light is polychromatic, the recorded interferogram will be a superposition of contributions from the different wavelength components. Therefore, it is clear that by applying a Fourier transform to those data one can retrieve the optical spectrum – more precisely, the power spectral density as a function of optical frequency or wavelength. Information on the spectral phase is not obtained. Some corrections need to be applied to the obtained spectrum, as explained below.\nFor a mathematically founded understanding, consider that the interferogram signal, resulting from the superposition of two optical electric fields with a certain time delay can be expressed as follows:\nwhere I(τ) can be the intensity of the interference signal or alternatively a photocurrent. That signal can be decomposed into a constant and a τ-dependent part; the latter is:\nThis is essentially just the autocorrelation of the electric field. According to the Wiener–Kinchine theorem, the Fourier transform of that is the intensity spectrum of the electric field, i.e., the optical spectrum.\nThe explained operation principle can easily be adapted for absorption spectroscopy. One can record an optical spectrum with and without a specimen inserted into the beam path – before or after the interferometer – and compare the computed spectral intensities to obtain the absorption of the sample in a wide range of wavelengths. More precisely, one obtains the loss of spectral intensity caused by the sample, which may not only be caused by absorption of the specimen but also by surface reflections, for example.\nNote that there are also other, less common forms of Fourier transform spectroscopy. For example, terahertz waveforms can be recorded in the time domain with an optical sampling technique based on a photoconductive antenna (see the article on terahertz detectors). One can then apply a Fourier transform to obtain the optical spectrum of a terahertz pulse, in that case also obtaining the spectral phase.\nVarious Practical Aspects\nRequired Spatial Range and Resolution\nThe obtained spectral resolution is limited by the maximum optical path length difference. This is easy to see considering the properties of discrete Fourier transforms, or simply recognizing that the range of path length differences determines the number of oscillation cycles which can be counted. Quantitatively, the resolution in terms of spectroscopic wavenumber is the inverse of the maximum optical path length difference. Simple instruments may work with only a few centimeters of path length difference, achieving spectral resolutions of somewhat better than 1 cm−1, while high precision spectrometers work with much longer path length differences, e.g. several meters.\nOn the other hand, the maximum wavenumber is half the inverse spatial resolution of the measured path length difference. Therefore, a not particularly high spatial resolution is required for instruments working only with relatively long optical wavelengths, while UV instruments are more demanding in that respect. The spatial accuracy, however, should be much higher – see below.\nFor calculations, note that the variation of pass length difference in a Michelson interferometer is twice the amount of movement of a retroreflector.\nFor spectrometers as used in infrared spectroscopy, often uses a very broadband light source for measuring optical properties of samples in a wide wavelength region. The light source should of course have a sufficiently high spectral flux and emit continuously with stable optical properties throughout the interferometer scan. For the near infrared, incandescent lamps are suitable, but there emission is limited to wavelengths below roughly 5 μm by the transmissivity of the bulb glass. For longer wavelength regions, and therefore uses for millimeters not requiring a glass bulb – for example, Nernst glowers based on an electrically heated rod made of zirconium/yttrium ceramics. Silicon carbide rods can even be used up to about 40 μm. Also there are mercury vapor lamps.\nFor the interferometer to work properly, one requires a light beam with high enough spatial coherence. This is because different spatial components of a beam can produce different contributions to the interferogram, effectively washing out the pattern.\nIdeally, one would have a Gaussian beam from a laser source. In practice, however, one often deals with incoherent sources, where the light has to be spatially filtered, accepting some loss of optical power. However, the possible power throughput is still substantially better than for a grating monochromator as used in other forms of spectroscopy, where light needs to be fed through a narrow optical slit. This is called the Jacquinot advantage, named after Pierre Jacquinot who identified it.\nThe Beam Splitter\nThe optical components of the interferometer should of course properly work over the full spectral region of interest. The most substantial challenge arises from the beam splitter, which would ideally exhibit a 50:50 splitting ratio for all relevant wavelengths. That is not strictly required, but it should at least not lead to highly asymmetric splitting or introduce high power losses e.g. by absorption in a substrate. In infrared spectroscopy, one often uses beam splitters with calcium fluoride (CaF2) substrates for wavelengths up to 8 μm. KBr-based beam splitters with a germanium-based coating can be used up to 25 μm wavelength, but that material is hygroscopic and must therefore be carefully protected against moisture. For the far infrared, one often uses polymer films.\nCalibration of Arm Length Variations\nThe interferometer arm length difference is usually varied with a motorized drive, which can normally not be trusted to provide sufficiently accurate variations of the position. Therefore, one often simultaneously records a second interferogram, using light from a narrow-linewidth laser with sufficiently stable wavelength. One can then computationally correct the data for any deviations of the movement from a perfectly linear movement.\nNote that it is not sufficient only to have a positional accuracy which allows one to clearly resolve the oscillations of an interferogram. This is because random position errors also limit the signal-to-noise ratio of the obtained spectra. Therefore, it is essential to realize Fourier transform spectrometers with highly accurate opto-mechanics and an accurate reference interferometer. That also provides a very high wavelength accuracy – better than in dispersive instruments.\nCalibration of Spectral Power Density\nA simple Fourier transform applied to the raw data will generally not deliver a calibrated optical spectrum, mostly because the responsivity of the used photodetector and the reflectivity of the beam splitter are wavelength-dependent; further influences can come from other optical elements of the setup. Such influences do not matter in absorption spectroscopy, because one only compares spectra with and without an absorbing sample, and the obtained intensity ratios are not affected; one only requires sufficiently strong signals for all relevant wavelengths.\nWhen measuring optical spectra of sources, however, one needs to apply a calibration. It may be done, for example, by comparing with the recorded spectrum of a light source with known spectral shape. In the infrared, one often uses black body radiation for that calibration.\nIn some cases, one may even calibrate a spectrometer for obtaining absolute values of the power spectral density. This is often not easy, however, for example because of influences of the required spatial filtering of the input beam (see above).\nDiscrete Fourier transforms can quite easily and efficiently be computed, using a Fast Fourier Transform (FFT) algorithm. In the simplest form, such an algorithm works with a number of data points which is a power of 2. Even on a relatively simple microprocessor, the FFT computation usually takes much less time than the acquisition of the raw data.\nInterference-based methods of spectroscopy have been used already in the early days of optics, for example by Hippolyte Fizeau, who resolved the doublet of the yellow sodium fluorescence line in the 19th} century (→ Fizeau interferometers). However, computations of optical spectra based on Fast Fourier transform have been implemented only from the middle of the 20th century on, when computers became available; first commercial devices appeared in the 1960s.\nReducing the Sensitivity to Mechanical Noise\nFor the kind of interferometer as explained above, the sensitivity to mechanical noise (vibrations and shocks or inaccuracies of an optical delay line) is quite high. That sensitivity can be massively reduced by using a common-path interferometer based on birefringence. This can be realized, for example, with a simple optical beam path where two polarization components finally interfere at a polarizer. The optical delay between the two polarization components can be adjusted by moving a wedged birefringent crystal . Because that does not only greatly reduce the sensitivity to vibrations, but also allows very accurate scanning of the delay range, it is particularly suitable for Fourier transform spectroscopy in relatively short wavelength regions.\nApplications of Fourier Transform Spectroscopy\nThe method of Fourier transform spectroscopy is most frequently used in conjunction with infrared light – for the following reasons:\n- Particularly in the far infrared, it is difficult to realize focal plane arrays as required for conventional spectrographs, for example. It is thus preferable to use a method where only a simple photodetector is required.\n- Due to the limited sensitivity of infrared detectors (particularly at very long wavelengths), it is important to use the light efficiently. It is thus beneficial to avoid excessive power losses at the input slit of a monochromator (Jacquinot advantage, see above). Besides, one also enjoys the Fellgett advantage (named after a Peter Berners Fellgett, the pioneer of the method): if the measurement noise is dominated by detector noise (e.g. thermal electronic noise) rather than by shot noise, the achievable signal-to-noise ratio is substantially better for the Fourier transform method than for scanning the spectrum with a tunable monochromator, where only a tiny part of the optical spectrum is utilized at any time. This is particularly true in cases where a high spectral resolution is required.\n- The Fourier transform method is even somewhat simpler to implement in the infrared, because the required spatial resolution is lower than for visible and ultraviolet light.\nThe main application of the method is in devices for measuring either optical spectra of light sources or wavelength-dependent properties of materials, such as the transmissivity (e.g. reduced by absorption lines) or the reflectivity.\nThe principle of Fourier transform spectroscopy is also applied in wavemeters, although those usually deliver only the peak wavelength rather than the full optical spectrum.\nThere are also applications of the principal in technical fields outside photonics, for example in the context of nuclear magnetic resonance imaging and mass spectroscopy\nQuestions and Comments from Users\nHere you can submit questions and comments. As far as they get accepted by the author, they will appear above this paragraph together with the author’s answer. The author will decide on acceptance based on certain criteria. Essentially, the issue must be of sufficiently broad interest.\nPlease do not enter personal data here; we would otherwise delete it soon. (See also our privacy declaration.) If you wish to receive personal feedback or consultancy from the author, please contact him e.g. via e-mail.\nBy submitting the information, you give your consent to the potential publication of your inputs on our website according to our rules. (If you later retract your consent, we will delete those inputs.) As your inputs are first reviewed by the author, they may be published with some delay.\n|||P. B. Fellgett, “Theory of infra-red sensitivities and its application to investigations of stellar radiation in the near infra-red” (PhD thesis, 1949)|\n|||P. B. Fellgett, “On the ultimate sensitivity and practical performance of radiation detectors”, J. Opt. Soc. Am. 39 (11), 970 (1949), doi:10.1364/JOSA.39.000970|\n|||P. Jacquinot, “New developments in interference spectroscopy”, Rep. Prog. Phys. 23 (1), 267 (1960), doi:10.1088/0034-4885/23/1/305|\n|||L. Mertz, “Astronomical photoelectric spectrometer”, Astron. J. 71, 749 (1966)|\n|||M. F. A’Hearn, F. J. Ahern and D. M. Zipoy, “Polarization Fourier spectrometer for astronomy”, Appl. Opt. 13 (5), 1147 (1974), doi:10.1364/AO.13.001147|\n|||F. Adler et al., “Mid-infrared Fourier transform spectroscopy with a broadband frequency comb”, Opt. Express 18 (21), 21861 (2010), doi:10.1364/OE.18.021861|\n|||A. Oriana et al., “Scanning Fourier transform spectrometer in the visible range based on birefringent wedges”, J. Opt. Soc. Am. A 33 (7), 1415 (2016), doi:10.1364/JOSAA.33.001415|\n|||F. Johnston, “In search of space: Fourier-spectroscopy”, Chapter 7 of Shinn, Terry and Joerges, Bernward (Eds), Instrumentation: Between Science, State and Industry, Kluwer Academic (2000), available online|\n|||S. P. Davis, M. C. Abrams and J. W. Brault, Fourier transform spectrometry, Academic Press, ISBN-13: 978-0120425105 (2001)|\n|||F. J. J. Clarke et al., “FTIR measurements – standards and accuracy”, Vib. Spectrosc. 30 (1), 25 ( 2002)|\n|||J. Mandon et al., “Fourier transform spectroscopy with a frequency comb”, Nature Photon. 3, 99 (2009), doi:10.1038/nphoton.2008.293|\nSee also: spectroscopy, Michelson interferometers, optical coherence tomography, white light interferometers\nand other articles in the categories light detection and characterization, optical metrology, methods']	['<urn:uuid:111c826b-1820-49fa-84ea-b483250c559d>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:47:23.148022	10	78	2282
87	As a busy teacher managing library - how can I delegate cataloging tasks?	You can delegate cataloging tasks to helpers like students, assistants and volunteers. A good school library system should support this by offering flexible license options for helpers at no extra cost, making it possible to run the library efficiently and cost-effectively.	"[""Three Common Challenges Faced by School Librarians and How a Good Library System Can Help\nSchool librarians face a unique set of challenges in their job of managing the school library. Having a reliable school library system in place is one of the best ways to help them overcome these obstacles. In this blog post, we’ll take a look at three common challenges that school librarians face and explore how a good school library system can help them address these issues.\nI'm busy and I am not a trained librarian!\nA lot of schools do not have the benefit of a full-time trained librarian running a school library. We fully support schools employing trained librarians but the reality is that many schools don’t for various reasons. Instead many school libraries are managed by busy teachers, office staff, parent volunteers, or school governors. This creates challenges like trying to find time in the day when faced with other duties and having to understand things like cataloguing, subject indexing, classification, lending, reports, and accession numbers without training.\nTo overcome these challenges, firstly make sure you select school library software that is easy to learn for staff who do not have formal library training. The best way to assess this is to try out the school library system before you commit to buying it. Are the screens intuitive? Can you find your way around it without hours of training? Is there an online help centre where you can search for articles giving you step-by-step instructions? Also, look for what support you get with the system. Can you phone or email to get answers to your questions or if something goes wrong?\nSecondly, to spread the load, consider delegating tasks like cataloguing or lending to helpers. Helpers could be students, assistants and other volunteers. A good school library system can support you by having flexible licence options for helpers at no extra cost. These features are essential for running a school library efficiently and cost-effectively.\nKeeping track of learning resources\nAnother challenge in running a school library is keeping track of learning resources. When you lend thirty textbooks to a class of students, how do you keep track each copy? How do you know when they are due back and which pupil has which copy?\nFirst of all, you need a list or catalogue of library resources. Some people try to use a spreadsheet or word-processed document to do this but soon find that it’s time-consuming to write accurate details for every single item. To overcome this challenge a good school library system will automate typical tasks involved in managing a school library. Check that there is a cataloguing tool where you type or scan the ISBN on the back of the book and then the software automatically adds the bibliographic details, book cover image and summary to your system.\nSecondly, you need a unique number for each physical item in your school library because when you loan items you need to know which numbered copy was lent to each pupil. See our blog article about the pros and cons of using ISBNs versus barcodes to track items. Good school library software will automatically create copies and assign a unique number to every item.\nThirdly, you need a list of pupils or borrowers in the system which again can be a chore to create. School library software that integrates with your school management information system (MIS) can import pupil data from your MIS and keep it up to date in your school library system. This saves a lot of time at the end of the year when pupils leave and at the beginning of the school year when you have new joiners. You can also be reassured that throughout the year your pupil data is accurate and up-to-date.\nNow you are all set up to lend your learning resources! Make sure that your school library software that has dedicated check-out and check-in screens which are easy for anyone to use. You can either allow pupils to record loans themselves or class teachers and helpers can record the loans when each class visits the library. Check that the school library system makes this a quick and streamlined process for all.\nA school library system can also help when it comes to getting loaned items back. Check that due dates are calculated correctly, loan recall notices can be sent out automatically and that you are alerted to missing items.\nPromoting literacy and pupil engagement\nImproving literacy and pupil engagement is a school-wide objective that the school librarian can actively support. The challenge of managing a school library is that the librarian must find creative ways to encourage reading and engage pupils with the library’s resources. One way a good library system can help is by offering a variety of reading materials that cater to different interests and reading levels.\nBy providing a diverse collection of books, magazines, and online resources, the librarian can pique the curiosity of pupils and encourage them to explore different genres and topics. Your school library software should have search and browse features that make it easy for pupils to find appropriate resources suitable for their reading level. This can be achieved by integrating your school library system with a reading scheme like Accelerated Reader and importing data about reading levels and quizzes.\nIn conclusion, if you don’t have a school library system, you might think you are saving money, but you are wasting staff time on inefficient methods for cataloguing, lending, and other administrative tasks. By implementing a good library system, you can ensure that your school’s library operates smoothly and efficiently, which will ultimately benefit your students. With a robust library system, staff can focus on their core duties of helping students develop reading skills, improve their academic performance, and nurture their love for books and learning.""]"	['<urn:uuid:eed07c7d-7b4c-4b05-b53a-975bea844177>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T19:47:23.148022	13	41	977
88	high aspect ratio boxes impact performance different collision detection methods	High aspect ratio boxes severely impact the performance of grids and quad trees, causing exponential divergence in complexity based on the aspect ratio. In tests with just 10,000 boxes, grids took almost a second to process intersections. With an aspect ratio of 1000, both jsts-quadtree and p2-grid's performance degraded to O(n^2). Even rbush's performance degraded, taking 40x longer with bulk operations and 70x longer without. The streaming segment tree algorithm was the only method that maintained consistent performance with high aspect ratio boxes.	['Previously in this series we covered the basics of collision detection and discussed some different approaches to finding intersections in sets of boxes:\nToday, we’ll see how well this theory squares with reality and put as many algorithms as we can find to the test. For those who want to follow along with some code, here is a link to the accompanying GitHub repo for this article:\nTo get a survey of the different ways people solve this problem in practice, I searched GitHub, Google and npm, and also took several polls via IRC and twitter. I hope that I managed to cover most of the popular libraries, but if there is anything here that I missed, please leave a comment and let me know.\nWhile it is not an objective measurement, I have also tried to document my subjective experiences working each library in this benchmark. In terms of effort, some libraries took far less time to install and configure than others. I also took notes on libraries which I considered, but rejected for various reasons. These generally could be lumped into 3 categories:\n- Broken: The library did not report correct results.\n- Too much work: Setting up the library took too long. I was not as rigorous with enforcing a tight bound here, as I tended to give more generous effort to libraries which were popular or well documented. Libraries with 0 stars and no README I generally skipped over.\n- Irrelevant: While the library may have at first looked like it was relevant, closer inspection revealed that it did not actually solve the problem of box intersection. This usually happened because the library had a suspicious name, or because it was some framework whose domain appeared to include this problem.\nAs a control I implemented a simple brute force algorithm in the obvious way. While it is not efficient for large problem sizes, it performs pretty well up to few hundred points.\nBounding volume hierarchy modules\nI found many modules implementing bounding volume hierarchies, usually in the form of R-trees. Here is a short summary of the ones which I tested:\n- rbush: This is one of the fastest libraries for box intersection detection. It also has a very simple API, though it did take a bit of time tuning the branching factor to get best performance on my machine. I highly recommend checking this one out!\n- rtree: An older rtree module, which appears to have largely been replaced by rbush. It has more features, is more complicated, and runs a bit slower. Still pretty easy to use though.\n- lazykdtree: I included this library because it was relatively easy to set up, even though it ended up being very slow. Also notable for working in any dimension.\n- simple-quadtree: Simple interface, but sluggish performance.\n- jsts-quadtree: Similar problems as jsts-strtree. Unlike strtree, also requires you to filter out the boxes by further pruning them against your query window. I do not know why it does this, but at least the behavior is well documented.\nBeyond this, there many other libraries which I investigated and rejected for various reasons. Here is a (non-exhaustive) list of some libraries that didn’t make the cut:\n- Google’s Closure Library: This library implements something called “quadtree”, but it doesn’t support any queries other than set membership. I am still not sure what the point of this data structure is.\n- quadtree2: Only supports ball queries, not boxes\n- Mike Chamber’s QuadTree: Returns incorrect results\n- node-trees: Returns incorrect results\n- giant-quadtree: Returns incorrect results\n- generic-quadtree: Only implements point-in-rectangle queries, not rectangle-rectangle (stabbing) queries.\n- quadtree: I’m don’t know what this module does, but it is definitely not a quad tree.\n- Box2D: Probably the de-facto 2D physics engine, has been extremely influential in realtime physics and game development. Unfortunately, the quality of the JS translations are much lower than the original C version. Supports sweep-and-prune and brute force for broad phase collision detection.\nI also considered the following physics engines, but ended up rejecting them for various reasons:\n- cannon.js: To its credit, cannon.js has a very clear API and well documented code. It is also by the same author as p2.js, so it is probably good. However, it uses spheres for broad phase collision detection, not boxes, and so it is not eligible for this test.\n- GoblinPhysics: Still at very early stages. Right now only supports brute force collision detection, but it seems to be progressing quickly. Probably good to keep an eye on this one.\n- PhysicsJS: I found this framework incredibly difficult to deal with. I wasted 2 days trying to get it to work before eventually giving up. The scant API documentation was inconsistent and incomplete. Also, it did not want to play nice in node.js or with any other library in the browser, hooking event handlers into all nooks and crannies of the DOM, effectively making it impossible to run as a standalone program for benchmarking purposes. Working with PhysicsJS made me upset.\n- Matter.js: After the fight with PhysicsJS, I didn’t have much patience for dealing with large broken libraries. Matter.js seems to have many of the same problems, again trying to patch a bunch of weird stuff onto the window/DOM on load, though at least the documentation is better. I spent about an hour with it before giving up.\n- ammo.js/physijs: This is an emscripten generated port of the popular bullet library, however due to the translation process the API is quite mangled. I couldn’t figure out how to access any of the collision detection methods or make it work in node, so I decided to pass on it.\nIn each of these experiments, a set of boxes was generated, and then sent to each library to compute a checksum of the set of pairs of intersections. Because different libraries expect their inputs in different formats, as a preprocessing step the boxes are converted into whatever data type is expected by the library. This conversion step is not counted towards the total running time. Note that the preparation phase does not include any time required to build associated data structures like search trees or grids; these timings are counted toward the total run time.\nBecause algorithms for collision detection are output sensitive, care was taken to ensure that the total number of intersections in each distribution is at most , in order to avoid measuring the reporting time for each method.\nA limitation of this protocol is that it favors batched algorithms (as would be typically required in CAD applications), and so it may unfairly penalize iterative algorithms like those used in many physics engines. To assess the performance of algorithms in the context of dynamic boxes more work would be needed.\nHere is a summary of the results from this benchmark. For a more in depth analysis of the data, please see the associated GitHub repo. All figures in this work were made with plot.ly, click on any of the images to get an interactive version.\nI began this study by testing each algorithm against the uniform distribution. To ensure that the number of intersections is at most , I borrowed a trick from Edelsbrunner and Zomorodian; and scaled the side length of each box to be while constraining the boxes to remain within the unit hypercube, . A typical sample from this distribution looks like this,\nTo save time, I split the trials into phases ordered by number of boxes; algorithms which performed took too long on smaller instances were not tested on larger problem sizes. One of the first and most shocking results was a test instance I ran with just 500 boxes:\nHere two libraries stand out for their incredibly bad performance: Box2D and lazykdtree. I am not sure why this situation is so bad, since I believe the C version of Box2D does not have these problems (though I need to verify this). Scaling out to 1500 boxes without these two libraries gives the following results:\nThe next two worst performing libraries were simple-quadtree and rtree. simple-quadtree appears to have worse than quadratic growth, suggesting fundamental algorithmic flaws. rtree’s growth is closer to , but due to the constants involved is still far too slow. As it took too long and there were 2 other representatives of the r-tree data structure in the benchmark, I decided to drop it from larger tests. Moving on to 10k boxes,\nAt this point, the performance of brute force begins to grow too large, and so it was omitted from the large brute force tests.\nBecause p2’s sweep method uses an insertion sort, it takes time with a cold start. This causes it to perform close to brute force in almost all cases which were considered. I suspect that the results would be better if p2 was used from a warm start or in a dynamic environment.\nBoth of jsts’ data structures continue to trend at about growth, however because the constants involved are so large they were also dropped from the large problem size.\nOne surprise is that within this regime, box-intersect is consistently 25-50% faster than p2-grid, even though one would expect the complexity of the grid algorithm to beat the time of box-intersect. An explanation for this phenomenon would be that box-intersect enjoys better cache locality (scaling linearly with block size), while hashing causes indirect main memory accesses for each box. If the size of a cache line is , then the cross over point should occur when in 2D. This is illustrated in the following chart which carries out the experiment to 250k boxes,\nAs expected, somewhere between 10000 and 20000 boxes, p2-grid finally surpasses box-intersect. This is expected as grids realize complexity for uniform distributions, which is ultimately faster than for sufficiently large .\nThe complexity of rbush on this distribution is more subtle. For the bulk insertion method, rbush uses the “overlap minimizing tree” (OMT) heuristic of Lee and Lee,\nT. Lee, S. Lee. (2003) “OMT: Overlap minimizing top-down bulk loading algorithm for R-Trees” CAiSE\nThe OMT heuristic partitions the space into an adaptive grid at each level of the tree along the quantiles. In the case of a uniform distribution of boxes, this reduces to uniform grid giving a query time of , which for finite gives means that rbush will find all intersections in time. As a result, we can expect that once , rbush-bulk should eventually surpass box-intersect in performance (though this did not occur in my benchmarks). This also suggests another way to interpret the OMT heuristic: it is basically a hedged version of the uniform grid. While not quite as fast in the uniform case, it is more adaptive to sparse data.\nOf course realistic data is hardly ever uniformly distributed. In CAD applications, most boxes tend to be concentrated in a lower dimensional sub-manifold, typically on the boundary of some region. As an example of this case, we consider boxes which are distributed over the surface of a sphere (again with the side lengths scaled to ensure that the expected number of collisions remains at most ).\nTo streamline these benchmarks, I excluded some of the worst performing libraries from the comparison. Starting with a small case, here are the results:\nAgain, brute force and p2’s sweep reach similar performance.\nMore significantly, p2-grid did not perform as well as in the uniform case, running an order of magnitude slower. This is as theory would predict, so no real surprises. Continuing the trend out to 50k boxes gives the following results,\nBoth p2-grid and jsts-quadtree diverge towards , as grid based partitioning fails for sparse data.\nOne way to understand the OMT heuristic in rbush, is that it is something like a grid, only hedged against sparse cases (like this circle). In the case of a uniform distribution, it is not quite as fast as a grid, suffering a penalty, while adding robustness against sparse data.\nAgain box-intersect delivers consistent performance, as predicted.\nHigh aspect ratio\nFinally, we come to the challenging case of high aspect ratio boxes. Here we generate boxes which are uniformly distributed in and and stretched along the -axis by a factor of :\nHigh aspect ratio boxes absolutely destroy the performance of grids and quad trees, causing exponential divergence in complexity based on the aspect ratio. Here are some results,\nEven for very small , grids take almost a second to process all intersections for just 10000 boxes. This number can be made arbitrarily high by choosing as extreme an aspect ratio as one likes. Here I select an aspect ratio of , forcing the asymptotic growth of both jsts-quadtree and p2-grid to be . If I had selected the aspect ratio as or , then it would have been possible to increase their running times to some arbitrarily large value. In this benchmark, rbush also grows though by a much slower . Continuing out to 100k boxes, eventually rbush also fails,\nIn this case rbush-bulk takes more than 40x slower and rbush-bulk more than 70x. Unlike in the case of grids however, these numbers are only realized by scaling in the number of boxes and so they cannot be made arbitrarily large. However, it does illustrate that for certain inputs rbush will fail catastrophically. box-intersect again continues to grow very slowly.\nThe only libraries which I found that implemented 3D box intersection detection were lazykdtree and oimo.js. As it is very popular, I decided to test out oimo’s implementation on a few small problem sizes. Results like the following are typical:\nFor large, uniform distributions of boxes, grids are still the best. For everything else, use segment trees.\n- Brute force is a good idea up to maybe 500 boxes or so.\n- Grids succeed spectacularly for large, uniform distributions, but fail catastrophically for anything more structured.\n- Quad trees (at least when properly implemented) realize similar performance as grids. While not as fast for uniform data, they are slightly hedged against boxes of wildly variable size.\n- RTrees with a tuned heuristic can give good performance in many practical cases, but due to theoretical limitations (see the previous post), they will always fail catastrophically in at least some cases, typically when dealing with boxes having a high aspect ratio.\n- Zomorodian & Edelsbrunner’s streaming segment tree algorithm gives robust worst case performance no matter what type of input you throw at it. It is even faster than grids for uniform distributions at small problem sizes(<10k) due to superior cache performance.\nOverall, streaming segment trees are probably the safest option to select as they are fastest in almost every case. The one exception is if you have a large number of uniformly sized boxes, in which case you might consider using a grid.']	['<urn:uuid:e09eb5ec-4b19-4e31-ac41-d7f627c8f01d>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T19:47:23.148022	10	83	2471
89	union confederate troops gettysburg positioning losses	In the morning, Union forces were arranging their positions with Hancock's corps connecting to Howard's, and Sickles was directed to connect with Hancock's left and Round Top Mountain. The Fifth corps was moved from the right to be in reserve on the left. The subsequent battle saw heavy casualties on both sides - the Confederates lost over 6,000 men while Union forces suffered approximately 1,500 casualties. The Confederate assault included a major infantry charge where they briefly penetrated Union lines before being repulsed with heavy losses, including the mortal wounding of Brigadier General Garnett.	"[""were massed on the right (enemy's), as Meade contemplated an attack from that flank-Hancock's corps connected with Howard's, and Sickles was on the left of Hancock, but he did not go into position until the afternoon. On page 405, Hancock says: “I was placed on the line connecting Cemetery Hill with Little Round Top Mountain, my line, however, not extending to Round Top, probably only about half way. General Sickles was directed to connect with my left and the Round Top Mountain, thus forming a continuous line from Cemetery Hill (which was held by Gen. Howard) to Round Top Mountain.” These arrangements were not made until the morning was considerably advanced. On page 331, Meade after stating his purpose to make an attack from his right says: “Major-General Slocum, however, reported that the character of the ground in front was unfavorable to making an attack; and the Sixth corps having so long a distance to march, and leaving at nine o'clock at night, did not reach the scene until about two o'clock in the afternoon. Under these circumstances I abandoned my intention to make an attack from my right, and as soon as the Sixth corps arrived, I directed the Fifth corps, then in reserve on the right, to move over and be in reserve on the left.” It was a division of the Fifth corps (General Sykes') that rescued the Round Top from the grasp of our assaulting column. Does not this show how weak the left was in the morning, and how easy it would have then been for our troops on the right to have gotten possession of the key to the position? That General Lee's plans were thwarted by the delay on the right, can any man doubt? On the occasion of the dedication of the Cemetery for the Federal soldiers killed at Gettysburg, Edward Everett, in the presence of President Lincoln, some of his cabinet, many members of Congress and officers of the army, and an immense concours) of citizens, delivered an address, in which he thus graphically describes the effect of the delay that took place: “And here I cannot but remark on the Providential inaction of the rebel army. Had the conflict been renewed by it at daylight on the 2nd of July, with the First and Eleventh corps exhausted by battle, the Third and Twelfth weary from their forced march, and the Second, Fifth, and Sixth not yet arrived, nothing but a miracle could have saved the army from a great disaster. Instead of this the day dawned, the sun rose, the cool hours of the morning passed, and a considerable part of the afternoon wore away without the slightest aggressive movement on the part of the enemy. Thus time was given for half of our forces to arrive and take their places in the lines, while the rest of the army enjoyed a much needed half-day's repose.” It is to be presumed that before preparing an address that was to assume a historical character, Mr. Everett had obtained accurate knowledge of all that transpired within the Federal lines\nThis text is part of:\nTable of Contents:\nBattle of Kelleysville , March 17th , 1863 -Reports of Generals J. E. B. Stuart and Fitz. Lee .\nCauses of the defeat of Gen. Lee 's Army at the battle of Gettysburg -opinions of leading Confederate soldiers.\nLetter from Gen J. A. Early .\nCauses of the Confederate defeat at Gettysburg .\nLetter from General E. P. Alexander , late Chief of artillery First corps , A. N. V .\nLeading Confederates on the battle of Gettysburg .\nLetter from General John B. Hood .\nOfficial Reports of the battle of Gettysburg .\nReport of General Patton Anderson of operations of his division from 30th of July to 31st of August , 1864 , including the battle of Jonesboro , Georgia .\nThe peace Commission .-letter from Ex-President Davis .\nLetter from Hon. J. P. Benjamin .\nFarewell address of Brigadier-General R. L. Gibson to the Louisiana brigade after the terms of surrender had been agreed upon between Lieut.-Gen. Richard Taylor , C. S. A. , and Major-Gen. E. R. S. Canby , U. S. A.\nReminiscences of torpedo service in Charleston Harbor by W. T. Glassel , Commander Confederate States Navy.\nThis work is licensed under a Creative Commons Attribution-ShareAlike 3.0 United States License.\nAn XML version of this text is available for download, with the additional restriction that you offer Perseus any modifications you make. Perseus provides credit for all accepted changes, storing new additions in a versioning system."", 'The decisive battle has been fought to-day, and the enemy repulsed with terrific loss. At daylight Lee’s right wing batteries opened upon our left, and shortly after those of his centre followed.\nAfter half an hour’s cannonading, doing but little damage to us, the fire slackened and only occasional shots were exchanged. Shortly afterwards the enemy’s left, composed entirely of infantry and sharpshooters, made an attack on our right wing so sudden and importunely that our skirmishers and front line were driven back from their entrenchments, but by the aid of the batteries in the rear and the bravery of the 12th corps, we regained the first position, capturing a considerable number of prisoners. Several hours of ominous silence followed this repulse. At 1 o’clock the enemy fired two shots, apparently the signal for the grandest artillery fight ever witnessed on this continent. Before a moment elapsed it is estimated at least 80 guns opened upon us. Our batteries returned the fire, and for more than one hour it seemed impossible that man or beast could live. The range as exhibited on the two previous days was wanting on this occasion, most of their shells exploding far in the rear of our front, and generally missing our batteries. Under cover of this Lee advanced his columns of infantry from their covers and made several desperate attempts to carry the lines by assault, but each successive attempt repelled with terrific havoc to them. Some of our batteries, whose ammunition being expended and the men exhausted, ceased to fire, and on the approach of the reserve batteries withdrew to the rear.\nThe enemy, on seeing the batteries withdrawn, and mistaking this for a retreat, made a rapid infantry charge upon the hill and obtained position in our lines, cutting to pieces and almost annihilating the small infantry supports, but before they had time to rejoice at their imaginary success, the breech batteries poured in a deadly fire of canister. The infantry reserve joined on either flank of the gap, charged them and added greatly to their destruction. They were completely surprised, and hundreds threw down their guns and asked for quarter. Nearly the entire brigade of Gen. Dick Garnett surrendered, and Garnett himself was wounded and barely made his escape.\nLongstreet was mortally wounded and captured. He is reported to have died in one hour afterward.\n– – –\nThis telegraphed report of an as-yet-unnamed battle near Gettysburg, Pennsylvania, was probably received, but not published by the Daily Democrat and News until July 6 as this newspaper did not print on the Fourth of July nor the next day, as it was a Sunday. Even so, the telegraph shortened the usual delay in news of the War by a week or more.\nIt’s important to note, however, that quick news doesn’t always mean accurate facts: As official reports later showed, Lieutenant General Longstreet* was unwounded. It was Brigadier General Garnett who suffered a fatal wound and died on the field.\nThe Brigadier General wasn’t the only one to fall in what would later be known as Pickett’s Charge, a bloody fight between an estimated 6,500 Union troops and 15,000 Confederate soldiers on the third day of the Battle of Gettysburg.\nUnion losses during this bloody battle, including the dead and the wounded, those missing in action or taken prisoner, totaled about 1,500. Confederate losses were over 6,000; roughly half those men were from Major General George Pickett’s division.\nThe evening of Pickett’s Charge, General Lee regrouped and waited for Major General Meade to attack, while heavy rains began to fall. Several small skirmishes took place on July 4th, but no further major battles, and by evening, General Lee had started to move his troops south.\nThe supply wagon train filled with Confederate wounded was reported to be 14 miles long.\n(posted by Amy D.)\n*Longstreet, Garnett, Pickett, and Lee all served with the Confederate forces. Major General Meade served with the Union.\nFor telegraph information leading up to the battle of Gettysburg, please click here.']"	['<urn:uuid:8064574d-1b7e-41b6-8279-5b72d4cbed30>', '<urn:uuid:a3f5157e-ed9a-4a6b-bf7a-ceb226113f30>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	6	94	1440
90	What are the uses and limitations of channelrhodopsin-2 in helping blind people see, and how does it work in practice?	Channelrhodopsin-2 (ChR2) is a light-sensitive protein from green algae that can help restore vision by making retinal cells responsive to light. When delivered into the retina of blind mice, it can partially restore visual function by making bipolar cells light-sensitive, bypassing the need for photoreceptors. However, it has several limitations: it only works in less than 10% of bipolar cells, the cells become sensitive to a much narrower range of light intensities compared to normal photoreceptors, and they require much stronger light stimulation to function.	['Scientifica team members will be at the Imaging, Structure & Function in the Nervous System course at Cold Spring Harbour 25 July - 14 August.\n#LabHacks: Choosing the best opsin for your optogenetics experiments\nAccording to Tye and Deisseroth 20121, designing an optogenetics experiment can be broken down into five main parts, the first of which is finding the most suitable opsin for your particular investigation.\nSince the establishment of optogenetics as a powerful technique to study neural circuit function, several research groups have worked on expanding the number of available opsins. As such the “optogenetics toolbox” has grown significantly, magnifying the number of processes that can be interrogated, but complicating the selection process.\nDeciding which opsin to use requires the consideration of several trade-offs, while always keeping the experimental goals in mind. These include – but are not limited to – ion conductance, peak activation wavelength, light sensitivity and kinetics.\nThe opsins used in optogenetics can be widely divided into type I (microbial opsins) and II (animal/vertebrate opsins)2. The microbial opsins first used in optogenetic photomanipulation of neural tissues and include the widely-known channelrhodopsin, halorhodopsin and bacteriorhodopsin families. They are single component light sensing systems (i.e. the same protein confers light sensitivity and ion conductance). Type I opsins work by directly altering the membrane potential of the neurons.\nType II opsins are mainly comprised of vertebrate rhodopsins, like the light-sensing proteins that enable you to see. They are light-sensitive G-protein coupled receptors (GPCRs). When activated by light a conformational change in the protein’s structure starts an intracellular signalling cascade which can ultimately alter the inhibitory or excitatory state within the cell.\nThe microbial opsins\nMicrobial opsins offer fast and precise manipulation of neuronal excitability by directly altering the membrane potential of the neuron to bring it closer to, above, or further away from the threshold for generating action potentials.\nOriginating from microalgae of the genus Chlamydomonas, the channelrhodopsins are light-gated ion channels3. Boyden et al. 20054 used Channelrhodopsin-2 (ChR2) to show optical control of neural activity in their famous Nature Neuroscience paper.\nChR2 is a cation channel that leads to rapid depolarization of a neuron by enabling cations (e.g. Na+, K+, Ca2+, H+) to move into the cell (in the direction of the electrochemical gradient), raising the membrane potential and evoking precisely timed action potentials5. ChR2 has a peak activation wavelength of 470 nm (blue light) and decay kinetics of ~10 ms at room temperature2.\nA variety of other rapidly depolarising optogenetic opsins have been developed from ChR2 mutations, channelrhodopsins from other species and the creation of chimaeras from combinations of channelrhodopsins. This has led to tools with slower kinetics, shifted wavelengths, higher sensitivity, etc. For a list of the properties on several channelrhodopsins read Yizhar et al. 20112.\nOne example is humanised ChR2 (hChR2), a widely-used opsin whose genetic code replaced algal codons with mammalian codons leading to better expression levels in mammalian animal models.\nBacteriorhodopsins (BR), proteorhodopsins (PR) and archeorhosopsins (AR) are all inhibitory proton pumps that hyperpolarize neurons by pumping H+ out of the cell. This leads to a more negative membrane potential making excitatory inputs into the cell less likely to reach the action potential threshold. Several naturally occurring light-sensitive proton pumps have been identified or engineered for slightly differing properties like enhanced expression on the cellular membrane, altered kinetics and different peak activation wavelengths.\nHalorhodopsin (NpHR), so called after the archaeon where it naturally occurs (Natronomonas pharaonic), is another inhibitory opsin that pumps chloride ions into the cell to hyperpolarize the membrane6. The mammalian codon optimised form of this opsin did not traffic to the outer cell membrane and accumulated in the endoplasmic reticulum. Further engineering led to eNpHR3.0 which induces a large photocurrent and localises to the outer membrane. This is now among the most commonly used inhibitory opsins and is often used in combination with an excitatory opsin to test individual circuit components7. Its peak activation wavelength of 590 nm (yellow light) is far enough from the commonly used ChR2 wavelengths (470 nm, blue light) to avoid involuntary cross-activation.\nThe animal opsins\nControl of intracellular signalling\nEngineered versions of vertebrate rhodopsins have been developed to take advantage of their light-activated nature and initiation of intracellular signalling pathways. The most common group of these, known as optoXRs, are opsin-receptor chimaeras of a rhodopsin and conventional ligand-gate GPCR8. In naming each member of the family, the X is replaced by the GPCR being used (e.g. Optoα1AR for a rhodopsin and α1 adrenergic receptor chimaera)9. They allow optical modulation of a variety of intracellular signalling cascades with temporally precise resolution suitable for studying behaviour in freely moving mice.\nStep function opsins (SFOs) are a family of ChR2 mutants that once activated by light, show prolonged activity after termination of the light stimulus10. The first member of the group was ChR2(C128S), so called because it’s 128th residue was altered from a cysteine to a serine, remains activated for approximately 1.7 minutes when activated by 470 nm light. They can also be inactivated by light of a different wavelength, 560 nm peak inactivation wavelength in the case of ChR2(C128S). The ability of the opsins to remain activated for extended periods even after cessation of a light pulse, and to be turned off make them useful for experimental paradigms precise modification of the firing rate of neuronal populations.\nMore complex experimental protocols can be developed with the introduction of different opsins into different cell types. In these cases, you will need to contemplate targeted expression specificity, the overlap of activation wavelengths, and whether light-activation of each opsin will occur in the same or different regions. Each of these factors may influence which opsin you choose.\nIn vitro, in vivo, light sensitivity and phototoxicity\nFurther considerations when choosing your opsins depend on what type of experiment you will be conducting. For example, different characteristics will be essential for in vitro network dynamics or in vivo behavioural experiments.\nFor in vitro experiments the peak activation wavelength of the opsin might not be the most important concern. You won’t necessarily be trying to penetrate deeply into thick, highly scattering tissue, so whether your opsin is best suited to a blue, yellow or red-shifted light source won’t affect performance too much. However, fast kinetics might be crucial for your experimental goals.\nFor behavioural experiments, the kinetics may be less important whereas light sensitivity may be crucial due to the power you need to achieve in the desired region and potential off-target effects caused by using too much light.\n- Tye, K. M. & Deisseroth, K. Optogenetic investigation of neural circuits underlying brain disease in animal models. Nat Rev Neurosci 13, 251–266 (2012).\n- Yizhar, O., Fenno, L. E., Davidson, T. J., Mogri, M. & Deisseroth, K. Optogenetics in Neural Systems. Neuron 71, 9–34 (2011).\n- Channelrhodopsins - OpenOptogenetics.org. Available at: http://www.openoptogenetics.org/index.php?title=Channelrhodopsins. (Accessed: 8th March 2017)\n- Boyden, E. S., Zhang, F., Bamberg, E., Nagel, G. & Deisseroth, K. Millisecond-timescale, genetically targeted optical control of neural activity. Nat Neurosci 8, 1263–1268 (2005).\n- Mattis, J. et al. Principles for applying optogenetic tools derived from direct comparative analysis of microbial opsins. Nat Meth 9, 159–172 (2012).\n- Guru, A., Post, R. J., Ho, Y.-Y. & Warden, M. R. Making Sense of Optogenetics. Int J Neuropsychopharmacol 18, (2015).\n- Han, X. & Boyden, E. S. Multiple-Color Optical Activation, Silencing, and Desynchronization of Neural Activity, with Single-Spike Temporal Resolution. PLoS ONE 2, (2007).\n- Airan, R. D., Thompson, K. R., Fenno, L. E., Bernstein, H. & Deisseroth, K. Temporally precise in vivo control of intracellular signalling. Nature 458, 1025–1029 (2009).\n- Opto-XRs - OpenOptogenetics.org. Available at: http://www.openoptogenetics.org/index.php?title=Opto-XRs. (Accessed: 8th March 2017)\n- Fenno, L., Yizhar, O. & Deisseroth, K. The development and application of optogenetics. Annu. Rev. Neurosci. 34, 389–412 (2011).\nBanner Image Credit: Wikimedia Commons\nYour quote request has been received and we will be in touch shortly.', 'New research shows that a protein found in green algae can partially restore visual function when delivered into the retina of blind mice.\nThe work brings us one step closer to a genetic therapy for various conditions in which the degeneration of retinal cells leads to imapired vision or complete blindness.\nNormally, light entering the eye falls upon the rods and cones at the back of the retina. These are the photoreceptors: they are packed with a light-sensitive protein called rhodopsin, which initiates an electrical signal when struck by photons (the particles which carry light).\nThe signals generated by the photoreceptors are transmitted to the bipolar cells, which in turn transmit the signals to ganglion cells, whose axons leave the retina to form the optic nerve. Thus, visual information is carried from the eye to the visual centres of the brain, via the lateral geniculate nucleus, a relay station in the thalamus.\nIn the retina, visual information passes along two separate and parallel channels, each one most sensitive to different types of light stimuli. Photoreceptors in the ON centre pathway are sensitive to bright stimuli against a dark background, whereas cells in the OFF centre pathway are sensitive to dark stimuli on a light background.\nBipolar cells receive converging inputs from several photoreceptors, and so have similar receptive field properties. ON bipolar cells are most responsive to a light spot on a dark background, and vice versa for OFF bipolar cells. The cells in the retina are therefore well-suited to detecting contrast, which is ideal for recognizing the edges of objects.\nContrast is further increased by a process called lateral inhibition, whereby activity in one pathway reduces activity in the other. In this way, the circuits in the retina carry out simple processing of the visual data entering the eye. They generate a very crude representation of the visual image, which is then sent to the brain for further processing.\nThe new research, led by Botond Roska of the Friedrich Miescher Institute for Biomedical Research in Basel, Switzerland, and Connie Cepko of Harvard Medical School, is reported online in an advance online publication in Nature Neuroscience. It uses a protein called channelrhodopsin-2 (ChR2), a light-sensitive ion channel that was recently isolated from the green algae Chlamydomonas reinhardtii.\nChR2 belongs to a family of proteins which are also found in bacteria, and are related to the rhodopsin proteins found in the retina of mammals. Unlike their mammalian counterparts, the channelrhodopsins are directly linked to a pore which spans the membrane of the cell, and can open or close in response to light of a specific wavelength and so alter the flow of electrical currents across the membrane. They can therefore confer light sensitivity onto cells that do not normally respond to light.\nIt is this property of the algal protein that was exploited in the new study. In conditions such as macular degeneration and retinitis pigmentosa, there is a progressive loss of photoreceptors. The researchers therefore reasoned that they could bypass the photoreceptors entirely, and restore visual function by using ChR2 to make the bipolar cells responsive to light.\nThe study was carried out on a strain of mutant mice which lack a gene that is required for the survival of photoreceptors, and which provide a good model for human retinal degenerative conditions. These animals lose most of their photoreceptors by 4 weeks of age, by which time, light stimulation does not elicit any detectable electrical activity in retinal cells. In other words, by that age, the mice are virtually blind.\nThe researchers first created a genetic construct consisting of the ChR2 gene fused to the gene encoding a fluorescent protein. This construct was then delivered to the animals’ retinas by in vivo electroporation, whereby cells are made transiently permeable to DNA by the application of a small external electric field.\nThe construct also contained a regulatory DNA sequence called a promoter, which normally drives the synthesis of a glutamate receptor in a specific cell type: the ON centre bipolar cells in the retina. So, when delivered to the retina, the construct may enter various types of cells, but will only be activated in the ON bipolar cells.\nIn mutant mice lacking photoreceptors, targeted expression of ChannelRhodopsin 2 (ChR2) confers light sensitivity upon bipolar cells. Bipolar cells expressing ChR2 are stained green. Scale bar = 10 micrometers. (From Lagali et al, 2008.)\nOnce inside, the construct is incorporated into the chromosomes, so that cells containing it synthesize both the fluorescent protein and ChR2. This should make the bipolar cells sensitive to light, and an electrode array which recorded electrical activity in the ganglion cells in response to light confirmed that this was indeed the case.\nThus, in the absence of photoreceptors, the bipolar cells synthesizing Chr2 were sensitive to light, which activated them, so that they generate an electrical signal which is then transmitted to the ganglion cells. The number of signals recorded was found to dependent on the intensity of light, so that the more intense the stimulus used, the more frequent were the bursts of ganglion cell activity.\nThis was further confirmed anatomically. The parallel ON and OFF bipolar cell streams have slightly different projections, with their processes terminating in two different layers of the retina, and confocal microscopy (above) showed that the cells targeted by the ChR2 construct project to the outer layer only, which is characteristic of ON bipolar cells.\nFurthermore, electrophysiological recordings further showed that the responses of the bipolar cells are transmitted to the brain: when the experimental animals were presented with light stimuli, the electrodes detected corresponding activity in cells in the visual cortical areas.\nThe new ability of the previously-blind mice also affected their behaviour. When placed in a box that is divided into light and dark compartments, the movements of normal mice, but not of blind ones, increase. And when the mutant mice with ChR2-expressing bipolar cells were placed in this apparatus, their movements were comparable to those of normal animals.\nElsewhere, researchers are trying to develop retinal implants for the treatment of eye diseases. A therapy based on ChR2 would have the advantage of not requiring the implantation of foreign metallic objects (electrodes) into the retina. And, because it involves genetic targeting of a specific type of retinal cell, it is an improvement on previous similar work, which has been less selective.\nAlthough the results of the present are very promising, there are several problems. First, the ChR2 protein was introduced into less than 10% of the bipolar cells in the retina, and was not evenly distributed; the bipolar cells were sensitive to a far narrower range of light intensities than are photoreceptors; and they required a far greater intensity to be stimulated into action.\nFinally, it should not be assumed that the animal model faithfully reproduces all aspects of eye diseases in humans. It is unclear, for example, how the connections between retinal cells remain intact during the course of progressive degeneration. Undoubtedly, these issues will be addressed in future work.\n- Restoring sight to the blind with deep brain stimulation\n- Control of rodent motor cortex with an optical neural interface\n- Optical on/off switch for neurons controls animal behaviour\n- Using light to control neuronal activity\nLagali, P. S., et al. (2008). Light-activated channels targeted to ON bipolar cells restore visual function in retinal degeneration. Nat. Neurosci. doi: 10.1038/nn.2117. [Abstract]']	['<urn:uuid:e7f62f58-4dbc-400c-90c2-fcde2f485de4>', '<urn:uuid:9a581122-d3e0-4da4-a2ba-c453139ddd9a>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	20	85	2545
91	what causes ammonia problems in poultry houses and how do circulation fans improve air temperature	Ammonia problems are primarily caused by litter moisture, with higher relative humidity (70-80%) leading to wet litter and increased ammonia production. When ventilation runs at minimum capacity in winter, temperature stratification occurs with warm air rising to the top of the building. Circulation fans solve this by forcing warm air down to the animal zone, which helps maintain uniform temperature and creates drier bedding conditions.	['Ammonia concentration versus ventilation rate\nMonitoring ammonia levels in poultry houses can help producers to adjust ventilation rates and thus prevent problems, say Michael Czarick (Extension Engineer) and Brian Fairchild (Extension Poultry Scientist) at the University of Georgia in the latest in the series of ‘Poultry Housing Tips’.\nOne of the biggest challenges with maintaining optimal air quality during cold weather is the fact that much of what we are trying to control is constantly being generated. For instance, if we had an empty house and filled it with smoke we could turn on a few exhaust fans and within minutes the smoke would be gone – problem solved.\nBut, when it comes to gases such as ammonia and carbon dioxide which are constantly being generated, it is a never-ending process to keep them under control. For example, during the minimum ventilation, “off cycle” ammonia levels will slowly rise as the ammonia produced by the litter builds. When the minimum ventilation fans turn on the ammonia levels will quickly decrease as fresh air is added to the house. But as soon as the exhaust fans turn off, the ammonia level will start to rise again and by the end of the “off cycle” it will be back to where it was prior to the fans turning on.\nThe cyclical nature of ammonia concentrations in poultry houses can an be seen in Figure 1 where between flocks two 48-inch fans were operating three minutes out of every 30 to help limit the build-up of ammonia and moisture.\nThough operating the exhaust fans lowered the concentration of ammonia in the air by approximately 15 parts per million (ppm), as soon as the fans turned off ammonia concentration started to rise and within 30 minutes, it was back to what it was before the exhaust fans turned on. The pattern continued for the next 14 hours of the study (Figure 2).\nLittle if any progress was made in lowering the overall average ammonia concentration.\nThe same cyclic pattern of ammonia concentrations can be seen in Figure 3 where minimum ventilation fans were operating 90 seconds out of five minutes in a house with three-week-old birds. Minimum fan operation decreased ammonia levels by approximately 7ppm but by the end of the minimum ventilation fan “off cycle” they rose the same 7ppm. The average ammonia concentration remained essentially the same, 33ppm, over the course of the night.\nFor any minimum ventilation fan setting, the ammonia levels will tend to reach a level of equilibrium. In Figures 1, 2 and 3, though the minimum ventilation fans reduced ammonia concentrations quickly, the reduction was temporary and over the short-term, average ammonia levels did not significantly change. The equilibrium ammonia concentration can, of course, be lowered by increasing the minimum ventilation rate.\nAn important fact to keep in mind is that the reduction in the equilibrium or “average” ammonia concentration will be roughly proportional to the change in exhaust fan run-time. This means that in order to make large changes in average ammonia concentrations, relatively large changes in minimum ventilation fan runtime need to be made. For instance, the minimum ventilation fan run-time in the house illustrated in Figure 3 was increased from 90 seconds to 120 seconds out of five minutes (4:00am).\nThe increase in fan run-time of 25 per cent reduced average ammonia concentration by 22 per cent (from 33 to 27ppm; Figure 4). Increasing fan runtime from 90 to 150 seconds, a 40 per cent increase, reduced ammonia concentration by 35 per cent (from 33ppm to 22ppm; Figure 5).\nSo in short, if you want to cut your ammonia levels in half, double your minimum ventilation rate. This holds true if the ammonia concentration is 80ppm and you want to drop it to 40ppm or even if the ammonia concentration is 40ppm and you want to decrease it to 20ppm.\nDoes this mean that if you have high ammonia concentrations that you may have to double or quadruple your minimum ventilation rates indefinitely? Not necessarily. Though high minimum ventilation rates will help to keep ammonia levels to a minimum, they will also tend to address the root cause of ammonia: litter moisture.\nOver time, higher ventilation rates will tend to reduce litter moisture levels, which in turn will reduce the ammonia generation rate, which in turn leads to lower ammonia levels and an overall reduction in the minimum ventilation rate required to keep ammonia concentrations to a reasonable level.\nCan this method of ammonia control prove expensive? Sure. But, the fact is that once you have an ammonia problem it is expensive to solve. The key to keeping ammonia levels and heating costs to a minimum is preventing high ammonia levels in the first place. Between flocks, remove cake from the house as soon as the birds leave and ventilate the house to help dry the litter. Use a litter treatment according to manufacturer’s recommendations prior to chick placement.\nMost importantly, closely manage house moisture levels by monitoring the relative humidity of the air in the house. The relative humidity of the air in a house is an indirect measure of litter moisture. Generally speaking, the ideal relative humidity would be approximately 50 per cent. As the relative humidity of the air in the house climbs, so does litter moisture. Increased levels of litter moisture results in higher ammonia production rates.\nIf you allow the average relative humidity climb to 70 to 80 per cent, you will have wet litter and high ammonia levels, which can only be reduced to acceptable levels by a dramatic increase in minimum ventilation rates.\nTo avoid this situation, make relatively small adjustments to the minimum ventilation rates on a daily basis. Record the relative humidity in your houses each morning. If you see the relative humidity climbing, make larger increases.\nBottom line: don’t wait until you have an ammonia problem to try to solve it. After all, “an ounce of prevention is worth a pound of cure”.\nPosted on May 26, 2015', 'Munters circulation fans\n- create the optimal indoor climate.\nMunters has over the years manufactured the well-known Euroemme® EDC air blower. At the beginning of 2014, Munters ambition to achieve sustainable solutions that provide both environmental and economic benefits for its customers was presented. The European ErP Ecodesign Directive 2009/125 IEC, has led to the introduction of the new Euroemme® EDC-HE air-blowers series.\nThe new EDC18HE and EDC24HE fans belong to the new generation of high energy efficient, very robust and reliable air blower fan range, suitable for an extensive variety of professional applications in order to create the optimal indoor climate. The new design of the EDC18HE and EDC24HE air blowers consist of a unique round shaped housing composed of the air conveyor (Venturi) and a discharge cone on the outlet side. Inside the round shape housing, the 3-blade propeller is attached directly to the electrical motor, which is fixed on to the airflow rectifiers. This unique design renders the EDC18HE and EDC24HE as the ideal air blowers which can deliver high airflow capacity with low power consumption. The EDC18HE and EDC24HE have been designed, tested and developed in Munters R&D laboratory, while the performances were confirmed by tests performed in Munters Fan Test Chamber - designed and built according to the ANSI AMCA 201-85 specifications.\nAir circulation in Greenhouses\nThe major advantage of the use of air circulation fans in a greenhouse is to obtain a uniform temperature within the structure. In this way the incidence of foliar diseases can be greatly reduced. The air in movement removes moisture from the plant and delivers a drier microclimate near crops. In case leaf temperatures cool below the air temperature, the dew point is reached and condensation leads to the growth of disease organisms. The horizontal air flow will greatly reduce this difference.\nIn a greenhouse during the day, photosynthesis uses the carbon dioxide that is in the boundary layer of air next to the leaf. Movement of the air will replace the air near the leaves with fresh air having higher carbon dioxide content. This is how the vital function of the plants will be greatly improved, both health- and productivity wise.\nAir circulation in Poultry houses\nDuring the winter period when the ventilation is running in minimum capacity the poultry producers can have problems with temperature stratification in the poultry houses. The warm air is in the top of the building, as warm are is less heavy then cold air. Circulation fans are the solution to get the warm air lead down to the animal zone. That will reduce heat cost and give the animals a more dry bedding. The new circulation fans from Munters can reduce the energy consumption by about 30% compared to the standard circulation fans.\nAir circulation in Dairy buildings\nDairy cows cannot take the winter jacket of during the summer period and it cost in productivity. The solution will be to create high air velocity above the cows. The draft will cool the cows. Munters has a huge portfolio of different types of circulation fans; both traditional fans, the new energy friendly fans and breeze fans.']	['<urn:uuid:34d40b29-9717-40a5-ba6c-8ad6e74aa2b5>', '<urn:uuid:8d03beba-320f-4266-8785-0259d66a565e>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T19:47:23.148022	15	65	1526
92	What role does water play in health and mythology?	Water plays a significant role both mythologically and in modern health concerns. In the myth of Narcissus, water appears as a clear pool with silvery waters that becomes the site of Narcissus's demise, while Echo's story involves river gods and nymphs. In modern contexts, water manifests as humidity in indoor environments, which can significantly impact health. According to the Observatory of Indoor Air Quality, 15% of households have visible fungal infections due to moisture. Humidity from daily activities like cooking, showers, and even human breathing can promote the growth of molds and mites, and cause the degradation of materials releasing harmful VOCs. This creates a parallel between water as a source of both beauty and danger in mythology, and as both necessity and potential health hazard in modern indoor environments.	['The Myth of Echo and Narcissus\nNarcissus was conceived when his mother, the blue nymph Leiriope, was encircled with the windings of the streams of the river-god Cephisus, and Ravished.\nAfter the child was born, his mother took him to the prophetic seer, Teiresias, who said that if the boy never came to know himself he would live to a ripe old age.\nFor a long time this pronouncement seemed to be nothing but empty words. However it was justified by the outcome of events, his death proved its truth.\nJupiter spent much of his time playing around with the forest nymphs. When his wife Hera would come to the forest searching for him, Echo, a forest nymph herself, would come running up and chit-chat with Hera until Jupiter and the other forest nymphs could get themselves together.\nWhen Hera discovered that Echo was doing this she CURSED Echo so that she would never be able to initiate another conversation. Echo would only be able to respond back to another addressing her.\nNarcissus grew to be a very beautiful young man who was loved by everyone, men and women. But his soft young body housed apride so unyielding that none dared to touch him. He spent much of his time CHASING timid deer through the forest.\nEcho had seen this beautiful lad and had fallen in love with him. But unable to speak unless spoken too, she had to content herself with following him through the forest, hiding in the rocks.\nOne day Narcissus wandered from his friends and was lost in the forest. He called out, “Where are you?” Echo seized upon her chance to speak and called back, “Where are you?”\nNarcissus answered, “I’m here.” She echoed, “I’m here!” “Come here.” Come here!” “I’m coming.” I’m coming!” and they ran toward each other.\nWhen Narcissus saw that it was Echo he shouted, “Away from these embraces! I would die before I would have y ou touch me!” And he PUSHED her away.\nEcho was dejected and fell back into the woods where out of unfulfilled love she died. Her bones remained in the rocks but then even they disappeared. Only her voice remained in the forest and in fact it can still be heard today.\nNarcissus, still lost from his friends, wandered in the forest and was attracted to a clear pool with shining silvery waters. Its peace was undisturbed. Around it was a grassy sward, and it was sheltered from the sun by a large tree.\nAs he reached to quench his thirst he became enchanted by the beautiful reflection he saw. He fell in love with an insubstantial hope, mistaking a mere shadow for a real body. Spell bound by his own self; he lay down on the bank and gazed into his eyes.\nUnwittingly he desired himself, at once seeking and sought, himself kindling that flame with which he burned. He vainly kissed the treacherous pool and again and again pluged his arms deep in the waters, trying to clasp the neck he saw. But he could not lay hold upon himself.\nNo thought of food or sleep could draw him from the spot and he lay there stretched on the shady grass gazing at the shape that was no true shape, and by his eyes he was undone.\nFinally he turned to the forest and raising his arms cried out: “Oh you woods, have you ever seen a love as cruel as this? That one should love another as beautiful as this and be deluded by this love. I am in love, and see my loved one, but that form which I see and love, I cannot reach. Alas, I am the boy I see. I know it. My own reflection does not deceive me. I am on fire with love for my own self. What should I do? Grief saps my strength. I am cut off in the flower of my youth!”\nSlowly his strength was drained away but still he gazed at that same face. Now too weak to raise himself to reach into the silver pool, even his TEARS would disturb the water, so that the pool rippled, and the image grew dim.\nHe cried out: “Stay, do not desert one who loves you! Let me look upon you, if I cannot touch you. Let me, by looking, feed my ill-starred love.”\nNow worn and wasted, slowly consumed by his hidden fire, Narcissus begins to die. Nothing remained of that body which Echo once had loved.\nAs the unhappy boy signed, “Alas,” her voice took up his sigh and repeated, “Alas!” When he beat his hands against his shoulders she too gave back the same sound of mourning. “Woe is me for the boy I loved in vain!” When he said his last farewell, “Farewell” said Echo too.\nHe laid down his weary head on the green grass and death closed his eyes. He was received into the abode of the dead in the waters of the river Styx.\nHis sisters, the nymphs of the spring, mourned for him, and cut off their hair in tribute to their brother. The wood nymphs mourned him too, and Echo sang her refrain to their lament.\nWhen the pyre, the tossing torches and the bier were prepared, they went to move his body, but it was nowhere to be found. Instead of his corpse, they discovered in its place, a flower with a circle of white petals around a yellow center.\nAnd even today this flower can be found growing in the moist places and along slow moving streams. It is called narcissus.\nThe Myth of Echo and Narcissus\n|Back to Catalog|', 'Indeed, the quality of our indoor air is paramount, while we spend about 80% of our time in closed places. So the reflection and suggestions on how to improve air quality at home will not be redundant.\nThe concept of indoor pollution conventionally refers to non-industrial interior environments in enclosed environments such as dwellings, public establishments, places of care and education, office buildings, health and medical-social establishments or methods of transportation. Urban dwellers spend more than 80% of their time inside these confined spaces, so exposure to air pollutants is primarily associated with indoor environments, particularly habitat (about 50% of the time).\nThe indoor environment is a dynamic universe characterized by pollution from outside and pollution of internal origin, related to the building and the occupants. Indoor pollution is diverse, marked by biological, physical and chemical pollutants. Every day we eat a kilogram of food, we drink two kilograms (liters) of water, and we breathe an average of twelve thousand liters of air.\nIndoor pollution is a growing concern for several reasons. First, for a number of years, to reduce energy consumption, buildings are increasingly isolated, which greatly reduces air infiltration. In addition, the renewal of air is sometimes insufficient to evacuate indoor pollutants (lack of ventilation, malfunction and/or inadequate maintenance of ventilation systems). The buildings can be occupied by sensitive populations: this is the case of children, the elderly, and subjects with asthma or suffering from chronic respiratory or cardiovascular pathologies, for example. Finally, the health aspects associated with different indoor pollutants are better recognized; although it is still difficult to assess the health effects of associations or interactions of several pollutants.\nThe Question of the Quality of the Air\nIf the outside air is polluted, indoor air is often of poorer quality: prolonged exposure to harmful products, persistent humidity … The degradation of indoor air quality (at home but also at the office, school, shops, transportation …) has become a prominent public health concern, too often neglected.\nThus, according to a Harris Interactive opinion poll conducted in late 2013, in Europe as in the United States, 9 out of 10 inhabitants consider the quality of the air in their homes to be good even though they do not know how to evaluate it. in general, the indoor air quality is worse than outside: 2 to 5 times higher according to the US Environmental Protection Agency (EPA).\nThe effects of indoor pollution on health are only partially known, and it is only recently (the 1970s) that the French began to worry about its quality, hence the concern for asbestos (prohibited in 1997), carbon monoxide and lead paints.\nContrary to what one might think, the quality of the indoor air is not necessarily better than that of the outside air. A problem is all the more important for public health that we have the habit of spending about 80% of our time locked inside a building, whatever it is.\nIn 2014, NAFSA (National Agency for Food Safety, Environment, and Labor) estimated that the number of premature deaths caused by the most well-known indoor air pollutants was almost 20,000 per year. The air we breathe inside our homes may indeed be of lower quality than the one we breathe on the outside. The fault of some specific pollutants that emanate from the materials we use for the construction, decoration or furnishing of our homes. Also responsible, some of our activities: smoking, DIY, cleaning, etc.\n*** Biological pollutants\nInfectious agents (bacteria, viruses, toxins) come from living beings or can proliferate in some poorly maintained equipment (ventilation, etc.) Mold can lead to infections such as nosocomial invasive aspergillosis. Allergens come from pets, plants, molds, insects, and mites.\n*** Chemical pollutants\nIt is very often found in indoor air. Chemical pollutants are among the list of those found abundantly in the air we breathe at home:\n– Carbon monoxide (CO) is a colorless, odorless and deadly gas with a high concentration. Heating or hot-water heating appliances are disengaged when they are old or poorly adjusted. The symptoms of intoxication are headache, dizziness, blurred vision, nausea, weakness of the legs, drowsiness, syncope…\n– Volatile organic compounds (VOCs) – such as formaldehyde, benzene or organic solvents – released by paints, perfumes or felts. Some are carcinogenic. Volatile organic compounds or VOCs (formaldehyde, organic solvents, glycol ethers, hydrocarbons …): they are found in new or recently renovated buildings. They can be released by many materials: glues, paints, cleaning products, chemical perfumes, felts, but also so-called “natural” materials such as treated wood. Most are carcinogenic.\n– Semi-volatile organic compounds (SVOC) – such as phthalates and bisphenols – are found in biocides or coatings. Lead in old paintings causes anemia and irreversible damage to the central nervous system. Phthalates contained in plastic, they cause reproductive disorders.\n– Cigarette smoke, pesticides – insecticides, for example – etc. Let’s go back to tobacco smoke. Tobacco smoke contains more than 3,000 dangerous substances. It causes irritation of the eyes and respiratory tract, may cause seizures in asthmatics, cancer by active or passive smoking, etc. Hence the importance of airing if someone smokes in the room. Or better, to smoke outside. As a reminder, it is anyway forbidden to smoke in public places today.\n*** Other indoor air pollution agents\nWe must add to them some living organisms evolving in our houses and which can also be sources of pollution. Mold spores can cause infections. And allergens are regularly emitted by plants, insects, mites or pets.\n*** Particles and fibers\nDust, for its part, may be composed of various contaminants such as pollen or spores. Suspended in the air, they can be inhaled by the inhabitants of the house, especially during certain activities such as DIY or cleaning.\nDust contains particles, the smallest of which can be inhaled. It also contains smoke, soot, pollen, spores, and allergens. Fibers are particles emitted by certain materials (cellulose, hemp, sisal, asbestos, glass wool, rock …). Some activities such as DIY, cooking or cleaning emit significant amounts of particles and fibers that remain suspended in the air.\nAsbestos is infamous because it is very carcinogenic.\n*** Radioactive gases\nFinally, a pollutant may be less known: radon, a natural radioactive gas, colorless and odorless. It can accumulate in homes and even more in poorly ventilated cellars. Attention therefore in the granite basement regions such as Brittany or the Massif Central.\nRadon is a natural radioactive gas contained in soil and rock all over the world, at varying concentrations depending on the nature of the soil and the degree of containment of the site.\nIt is not perceptible (painless, colorless) and nevertheless dangerous, indeed, radon is a definite pulmonary carcinogen for humans and belongs to group I in the classification of the International Center for Research on Cancer (CIR). As such, “exposure to radon is one of the major risk factors in environmental health and this gas is significantly involved in the occurrence of a particularly common and serious disease: lung cancer,” says Hajo Zeeb, Professor Epidemiology and Zhanat Carr, Researcher at the World Health Organization (WHO).\nIn France, domestic radon is the second leading cause of lung cancer deaths after smoking.\nIt can accumulate in rooms where the renewal of air is very low (cellars, etc.)\nThis risk mainly concerns the regions with granitic or volcanic subsoil (Brittany, Massif Central, Corsica).\n*** Moisture, indirect pollution\nAs for humidity, if it is not intrinsically considered as polluting, it can be at the origin of degradation of the quality of the indoor air. Cooking, drying clothes, shower, mobile auxiliary heating and even breathing: the sources of moisture at home are numerous. However, it is not recommended to live in a dwelling that is too humid, because the humidity favors the development of mites and molds, for example, but also the release of VOC by degradation of the glues on the furniture.\nThe sources of moisture are not lacking: shower, cooking, washing, drying (up to 1 kg of water vapor for laundry) toilets, human metabolism (an adult produces about 55g of water vapor per hour), water damage…\nOf course, moisture is not directly polluting, but it promotes the proliferation of molds and mites. Moisture also degrades the glue on particle board, which releases formaldehyde.\nAccording to the Observatory of Indoor Air Quality (OIAQ), 15% of households have visible fungal infections, micro-organisms that can induce various respiratory diseases such as allergies, infections or toxic-infections. the occupants.\nHowever, a study led by Stéphane Moularat of the Scientific and Technical Center for Building Construction (SCBC) found that thyme essential oil and garlic extract, natural biocides, have been shown to be effective against fungal and bacterial overgrowth wood.\nThe Health and Economic Costs of Indoor Air Pollution\nAccording to a study by NAFSA in April 2014 entitled “Exploratory study of the socio-economic cost of indoor air pollutants”, indoor air pollution would kill nearly 20,000 French people a year.\nKidney cancer caused by the inhalation of trichloroethylene; leukemia attributable to exposure to benzene; lung cancers related to radon or passive smoking; carbon monoxide poisoning, cardiovascular diseases caused by particulate respiration, etc. : the cost of this mortality and the decline in the quality of life of sick people amounts to 18 billion euros, to which are added the loss of days worked, and costs of care, and those related to research, about 20 billion euros.\nNote that this is a low estimate for which scientists have retained only six pollutants for this first study: benzene, trichloroethylene, carbon monoxide, radon, particulates, and passive smoking.\nThe effects on our health can be immediate or long-term (chronic diseases or serious diseases). They depend on the nature of the pollutant and the quantities inhaled, the age of the subject, his vulnerability, his habits.\nAccording to the Observatory of Indoor Air Quality, the French lose on average 9 months of life expectancy because of the polluted air in their homes, up to 13 months in the most polluted countries according to the WHO.\nWhen the effects are immediate, they can result in discomfort, discomfort: irritation (eyes, nose, throat, etc.), uncomfortable odor, nausea, cough, asthma attacks, intoxications, suffocation … It should be noted that dust, because of its irritating power, amplifies the harmfulness of other pollutants.\nLong-term effects are less obvious: they occur long after exposure and it is difficult to determine their causes. Low intensity but prolonged exposure to pollutants can result in serious diseases: cancers, respiratory diseases (asthma or various disorders), aggravation of cardiovascular diseases … In France, the number of allergic people has doubled between 1980 and 2000 and 3.5 million people suffer from asthma.\nHow to Improve Air Quality at Home?\nAccording to the Harris Interactive opinion survey, most residents say they feel uninformed about the quality of the air in their homes, how to measure it and how to improve it. Here are some tips for reducing indoor air pollution.\nChoose products that contain as few pollutants as possible from the moment of purchase. Some information has been put in place to help you:\n* The VOC label (mandatory for all products sold in France by September 2013): indicates the level of emissions of volatile organic compounds, from a scale of A + (very low emissions) to C (strong emissions).\n* Eco-labels: preferably buy products bearing the European Ecolabel\n* The pictograms: they are there to alert you of possible dangers (flammable, toxic product, etc.)\n– Read the instructions carefully before using a product, and observe the recommended doses and safety instructions.\n– Install reliable and efficient appliances (heating, ventilation, etc.) and always ensure proper maintenance.\n– Beware of mixtures of products (DIY, maintenance …) that can cause dangerous emanations. For example, bleach, mixed with an acid (such as a descaling agent), releases chlorine, an irritant gas. Odor products (paints, glues, perfumes, cosmetics, deodorants) all release VOCs, so do not overdo it. Ditto for scented candles and incense, whose combustion releases carbon monoxide, very toxic.\n– Evacuate moisture: in case of infiltration or capillary rise in the walls, it is necessary to improve the water tightness of your house.\n– For ventilation, choose your VMC with the help of a professional, because the right choice depends on the context of each dwelling. It must be cleaned regularly to avoid clogging and remain effective.\n– Warning! A home cleaned with toxic products is not necessarily a healthy home!\n– Unfortunately, green plants do not significantly cleanse indoor air.\nThe best solution is to prevent pollution.\nIn Conclusion: Health Issues\nThe health impacts of indoor pollution are many, of varying nature and severity: they range from simple nuisances (feelings of discomfort, discomfort or confinement, olfactory nuisance, various non-specific symptoms, ENT, ocular, cutaneous or respiratory, headaches, tiredness, malaise, difficulty concentrating, etc.) but which can have an impact on absenteeism and productivity, up to diseases such as poisoning (oxycarbonates, lead poisoning, etc.), infections (legionellosis , viruses, tuberculosis, etc.), respiratory allergies (rhinitis, asthma, dermatitis, etc.) sometimes favored by the humidity of buildings. But also indoor pollution has risks more or less long term: chronic respiratory diseases, cardiovascular, tumoral, neurological, etc.\nAir pollution is now recognized as the main environmental risk for health in the world. It would be responsible in 2012, according to the World Health Organization, for seven million annual deaths worldwide (representing one in eight deaths), including 4.3 million for indoor pollution. Every European citizen is deprived on average of 8.6 months of life. Air pollution has been classified as a certain carcinogen for humans by the International Agency for Research on Cancer, such as outdoor air particles and exhaust effluents from diesel engines. Epidemiological studies show a strong causal link between fine particles (PM2.5) and cardiovascular and respiratory mortality.\nChronic particulate pollution (PM2.5) causes atherosclerosis, adverse pregnancy outcomes and respiratory illness in children. Long-term exposure to ozone affects respiratory mortality and death in people with chronic conditions. An increase in mortality, hospitalizations, and respiratory disorders is found with exposure to nitrogen dioxide. Exceeding WHO’s guideline value for PM2.5 translates into 19,000 premature deaths each year (including 2,900 for the nine French cities), including 15,000 cardiovascular deaths (1,500 of which occurred in nine cities). French) – European study APHEKOM (2012). In the most polluted cities in Europe, it is almost two years of life expectancy that could be earned if pollution could be reduced to the concentrations advocated by WHO (average life expectancy at thirty years for the nine French cities from 3.6 to 7.5 months).\nAccording to the Health Observatory of Ile-de-France, living near roads with high traffic density would be responsible for 16% of new cases of asthma in children and 16 to 29% of exacerbations of respiratory symptoms (asthma attacks, hospitalizations).\nAllergic diseases related to the air or food environment concern 25 to 30% of the French population and have a strong socio-economic impact (cost, absenteeism, quality of life). Their prevalence has doubled in twenty years in developed countries.\nIn its 2014 report on the health impact of exposure of the general population to pollens, NAFSA indicates that in epidemiological surveys carried out in France from 1994 to 2006, the prevalence of allergic rhinitis is estimated at most 7% in children aged 6 to 7, 20% in children 9 to 11 years old (with an awareness of nearly 27% of children with at least one aeroallergen), 18% among adolescents aged 13 to 14, 31 to 34% in adults. NAFSA specifies that certain chemical pollutants can modulate the allergic reaction by acting directly on the sensitized subjects, or by acting on the pollen grains, in particular on their wall and on their protein content.']	['<urn:uuid:67fe6d93-a984-4e1d-b841-d7aeeae88c4b>', '<urn:uuid:0112f9d7-bb6c-451b-80c0-5a1c66f690bf>']	open-ended	direct	concise-and-natural	distant-from-document	three-doc	expert	2025-05-12T19:47:23.148022	9	130	3544
93	How do cattle digest food efficiently, and what happens if E. coli contaminates their meat?	Cattle, as ruminants, have four stomachs with the rumen being the largest, where microbes break down fibrous roughage into nutrients. They can eat quickly and regurgitate food for thorough chewing. However, if their meat gets contaminated with E. coli O157:H7 from their digestive tract during slaughter, it can pose serious health risks to humans, particularly when the meat is not cooked thoroughly. This strain is especially common in grain-fed beef and can cause severe infections even in minimal amounts.	"['To feed cattle, it helps to understand how they digest their food. As ruminants, they have four stomachs, the largest of which is the rumen. Cattle can eat a lot of food in a hurry and then belch it back up for thorough chewing. Microbes in the rumen break down fibrous roughage into usable nutrients. Cattle can do well on a wide variety of forages, but there are some basic things a farmer needs to consider.\n1. Adequate Protein\nYoung, growing animals and lactating cows need more protein than mature dry cows; growth and milk production require protein. A baby calf gets adequate protein from milk. But after its rumen starts to develop, it needs to eat forages, and the forage should contain protein, especially after weaning.\nGreen grass and legumes (alfalfa, clover, and so on) have more protein than mature grass or straw. Green pasture or high-quality hay that was cut and put up at the right time—leafy and not overly mature—supplies adequate protein for weaned calves or lactating cows. Cattle do well on roughages but need protein to “feed” the microbes in the rumen so they can break down fiber. Mature cattle can eat straw if they have a protein supplement to facilitate rumen digestion.\n2. Adequate Fiber\nA ruminant is efficient at breaking down fiber, converting it into energy. The rumen functions best and the animal is healthiest when fiber is in the diet. Young calves (before the rumen is fully developed) do well on milk or just grain, digesting it in the abomasum, which functions like a human stomach. However, older animals need more fiber. If they are on lush green pasture or legume hay without enough fiber, they might be short on energy, their manure becomes liquid and, if the weather is cold, they can’t generate enough body heat to stay warm. Fermentation breakdown of fiber in the rumen generates heat and energy. In cold weather, provide more roughage such as mature grass hay or straw.\n3. Vitamins & Minerals\nGood pasture and high-quality hay will usually provide adequate vitamins and minerals—except salt—but in some situations, cattle need more. Hay that has been stored a long time is short on vitamin A. Feeds grown in deficient soils might be short on copper, selenium and other vital trace minerals. Always provide salt, and ask a knowledgeable cattleman or your county extension agent if you need to provide a mineral supplement to keep your cattle healthy.\n4. Clean Feed\nSome kinds of mold—especially in grain—can make cattle sick or even kill them. When feeding hay, make sure there’s no mold, sharp seed heads (foxtail, cheat grass, and so on), sharp objects, sticks, wire or other junk baled up in it. If feeding hay on the ground, feed on clean grass and not in the mud. In a pen, use a feeder or feed bunk, so cattle won’t lie on or poop on the hay; otherwise, they’ll waste it. Many cattle diseases are spread via pathogens in feces.\n5. Adequate Clean Water\nWater is just as important as food; cattle won’t do well without adequate water and might get sick if water is contaminated with toxins or disease pathogens. If cattle drink from a tub, tank or trough, keep it full enough, clean it regularly and make sure it doesn’t freeze in winter or get too hot or mossy in summer.\nThis story originally appeared in the January/February 2018 issue of Hobby Farms.', ""Beef Bacterial Contamination with E. Coli\nThere is no need to immediately panic when hearing the term E. coli. Escherichia coli, a rod-shaped bacteria, is commonly a part of the physiological gut flora of warm-blooded organisms, including humans. E. coli itself is relatively harmless if it stays where it belongs in the lower intestine of our digestive tract. The human body is colonized by E. coli right after birth and these colonies are then part of our digestive system, along with many other bacteria.\nThe trouble comes when this pathogenic bacteria migrates where it doesn't belong. Also, most strains of E. coli do not cause disease, but there are a few virulent strains that are dangerous to human health. These might cause gastroenteritis, urinary tract infection, neonatal meningitis, in some rare cases even bowel necrosis, mastitis, and gram-negative pneumonia. One specific strain of E. coli causes premature destruction of red blood cells which leads to hemolytic-uremic syndrome.\nIntestinal infections caused by E. coli are usually accompanied by diarrhea, in extreme cases resulting in dehydration or even death. But how does this bacteria get to our stomach from the outside? First, via water that has been contaminated with feces. Secondly you can become infected via food. The most likely contaminated food sources are raw milk and meat that has not been cooked properly, even just cross contamination during cooking. Fruit and vegetable that have been watered or cleaned with contaminated water could also make you sick. The dose required to contract the infection for this strain of E. coli is very low, only a miniscule amount of it is necessary to start an infection.\nJust as almost everything has two sides, we also know of E. coli strains that are somehow helpful in modern medicine. For example E. coli nissle, discovered in 1917, is proven to lower the risk of ulcerative colitis. E. coli K-12 was used during many important studies. One type of synthesized insulin was modified from E. coli bacteria and is helping type 1 diabetics who have a damaged pancreas and are not able to absorb insulin that is commonly used. Last but not least, there is a vaccine created with E.coli that is used against Lyme disease in USA.\nThe most important (and dangerous) strain of E.coli is O157:H7. It is present in animals digestive tracts and later found in feces, particularly of cows. Fecal matter can easily get into slaughtered meat in a factory because of careless manipulation. Then, all it needs is not enough cooking time and infection can start in the body of a consumer. Thorough cooking is very important with grain fed beef, as E.coli O157:H7 is mostly found in cows that were fed or finished with grains.\nLearn more about E. coli, the dangers and health consequences, but also find out if its necessary or better for your health to drop beef from your menu for good. Learn what you deserve to know to be able to make healthy and safe food choices for yourself and your family.""]"	['<urn:uuid:76f4024b-8dc2-4ac6-82f3-4b28090ba72e>', '<urn:uuid:2c739922-db45-47c5-ba43-31681a025afd>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T19:47:23.148022	15	79	1081
94	I've been hearing about different forms of marijuana dabs in my research - could you explain the distinction between shatter and budder concentrates in terms of their production methods and physical properties?	Shatter and budder are two distinct forms of marijuana concentrates that differ in both appearance and production. Shatter is a glassy, solid concentrate with an amber or honey color that's brittle and shatters easily. It's made through solvent extraction on dried marijuana, with the resulting oil heated at 90-100°F in a vacuum oven to purge excess butane. Budder, on the other hand, is a smooth, usually golden-colored substance with the consistency of very soft butter. It's made through solvent extraction as well, but the resulting oil is whipped during heating to create its signature texture.	['What Are Dabs and What Is Dabbing?\nWith the legalization of cannabis for medical and recreational use in many states, there are more ways to ingest greater amounts of THC than ever before. When marijuana is chemically concentrated into shatter, wax, hash oil, and similar other forms, these products are collectively called “dabs” and are consumed by “smoking dabs” or “dabbing.”\nThe use of marijuana concentrates may not be new, but dabs are currently exploding in popularity—especially among teens and young adults.¹ So, what are dabs, how are they used, and what are the dangers of producing and ingesting highly concentrated marijuana dabs?\nWhat Are Dabs Made Of?\nDabs come in many forms, but almost all are produced using butane oil to concentrate the active ingredients of the marijuana plant. This creates a product called butane hash oil or BHO. Dabs contain THC concentrations up to 80%, compared to an average of 10–15% in cannabis flower buds.1 This makes dab weed far more potent and potentially habit-forming than marijuana in its natural form.\nHow Are Dabs Related to Marijuana?\nAlthough dabs are produced from marijuana plant matter, dabs and weed are quite different. In addition to the fact that they look nothing alike—weed looks like plant material, dabs look like processed resins and oils—the concentrated result of extraction gives dabs a significantly higher THC content than that of weed.\nAs marijuana concentrates, dabs do have similar effects and risks to those of weed. For young users especially, marijuana and its derivatives can have lasting negative effects on their still-developing brains.2 However, due to the extremely high potency of the dab drug and the possibility of chemical contamination, smoking dabs may carry additional risks.\nTypes of Dabs\nDifferent processes can produce different types of marijuana dabs. Five of the most common forms are shatter, budder, oil, live resin, and wax. All these dabs have THC levels far greater than the plant material from which they are made.\nWhat it is: The oily base extract of marijuana—often referred to as Butane Hash Oil (BHO) or Honey Oil—is usually yellow/amber in color. Stored in vials or syringes without needles, it sometimes develops crystals for a chunky appearance.\nHow it’s usually made: Solvent extraction is performed by adding liquid butane gas as a solvent to dried or cured marijuana plant matter. Heat and vacuum pressure are then carefully applied to remove the remaining butane without changing the liquid consistency of the extract.\nWhat it is: A glassy, solid concentrate with an amber or honey color, as the name implies, it looks like amber glass, is brittle, and shatters easily.\nHow it’s usually made: Solvent extraction is performed on dried or cured marijuana, and the resulting oil is heated at temperatures around 90 to 100° F in a vacuum oven to purge excess butane.³ The temperature at which this purging occurs produces its glassy, solid appearance.\nWhat it is: A smooth, usually golden-colored substance with the consistency of very soft butter, it may be distributed in small jars or wax paper.\nHow it’s usually made: Solvent extraction is performed on dried or cured marijuana, and the resulting oil is whipped during heating to create the signature texture.\nWhat it is: A yellowish, semi-solid substance with the consistency of wax, some users consider this “real dabs” as the wax is “dabbed” onto the user’s equipment. Wax is usually stored in a shallow jar or a small container with a wide lid.\nHow it’s usually made: Solvent extraction is performed on dried or cured marijuana, and the resulting oil is purged of butane using a vacuum pump and little to no heat. Heat is kept low, and the product may be whipped to produce a thicker consistency without it turning into shatter or budder.\nWhat it is: This is a viscous, honey-colored substance that is considered to be more aromatic than other types of dabs.\nHow it’s usually made: Extraction is performed on fresh and frozen marijuana plant matter using either butane or carbon dioxide as a solvent. When made using carbon dioxide as the solvent, it is often called CO2 oil. The resulting extract is similar to BHO, but it is more sap-like and can be especially concentrated.\nWhat Is a Dabbing Rig?\nUnlike traditional means of smoking buds, marijuana dabs involve inhaling the vaporized product by heating the dab substance. This is generally done using a dab rig or vaporizer. These two pieces of equipment look very different.\nA rig is a specialized water pipe in which dabs are heated, producing a vapor that can be inhaled through a mouthpiece. An oil rig is a simple pipe that involves heating a nail—usually made from glass, titanium, or quartz—with a blowtorch, and then pressing it into the dab substance. More advanced rigs may use a battery-powered heating element for controlling the nail temperature.\nHand-held vape pens or “dab pens” are battery-powered products that contain an atomizing chamber in which the vapor is produced. Some vaporizers are limited to commercial vape oils, but others allow users to dab with any substance.\nIs Dabbing Dangerous?\nMany people do not consider marijuana a dangerous drug and may have used it in years past, making this assessment based on the milder effects of smoking marijuana. However, due to the highly concentrated THC levels and dangerous chemicals used in their production, dabs may carry increased health risks for individuals who use them.\nThe Side Effects of Dabbing\nBecause using dabs involves inhaling concentrated vapors with a rapid and intense effect, rather than the more gradual intoxication produced by smoking a joint, physical side effects like these may occur:2\n- Rapid heartbeat\n- Loss of consciousness or fainting\n- Blackouts or failing to remember periods of time after use\n- Crawling skin sensations\n- Hallucinations, anxiety, and paranoia\nSerious Health Risks of Dabs\nIf the butane is not purged sufficiently from the dab material during production, users are at risk of inhaling significant amounts of butane gas while dabbing, exposing them to additional health risks like irregular heartbeat, confusion, headaches, nausea, muscle weakness, and abdominal pain.3\nAny chemical used during processing could contaminate the final product and be directly inhaled. Research published in the Journal of Toxicological Science found that as many as 80% of cannabis concentrate samples showed contamination from remaining solvents or pesticides used in production.4 Studies have also shown that using high-temperature vaporization of naturally occurring substances called terpenes can also create toxic byproducts.2\nTolerance builds quickly with these concentrated substances, leading users to higher and more dangerous doses at a much faster rate than other types of cannabis use. The fast, powerful high of dabs can also lead highly intoxicated individuals to make life-threatening decisions or behave recklessly, especially if the drug has significantly affected their mental health.\nThe Dangers of Making Dabs\nOne of the greatest risks of dabbing lies in trying to make dabs at home. Many young people, especially, fail to realize how dangerous and highly flammable the process and materials are. If improperly handled, transported, or stored, liquid butane poses risks like an explosion, fire, severe burns, and asphyxiation. Errors in the chemistry of the process can also lead to unexpected reactions and injuries.\nThere have been numerous reported cases of people being seriously hurt during dab processing, including the following:\n- Creating hash oil and dabs is illegal. A 20-year-old man discovered this the hard way after his home-based lab exploded and caused second-degree burns to extensive parts of his body. Even though no one else was injured, he was arrested for manufacturing dabs as soon as he had recovered.5\n- In San Diego, another home lab explosion blew a garage door 20 feet off its hinges, melted the windshield of a car, and set a man on fire when dozens of stored butane cans exploded. Three other people suffered severe burns and had to be rushed to hospital, while the man responsible ran down the suburban street with his clothes and skin still on fire.6\nIs Dabbing Addictive?\nAny psychoactive substance can be habit-forming, and concentrated forms of the active ingredients of marijuana in dabs create the type of sudden and intense brain and body reaction that makes a drug addictive. The DSM5 recognizes cannabis as a drug that can be the cause of a substance use disorder and related withdrawal symptoms, along with caffeine, tobacco, alcohol, hypnotics, hallucinogens, and sedatives.7\nAbout Yellowstone Recovery – a Drug Rehab Center in Orange County, California\nMany people find that they need professional help to stop using dabs, and there are treatment programs available. At Yellowstone Recovery, we welcome all individuals who are struggling with substance use disorders. Our evidence-based therapies, residential and outpatient programs, and compassionate staff members can help you or a loved one stop using dabs or any substance that has triggered the disease of addiction.\nYellowstone Recovery in Orange County California may be the best way to stop using these addictive substances. As a 501(c)(3) non-profit center for addiction treatment for over 20 years, we have helped thousands of individuals break free from substance use and chart a new course in life. Call or contact us today for answers to your questions about the process and how you or a loved one can recover in a place of healing support.\nIntensive Outpatient (IOP)\nIntensive OutPatient treatment (IOP) helps people establish the foundations for lifelong sobriety…read more...\nOur California alcohol and drug addiction treatment program is predicated on restoring you to your ideal health. Detox is a crucial part…read more...\nPrimary Care – Residential Treatment\nThe first phase of our program is primary care. During this phase clients will be in a 30-day “blackout” period…read more...\nResidential Treatment – Extended Care\nResidential treatment extended care starts on day 31 and goes through day 90. This period is very important for a client in early recovery…read more...\nOutpatient treatment serves as aftercare for clients who have completed extended care (90 days of residential treatment)…read more...\nSober living at Yellowstone begins after the initial residential treatment portion of the program is successfully completed…read more...']	['<urn:uuid:4eead254-a468-49c2-9399-b903bb38758c>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T19:47:23.148022	32	95	1680
95	Have Jerry Leake and Sarah Lancman both performed with Italian musicians?	Sarah Lancman performs with Italian pianist Giovanni Mirabassi, while there is no mention of Jerry Leake performing with Italian musicians. The documents show Leake performing with various artists from other backgrounds including Persian, African, and Indian musicians.	['Instructor, World Percussion\nJerry Leake is an Associate Professor at Berklee College of Music and the New England Conservatory. He is co-founder of the acclaimed world-music ensemble Natraj, whose four recordings feature his diverse percussion talents. He also performs with Club d’Elf, R.A.R.E, Moksha, Another Realm, and the Agbekor Drum and Dance Society. He is featured on numerous CDs and has released six CDs of his own music. On tabla, he has accompanied Ali Akbar Khan, Steve Gorn, Sharafat Ali Khan, Kumkum Sanyal, Chitravena Ravikiran, Peter Row, Warren Senders, Purnima Sen, Nandkishor Muley, and kathak dancer Gretchen Hayden. He has written eight widely used texts on North Indian, West African, Latin American percussion, and rhythm theory.\nJerry has toured with Persian masters, Shahram and Hafez Nazeri; ongoing performances with flamenco virtuoso, Juanito Pascual, including the Museum of Fine Arts, 92nd Street Y and throughout N.E.; performances in JazzYatra, Bombay, India; Royal Ontario Museum, Toronto; Lake George Jazz Weekend, NY; PANAFEST, Accra and Cape Coast, Ghana; Oberlin College, OH; Prayojana Festival, Bangalore and Chennai, India; India-50 Celebration, Hatch Shell, MA; Guelph Jazz Festival, ON; New Year’s Portland ME; Festival of Black Music, MA; Saranac Lake Concerts, NY; U. of Texas, TX; Portsmouth Jazz Weekend, NH; Tsai Performance Center, MA; CB’s 313 Gallery, NY; Asian American Creative Music Festival, MA; CAV, RI; Institute of Technology, MA; Musikfest, PA; First Night, MA; Boston Globe Jazz Festival, MA; Community Arts Series, MA; Academy of Indian Performing Arts; MA; Anthony’s Southside, PA; Norwood Village Green Concerts, NY; Cambridge River Festival, MA; L/A Arts, ME; First Night Hartford, CT; Regattabar Jazz Festival, MA; Waterfront Park Concert Series, MA; Columbia University, NY; Moses Greeley Parker Concert Series, MA; Peabody Essex Museum, MA; Bowdoin College, ME; Berklee Performance Center, MA; MIT Kresge Auditorium w/Hands On’Semble, MA; Jacob’s Pillow, MA; Jordan Hall, MA; Jothi Raghavan’s Karnatak Theater Company, MA; Cultural Constructions, MA; David Maxwell Ensemble; Bob Moses; George Schuller; Les Miserables Brass Band, MA; Natalie Twigg Arts Foundation, MO; solo performance: Live Music Week, Emerson College Radio (WERS), MA; Lizard Lounge, MA: performances with John Medesky, Hassan Hakmoun, Mat Maneri, Brahim Fribgane, Bob Moses, Dave Tronzo, Duke Levine, Randy Roos, Gerry Leonard.\nJerry has been a guest lecturer at the U. of Wisconsin, U. of S. Maine, Gordon Col., Salem State Col., B.U., U. of Amherst, North Hampton Col., Dartmouth Univ., MIT, Harvard Univ., Holy Cross Col., Boston Col., Emerson Col., master class with Bob Becker (NEC) • In Chennai, India at Sri Jaya Ganesh Tala Vadya Vidyalaya School of Karnatak music; Kalekshetra Dance Foundation; two-day workshop and performance (sponsored by the U.S. Consulate) with the National Folklore Support Center and the Tamil theatre group “Koothu-p-Pattarai.” Program included live 2 hr. radio performance of solo percussion.\nJerry graduated from the Berklee College of Music where he studied jazz vibraphone with Gary Burton and hand percussion with Pablo Landrum. He studied tabla in Pune, India with Rajeev Devasthali, and studies in Boston with Todd Nardin and Koashal Anand. In Chennai, India, Jerry studied Karnatic rhythm theory and mridangam with T. K. Ramakrishnan. He continues to study African music with Dolsi-Naa Abubakari Luna of the Dagomba tradition (northern Ghana) and has studied Ewe music (southern Ghana) with Godwin Agbeli and David Locke, and balafon/djembe with the Coulibaly family in Burkina Faso.\nJerry teaches a summer graduate course on world percussion at the University of Southern Maine, and directed a 6-day rhythm theory seminar at the New England Conservatory in 2002. He presents percussion clinics and solo concerts throughout New England. Jerry is former president of the Massachusetts Chapter of the Percussive Arts Society and has been a composer and member of the Portland Symphony Kinder Koncert percussion ensemble since 1984.', 'Europas ledande jazztidskrifter och webbsidor som skriver om jazz har gått samman under namnet Europe Jazz Media (EJM). Målet är att arbeta objektivt med gemensamma mål för att uppmuntra och stärka intresset för jazz och sprida kunskap och medvetenhet om utvecklingen och mångfalden inom jazzen. Ett av de första initiativen är Europe Jazz Media Chart som uppdateras månadsvis. Listan är inte försäljningsrelaterad utan ställer samman uppgifter om vilka skivor de olika tidskrifterna och webbsidorna rekommenderar och tipsar om.\nPatrik Sandberg, OrkesterJournalen\nPat Metheny: From This Place (Nonesuch)\nFrom This Place is Pat Methenys first record as a bandleader since Kin (Nonesuch) from 2014. One element that really holds Methenys music together and makes it accessible to more than jazz fans is that it he makes tunes that really get stuck and have substance. Meheny is joined by Antonio Sanchez, Gwilym Simcock, Linda May Han Oh and guests Meshell Ndegeocello and Gregory Mairet on this epic storytelling album.\nAnna Filipieva, Jazz.Ru\nKruglov-Sooäär Quartet: Tchaikovsky (ArtBeat Music)\nThe second time the international Russian-Estonian quartet co-led by saxophonist Alexey Kruglov and guitarist Jaak Sooäär is focusing on the music written by Russian classical composers, since the 2015 The Mighty Five. This time, all music by none other than Pyotr Ilyich Tchaikovsky, who was not only a great composer, but also a fierce music critic, famous for trashing performers who did not play up to his strict standards. Three Estonians and one Russian did not stay overly true to the letter of Tchaikovsky’s scores, but nevertheless they comprehended the composer’s spirit: his crowd-pleasing aspirations come in the form of intensely controlled avant jazz rock fusion drama. Hopefully, Tchaikovsky the critic would be satisfied.\nChristine Stephan, Jazzthetik\nKadri Voorand: In Duo With Mihkel Mälgand (ACT)\nViktor Bensusan, jazzdergisi.com\nSarah Lancman: Parisienne (Jazz Eleven)\nSarah Lancman is unique in her compositions while she is walking firmly on the footsteps of her legendary namesake, Sarah Vaughan, in interpretation … Her musical partner, romantic Italian pianist Giovanni Mirabassi is a great asset along with her impeccable vocal texture.\nCim Meyer, Jazz Special\nSatoko Fujii Orchestra Berlin: Ninety-Nine Years (Libra Records)\nLars Mossefinn, Dag og tid\nGard Nilssen’s Supersonic Orchestra: If You Listen Carefully The Music Is Yours (Odin)\nMatthieu Jouan, citizenjazz.com\nClaudia Solal & Benoit Delbecq: Hopetown (Rogueart)\nA delicate illustration of poetry in movement, in sound, in colour. A quirky duo that confirms the timelessness of the piano-vocal duo.\nJan Granlie, salt-peanuts.eu\nLarge Unit & Fendika: Ethiobraz (PNL Records)\nWe speak Molde Jazz on the westcoast of Norway. There is certainly rain and wind in the air and it is on July 19, 2018, and on the stage of Theatret Vårt is a mastodont of a band led by drummer Paal Nilssen-Love. The Large Unit debuted in Molde at the festival in 2013, and after that they have toured and performed concerts all over the world, in Ethiopia, Brazil, Japan, USA and many places in Europe. They have done a number of recordings, and the collaboration with the Ethiopians in the band Fendika is in many ways a new way to go for the band. For here they do not play songs by the band members or by Paal Nilssen-Love, but mainly traditional music from Ethiopia, which is treated in a very unique and strong way by Large Units.\nOf course, this band constellation should be played wherever there is a big enough stage to present this connection of musicians, singers and dancers. For this constellation, I am convinced that it is helping to make the world a better place. It is music that gets smiles at the sourest of the laughing stock among us, the members of the Swedish Democrat Party, the American Republicans or Fremskrittspartiet in Norway. It is music that unites and creates understanding, which makes us as listeners happy at heart, and it is only to the extent that this reviewer does not pull up from the chair to take any free-range dance steps while ”firing it loose”.\nA wonderful live recording, with a raw band, which together with Ethiopian Fendika and Terrie Ex, really found the tone this evening in the Teatret Vårt in Molde. And I’m pretty sure the sun was shining from a clear sky as when we moved out of the theater in Molde that evening!\nAxel Stinshoff, Jazz thing\nWolfgang Haffner: Kind of Tango (ACT)\nLuca Vitali, Giornale della Musica\nChristine Abdelnour/Magda Mayas: The Setting Sun Is Beautiful Because Of All It Makes Us Lose (Sofa Music)\nMadli-Liis Parts, Muusika\nBrian Melvin/Matt Renzi/Mart Soo: Shapes (Yama Records)\nPaweł Brodowski, Jazz Forum\nMarta Wajdzik Quartet: Marta Wajdzik Quartet (Zadymka Multiproduction)\nThis a debut album by the latest discovery of Polish jazz. The 19-year old saxophonist is one of the best jazz artists of her generation, a winner of several national competitions, including last year’s Grand Prix of Młyn Jazz Festival, where she received a grant to attend the NYU Summer Jazz Improvisation Workshop in Steinhardt this summer.\nInfluenced by Kenny Garrett, she has a fully formed sound, improvisational flair and a mature approach to music. On her first album, filled up with her original compositions, she is supported by three of the leading musicians of the Polish jazz scene today. The lineup is Marta Wajdzik – saxophones, flutes, programming; Paweł Tomaszewski – keyboards, programming; Robert Kubiszyn – bass guitar, double bass; Paweł Dobrowolski – drums\nThe record has been produced and released by Bielska Zadymka Jazzowa, the great international jazz festival organized in Marta’s home city Bielsko-Biała in southern Poland.\nMike Flynn, Jazzwise\nWako: Wako (Øra Fonogram)']	['<urn:uuid:0d425abc-e626-427e-b791-92ea04d297a6>', '<urn:uuid:16319cd0-9ebb-49d5-9144-cfb0de4677b5>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T19:47:23.148022	11	37	1547
96	What's different between space and home water purification methods?	Space water purification on the ISS uses sophisticated technology including ion-exchange, electrolysis, and electrodialysis units to achieve an 85% recycling rate. The system removes calcium and magnesium through ion exchange, breaks down organic matter through high-temperature and high-pressure electrolysis, and removes ions through electrodialysis. The resin for ion exchange regenerates within the system, eliminating the need for replacements. In contrast, home water conservation methods focus on reducing consumption through practical measures like low-flow fixtures, water collection buckets, and reuse of greywater, rather than intensive purification. For example, shower water can be collected and reused for toilet flushing, and kitchen water can be reused for composting.	"['A new lighter, energy-efficient Japanese water recycling technology is being tested on the International Space Station.\nMission criticality in space\nThe International Space State (ISS) is a great example of a closed-loop system where water recycling is not a luxury or optional, but essential to keep astronauts onboard alive.\nBy filtering out impurities and contaminants from astronauts’ urine, sweat or even moisture from their breath, the produced potable water can be used again and again to drink, rehydrate and even bathe.\nAs a result, the systems and technologies used are \'mission critical\', as a failure would result in the ISS running out of water.\nDespite water recycling systems already in operation on the ISS, Japanese engineers are looking to improve the energy consumption and weight and size of the technologies.\nFine-tuning water recycling in \'Kibo’\nLater this year, “verification testing” of a new system will start in the Japanese experiment science module, nicknamed \'Kibo\', on the International Space Station.\nThe updated technology promises to be a lighter, smaller water recycling system that uses half the energy of existing technology.\nA joint research agreement started in 2011 between Kurita Water Industries and the Japan Aerospace Exploration Agency (JAXA), resulting in an order being placed in October 2015 to manufacture a new system.\nSince then, the two organisations have tested performance stability and verified durability under the conditions of use at the ISS to establish the specifications for the equipment.\nOne of the broader ambitions for the project is to build what is being called an independent Environmental Control and Life Support System (ECLSS) for manned space explorations in the future.\nHow does water recycling in space work?\nUsing the new system, a water recycling rate of 85 per cent of higher is achievable using ion-exchange, electrolysis and electrodialysis units, according to Kurita.\nTo purify urine to a potable water standard, the treatment system removes calcium and magnesium components contained in the urine through ion exchange, subjecting the organic matter to electrolysis, and removing the ions through electrodialysis.\nPersistent organic matter can be broken down by the electrolysis is carried out under high temperature and high pressure.\nFurthermore, since the resin for the ion exchange unit is regenerated within the system, it eliminates the need to replace consumables.\nHow does water recycling\nThe ISS can recycle approximately 6000 litres of water per year but also has 2000 litres of backup water in case of an emergency.\nThankfully astronauts are trained to use minimal amounts of water for daily activities.\nAccording to the European Space Agency (ESA), one astronaut uses about 0.5 litres of water per day for hygiene purposes. This compares to 50 litres typically used for showering and bathing back on earth.\nIn an infographic produced for World Water Day, ESA astronaut Frank De Winne said: ""On a given day, you drink a cup of coffee. The next day you drink the same cup of coffee again, and a few days later, you end up drinking your colleague\'s cup of coffee as well.""\nIn a popular video featuring former ISS commander Chris Hadfield, he said: “Before you cringe at the thought of drinking your leftover wash water and your leftover urine, keep in mind that the water that we end up with is purer than most of the water that you drink at home.”\nHe referred to the ISS as its own “self-contained environment” that’s a critical step towards living for long periods off of planet earth.', 'This page is intended to share methods we are experimenting with in order reduce our resource consumption and impact to the environment. We hope it stimulates a conversation amongst our followers in order to share ideas, provide constructive criticisms, and help people find practical methods of sustainability that work for their lifestyles. We will update this page with data we collect as soon as we have it available.\nWater Saving Techniques: Here’s a great water footprint calculator from National Geographic you can use to see how much water you use everyday.\nAccording to the EPA, “The average family of four can use 400 gallons of water every day, and, on average, approximately 70 percent of that water is used indoors.” That’s 146,000 gallons of water a year. According to Solomon in Water, The Epic Struggle for Wealth, Power, and Civilization, 2.5% of all Earth’s water is fresh; 2/3 of that freshwater is locked away in ice caps and glaciers; less than 3/10 of 1% of total freshwater is available in liquid form at the surface of the planet while the remainder is in permafrost, soil moisture, plants and animals, and water vapor while the most accessed source of water for humans is rivers and streams – these hold about 6/1000s of 1% of the total.\nIn The Bathroom\nOn Earth Day 2012, the city of Flagstaff was passing out low flow shower heads/faucets and shower timers. The great thing about these is they are adjustable from .5 to 1.5 gallons per minute (gpm). With the two minute shower timer, we are able to limit our water use from our showers and at that rate, each person uses 3 gallons of water per shower versus a standard shower head ( ~3.8 gpm) or 7.4 gallons per 2 minute shower per person, a savings for us of 4.4 gallons. That equates to 1606 gallons of water saved per year taking a 2 minute shower everyday. Imagine if the entire US population did this; that’s quite a savings!\nTo explore this even further, while lathering, shaving, or shampooing, we are able to turn the shower down to .5 gpm, which means we actually only run the shower at 1.5 gpm for half the time. That means we actually only use ~2 gallons per shower saving a total of 5.4 gallons/2 minute shower = 1971 gal/yr/person. Some people will even turn the water off while lathering, shampooing, and shaving which saves even more water! However, this is practical sustainability; some people like to keep the water flowing while showering, others don’t mind turning it off. What are your limits? Turning it off is best for conservation, but where is the balance for you? Can you reduce your time, the amount, or both?\nAgain, going further, we have buckets in each shower (below left). We collect the water while it is warming and as much as possible while showering. When the bucket is full, we use it to fill up the toilet tank instead of using fresh, clean water from the pipes. This helps to save more water just from flushing.\nWe also put 1 liter bottles in the toilet tanks (above right) to displace water with each flush and we also institute the “mellow yellow” philosophy, but capped it to 3 uses before the next flush. Based on the data we collected for one month of toilet use we saved the following:\n- Displacement with bottles: 109 gallons\n- Flushing with recycled shower water: 96 gallons\n- Not flushing every time: 1143 gallons\n- Total water savings in one month: 1348 gallons\nJust this week, we replaced the old toilets (3 gallons per flush) with dual flush, high efficiency toilets. The small flush is 1.1 gallons and the large flush is 1.6 gallons. Another savings in water and we will post more data as we collect it. Each toilet cost $100 and there will be $100 water credit to your water bill from Arizona in July of 2013 for installing high efficiency toilets, so save your receipts if you purchase one. Unfortunately, because we are on a well, we won’t be able to take advantage of that credit, but we believe having high efficiency toilets is the right thing to do so it is worth the $300 investment to reduce our impact on the environment.\nIn The Kitchen\nHere too, we installed a low flow, adjustable faucet. It makes it nice to turn it on all the way up when filling up reusable water bottles or pots for cooking, but we can turn it down during dishwashing to reduce our use. We also have three buckets to collect and reuse the water from the kitchen.\nYou’ll notice in the picture above, there is a compost bin on the window sill to reuse any food scraps or collect uneaten food. The major rinse bucket is the white bucket on the right, followed by the black wash bin in the middle, and finally the rinse bin on the left. We dump or rinse off the majority of any scraps into the white bucket, which preserves the quality of the water in the wash bin and prevents us from having to dump it before we are done washing all the dishes. We do have a dishwasher but do not use it in order to save water and electricity. We feel like it is an unnecessary luxury and also enjoy the time spent together in the kitchen washing dishes, talking, and sharing stories or jokes; again, practical sustainability and building relationships. After we wash all the dishes with biodegradable soap, we dump the water collected in the three bins onto our compost bins in a specific rotation so we do not “overwhelm” or inundate any area with too much water. This allows the environment to absorb that water in a timely fashion, and saves sending unnecessary amounts of water to the septic tank.\nEnergy Saving Techniques:\nHere’s some energy calculators:\nIn The Living Room\nWhen this house was built in 1974, sustainability did not seem to be on the forefront of the builder or designer’s minds. Although the view is beautiful, it is north facing, with large windows, and a very thin roof. We are losing lots of heat through the roof and windows, and get practically no direct sunlight to heat up the house. Next spring, we will explore more options of cutting down trees, installing solar, wind, or geothermal energy systems, but for right now, we are trying to work with what we have. We have a Lopi stove with a blower that has been helping, a propane furnace, but propane is extremely expensive. We keep the furnace set at 54 degrees, burn wood, and have a small electric heater. We have also applied plastic to the windows, put up insulation over our top windows, and have hung thermal drapes. All together, the house has been staying around a constant, balmy, 60 degrees. Understanding that it is winter, we add an extra layer of clothes and spend a lot of time around the fireplace working, talking, and eating which has really helped build a stronger community of people at the EcoRanch. Last, we put in weather stripping on all the doors in the house and put in insulation behind every outlet and light switch which surprisingly, can be quite drafty.\nFinally, we turned the water heater down to 115 degrees and insulated the hot water tank and all the hot water lines to reduce heat loss. We figured, what’s the point in heating water to a temperature that is too hot to touch so we need to waste more water by turning on the cold water to cool it off after we spent the money and energy to heat it up to an unusable temperature? Why not just turn down the temperature and save energy in the first place!\nAgain, we hope these ideas stimulate a discussion below or on our Facebook page and that you share your water and energy saving techniques with the rest of us. Remember, sustainability and conservation should not be an overwhelming process, but rather finding gradual, comfortable changes you can make to your lives. And, if all of us can make small changes, they will add up to very large, positive impacts to our shared commons – the environment we depend on everyday. Thanks and we look forward to your replies.']"	['<urn:uuid:9b2157e1-9761-4682-8eb1-ffb3724e1c92>', '<urn:uuid:05468f39-0f3d-413e-b7e4-b22f1c2cc420>']	open-ended	with-premise	concise-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T19:47:23.148022	9	105	1977
97	glacier melting effects qinghai tibetan plateau	The melting of glaciers in the Qinghai-Tibetan Plateau has severe consequences. It reduces water availability for Tibetan villagers, nomads, and billions of people who depend on major rivers like the Yangtze, Yellow, Ganges, and Indus. The melting also leads to the formation of dangerous glacial lakes that can cause destructive floods. Additionally, it affects the permafrost, endangering fragile ecosystems and endangered species like the Tibetan antelope.	"['Climate change is causing glaciers to melt throughout China, especially in the Qinghai-Tibetan Plateau, with devastating consequences for the billions of people who depend on them.\nThe Qinghai-Tibetan Plateau is unique in the world, not only for its extraordinary isolated landscapes. This remote region of soaring peaks, high steppes and glaciers cover a million square miles – about a quarter of China\'s land surface. Altitudes are uniformly high, averaging 4,000 meters (13,000 feet) above sea level. It\'s home to the Himalayas, including Mt. Everest, and many of the highest mountains in the world.\nA Greenpeace activist holds up a photo of the Middle Rongbuk Glacier taken in 1968 to compare it with the present. The glacier has deterioriated and retreated by 2km in just under 40 years.\nThe plateau is also home to the largest concentration of ice outside the north and south poles – including 47% of China\'s glaciers – earning it the name of the ""Third Pole"". As such, the Qinghai-Tibetan Plateau is critical to regulating water supply in Asia, as well as local climates and ecosystems. Glaciers here are the source of the major rivers in Asia, including the Yangtze, Yellow River, Mekong, Ganges and Indus.\nBut its glaciers are now melting at unprecedented rates. That melting is accelerated for the Himalayan glaciers by black carbon, which is the soot produced by cooking with biofuels such as wood, crop waste, and dung. Black carbon settles on to the glaciers, darkening the ice and snow and causing them to absorb more heat. As a result, the glaciers melt earlier in the summer, and for longer periods of time.\nImpacts of Glacier Retreat\nRetreating glaciers means that there is less water for the Tibetan villagers and nomads that live in the region. It also means less water for the rivers fed by the glaciers, and thus less water for the billions of people who depend on the Yangtze, Yellow, Ganges and Indus.\n79-year-old Ben Luo once owned 1,400 sheep and 140 yaks, but has lost her livelihood due to drought-degraded grassland. Now she lives off government rations and has no income.\nThe warming Himalayas also has wider implications for the global climate – it may, for example, weaken the Indian monsoon, which will in turn have other effects on worldwide rainfall patterns.\nIn 2007, Greenpeace undertook an expedition to Mt. Everest to investigate the effects of climate change. We documented the retreating Rongbuk Glacier, and spoke to lamas at the local monastery about the increased incidence of drought in recent years.\nIn 2005, Greenpeace also commissioned a report with Chinese university scientists on climate change\'s impact on the glaciers at the source of the Yellow River. The study, ""Yellow River at Risk,"" found that the region had lost 17% of its glaciers in the last 30 years, and that the ice now melted at rates 10 times faster than in the previous 300 years. It identified a domino chain of environmental effects, including dried lakes, advancing deserts, subsidence from melted permafrost, soil erosion and endangered species such as the Tibetan lynx and snow leopard.\nA nomadic family risks losing their livelihood as a result of desertification of their traditional grasslands. Eling Lake, Madou county, Qinghai.\nAs glaciers melt and retreat, the melting water builds up into lakes, dammed up by rocks and other debris left behind by the glacier. With rising temperatures, most meltwater lakes are growing at unprecedented rates and even expanding to join adjacent lakes. This results in precariously balanced lakes, teetering on the brink of overflow. A sudden earthquake, avalanche, landslide or other disruptive vents can rupture the dams, resulting in a massively destructive flood. This is what scientists call a GLOF, or a Glacial Lake Outburst Flood.\nA glacier melt lake, formed by the deterioration of the Middle Rongbuk Glacier, south of Everest Base Camp.\nGLOFs can be highly destructive: just imagine the effect of an inland tsunami of ice-cold, debris-filled water surging down inhabited valleys. Many Tibetan villages and nomads live in these high valleys, and they are ill prepared to deal with such flash floods.\nOne of China\'s most infamous GLOFs occurred at Sangwang Lake in 1954. Over 300 million cubic meters of debris-filled water flash-flooded the Nianchu River valley. The flow buried the valley in debris three to five meters thick. An estimated 691 people lost their lives and 16,000 more lost crops, land, houses, and livestock.\nPermafrost – No Longer so Permanent?\nAccording to the UNEP, the temperatures of the permafrost of the Qinghai-Tibetan plateau have warmed faster than anywhere else in the world except for the poles. A layer of earth that\'s completely frozen for part of each year, permafrost traps water in the topmost layer of soil, allowing plants to grow at altitudes that would otherwise be too harsh for life. The loss of permafrost would endanger the fragile ecosystems in these alpine regions, which support endangered Tibetan antelope.\nThe Qinghai-Tibetgan glaciers are not alone. All over the world, on every continent, glaciers are retreating, from the Pamirs in Pakistan to the Austrian alps, from New Zealand to Canada to the Andes in South America. And their retreating will have the same dangerous impacts on water supply, ecosystem balance and climate.\nGlacial melt is like the canary in the mine; it is up to us to heed the warning on climate change and decide to take action.\nLearn more about:\nHealth, Food and Water\nSea Level Rise\nHabitat Loss and Species Extinction']"	['<urn:uuid:16d71415-30cc-41ea-9967-4d2d728bef52>']	open-ended	direct	short-search-query	similar-to-document	single-doc	novice	2025-05-12T19:47:23.148022	6	66	913
98	golden eagle bald eagle nesting behavior feeding habits differences	Bald eagles build nests where they feed their chicks fish like gizzard shad and mammals, with remains littering the nest edges. Golden eagles in agricultural areas primarily hunt avian prey, especially waterfowl like mallards, with the male bringing prey while the female stays at the nest shading young from heat in early brood rearing.	"['Trespassing, traffic reported near bald eagles\' nest site\nFans of York County\'s most famous eagle family are so eager to see the pair of growing chicks that some are causing traffic problems on a rural back road near the Hanover-area nest site, officials said.\nThe best view of the eagles remains on the Internet, where the Pennsylvania Game Commission\'s website offers a free live-steaming ""eagle cam."" People from around the world have been keeping up with the raptors through the live stream video, according to Travis Lau, commission spokesman.\nEagle enthusiasts have also been making pilgrimages to Codorus State Park, according to park manager Warren Werntz.\nVolunteer birders set up spotting scopes at the park\'s classroom building — located behind the park\'s marina — to help people view the raptors from across Lake Marburg, he said.\n""Mostly there are birders there every day,"" Werntz said.\nThe nest is on private property near the park and can be seen with binoculars from the park\'s marina and sailboat area, according to Hanover bird expert Karen Lippy.\nBut cars are stopping on the narrow back road to see the nest, forcing traffic to drive around them, she said.\nSome cars drive too fast, according to Lippy, and others block private driveways when they stop.\nWarning signs: Werntz said park rangers have heard about vehicles blocking a private driveway.\n""We\'re going to be putting up signs in the driveway telling people not to (trespass),"" he said.\nSouthwestern Regional Police Chief Greg Bean confirmed he\'s aware of neighbors\' concerns about trespassing and parking.\n""It\'s a sparsely used road, so any new traffic does raise some eyebrows for residents,"" he said. ""Officers from the park and from our department have observed vehicles that are stopped along the roadway. We do ask them to move. ... It\'s not safe and it\'s illegal to obstruct the roadway.""\nBean said police will enforce trespassing laws against anyone who walks onto private property there without permission.\n""We\'ve been proactive and have increased patrols in the area,"" he said. ""Officers have been in contact with a dozen or so (people who) parked on the roadway or had trespassed on private property.""\n\'Sensitive time\': The neighborhood is so quiet that any vehicle slowing or stopping there sticks out like a sore thumb, according to the chief.\nLau said he understands why people are driving to the nest site.\n""They want to see the famous eagles,"" he said. ""But you don\'t want to be the one who\'s responsible for a nest failure. And that\'s the risk you run if you try to (get too close). It\'s a sensitive time in the nesting cycle.""\nDisturbing an eagle\'s nest can result in local and federal charges.\nLau urged people to watch the eagles either from Codorus State Park or on the eagle cam.\n""You\'re not going to get a better look than you get on the camera,"" he said.\nChick update: The two eagle chicks, which hatched March 24 and 25, are busy eating and sleeping, Lau confirmed, and appear to be doing well.\n""They\'re starting to walk around a little bit and explore the nest a little,"" he said.\nThe adult eagles have been stocking up on food, which litters the edges of the nest. That includes fish people are calling gizzard shad, Lau said, as well as some mammals.\nLook closely at the eagle cam and it\'s not hard to spot the multitude of flies.\nThe ever-dwindling remains of a skunk were still at the edge of the nest as of Monday afternoon.\n""Their sense of smell isn\'t very good,"" Lau said of bald eagles.\n— Reach Liz Evans Scolforo at email@example.com.\n""There are few sights more thrilling than a bald eagle at its nest or in action along a shoreline,"" the Pennsylvania Game Commission notes on its website, but acting responsibly around them is very important, as eagle nests are easily disturbed.\nDisturbing nests can endanger the eaglets in it and cause them stress, according to the commission, which urges people to enjoy eagles from a distance and to respect the birds\' space.\nThe commission website offers etiquette guidelines for watching eagle nests, which are reprinted here:\n* Stay at least 1,000 feet from an active nest, roost or feeding area. Use binoculars or a telescope to view the eagles from a distance.\n* Be quiet. If you must talk, whisper.\n* Try to be unobtrusive. Use your vehicle or boat as a blind, because eagles often are more alarmed by pedestrians than vehicles.\n* Avoid sudden movements — and movements directly toward the eagles or the nest — while on foot or in a vehicle or boat.\n* Don\'t make the birds fly, known as ""flushing."" Flushing an eagle off a nest may expose the eggs or young eaglets to cold or wet weather, or to a predator. It also wastes precious energy and may cause eagles to leave behind a valuable meal or abandon a nest that\'s under construction.\n* Pay attention to how an eagle reacts to your presence. If it appears agitated, vocalizes repeatedly or starts moving away, you are too close.\n* Respect restricted zones, which protect eagle nesting areas. You are breaking state and federal laws if you enter them.\n* Respect the privacy of the landowner. Don\'t tell everyone about a new eagle nest, as it will attract people who don\'t know proper etiquette as well as other unnecessary attention to a nest. If you unexpectedly stumble onto an eagle nest, or hear an eagle vocalizing overhead, leave immediately and quietly.\n— Courtesy of Pa. Game Commission.', ""This past summer I spent considerable time observing and photographing a nesting pair of golden eagles in southern Alberta. These golden eagles nest in the Rosebud River valley as it carves through a landscape dominated by cultivation, alternating between cliff and tree nest locations in some years. These eagles are unique because golden eagles are typically found at a relatively low densities in ideal habitat and normally absent from areas heavily disturbed by modern agriculture or development. My goal with this endeavor wasn't only to get some unique photos of nesting golden eagles but to observe the behavior of the breeding pair. I was particularly interested in what sort of prey they were exploiting and the remote camera was successful in answering some of my queries.\nThe eagles are year-round residents in the area with courtship activities and nest building initiated in mid-February. For the 2015 breeding season a cliff nest used previously was chosen and the female began incubation on April 1st. Early on I had planned on photographing nesting activities through the use of a remote camera setup that I could trigger from a long distance while observing with a spotting scope from a kilometer away. Golden eagles are particularly sensitive to disturbance near the nest site so it was imperative that a remote camera setup be used.\nThrough photos and feather collection around the cliff tops at the nest site it was clear that these eagles were primarily killing avian prey, mostly waterfowl. I watched the male eagle bring in numerous mallards and saw him on several hunting sorties while soaring thousands of feet over the river valley. Golden eagles are able to exploit a broad prey base and most studies show that eagles are extremely adaptable to whichever species is abundant in the area. In agricultural landscapes near the Rosebud River mallards are numerous during the spring nesting season and are particularly vulnerable to predation as they wander cultivated fields in newly formed breeding pairs. Because of his colorful spring breeding plumage the drake mallard is at much greater risk compared to the more cryptic hen. The male eagle had a taste for drake mallards but I also observed gull, raven, magpie, and a mule deer fawn brought into the nest.\nDuring the early brood rearing period the female eagle spends virtually all her time on or in areas immediately surrounding the nest site while the male brings in prey. Because the cliff nest is south facing the female spent much of her time shading her young from the summer heat. This south facing location makes for challenging photography conditions because of the harsh summer light but also from heat diffraction coming off the cliff face. The use of a telephoto lens during these conditions is an exercise of futility with heat waves rendering images painfully blurry. Cloudy days provided for the best photography conditions but we don't seem to get many in sunny southern Alberta. As the young grew to where they could thermoregulate with greater efficiency, the female began to hunt as well, at this point the pair would often hunt cooperatively, no bird is safe when a pair of eagles hunt together.\nLate morning in in mid-June the male eagle was soaring high over the territory while the female sat on a cliff top near the nest. I was watching through a scope from a kilometer away after setting up the camera early in the morning. I hadn't observed any cooperative hunting flights at this point of the breeding season but I was in for a treat this day. As I watched the female in the scope while keeping an eye on the soaring male I noticed the female take off the cliff top and disappear over the bank. I didn't think much of it until I looked up at the male in a full stoop hurtling towards the valley below. I hurriedly grabbed my binoculars to follow the male downwards as he raced towards the earth at breakneck speed. He leveled out and abruptly shot to the side as if fired from a slingshot before merging with a large white bird over the badlands. His prey dodged but the female now showed up chasing in tandem with the male as he used the momentum generated from the first stoop to dive upon the bird again. Initially I thought their quarry was a large gull but I soon determined they were chasing an osprey. This wasn't a nest defense scenario, they were intent on killing the osprey to eat. The osprey managed to dodge both eagles numerous times before escaping into a stand of poplar trees in the valley bottom to avoid the onslaught. It was an incredible show of agility and power that eagles possess, they are an impressive predator capable or air superiority compared to virtually all birds out there, I'd be lying if I said I was cheering for the osprey on this occasion.""]"	['<urn:uuid:1886c779-e7d6-4f15-b066-006f418452f2>', '<urn:uuid:d75e4e76-c82d-4ac7-9d84-772a95e10425>']	factoid	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T19:47:23.148022	9	54	1756
99	avg aminoethoxyvinylglycine treatment effects apple ripening storage firmness ethylene production experimental results	AVG (aminoethoxyvinylglycine) has significant effects on apple ripening and storage. In experiments with Golden Delicious apples, untreated fruit showed increased respiration and ethylene levels over 50 ppm after 10 days, while fruit dipped in 100 ppm AVG had delayed respiratory climacteric by six weeks with lower ethylene levels. At 200 ppm, respiratory increase was delayed nine weeks with only 4 ppm ethylene, and at 400 ppm, there was no respiration increase and near-zero ethylene even after nine weeks. After nine weeks, untreated apples were yellow and shriveled while AVG-treated apples remained green and firmer. In separate trials with Delicious apples, pre-harvest AVG treatment resulted in firmer fruit after storage - at 30 weeks, AVG-treated fruit was a full pound firmer than untreated fruit.	"['Ethylene in Fruit Physiology\nThe apple is a remarkable fruit. When harvested at optimum storage potential, it may store for as long as 12 months under proper temperature and atmospheric conditions. Correct harvest timing is based on defining the stage of maturity of the apple, which may be best determined by measuring fruit ethylene biosynthesis. Ethylene was discovered in the late 1800\'s. Like many other discoveries, it was somewhat accidental. Someone had observed several trees along a tree-lined boulevard had lost their leaves. It was soon discovered that there was a leak in one of the underground pipes carrying natural gas. It didn\'t take long to learn ethylene was the causative agent. Since then, hundreds of papers have been written on ethylene action, biosynthesis, promotion, inhibition, and molecular genetics.\nEthylene is found in most living tissues. In most terrestrial mammals, small amounts of ethylene are expressed with every exhaled breath, but in animals, ethylene is not considered a hormone as it is in plants. There are seven major effects of ethylene in plants which are: promoting ripening, inducing fruit abscission, inducing flowering, promoting seed germination, breaking dormancy, promoting root initiation and inducing vegetative dwarfing. In the science of fruit growing, most of these effects are put to use for the grower\'s benefit. This paper gives an overview of ethylene\'s involvement in the process of fruit ripening, how it is measured, and what the measurements mean.\nEthylene production is largely pre-determined in both time and amount by the genetics of the fruit and, depending on when the fruit blossoms and the climate, will determine to a large degree the development of the respiratory climacteric. During fruit development, respiration (i.e., the generation of carbon dioxide), which is a measure of metabolic activity, declines gradually throughout the season until several weeks before it ripens where it reaches what is known as a preclimacteric minimum. At this point, the metabolic functions of the fruit are in a near resting stage in preparation for a burst of metabolic activity signifying ripening. During ripening both carbon dioxide and ethylene increase significantly. The main developmental stage of the fruit is referred to as maturation, in which photosynthate is converted to starch. The ripening phase is when the starch is converted to sugar. Senescence is that stage in which the membrane functions break down due to degradation of lipid bilayers leading to cell damage and necrosis.\nOptimum harvest is a subjective measurement defined as a fruit with good keeping quality and good eating quality. If picked too early during the maturation stage, insufficient starch will be converted to sugar and the fruit will keep well enough, but the eating quality will be poor. On the other hand, if the fruit is picked too late, there will be insufficient starch and acid reserves for metabolic maintenance in storage, but the eating quality will be good. Therefore, the optimum timing is critical to the proper storage (and marketing) of apples. Researchers throughout the world have spent decades defining the optimum time of harvest for long-term CA storage and yet, as new varieties enter the marketplace, optimums must be redefined for each individual cultivar.\nAfter the respiratory preclimacteric minimum comes the respiratory climacteric. This is the point at which the fruit will generate a high, sustained respiration and ethylene biosynthesis. It occurs at different times for different cultivars. For example, a Yellow Transparent apple, which ripens in early July, has a very quick respiratory climacteric and may ripen within a few days. In fact, there are times when I have gone out one day and seen green fruit on the tree and two days later, these fruit are yellow and on the orchard floor. The respiratory climacteric of this apple is quick and peak is high. On the other hand, a Delicious apple, which ripens sometime in mid-September, has a somewhat longer climacteric period. This fruit may develop a respiratory climacteric over a period of 10 days, for example. This is advantageous to a grower because it allows time to harvest all of the fruit without being so concerned that the fruit will ripen early and even drop from the tree. Further, there are cultivars that ripen later, such as Fuji, Braeburn, or Granny Smith. These cultivars in turn have an even broader climacteric and the peak of respiration (when similarly measured) is lower than in the previous scenarios.\nConsider, for example, ethylene from the cultivar Braeburn. In one of our trials, fruit was picked over a period of 10 weeks beginning around August 15. If we measure the ethylene from the core by sampling with a syringe 24 hours after the fruit has been harvested, we see a very low ethylene production within the fruit tissue--generally less than 2 ppm. On the other hand, if we wait for four days before sampling the core gases of the fruit, we see significantly higher levels of ethylene reaching about 10 ppm at proper harvest. Waiting for seven days after the fruit is harvested before sampling, results in ethylene values in the 60-70 ppm range at harvest time. This suggests that delaying cooling (storage) of the fruit after it has been picked hastens the ripening process and, therefore, is deleterious to the condition of the fruit out of storage. This is important because often times a grower will harvest his fruit in bins and it may stay in his orchard or on the warehouse receiving dock before it is properly cooled. The longer the delay between harvest and storage the shorter will be the storage life and the poorer the condition of the fruit after it is removed from storage.\nSampling Fruit Ethylene\nClearly, the accurate measurement of ethylene in apples is vital to their proper storage and therefore their condition out of storage. But with so many fruit (100 million boxes of roughly 100 apples each in Washington State alone, and at least 4 times that throughout the world) the problem has become not the accuracy of sampling, but the unfathomable number of samples to assess.\nEarly efforts to analyze gas samples used a bulk approach. A bushel of apples would be placed in a large glass container and a sample of the headspace gas would be withdrawn and sampled. If each apple were very similar, this approach would accurately depict the physiological state of the individual fruit and, therefore, the proper regime for optimum storage. This is hardly the case. Data obtained from this method was qualitative, however, because there always exists the possibility that one apple could be riper than the others, thereby generating most all the ethylene. Thus, although all of the other fruit could have extremely low ethylene levels, the single mature apple would indicate the ""average"" ethylene was quite high. Because there are differences due to size, color, tree position, stress, variety, rootstock, soil, and a host of other factors, the best that can be said of this method is that it would always overestimate the average ethylene production.\nIndividual apples were then evaluated for ethylene production. The most common method was to insert an 18-gauge needle into the apple through the calyx end extending through to the core. A 0.5 to 1.0 mL sample of gas could be withdrawn and subsequently analyzed by gas chromatography. (Routine analysis of ethylene by megabore gas chromatography is sensitive to the ten parts per billion range.) By increasing the number of samples and examining the variability among samples, one could obtain a more accurate picture of the physiological state of the part of the orchard sampled. Harvest decisions could then be based on both the amount of ethylene being produced as well as on the degree of variation of samples. High ethylene blocks (experimental sites within and orchard) with high variability might well indicate that very few fruit are producing significant amounts, whereas, high ethylene blocks with little variability might indicate the entire block is ripening more evenly. The concern with this method lies in not knowing the physiological meaning of a gas sample taken from the fruit center. Again, there are differences due, for example, to size of seed cavity, amount of water in the fruit, amount of interstitial space, cell density and the amount of wounding from the needle insertion. Generally, any damage to the fruit tissues induces ethylene biosynthesis.\nThe method presently under design of assessing individual fruit ethylene is to place each apple in a small, 3-liter Plexiglas chamber with a low flow of scrubbed air. Micro-processors may sample the effluent gas as often as required 24 hours a day. This takes a single person about 1 hour to set up the system capable of analyzing gas composition of hundreds of apples--a process that manually takes about 2 to 3 minutes per sample. The greater number of samples increases the likelihood that apples will reach the consumer with optimum quality.\nWhat is needed is a method to assess quickly and accurately the ethylene generated by the whole apple, or one that uses part of the fruit that is most directly correlated with the physiological stage of fruit maturity. As for sampling a portion of tissue, it almost seems as though Heisenberg\'s principal is at work here in that any invasive action to the fruit will change its physiological stage and response.\nPerhaps the closest thing the apple industry has to assessing individual fruit maturity is in the fruit sorting process. In modern fruit packing warehouses individual fruit are mechanically sorted into weight categories and optically scanned for color sorting. Not far away are sensors for inline determination of firmness, sugar content and internal disorders. Practically everything can be determined about an individual fruit without damaging it. So far, however, only ethylene gives a meaningful picture of the physiological stage of ripeness. Thus, the problem remains; how does the apple industry nondestructively assess ethylene of 10 billion apples.\nInhibiting Ethylene Biosynthesis\nAbbott Laboratories has recently developed a new chemical tool for use in apple production. AVG (aminoethoxyvinylglycine) was discovered in 1976 by researchers at Hoffmann-LaRoche, but it wasn\'t until the last few years that it was made commercially available. This compound is a substituted amino acid produced by a streptococcal mold and is specifically targeted to inhibit the enzymatic production of ACC, which is the precursor of ethylene. In 1980, Williams in Wenatchee showed that AVG could be used to inhibit internal ethylene production thereby reducing fruit abscission, but could not be used to compensate for external ethylene, no matter the source. In 1981, we did some experiments on Golden Delicious apples, in which we took fruit that was harvested at optimum harvest time and dipped them in solutions containing 0, 100, 200, or 400 ppm AVG (Curry, et al.). These fruit were stored in Plexiglas chambers and the ethylene and carbon dioxide were measured daily for a period of seven weeks. Ater about 10 days, the untreated fruit showed an increased rate of respiration that coincided with the climacteric rise. For this treatment, ethylene exceeded 50 ppm for the remainder of the study. Fruit dipped in 100 ppm AVG had a respiratory climacteric delayed about six weeks. At seven weeks, ethylene levels were about 20 ppm and by the ninth week, ethylene had not exceeded 30 ppm. At 200 ppm, it took approximately nine weeks to see a slight increase in carbon dioxide and at that time, ethylene was approximately 4 ppm. Lastly, at 400 ppm, Golden Delicious apples dipped after harvest never showed an increase in respiration and the ethylene levels were near zero, even at nine weeks after harvest. At the end of the nine weeks, the control apples were yellow and shriveled whereas apples dipped in AVG were still green and noticeably firmer.\nIn another trial, Williams (1980) treated Delicious apples with pre-harvest applications of AVG approximately three days before harvest. These fruit were stored in regular cold storage and after three weeks, showed no difference from those that were not treated with AVG. By 16 weeks, however, differences began to occur. Firmness of untreated apples was 13.9 pounds, whereas those treated with 450 ppm AVG three days before harvest showed an average firmness of 15.2. Thirty weeks after harvest control fruit had a firmness of 12.8 pounds, whereas those treated with AVG were a full pound firmer.\nLow Ethylene in Controlled Atmosphere\nControlled atmosphere was developed to provide for the storage of apples as production increased throughout the United States and the world. Controlled atmosphere that pertains to fruit storage is defined simply as the regulation and control of major gases, such as oxygen and CO2, and minor gases, such as ethylene or other volatiles. This is in contrast to an uncontrolled atmosphere or regular storage where none of the gases are regulated, but may be monitored. This atmosphere also changes composition as the apples continue to respire, but it depends entirely upon the venting of the room. Before controlled atmosphere storage, apple storage was limited to the time that the apple would remain marketable (firm) in regular cold storage. This might be either in refrigerated units or natural caves in countries where energy is limited and therefore expensive. With new technology, however, apples may be stored for as long as 12 months. This complicates a grower\'s life in that in the 12th month of storage, an apple producer has to be considering three apple crops. The first would be the crop still in storage after 12 months. The second would be the current crop that is now on the tree waiting to be harvested. The third would be that which is on the tree in the form of a bud, which has a primordial flower and which is capable of sustaining winter injury. That\'s a lot of apples to consider at any one time.\nIn this brief paper, I have tried to indicate why ethylene is one of the main physiological parameter of apple development that must be considered for optimum fruit storage. The implications of ethylene measurement and regulation for fruit and storage are now being realized. Researchers with ARS have genetically engineered a tomato called Endless Summer, in which tissues have a reduced capability of ethylene synthesis and therefore ripen more slowly. More recently, another group of ARS scientists is advancing the pursuit of fruit that has selective tissue sensitivity to ethylene. This could result, for example, in the development of fruit in which only the hypodermis or epidermis would ripen producing enhanced color and flavor or aroma volatiles, but whose cortex would be much less sensitive to ethylene and therefore ripen much less slowly. This might lead to an apple which would ripen on the exterior, and be firm on the interior. Or perhaps we could turn the signal off completely and ripen by external ethylene only. Indeed, if it can be conceived, it can be accomplished. As our technology improves and our understanding of the ripening process increases, we may be able to develop fruit that will ripen exactly how and when we want them to.\nWilliams, M.W. 1980. Retention of fruit flesh firmness and increase in vegetative growth and fruit set of apples with aminoethoxyvinylglycine. HortScience 15(1): 76-77.\nCurry, E.A. and M.E. Patterson. 1993. Controlling ethylene biosynthesis with natural compounds. Proc. Wash. State Hort. Soc.: 312-313.\nDr. Eric A. Curry, Plant Physiologist\nUSDA, ARS Tree Fruit Research Laboratory\n1104 N. Western Ave., Wenatchee, WA 98801\n14th Annual Postharvest Conference,\nMarch 10-11, 1998']"	['<urn:uuid:2a8ecf04-7e52-4a76-9ded-49561e851a48>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T19:47:23.148022	12	123	2566
100	What happens inside your body that makes a person develop swelling and puffiness when they have focal segmental glomerulosclerosis?	The swelling occurs because the kidneys leak protein, resulting in low protein levels in the blood. This causes fluid to shift out of vessels into tissues, leading to puffiness (medically called oedema) particularly around the eyes, abdomen, feet and legs, as well as weight gain.	"[""This information from Great Ormond Street Hospital (GOSH) is about Focal segmental glomerulosclerosis (FSGS). FSGS is not one but many different diseases.\nHow does the urinary system work?\nThe urinary system consists of the kidneys, ureters, the bladder and urethra. The kidneys filter the blood to remove waste products and produce urine. The urine flows from the kidneys down through the ureters to the bladder, where it is stored until we go to the toilet. It passes through another tube called the urethra to the outside when urinating (peeing).\nThe kidneys contain millions of tiny filtering units called glomeruli. As blood passes through the tubes in the glomerulus, waste products pass through the walls of the tubes to form urine. Blood cells and other things such as protein cannot pass through the walls because they are too big.\nWhen someone has focal segmental glomerulosclerosis (FSGS), some of the glomeruli are damaged and become scarred. This stops them working properly, so that protein can pass through the walls of the glomerulus and into the urine. This means that the child develops nephrotic syndrome.\nWhat are the symptoms of FSGS?\nThe main symptom of FSGS is the development of nephrotic syndrome, that is, swelling due to increased fluid in the body. Nephrotic syndrome occurs because the kidneys leak protein. This results in low protein levels in the blood, which means that fluid is shifted out of vessels into tissues. This causes puffiness, particularly around the eyes, abdomen, feet and legs, and weight gain. The medical word for this puffiness is oedema. As the kidney leaks protein, the urine’s appearance changes so that it appears foamy, which is a sign of proteinuria.\nHow is FSGS diagnosed?\nThe doctor will ask lots of questions about how the child became ill and examine them. Various tests will be needed to confirm or rule out FSGS:\n- Urine tests to see how much protein is present in the urine.\n- Blood tests to check general health, and possibly genetic tests, which we will discuss with the family.\n- Kidney biopsy to see if any areas contain scarring.\nOther tests may also be needed if the results of these tests are not completely clear.\nWhat causes FSGS?\nThere are many different kinds of FSGS. Some are caused by inherited diseases and can thus affect several members of the family. In other cases the cause is unknown and it might be caused by an autoimmune reaction, where the body starts to attack itself after mistakenly recognising itself as the ‘enemy’.\nHow common is FSGS?\nWe do not yet know exactly how many children each year are affected by FSGS but at GOSH, we have looked after at least 66 children with this condition over the last 15 years.\nReports vary but it would seem that between seven and 10 per cent of kidney biopsies to investigate proteinuria lead to a diagnosis of FSGS. It seems to be more common in certain racial groups but again we do not yet know why.\nHow is FSGS treated?\nNormally the first sign of FSGS is nephrotic syndrome, where protein is continuously lost in the urine. The child will start with the standard treatment for nephrotic syndrome, which is steroid medicine.\nMany cases of FSGS do not respond to steroids, so are called 'steroid resistant'. In those cases, we try immunosuppressive medicines that damp down the immune system. Unfortunately a large proportion of FSGS does not respond to this kind of medicines.\nWhether the child is taking steroids or immunosuppressive medicines, families may need to postpone some of their child’s immunisations while taking the medicines. It should also be reported to the family doctor (GP) if a child comes into contact with chicken pox unless he or she has already had it. Other precautions are included in our medicines information sheets.\nWe also use a group of medicines called ACE (angiotensin converting enzyme) inhibitors and ARBs (angiotensin receptor blockers) to reduce the protein loss and blood pressure. Diuretics (water tablets) are also prescribed to reduce the puffiness.\nIt is important that the amount of protein lost in the child’s urine is monitored closely. We may ask families to do these measurements every day at home, using a dipstick. We would explain how to do this. These measurements will need to be recorded alongside details of the child’s steroid dose and any other medicines for each day, which should be brought to each clinic appointment, as they are a good indicator of how treatment is working.\nDiet also plays a part in treating FSGS. We may suggest reducing the amount of salt used at home. Our dietitian will explain exactly what needs to be done and will be available for advice afterwards.\nWhat is the outlook for children with FSGS?\nFSGS is a chronic condition. An infant's health may improve for a while, but many children do not respond very well to the treatment. In others who respond the symptoms might come back. If this happens, he or she will have to start taking the steroid or immunosuppressive medicines again.\nOver time, more areas of the glomeruli become scarred, which affects the child’s overall kidney function. FSGS may eventually progress to irreversible kidney failure, although this could take many years.\nIn our experience of caring for children with FSGS at GOSH, three in 100 children died and a quarter of patients had irreversible kidney failure requiring kidney transplantation having been followed up for 10 years.\nOnce a child’s kidney function has dropped to a level where it is affecting his or her daily life, we would discuss dialysis and transplantation. This is called renal replacement therapy (RRT) and aims to keep children well until a transplant is possible. There is unfortunately a risk that the disease will come back in the transplanted kidney.\nOur patients provide us with a range of extraordinary stories. Catch up with their their own accounts in which they describe how they battle the most complex illnesses.""]"	['<urn:uuid:79d7a6a3-1206-48bd-945b-06b977cd1f71>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T19:47:23.148022	19	45	998
