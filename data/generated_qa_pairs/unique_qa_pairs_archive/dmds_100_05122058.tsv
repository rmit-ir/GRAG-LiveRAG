qid	question	answer	context	document_ids	question_factuality	question_premise	question_phrasing	question_linguistic_variation	question_multi-doc	user_expertise-categorization	generation_timestamp	question_length	answer_length	context_length
1	What prevention strategies exist for structural damage underwater?	For well structures, prevention strategies include good centralization of casing, proper clearance around pipes, thorough hole-cleaning, using chemicals to break mud cakes, and correctly designed displacement rates. For coral reefs, prevention involves installing navigational buoys to mark reef locations in vulnerable areas - for instance, since buoys were installed in Culebra, Puerto Rico, no incidents have been reported.	"['Micro annuli in cement are a constant problem in oilfield well cementing. In this article, I will look closer at the most common reasons and discuss how to deal with them.\nIt is one numerous well integrity problem that often results in production reduction and expensive remedial costs.\nPhoto by Oliver Paaske\nWe all know the consequences of a well not properly sealed off. As we speak, there are thousands of leaking wells on the planet, and they are a challenge in many aspects.\nA significant number of these wells are shut off despite having years of production ahead of them.\nIn other words, instead of losing money, the owners of these wells would see a significant payback of their investment if they had a way to stop the leakage (other than shutting off the well completely).\nWhat are micro-annuli?\nMicro annuli are micro-separation between pipe and cement caused by a change in temperature or pressure during the cementing setting process, or after the cement is set.\nMicro-annulus between cement and formation can also occur. Although less common, it can result in a lack of bonding to the casing/formation and a possible communication channel.\nHow do micro-annuli occur?\nIt is during the setting phase of the cement job one needs to pay extra attention to the behavior of the cement. The slurry is constantly exposed to forces and obstructions of different character while being in the wet state. It is under these conditions that micro-annuli are most likely to occur.\nThese are the most common causes:\n1. Cracks in the cement\nCracking under stress. The forces that occur during setting can not only lead to de-bonding, it may also create micro fractures in the cement. Shrinkage of cement over time can do the same.\n2. Inadequate cleaning\nFailure to remove layers of mud or other materials on the wall of formation or the annulus, resulting in weak bonding.\n3. Expansion and contraction\nDynamics related to this can lead to debonding from the formation. It could be just gradual pressure reduction under construction of the well, following a decrease in reservoir pressure. It could also be a sudden change when displacing the well from mud to water or stresses seen during a frac job.\n4. The hydrostatic head\nWhen you set cement in a zone with less hydrostatic head on the formation, high gas pressure from the bottom may cause the gas to migrate during the setting process. The gas will find its way through the mud cake, which in turn will make a fluid column in the cement.\nYou may also like our blog article ""Materials for plug and abandonment of oil and gas wells"" and our case ""Sustained casing pressure"".\nDealing with micro-annuli\nThere are a few things you can do to prevent micro-annuli. But even the best preparations would not be enough to avoid the problem entirely. You must have a backup plan as well.\n- Good centralization of casing\n- Clearance all around the pipe to place the cement properly\n- Good hole-cleaning before cementing\n- Chemicals to break the mud cakes and to separate cement from mud – spacers\n- Correctly designed and executed displacement rates to replace the mud with cement efficiently.\nOnce the damage is done - what do you do?\n- Use small-particle cement\n- Try to squeeze ultra-fine cement in\n- Particles are very often too big to penetrate the annuli effectively\nIt happens all the time: On a given job a certain percentage of the gaps will be filled and sealed, but there will be quite a few that are not. Then you have an ongoing process in which you try to get to the problem areas and seal them off.\nIn many cases, it is simply impossible due to the particle size of the cement.\n- Use low-viscosity resin material\n- Particle free - you can get it in everywhere you can pump water. A more viable choice for sealing off micro-annuli\n- Less prone to bridging off. It\' s not going to block the entrance or access path.\nReading tip: Resin curing process.\nMicro-annuli will be an ever-present issue in the cementing process. However, proper planning that involves process and material selections will minimize their occurrences. Proper contingencies should also be in place to deal with problems areas immediately if a micro-annuli is detected.\nDid you enjoy this article? Please add your comments below.\nYou may also download this free guide:', 'Growing less than a quarter inch per year, the elaborate coral reefs off the south coast of Puerto Rico originally took thousands of years to form. And over the course of two days in late April 2006, portions of them were ground into dust.\nThe tanker Margara ran aground on these reefs near the entrance to Guayanilla Bay. Then, in the attempt to remove and refloat the ship, it made contact with the bottom several times and became grounded again. By the end, roughly two acres of coral were lost or injured. The seafloor was flattened and delicate corals crushed. Even today, a carpet of broken coral and rock remains in part of the area. This loose rubble becomes stirred up during storms, smothering young coral and preventing the reef’s full recovery.\nNOAA and the Puerto Rico Department of Natural and Environmental Resources have been working on a restoration plan for this area, a draft of which they released for public comment in September 2014 [PDF]. In order to stabilize these rubble fields and return topographic complexity to the flattened seafloor, they proposed placing limestone and large boulders over the rubble and then transplanting corals to the area.\nThis is in addition to two years of emergency restoration actions, which included stabilizing some of the large rubble, reattaching around 10,500 corals, and monitoring the slow comeback and survival of young coral. In the future, even more restoration will be in the works to make up for the full suite of environmental impacts from this incident.\nCaribbean Cruising for a Bruising\nUnfortunately, the story of the Margara is not an unusual one. In 2014 alone, NOAA received reports of 37 vessel groundings in Puerto Rico and the U.S. Virgin Islands. About half of these cases threatened corals, prompting NOAA’s Restoration Center to send divers to investigate.\nAfter a ship gets stuck on a coral reef, the first step for NOAA is assessing the situation underwater. If the vessel hasn’t been removed yet, NOAA often provides the salvage company with information such as known coral locations and water depths, which helps them determine how to remove the ship with minimal further damage to corals. Sometimes that means temporarily removing corals to protect them during salvage or figuring out areas to avoid hitting as the ship is extracted.\nOnce the ship is gone, NOAA divers estimate how many corals and which species were affected, as well as how deep the damage was to the structure of the reef itself. This gives them an idea of the scale of restoration needed. For example, if less than 100 corals were injured, restoration likely will take a few days. On the other hand, dealing with thousands of corals may take months.\nNOAA already has done some form of restoration at two-thirds of the 18 vessel groundings with coral damage in the region this year. They have reattached 2,132 corals to date.\nWhat does this look like? At first, it’s a lot of preparation. Divers collect the corals and fragments knocked loose by the ship; transport them to a safe, stable underwater location where they won’t be moved around; and dig out any corals buried in debris. When NOAA is ready to reattach corals, divers clear the transplant area (sometimes that means using a special undersea vacuum). On the ocean surface, people in a boat mix cement and send it down in five-gallon buckets to the divers below. Working with nails, rebar, and cement, the divers carefully reattach the corals to the seafloor, with the cement solidifying in a couple hours.\nProtecting Coral, From the Law to the High Seas\nNearly a third of the total reported groundings in Puerto Rico and the U.S. Virgin Islands this year have involved corals listed as threatened under the Endangered Species Act. In previous years, only 10 percent of the groundings involved threatened corals. What changed this year was the Endangered Species Act listing of five additional coral species in the Caribbean.\nAnother form of protection for corals is installing buoys to mark the location of reefs in areas where ships keep grounding on them. Since these navigational aids were put in place at one vulnerable site in Culebra, Puerto Rico this summer, NOAA hasn’t been called in to an incident there yet.\nBut restoring coral reefs after a ship grounding almost wouldn’t be possible without coral nurseries. Here, NOAA is able to regrow and rehabilitate coral, a technique being used at the site of the T/V Margara grounding. Stay tuned because we’ll be going more in depth on coral nurseries, what they look like, and how they help us restore these amazingly diverse ocean habitats. [Update: Read how NOAA uses coral nurseries to restore damaged reefs.]']"	['<urn:uuid:ab768a97-70fd-4635-8f13-43532024ba24>', '<urn:uuid:80d403e7-bb6a-49e9-915c-5140b9c8d1a5>']	factoid	direct	concise-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:58:04.490895	8	58	1529
2	world animal day history first organization date location attendance	World Animal Day was first organized by Heinrich Zimmermann, the German Writer on March 24, 1925, in Berlin. More than five thousand people attended this event. Later in 1929, the event was moved to October 4, and in May 1931, the International Animal Protection Congress in Florence, Italy, officially designated October 4 as World Animal Day.	"[""World Animal Day 2023: October 4 - History, Theme, Significance, Quotes\nOct 05 2023\nWorld Animal Day 2023: World Animal Day is celebrated on 4th October of every year. This day is observed to raise awareness about the welfare standards of the animals. Generally, Animals are multicellular, eukaryotic organisms in the Animalia biological kingdom. Animals are important to support the web activity in functioning the ecosystem. Animals are generally classified into two types. They are domestic animals and wild animals. The animals which are trained to help people are known as domestic animals. These domesticated animals have been selectively bred and genetically adapted over generations to live alongside humans. An animal that lives on its own without help from people is known as a wild animal. These animals find their own food, shelter, water, and all other needs without the help of humans. Animals are important to our environment and it plays a major role in balancing the ecosystem. This article will help you to know a detailed explanation of World Animal Day History, World Animal Day 2023 Theme, World Animal Day 2023 Significance, World Animal Day 2023 quotes, and some important FAQs related to World Animal Day 2023.\nWorld Animal Day 2023: History\n- World Animal Day was first organized by Heinrich Zimmermann, the German Writer on March 24, 1925, in Berlin. More than five thousand people attended this event.\n- Later the event of World Animal Day was moved to October 4 in the year of 1929. Every year Zimmermann worked hard for the promotion of World Animal Day.\n- In May 1931, the International Animal Protection Congress in Florence, Italy, adopted a resolution accepted his proposal, and announced the 4th October as World Animal Day.\n- The Finnish Association of Animal Protection Associations (SEY) organized various events on Animal Week and distributed material to the schools.\nWorld Animal Day 2023: Theme\n- Every year we celebrate World Animal Day with a special theme. This year, World Animal Day 2023 Theme is “Great or Small, Love Them All”.\nWorld Animal Day 2023: Significance\n- The World Animal Day is celebrated to protect their welfare and create awareness about the welfare of the animals.\n- World Animal Day is organized in different places in different ways all over the world. Many events and activities were organized regarding the animal's welfare.\n- On this day we may raise awareness about the laws made for animals and stop the cruelty and harassment against animals.\n- There are many species that are in the state of extinction so we have to save animals from the extinction state and protect them.\nWorld Animal Day 2023: Quotes\nHere we are providing some quotes that are related to World Animal Day,\n- “As custodians of the planet, it is our responsibility to deal with all species with kindness, love, and compassion. That these animals suffer through human cruelty is beyond understanding. Please help to stop this madness.”\n- “Killing animals for sport, for pleasure, for adventure, and for hides and furs is a phenomenon which is at once disgusting and distressing. There is no justification in indulging in such acts of brutality.”\n- “Non-violence leads to the highest ethics, which is the goal of all evolution. Until we stop harming all other living beings, we are still savages.”\n|Current Affairs - Daily, Weekly, Monthly\n|English Language Free PDFs\n|Quantitative Aptitude Free PDFs\n|Free Bundle PDF\nWorld Animal Day 2023: FAQs\nQ: When is World Animal Day celebrated?\nA: World Animal Day is celebrated on the 4th October of every year.\nQ: Who organized World Animal Day?\nA: World Animal Day is organized by Heinrich Zimmermann, the German Writer.\nDaily Current Affairs & Quiz 2023-24: Latest Current Affairs, Quiz & News For Today\nMar 02 2024\nNIACL Assistant Prelims Shift Timings 2024: Time Duration, Reporting Time And Exam Instructions\nMar 02 2024\nNobel Prize Winners 2023 PDF: Nobel Prize Winners Across Various Fields\nMar 01 2024\nGeneral Awareness Smart Analysis\n- Get Weekly 4 set Test\n- Each Set consist of 50 Questions\n- Compare your progress with Test 1 & 2 & Test 3 & 4\n- Deep Analysis in topic wise questions\n- Bundle PDF Course 2023\n- Ultimate Bundle PDF Course 2022\n- Grand Bundle PDF Course 2021\n- English Bundle PDF Course\n- Insurance & Financial Market Awareness Bundle PDF Course\n- Descriptive Papers for Bank & Insurance Exams\n- Interview Bundle PDF Course\n- General Awareness Smart Analysis\n- All Bank Exams Video Course\n- All Mock Test Series (Platinum Package)\n- All Premium eBooks\nPremium PDF Course 12 Months\n- Bundle PDF Course 2024\n- Prime PDF Course 2023\n- Ultimate Bundle PDF Course 2022""]"	['<urn:uuid:cf8f6f80-48df-4ef9-96b5-4b252f406d2d>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T20:58:04.490895	9	56	785
3	What assumptions did Eratosthenes make about the Sun and Earth when he tried to measure Earth's size using shadows?	Eratosthenes made two key assumptions in his calculations: First, he assumed that the Sun's rays strike the Earth in parallel lines. Second, he assumed that the Earth is round, which was not a universally accepted belief at that time and for many years afterward.	"[""The Intersection of Euclid & Eratosthenes\nIn order to understand how the intersection of Euclid and Eratosthenes led to the first estimate of the Earth's circumference (with remarkable accuracy!), we looked in detail at Euclid's Proposition 29 which states that:\nIf two straight lines are parallel, then a straight line that meets them makes the alternate angles equal, it makes the exterior angle equal to the opposite interior angle on the same side, and it makes the interior angles on the same side equal to two right angles.\nTogether, we broke down the statement to identify each part and students restated the definition using the names of lines and angles from an example diagram to make sure the statement made sense to them before we moved forward.\nLine a falling on line b and line c makes:\nUsing our understanding of Euclid's Proposition 29, we were then able to see how Eratosthenes applied this principle (and excellent critical thinking skills) to obtain his famous estimate of Earth's circumference.\nEratosthenes knew that in Syene, which is located in southern Egypt, the Sun at noon on the summer solstice shines down to the bottom of a well. He also knew that at noon on this, the longest day of the year, objects do not cast shadows. However, in Alexandria, which is farther north, at noon on the summer solstice, he noticed that objects do cast shadows.\nEratosthenes used this simple observation to calculate the circumference of the Earth. He assumed that 1) the Sun's rays strike the Earth in parallel lines, and 2) that the Earth is round (recall, not everyone at this time and for many years to come believed in a round Earth!). The approach he used was to measure the distance between Syene and Alexandria based on the number of steps someone took to walk between the two cities. He then measured the shadow cast by a stick placed vertically in the ground at Alexandria at noon on the summer solstice. He measured the angle from the stick to the shadow and found it be 7.5 degrees, or approximately one-fiftieth of a circle. Since Eratosthenes was familiar with Euclid's work and The Elements, he knew that the angle of the shadow cast by the pole is equal to the angle at the Earth's center between Syene and Alexandria. By multiplying the distance from Syene to Alexandria times 50, Eratosthenes was able to calculate the circumference of the Earth with amazing accuracy! At that time, distance was measured in stadia. The distance between the two cities was approximated to be 5,000 stadia which is equivalent to about 800 km, or 500 miles. Eratosthenes estimated the circumference of Earth to be 25,000 miles - that's within 2% of our current best estimates!\nFinal Projects and Homework\nDuring the last 15 minutes of class students worked independently on a review/test which covers concepts from Chapters 14-18 in the textbook. We will go over the answers together in class next week.\nI met with each student individually to discuss their ideas for a final science fair project. It looks like we will have a great variety of topics covered and I'm looking forward to seeing where the students take their ideas and what they will create for final presentations!\nFor homework this week, I would like students to prepare a one-paragraph description of their project and the approach they plan to take - you will need to hand these in to me on Monday. This will help ensure that everyone has clearly thought out and can articulate their project. Please take time this week to think through what you will research, build, and present (visually and orally) to the class. We have only a few short weeks left in the semester and everyone should have a definite project idea and be moving forward. Any questions or concerns, I am here to help you, so please don't hesitate to contact me.\nAdditionally, students should read Chapters 20 and 21 in the text.\nSee you next week!""]"	['<urn:uuid:dc43acc5-9717-4677-a84d-8d1f3b94dca2>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:58:04.490895	19	44	672
4	student doing research which chemistry field has more opportunities comparing michigan organic chemistry research and delaware chemistry biology interface program	Both programs offer significant research opportunities, but they focus on different areas. Michigan's Organic Chemistry program specializes in Organometallic Chemistry, Organic Synthesis, Bioorganic Chemistry, and Organic Materials, while Delaware's Chemistry-Biology Interface program covers a broader range of disciplines including organic chemistry, bioinorganic chemistry, analytical chemistry, biochemistry, structural biology, molecular biology, cell biology, systems biology/bioinformatics, plant biology, virology, and developmental biology.	"['Michigan offers a diverse selection of research opportunities in Organic Chemistry with particular strengths in Organometallic Chemistry, Organic Synthesis, Bioorganic Chemistry, and Organic Materials. Our innovative research rotation program allows students to explore a range of exciting possibilities before choosing an advisor. Specific research projects in each area are highlighted below.\nFaculty currently associated with this research cluster.\nUpcoming Events currently associated with this research cluster.\nTransition-metal mediated bond formation can be a versatile and efficient transformation in any organic synthesis. At Michigan, several research groups are focused on the use of transition metals in reaction discovery, synthetic methodology development, mechanistic chemistry, and complex molecule synthesis. Highlights of ongoing research projects include: (1) Developing new transition-metal catalyzed reactions for the stereoselective synthesis of saturated heterocycles such as tetrahydrofurans and pyrrolidines. (2) Discovering methods to transform inert carbon-hydrogen bonds into new functional groups. (3) Developing novel routes for functionalizing readily available organic building blocks such as alkenes and alkynes. (4) Exploring the reductive coupling of aldehydes and alkynes or enones and alkynes. Recent studies have demonstrated strategies involving redox isomerization to avoid the use of reducing agents in processes of this type. (5) The discovery of new glycosylation methods and their application in collaborative projects involving enzymatic C-H oxidation reactions.\nFigure 1. Nickel-catalyzed coupling of an aldehyde, alkyne and enone via redox isomerization (Montgomery).\nFigure 2. Palladium-catalyzed reaction of an γ-amino olefin and an aryl bromide (Wolfe).\nDesigning efficient synthetic routes to complex organic molecules remains a challenge. At Michigan, several research groups are working on total syntheses using methods developed in their labs. Recent synthetic targets include aigialomycin D, cephalotaxine, and aphanorphine.\nFigure 1. Palladium-catalyzed C-H functionalization of an arene (Sanford)\nResearch efforts in Bioorganic Chemistry at Michigan focuses on applying chemical principles, particularly those of organic chemistry, to solve problems in human health. Highlights of ongoing research projects include: (1) Developing strategies for reprogramming gene expression using small molecules, both natural products and ‘designer’ molecules. These molecules are being used as mechanistic probes of eukaryotic transcription as well as prototypes for transcription-targeted therapeutics for diseases ranging from diabetes to cancer. (2) Identifying novel molecular targets and small molecules for the treatment of autoimmune diseases and cancer. (3) Developing and applying chemical tools to study the role of oxidants as signaling molecules and the biological basis of aging. (4) Elucidating catalytic mechanisms and essential active site features of metalloenzymes and ribozymes, including protein farnesyltransferase, UDP-3-O-acyl-GlcNAC deacetylase, histone deacetylase and ribonuclease P. These studies should enhance our ability to design potent inhibitors of these enzymes useful for the treatment of cancer or bacterial infections. (5) Exploring the interface between biological macromolecules and materials chemistry through the de-novo design of extensively fluorinated “Teflon” proteins. Such proteins exhibit useful new properties such as increased thermal stability, resistance to unfolding in organic solvents, and resistance to degradation by proteases.\nFigure 1. Fluorinated amino acids can be used to make super-stable “Teflon” proteins; here the interior of a small protein is packed with the fluorous amino acid hexafluoroleucine (Marsh).\nOrganic Materials research at Michigan is focused on creating functional properties through synthetic modifications to polymers and small-molecules. Highlights of ongoing research projects include: (1) Developing techniques to control the process of crystallization using organic polymers as phase directors with the goal of making materials with improved function. Combinatorial materials chemistry plays a vital role in these efforts. (2) Developing novel routes for the efficient production of new members of conjugated oligomers and polymers with planar structural constraints, such as fused oligothiophenes, and studying their behavior in the solid state as it relates to important device applications such as organic thin film transistors (OTFTs). These structure types also find application in the construction of porous solids for applications ranging from gas storage to catalysis. (3) Designing new molecules that induce hydrogelation in the presence of an analyte for use in chemical and biological sensing. (4) Developing living polymerization methods for the synthesis of new conjugated polymer architectures for optoelectronic applications.\nFigure 1. Detecting redox-regulated proteins in living cells. (A) Sulfenic acid post-translational modifications are selectively detected via the cell-permeable probe DAz-1. (B) Visualization of protein sulfenic acids in MEF cells treated with DAz-1 (right) or untreated (left) (Carroll).\nFigure 2. Optical microscope image of a novel small-molecule based hydrogel (McNeil).\nFigure 3. Methodology development for the synthesis of conjugated polymers and oligomers of fused-ring thiophene compounds (Matzger).', ""The objective of this Chemistry-Biology Interface (CBI) Predoctoral Program is to provide cross-disciplinary training to talented students with diverse interests that will enable them to apply the mechanistic and atomistic perspective of chemistry to important biological problems. The program brings together outstanding faculty trainers from six academic units at the University of Delaware that represent diverse disciplines of organic chemistry, bioinorganic chemistry, analytical chemistry, biochemistry, structural biology, molecular biology, cell biology, systems biology/bioinformatics, molecular biology, cell biology, plant biology, virology, and developmental biology. The faculty trainers represent promising new investigators and established researchers with vibrant research programs in biomolecular science and experience in training graduate students. Trainees with diverse undergraduate educational backgrounds are selected on the basis of their interests in interdisciplinary science, their GRE scores, undergraduate GPA and letters of recommendation. Trainees are admitted through existing graduate programs of Chemistry &Biochemistry, Chemical Engineering or Biological sciences. Trainees will satisfy the degree requirements for their specific departmental program in addition to the requirements for the CBI program. Five one semester courses (15credits) will be selected from a diverse list of course offerings from six departments. A key feature of the program will be three laboratory rotations to provide them with hands-on experience in the different disciplines. Between courses and laboratory rotations students are expected to have exposure to concepts and methods from the atomistic to the cellular. A course on scientific integrity and the responsible conduct of research is also required. A weekly seminar series will provide trainees opportunities to present their own work, learn from both faculty trainers and outside speakers. Trainees will undertake an intensive independent research experience culminating in a dissertation representing an original contribution to a field at the chemistry-biology interface. This program follows a successful model for training scientists with both broad scientific knowledge as well as solid foundations in a chosen core discipline.\nAdvances in molecular medicine are often impeded by traditional training paradigms in which chemists and biologists often do not speak the same scientific language nor understand advances in each other's fields. This program provides trainees opportunities to learn to share ideas across traditionally separate fields in order to fertilize new ideas and innovations that require working knowledge of both chemistry and biology.\n|Lauro, Mackenzie L; D'Ambrosio, Elizabeth A; Bahnson, Brian J et al. (2016) Molecular Recognition of Muramyl Dipeptide Occurs in the Leucine-rich Repeat Domain of Nod2. ACS Infect Dis :|\n|Crown, Scott B; Long, Christopher P; Antoniewicz, Maciek R (2016) Optimal tracers for parallel labeling experiments and (13)C metabolic flux analysis: A new precision and synergy scoring system. Metab Eng 38:10-18|\n|Cordova, Lauren T; Lu, Jing; Cipolla, Robert M et al. (2016) Co-utilization of glucose and xylose by evolved Thermus thermophilus LC113 strain elucidated by (13)C metabolic flux analysis and whole genome sequencing. Metab Eng 37:63-71|\n|Lauro, Mackenzie L; Burch, Jason M; Grimes, Catherine Leimkuhler (2016) The effect of NOD2 on the microbiota in Crohn's disease. Curr Opin Biotechnol 40:97-102|\n|Yao, Mengyin; Elling, Felix J; Jones, CarriAyne et al. (2016) Heterotrophic bacteria from an extremely phosphate-poor lake have conditionally reduced phosphorus demand and utilize diverse sources of phosphorus. Environ Microbiol 18:656-67|\n|DeAngelis, Andrew; Panish, Robert; Fox, Joseph M (2016) Rh-Catalyzed Intermolecular Reactions of Î±-Alkyl-Î±-Diazo Carbonyl Compounds with Selectivity over Î²-Hydride Migration. Acc Chem Res 49:115-27|\n|Rehmann, Matthew S; Luna, Jesus I; Maverakis, Emanual et al. (2016) Tuning microenvironment modulus and biochemical composition promotes human mesenchymal stem cell tenogenic differentiation. J Biomed Mater Res A 104:1162-74|\n|Panish, Robert A; Chintala, Srinivasa R; Fox, Joseph M (2016) A Mixed-Ligand Chiral Rhodium(II) Catalyst Enables the Enantioselective Total Synthesis of Piperarborenineâ€…B. Angew Chem Int Ed Engl 55:4983-7|\n|Chiu, Josephine; Valente, Kristin N; Levy, Nicholas E et al. (2016) Knockout of a difficult-to-remove CHO host cell protein, lipoprotein lipase, for improved polysorbate stability in monoclonal antibody formulations. Biotechnol Bioeng :|\n|Long, Christopher P; Au, Jennifer; Gonzalez, Jacqueline E et al. (2016) (13)C metabolic flux analysis of microbial and mammalian systems is enhanced with GC-MS measurements of glycogen and RNA labeling. Metab Eng 38:65-72|\nShowing the most recent 10 out of 114 publications""]"	['<urn:uuid:d0f9fe29-e93a-4493-99ed-a46ad28e8a37>', '<urn:uuid:9e90c2d3-3b03-4819-9e9a-8274095581d4>']	factoid	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T20:58:04.490895	20	60	1408
5	As a historian of ancient combat, I wonder: which had more protection, Thracian gladiators or Buddhist reliquaries?	Buddhist reliquaries had more protection, being made of durable materials like wood, bone, ivory, quartz, glass, semiprecious stones, and metals including gold, silver, bronze, and copper, often decorated with chasing, enamelwork, and precious stones. In contrast, Thracian gladiators only wore light armor to enable faster movement, with just a wide-brimmed helmet and a small round or rectangular shield.	"[""Our editors will review what you’ve submitted and determine whether to revise the article.Join Britannica's Publishing Partner Program and our community of experts to gain a global audience for your work!\n- Types of sacred settings for ceremonial and ritualistic objects\n- Forms of ceremonial and ritualistic objects according to their functions\n- Summoning, mediating, and expelling devices\nPlants and plant representations\nIn all civilizations, plants and trees have been viewed as sacred. Generally, the tree is either a god’s habitat or the god itself and is worshipped. Such was the case, for example, in early Indian Buddhism. Trees may also be associated with the divine order because of some incident and subsequently venerated, as was the bodhi tree, under which the Buddha received his Enlightenment. Fences or even open-air temples, the form adopted for the early Bodh Gaya Buddhist temples, are built around such trees. Innumerable cases of sacred or divine trees and their painted or sculpted representations are found throughout written religious tradition and in the ethnological data. The branches of trees such as the palm, olive, and laurel are often associated with the gods; such branches may crown the god or be included among divine attributes. Many are used in worship, as are the branches of the bilva (wood-apple tree) among the adepts of Shiva, and the tulasi (basil), symbol of Lakshmi (Hindu goddess of prosperity and Vishnu’s wife) and sacred plant of the Vaishnavites.\nAs symbols of life and immortality, plants such as the vine of the Greco-Roman and the Christian world and the haoma (a trance-inducing or intoxicating plant) of pre-Islamic Iran are planted near tombs or represented on funerary stelae, tombstones, and sarcophagi. Two similar and related rites involving plants, the haoma, noted in the Avesta (ancient Zoroastrian scriptures), and the soma, noted in the Vedas (orthodox scriptures of Hinduism), pertain to the ritual production of exalted beverages presumed to confer immortality. The ritualistic objects for this ceremony included a stone-slab altar, a basin for water, a small pot and a larger one for pouring the water, a mortar and pestle for grinding the plants, a cup into which the juice drips and a filter or strainer for decanting it, and cups for consuming the beverage obtained. In many sacrifices, branches or leaves of sacred plants, such as the kusha plant (a sacred grass used as fodder) of the Vedic sacrifice and the Brahmanic puja (ritual), are used in rituals such as the Zoroastrian sprinkling (bareshnum), or great purification, rite, in which the notion of fertility and prosperity is combined with their sacred characters (see purification rite).\nOther representational objects\nThe staves of martial banners or standards are often surmounted by the figure of a god, which is frequently in its animal form. Such effigies, used by the Indo-Iranians, the Romans, the Germanic tribes, the Celts, and other ancient peoples, were probably meant to ensure the presence of the god among the armies. From the 4th century on, Byzantine armies placed on their standards the labarum (a cross bearing the Greek letters XP, signifying Christ [Chi and rho are the first two letters of the name ΧΡΙΣΤΟΣ, Christos]). Shields, such as the Greek gorgonōtos (“Gorgon-headed”), were also often decorated with sacred figures, emblems, and symbolic themes, particularly in post-Gupta (4th-century-ce) India, as seen in the 6th-century findings from the frescoes of Ajanta. In the Mycenaean civilization (15th–12th centuries bce) of ancient Greece, shields were worshipped in front of the temple, and at Knossos (on Crete) votive offerings were made of clay and ivory in the form of shields. The famous ancilia (“figure of eight” shields) of Rome were kept by the Fratres Arvales (a college of priests) and used by the Salii (Leapers), or warrior-priests, for their semiannual dances (in March and October) honouring the god Mars.\nRelics of saints, founders of religions, and other religious personages, which are often objects of worship or veneration, generally consist of all or part of the skeleton (such as the skull, hand, finger, foot, or tooth), a piece or lock of hair, a fingernail, or garments or fragments of clothing. Such veneration is nearly universal, as is the production of reliquaries, or shrines that contain relics. The size, form, and materials of reliquaries vary greatly and often depend on the nature of the relic being exhibited. They may be fixed but are generally portable so that they can be carried in processions or on pilgrimages. Wood, bone, ivory, quartz, glass, semiprecious stones, and metals such as gold, silver, bronze, and copper are frequently used materials, and chasing (embossing), enamelwork, and precious stones often ornament reliquaries. They vary considerably in form; like the Tibetan reliquaries, or ga’u, they may be constructed on a small scale to look like churches, chapels, towers, stupas, or sarcophagi, but sometimes they assume the form of the relic, such as in the form of anthropomorphic statues, busts, hands, feet, and other forms. Occasionally, as in Tantrism and Tibetan Buddhism, the bones of holy persons are used to make ritual musical instruments—flutes, horns (rkang-gling), and drums (ḍamaru)—or objects such as the ritual scoop made of a skull cup (thodkhrag) and a long iron handle encrusted with silver.\nIn many Asian regions, however, human relics are replaced by copies of sacred texts introduced into statues of bronze, as in Yunnan and Tibet, China, or of stucco, as in Afghanistan (Hadda, an archeological site near Jalalabad excavated since 1928) about the 4th–6th century."", 'The term gladiator came from the Latin word gladius for the Short Sword used by Roman troop soldiers. In the beginning, gladiator was an swordsman who participated in combat competitions. In later stage, gladiators were professional fighters at the time of the public combat tournaments in ancient Rome. Historians believed that tournament of gladiators originated from the ancient Etruscans for the purpose of funeral ceremonies, while the another opinions advocate idea that those games originated from the eastern civilizations.\nGladiators were war prisoners and slaves, convicted criminals, professional fighters and Christians (at the time of their persecution) or free Roman citizens with their own will. It was known according to Titus Livius (Livy) that combats of gladiators in Roman Republic held for the first time in 264 BC, but that does not mean they were not organized before this date. During the time of the first combat games, gladiators were named bustuarius or grave because those fighters were prisoners whose blood was sacrificed for decedent.\nA more massive events of the gladiator games begins at the time of the territorial spreading of the Roman Republic and during the Punic Wars between Rome and Carthage. Marcus Aemilius Lepidus in 216 BC organized in Rome games involving twenty-two pairs of gladiators. Consul Publius Licinius in 183 BC organized three days of games involving around 120 gladiators. In the later stage of the Roman Republic, the number of gladiators who participated in the tournaments increased even more. Thus, Julius Caesar organized the games (after he was elected for curule aediles) in 65 BC involving 320 pairs of gladiators who wore a silver armor and finally roman emperor Trajan in order to mark his victory over the Dacians in 108 AD organized 123 days of games used around 10.000 gladiators and 11.000 wild animals. During the events gladiators fought against the animals or between themselves. The gladiators games shows also another important function in the roman society: in addition to the display of Roman sense of superiority, it served as a social outlet and despite strict class – distributed seating, it was united together the Roman citizens in the moment of the celebration and cheer, giving them a sense of community. Plebs and Aristocracy united in a fan base. Even the poorest beggar could feel lord of life and death in these few little moments as he waved his handkerchief for his losing favorite. At the same time, the amphitheater gave the people the opportunity to express their displeasure directly to the emperor: The loud cheers when the Emperor entered into arena and took his place to seat. The mass of the gladiator fans in this moment neglects or approves his presence depending on his previews governmental decisions.\nType of gladiators\nIn ancient Roman society there was around twenty different types of gladiators. But there was also five classic gladiator types, which differed by body armor and offensive weapons.\nThe Samnite gladiator belonged the people of Samnium or Samnites who were inhabited south-central Apennine Peninsula. At the beginning of the territorial spreading, Roman Republic fought against Etruscans, Celts and Samnites. After three long wars against Samnites (343–341; 327–304 and 298–290 BC) Romans managed to achieved victory capturing many prisoners and taking them to Rome. On that way, many Samnites warriors (who were armed with gladius or short sword, rectangular schield or scutum, helmet and greave or orca) became Roman slaves who had to fight in combat games. Since the Samnite warriors fought in the games for their masters in the early stages of the Roman Republic this type of gladiator can be considered as prototype of all other types of gladiators.\nThraex of Thracian gladiator wore a wide-brimmed helmet, a small round, sometimes rectangular shield and fought with the sica, a curved sword, the traditional weapon of the Danube warriors. Thracian gladiator was equipped with light armor and this was made easier for him to move faster.\nGallus or Gaul Gladiators like the Samnite and Thraex were also war prisoners at the early stage of the Roman Republic. The number of slaves who were taken from Gaul increased especially after the military campaign of Gaius Marius at the beginning of the II century BC and during the Gallic Wars led by Julius Caesar 58-51 BC. His Gallic equipment consisted of the flat shield with a hump and the longsword. His curved helmet looked scary. If these fighters were truly like traditional Celts (or Gauls), probably those gladiators preferred to plunge as a hero in battle. In the Roman Imperial Age, the name changed to murmillo, because the fish mormyrus was used as inspiration to create a helmet shape and decoration. The added helmet had no visor (instead it is used metal mesh) and the shield was modified to a long, octagonal shape. Considering that this type of gladiator held in his hand a sword long 60-80 cm (within weight around 1.5 kg) this required a special physical readiness to fight.\nRetiarius (which on Latin meaning fisherman) or net-fighter was also one of the most famous type gladiator. He was not really a gladiator in the true sense, since he did not used standard sword (gladius). The retiarius only wore armor on his left side of the body to at least try to block the blows of his opponents. He only used weapons just like a true fisherman: fish net, trident and dagger. His advantages consisted by maneuverability and the greater distance of the trident. If he succeeded in throwing his net so skillfully that his opponent caught himself and hindered himself, retiarius had good chances to finish the fight victoriously. If he missed the chance to hit his opponent, he had to run away and look for another opportunity to pick up net safely.\nThe secutor wore the rectangular scutum and the gladius of the legionnaires, shield for arm made of thin plates. His helmet was designed in order to fight easily against retairius. Helmet was egg-shaped and rimless. The visor was folded up like the medieval knights’ helmets. Obviously, this helmet had been specially designed so that it would not easily catch in the net of its main opponent, the retiarius, while at the same time allowing a good view. Secutor roughly meant “pursuer or chaser” perhaps because he was often busy running after the retiarius.\nIn addition to these standard classes, more gladiator types were introduced especially in the Roman imperial era. During the time armor and weapons of the existing gladiators changed so that new fighting methods grew out of these encounters. Dose changes developed new types like the dimachaerus (gladiators who fought with two swords), eques (gladiator horsemen) and essedarius (gladiator who fought from a chariot). A grotesque development of secutor helmet used by andabata gladiator.\nThe visor of the helmet were completely closed, so the andabata wore a helmet with no eyeholes and fought blind like the victim in the Etruscan Phersu games.\nThe usual gladiator combat held in pairs or one against the another. Their name speaks for itself: they were called ordinarii. Another concept was group against group. Such fighters were called catervarii. Group fighting was harder for referees and for the spectators to observe down the scuffle.\nVery rarely, women also went into the arena. Roman emperor Nero forced aristocratic womens and men into the arena for his pleasure: Emperor Domitian also organized women fighting by torchlight and another time against dwarfs. Emperor Septimius Severus, however, at the beginning of the 3rd century AD, banned by the law combats of the female gladiators.\nInitially, the fights of gladiators were conducted within combination of non-experienced and experienced warriors. Those initially fights were place for execution in which only a few have survived. The survivor was often “ad ludum” or trained one who had the opportunity to earn his freedom through several successful fights. Warriors had the great advantage because they were already trained to use weapons and they could offer to spectators a more interesting fight compare to laymen. In the period of late Roman Republic, the popularity of gladiator fights rose immensely. Because of the worship of the gladiators, more and more Roman free citizens had to will to become gladiators. At first, those volunteers citizens were from the lower classes. This high percentage was surprising because a free citizen entered into some kind of slavery when they entered into school of gladiators. Although they kept their citizenship, they had to prescribe their body and soul to lanista, or the owner of the gladiators and training school. Gladiators had to say an oath or sacramentum like “Uri, vinciri, uerberari, ferroque, necari” which means “Be burned, bound, beaten, and die by the steel (sword)”.\nThe training of the gladiators took place in the ludus (training school). One of the most famous fighting schools of the republican period was in Capua where Spartacus launched 73 BC his rebellion. During the imperial era there were four famous gladiator schools in Rome: Ludus Magnus (the largest one was built by the emperor Domitian), Ludus Dacicus (founded by Domitian and finished by emperor Trajan), Ludus Gallicus and Ludus Matutinus – in which about 2,000 gladiators were trained.\nThe gladiator schools were well-organized with their own weapon makers and doctors. In particular, the surgical skills of these doctors were at high standard for that time. Like the theaters, the gladiator schools had also seats for spectators and it was a favorite time for Romans to observe the training of their gladiators. The lanista earned his money through the sale or rent gladiators. In the imperial era the ludi (schools) were also financed by the emperor and by the high officials like the curators or procurators. The schools of gladiators were virtually controlled by the emperor, because it would be too dangerous for empire to leave paramilitary troops in the hands of a private person. The shelter in the barracks of the school was comfortable and food quality was better compare to the food of other roman citizens outside of training school. Because the gladiators were cost a lot of money the owners treated them with care. The combat training was very hard and rough. The harder the training, the greater was chances for gladiator to leave the arena alive.\nThe life or death of a gladiator ?\nThe fight of the gladiators began with a loud trumpet sound. During the gladiator games Roman orchestra consisted of trumpets, horns and water organs or hydraulos. Different types of gladiators tried to use the advantages of their weapons and they trained to defeat their opponents – while the opponent of course tried the same. The winner of a fight, always left the arena alive. He may have succumbed to his injuries afterwards, but he rarely died in front of the audience.\nThe loser could give up: He can threw his weapon and shield, crouch on his knees and begged for mercy with outstretched forefinger. In a true Titan fight in which no winner could be determined, they ended the fight in a draw when both gladiators at the same time throw their weapons and gave up. The editor (sponsor and organizer of the games) had called for a fight with the finger, while the audience wanted to stop this fight and finally editor and audience could agreed to this compromise. In the arena sometimes several duels took place simultaneously and each coach was a referee at each fight. So the loser had a chance to recover even if he fainted from exhaustion. As it could be seen from the mosaic on the tomb from ancient city Pompeii, there was a referee intervenes when they hold the victorious gladiator from the deathblow. The loser’s life ultimately depended whether he was a good fighter. The final decision was made always by editor during a munus (commemorative duty). When the gladiator had given up, it was important for him to face death as stoically as possible, as the audience wanted to see the death of their intrepid heroes. The audience influenced the editor, with shouts and gestures, which finally decided on the further fate of the inferior.\nWhen the fight ended with the death of a gladiator, an servant dressed as god Mercury (gr. Hermes Psychopomps = “the soul-accompanying Hermes”) entered into arena and tested if gladiator was still alive. If the gladiator was really death, then the underworld god Charon, a masked priest and the goddess of the funerals and burial Libitina, joined in the arena. They claimed the body of the dead gladiator with the stroke of a hammer on the forehead. This method was originated from the ancient Etruscan practice, who were sacrificed animals in honor of Libitina. Mercury dragged the body with a hook through the porta Libitinensis, a small gate in the arena wall. A hook was used to avoid contact with the dead body.\nIf loser survived the fight but sentenced to death by the editor, there was no mercy. In that case gladiator was killed outside the arena. However, if the audience was in a particularly bloodthirsty mood, they could demand from the editor to execute gladiator looser in front of their eyes. This must be a honorable death for gladiator: he kneels down, clung to his thigh, and bowed his head. The victor gladiator held the helmet or head of the defeated one with one hand, while he severed the cervical vertebrae with his sword on the another hand. Killing the wounded gladiator in the arena was the norm among convicted criminals.\nVictory and freedom of the gladiator\nThe winner was rewarded with victory palms and later as the professional gladiatorial industry became more prevalent he also got a winning prizes in money. According to Cicero a successful gladiator was called the “plurimarum palmarum gladiator” or “the gladiator with the most palm trees.” The audience could ask the editor to give freedom to old gladiator or especially popular one. If the wish from audience also granted by editor, those gladiators got a wooden sword (rudis) as symbol of freedom. Freed gladiators were also called rudiarii. This took place usually after about three years of serving as gladiator. In rare cases for extraordinary courage novice (lat. novicius) gladiator could also gain a freedom from the editor. Although gladiators were very popular, as free citizen they were not socially respected as actors and prostitutes. A free citizen who had once been a gladiator had stained himself in the eyes of his fellow citizens with disgrace. He had no longer the necessary reputation to progress in roman society compare to other free roman citizens.\nSources / References:\n- M. Tullii Ciceronis, Orationes, Tom I, Pars I, Ex typographia P. & J. Blaev, 1699.\n- Alexander Adam, Roman Antiquities: Or, An Account of the Manners and Customs of the Romans, London 1825.\n- William Smith, A Dictionary of Greek and Roman Antiquities, London 1842.\n- Samuel Ball Platner, A Topographical Dictionary of Ancient Rome, London 1929.\n- Roland Auguet, Cruelty and Civilization – The Roman Games, New York 1994.\n- Murat Aydaş, “Gladiatorial Inscriptions from Stratonikeia in Caria”, Epigraphica Anatolica 39 (2006), p 105–110.\n- Wolfgang Neubauer, Christian Gugl, Markus Scholz, Geert Verhoeven, Immo Trinks, Klaus Locker, Michael Doneus, Timothy Saey & Marc Van Meirvenne, “The discovery of the school of gladiators at Carnuntum, Austria”, Antiquity Publications Ltd. 2014, vol 88, online issue, 2014, 173-190.']"	['<urn:uuid:7b40b859-faec-498e-95b2-1785f5a9ce04>', '<urn:uuid:034794e8-e384-444a-a819-422a49b4fe60>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T20:58:04.490895	17	58	3468
6	Why do kids get nosebleeds in winter, and what's the ideal humidity level to prevent this issue at home?	Children get nosebleeds in winter due to low humidity and dry air when heat is turned on, which can dry out the lining of the nose. The ideal humidity level to prevent this should be between 35-40 percent, though it can be adjusted slightly for personal preference, but should not exceed 50 percent according to the Environmental Protection Agency.	"['At Westchester Health, we’ve seen our share of nosebleeds over the years. Starting when your child is in preschool and continuing through the teenage years, periodic nosebleeds are just a fact of life, explains Heather Magnan, MD, a pediatrician with our Westchester Health Pediatrics group, in a recent blog. Although nosebleeds can be alarming, most are not serious. Here’s what might be causing them and how you can treat, and hopefully even prevent, them.\nWhat causes nosebleeds\nA wide range of factors can cause nosebleeds, including:\n- Colds and allergies: A cold or allergy causes swelling and irritation inside the nose and may lead to spontaneous bleeding.\n- Trauma: A child can get a nosebleed from picking his/her nose, or putting something into it, or just blowing it too hard. A nosebleed can also result from being hit in the nose by a ball or other object, or from falling hitting the nose.\n- Low humidity or irritating fumes: If your house is very dry, or if you live in a dry climate, the lining of your child’s nose may dry out, making it more likely to bleed. In addition, if he/she is frequently exposed to toxic fumes, this can cause nosebleeds.\n- Anatomical problems: Any abnormal structure inside the nose can lead to crusting and bleeding.\n- Abnormal growths: Abnormal tissue growing in the nose may cause bleeding. Although most of these growths (usually polyps) are benign, they still should be evaluated promptly by your child’s pediatrician.\n- Abnormal blood clotting: Anything that interferes with blood clotting can lead to nosebleeds. Medications, even common ones like aspirin, can alter the body’s blood-clotting mechanism just enough to cause bleeding. Blood diseases, such as hemophilia or platelet disorders, also can provoke nosebleeds.\n- Chronic illness: A child with a long-term illness, or who requires extra oxygen or other medication that can dry out or affect the lining of the nose, is likely to have nosebleeds.\nThe do’s and don’ts of treating nosebleeds\n- Remain calm. A nosebleed can be scary, for you and your child, but is rarely serious.\n- Keep your child in a sitting or standing position. Tilt the head slightly forward and have your child gently blow his/her nose if old enough.\n- Pinch the lower half of your child’s nose (the soft part) between your thumb and finger and hold it firmly for a full ten minutes. If your child is old enough, he/she can do this him/herself. Do not release the nose to see if it is still bleeding. After ten minutes, release the pressure and wait, keeping your child quiet. If the bleeding has not stopped, repeat this step. If after ten more minutes of pressure, the bleeding has still not stopped, call your pediatrician or go to the nearest emergency department.\n- Panic. This will just scare your child more.\n- Have your child lie down or tilt back the head.\n- Stuff tissues, gauze or any other material into your child’s nose to stop the bleeding.\nIt’s time to call your pediatrician if:\n- You think your child may have lost too much blood. (Keep in mind that blood coming out of the nose always looks like a lot.)\n- Blood is coming from your child’s mouth, or he/she is coughing up or vomiting blood or brown material that looks like coffee grounds.\n- Your child is unusually pale or sweaty or is not responsive. Call your pediatrician immediately in this case and take your child straight to the emergency room.\n- Your child has a lot of nosebleeds, along with a chronically stuffy nose. This may signal a broken blood vessel in the nose or on the surface of the lining of the nose, or a growth in the nasal passages.\nHow to prevent your child’s nosebleeds\n- Keep the inside of the nose moist. Nasal dryness can cause nosebleeds. An over the counter nasal saline spray or gel may be used daily as often as needed.\n- Use a vaporizer or humidifier. Your child’s nostrils might be dry because the air in your house is dry. A humidifier or vaporizer will help maintain your home’s humidity at a level high enough to prevent nasal drying.\n- Don’t smoke. Smoking (including secondhand smoke) can irritate the inside of the nose and dry it out.\n- Don’t pick the nose. Also, don’t blow or rub it too hard. If your child is getting nosebleeds, keep his/her fingernails short and discourage him/her from picking his/her nose.\n- Don’t use allergy nose sprays too often. These can also dry out the nose. In some cases, certain medications can cause nosebleeds or make them worse so discuss all medications with your child’s doctor.\nIf you’re concerned about your child’s nosebleeds, please come see us.\nIf your child is experiencing nosebleeds more frequently than seems normal, please come in and see one of our Westchester Health pediatricians. Together, we’ll figure out what might be causing them and what would be the most effective ways to stop and/or prevent them from recurring. Whenever, wherever you need us, we’re here for you.\nTo read Dr. Magnan’s blog in full, click here.', ""- Ideas & How-Tos\nChoose Your Savings\nIf winter in your home means dry skin, scratchy throats and lots of static electricity buildup, you may have a problem with low humidity. When you close the windows and turn the heat on in the winter, you begin to reduce the humidity in your home. Here are some ways to balance it back out.\nDry air in your home can cause or aggravate respiratory problems, dry out nasal passages and make you more susceptible to colds or the flu. Although winter weather is often blamed for these problems, another major cause is dry air produced by artificial heating. Humidifying your home to provide proper moisture levels will help alleviate these symptoms.\nA humidity level of 35-40 percent is considered best. You may want to raise or lower it slightly, depending on personal preference, but the Environmental Protection Agency recommends a humidity level no higher than 50 percent.\nHumidifiers increase the humidity in the air in a safe, water vapor form to help make your home healthy and comfortable.\nFinancial Benefits of Humidifying Your Home\nControlling humidity can also help you save money on energy bills. The heat our bodies feel is a combination of temperature and humidity. In other words, the more humid the air, the warmer you feel. If you add humidity to dry, heated air in the winter, you can set your thermostat lower and still be comfortable.\nA humidifier's capacity should match your household's needs. Capacity is measured in gallons per day of operation. One method of estimating the capacity you need is to determine the square footage of the area you want to humidify. Use the chart below to determine what output level is best for you:\n|Area||Output Rating (Gallons per day)|\n|500 sq. ft. or lower||1.5--2.0|\n|530-600 sq. ft.||2.2--2.5|\n|700-800 sq. ft.||3.0--3.5|\n|900-1000 sq. ft.||4.0--5.0|\n|1000--2000 sq. ft.||7.0--9.0|\n|Over 2000 sq. ft.||10.0 or higher|\nWhen selecting a humidifier, take into account the purchase price, operating costs and maintenance costs of the unit. Some models consume more energy than others, so choosing a model that is right for your home and budget is important. Some of the most popular types of humidifiers are listed below:\nWhether you choose a warm mist or cool mist humidifier is a matter of personal taste. Both types raise the humidity level and make your home more comfortable. The cool mist humidifier is the most effective in adding moisture to the air: it works faster, doesn't make the room hot and lasts longer. Also, with a cool mist humidifier there is no risk of being scalded with hot water or steam.\nWhole-house humidifiers are installed in the ductwork, next to your furnace. They add humidity to your entire home. Most have humidistats, allowing you to set the exact level of humidity you want. Installing a humidifier is an easy job if you're replacing your furnace, but you can also have a humidifier fitted to your current system.\nMost whole-house humidifiers operate on the basis of a simple concept. Air heated by your furnace or heat pump passes through a ceramic-coated pad in the humidifier, called an evaporator pad. The evaporator pad is saturated with water. The air absorbs moisture from the pad and adds humidity throughout the home as it circulates. Depending on the model you choose and the size of your home, a humidifier will use from 1.5 to 12 gallons of water per day when the furnace is operating. But don't worry, this small amount of water isn't enough to notice a difference on your water bill.\nAfter each use, clean the inside to prevent the accumulation of concentrated minerals and to prevent bacterial growth. Follow the manufacturer's instructions.\nIf your humidifier has a filter, make sure to replace it at least once during the season. As a filter collects impurities, it begins to discolor. When the lower portion of the filter shows discoloration, it's time to change it. If you have hard water or water with high mineral content, you need to replace your filter more often. If you haven't used your humidifier for an extended period of time, dispose of the filter and install a fresh one.\nWhen warm, moist air comes in contact with a cold, dry surface, the water in the air condenses, creating water droplets. Your humidistat is set too high if this moisture is excessive. Here are some ways to determine excess humidity:\nKeep in mind that a tight, energy-efficient house holds more moisture. Adjust your humidistat until you reach an appropriate humidity level. Additionally, you may want to run a kitchen or bath ventilating fan or open a window briefly if the humidity level gets too high.""]"	['<urn:uuid:2e9cc80b-2c52-4216-bb00-51d530c82024>', '<urn:uuid:8dcd84dd-5cd0-4699-bef0-422bf27d4e65>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T20:58:04.490895	19	59	1645
7	hidden doors and secret areas unlock methods in sims 3 and final fantasy xv games	In The Sims 3 World Adventures, hidden doors can be unlocked through various triggers like stepping on floor switches or inserting keystones into Keystone Panels. Some doors are initially invisible and must be inspected to be discovered, while 'Non-Discoverable' doors only appear when activated by the correct trigger. In Final Fantasy XV, secret areas often contain Oracle ascension coins which can be found in various locations including dungeon rooms, building rooftops, and behind shops.	"['In The Sims 3 World Adventures, you can build like never before and share simple or complex tombs, hidden caves, and catacombs you\'ve created with other Sims players. Building tombs is an entirely new way to enjoy Build and Buy Modes in the game and adds a new element to the adventure gameplay as a first for The Sims. To continue the lesson in adventure building, we\'ve outlined additional objects that you can use and triggers that will activate behaviors—cause and effect style. Plus, we have more tips. We can\'t wait to see what you\'ll create!\nAdditional New Objects + Tweaks to Existing Objects\nNote: Triggers and Activated Behaviors are set up using the Ctrl + Shift + Left Click Buy Mode interaction described above.\n- Hole in the Wall and Floor: The Hole in the Wall and Floor objects are small, rune inscribed holes in which Sims can seek treasure or hidden switches. Beware, as bugs may come flowing out! The Hidden Switch Trigger can be used to set off Activated Behaviors. Holes in the Wall and Floor can be set to Appear or Disappear. Furthermore, treasure can be hidden inside them for Sims who take the time to look!\n- Keystone Panel and Keystones: Every tomb should have keys! Keystone Panels have the Insert Keystone Trigger, which can be used to set off Activated Behaviors. Most commonly are opening of locked doors or treasure chests. Keystone Panels can be set to accept generic Heart, Star, and Crescent keystones. Keystones can be found in tombs, inside chests, underneath rubble piles, or anywhere in which a treasure can be hidden.\n- Ancient Coin Pile: Ancient Coin piles are a great reward for Sims. They can be used to buy Adventure Rewards from the special merchant. Ancient Coin piles can be set to be Very Small, Small, Large, or Very Large.\n- Rubble Pile: Caves and ancient structures need cave-ins. You can block door ways, hallways, and rewards like Ancient Coins and Treasure Chests by placing a rubble pile on top of it. There are corner and flat wall rubble piles, as well as dark and light stone variants. Over time, Sims will hack away at them and clear them eventually to reveal switches or treasures. Tomb Designers are able to set the length of time it takes to clear the tomb.\n- Hidden Doors: Hidden Doors are great for hiding secret areas and surprising the players. These can be set to Locked, Unlocked, Visible, or Invisible, or even ""Non-Discoverable."" Visible ones must be opened by patient and strong Sims, whereas Hidden ones must first be inspected. If locked, the doors can only be opened by a Trigger, as Locking, Unlocking, Hide, and Unhide are Activated Behaviors. Non-Discoverable doors cannot be inspected. In fact, they can only be found when the correct Trigger makes them appear!\n- Locking and Unlocking Doors: Doors can be set to Locked or Unlocked in tombs. This is a very straightforward way to gate Sims until they complete a puzzle. A simple example is to set the Unlock Activated Behavior on a Locked Door, that is Triggered when the Sim Steps On a Floor Switch.\n- Gigantic Boulder: These can be placed to block entrance to special areas, or an entire tomb. These huge boulders can only be destroyed with the mystical Pangu\'s Axe, which can be found via an Adventure in China. The Gigantic Boulder is a great way to give players a reason to return to your tomb once they have Pangu\'s Axe.\nTrigger Flow and Content Examples\nIt is best to follow a specific order to design straightforward, intuitive tombs for others to experience. It\'s possible to modify the process for what best fits your creative side, but this is a good method with which to begin.\n1.) Design the Layout and Theme for your tomb. Is it going to be a tomb filled with fire? What about a watery graveyard with pools, fountains, and steam traps? Before you get caught up with individual puzzles, and tiny details, know where you\'re going to place things and what you want the overall experience to be.\n2.) Build the Tomb and Place all objects: Make sure your traps fit in the tight hallways, there\'s plenty of room for your dive wells, and be sure to confirm that there is room for statues to be pushed and pulled towards completion of the puzzle.\n3.) Set up Triggers and Activated Behaviors: Depending on how much thought you put into your puzzles in the earlier steps, this part may be a breeze or quite complicated. There is a very specific flow that must be used here in order for things to work properly.\nTIP: In order for the tomb to act like a tomb, you must set up a series of Triggers and Activated Behaviors on the placed tomb objects. It is possible to access these commands by entering Buy Mode, and Ctrl + Shift Left Clicking on objects. Examples will be provided on how to use these commands, but it\'s important to explain how to access them in the first place!\nTriggers and Activated Behaviors\nWhen you want something to activate something else, essentially a cause and effect, you must first designate a Trigger, then Activate a behavior. For example, you want to make it so that when a Sim steps on a floor switch, a door is unlocked.\nFloor Switch to Unlock Door\n1.) Place the Door\n2.) Place the floor switch\n3.) Lock the Door using Buy Mode Interactions\n4.) Use the ""Link Triggers…Step On"" interaction on the Floor Switch\na. Here, you are saying that the Trigger is Step On\n5.) Use the ""Link to Activated Behavior…Unlock"" interaction on the Door\na. Here, you are saying that the Activated Behavior for the last Trigger is to Unlock\nTIPS: The ""Link to Activated Behavior"" interaction only appears once a Trigger is set. If you want to change what the current trigger is, simply click on an object and select ""Reset Current Link."" If you want to see the Triggers current set from an object, hover over the object with the mouse cursor. Lines will be drawn to the connecting objects. If you want to see all triggers set up, select ""Triggers…Draw All Links on Lot."" If you want to delete a Trigger, click the object, select ""Triggers…"" with the name of the Trigger. When doing ""Link to Activated Behavior,"" all available Activated Behaviors that have not already been selected from the Trigger object will be displayed.\nTo further acquaint you with this building system, here are some common tomb object components and setups for you to use. Remember, these are just examples! Triggers and Activated Behaviors can be mixed and matched to create a huge variety of puzzles.\nKeystone Panel to Unlock a Door\n1.) Place the Door\n2.) Place the Keystone Panel on the wall near the door\n3.) Place a keystone in the room\n4.) Lock the Door\n5.) Use the ""Link Triggers…Insert Keystone"" interaction on the Keystone Panel\n6.) Use the ""Add the Ability to Add Triggers and Activated Behaviors"" interaction on the Door\n7.) Use the ""Link to Activated Behavior…Unlock"" interaction on the Door\nWhen inserting the keystone into the Keystone Panel, the door will unlock.\nDisable Fire Trap with Steam Trap\n1.) Place a trap on the floor\n2.) Place a trap as close as possible to the first one on the wall\n3.) Place a Floor Switch\n4.) Use the ""Set Trap Type…Fire"" interaction on the floor trap\n5.) Use the ""Set Trap Type…Steam"" interaction on the wall trap\n6.) Use the ""Set Trap State… Firing and Visible"" interaction on the floor trap\n7.) Use the ""Set Trap State…Armed and Visible"" interaction on the wall trap\n8.) Use the ""Link Triggers…Step On"" interaction on the floor switch\n9.) Use the ""Link to Activated Behaviors…Fire Once"" interaction on the wall trap\nThe Sim cannot get past the fire trap. However, when the Sim steps on the floor switch, it activates the steam trap which puts water onto the fire trap. The fire trap is now disabled and the Sim can pass!\nUse Pushable Statue to Disable Trap\n1.) Place Pushable Statue\n2.) Place trap on the floor\n3.) Use the ""Make Visibly Movable – (Currently: None)"" interaction on the Pushable Statue\n4.) Use the ""Set Trap Type…Electricity"" interaction on the floor trap\n5.) Use the ""Set Trap State…Armed and Visible"" interaction on the floor trap\nThe Sim probably doesn\'t want to risk walking across this armed electrical trap. The Sim can now push the statue onto the trap, which will disarm it for safe passage!\nUse Pushable Statue on Floor Switch to make Floor Switch Appear and unlock Door\n1.) Place Pushable Statue\n2.) Place two floor switches (Switch A and B)\n3.) Place Door\n4.) Lock the Door\n5.) Use the ""Hide"" interaction on Switch B\n6.) Use the ""Make Visibly Movable – (Currently: None)"" interaction on the Pushable statue\n7.) Use the ""Link Triggers…Step On"" interaction on Switch A\n8.) Use the ""Link to Activated Behaviors…Appear"" interaction on Switch B\n9.) Use the ""Link Triggers…Step Off"" interaction on Switch A\n10.) Use the ""Link to Activated Behaviors…Disappear"" interaction on Switch B\n11.) Use the ""Link Triggers…Step On"" interaction on Switch B\n12.) Use the ""Add the Ability to Add Triggers and Activated Behaviors"" interaction on the Door\n13.) Use the ""Link to Activated Behaviors…Unlock"" interaction on the Door\nIf the Sim steps on Switch A, Switch B appears! However, it quickly disappears when the Sim gets off Switch A. The solution is to push the statue onto Switch A, which makes Switch B permanently visible. The Sim can now step onto Switch B to unlock the door.', 'Oracle ascension coins\nare a key item in Final Fantasy XV that can be found throughout the game. These items can be sold for a small amount of gil, and may appear to be trinkets at first.\nTheir real use remains hidden until much later in the game, so you should hold onto them. In this guide, we’re going to show you all\noracle ascension coin locations in Final Fantasy XV\n, along with a map.\nFFXV Oracle Coin Locations\nOracle coins are special coins made to commemorate the ascension of Lunafreya Nox Fleuret, Oracle extraordinaire and future bride of Noctis, the game’s protagonist.\nThey’re scattered across the kingdom of Lucis, but their purpose becomes clear once you reach the last third of the game. There is a special vendor that sells very valuable items, but as currency he accepts only oracle ascension coins. The most expensive item he sells is an accessory that costs 40 ascension coins and provides immunity to all ailments (poison, petrification, toad and so on). This makes the item one of the most useful in the game and it will help you defeat the hardest end game bosses. Bellow is a map of all Oracle ascension coin locations we have found so far with a list describing where you can pick them up at each location. Thi guide is still work in progress and we’ll be adding more locations until we find all 40.\nNo. Oracle Ascension location guide Map Screenshot 1. Found in Galdin Quay restaurant. From the central desk, where you buy food, go a bit towards the docks and to the left. Coin is on one of the tables. 2. Found inside Keycatrich Trench dungeon. Inside the room with locked doors. Next to some barrels and a blue wooden board leaned against the wall. Look at the provided image for exact locations of both this one and number 3. 3. When in Keycatrich Trench dungeon you go through a small section because all the other doors are lock, the coin is in the rubble of a collapsed tunnel’s end. 4. You unlock Praire outpost in northern Leide once you start Chapter 2 of the game. There is a vendor across the hunter’s outpost. There are some wrecked parked cars there and a brick building. You can actually jump on top of the roof of that building and you’ll find the ascension coin there. 5. From Coernix Station – Alstor outpost go across the road south. There is a small parking lot there with some construction rubble. Coin is between a couple of abandoned cars. 6. I found one in Lestallum while doing the main quest a Stroll for Two. Best check out the screenshot provided which shows the zoomed in map of exact location. 7. Go to Burbost Souvenir Emporium outpost. As you exit the car there are traffic cones behind you and the ascension coin is located next to one. 8. Greyshire Grotto dungeon is part of the main story quest. Inside is another coin you can find towards the end of the dungeon, as seen on the map screenshot, showing the exact location of the collectible. 9. Trial of Ramuh Chapter 5 main quest takes place in Fociaugh Hollow dungeon. One Oracle Ascension Coin is found next to the first lootable plants you come across as you make your way through the Hollow. 10. Cauthess Rest Area outpost has several buildings southeast of the gas pump. You will need to get on top of the farther one, because the ascension coin is located on its roof. You can take the stairs there. 11. Visit Coernix Station – Cauthess in Duscae. There is a parking lot with construction rubble and a couple of buildings across the road from the gas station. When you cross the road you can go down some stairs and the coin is located right next to the staircase wall. 12. Taelpar Rest Area is the next outpost you can visit for an ascension coin. Behind the diner there (Crow’s Nest) you will find the shine of the coin item on a metal cupboard (you’ll see some wooden logs there). Look behind the weapons vendor for it. 13. There is an outpost when you make your way to the volcano in western Cleigne called Verinas Mart – Ravatogh. Just right of the weapons vendor is where the building ends. Among some hardware, behind a trash can, is where the coin is at. Please reference the screenshot and the minimap on it for exact location. 14. There are at least 8x oracle ascension coins to be found in the Pitioss secret dungeon at the end of the game. More info about these will be added in a separate guide. 15. There’s one coin behind the shop at Hammerhead garage, on top of an AC unit. 16. One is north of the camping spot just north of Hammerhead. Climb the rocks, than jump onto the base of the tower. The coin is inside. 17. You can find one at the Longwythe rest area, betweenhe white tanks in the east. 18. On the northern edge of the Wiz Chocobo Post compound. 19. Near the end of the Glacial Grotto dungeon, a bit southeast from the lightning deposit. 20. One is on the final floor of Steyliff Grove dungeon, in the small room to the east of the deposits you reach before the boss fight. 21. In Altissia, in the Parco Listro district. It’s in the western corner of the plaza. 22. In Balouve Mines, you’ll find one in the room where you fight the Aramusha boss. It’s on the eastern edge of the room. 23. In Crestholm Channels, you can get one in the first southbound corridor west of the room where you fight Jormungand. 24. There’s another one in Myrlwood, in the western part of the first open area. 25. You can pick one up in Malmalam Thicket, in the last open area before the boss fight. It’s a bit north of the fire deposit, near the camping spot. 26. One is hidden in the Rock of Ravatogh dungeon. It’s just south of the entrance.\n27. Each part has at least one: Keycatrich Trench (1), Balouve Mines (1), Fociaugh Hollow (1), Glacial Grotto (1), Daurel Caverns (3), Crestholm Channels (2), Costlemark Tower (1), Steyliff Grove (2). locked door dungeon What are Oracle Ascension Coins forThere is an oracle coin vendor later in the game, in Altissia, who accepts the coins as currency. His name is Alessio, and he can be found in a cafe in the northeast of the city. His inventory includes a mega elixir and a bunch of great accessories.']"	['<urn:uuid:a9c0ff2e-464c-4c85-bc5b-45ed89bf23eb>', '<urn:uuid:61a182c5-f9d2-4ef5-a3f7-3ba63ada3eec>']	factoid	direct	long-search-query	distant-from-document	three-doc	novice	2025-05-12T20:58:04.490895	15	74	2741
8	image alt text seo accessibility legal requirements	Alt text serves both SEO and legal accessibility requirements. For SEO, it provides search engines more content to crawl and index, strengthening your site's visibility. From a legal standpoint, web accessibility is required for many businesses, making alt text essential since it allows screen readers to describe images to visually impaired users. Alt text should be descriptive and include relevant keywords while accurately describing the image content for both search engines and users with disabilities.	['Image SEO: 6 Ways to SEO-Optimize Images in WordPress\nIn this article, we’re sharing our most important tips and tactics for optimizing your website’s images for SEO.\nWhy do the images on your WordPress site matter for SEO? Let’s start with why images matter for humans—readability.\nTable of Contents\nIf you want people to stick around long enough to read your whole, your content has to be easily readable.\nThat’s where images come in.\nIncorporating images helps both your readability and SEO by highlighting important information and breaking up your text. This makes it easier for search engines to crawl and humans to read.\nOne more thing to consider is that web pages that contain images and videos generate 94% more impressions than pages with text alone.\nAnd nearly ⅓ of internet marketers worldwide say that visual images are the most important type of content for their business. Images encourage people to stay and read—and share—your articles. And yes, social shares really do help with SEO.\nThe other reason why images matter for SEO is simply because they give search engines more content to crawl and index.\nWe talk about alt text below which strengthens the message of your articles with search engine spiders and improves the accessibility of your website.\nWhere to get the best images for your WordPress website\nWhile using cookie-cutter stock images may be tempting because they are so easy to find, please try to use original imagery on your website. Google loves original content, including images.\nIf you don’t have original images to use there are plenty of places to find free pictures and imagery that don’t feel so stock-ish.\nWe often use Shutterstock for images because it has the biggest database of imagery and includes photos and graphics.\nWe search their database just like we search in Google for information and usually find exactly what we need for images.\n6 Image SEO tips for WordPress that will help your webpage rank\nImages make your website more interesting and attractive — and they play a valuable role in your site’s SEO.\nThe process you use to name and tag your images is just as important as the words you use in links and the search terms you incorporate into page and blog post headlines.\nMake your images a part of your SEO strategy by paying careful attention to the keywords you associate with those images, file sizes, and other factors below.\n1. Optimize Image Size and Quality\nUploading huge images to your website can make your site sluggish. Make sure your site loads quickly by optimizing the size of your images.\nAs a general rule, images should be smaller than 100 kb and re-sized to the size you want it displayed. We like this plugin to help resize images quickly.\nYour website’s load time and readability are huge factors that optimizing your images for SEO will help with. A free tool you can use right now to check your website’s speed is Pingdom.\nIf you’re worried about your images slowing down your pages, check out our guide on how optimizing images can speed up your website.[Bonus: here’s how to go beyond images and improve your website’s Google PageSpeed score.]\nThink about your own experience…\nIf you go to a website and sit for just a few seconds while it loads, what is your first thought?\nIf you’re anything like me, you have little patience for a slow website, and for good reason.\nThere are just too many websites out there for us to sit around and wait for one to load.\nLeave it, and move on to the next.\nGoogle and your visitors want your website to load faster than they can blink. If your website is slow to load and other similar websites load faster, those other websites could outrank you.\nAnd this is just one of many reasons why image size and quality throughout your website matter.\nWhile you’re editing your image for size, you should also rename the image. Keep reading.\n2. Rename Every Image Before You Put It On Your Website\nNo matter where your image comes from, it will probably have a name that won’t do you much good from an SEO perspective.\nBasically, Google wants to know what the image is about without even looking at it.\nFor example, if you downloaded the image from an image bank, it probably has some kind of file number as the image name.\nIf you uploaded it from your camera, it may have a filename that starts with three letters and then several numbers.\nEither way, you’ll need to rename the photo to something more useful before you upload it to your website.\nGive the photo a name that includes a search term and keywords that are relevant to your website.\nUsing your main keyphrase at the beginning of the file name is best.\nImage SEO Pro Tip\nIf you forget to rename your image before you upload it, you’ll have to rename it on your desktop and then upload it again.\nYou can also use Enable Media Replace, a handy WordPress plugin that helps you replace images in your website’s media library without having to delete them manually.\n3. Use Hyphens in Image Names\nWhen you rename photos, remember that using hyphens between the terms separates them, while using underscores joins them.\nSo if you want to boost your SEO for the term “red plaid shirt,” name the relevant image “red-plaid-shirt.”\nIf you use “red_plaid_shirt,” Google will read it as “redplaidshirt” — and will search right over it.\n(The same thing happens in URLs, too, so make sure your URLs use hyphens and not underscores.)\nHere’s a quick video about that hyphens vs. underscores—\n4. Keep Image SEO Simple\nDon’t get too fancy when naming an image, and don’t overload the image name with keywords. If it’s your company logo, a simple “XYZ-company-logo” is fine.\nTreat image keywords like link keywords, and use different terms over time if necessary.\n5. Fill In the Title and Alt Fields\nThough it may be small, alt text serves a huge purpose in web design accessibility and SEO (unlike meta tags).\nWhat the heck is alt text, you ask? It stands for “alternative text.”\n“Alt text (alternative text), also known as “alt attributes”, “alt descriptions,” and technically incorrectly as “alt tags,” are used within an HTML code to describe the appearance and function of an image on a page.”\nThese fields are attached to your image after you’ve uploaded it to your website. Use a short sentence or keyphrase to describe the image, including the relevant SEO keywords you want Google and other search engines to see.\nAlt text displays when there’s a problem loading the image, and special browsers that read websites for visually impaired visitors use alt text to describe what they “see.”\nIf you’re unsure if you’re doing your alt text correctly, contact us for help.\nBut simply put, your alt text should describe what is in the image so both search engines and people can make sense of it.\n6. Make Changes When You Change Your SEO Strategy\nIf your organization decides to revamp its SEO strategy, don’t forget that updating images needs to be a part of the process.\nIt is SEO strategy, after all.\nGo back and change the ALT tags, and consider uploading new renamed files as well.\nWhile Google doesn’t disclose the weight images have on search results, I do know traffic from Google Image searches resulted in about 15 to 20 percent of the traffic on my website.\nThat’s a big deal — because if people aren’t going to my site, they’re going somewhere else.\nSEO is a constantly changing science, but remembering that images have a role to play will help you get the most out of your SEO efforts.\nPick relevant images, name them correctly, and fill in the necessary fields to get your images to work for you and help your website rank.\nYou can try to download an image off Google (assuming you have the rights) and upload it directly into WordPress without making any changes to the image file.\nBut what if that image file size is too large, doesn’t have context for the visually impaired, or is named something out of context? Your SEO won’t reach its full potential.\nRock-solid SEO involves a bunch of factors, but you can consider image SEO “low hanging fruit.”\nIn other words, optimizing images for SEO in WordPress can be simple if you follow our tactics and advice above.\nYoast is one of the most popular SEO WordPress tools available today and is used by hundreds of thousands of WordPress websites.\nWe looked to Yoast for their best practices when it comes to image SEO and here’s a few articles you can use for further guidance on the topic:', 'The internet has become a major part of our lives, and we rely on it for countless things like shopping, conducting business, entertainment, and education. However, there are many people with disabilities who are unable to access digital content and services the way they were intended to. In recent years, web accessibility has become a crucial issue that needs attention from web designers, developers, and SEOs. In this blog post, we will explore the importance of web accessibility, key guidelines and best practices for website design, tools and resources to test and improve website accessibility, case studies of companies that have improved user experience through accessible web design, and how to integrate accessibility into the web design process from the beginning.\nThe Importance of Web Accessibility\nWeb accessibility refers to the inclusive practice of designing and developing websites that can be used by everyone, including people with disabilities. Web accessibility is important because the internet is a fundamental aspect of modern life, and everyone should have equal access to digital content and services regardless of their abilities. Website accessibility is a legal requirement for many businesses, but it also makes good business sense for companies to make their sites accessible to all users. Some of the benefits of web accessibility include:\nImproved user experience: Accessible websites are easier to use and navigate, providing a better user experience for all users.\nIncreased traffic: Accessible websites can attract more visitors, including people with disabilities who may not be able to access other websites.\nHigher search rankings: Accessible websites can improve search optimization, leading to higher search rankings and better visibility.\nEnhanced reputation: Companies that prioritize web accessibility are seen as more welcoming, inclusive, and socially responsible.\nAccessibility Guidelines and Best Practices\nTo design an accessible website, designers must follow standard guidelines and best practices. The Web Content Accessibility Guidelines (WCAG) is a set of guidelines developed by the World Wide Web Consortium (W3C) that provides a framework for creating accessible websites. The WCAG 2.1 guidelines cover four key principles: perceivable, operable, understandable, and robust. Following are some of the best practices for web accessibility:\n- Use descriptive alt text for images and non-text content to provide context and accessibility for screen readers.\n- Use clear, easy-to-read fonts and avoid low contrast that makes the text hard to read.\n- Provide closed captions and transcripts for videos and audio content.\n- Ensure keyboard accessibility for all functions and avoid relying on mouse and touch inputs.\n- Ensure all important content is accessible without the use of color alone.\nTools and Resources for Testing and Improving Web Accessibility\nThere are many tools and resources available to help designers test and improve their website accessibility. Some of these tools include:\nThe WAVE Web Accessibility Evaluation Tool: This tool provides feedback on the accessibility of your website, including errors and suggestions for improvement.\nAccessibility Insights: This tool provides automated testing for accessibility issues, as well as manual testing resources.\nARIA (Accessible Rich Internet Applications) standards: A set of standards that help developers make dynamic web content accessible.\nCase Studies of Improved User Experience through Accessible Web Design\nMany companies have implemented accessible web design and achieved positive results. For example, the Royal National Institute of Blind People (RNIB) re-launched its website in 2017 with a focus on accessibility features like high contrast, clear fonts, and simplified layouts. As a result, the site saw a 117% increase in traffic, with users staying on the site for longer periods of time.\nIntegrating Accessibility into the Web Design Process\nIt’s important to integrate accessibility into the web design process from the beginning, rather than attempting to retrofit it after the website is launched. This involves creating a culture of accessibility within the design team, including education on accessibility requirements, establishing accessibility-focused standards, and conducting regular accessibility audits. It’s also important to involve users with disabilities in the design process to ensure their needs are accounted for.\nIn conclusion, designing inclusive websites that are accessible to all users is not only a legal requirement but also has significant benefits for businesses and their users. To achieve web accessibility, designers must follow standard guidelines and best practices, use available testing tools and resources, and integrate accessibility into the design process from the beginning. By making web accessibility a priority, we can create a more inclusive online experience for everyone.']	['<urn:uuid:96b7e045-22ee-42ab-b8eb-5f72fc7871df>', '<urn:uuid:21dda411-a473-44f5-affb-a35ee2c688ec>']	factoid	with-premise	short-search-query	similar-to-document	three-doc	novice	2025-05-12T20:58:04.490895	7	75	2203
9	What are the security benefits of implementing encryption through the DWT architecture, and what specific hardware features are required in signal processors to support real-time multimedia processing?	The DWT architecture provides lightweight encryption with zero hardware overhead, offering sufficient security for mobile multimedia applications through parameterization that enables up to 144-bit keyspace for 512x512 images. To support such real-time multimedia processing, signal processors require Harvard architecture with dual-ported memory for simultaneous data access, sophisticated interrupt handling with fast context switching, and computational units with features like saturation logic and single-cycle barrel shifters. They also need dual address generators and circular buffer hardware for efficient data handling.	"['Implementation of an Efficient Hardware Architecture for Multimedia Encryption and Authentication using the Discrete Wavelet Transform\nDhanalaxmi Banavathα & Srinivasulu Tadisettyσ\nThis paper introduces a zero-overhead encryption and authentication scheme for real-time embedded multimedia systems. The parameterized construction of the Discrete Wavelet Transform (DWT) compression block is used to introduce a free parameter in the design. It allows building a key space for lightweight multimedia encryption. The parameterization yields rational coefficients leading to an efficient fixed point hardware implementation. Comparison with existing approaches was performed to indicate the high throughput and low hardware overhead in adding the security feature to the DWT architecture. The project will be implemented using HDL. Simulation will be done to verify the functionality and synthesis will be done to get the NETLIST. Simulation and synthesis will be done using Xilinx Tools.\nKeywords: compression, encryption, DWT, multimedia.\nAuthor α σ: Ku College of Engineering & Technology, Kakatiya University, Warangal, T.s., India.\nDigital image processing is an area characterized by the need for extensive experimental work to establish the viability of proposed solutions to a given problem. An important characteristic underlying the design of image processing systems is the significant level of testing & experimentation that normally is required before arriving at an acceptable solution. This characteristic implies that the ability to formulate approaches & quickly prototype candidate solutions generally plays a major role in reducing the cost and time required to arrive at a viable system implementation.\nAn image may be defined as a two-dimensional function f(x, y), where x & y are spatial coordinates, & the amplitude of f at any pair of coordinates (x, y) is called the intensity or gray level of the image at that point. When x, y & the amplitude values of f are all finite discrete quantities, we call the image a digital image. The field of DIP refers to processing digital image by means of digital computer. Digital image is composed of a finite number of elements, each of which has a particular location & value. The elements are called pixels.\nA new parameterized construction of a DWT filter with rational coefficients is proposed. The parameterized construction can be used to build a key scheme while the rational coefficients of the DWT enable an efficient hardware architecture using fixed point arithmetic. The DWT, is an essential part of modern multimedia compression algorithms, thus serves as a transformation- cum-encryption block. The main contribution of this work can be summarized as ‘Introduction to the concept of the parameterized DWT architecture for providing security to the images’. The new DWT architecture implements DWT as an encryption operation, Optimize and pipeline the hardware architecture to achieve a high clock frequency of 244 MHz with minimum hardware requirements, Provide some experimental results of image encryption and watermarking using the parameterized DWT operation.\nImage compression algorithm, have the property that the bits in the bit stream are generated in order of importance, yielding a fully embedded code. The embedded code represents a sequence of binary decision that distinguishes an image from the “null” image. Using an embedded coding algorithm, an encoder can terminate the encoding at any point there by allowing a target rate to be met exactly. Algorithm, which generates a separate embedded bit stream for each code-block, is named as Bi. The coder is essentially a bit-plane coder.\nThe wavelet transformation divides image to low and high pass filtered parts. The traditional JPEG compression technique requires lower computation power with feasible losses, when only compression is needed. The methods are intended to the applications in which the image analyzing is done parallel with compression. Furthermore, high frequency bands can be used to detect changes or edges. Wavelets enable hierarchical analysis for low pass filtered sub-images. The first analysis can be done for a small image, and only if any interesting result is found, the whole image is processed or reconstructed.\nMultimedia data security is important for multimedia commerce. Previous cryptography studies have focused on text data. The encryption algorithms developed to secure text data may not be suitable to multimedia applications because of large data sizes and real time constraint. For multimedia applications, lightweight encryption algorithms are attractive.\nWhile encryption standards such as DES and RSA can be used to encrypt the entire video file, but it main drawbacks, since multimedia data is usually large and requires real-time processing, DES and RSA incur significant overhead. Recent video encryption algorithms have focused on protecting the more important parts of the video stream to reduce this overhead.\nThe architecture of a fully pipelined AES encryption processor is made on a single chip FPGA. By using loop unrolling and inner-round and outer-round pipelining techniques, a maximum throughput of 21.54 Gbits/s is achieved. A fast and area efficient composite field implementation of the byte substitution phase is designed using an optimum number of pipeline stages for FPGA implementation. Advanced Encryption Standard has led to intensive study of both hardware and software implementations.\nA high performance encryption/decryption core of the advanced encryption standard (AES) is also presented. This architecture is implemented on a single-chip FPGA using a fully pipelined approach. The results show that this design offers up to 25.06% less area and yields up to 27.23% higher throughput than the fastest AES. FPGA implementations reported to date\nThese restrictions can be alleviated by developing a scheme that integrates both encryption and compression operations into in a Figure 1.\nFigure 1: Lightweight multimedia encryption scheme\nConsider an example to explain the significance of lightweight multimedia encryption schemes for embedded systems. In Figure 2 a surveillance aircraft (A) is sending aerial surveys and other important information to the ground troops (B), crucial for their attack on the enemy base (C). In this scenario, typical encoding schemes would require large computational resources and hence high power consumption making them unsuitable for real-world embedded systems. Moreover such conventional ciphers would incur a large latency in image transmission which can be critical for ground troops’ (B) operation.\nFigure 2: Example scenario for proposed lightweight multimedia encryption\nEfficient hardware architecture using parameterized DWT\nFor image compression purposes, JPEG2000 recommends an alternate row/column-based structure as presented in Figure 3\nFigure 3: 3-Level DWT decomposition tree\nTo obtain the first transformation level of DWT the BWFB derivation was done. Through this derivation two filter characteristic equations namely synthesis low pass filter and analysis high pass filter characteristic equations were obtained, which are as follows:\nH1 (z) = [(-9/64) a+ (1/32) a2 + (15/64) - (1/8) (1/a)] (z4+1/z4) + [(-1/16) a2 + (11/32) a - (11/16) + (1/2) (1/a)] (z3+1/z3) + [(1/8) - (1/2) (1/a)] (z2+1/z2) + [(-11/32) a + (1/16) a2 + (15/16)-(1/2) (1/a)] (z+1/z)+[(9/32) a-(1/16) a2 - (7/32) + (5/4) (1/a)] …………………….(1)\nH2 (z) = [(1/32)-(1/32) a](z3+1/z3) + [(1/8)-(1/16) a] (z2+1/z2) + [(7/32) + (1/32) a] (z+1/z) +\n[1/4+ (1/8) a] …………………….(2)\nThese two equations mainly consists of more number of adders, multipliers, and irrational coefficients which results in much requirement of hardware, thus more power consumption, more delay and reduced amount of efficiency. Thus to overcome the above problem the above equations are simplified to their binary equivalent form, which can be expressed as follows:\nH1 (z) = [-(1/23+1/26) a + (1/25) a2 + (1/22-1/26) – (1/23) (1/a)] (z4+1/z4) + [(-1/24) a2 + (1/22+1/24+1/25) a + (1/2+1/23+1/24) + [(1/2) (1/a)] (z3+1/z3) + [(1/23-1/2 (1/a))] (z2+1/z2) +[-(1/22+1/24+1/25) a + ( 1/24) a2 + (1-( 1/24) – (1/2)(1/a)](z+1/z)+[ (1/22+1/25) a -( 1/24) a2 - (1/22-1/25) + (1+ 1/22) (1/a)]………………………(3)\nH2 (z) = [(1/25-(1/25) a] (z3+1/z3) + [(1/23- (1/24) a] ((z2+1/z2) + [(1/22-(1/25) + (1/25) a])] (z+1/z) + [(1/22 + (1/23) a] …………………..(4)\nThus it can be concluded that the above equations mainly consists of less number of adders, multipliers and shifting operations only, which can be implemented using less amount of hardware.\nV. DATA ENCRYPTION\nMounting concern over the new threats to privacy and security has lead to widespread adoption of cryptography. Cryptography is the science of transforming documents. It has main functions are encryption and decryption. Figure 4 shows process of Encryption and Decryption only after decoding the cipher text using the key the content of the document is revealed to the common people Figure 4: Encryption and Decryption.\nFigure 4: Encryption and Decryption\n5.1 Implementation of Encryption through DWT Architecture\nUsing free parameter ‘a’ which is introduced in previous Parameterized DWT, we can provide encryption for image with a zero-overhead of hardware. Number of bits of keyspace ‘a’ depends of image which is going to transmit and DWT decomposition. For Example the number of DWT operations ‘N’ in an image of size M×M pixels is bounded by the limit N≤. we can obtain go up to maximum of nine levels of wavelet decomposition for an image of size 512×512 pixels. One level of wavelet decomposition involves two filtering operations: one each along the row and column directions. 9x2 (rows + column) =18. Thus, we can choose up to 18 different ‘a’ values, one each for the 18 different instances of DWT kernels being used in the operation. Each ‘a’ represents 8-bits so totally 144-bits of keyspace for 512x512 image. 18x8=144-bits keyspace. These 144-bits keyspace can be used to encrypt the input frame. This level of security is sufficient for any mobile multimedia application.\nThis paper introduces a multimedia encryption and watermark authentication framework based on parameterized construction of DWT. The parameterization enables an efficient, pipelined, high throughput implementation in hardware. The qualitative and quantitative results in terms of both hardware performance and image security promise a secure framework for real-time multimedia delivery over embedded systems\nThe idea of parameterization can also be extended to other multimedia encoding blocks to obtain a more powerful integrated-encryption-scheme for embedded multimedia systems.\n- Ali Saman Tosun and Wu-chi Feng “Lightweight Security Mechanisms for Wireless Video Transmission”, Department of Computer and Information Science.\n- Ankush Mittal “Content-based Network Resource Allocation for Mobile Engineering Laboratory Applications”\n- Chakrabarti. C, Vishwanath. M, and R. M. Owens, “‘A Survey of Architectures for the Discrete and Continuous Wavelet Transforms.”\n- Changgui Shi and Bharat Bhargava “An Efficient MPEG Video Encryption Algorithm”, Department of Computer Sciences”.\n- Dominik Engel and Andreas Uhl “Parameterized Biorthogonal Wavelet Lifting for Lightweight JPEG2000 Transparent Encryption”\n- Herrero, J. Cerdà, R. Gadea, M. Martínez, A. Sebastià “Implementation of 1-D Daubechies Wavelet Transform on FPGA”\n- Abdullah Al Muhit, Md. Shabiul Islam and Masuri Othman “VLSI Implementation of Discrete Wavelet Transform (DWT) for Image Compression” 2nd International Conference on Autonomous Robots and Agents December 13-15, 2004 Palmerston North, New Zealand.\n- Acharya. T and Chakrabarti. C, “A Survey on Lifting-based Discrete Wavelet Transform Architectures,” Journal of VLSI Signal Processing Systems, vol. 42, no. 3, pp. 321–339, 2006.\n- Bilgin .A, “Quantifying the parent-child coding gain in zero-tree-based coders,” Signal Processing Letters, IEEE, vol. 8, no. 3,pp. 67–69, Mar 2001.\n- Christopoulos. C, Skodras. A, and Ebrahimi. T, “The JPEG2000 still image coding system: an overview,” IEEE Transactions on Consumer Electronics, vol. 46, no. 4, pp. 1103–1127, Nov 2000.\n- Joseph J.K. _O Ruanaidh and Thierry Pun “Rotation, Scale and Translation Invariant Digital Image Watermarking”', 'Increasingly, electronic equipment applications involve signal processing. Home theatre, computer graphics, medical imaging and telecommunications all rely on signal-processing technology. Signal processing requires fast math in complex, but repetitive algorithms. And many applications require computations in real-time: i.e., the signal is a continuous function of time, which must be sampled and converted to digital, for numerical processing. The processor must thus execute algorithms performing discrete computations on the samples as they arrive.\nThe architecture of a digital signal processor (DSP) is optimized to handle such algorithms. The characteristics of a good signal processing engine include: fast, flexible arithmetic computation units (e.g., multipliers, accumulators, barrel shifters); unconstrained data flow to and from the computation units; extended precision and dynamic range in the computation units (to avoid overflow and minimize roundoff errors); dual address generators (for simultaneous handling of both inputs to a dyadic operation); efficient program sequencing (including ability to deal with loops and interrupts effectively); and ease of programming.\nA DSP has some of these features in common with a reduced-instruction-set-computer (RISC). In addition, both are constructed around certain core instructions, enabling them to operate at very high instruction rates; and both eschew internal microcode. However, they are fundamentally different ""animals"". The differences between RISCs and DSPs are most pronounced in the processors\'\n- computational units\n- data address generators\n- memory architectures\n- interrupt capabilities\n- looping hardware\n- conditional instructions\n- interface features\nDSPs belong to two basic classes: fixed point, a (typically) 16-bit architecture based on 16 bit integer data types, and floating point, usually with a 32-bit architecture, based on a data type that has both mantissa and exponent.\nComputational Units: DSPs all contain parallel hardware multipliers to support single-cycle multiplication, and their multipliers often combine multiplication and accumulation in a single cycle. DSPs have dedicated accumulators with registers significantly wider than the nominal word size to preserve precision-for example, 80 bits in the 32-bit ADSP-2106x SHARC family (Figure 1). Hardware may support recovery from accumulator overflows, as with the ADSP-21xx family. In addition, DSPs all contain full-featured arithmetic-logic units (ALUs), independent of the multiplier.\nThe ALU may have special features, such as the ability to produce simultaneous sums and differences to accelerate the kernel routine in the fast Fourier transform (FFT)-an algorithm for transforming signals between the time domain and the frequency domain. An advanced DSP will contain saturation logic in the computational units to prevent data overflow. It also may offer zero-overhead (i.e., without requiring additional clock cycles) traps to interrupt routines on arithmetic exceptions.\nA sophisticated DSP may also contain a single-cycle barrel shifter (i.e., one capable of shifting a word an arbitrary number of bits left or right in one clock cycle), with a priority encoder for data scaling, data compression/expansion or packing/unpacking and bit manipulation. It may also include dedicated hardware to minimize the time required for fast division, square root, and transcendental-function calculation. Computational elements with these specialized features are not found on RISC processors.\nAddress Generation: An efficient DSP will keep its computational units fed with data from at least two independent data-address generators. Tapped delay lines and coefficient buffers are characteristic of DSPs, yet are mostly unknown in general-purpose computing. An efficient DSP needs circular-buffer hardware to support the buffers. Circular buffer pointers need to be updated every cycle without overhead. Furthermore, a comparison test for end-of-buffer needs a no-delay command to reset the pointer at the end-of-buffer. On the other hand, a RISC processor requires an additional cycle for each comparison test.\nMemory Architectures: DSPs typically support system memory architectures that differ from those in general-purpose computing systems. DSPs utilize a Harvard architecture, which permits sustained single-cycle access to two words of data from two distinct external memories. Analog Devices SHARC DSPs, for example, feature 2 or 4-Mbits of dual-ported SRAM integrated on-chip. This memory is directly addressed-not a cache, as would be found in RISC processors. To the CPU, this on-chip memory looks like a unique piece of memory, not merely a high-speed replica of memory elsewhere in the system. The reason is that DSPs are typically embedded processors. Their on-chip memory is often adequate to contain the complete, repetitive DSP program necessary to the task. Each memory block is dual-ported for single-cycle, independent accesses by the core processor and I/O processor or DMA controller (Figure 2). The dual-ported memory and separate on-chip buses allow two data transfers from the core and one from I/O, all in a single cycle.\nInterrupt capabilities: Because DSPs are intended for operation in real-time systems, efficient, sophisticated, and predictable interrupt handling is critical to a DSP. RISC processors, with their highly-pipelined architectures, tend to have slow interrupt response times and limited interrupt capabilities. Context switches should be very fast. Advanced DSPs, like the new ADSP-21csp01 and Analog Devices\' ADSP-2106x floating-point family support complete sets of alternative registers, allowing a single-cycle switch of context to support interrupt handling. (Register-file windowing differs, in that its purpose is to accelerate parameter passing, not save an entire context.)\nAn advanced DSP will support at least four independent external interrupts in addition to internal interrupts. Interrupt latency will be kept to just a few cycles and must be predictable. Interrupts should be nestable and prioritizable. In addition, it should be easy to enable and disable particular interrupts in real time.\nHardware looping: Efficient looping is critical to digital signal processing because signal-processing algorithms are repetitive. A good DSP will support zero-overhead loops with dedicated internal hardware. That is, the chip will monitor loop conditions and terminations to decide-in parallel with all other operations-whether to increment the program counter or branch without cycle-time penalty to the top of the loop. A RISC processor, on the other hand, has to do a test-and-branch at the end of every loop, costing at least an additional cycle every loop and every pass. Nested loops are also very common in signal processing algorithms; the DSP looping hardware should support a depth of at least four levels of nested loops. RISC processors have yet to evolve to support these basic signal processing needs.\nConditional execution: Data-dependent execution is important for signal processing. For this reason, advanced DSPs, like the ADSP-2100 family and the ADSP-2106x SHARC floating-point family, support conditional execution of most of their basic instructions: in a single instruction, the processor tests a condition code and, if true, performs an operation in the same cycle. This can make an enormous difference to computationally intensive algorithms. Intel discovered this problem with the i860 and added a graphics unit to handle the conditional store operations necessary for high-performance Z-buffering.\nInterfaces: DSPs operate on real-world signals coming from analog-to-digital converters, and they send their results to D/A converters. For this reason, DSPs often contain serial ports for an inexpensive interface to these devices. Advanced DSPs add hardware to make the operation efficient, for example, double-buffering and auto-buffering. Because these input/output signals may come from and go to nonlinear codecs, an advanced DSP may have dedicated hardware for zero-overhead A-law and µ-law companding. In addition, the serial ports may have features to simplify interfacing with T1 and CEPT data transmission lines.\nThe SHARC serial ports are designed to maximize throughput with hardware that is flexible, yet tuned to various signal types. Their features include: each serial port can automatically receive and/or transmit an entire block of data, independently transmit and receive-each with a data buffer register as well as a shift register, multi-channel mode for TDM.\nProgramming considerations: At one time, a significant difference between DSPs and RISCs was their programming models. DSP is inherently performance-driven, so programming of DSPs was done mostly in assembly language to get the best performance from the processor. This is generally still true for fixed-point DSPs, but much more easily so with the ADSP-2100 family\'s intuitive algebraic assembly language (Figure 3). At no sacrifice to performance, it ameliorates the ease-of-use issue that drives many programmers to favor high-level languages like C.\nOn the other hand, floating-point DSPs are more efficiently programmed in high-level language. Floating-point calculations avoid fractional data types, which do not exist in C. In addition, architectural decisions can affect compiler efficiency. The large, unified address space of the ADSP-2106x SHARC family, for example, makes memory allocation easier for the compiler. In addition, their large, flexible register file improves efficiency.\nCentral to our product strategy is providing the tools and DSP cores that make it possible to efficiently program our fixed- and floating-point DSPs in high-level language. This is the driving force behind the ADSP-21csp, a new family of concurrent signal processors. Nevertheless, though using high-level languages, a DSP programmer must be able to descend in language level (with minimal pain) to improve performance of time-critical routines.\nIncreasingly, DSP designs are programmed in this sequence: first, a software prototype is written and debugged in a high-level language. This prototype often results in adequate performance. More generally however, increased performance will be required, so the high-level code is histogrammed in simulations to find the sections needing the most execution time. The critical sections are then hand-coded in assembly language. The histogramming and hand-coding process is iterated until performance targets are met.\nWhile the differences between DSPs and RISCs are many, the two architectures tend to converge in the area of programming, a convergence driven by time-to-market and the evolving role of DSP in applications. Programmers, skilled at quickly developing working C programs, use them to bring product to market faster. Meanwhile, DSPs take on more system-management functions, such as the user interface or system control, and will need to offer high-level language efficiency to compete with the µCs and RISC processors formerly assigned these control tasks.']"	['<urn:uuid:ebfc6002-2062-4963-96d8-37d90c11b4b4>', '<urn:uuid:65d17abe-c079-405d-98a3-c4bb316c7555>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:58:04.490895	27	79	3455
10	why do people trade goods services when does market competition become more intense	People trade goods and services because they expect to gain from the exchange - they give up something of lesser value to get something of more value. Both parties in a trade must expect to benefit, otherwise the exchange won't occur. Market competition becomes more intense in several situations: during the decline stage of an industry's life cycle when growth slows down, when globalization allows more foreign players to enter the market, when technological advances create substitute products, and when transportation improvements make it easier for goods to flow between markets. Competition also intensifies when industry growth rates decline, forcing companies to fight over existing customers to maintain sales.	['Trade is the voluntary exchange of goods and services. People engaging in trade must be willing to bear a cost (give up something). Therefore, we know that people will only participate voluntarily when they expect to gain from the exchange. If even one of the trading partners believes he cannot gain, the exchange will not take place.\nEconomic Content Standards:\nStandard 5: Voluntary Exchange\nVoluntary exchange occurs only when all participating parties expect to gain. This is true for trade among individuals or organizations within a nation, and among individuals or organizations in different nations.\n- Exchange is trading goods and services with people for other goods and services or money.\n- People voluntarily exchange goods and services because they expect to be better off after the exchange.\n- When people buy something, they value it more than it costs them; when people sell something, they value it less than the payment they receive.\nThis lesson involves students in a trading simulation designed to illustrate a complex marketplace in which goods and services are traded. Students use this experience to investigate the conditions that encourage or discourage trade among individuals.\nOne class period.\nA large number of small, easy to exchange items – such as miniature candy bars, small boxes of raisins, inexpensive small toys, packages of sticky notes, pencils, stickers, library passes, hall passes, answers to a quiz, etc., and enough small brown bags for each student.\n1. Before beginning the simulation, place the trading articles, unequally, in brown paper bags and seal them. Divide the bags into groups of about 5 or 6, depending on class size, and mark all bags in each group with the same colored dot or letter. Mix all the bags together in a large box or trash bag.\n2. Explain to students that today they have an opportunity to participate in a trading activity. The purpose of the activity is to explore why people trade.\n3. Ask, “Why do people trade?” Record some student responses on the board and indicate that these responses are hypotheses. Explain to the class that today’s activity will provide information and experience with which to test the hypotheses.\n4. Describe the following situation to the class. Imagine that a teenage music lover walks into a music store, picks out the latest CD by his favorite artist and pays the owner $18. Who gained and who lost in this transaction?\n(Both people gained in the trade. The music lover gave up something of lesser value, $18, to get something of more value, the CD. The owner gave up something of lesser value, the CD, to get something of more value, $18. Both the music lover and owner ended up with something of more value to them. Hence, they both gain.)\n5. Announce to students that you are going to give them bags, which they will then own, and ask them not to open the bags until told to do so. Randomly distribute the bags and emphasize” “Whatever is in the bag is yours.”\n6. Ask students to open their bags and look at the object WITHOUT removing it from the bag or showing it to anyone else. Direct students to rate their satisfaction with the bags using a show of hands and a 1-5 rating system in which 5 is high and 1 is low.\n(Ask for a show of hands for each rating – 1, 2, 3, etc., and record tally on the board or overhead transparency. Caution: Ensure that every student votes in each round.)\n7. Tell students they may now take the objects out of the bags. Direct them to move to designated locations around the room according to the symbol (or color) on their bags. Remind students that “whatever is in the bag is yours,” and that they may trade or not, open the package or not, trade parts or all or nothing. After several minutes, direct students to return to their seats. Repeat the 1-5 evaluation by show of hands, reminding students to rate what they now have in their possession, and reminding them that they must rate their bags again whether they traded or not. (Note that students may change their ratings even if they don’t trade. Be sure that every student votes, even if he hasn’t traded or changed his satisfaction rating. Record the tally on the board with a different colored marker than the first tally.)\n8. Conduct one or more additional trading rounds, combining groups, etc. with the last round involving all class members. In each successive round, increase the size of the trading area by combining groups. (For example, blues and greens may trade, yellows and reds may trade, etc.) In the last round allow students to trade with anyone in the class. Do a “satisfaction” rating after each round and record the tally.\n9. While the students are trading, or after all trading is completed, calculate the total “satisfaction points” for each round. Record the total below each tally.\n- How many people made trades?\n- Ask several traders what they traded and why.\n- Follow the students’ explanations by asking how they felt after the trade. (Most students will be “happier,” and will feel that they got the best end of the deal.)\n- Find the student who was the other party to the exchange and ask why he/she traded and how he/she felt after the trade. (Most of the trading partners will also be “happier.” If a student does not report feeling better off, find out why. This is a chance to emphasize that costs occur in the future and that sometimes we make mistakes in anticipating that we’ll benefit more than we actually do. See debriefing questions on cost below.) (Most of the students will have made trades; however, there will be a few who were either satisfied with what they had and did not trade, or who had something that no one would trade to obtain.)\n- Go back to some of the students who discussed their trades in response to question #1 and ask what it cost to make the exchange. (Students had to give up some or all of what was in their possession in order to make the exchange. Emphasize the definition of opportunity cost as the foregone alternative. Emphasize that costs exist because of scarcity.)\n- Did anyone trade more than once? Why? Did anyone not trade? Why?\n(Several people should have made numerous trades. Elicit articulation of the fact that the trades continued only as long as the traders perceived they would continue to benefit. Even the person making multiple trades stopped when anticipated no gain from the next trade. Then call on some of the students who did not trade at all, and ask why they didn’t. Expect to hear either that the person saw nothing he valued more than what he had originally and wouldn’t trade, or that no one else valued what was originally in the bag, so the person couldn’t trade. Emphasize, again, that voluntary trade is based on the mutual perception of benefit.)\n- Point to the tally of satisfaction points on the board as empirical evidence of “increased wealth.”\n- How did wealth increase when nothing new was added? (There was more wealth because through voluntary trade, the articles in the bag went from people who valued them less to people who valued them more, increasing the wealth of both trading partners.)\n- What generalizations might we make about trade based on how the tally changed from round to round? (Expect a variety of answers, including: The value of things is subjective; some people value a particular thing more than other people do.[Some students may comment that subjective valuations change just by having something to compare to. Some people changed the ratings of their objects after seeing what other people had.] Having more trading partners was better than having few. More trades meant more satisfaction points; more trade means more wealth. Etc.)\n- Why do people trade?\n(People trade to get something of more value by giving up something of less value.)\n- Did trading behavior confirm or contradict the hypotheses we listed at the beginning of the activity? (Often the initial responses are “to get something they don’t have, or to take advantage or sucker someone.” Students should see that trade only takes place when both parties expect to gain. Sometimes, however, the gain is not material as when students trade to make someone else feel good.)\n- Was it possible to trade without bearing a cost? Why? (No. Because of scarcity, we cannot have everything we want. There is always a trade-off.)\n- What was the cost and what was the benefit of each trade? (What was traded away was the cost of the trade. What was received was the benefit.).\n- What were the necessary conditions for wealth-creating trade to take place? (Emphasize the importance of 2 “rules of the game” (institutions):\n- property rights – remind students that you emphasized that what was in the bags was theirs, and\n- voluntary exchange – no one was forced to make an exchange.\n- What would have happened if you had been forced to trade? (Students should recognize that they would not have experienced the same overall increase in satisfaction.)\nEncourage students to look for the role of institutions as we continue our study of economics.\n- Does the creation of wealth make everyone happy? (Definitely not. Students who had little to trade may not have been pleased. Students who couldn’t find what they wanted may have been dissatisfied. Students who traded and then realized they missed a better trade may have been unhappy. Students who either underestimated the costs of a trade or overestimated its benefits — or both — may have been unhappy. Emphasize that economists don’t say that trade make people happy; they argue that it creates wealth. Also note that saying “trade creates wealth” doesn’t mean that every individual person will be wealthier. Economists merely maintain that trade creates wealth overall and that trade will continue if people anticipate that they will be better off after the trade than if they do not trade at all.)\n- If we were to observe twenty people buying items at an outdoor Farmers market, what could we conclude about their gains and losses? Their wealth?\n(Each transaction takes place because both parties expect to gain. If one party does not expect to gain, there is no transaction and we would have nothing to observe! Therefore, we observe people trading money for we can conclude their wealth and the wealth of the grocery store has increased.)\nThe Magic Of Markets combines procedures used in “Why Do People Trade,” Master Curriculum Guide In Economics International Trade, by Donald R. Wentworth and Kenneth E. Leonard (Copyright 1988 by the National Council on Economic Education, 1140 Avenue of the Americas, New York, NY 10036. All rights reserved. No part of the Master Curriculum Guide In Economics International Trade materials may be kept in an information storage or retrieval system, transmitted, or reproduced in any form or by any means without permission in writing from the National Council. Used with permission.) Questioning strategies and procedures developed by Kathy Ratté and Kenneth Leonard for the Foundation for Teaching Economics.\nFoundation for Teaching Economics is proud to announce that Debbie Henney, director of curriculum for the Foundation for Teaching…\nTed Tucker, Executive Director, Foundation for Teaching Economics October 26, 2022 More high schools are offering courses on personal finance…', 'Some markets become competitive for several reasons. Globalization is the first reason. It makes the competition map wider because it involves foreign players. For example, it encourages foreign goods to easily enter the domestic market, increasing supply. Thus, domestic companies must compete not only with other domestic players but also with foreign companies.\nThen, if we relate to Porter’s five forces, markets become competitive, for example, when substitutes emerge. They may offer lower prices, more readily available in the market, or superior quality. Thus, substitute products divert consumers from the current market. Good examples are the electric car and conventional car markets. Both are close substitutes. Increasing consumer awareness of the environment is driving customers to switch to electric cars, reducing the market size of conventional cars.\nThe industry life cycle also has implications for competition. For example, competition will tend to be intense when the industry enters a decline stage. This is because players can only rely on repeat purchases from existing customers, encouraging them to fight over each other’s customers.\nBefore going into more detail about what factors drive markets to be competitive, let’s briefly discuss the industry life cycle and Porter’s five forces model. Both are important to explain how market competition is becoming more intense.\nWhat is Porter’s five forces model?\nMichael Porter outlines five forces to explain competitive pressures in the marketplace. In some applications, these five forces explain why profitability in one market is higher than in another, associated with competitive pressures. The five forces are:\n- Threat of new entrants\n- Threat of substitution\n- Bargaining power of buyers\n- Bargaining power of suppliers\n- Rivalry between players\nThe first four determine the rivalry between players in the market. In other words, rivalry in the market depends not only on the nature of the market, for example, the number of players and their size, but also on the other four forces.\nFor example, competition becomes more intense when barriers to entry are low. Thus, it is easy for new players to enter the market. They bring additional capacity to the market, push prices down, and increase pressure on incumbents.\nOr, increased rivalry occurs due to a high threat of substitution. Substitution diverts customers out of the market, lowering demand in the market. As a result, existing players have to compete more fiercely for a smaller market share.\nRivalry between players\nCompetition is high when many players in an industry of relatively similar size. Thus, each cannot affect output and prices in the market.\nIn addition, it is difficult for companies to coordinate competitive strategies with many players of similar sizes, such as through signaling. Hence, they have no chance to engage in unfair competition, such as engaging in collusion.\nThe high rivalry may be due to internal factors, namely the nature of the market. For example, markets require low economies of scale and small investments to operate and compete effectively. Thus, many small companies easily enter the market.\nOr, high rivalry can also occur due to external factors. For example, the government lifted investment bans for foreign investors or trade barriers. These policies allow more players to enter the market.\nWhat is an industry life cycle?\nIndustry life cycles tell us how an industry evolves over time. It involves several stages, each of which has implications for market size, growth, profitability, and competition. The five typical stages include:\nIn the introduction stage, competition is low. Even so, companies usually still face losses. Zero or very low revenue. But, on the other hand, costs are still high because they have to educate consumers about why they should use the product. Most consumers are not aware of the products offered.\nThen, in the growth stage, competition is also still low. Companies can generate sales by acquiring new customers. The growth potential is still large, opening up opportunities for all players to record high sales.\nIn the early period of the growth stage, demand comes mostly from new customers. But, once it was close to the maturity stage, the new customers available started to become scarce. And players are growing from repeat sales, and some may still be able to acquire new customers.\nThen, in the maturity stage, demand comes mostly from repeat sales. Growth slowed, and competition began to intensify as players had to seize customers from competitors to increase market share and record high sales.\nFinally, in the decline stage, some companies stay in the industry. Others opted out to pursue growth in other markets.\nExisting companies struggled to book sales. As industry growth declines, seizing customers from competitors is the only way to maintain sales. This situation leads to intense competition, putting pressure on profitability.\nThen, to reduce the competition intensity, the industry usually consolidates. For example, some may merge with other companies. Or, a big player acquires a smaller competitor.\nWhy are some markets becoming more competitive?\nCompetitive rivalry can be attributed to changes in Porter’s five forces or industry life cycles. For example, the technological change raises the threat of substitution, increasing competition because existing players do not only face players in the market. However, they also have to compete with players in the substitute market to satisfy customer needs.\nIn addition, technological advances can also speed up the cycle, making the market quickly transition from a growth stage to a decline stage. Portable music players or MP3 players are good examples, where technology is quickly being replaced by smartphones, likewise with cameras and camcorders.\nGlobalization is the first reason why some markets are becoming competitive. It is a process by which the world becomes more integrated. Thus, foreign goods easily flow into the domestic market. But on the other hand, domestic goods also easy to flow into the international market.\nGlobalization increases competition through increasing supply to the domestic market. Thus, the company does not only compete with local players but also foreign players. Although foreign players do not operate in the domestic market, they can sell their products by exporting to the domestic market.\nAnd globalization makes trade more intensive. It requires countries to remove or reduce trade barriers such as tariffs and quotas. Thus, the barriers to entry into the domestic market will be lower.\nDue to low barriers to entry, the domestic market involves more players to meet consumer needs. It’s not only local players but also foreign players, although maybe not directly. This situation makes the rivalry between players increasingly fierce.\nIn addition, globalization also makes it easier for consumers to buy foreign goods. In addition to low trade barriers, access to international markets is easier because the information is more readily available. As a result, it increases the bargaining power of buyers. If they dislike domestic products, for example, because they are expensive or of poor quality, they can switch to foreign products.\nImprovement in transportation\nTransportation improvements make goods easier and cheaper to transport, either from the international market to the domestic market or from one region to another within the country. As a result, it increases the supply in the market.\nGlobalization is becoming increasingly intense, one of which is driven by improvements in transportation. Combined with advances in technology, transportation costs have decreased. In addition, technological advances have also accelerated delivery from one country to another through innovation in transportation modes.\nThe internet is the reason why some markets have become competitive. It not only increases the bargaining power of consumers. But it also lowers switching costs.\nFor example, the internet, combined with technological advances, facilitates us to find suitable products. We can find many alternatives to buy through our smartphones. Thus, it improves our bargaining position against the company. For example, if we don’t like local products, we can easily surf to find alternative products abroad.\nIn addition, the internet also lowers switching costs. It makes information more available, thereby reducing search costs. For instance, in the past, we might have had to go from one store to another to find a suitable product. But now, we don’t have to do that. Instead, we simply open our smartphones and surf various e-commerce sites to find what we are looking for. This reduces search costs, including effort, time, and transportation.\nTechnological advances also give rise to substitution. It increases competition because it satisfies similar needs. Thus, consumers turn to substitute markets when they are more readily available and cheaper. As a result, existing players must also compete with substitute products to satisfy customer needs.\nIndustry growth rate\nIndustry growth rates also affect competition. The decline in growth encourages competition to intensify as existing players compete for a smaller market share.\nCompetition becomes intense when market growth occurs in industries such as oil refineries with high fixed costs. Companies must achieve high sales to achieve economies of scale and break even. As growth declines, they must seize sales from competitors to maintain high sales.\nIn addition, reduced growth also increases competition when there is idle capacity. Companies struggle to drive sales to produce at optimal capacity. As a result, they may lower the selling price to boost sales. That could lead to a price war.\nThen, if we relate to the industry life cycle, negative growth occurs at the decline stage. At this stage, the company can only maintain sales by seizing customers from competitors. At this stage, all potential consumers have used the product; therefore, sales only come from repeat purchases.\nFor example, suppose a market consists of 100 companies. Meanwhile, market growth declined from $100 billion per year to $90 billion per year. Say, each company has the exact same market share. As a result, the market size per player fell from $1 billion to just $0.9 billion. If a company wants to increase sales to $1 billion, it must seize sales from competitors.\nConversely, if market growth increases to $120 billion, getting $1 billion in sales is easier without involving more intense competition with competitors.\nDecreased growth can occur for several reasons. For example, technological changes present substitute products in the market. It has the potential to divert purchases by existing consumers. Typewriters and personal computers are good examples. Personal computers have replaced typewriters and are now being replaced by laptops. Consumers switch from typewriters to personal computers, then to laptops. Likewise, e-commerce exists to substitute conventional retail, encouraging people to switch to online shopping.\nGovernment policies also give reasons why competition is competitive. For example, the government lifted the ban on foreign companies from investing in certain industries. The policy opens opportunities to add new players to the market, increasing competition.\nLikewise, privatization also opens up more competition in the market. If, in the past, the service was controlled by the government, it is now left to the private sector to be managed, allowing more private companies to get involved.\nThen, removing trade barriers also opens up more supply from abroad. Foreign goods become cheaper when they enter the domestic market, for example, because they are not subject to tariffs. Finally, it forced domestic players to lower prices to be competitive.\nWhat to read next\n- Competitive Market: Characteristics and Examples\n- Why are some markets becoming more competitive?\n- How Do Businesses Respond to A More Competitive Market?']	['<urn:uuid:8df930bd-5d40-4f0b-b1d4-0be7a4419bac>', '<urn:uuid:d0af79ea-0bbb-49cd-a0be-7927c9b6d4f3>']	open-ended	direct	long-search-query	similar-to-document	three-doc	novice	2025-05-12T20:58:04.490895	13	109	3792
11	difference yameru yameru kanji meanings	The verb 'yameru' can be written with different kanji depending on context: 止める (yameru) is used for the general meaning of 'to stop', while 辞める (yameru) is specifically used when talking about quitting a job.	['I’ve noticed that in Japanese they are many, many ways to say something, each with a different connotation or feeling. I’m sure this is the case in English as well, but I don’t typically think about it in the same analytical and logical way, so I haven’t really noticed it. Having said that, I feel that Japanese still wins out in this area if you look at how many expressions can be made just by changing the sentence a little.\nFor this post I’ll give a list of such variations on a simple theme, “stop”, in various shades of request or command. I’ll stick to expressions with the verb やめる, which could be written 止める in this case. (辞める is used when talking about quitting a job)\nI’ll try to explain the nuance of each of these, but for some of them its hard to put into words. When you hear these phrases used by someone, try and remember the context you heard them in and the feeling they gave you.\nRemember that just like in English, the feeling of the expression is modified by the tone of voice used and facial expression.\n- やめて => fairly neutral way to say “stop”\n- やめてね => softer/feminine way than above\n- やめろ => very rough way which can be seen as rude, used mostly by men\n- やめれ => slang version, roughly same as above\n- やめろよ => slightly softened version of the above\n- やめようよ => a softer way to express, “let’s stop”, as if warning someone what they are doing is wrong\n- やめなよ => masculine way to warn\n- やめなさい => strict command form, used by a superior to someone below him/her. Has a refined feel to it.\n- やめなちゃい => baby talk version of previous\n- おやめ => older phrase usually used by older women\n- やめたまえ => older, refined phrase used by older men\n- やめてくれ => “stop for me” with a slightly refined feel\n- やめてください => polite “please stop” (making a request)\n- やめちまえ => very aggressive/rude, with apathetic/uncaring feeling\n- やめるんだ => literally “you are going to stop”. If the pitch at the end of the word raises, it can have a positive connotation. If it drops, it has more a feeling of a warning.\n- やめたら？ => trying to convince someone its a good idea to stop, “why don’t you stop?”\n- やめれば？ => same as previous\n- やめたほうがいいよ => “It’s better if you stop”, warning someone\n- やめてくれる？ => “Would you mind stopping (for me)?” [and other variants with もらえる, etc.]\n- やめてくれない？ => Similar to above. Can be used when the other person is persistently doing something you don’t like.\n- やめてくれないかな => “I was wondering if you would stop (for me)”, softer indirect request\n- やめること => dry command, I have seen this used on the inside of a car hood\nUnderstanding the variations in tone and feeling between different expressions is one of the things needed to become truly fluent. You’ll naturally distinguish these in your native language, but it will require some extra effort for most people studying a foreign language.']	['<urn:uuid:869e42de-3d79-4262-ae6e-71e8544d52ed>']	open-ended	direct	short-search-query	similar-to-document	single-doc	expert	2025-05-12T20:58:04.490895	5	35	524
12	blockchain permissioned vs talmud change process	Permissioned blockchain networks allow specific entities to establish protocols and verify transactions, similar to how the Talmud enables gradual changes through interpretation of past work. However, while the Talmud strictly prohibits rejection of past interpretations, blockchain networks must additionally implement technical security controls and identity management to prevent unauthorized changes and maintain system integrity.	['The Jewish Talmud is a remarkable object. It is the product of hundreds of years of intense, rigorous, and highly formal debate and scholarship. It has served as the backbone of the Jewish people. The trunk of the tree.\nThe Talmud has a very interesting property, the inspection of which will prove illuminating.\nFirst, one of the fundamental rules of Talmudic scholarship is that a recent scholar cannot contradict, reject, or overrule an older scholar. If a recent scholar takes issue with an historical analysis, the only avenue available to them is to reinterpret the intention of the older scholar. This chain of interpretation goes back, in an unbroken chain, to the first commentaries and ultimately to the old testament and the ten commandments.\nAs such, one can in theory always trace a current value, decision, or opinion back through history. Further, a concept or idea cannot be introduced arbitrarily, but must be rooted and stem from an existing concept or idea. Similarly, once an idea has been accepted, it can never be fully rejected, only reinterpreted.\nThe quality of the interpretations, as well as the intention of the interpreters, is a subject of ongoing debate and, well, interpretation. But this property in general holds.\nThis has several important implications.\nFirst, there is no possibility of complete revolution. A “revolution” in Judaism, defined as a complete rejection of what has come before and the attempt to institute a new faith on entirely new foundations, could never occur. If such a thing were attempted, those individuals would be seen as a new sect, ultimately disconnected from primary Judaism. The core of Judaism, defined as those who adhere to the teachings of the Talmud and associated texts, is fundamentally connected to this canon. Jews hold the Talmud as the primary authority. No one is forcing the Jews to respect the Talmud; it is simply that the study of the Talmud and its instruction is the common denominator for the Jewish identity. Any individual Jew is, at any time, free to completely reject the entirety of the Talmud. Such a person, however, would cease to be accepted by their community. In this way, the Talmud coordinates the self-identifying community of Jews.\nSecond, the faith is capable of substantial dynamism. Interpretations can be fanciful and radical. Although new work must be based on historical scholarship, it is often the case that scholars will not agree with their contemporaries. In this way, the faith subdivides into movements, each respecting a particular strand of interpretation.\nThird, there is no need for a central authority. The extent to which any individual person holds themselves accountable to these laws and interpretations is, of course, a personal decision. Orthodox Jews take these laws very literally, and yet various schools of the orthodoxy have varying interpretations of some of the more ambiguous implementations of the faith. The Conservative, Reform, Reconstructionist, and other more progressive flavors permit more liberal general interpretations – interpretations which are, of course, rejected by the orthodoxy. The key here is that this is a single canon, to which all Jews can be seen as being in relation. Particularly relevant is that while inter-movement relationships are undefined and may be nonexistent or even hostile, the overall coordination of the movements is implicitly achieved.\nFourth, the faith as a whole can never be destroyed, as the Talmud functions also as a memory. Regardless of what may occur to some or even a majority of the adherents of the faith, the survivors will be able to rebuild the community to within an arbitrary precision. This implication is well-documented by historical experience.\nFifth, the Talmud is “bigger” and “wiser” than any individual. Considering Plato’s “Philosopher King”, we observe that individual humans are insufficient for the task. A shared, dynamic history of thought, however, might be. As the product of a history of reason and debate, the Talmud represents a cultural history orders of magnitude larger than any individual person. It has a fundamentally different, unique, and very functionally-relevant ontology.\nSixth and relatedly, the Talmud then exhibits “maximal intelligence”, in the sense that the unbroken chain of interpretation represents more overall experience than any record which allowed for erasure and editing.\nLet us briefly consider the similarities and differences between Talmud-based governance and the kind of Democratic governance exemplified by the United States.\nIn both cases, we have the principle that decisions must occur within set boundaries. In the case of the Talmud, those boundaries are historical scholarship and foundational texts. In the case of the United States, those boundaries are the Constitution and Bill of Rights.\nIn both cases, there are established mechanisms for making changes. In the case of the Talmud, changes are made via extrapolation and interpretation of past work. In the case of the United States, changes are made via a legislative process.\nA salient difference is that in the case of the United States, future changes are disconnected from past changes; if the basic boundaries are respected, then anything goes. It is technically possible to “reinterpret” the basic boundaries by amending the constitution, but this seems highly unlikely.\nAs such, we see the Talmudic process as having more gradual changes, while the US process sways more easily in changing political winds.\nOf course, much of this difference can be rooted in the fact that the United States must secure territory, and relies on the use of force to enforce rules. As a faith, the Jews permit less well-defined borders. The United States and the Jewish people, like all states and religious communities, are entities of a fundamentally different nature. As such, they seem to necessitate fundamentally different approaches to change and control. Religion is currently (although not historically) an opt-in experience; citizenship typically is not.\nYet, we see some shared principles in both forms of governance. The differences appear to be in large part necessary differences coming from the fundamentally different natures of these entities, in particular with regards to group membership. As such, we should not necessarily feel obligated to reconcile them.\nThe Talmud, its properties, and role in Jewish life provides a crucial case study for those interested in effective means of coordinating large groups of people absent a central authority. The fundamental mechanic is the strict requirement that future change emerges from past work, prohibiting both the introduction of the completely novel and the rejection of any history. The historical experience of the Jews has shown that, if put in motion upon adequate foundations, such a mechanic is sufficient for the decentralized coordination of the activity of millions of people across time and space.\nRecently, we have seen the emergence of technology which shares this property. The Blockchain, first described in the 2008 paper “Bitcon: A Peer to Peer Electronic Cash System”, is in essence a decentralized public ledger, in which anything can be recorded and made publically available. The principal mechanic of the Blockchain is that future entries must build upon past entries, and that any entry in the chain, once accepted, can never be deleted. In the words of the author, Satoshi Nakamoto:\nThe only way to confirm the absence of a transaction is to be aware of all transactions. In the mint based model, the mint was aware of all transactions and decided which arrived first. To accomplish this without a trusted party, transactions must be publicly announced, and we need a system for participants to agree on a single history of the order in which they were received.\nThe Blockchain is thus a decentralized authority in which all new changes must be a continuation of past work. As such, the example of the Talmud suggests that such a tool could be used for the effective decentralized coordination of large groups of people across time and space, without the need for a central authority or any force.\nIn order for the Blockhain to be used in this way, it will be necessary for a large group of people to regard the Blockchain as an authority on a wide variety of issues. As in the case of Jews and the Talmud, the Talmud is an authority because Jews see it as an authority. In an important sense, this is arbitrary. Fortunately, this sense suggests that there is no fundamental shortcoming which prevents a Blockchain from serving a similar purpose.\nThis leads us to some interesting questions:\nWhat kind of information should this Blockchain contain?\nHow should this Blockchain be updated?\nWhat content, if any, should be placed at the base of the Blockchain?\nAs a first pass, it would seem as though this Blockchain should function as a repository of social values. As the community defined by the Blockchain adapts, and external circumstances change, these values would be updated. At any point, any member of the community could inspect the history of these values and the reasoning for their changes. As time passed, this record would become a deep and strong foundation for that community, and a trusted authority on the values and purpose of that community. Coordination without an authority. Updates to this Blockchain would occur on a rough consensus basis, allowing for the possibility of a split of the Blockchain at some point if there emerged a major disagreement over the direction of the community.\nGiven the earlier discussion of Democracy, it does not seem at this time that the Blockchain can be used effectively to govern a state; issues of control and security seem to preclude the rational, gradual, consensus-based change process we are discussing. However, it does seem as though the Blockchain could serve as an effective authority and memory for a self-selected community with shared values and without borders.\nOne could point to other self-selecting communities with rough consensus-based decision-making processes, and observe that they succeed while employing very different decision-making systems. The Python community, with BDFL and a PEP-based system of improvement, has been pretty effective. Yet a series of disconnected proposals is ontologically dissimilar to an unbroken chain of decisions. The importance of this dissimilarity is to be determined.\nIt would be very exciting to be a part of such a community.\nUpdate (Feb 1)\nI shared this post with a few friends of mine with relevant domain knowledge. They gave valuable feedback and raised additional questions. Their responses are replicated below.\nFrom a Professor of Political Economy:\nInteresting read–but I wonder about practicality. The beauty of many societies is being able to effect rapid change. The way you lay this out, suggests that this may be a lot more difficult. It may be a way to insure calmer legislation, etc. But it doesn’t move really seem to me to lead to faster movement of anything.\nFrom a Rabbi:\nThis was fascinating. Thanks for sharing. You know, I think the question I’m sitting with is, given decentralized systems of law or commerce, what role does the organizer or covener have, and how much authority is in that role. The Talmud, for example, is a collection of many voices, but someone did the collecting. The work of that someone, the editor (or likely, editors) is what academics are particularly fascinated with these days. So, too, with any wiki. There is someone who hosts it. How much control do they have? And I imagine that there are also decisionmakers with a Blockchain. What does it mean, then, to be at the center of a decentralizes system?', 'Recent research revealed that blockchain is set to become ubiquitous by 2025, entering mainstream business and underpinning supply chains worldwide.\nThis technology is set to provide greater transparency, traceability and immutability, allowing people and organizations to share data without having to be concerned about security. However, blockchain is only as strong as its weakest link. Despite the hails surrounding blockchain’s immutable security, there are still risks surrounding it that organizations must be aware of – and mitigate – prior to implementation.\nIt is important to understand that there are two types of blockchain – permissionless and permissioned. The most prominent example of permissionless blockchain is Bitcoin – a public blockchain network that anyone can participate in. Cryptocurrencies like bitcoin favor this type of blockchain technology because it enables all users to track, verify and confirm transactions, regardless of whether users choose to be anonymous or not.\nThe other blockchain model is permissioned (also known as private blockchain) – and is mainly used for business applications. These networks are only accessible to known entities such as partners, suppliers or customers. With permissioned blockchain, a company establishes protocols to achieve consensus, and verify and assemble blocks. This set up can deliver thousands of transactions per second and provide granular management and control over who sees and accesses the transactions.\nIn both cases, the main benefit is the trust and transparency that blockchain brings – all parties involved in the network have total visibility into the transactions recorded in the blockchain ledger and each block is tied to the block before it.\nThis transparency makes blockchain extremely difficult to manipulate at scale. While the blockchain platform itself may be secure, there is still some work to be done to ensure organizations are equipped to make their networks secure end to end. For true security, organizations must focus on the last mile connection between a physical event and the digitized record of this event.\nIf these points of entry to the platform are tampered with, the blockchain is rendered worthless. It is therefore imperative that organizations secure all points of entry, and assess the risks, before they consider deploying blockchain on a broad scale. They will need to consider security at all layers, most importantly:\nThis starts with ensuring data and transactions entered in the blockchain ecosystem are adequately protected from manipulation. The infrastructure these networks resides on must also have the necessary protections in place. With blockchain, you are only as strong as your weakest link.\nIf integration points are compromised, the entire blockchain ecosystem could be at risk, meaning that blockchain credentials and data could be exposed to unauthorized users.\nIdentity and access management\nTo prevent unauthorized parties from accessing blockchain data, a combination of encryption and identity management tools are needed. Stolen credentials could potentially allow a cybercriminal to access the blockchain platform, regardless of how secure it is. Organizations must deploy identity and access management controls. Encryption should also be deployed to ensure that data is not stolen, manipulated or leaked in transit.\nThe insider threat should be a focal concern when it comes to blockchain too. Organizations must consider that employees, partners and suppliers – be it unintentionally or maliciously – can cause security incidents that impact the blockchain.\nTo mitigate this, organizations should deploy security awareness training for employees and outline clear security parameters and responsibilities with partners. This will stop employees from making careless mistakes and may also ward off malicious insiders. In line with these requirements, blockchain can provide advanced security controls – for example, leveraging the public key infrastructure (PKI) to authenticate and authorize parties, and encrypt their communications.\nBlockchain-based networks are built on shared business interests creating a system of trust. However, as the network grows, participating entities could leave the network and new ones may join, leading to ambiguities around operational considerations around data sharing and data ownership. These could result in serious regulatory and reputational repercussions for organizations as data owners, unable to secure the customer data.\nOrganizations are multi-faceted and have multiple revenue streams, often linked to each other. One of the major challenges to blockchain adoption has been a lack of interoperability across different blockchain networks. There have been recent developments, with major players embarking on developing interoperable networks, which could boost blockchain interest to a different level, at the same time introducing additional levels of vulnerability.\nA key component of blockchain networks is the Smart Contracts, which are developed using different languages on the platform being used, like Solidity being used in Ethereum. These languages allow developers to make changes to the underlying blockchain networks, causing vulnerabilities. However, from an enterprise blockchain perspective, a solid governance mechanism using permissioned chain can establish a secure system in place to restrict the privileges to governing body.\nTo achieve the most value from blockchain, both now and in the future, organizations must take responsibility for their safety and security at all levels – application, Infrastructure, data and partners.\nBy conducting a blockchain risk assessment and addressing key risks, organizations can make sure they are well positioned to leverage the efficiencies, transparency and cost-effectiveness provided by blockchain without opening themselves up to unexpected risks. The most pragmatic way for organizations interested in blockchain is to test the concept through pilot programs. Pilots should be focused on the areas that offer organizations the most control and companies should take these weak links into consideration.\nUltimately, blockchain has the ability to solve business issues relating to traceability, responsiveness, and trust. By taking a carefully planned approach to implementation, and understanding blockchain’s weak links, organizations can unlock the true value of blockchain, creating new opportunities and reducing inefficiencies.']	['<urn:uuid:5719b54f-9ae1-4159-aaf4-190033f86a4e>', '<urn:uuid:aa3cc425-6945-4d18-8ba5-24e7cdc911d7>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:58:04.490895	6	54	2842
13	What did scientists accidentally find in 1964?	In 1964, two astronomers working for Bell Labs, Arno Penzias and Robert Wilson, accidentally discovered cosmic microwave background (CMB) radiation, which helped support the expanding universe model over the steady-state model.	['Early universe observations by the James Webb Space Telescope (JWST) cannot be explained by current cosmological models. These models estimate the universe to be 13.8 billion years in age, based on the big-bang expanding universe concept.\nImpossible early galaxies refer to the fact that some galaxies dating to the cosmic dawn — 500 to 800 million years after the big bang — have discs and bulges similar to those which have passed through a long period of evolution. And smaller in size galaxies are apparently more massive than larger ones, which is quite the opposite of expectation.\nFrequency and distance\nThis age estimate is derived from the universe’s expansion rate by measuring the redshift of spectral lines in the light emitted by distant galaxies. An earlier explanation of the redshift was based on the hypothesis that light loses energy as it travels cosmic distances. This “tired light” explanation was rejected as it could not explain many observations.\nThe redshift of light is similar to the Doppler effect on sound: noises appear to have higher frequency (pitch) when approaching, and lower when receding. Redshift, a lower light frequency, indicates when an object is receding from us; the larger the galaxy distance, the higher the recessional speed and redshift.\nAn alternative explanation for the redshift was due to the Doppler effect: distant galaxies are receding from us at speeds proportional to their distance, indicating that the universe is expanding. The expanding universe model became favored by most astronomers after two astronomers working for Bell Labs, Arno Penzias and Robert Wilson, accidentally discovered cosmic microwave background (CMB) radiation in 1964, which the steady-state model could not satisfactorily explain.\nThe rate of expansion essentially determines the age of the universe. Until the launch of the Hubble Space Telescope in the 1990s, uncertainty in the expansion rate estimated the universe’s age ranging from seven to 20 billion years. Other observations led to the currently accepted value of 13.8 billion years, putting the big-bang model on the cosmology pedestal.\nLimitations of previous models\nResearch published last year proposed to resolve the impossible early galaxy problem using the tired light model. However, tired light cannot satisfactorily explain other cosmological observations like supernovae redshifts and uniformity of the cosmic microwave background.\nI attempted to combine the standard big-bang model with the tired light model to see how it fits the supernovae data and the JWST data, but it did not fit the latter well. It did, however, increase the universe’s age to 19.3 billion years.\nNext, I tried creating a hybrid model comprising the tired light and a cosmological model I had developed based on the evolving coupling constants proposed by British physicist Paul Dirac in 1937. This fitted both the data well, but almost doubled the universe’s age.\nThe new model stretches galaxy formation time 10 to 20 fold over the standard model, giving enough time for the formation of well-evolved “impossible” early galaxies as observed.\nAs with any model, it will need to provide a satisfactory explanation for all those observations that are satisfied by the standard cosmological model.\nThe approach of mixing two models to explain new observations is not new. Isaac Newton considered that light propagates as particles in his theory of light, which prevailed until it was replaced by the wave theory of light in the 19th century to explain diffraction patterns observed with monochromatic light.\nAlbert Einstein resurrected the particle-like nature of light to explain the photoelectric effect — that light has dual characteristics: particle-like in some observations and wave-like in others. It has since become well-established that all particles have such dual characteristics.\nAnother way of measuring the age of the universe is to estimate the age of stars in globular clusters in our own galaxy — the Milky Way. Globular clusters include up to a million stars, all of which appear to have formed at the same time in the early universe.\nAssuming all galaxies and clusters started to form simultaneously, the age of the oldest star in the cluster should provide the age of the universe (less the time when the galaxies began to form). For some stars such as Methuselah, believed to be oldest in the galaxy, astrophysical modeling yields an age greater than the age of the universe determined using the standard model, which is impossible.\nEinstein believed that the universe is the same observed from any point at any time — homogeneous, isotropic and timeless. To explain the observed redshift of distant galaxies in such a steady-state universe, which appeared to increase in proportion to their distance (Hubble’s law), Swiss astronomer Fritz Zwicky, proposed the tired light theory in 1929.\nWhile some Hubble Space Telescope observations did point towards the impossible early galaxy problem, it was not until the launch of JWST in December 2021, and the data it provided since mid-2022, that this problem was firmly established.\nTo defend the standard big-bang model, astronomers have tried to resolve the problem by compressing the timeline for forming massive stars and primordial black holes accreting mass at unphysically high rates.\nHowever, a consensus is developing towards new physics to explain these JWST observations.\nArticle written by Rajendra Gupta, Adjunct professor, Physics, L’Université d’Ottawa/University of Ottawa\nYou might also be interested in:\n- JWST spots the universe’s oldest galaxy 12.5 billion light years away from Earth\n- Stephen Hawking’s unnerving theory confirmed: Everything in the universe will evaporate\n- Are we alone in the universe? Astronomy supergroup assembles to discover origins of life']	['<urn:uuid:4adb967a-9b1a-4754-a6fd-4f845c3777a8>']	factoid	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:58:04.490895	7	31	915
14	cement production energy cost health risks	Cement production is an energy-intensive process with energy costs representing 20-40% of production costs. The production process also poses significant health risks as it generates silica dust (100 times smaller than sand) that can lead to serious respiratory conditions like silicosis, COPD, tuberculosis, and lung cancer when inhaled by workers.	"['small particles to make them more reactive, ... Cement manufacturing process ... Cement plants are usually located closely either to hot spots in the market or to areas with .\nCement is the main basic ingredient of readymix concrete. Whether in bags or in bulk, CEMEX provides its customers with highquality branded cement products for their construction needs. Cement is the most widely used construction material worldwide. It provides beneficial as well as desirable properties, such as compressive strength (construction material with highest strength per unit cost), durability, and .\nAAC Plants. Even though regular cement mortar can be used, 98% of the buildings erected with AAC materials use thin bed mortar, which comes to deployment in a thickness of 3 mm. This varies according to national building codes and creates solid and compact building members. AAC material can be coated with a stucco compound or cement plaster OR POP. Materials.\nArmoured fan technology for cement process plants. b ... The heavy duty centrifugal process fans we produce for the cement industry are the culmination of our unique global experience and our continual investment in research. The ... to cement plant operators is our ability to raise\nMini cement plant Manufacturer. As a manufacturers and suppliers of turnkey cement plants and mini cement plants and clinker grinding unit plants starting to execution of complete Mini cement plants and their units. Our Minicement plants equipments are based upon vertical kiln .\nManufacturers of Cement Plants, Cement Plant Project, Cement Plant Equipment, Rotary Kiln Cement Plants, Industrial Cement Plants, Cement Manufacturing Plant, Large Cement Plant, Mini Cement Plant, Clinker Grinding Units, Grinding Mills, Ashoka Machine Tools Corporation.\nCement and glass makers need integrated process control systems that can improve plantwide efficiency and productivity. Our awardwinning process control solutions provide easy ""single window"" access to the process, production, quality and business information – from the most remote location to corporate headquarters.\nGlobally, the cement sector is dominated by a small number of large companies. Largest cement companies, and their capacities and sales are also provided below. Cement production is an energy intensive process, with energy costs representing 2040% of production costs (IEA, 2007. p. 145).\nCement Plants, Manufacturing Facilities, and Other Process Plants. Long International personnel have been involved in numerous claims and project consulting assignments associated with cement plants, manufacturing facilities, and other process plants. Certain of these projects have been located outside the United States.\nAug 30, 2012· Cement Manufacturing Process Phase 1: Raw Material Extraction. It is combined with much smaller proportions of sand and clay. Sand clay fulfill the need of silicon, iron and aluminum. Extraction of raw material and crushing of material Generally cement plants are fixed where the quarry of limestone is near bye.\nWall Putty Manufacturing Process/Small Plant Cost,MTW Grinding Plant For Wall Putty Manufacturing Processing India Wall putty is white cement mineral based putty for use on cement concrete. Wall putty has a variety of applications and can be used on concrete, rendered walls, hollow blocks, precast walls, concrete ceilings, calcium silicate bricks, aerated lightweight blocks etc.\nPortland cement manufacturing plants are part of hydraulic cement manufacturing, which also includes natural, masonry, and pozzolanic cement. The sixdigit Source Classification Code (SCC) for portland cement plants with wet process kilns is 305006, and the sixdigit SCC for plants with dry process kilns is .\nAbout American Mixers and Plants. American Mixers Plants is the number 1 source for innovation in Concrete Batch Plants, Silos, 34 Yard Mixers, New Accura Box Loading System and Custom Equipment. Take a closer look at our product line.\nRAW CEMENT GRINDING This phase of grinding may be either the wet or dry process, the end product going to a kiln. Material ground consists of limestone, cement rock, marl or marine shells along with secondary materials usually shale or clay.\nAs a professional manufacturer and supplier of 2500t/d dry process cement plant in China, Jiangsu Pengfei Group Co., Ltd can also provide you with various other machines, such as 100,000tons/year small cement production equipment, new type dry process cement production line, cement grinding plant, industrial kiln and furnace, grinding equipment ...\n• Chemically, cement is a mixture of calcium silicates and small amounts of calcium aluminates that react with water and cause the cement to set. • Calcium derives from limestone and clay, mudstone or shale as the source of the silica and alumina. • The mix is completed with the addition of 5% gypsum to help retard the setting time of the cement.', 'Across numerous industries, silica dust protection is a critical component of workplace safety.\nTypically, this dangerous, even deadly dust is generated during activities such as cutting, grinding, drilling, and crushing materials that contain crystalline silica, such as concrete, stone, and sand.\nSince it’s 100 times smaller than a grain of sand, workers can breathe it in without realizing the dangers.\nBreathing in silica dust can have severe health consequences, including respiratory diseases and lung cancer. And, approx. 2.3 million people in the U.S. are exposed to silica at work every year.\nSo, what are some of the dangers? More importantly, what do you need to know about silica dust protection for you and your team?\nLet’s break it down.\nRelated Article: OSHA Targets Silica Hazards in Cut Stone Industry\nRelated Article: How to Protect Your Team from the Effects of Silica\nHealth Implications of Silica Dust Exposure\nWhen inhaled, silica dust particles can penetrate deep into the lungs and cause various respiratory ailments.\nProlonged exposure to high levels of silica dust can lead to the development of silicosis, a debilitating and irreversible lung disease. Silicosis is characterized by the formation of scar tissue in the lungs, reducing their ability to function effectively.\nSymptoms of silicosis include persistent cough, shortness of breath, chest pain, and fatigue.\nIn addition to silicosis, silica dust exposure is also associated with an increased risk of developing other respiratory conditions. These include chronic obstructive pulmonary disease (COPD), tuberculosis, lung cancer, and kidney disease. Furthermore, silica dust has been linked to an increased susceptibility to respiratory infections and exacerbation of existing respiratory conditions, such as asthma.\nRelated Article: NIOSH Takes Step toward Reducing Silica Dust Exposure in Mines\nThe Importance of a Workplace Respiratory Protection Program\nGiven the serious health risks posed by silica dust, it is crucial for workers to use appropriate respiratory protection measures to minimize their exposure. Respiratory protection can effectively reduce the inhalation of silica dust particles and safeguard the respiratory system. There are various types of respiratory protection equipment available, and the choice of the most suitable option depends on the nature of the work being performed.\nFor proper silica dust protection, your team should be equipped with one of the following.\n1. N95 Respirators\nN95 respirators are a common form of respiratory protection widely used in industries where workers are exposed to silica dust. These respirators filter out at least 95% of airborne particles, including silica dust, with a diameter of 0.3 micrometers or larger.\nN95 respirators create a seal around the nose and mouth, ensuring that the air breathed in is properly filtered. Remember that for silica dust protection, it’s essential to conduct fit testing to ensure a proper fit.\n2. Powered Air-Purifying Respirators (PAPRs)\nPAPRs are another option for respiratory protection, particularly in situations where the concentration of silica dust is high.\nPAPRs use a battery-powered fan to draw air through filters before delivering it to the wearer’s breathing zone. These respirators provide a higher level of protection than N95 respirators, and are more comfortable to wear for extended periods.\nHowever, they require regular maintenance, including filter replacement and battery recharging.\n3. Supplied Air Respirators (SARs)\nSARs, also known as airline respirators, provide a continuous supply of clean air from a remote source.\nThey are suitable for environments with extremely high levels of silica dust or where the oxygen concentration is insufficient. SARs are typically used in confined spaces, such as underground mines or tunnels. These respirators offer excellent silica dust protection, they can sometimes restrict mobility due to the need for an air supply line.\nImplementing an Effective Silica Dust Protection Program at Your Worksite\nTo ensure effective respiratory protection against silica dust, employers should establish comprehensive respiratory protection programs.\nThese programs should include the following elements:\n1. Hazard Assessment\nEmployers must conduct a thorough assessment of workplace hazards to determine the extent of silica dust exposure. This assessment helps identify areas requiring respiratory protection and enables the selection of appropriate equipment.\n2. Silica Dust Protection Training and Education\nWorkers should receive proper training on the risks associated with silica dust exposure, the proper use and maintenance of respiratory protection equipment, and the importance of following safe work practices. Regular refresher training sessions should be conducted to reinforce these principles.\n3. Proper Equipment Selection\nEmployers should select respiratory protection equipment that meets the required standards and is appropriate for the specific work environment. This includes ensuring that respirators have the correct filter type and efficiency level for silica dust particles.\n4. Fit Testing and Maintenance\nFit testing should be conducted to ensure that respirators fit properly and provide an effective seal. Regular maintenance, including inspection, cleaning, and replacement of filters, is essential to keep respiratory protection equipment in good working condition.\n5. Monitoring and Evaluation\nRegular monitoring of silica dust levels in the workplace is necessary to evaluate the effectiveness of control measures and identify any necessary adjustments or improvements to the respiratory protection program.\nSee OSHA’s guidelines on respirable crystalline silica- 1926.1153.\nBringing It Together\nProtecting workers from the adverse effects of silica dust is essential for maintaining occupational health and safety. By understanding the health hazards associated with silica dust exposure and implementing appropriate respiratory protection measures, employers can safeguard the well-being of their workforce.\nRespiratory protection programs, including proper equipment selection, fit testing, training, and maintenance, play a crucial role in mitigating the risks posed by silica dust, enabling workers to breathe safely and maintain their long-term respiratory health.\nAbout Worksite Medical\nIn most cases, OSHA requires medical surveillance testing, and at no cost to employees.\nWorksite Medical makes that program easier with mobile medical testing.\nWe conduct on-site respirator fit tests, as well as audiometric exams, pulmonary function tests and heavy metal lab work, right on your job site. We also keep accurate, easy-to-access medical records for your convenience. You’ll keep your employees at work, and stay ahead of OSHA inspections.']"	['<urn:uuid:fd6a242a-262f-4c72-a3ea-07c14bb0f74c>', '<urn:uuid:8f4da841-15f6-4f16-b21f-73c3a6c35f3c>']	factoid	direct	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T20:58:04.490895	6	50	1739
15	I'm looking to set up a water reuse system in my building - what steps do I need to follow to get it approved?	The process requires several steps: 1) Submit a Water Budget Application to the SFPUC, 2) Submit an Engineering Report to the Department of Public Health (prepared by a registered professional engineer), 3) Obtain Building Permits and construct the system, 4) Schedule a Cross-Connection Test, 5) Obtain a Permit to Operate from the Department of Public Health, and 6) Conduct ongoing monitoring, reporting, and inspections. Note that some projects, like rainwater harvesting for non-spray irrigation and graywater projects for subsurface irrigation, may not need to complete all these steps.	"[""In September 2012, the City and County of San Francisco adopted the Onsite Water Reuse for Commercial, Multi-family, and Mixed Use Development Ordinance. Commonly known as the Non-potable Water Ordinance, it added Article 12C to the San Francisco Health Code, allowing for the collection, treatment, and use of alternate water sources for non-potable applications. In October 2013, the ordinance was amended to allow district-scale water systems consisting of two or more buildings sharing non-potable water.\nThe Non-potable Water Program (NP Program) Guidebook details the steps presented below that must be taken to collect, treat, and use non-potable water in commercial, mixed-use, and multi-family residential developments. The program also outlines the oversight of the San Francisco Public Utilities Commission (SFPUC) and the City’s Departments of Public Health (SFDPH) and Building Inspection (SFDBI) during the review process.\nSeveral developments in San Francisco, including SFPUC Headquarters (525 Golden Gate Ave.) are operating or are in the process of installing a non-potable water system. For more information about these developments, please see the San Francisco’s Non-Potable Water System Projects case studies.\nThe SFPUC also created a grant assistance program that provides up to $250,000 or $500,000 for projects meeting sepcific eligibility criteria.\nStep 1: Submit a Water Budget Application to the SFPUC\nTwo Water Use Calculators are available from the SFPUC to help estimate your demands. One of the below calculators must be completed and submitted with your project's Water Budget Application.\nStep 2: Submit an Engineering Report to SFDPH\nThe Engineering Report must prepared by a registered professional engineer. Rainwater harvesting projects for non-spray irrigation, and foundation drainage or gray water projects for subsurface irrigation, do not need to submit an Engineering Report.\nStep 3: Obtain Building Permits and Construct the Onsite Water System\nSFDPH approval of the Engineering Report is required before SFDBI will issue a plumbing permit.\nStep 4: Schedule a Cross-Connection Test\nNon-potable water systems must include the required level of backflow prote ction as set forth by the San Francisco Public Utilities Commission Cross-Connection Control Program. Please see Required Levels of Backflow Protection for Non-potable Water Systems.\nStep 5: Obtain a Permit to Operate from SFDPH\nA Construction Certification letter and successful Cross Connection test are required prior to operation. Rainwater harvesting projects for non-spray irrigation, and foundation drainage or graywater projects for subsurface irrigation, do not need to obtain a Permit to Operate.\nStep 6: Conduct Ongoing Monitoring, Reporting, and Inspections.\nOngoing monitoring is crucial to ensure the onsite water system is in proper working order. Operators report the results of the ongoing monitoring to SFDPH using the Discharge Monitoring Report form at the frequency identified in their Permit to Operate.\nNew Legislation Effective November 1, 2015\nIn July 2015, the Non-potable Water Ordinance was amended to require the following beginning on November 1, 2015:\n- that all new buildings of 250,000 square feet or more of gross floor, located within the boundaries of San Francisco's designated recycled water use area be constructed, operated, and maintained using available alternate water sources for toilet and urinal flushing and irrigation;\n- that all new buildings in San Francisco of 40,000 square feet or more of gross floor area prepare water budget calculations;\n- that subdivision approval requirements include compliance with Article 12C of the San Francisco Health Code; and\n- facilities constructed in accordance with Article 12C of the San Francisco Health Code and located in public rights-of-way are subject to approval as minor encroachments and exempt from payment of public right-of-way occupancy assessment fees.\nThe July 2015 amendments to the Non-potable Water Ordinance also required the following beginning on November 1, 2016:\n- that new buildings of 250,000 square feet or more of gross floor located outside the boundaries of San Francisco's designated recycled water use area be constructed, operated, and maintained using available alternate water sources for toilet and urinal flushing and irrigation.\nInnovation in Urban Water Systems\nOn May 29-30, 2014, a dedicated group of water agencies, public health departments, and research institutions from across North America met in San Francisco to discuss onsite water treatment systems at the Innovation in Urban Water Systems Meeting. Using the experiences shared from those organizations present at the Meeting, the group developed the “Blueprint for Onsite Water Systems” to serve as a how-to guide for communities interested in implementing onsite treatment programs. Please visit the Innovation in Urban Water Systems webpage to find out more information about the meeting and participating agencies.\nNon-potable Program Resources:""]"	['<urn:uuid:391a0aaf-ca5f-4987-9370-418a10a964eb>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:58:04.490895	24	88	745
16	sky electricity types differences explain	There are several types of atmospheric electrical phenomena: Sprites are reddish, short-lived forms of lightning that occur over thunderheads and last only a few milliseconds. Blue jets are narrow, bluish, cone-shaped fountains of light that burst from the tops of thunderheads. Elves are diffuse luminous glows caused by electromagnetic pulses from storms. The newly discovered gigantic jets are different - they branch upward from thunderclouds, spreading to a diameter of more than 20 miles at an altitude of more than 50 miles.	"['Electrical oddities give sky-watchers a charge / Jets, sprites make lightning look like minor sparks\nPublished 4:00 am, Monday, June 30, 2003\nTitanic, lightninglike flashes that shoot high over oceanic thunderstorms are thrilling scientists who chart the seas of electrical current flowing through Earth\'s atmosphere.\nGigantic jets, the new phenomena have been dubbed. They are the latest entrants in the menagerie of aerial electrical oddities -- the whimsically named blue jets, sprites and elves -- discovered over the past 14 years.\nA typical gigantic jet is more than 40 miles tall, more than 200 times the height of San Francisco\'s Transamerica building. Resembling luminous carrots and trees, they leap from the tops of thunderclouds to the edge of outer space and disappear in less than a second, Taiwanese and Japanese scientists report in the June 26 issue of Nature.\nThe team, led by physicist H.T. Su of Taiwan\'s National Cheng Kung University, observed five gigantic jets during a storm over the South China Sea in July. Their instruments indicated that the gigantic jets are surges of intense electrical flow that link thunderstorms in the lower atmosphere and the ionosphere, a realm of electrically charged particles and free electrons that begins more than 40 miles above the ground.\n""The gigantic jets are definitely a very important discovery,"" says Umran Inan, director of Stanford\'s Very Low Frequency Research Group, which studies Earth\'s electrical environment including the upper air. Inan was not connected with the Su report.\nENERGY GATHERING, EXPLODING\nThe gigantic jets expose a major new component in the global electrical circuit, in which electrical energy continually flows around Earth. Sometimes, like water accumulating behind a dam, that electrical energy gathers in a single place -- typically a thunderstorm. And sometimes, like a bursting dam, the electrical energy explodes outward: Witness the ferocity of lightning bolts and the 40-mile-high aura of a gigantic jet.\nAll in all, it\'s a red-letter year for the scientific study of atmospheric electricity, which began in the 18th century. Back then, as every schoolchild knows, Benjamin Franklin flew a kite in a thunderstorm to prove that lightning is electricity.\nBut despite lightning bolts\' reputation for mayhem -- they zing golfers, zap utilities and inflame forests -- they\'re only minor sparks in the overall planetary electrical circuit. Over the last 14 years, unexpected links in the circuit have been discovered: the sprites, blue jets, elves -- and, now, the gigantic jets.\nThe post-Franklin epoch of atmospheric electricity research began in Minnesota one night in July 1989. Scientists were testing a low-light camera used to record the launch of small research rockets. By coincidence, a thunderstorm was rumbling on the horizon. The videotape accidentally recorded brief, immense bursts of light -- sprites -- far above the storm.\nSprites are reddish, short-lived (a few milliseconds) forms of lightning that occur over thunderheads. In 1994, scientists discovered blue jets, which are narrow, bluish, cone-shaped fountains of light that burst from the tops of thunderheads.\nLater, investigators discovered a third strange denizen of the upper atmosphere -- elves. These are diffuse luminous glows probably caused when a storm unleashes an electromagnetic pulse into the atmosphere. Now this weird trio has been joined by a fourth eccentric, something that H.P. Lovecraft or J. K. Rowling might have dreamed up -- the gigantic jets.\n""It is clear that the events observed by Su et al are very different from sprites, which typically start at altitudes of about 70 kilometers (about 40 miles) and propagate downward,"" Victor Pasko of Penn State University writes in a commentary published in the same issue of Nature.\nBy contrast, ""the gigantic jets seen by Su and colleagues branch upward from thunderclouds, spreading to a diameter of about 40 kilometers (more than 20 miles) at an altitude of 85 to 90 kilometers (more than 50 miles).""\nTHE ONE THAT GOT AWAY\nLast year, Pasko and his colleagues reported observing a stunningly tall blue jet. It leaped from the top of a thunderstorm to a height of more than 40 miles. That reinforced scientists\' suspicion that electricity can leap straight from the lower atmosphere to the edge of outer space -- and the gigantic jets have proven it beyond reasonable doubt.\n""The Taiwanese observations are quite truly amazing . . . clearly showing a direct electrical link between storm top and ionosphere,"" says Walter Lyons, a noted sprites investigator in Colorado. He adds ruefully: ""I saw one such giant plume over Montana back in 1992 -- but nobody would believe me then.""\nQuestions arise: Why do gigantic jets only seem to occur over oceanic thunderstorms, not over land? Scientists have no idea.\nAlso, do gigantic jets threaten air travelers? They are ""possibly a hazard to aviation, as with any lightning strike to an aircraft,"" says Earle Williams,\na top sprite expert and research scientist at the Massachusetts Institute of Technology. ""NASA has always been concerned with how the space shuttle will respond to lightning discharges, and this type of lightning they would certainly seek to avoid.""\nEARLY SIGHTINGS BY LAYPEOPLE\nGigantic jets, sprites and their high-flying kin could have been discovered long ago, had anyone been paying attention to observations made mostly by nonscientists.\nAs far back as 1886, two writers reported in Nature a peculiar storm: Over it flashed ""continuous darts of light"" that ""ascended to a considerable altitude, resembling rockets more than lightning."" And for decades before the first videotaping of sprites in 1989, airline pilots reported seeing strange flashes over thunderstorms.\nYet before 1989, such reports were ignored by the scientific community. One exception was the late Bernard Vonnegut of the State University of Albany in New York. One of the grand old men of modern atmospheric science -- and brother of novelist Kurt Vonnegut Jr. -- he published a summary of the pilots\' reports in the journal Weather in 1980.\nWhich raises a sticky question for scientists and philosophers of science: In the future, should scientists take more seriously observations of strange natural phenomena reported by nonscientists? Especially observations that are purely anecdotal -- ""I saw a bright light"" -- rather than quantified or ""scientific""?\nThere\'s a historical parallel to the sprites saga: meteorites. For centuries, country folk reported seeing ""falling stars"" and finding rocks that supposedly fell from the sky. Scientists largely ignored such reports; even Thomas Jefferson, himself a talented amateur scientist, made fun of them. Yet in the early 19th century, scientists admitted the ""hicks"" had been right: Meteorites hit Earth all the time, many tons of them yearly.\nSummer is the biggest thunderstorm season, so Lyons invites laypeople to report their observations of upper atmosphere electrical phenomena to his Web site, www.sky-fire.tv.\n""If you\'re in a rural area and let your eyes become dark adapted, look above the area where large thunderstorms are occurring 100 to 300 miles away,"" the Web site advises. ""You just might glimpse the brief life of a sprite -- one of lightning\'s children dancing through the thin air of the upper atmosphere near the edge of space.""']"	['<urn:uuid:6a4924b3-72d8-4f1c-8dde-484b68503806>']	open-ended	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T20:58:04.490895	5	82	1169
17	In my research on animal models of genetic disorders, I've noticed varying degrees of symptom replication - how well do Ube3a mutated mice reflect the cognitive impairments seen in human Angelman syndrome patients?	"While Ube3a mutations are linked to profound cognitive impairments in humans, the cognitive deficits in mouse models are described as ""rather mild."" The researchers noted that ""a paradigm that assesses cognitive function"" is lacking in their test battery, despite this being an important clinical feature of Angelman disease. This represents a limitation in how well the mouse models replicate this particular aspect of the human condition."	['Behavioral Tests on Mice May Help Identify Therapies for Angelman Trials\nEvaluating mice bearing Ube3a mutations with a set of five behavioral tests may be a useful tool for preclinical therapy testing to identify treatments for Angelman syndrome, a study suggests.\nThe study, “A behavioral test battery for mouse models of Angelman syndrome: a powerful tool for testing drugs and novel Ube3a mutants,” was published in the journal Molecular Autism.\nMouse models are important tools for detailed biological processes linked to disease, as well as preclinical assessment of new therapies. However, for the retrieved information to have medical validity, the models must effectively mimic human disease to the greatest extent possible. Also, the measures used to assess the animals and their clinical manifestations must accurately identify markers of the disease.\nResearchers at Erasmus Medical Center in the Netherlands proposed a set of five standardized tests to evaluate mice engineered to have Ube3a gene mutations, the genetic cause of Angelman syndrome.\nThe first test measures the time animals are able to maintain their balance on top of an accelerating cylinder, allowing them to measure mice motor function.\n“Individuals with Angelman syndrome show clear motor impairments, and impaired performance on the accelerating rotarod is the most frequently described phenotype in Ube3a mice,” researchers wrote.\nIn the second test, animals are placed in an open field and allowed to walk freely. Researchers are then able to evaluate their anxiety levels and locomotor skills.\nThe physical capacity of mice is tested using a forced swim test. The animals are placed in a cylindrical tank of water for two minutes. Researchers then record the time during which the mice actively swim or just float.\nNest-building and marble-burying tests are used to see whether animals retain common behaviors of their species. In these tests, nest-building materials or marbles are placed inside mice’s cages and researchers record whether the animals either build a nest or bury the marbles.\nThe team applied these tests to 111 Ube3a mutated mice and 120 healthy controls. Ill mice consistently showed impaired response in all five tests. They had reduced balance and locomotor skills, spent more time floating, used less material to build their nests, and buried fewer marbles than control mice.\nTo further confirm these results, researchers repeated the experiments with two different mouse models bearing Ube3a mutations. The results were consistent to those reported in the first model.\n“Taken together, these data suggest that the identified set of behavioral [manifestations] in this test battery is present in three independently derived Ube3a-mutant lines,” researchers wrote.\nAlthough these tests cover several of the main manifestations of the human disease presentation,“a paradigm that assesses cognitive function” is lacking — “an important clinical feature of Angelman disease,” researchers noted.\nIn humans, Ube3a mutations are linked to profound cognitive impairments. However, in these mouse models, learning deficits “are rather mild,”researchers said.\nTo explore the preclinical potential of this battery of tests, the team treated Ube3a mutated mice with minocycline and levodopa and assessed the impact of the therapies. These two therapies have been tested in clinical trials in patients (NCT01531582, NCT02056665, NCT01281475), but have failed to demonstrate efficiency despite positive preclinical data.\nSimilar to what happened in the clinical trials, the two investigational compounds failed to promote significant improvements in all five test results, reinforcing that this behavioral evaluation approach can effectively screen for therapies that are more likely to be successful in clinical trials.\nBecause the behavioral impairments “are observed in several independently derived Ube3a lines,” researchers believe that this test battery “can also be employed to investigate the effect of specific Ube3a mutations.”']	['<urn:uuid:7ae2214f-41c5-4910-b089-8cce3af2f52c>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:58:04.490895	33	66	600
18	How do biological additives and regular maintenance compare in terms of their effectiveness for extending the lifespan of tanks in both septic systems and water heaters?	According to the documents, for septic tanks, while biological enzymes can help efficiency, experts argue they aren't necessary as regular pumping out is sufficient for optimal function. For water heaters, regular maintenance including annual flushing and replacing the anode rod every 2-3 years is crucial and can help tanks last 20 years or more, making regular maintenance the more proven approach for both systems.	['This article will cover the septic tank and septic tank enzymes. This septic system component has always been seen as the major player in the wastewater treatment system of your household. Think about it. Your household produces wastewater on a daily basis. Where does it all go? Where else but the septic tank? You cannot expect the wastewater to just pass by the septic tank and expect it to be pre-treated already. The wastewater has to stay and be held in the septic tank for a long while before it actually gets dispersed into the drain field to be treated again.\nIn the septic tank, the processing of the wastewater begins. Main work on the wastewater is done in the septic tank. It is only natural for it to be well-taken care of because usually, if the septic tank is neglected, the entire system perishes. Your household would be bound to a life of exposure to septic odors and raw sewage. To prevent septic system malfunction or failure, the septic tank should receive the quality of maintenance recommended by your septic expert.\nWith this, numerous septic system products are now available for you to choose from. Manufacturers never stop developing septic system enhancers and cleaners to make sure that the system remains optimal. Sadly, the slightly heightened need of the homeowners to have a low-maintenance septic system promotes manufacturers to promise their consumers exactly what they want. Pumping out is still an essential method of care for septic tanks even if you have additives administered. But because of the persistence of these companies, more and more homeowners just use the additives without regular inspection and pumping out.\nThere are many septic system additives but they are all classified into three—inorganic, organic, and biological additives or enzymes. Inorganic additives are the strong alkalines and acids that usually kill off the resident bacteria and corrode the physical components of the septic tank. Organic septic system additives such as yeast and baking soda are said to benefit the septic tank for a time but when used excessively will be disadvantageous to it. The most acceptable additives for the septic tank are the biological septic system enzymes. This type of additive does not bring any harm to the resident bacteria in the septic tank and to the septic tank per se. It makes the decomposition of the solid wastes much faster. Also, septic system enzymes do not leave harmful or toxic chemical discharge to the surrounding environment, which is good because it doesn’t contribute to the pollution of the water systems around your home.\nBut there are experts who argue that there is no need for septic system enzymes at all. They say that human wastes are enough for the septic tank to function optimally. The enzymes in the human waste are enough to stimulate better solid waste decomposition. If you adhere to pump out schedules, then you will not have any problems with your septic tank or septic system in general at all. Pumping out is a very inexpensive way of maintaining your septic tank.\nIt is not necessary for you to use septic system enzymes but you have the prerogative to use this additive on your septic tank. Septic tank enzymes could help the efficiency of your septic tank if it’s used properly. But always remember that even if you use it, you still have the responsibility to check you septic tank. Because once the septic tank is neglected, the drain field fails and so will the entire system. You should also take note that caring for the septic tank starts inside your home. You should use bacteria and environment-friendly household cleaners; divert the rain gutter away from the septic tank area; remove vehicles or construction over the septic tank; and lessen the water load that enters the septic tank.\nSeptic tank enzymes are there to assist you in maintaining your septic tank. It is very expensive to have a brand new septic tank or a brand new septic system installed because of your neglect. Ask the guidance of your septic expert so that the septic tank enzymes could be used properly.', 'Whether you are buying a house or your water heater is not producing hot water, water heater replacement may be on the horizon. Often, homeowners are not prepared for water heater replacement. When a water heater quits, it can become a shock to us and our wallet.\nPart of preparing and budget for a water heater replacement is knowing how long a water heater will last.\nAn electric tank water heater lasts 10 to 15 years. A gas tank water heater lasts 8 to 12 years. With routine maintenance, tank water heaters can last 20 years or more. Tank water heater repairs cost $200 to $300 while replacement costs $1500 to $2000. Tankless water heaters can last 15 to 20 years. With more functional parts that can fail, tankless water heater repair costs about $575, and replacement costs $1500 to $3000.\nDo you need a Licensed Plumber? We can help!\nGet a free estimate from top-rated, screened, and licensed plumbers in your area!\nA well-maintained gas or electric water heater can last 20 years or more. Water heater maintenance includes annual water heater flushing maintenance and replacing the anode rod every 2-3 years to protect the inner tank lining.\nIn this article, we will focus on tank-style water heaters. You can get details on how long a tankless water heater lasts here. Many factors go into determining how long a water heater will last. The number one factor in extending the life of a water heater is proper maintenance. Even inexpensive water heaters can outlast more expensive models provided it’s properly maintained.\nHowever, if the water heater is not maintained well, you can expect the lifespan to be shorter, and you could risk sudden failure or damage to your house.\nMost inexpensive water heaters come with a 6-year manufacturer warranty. However, some better quality models can have manufacturer warranties of 10 to 12 years. In contrast, a few high-end models offer lifetime warranties on the water heater.\nWater heaters with a fiberglass tank tend to last considerably longer than water heaters with steel tanks. Regardless of type, it is essential to remember that all tanks eventually fail no matter how well it’s maintained.\nCost to Repair a Tank Water Heater\nIf caught in time, most water heater repairs are relatively inexpensive and easy to do. However, this may not always be the best option. Here are some things that you should consider:\nYou should consider the repair cost and whether you can do that repair yourself or if you need to hire a licensed plumber. A plumber will cost about $50 to $150 per hour and often carries a minimum trip charge. If the water heater is experiencing a higher maintenance level, a replacement water heater will be your best option.\nThere’s no point in throwing good money after bad. If your repair cost will be 50% the cost of a new water heater and will only provide a small amount of remaining life expectancy, then replacement would be your best option.\n|Water Heater Repair||Part Cost||Repair with Labor|\n|Thermocouple||$20||$70 to $170 (1 hour)|\n|Gas Control Thermostat||$80||$130 to $210 (1 hour)|\n|Heating Elements||$40||$210 to $360 (2 hours)|\n|Electric Thermostat||$20||Included in Heating Element Replacement|\n|Leaking||Varies||Up to $1000|\nCost to Replace a Tank Water Heater\nNow that you’ve determined it’s time to replace your water heater, there are some things you need to consider:\n- Will you be replacing the water heater with a similarly sized unit? When you decided that it’s time to replace your water heater, it may be an excellent time to make an upgrade or to switch to a tankless system. If your house is damaged by a leak or needs to move the water heater, replacement is recommended.\n- Do you need a larger tank to meet your family’s needs? If your family has grown since you installed the water heater last time, a larger water heater may be desired. However, there are limitations to size when choosing tank water heaters, depending on where you live.\n- Will you install the water heater yourself or hire a licensed plumber? It is always recommended that you hire a licensed plumber to install a new water heater. Licensed plumbers in your area will know local building codes that you may not be aware of. However, depending on your skill level, you can install a new water heater yourself.\nNew Water Heater Tank and Installation Costs\nAccording to HomeDepot.com, the average cost of a new water heater installation is approximately $1,308. Nationwide costs typically range from $952 to $2,098. This replacement estimate includes the following components:\n- Traditional tank water heater – typical water heater size is 40 or 50 gallons.\n- Permits to install new water (some locations do not require this).\n- Installation materials such as fittings, shut-off valves, piping, etc.\n- Installation labor by a local, licensed, and insured plumber.\n- Removal of the old unit following local laws.\nIf you choose to purchase and install a water heater yourself, you can expect to pay between $350 to $2000 depending on the size and type of water heater you decide to purchase.\n|Water Heater Type/Size||Average Cost||Installed|\n|Electric, 40 Gallon||$400 – $450||$600 – $1050|\n|Electric, 50 Gallon||$450 – $1000||$650 – $1600|\n|Electric Hybrid, 50 Gallon||$1000 – $1500||$1200 – $2100|\n|Electric Hybrid, 80 Gallon||$1500 – $2250||$1700 – $2850|\n|Gas, 40 Gallon||$460 – $550||$660 – $1150|\n|Gas, 50 Gallon||$560 – $1200||$760 – $1800|\n|Gas, 75+ Gallon||$1400 – $1900||$1600 – $2500|\nBe Aware of Hidden Installation Costs\nWe touched on building codes and number three in the previous section. When you replace a water heater, you will be required to meet all current building codes.\nThere may be costs greater than just the water heater itself. Some of the hidden costs you may see when installing a new water heater include:\n- Expansion tanks – expansion tanks are used to help control pressure inside and water heater.\n- Seismic strapping – depending on where you live, you may need seismic strapping if you live in an area prone to earthquakes.\n- Drain pans – drain pans are placed under the water heater to catch leaks and discharge water to the exterior to protect the structure of your home.\n- Plumbing pipe upgrades – Depending on the type of plumbing supply pipes you have in your house, you may need upgrades. For example, if your home has polybutylene piping, you will likely need to replace all accessible piping during installation.\nWhy Hot Water Heaters Fail\nA water heater may seem to fail out of nowhere, but that’s not the case. Water heaters fail over time. It is recommended you drain and flush your water heater annually. It would be best to replace the anode rod every 2-3 years. This sacrificial rod protects the interior tank lining.\nYou can often predict water heater failure if you are aware of the signs. There are two main reasons why tank water heaters fail:\nToo much pressure inside the tank\nAs the water heats up, it expands, creating pressure inside the tank. If the water inside the tank heats up to the point that it exceeds the tank’s pressure specification, it could explode. One of the ways to avoid this is keeping your hot water temperature setting under 140°F.\nModern water heaters are equipped with a temperature and pressure relief valve (T&P valve). The temperature and pressure relief valve open when the pressure inside the tank reaches the tank’s temperature threshold. This valve expels hot water through a discharge pipe to relieve the pressure inside the tank.\nAnother safety feature that you can add is an expansion tank. Expansion tanks help control pressure inside the water heater tank by providing a place for hot water to expand. In a 50 gallon tank, hot water expands to 52 gallons. A 2-gallon expansion tank can store the expanded water.\nOne of the problems with old water heaters is that they lack a T&P valve and an expansion tank to help control the water heater’s pressure. With nowhere to expand, the pressure inside the tank can cause the water heater to explode, turning many old water heaters into a ticking time bomb.\nFlush Sediment Buildup Inside the Water Heater\nThe second leading cause of water heater failure is sediment buildup inside the tank. Chemicals and contaminants found in the water supply cause corrosion inside the tank, eventually rusting the tank, causing it to leak.\nExplosions occur when a flaw develops inside the tank. The tank’s pressure builds and forces water through these weak areas.\nAs part of your annual maintenance on your water heater, you should drain and flush out any sediment inside the tank. Tank water heaters have a drain valve at the base of the tank to do this. Here’s a quick rundown of how you can drain and flush your water heater.\n- Turn off the water supply before working on the water heater.\n- Turn off power or gas to the water heater before starting work.\n- Attach the hose to the drain valve and run it to the exterior to drain freely and safely. Remember the water will be hot.\n- Open hot water tap at a nearby sink.\n- Open the drain valve on the water heater to begin draining.\n- Once the water heater has drained, turn the water supply back on the flush the tank.\n- Close the drain valve once the water heater has been flushed and water running from the hose is clear.\n- Allow the water heater tank to refill.\n- Turn the power or gas back on to the water heater back. If you have a gas water heater, you’ll likely need to light the pilot.\nFor a more detailed step by step guide see, Water Heater Maintenance Tips to 2X Your Tanks Lifespan.\nReplace Your Anode Rod\nWater heaters with glass liners are equipped with an anode rod that attracts contaminants in water. Over time the contaminants will eat away at the anode rod protecting the tank’s liner. Once the anode rod has deteriorated, contaminants begin attacking the liner, weakening the water heater. You can expect anode rods to last 3-5 years, depending on the water quality and the type of anode rod installed.\nAnode rods are made from magnesium, aluminum, or aluminum/zinc alloy. Replacement anode rods can be purchased at local home improvement stores and are relatively easy to install.\nIt would help if you replaced your anode rod every 3 to 5 years. You can do this during your annual flushing maintenance of the water heater. You may want to replace a low-quality anode rod with a better quality one that will last longer.\nFor a more detailed step by step guide see, Water Heater Anode Rods: What it is & How to Replace it\nSigns You Need to Replace Your Water Heater\nAge of the Water Heater\nAge is a significant factor in your water heater’s performance. If your water heater is over ten years old, you should expect to have higher maintenance costs or to replace it in the near future.\nIf you’ve properly maintained your water heater, you may get up to 12 to15 years of life from a gas water heater and 10 to 15 years from electric water heaters. Beyond that, replacement is recommended mainly due to tank condition, rapid advances in safety, and improvements in energy efficiency.\nEnergy efficiency is crucial when it comes to gas water heaters. Energy consumption is higher in older water heaters versus newer models, which will pay for itself over time.\nRusty Water From Water Heater\nIf you see rusty or reddish-brown water from your water heater, that’s an indication that rust is mixing with water inside your tank. It’s a clear indication that your water heater needs replacement.\nIf the water heater is not leaking, you may be able to buy some time by performing a drain and flush maintenance on the water heater to remove loose sediment and change your anode rod. However, depending on the condition of the tank’s interior, this service may not be sufficient.\nUltimately, if the tank is damaged or starting to leak, there is no other option but replacement.\nGurgling Sounds Inside Your Water Heater\nAs your water heater gets older, sediment begins to build up at the bottom of the tank. When the sediment is heated and reheated over time, it begins to harden.\nBanging, crackling or gurgling sounds from inside your water heater indicates a sediment buildup inside the tank. It is recommended that you try to flush and drain your water heater. Depending on the amount of sediment build-up inside the water heater, the drain valve may be clogged or blocked, preventing it from draining water properly. One option would be to try replacing the drain valve to clear sediment from the water heater.\nIf all attempts to drain and remove the sediment fails, then replacement is the best option. In this case, the water heater’s replacement is recommended before a catastrophic leak occurs, which could cost thousands of dollars of damage to your house.\nLeaking from the Bottom of a Water Heater Tank\nIf your water heater is leaking, that is a good indication that there is something wrong. See our How to Repair a Leaking Water Heater – Complete Step by Step Guide for steps to repair a leaking water heater. You need to contact a licensed plumber to have them examine your water heater for repair or replacement.\nSome leaks can be repaired while others can’t. Leaks located around the top of the water heater, such as around the supply pipe fittings or the T&P valve, are likely repairable. However, this is not always the case and still an indication that something is going wrong internally that needs to be addressed.\nIf you’ve noticed water on the floor around your water heater, then you could have a severe problem. You can’t repair internal leaks in tanks. If water is leaking from the bottom of your tank, you should plan on an immediate replacement and call a licensed plumber.\nThe Water Heater Stops Producing Hot Water\nHot water is a daily luxury we’ve all become accustomed to. No hot water can be an inconvenience. If the electric water heater has no hot water, many things could be the cause. One is there is likely a bad heating element or a bad electric thermostat.\nAnother possibility is a broken drip tube. The drip tube is a plastic pipe that runs from the cold water inlet to the tank’s bottom. If this pipe is broken, cold water and hot water mix resulting in lukewarm water from the tank.\nThese items do commonly fail over time and can be repaired by a licensed plumber. These items are relatively easy to replace for the handy do-it-yourself person.']	['<urn:uuid:ffa0cf41-e609-4357-9f66-7f3bef6a1a6d>', '<urn:uuid:6bd65563-e38d-437b-9468-f0cdda09b846>']	factoid	direct	verbose-and-natural	distant-from-document	comparison	expert	2025-05-12T20:58:04.490895	26	64	3161
19	As an environmental consultant, I'm curious about how circular economy affects both business profitability and environmental health. What are the financial advantages for companies adopting circular practices, and what positive impacts does this create for natural ecosystems?	Circular economy practices offer several financial benefits for companies while positively impacting ecosystems. On the business side, companies can achieve lower input costs through reduced material usage (up to 70% material savings compared to raw material extraction), and create new profit streams through innovative services like collection logistics, product marketing platforms, and remanufacturing. Companies also become more resilient to supply chain disruptions and price volatility by reducing dependence on raw materials. Regarding environmental benefits, circular economy practices significantly reduce greenhouse gas emissions (potentially halving CO2 emissions by 2030), improve soil health by returning nutrients through composting and anaerobic processes, and decrease pollution. The model eliminates waste through superior product design and materials use, while promoting renewable energy adoption. Additionally, it helps regenerate natural systems by reducing toxic chemical use and optimizing agricultural productivity.	['Priorities and resources shaping the transition towards a circular economy in Europe and become more energy efficient\nIn 2020, the SARS-CoV-2 pandemic forced us to rethink many aspects of daily life. The amount of people commuting every day has dramatically dropped, with whole sectors switching to remote working, resulting in a temporary but significant reduction of air pollution levels around the world. However, many processes driving climate change are continuing at the same pace, or have even accelerated. New policy guidelines are therefore encouraging companies to make a more efficient use of their resources and adopt new business models, in order to cut energy consumption and at the same time save money that could be reinvested in people and innovation. At QuestPair we have reviewed some of the current available resources to help small and medium companies taking on these challenges.\nMeeting EU Green Deal goals\nFor any European company with the ambition to become more sustainable it is useful to understand the Green Deal goals and use them as a guideline to shape their transition towards a greener business model.\nThe Green Deal, the framework used by the European Commission to translate the United Nation’s 2030 Agenda into more measurable objectives, covers different areas of the productive and social life. It is made of eight different goals:\n- Increasing the EU’s climate mitigation and/or adaptation ambition\n- Supplying clean, affordable and secure energy\n- Transitioning of industry to a clean and/or circular economy (including waste prevention and/or recycling)\n- Building and renovating in an energy and resource efficient way\n- Accelerating the shift to sustainable and smart mobility\n- Transition to a fair, healthy and environmentally-friendly food system\n- Preserving and restoring ecosystems and biodiversity\n- Realising a zero pollution ambition and a toxic-free environment\nQuestPair wants to promote a sustainable society and wants to contribute positively to the implementation of the Green Deal. We have asked tech companies and scientists to which of the eight Green Deal goals their business contributes or their research can have an impact (Fig. 2). Interestingly, a large amount of respondents indicated that they can contribute to implementing one or more goals, highlighting the use of QuestPair as a means to find and employ suitable expertise and technology to do so.\nIn March 2020 the European Commission adopted a New Circular Economy Action Plan, described as “one of the main building blocks of the European Green Deal” (find the full description here). In general, the circular economy concept aims to prevent the erosion of resources, close energy and material loops, and facilitate sustainable development at different levels – for enterprises, consumers, economic agents, local and central governments (see Kirchherr et al. 2017; Kristensen and Mosgaard, 2020).\nThe capacity of SMEs to adopt circular economy practices plays a very important role in the implementation of EU objectives given that SMEs represent 99% of all businesses in the EU and provide two-thirds of the total private sector employment in the EU (Eurostat, 2018).\nIn order to implement this circularity, and make operational its emerging components, the Organization for Economic Co-operation and Development (OECD) recently given some broad definitions of possible circular economy business models:\n- Circular supply models: by replacing traditional material inputs derived from virgin resources with bio-based, renewable, or recovered materials, reduce demand for virgin resource extraction in the long run.\n- Resource recovery models: recycle waste into secondary raw materials, thereby diverting waste from final disposal while also displacing the extraction and processing of virgin natural resources.\n- Product life extension models extend the use period of existing products, slow the flow of constituent materials through the economy, and reduce the rate of resource extraction and waste generation.\n- Sharing models facilitate the sharing of under-utilised products, and can therefore reduce demand for new products and their embedded raw materials.\n- Product service system models, where services rather than products are marketed, improve incentives for green product design and more efficient product use, thereby promoting a more sparing use of natural resources.\nWhile the benefits of transitioning to a circular economy are evident both in terms of business advantage and societal impact, there are also several barriers challenging small and medium businesses, among which a lack of financial resources and a lack of technical skills.\nA good starting point for many can be the free training modules developed by the European Commission and DG Environment to help SMEs to go circular (available here). The modules are designed to be accessible but at the same time also indicate where to find additional resources on all the aspects covered.\nRecent studies also focused on identifying some key elements to assess the degree of circular economy implementation, so that small and medium‐sized enterprises can understand where they are and what they need to do to improve their performance (e.g. Prieto‐Sandoval et al., 2018).\nMoreover, there is a clear need for new strategies and tools supporting the SMEs implementing green business models. As stated by a frequently cited study on barriers and enablers encountered by SMEs implementing circular economy models, this support and recognition “can be achieved through the creation of dedicated marketplaces and communities of practice” (Rizos et al., 2016). QuestPair aims to make expertise required to set up circular business models and technologies more available and accessible.\nReducing energy consumption is becoming a strategic priority for the European Union. In fact, “Energy efficiency measures are increasingly recognised as a means not only to achieve a sustainable energy supply, cut greenhouse gas emissions, improve security of supply and reduce import bills, but also to promote the EU’s competitiveness” (source: European Parliament, Energy efficiency fact sheet). More general information on the EU framework on Energy Efficiency and its sub-sectors can be found here.\nA good starting point to lower the carbon footprint of your company and save money is an energy audit, to see how much energy they are currently using and to spot specific areas where they can make changes and improvements. A more accurate description of different kinds of audits can be found on the Energy Efficiency Network Europe platform, which aims to bring together energy supply actors, private companies and ESCOs in one international platform on energy performance contracting.\nA study on 280 energy audits in 7 countries across Europe found that SMEs saved on average 5% in energy consumption with the application of high quality and cost effective energy audits, with a potential of up to 20% (Fresner et al., 2017). Even more energy (and thus expenses) can potentially be saved by thoroughly reviewing core processes used in businesses. Much of the time, true energy and cost savings are made by using new technologies and through innovation. QuestPair enables companies to find and hire consultants and researchers for consultancy, independent reviews, and contract research projects and collaborations.', 'Sustainable development requires disruptive changes in the way our societies and businesses are organized. The circular economy (CE) model offers a new chance of innovation and integration between natural ecosystems, businesses, our daily lives, and waste management. Find out below the definition, meaning, principles, advantages, and barriers to a circular economy model.\nDefinitions Of Circular Economy\nSimple Definition Of Circular Economy\nIn the linear economy, raw natural resources are taken, transformed them into products and get disposed of. On the opposite, a circular economy model aims to close the gap between our cycle of production and the natural ecosystems’ cycle, as we ultimately depend on them. This means, on one hand, eliminating waste – composting biodegradable waste or, if it’s a transformed and non-biodegradable waste, reusing, remanufacturing and finally recycling it. On the other hand, it also means cutting off the use of chemical substances (a way to help regenerate natural systems) and betting on renewable energy.\nThe World Economic Forum’s Definition Of Circular Economy\n“A circular economy is an industrial system that is restorative or regenerative by intention and design. It replaces the end-of-life concept with restoration, shifts towards the use of renewable energy, eliminates the use of toxic chemicals, which impair reuse and return to the biosphere, and aims for the elimination of waste through the superior design of materials, products, systems and business models. ”\nEllen McArthur Foundation’s Definition Of Circular Economy\n“Looking beyond the current take-make-dispose extractive industrial model, a circular economy aims to redefine growth, focusing on positive society-wide benefits. It entails gradually decoupling economic activity from the consumption of finite resources and designing waste out of the system. Underpinned by a transition to renewable energy sources, the circular model builds economic, natural, and social capital. It is based on three principles: design out waste and pollution; keep products and materials in use; regenerate natural systems.”\nThe Principles Of A Circular Economy\nThe Principles Of The Circular Economy: Energy and Resources Are Gold\nAt its core, a circular economy model has the intention of designing out waste. In fact, a circular economy is based on the idea that there is no such thing as waste. In order to achieve this, products are designed to last (good quality materials are used) and optimized for a cycle of disassembly and reuse that will make it easier to handle and transform or renew them.\nIn the end, these tight product cycles differentiate the circular economy model apart from disposal and recycling, where large amounts of embedded energy and labor are lost. The ultimate the goal is to preserve and enhance natural capital by controlling finite stocks and balancing renewable resources flows.\nThe Principles Of The Circular Economy: Following Nature’s Cycles And Designs\nThe circular economy model makes a distinction between technical and biological cycles. Consumption happens only in biological cycles, where biologically-based materials (such as food, linen or cork) are designed to feed back into the system through processes like anaerobic digestion and composting. These cycles regenerate living systems, such as soil or the oceans, which provide renewable resources for the economy. By their turn, technical cycles recover and restore products (e.g. washing machines), components (e.g. motherboards), and materials (e.g. limestone) through strategies like reuse, repair, remanufacture or recycling.\nUltimately, one of the purposes of the circular economy is to optimize resource yields by circulating products, components, and the materials in use at the highest utility at all times in both technical and biological cycles.\nThe Principles Of The Circular Economy: All In With Renewable Energies\nThe last principle of a circular economy has to do with the fact that the energy required to fuel this cycle should be renewable by nature, with the purpose of decreasing resource dependence and increasing systems’ resilience. In this sense, this principle is about developing the systems’ effectiveness by revealing and designing out negative externalities.\nBenefits Of The Circular Economy Model\nSince industrial evolution, humankind has been following a linear model of production and consumption. Raw materials have been transformed into goods that are afterward sold, used and turned into waste that has been many times unconsciously discarded and managed. On the opposite, the circular economy is an industrial model that is regenerative by intention and design and aims to improve resources’ performance and fight the volatility that climate change might bring to businesses. It has benefits that are operational as well as strategic and brings together a huge potential for value creation the economic, business, environment and social spheres.\nFewer Greenhouse Gas Emissions – Environmental Benefits Of The Circular Economy\nOne of the goals of the circular economy is to have a positive effect on the planet’s ecosystems and to fight the excessive exploitation of natural resources. The circular economy has the potential to reduce greenhouse gases emissions and the use of raw materials, optimize agricultural productivity and decrease the negative externalities brought by the linear model. When it comes to reducing greenhouse gases, a circular economy can be helpful:\n- Because it uses renewable energy that in the long run is less polluting than fossil fuels.\n- Thanks to reusing and dematerializing, fewer materials and productions processes are needed to provide good and functional products.\n- Because residues are seen as valuable and they are absorbed as much as possible in order to be reused in the process.\n- Since the preferred choices will be energy-efficient and non-toxic materials and manufacturing and recycling processes will be selected.\nAs a matter of fact, an Ellen MacArthur Foundation study found out that a circular economy development path could halve carbon dioxide emissions by 2030, relative to today’s levels.\nHealthy And Resilient Soils – Environmental Benefits Of The Circular Economy\nThe principles of the circular economy on the farming system ensure that important nutrients are returned to the soil through anaerobic processes or composting, which softens the exploitation of land and natural ecosystems. In this way, as “waste” is returned to the soil, besides having fewer residues to deal with, the soil gets healthier and more resilient, allowing a greater balance in the ecosystems that surround it. As well, since the soil degradation costs an estimated US$ 40 billion annually worldwide, and has hidden costs such as the increase of fertilizer use, loss of biodiversity and loss of unique landscapes – a circular economy could prove to be really useful for both the soils and the economy.\nIn reality, a circular economy model working in Europe’s food systems has the potential to decrease 80% of the use of artificial fertilizer and therefore contributing to the natural balance in the soil, according to a study from the Ellen MacArthur Foundation.\n- More information on the negative impacts of the linear economy on the soil in our article > The ecological impact of coffee.\nFewer Negative Externalities – Environmental Benefits Of The Circular Economy\nFollowing the circular economy’s principles, negative externalities such as land use, soil, water and air pollution are better managed, as well as the emission of toxic substances and climate change.\n- More information about this topic in our page > Climate change: meaning, causes and consequences.\nIncreased Potential For Economic Growth – Economic Benefits Of The Circular Economy\nIt is important to decouple economic growth from resource consumption. The increase in revenues from new circular activities, together with a cheaper production by getting products and materials more functional and easily disassembled and reused, has the power to increase GDP and therefore economic growth, according to a McKinsey report.\nMore Resources Saved – Economic Benefits Of The Circular Economy\nWhen compared with the raw material extraction that’s common on the linear approach, the circular economy model has the potential to lead to a bigger (up to 70%) amount of material savings. Considering that the total demand for materials will increase due the growth of the world population and middle classes, a circular economy leads to lower material needs, as it skips landfills and avoids recycling, focusing on making materials’ cycles last longer. On the environmental side, it also avoids bigger pollution that extracting new materials would represent.\nEmployment Growth – Economic Benefits Of The Circular Economy\nAccording to the ‘world economic forum‘, the development of a circular economy model, together with a new regulation (including taxation) and organization of the labor markets, can bring greater local employment in entry-level and semi-skilled jobs. As well, the ExTax tax report made by specialists from several top consultancy firms also concluded about the potential of the circular economy to create new job. The same conclusion was reached by an August 2018 study on the development of practices to implement circular economy says that 50,000 new jobs could be generated in the UK and 54,000 in the Netherlands.\nAnother study conducted by the Ellen MacArthur Foundation and McKinsey also concluded about the changes in employment growth in case of a shift to a circular economy model. The study says that this new jobs will be created through increases in:\n- Recycling and repairing practices, where one could add new designers and mechanical engineers to make lasting and easily disassembled products and materials at the transformation/production stages;\n- An increase in new businesses (and niches) due to innovation processes and new business models;\n- An increase in consumption and spending by lower prices.\nNew Profit Opportunities – Benefits Of The Circular Economy On Businesses\nLower input costs and in some cases create entirely new profit streams can be achieved by businesses that move to the circular economy model. In this circular sphere, profit opportunities may come from playing in new markets, cutting costs off with waste and energy reductions and the assurance of continuity of supply.\nMore information on new opportunities in our article > China is banning plastic waste, can this be good?\nVolatility Reduction And Safeguarded Supplies – Benefits Of The Circular Economy On Businesses:\nMoving towards a circular economy model meanings reducing the number of raw materials used. Instead, more recycled (or even reusable or easily transformed) inputs that have a higher share of labor costs would be used, leaving companies less dependent on the volatility of the price of raw materials. This would also protect companies of geopolitical crisis and safeguard them regarding their supply chains – whose probably to be destroyed or damaged because of climate change events is increasing every day. In the end, the circular economy model would turn businesses more resilient, or in other words, make them more resistant and prepared to deal with unexpected changes.\nThe Demand For New Services – Benefits Of The Circular Economy On Businesses\nAccording to Ellen McArthur’s Foundation report, a circular economy model has the potential to create demand for new services and new job opportunities such as:\n- “Collection and reverse logistics companies that support end of life products being reintroduced into the system\n- Product marketers and sales platforms that facilitate longer lives or higher utilization of products\n- Parts and component remanufacturing and product refurbishment offering specialized knowledge “\nThese new services can be both identified by the top management decision-makers, or as well, in a well-developed green by employees from all levels and departments. To get to know more follow on our page > Green human resources management.\nGetting To Know Clients Better – Benefits Of The Circular Economy on Businesses\nThe circular economy model seems to foster business models where products are rented or leased by customers during different periods of time, depending on the type of products. This gives businesses the chance to learn about their customer’s usage patterns and behaviors, as they get to interact more often with them. Ultimately, this new relationship might just improve customer satisfaction and loyalty, and contribute as well for the development of products and services that suit clients better. In a market where suppliers remain responsible for the product supplied for a longer period, communicating well and understanding the clients’ preferences and needs is more important than ever.\n- More information about this topic in our page > stakeholder engagement definition & strategies.\nBarriers To The Implementation Of A Circular Economy Model\nImplementing a circular economic model would have several benefits for the environment, economy and businesses, as we’ve discussed above. Nevertheless, there are some reasons that explain why this model has been growing slowly.\nEconomic Barriers To A Circular Economy Model\nIn our current economic system, there are some barriers to the implementation of a circular economy model, such as:\n- Social and environmental externalities are not considered in prices, privileging financial market signals instead of people and nature when economic decisions are made;\n- Prices of raw materials are fickle and at low prices alternative, good quality secondary resources are not competitive;\n- Circular economy business models are harder to develop, as most investors are still working under a linear economy logic and sometimes upfront investments are required;\n- The demand for circular products and alternatives is still small,\n- There aren’t still many qualified professionals with technical or ‘information and communication technology’ (ICT) knowledge.\nInstitutional Barriers To A Circular Economy Model\nWhen it comes to implementing and developing the circular economy, many different barriers might need to be overcome, such as:\n- The fact that our current economic system is geared towards the demand of the linear economy and ain’t yet prepared to deal with circular economy entrepreneurs;\n- New business models may be challenging to implement and develop because of laws and regulations that aren’t prepared for this kind of innovations;\n- Plenty of business rely on old and/or strong alliances, making it harder to create new alliances and therefore to close loops;\n- Many companies still have goals and appraisal systems that focus on short-term value creation, whereas the circular economy model is a long-term value creation model;\n- The GDP index doesn’t consider social and environmental externalities, discouraging the creation of value in both these areas;\nA Broad Perspective On The Barriers To A Circular Economy Model\nA Swedish study conducted in 2017 that aimed to integrate different perspectives on this topic suggests that the main barriers to moving towards the circular economy model can be divided into financial, structural, operational, attitudinal and technological.\nThe first barrier has to do with the challenge of measuring the financial benefits of CE and its profitability. The ‘structural’ barrier that follows has to do with being unclear of gets responsible for CE within companies. By their turn, ‘operational’ challenges represent the difficulty of dealing and staying in control of processes within the value-chain. The fourth barrier, ‘attitudinal’, has mostly demonstrated the lack of knowledge about sustainability issues and also a big risk aversion – it shows that disruptive changes aren’t the best way to develop circular strategies. The last block on circular economy, named ‘technological’, has to do with the need for changing and re-designing products and production/ take-back systems. These needs end up creating concerns about the ability to do this and still being competitive and having quality products.\n[box] Some of the sources used to find out more about the circular economy:\n- European Comission\n- World Economic Forum Report\n- Science Direct Article 1\n- Science Direct Article 2\n- Ellen McArthur Foundation\n- Ellen McArthur Foundation’s Report 2015\n- Ellen McArthur Foundation’s Report 2016\n- Fiscal Reforms for Circular Economy Report\n- KENNISKAARTEN Knowledge Map\n- McKinsey Report\n- The Ex’Tax Project [/box]\nImage credits for Shutterstock on: circular economy model definition, circular economy definition principles, circular economy model benefits, circular economy model economic benefits, circular economy barriers & circular economy sustainable development']	['<urn:uuid:f6001fef-2ff0-439b-b32e-c1bf75f0044a>', '<urn:uuid:25ec525f-9ddc-4f38-aa9a-78be85e3c59d>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:58:04.490895	37	133	3725
20	scientific research mind wandering productivity gains	According to a study by University of California, Santa Barbara researchers published in Psychological Science, participants who engaged in undemanding tasks during breaks performed 41% better at problem-solving compared to those who rested, had no break, or performed demanding tasks during the break period.	['No matter how many years of experience you have, it seems that there is always something new in the world of programming. It’s a constant process of learning, as programming languages, scripts, tools, software, methods and best practices evolve. Depending on the project, the work can also become abstract and quite challenging; especially when trying to come up with something unique or doing something new for the first time. Every programmer occasionally gets stuck and sometimes documentation isn’t enough, searching doesn’t reveal an acceptable answer and co-workers don’t have any helpful clues. While it may not work for every programmer, I’ve solved several of my most difficult coding issues away from a computer. Not in front of a whiteboard or with a notebook, but by completely taking a break from trying to solve the issue.\nThis concept certainly isn’t anything new and the sudden insights achieved away from the task at hand are usually referred to as “eureka” moments, thanks to the reaction Archimedes had after figuring out how to determine whether King Hiero II’s crown was pure gold while sitting in a bathtub (or so the story goes). There are many other examples throughout history. For example, Einstein solved special relativity while riding a streetcar home, alternating current came to Nikola Tesla on a walk, Newton famously came up with universal gravitation after seeing an apple fall from a tree, Otto Loewi came up with an experiment to determine how nerve cells transmit messages after dreaming about it and Descartes came up with the Cartesian coordinate system after watching a fly crawl across his ceiling in bed one morning.\nWhile these examples are groundbreaking discoveries, the important takeaway is recognizing the pattern of how and when the problems were solved. Taking a break and performing a simple task to let the mind wander can be absolutely pivotal in complex problem solving. It is so easy to get locked in when you’re stuck on a programming issue, doing guess and check work over and over with no success or frantically searching Stackoverflow as the time ticks away. When I get stuck on a difficult problem and I’m unable to solve it in a reasonable amount of time, I try to step away. At Westwerk, this means playing a round of foosball, talking with a co-worker, going for a walk or even taking a bathroom break. Just sitting stationary staring at a screen and racking your brain as the stress levels rise is usually not the best approach in my experience.\nStudies on how this might work break human thought into conscious and unconscious. It’s hypothesized that there is a high relation between unconscious thought and divergent thinking, which is defined as creative thinking, such as generating several possible solutions to a problem. From the conclusion of “Where creativity resides: The generative power of unconscious thought” by Dijksterhuis and Meurs published in volume 15, issue 1, March 2006 of Consciousness and Cognition:\n“One could say that unconscious thought is more ‘liberal’ than conscious thought and leads to the generation of items or ideas that are less obvious, less accessible and more creative. Upon being confronted with a task that requires a certain degree of creativity, it pays off to delegate the labor of thinking to the unconscious mind.”\nThe real creative boost seems to happen after performing an undemanding and unrelated task during the break, not by just doing nothing. In a study by researchers at the University of California, Santa Barbara titled “Inspired by Distraction: Mind Wandering Facilitates Creative Incubation” that was published in volume 23, August 2012 of Psychological Science, it was found that in a repeated-exposure condition (coming back to the same problem), participants that engaged in an undemanding task during a break period performed 41% better than participants that rested, had no break or performed a demanding task during the break.\nThat’s a number that programmers and employers shouldn’t ignore. Having to generate new ideas to solve demanding problems is something that comes up in programming all the time, and the evidence from the studies referenced above, among several others, show that breaks can be an important part of the process. Giving yourself and/or your employees the flexibility to approach problem solving in ways that might not be considered normal in a typical office environment could translate to significant performance boosts and increases in efficiency.']	['<urn:uuid:b4e17a2b-c0c5-44ef-8f3b-4f2d42df1eb9>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T20:58:04.490895	6	44	728
21	I noticed that my friends react differently to social situations - some get energized while others get anxious. How do personality traits like extraversion and neuroticism influence how people respond to social interactions and stress?	Extraversion and neuroticism indeed influence how people respond to social situations quite differently. People who score high on extraversion show enthusiasm and stimulation from other people's company - they're full of energy and actively seek attention from others. They tend to be competitive and highly involved in many social circles. In contrast, people who score high on neuroticism are more prone to experience anxiety and stress in common situations, often perceiving normal interactions as threatening. This links to behavioral research showing that extroverts/low-anxiety individuals tend to respond more positively to social reinforcement, while introverted/high-anxiety individuals may respond differently to social situations and negative reinforcement.	['Applied Behavior Analysis and Personality Psychology at first glance have very little in common. Applied Behavior Analysis (ABA) comes from the behaviorist tradition of the purely observable, and Personality Psychology features variables that are often seen within the individual and outside of direct measurement. As time moves on in the field of psychology, and the behavioral fields specifically, there is a call for greater breadth and understanding from practitioners across more than one domain. Behaviorism as a field of psychology is alive and well, but sometimes practitioners can pigeonhole themselves (pardon the pun) into the strict traditionalist ideas of the early 20th century, leaving the cognitive revolution and relevant psychological progress aside.\nFew people realize, that this is not too a large gulf to bridge.\nThe topic of personality and temperament in individuals was touched on by B.F Skinner himself in “Science and Human Behavior” (1951) and “Beyond Freedom and Dignity” (1971), but as many would suspect, the meaning of the word personality was operationalized to a series of observable concepts such as “response tendencies”. These tendencies of responding were used to explain how individuals varied in their sensitivity to stimuli. It stands to reason that everyone in their life has come across another individual who was not impacted by a stimulus in the same way as themselves. This is a basic part of humanity. This is the reason we need to clinically perform preference assessments. Individual differences occur regardless of standardized stimuli. No matter how precisely we form a potential reinforcer, no matter how accurate the degree of the amount, or intensity, or even how carefully a schedule is arranged; one person may respond differently to it than another. And that is not including motivating operation factors like deprivation and satiation. Sometimes people are affected by different things in different ways, and they respond to different things in different ways.\nPersonality Psychology concerns itself with these individual differences. It is a field that is interested in the unique differences of the thinking, behaving, and feeling of individuals. Personality Psychology studies traits or factors based on the similarities and differences of individuals. Some feature traits such as Extraversion, Neuroticism, and Psychoticism (Eysenck Personality Inventory), Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism (The Big Five). Others add in the traits of Honesty and Humility (HEXACO). Although there are many different theories on how these personality traits are formed, are measured, and are predictive; they still aim to explain something that strict observation of antecedent or consequence stimuli appears to miss. Behaviorists and practitioners of Applied Behavior Analysis may look at these things and pump their brakes. After all, it seems like a challenge to align the methods found in Personality Psychology to the dimensions of behavior analysis that Baer, et al. constructed in 1968. How does personality fit into a strictly behavioral framework? What about making personality framework conceptually systematic? Or could an experimenter even demonstrate control in a way to be analytic? Baer, Wolf, and Risley themselves said that a self-reported verbal behavior could not be accepted as measurable unless it was substantiated independently. How do we do it, then?\nFirst, we may want to take a step back and work on defining what we are looking at. Behaviorists and ABA practitioners are used to a functional analytic approach which aims to identify exactly that; functional relationships between the environment and clinically targeted behaviors. Personality Psychology, on the other hand, is a little more topographical in how traits are defined. They look at classifying traits by what they present as, how they appear, and reports of how people act, and think, with less emphasis on that environment link. One of the great researchers to bridge these two ways of studying personalities, tendencies, and behavior, was Jeffrey Gray who looked at the personality inventories and questionnaires of Hans Jürgen Eysenck, and developed a theoretical model which related these personality and temperament factors to behavioral inhibition (behaviors likely to be inhibited where cues of punishment or lack of reinforcement are found), and behavioral activation (behaviors likely to be activated in the presence of possible reinforcement or cues of no punishment). Here, personality traits of extraversion and introversion, for example, were related to dimensions of anxiety or impulsivity which could be easier to define and study behaviorally. Gray (1981) was interested in how these traits could explain “sensitivity” (higher responding) or “hypo-responsiveness” (lower responding) to punishment and reinforcement stimuli.\nWould someone who was rated higher in extraversion/low-anxiety respond a certain way to social positive reinforcement?\nWould someone who was rated higher in introversion/high-anxiety respond a certain way to social negative reinforcement?\nThese are some questions that might pique the interest on both sides of the fence, both Behavior Analytic, and Personality Psychology. Take any one of those personality traits above, and you may find similar ways to study it behaviorally. The literature on this type of work is impressive. Gray’s work which began in the 1970s, went on for over 30 years. There is a wealth of literature on the topic of his theoretical models, and the topics of the Behavioral Inhibition System (BIS) which relates factors that impact a reduction of responding, and Behavioral Activation System (BAS) which relates factors that impact an increase in response activation, from Gray’s work in 1981. In 2000, Gray & McNaughton presented a third theoretical system called FFFS (fight-flight-freeze system) to explain responses to unconditioned aversive stimuli in which emotionally regulated states of “fear and panic” play a role in defensive aggression or avoidance behaviors. These took into account neuropsychology and went even further to suggest links to conflict avoidance in humans in day to day life. The literature on this is absolutely fascinating in how it finds a way to bring behavioral analytic concepts to a new arena.\nCould it be possible for one day to see Personality Psychologists talking about reinforcement and punishment sensitivity? How about Behavior Analysts talking about traits when considering consequence strategies? At the very least, it’s a conversation that neither field might have had without knowing. We can only hope to gain from stepping outside of traditional boundaries and broaden our intellectual horizons.\nComments? Questions? Thoughts? Leave them below!\nBaer, D. M., Wolf, M. M., & Risley, T. R. (1968). Some\ncurrent dimensions of applied behavior anlysis. Journal of\napplied behavior analysis, 1(1), 91-97.\nHEXACO model of personality structure. (2018, April 22). Retrieved from https://en.wikipedia.org/wiki/HEXACO_model_of_personality_structure', 'Personality traits are the distinguishing qualities or characteristics of a person. People’s traits are viewed along a continuum where people are seen to differ in the quantity of a characteristic they display. Yet, how are personality traits grouped and what are some words that describe personality traits?\nThe field of psychology’s Big Five model uses five words that describe personality traits. The personality traits can be grouped into these categories:\nThe model is a comprehensive data-driven finding on the most basic personality traits in people. It is a very broad model with several subgroups to describe more specific traits.\nPeople who display the openness trait appreciate adventure, imagination, curiosity, new ideas, emotion, and diversity of experiences. This category separates people who are imaginative from those who are more practical. Open people are often appreciative of art and in amazement of the natural beauty that surrounds them. They are often travelers seeking new cultures and ideas to add to their own experiences.\nScoring high on the openness scale often indicates people who are more creative and able to express and understand their emotions. Scoring low on the scale generally indicates people who are more straightforward or traditional. People with low scores are generally uninterested in the arts and sciences and prefer a simple solution to a more complex or inventive one. Any words that describe adventurous and imaginative people can be used to describe individuals with the openness personality type.\nPeople who score high on the conscientiousness scale show great self-discipline and awareness of their responsibilities to themselves and society. They often have a very high regard for achievement and will use achievement as a means to measure themselves against others. This leads to conscientious people to be very organized, academically prepared and successful in a variety of situations. In fact after intelligence was factored conscientiousness was found to be the most useful predictor of performance in the workplace out of any of the Big Five traits.\nPeople who are conscientious are very organized and follow schedules, but low scores on the conscientiousness scale correlate with people who are unorganized and seemingly unable of accomplishing tasks that they would like to. Low scores are also strongly correlated with procrastination. Any words that describe organized and hard working people can be used to describe individuals with the conscientiousness personality type.\nPeople who score high on the extraversion scale show enthusiasm, self-confidence, and stimulation from other people’s company. They are often full of energy and actively seek out attention from others. Extroverts are often competitive in nature and highly involved in many social circles and activities. Any words that describe fun loving people who are the life of the party can be used to describe individuals with the extraversion personality type.\nPeople who score high on the agreeableness scale show great compassion, cooperativeness, and empathy for all the members of society. They value being congenial with others and will go to great lengths to secure and maintain their relationships. Agreeable people have an optimistic view on human nature and believe that people in general and honest and cooperative. This altruistic view on life leads to them being involved in many social causes. Any words that describe cooperative and caring people can be used to describe individuals with the agreeableness personality type.\nPeople who score high on the neuroticism scale show a propensity to experience anxiety, depression, or anger. They are often easily disturbed or stressed by a variety of common situations leading them to express a negative emotion. This often leads to them experiencing normal interactions as threatening and trivial problems as impossibly difficult. Any words that describe people who are often angry or anxious about normal situations can be used to describe individuals with the neuroticism personality type.']	['<urn:uuid:9930f3dc-6c25-48d8-a08d-85bc8b36bcd9>', '<urn:uuid:b6f9066c-38c2-43ce-bcd2-fd6ecbd5cfbd>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T20:58:04.490895	35	104	1690
22	What are the business models on dubizzle.com and what legal concerns exist for copying its content?	Dubizzle.com features business and economy models, portfolio experience models, and advertising models for cars and property. They also offer services for building traffic and revenue generation. Regarding legal concerns, copying content like car advertisements would raise copyright issues since website content is automatically protected by copyright law. The creator has exclusive rights to control how their work is used, and copying without permission would violate these rights under international copyright agreements.	['Revenue model is a design strategy of e-commerce used to demonstrate models of sales, company products, services and revenue forecasts of accompany. Revenue models also play an important role in value appropriation. It complements business model design just like product design can be complemented by pricing strategy. The statements of cash flow in accompany can thus be demonstrated using revenue model through the development of account receivable (Zott and Amit, 2010).\nThe first website, that is, www.souq.com comprises of revenue model that can be used for online shopping. This includes selling Microsoft products such as Windows ultimate and other services. The revenue model in this site also permits buying and exchange of goods in a market. Other models demonstrated here include those of Hp laptops, notebooks, other blackberry products among others. Hostels with traditional retail and dining experience, business and holiday travels as well as tourism are some of the services demonstrated using revenue model within this site. Others include shipment and advertisement of various goods and services.\nThe second web site- www.dubizzle.com contains a number of things. There is the issue of business and economy with ranch model and a variety of internet services for building traffic and revenue generation. The site also demonstrates the portfolio experience model along with art model for photography portfolio. The site also demonstrates the model for city exhibitions and forums. Moreover, advertisement business such as careers and job vacancies are part of models illustrated in the web site through revenue model.others includes data concerning page visits, page views, earnings in addition to evaluation of site value. Others are cars and property model advertisement.\nThe third web site which is www.dailymotion.com illustrate business model for revenue share through perfect online business model. The services for video streaming based on results are also described. The streaming services illustrated are such that everyone can earn although recruitment is optional. The site also illustrates the issue of enriching the web content and maximizing revenue by embedding daily motion videos on the given site for payments. There is also sales model for selling streaming services without having to be based on bandwidth and time on the system. Another revenue model is that of increasing advertising revenue directly from the daily motion’s library as well as empowering publishers to their sites by the daily Motion Matchbox. Also included is the implementation of the crowdfunded revenue model.\nThis site identifies government to consumer e-commerce applications for UAE. The web site used for this information gives one article published at Eiilm University in India. The Article is entiled,”Electronic Governance”. The site is therefore relevance given that this article describes various information delivery methods, one of which is government to customer/consumer delivery. Non internet e-government application has been mentioned in this article. The examples of non internet e-government include telephone, fax, SMS, PDAs, MMS, wireless networks, Bluetooth, CCTV, biometric identifications, tracking systems and many others. Internet examples include regulatory services, general holidays, public hearing schedules, issue briefs, notifications, and others (Jorethang, 2013).\nSite 2: http://arxiv.org/abs/1105.6358\nThis is a scholarly web site which explains innovative approach for e-government transformation in UAE government. The article,”An innovative approach for e-Government transformation” in this site further gives relevant explanation on how technology is employed to facilitate services to the citizens and customers. E-government is explained in this case as the use of technology by governments for the advancement of government service access to citizens and to permit citizens to make online dealings. Web based internet applications is used to facilitate such online transactions for government to customer service delivery. Quick advancement in technology and the coming on of these internet applications made public expectations to be redefined concerning government services. This technology has led to elimination of paper transactions. The success of these government services is dependent on the assumption of sound infrastructure for telecommunication (Al-Khouri, 2011).\nThis website, same as the one given previously as site 1 also give an additional application used for service delivery to consumers by the government. This application is the customer relationship management (CRM) system (Jorethang, 2013).\nThis is a scholarly source web site. It offers one article that describes business to business e-commerce applications used in UAE. The article,” The Impact of E-commerce on Developed and Developing Countries” is a case study taken from Egypt and the United states.\nBusiness to business e-commerce takes place between businesses. The dealings are often conducted using electronic data interchange (EDI) applications. EDI permits extra transparency among the stakeholder businesses thereby enhancing efficiency. EDI is normally a standard format for communicating business related data. It is therefore a form of e-commerce that includes fax and email for transactions. It is only used by parties that know one another well so that arrangements for one to one link can be completed through dial up. Electronic data interchange is a computer to computer, inter-organizational exchange of documents for business (Zeinab, 2012).\nThis website is also important since it has been used by the author of the article mentioned above in the EDI case. The site thus gives additional application among others which can be used for business to business communication for enhancing transactions in the field of e-commerce.\nElectronic funds transfer system (EFTS) is one of the application mentioned in this site. The application is very useful in optimizing electronic payments. These type of payments are equipped with electronically remittance information. This system can therefore be used to send financial electronic information from say, one bank to the other without handling cash. In other words, EFTS has led to cashless society (Zeinab, 2012).\nIf a company happen to copy the car advertisements from dubizzle.com and paste on their own website – to attract more users, then the question of whether, then the question of whether there is any ethical or legal concern in it can be argued out from the point of view of Rules for Ethical Decision. These rules include the utilitarian rule, moral rights rule, justice rule and practical rule. The utilitarian rule is where an act becomes ethical when it produces the greatest good for the greatest number of people. This means that managers should consider how the action of copying car advertisement from another company can harm or benefit different stakeholders involved. The action that offers the highest benefit or least harm to stakeholders is then chosen (Ferrell and Fraedrich, 2014).\nThe moral rights rule considers an action to be ethical if the action maintains or protects the basic or unchallengeable rights of the people. This means that the action that guards people’s rights to liberty, life, security, property, privacy, free speech and freedom of beliefs is ethical. The rule is thus based on the opinion of doing well to others if you too expect them to do the same to you. In the justice rule, if an act can distribute benefit and harm among people and groups, in affair way, then it is considered ethical. This implies that managers in this case have to compare and contrast alternative courses of action. This comparison will depend on the level to which result in equitable distribution of outcome to stakeholders. As a result, if the act of copying advertisement harms any of the stakeholders more than the other or benefits the other stakeholder less than the other, then it should be avoided (Ferrell and Fraedrich, 2014).\nThe practical rule comes in where the rights, interests, goals and incentives of various stakeholders are at conflict. In this case, an act will thus be considered ethical if a manager has no hesitation about communicating to society since the typical person will think it is acceptable. In other words, it is ethical if the action falls within the acceptable values or standards that typically apply in the current business situation. Second, it is ethical if the person making the decision is willing to see the action communicated to all people and groups affected. Lastly, it is ethical if the people with whom the decision-maker has significant personal relationship such as managers in other organizations can approve the action. This rule is thus more general as opposed to the first three which mainly applies in sorting out the ethics of a particular course of action (Ferrell and Fraedrich, 2014).\nZott, Christoph, and Raphael Amit. “Business model design: an activity system perspective.” Range planning 43.2 (2010): 216-226.\nJorethang. (2013).”Electronic Governance”, Eiilm University, District Namchi, Sikkim- 737121, online: Available at:\nAl-Khouri, Ali M. “An innovative approach for e-Government transformation.” arXiv preprint arXiv: 1105.6358 (2011).\nZeinab Mohamed El Gawady, “The Impact of E-commerce on Developed and Developing Countries Case Study: Egypt and United States”, phD, Dept. Business & economics, Misr University for Science and Technology, Giza-Egypt, 2012.\nFerrell, Odies C., and John Fraedrich. “Business ethics: Ethical decision making & cases. “Cengage Learning, 2014.\n(Ferrell and Fraedrich, 2014).\nPLACE THIS ORDER OR A SIMILAR ORDER WITH GRADE VALLEY TODAY AND GET AN AMAZING DISCOUNT', 'Along with blogging comes responsibility and ignorance of the law is no excuse. Copyrightable works include but are not limited to literary works such as articles, blog posts, stories, journals, or computer programs, pictures and graphics, as well as audio and video recordings. Copyrights do not need not be applied for as they are vested in the creators of intellectual property. When we create something — we own the copyright, which is our exclusive right as the creator to control who else can use our work and in what manner. *\nBerne Convention – International Copyright Agreement\nThe Berne Convention (for the Protection of Literary and Artistic Works) was established in 1886 and is an international agreement that governs copyright. The Berne Convention requires its signatories to recognize the copyright of works of authors from other signatory countries (known as members of the Berne Union) in the same way as it recognizes the copyright of its own nationals.\nIn the United States, the Library of Congress officially registers copyrights which now last for the life of the author plus 70 years.\n- In the case of a joint work prepared by two or more authors who did not do a work for hire, the term lasts for 70 years after the last surviving author’s death.\n- In the case of works for hire, and for anonymous and pseudonymous works (unless the author’s identity is revealed in Copyright Office records), the duration of copyright will be 95 years from publication or 120 years from creation, whichever is shorter.\nThanks to organizations like Creative Commons, licenses like the GNU Free Documentation License, and the public domain, there are many images, songs, movies and documents freely available for you to download and republish without fear of violating copyright.\nPublic domain definition: The public domain is generally defined as consisting of works that are either ineligible for copyright protection or with expired copyrights. Public domain refers to the total absence of copyright protection for work The public domain is a range of abstract materials commonly referred to as intellectual property which are not owned or controlled by anyone. The term indicates that these materials are therefore “public property”, and available for anyone to use for any purpose.\nOnce in the public domain, it is always in the public domain. However, any variation on any public domain work becomes the property of the person making the variation, and it receives an automatic copyright, just as do completely original works.\nWhen Copyright Protection Becomes Public Domain\nThe data below will let you know when you can safely use a piece of art or music without permission because it is now in public domain after copyright protection expiration, or how long the copyright protection will last.\n|Published before 1923 – now in public domain.\n||Published from 1923 to 1963 – When published with a copyright notice © or “Copyright [dates] by [author/owner]” – copyright protection lasts 28 years and could be renewed for an additional 67 years for a total of 95 years. If not renewed, now in public domain.||Published from 1923 to 1963 – When published with no notice – now in public domain.||Published from 1964 to 1977 – When published with notice – copyright protection lasts 28 years for first term; automatic extension of 67 years for second term for a total of 95 years.|\n|Created before 1/1/1978 but not published – copyright notice is irrelevant – copyright protection lasts for the life of author and 70 years or 12/31/2002, whichever is greater.||Created before 1/1/1978 and published between 1/1/1978 and 12/31/2002 – notice is irrelevant – copyright protection lasts the life of author and 70 years or 12/31/2047, whichever is greater.||Created 1/1/1978 or after – When work is fixed in tangible medium of expression – notice is irrelevant – copyright protection lasts for the life of author and 70 years based on the longest living author if jointly created or if work of corporate authorship, works for hire, or anonymous and pseudonymous works, the shorter of 95 years from publication, or 120 years from creation.|\n* Reblogging and WordPress.com Terms of Service: “By submitting Content to Automattic for inclusion on your Website, you grant Automattic a world-wide, royalty-free, and non-exclusive license to reproduce, modify, adapt and publish the Content solely for the purpose of displaying, distributing and promoting your blog.”\nI have listed many free sources of images on my Resources page and my list is by no means complete. Do you have favorite sources of free images, songs, movies and documents you would like to share? If so please comment so I can include your sources on my Resources page as well.\nRelated posts found in this blog:\nCopyright basics for bloggers\nHow to copyright your digital works\nContent theft: The come and get it solution\nSplog Off! Dealing with content theft\nSplogSpot: Dealing with content thieves\nCopyright: Fair Use Limitations\nWhat is copyright?']	['<urn:uuid:e5e3d7a7-8b35-44cd-b0a5-1e4a1505b874>', '<urn:uuid:70d48e88-fb82-4226-a927-1bec67f79226>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T20:58:04.490895	16	71	2304
23	knee specialist explain why young patients osteotomy instead knee replacement surgery benefits limitations	For patients under 55 years old, knee replacement is often not recommended because the replaced joint can wear down much more quickly compared to older patients. Instead, these younger patients may undergo an osteotomy, which involves removing or adding part of a bone in the leg to alleviate pressure on the knee. However, this is generally considered a temporary solution, as these patients will usually need knee replacement surgery in the following years.	['About Osteoarthritis Treatment Abroad\nAbout Osteoarthritis Treatment\nOsteoarthritis treatment aims to alleviate symptoms of osteoarthritis, a condition that causes pain, swelling, and stiffness in joints in the body. The condition is caused by a breakdown of cartilage, which is a buffer between the joints and the bone and protects the joints. Cartilage wears down overtime and the body is usually able to repair the damage and continue protecting the joints, however, when it does not repair the cartilage, friction between the bone and joint occurs. The joint may deteriorate, causing pain and stiffness.\nOsteoarthritis is not a curable condition, however treatment can help to relieve and manage symptoms. Many patients with the condition may experience limited motion with the affected joints, due to the stiffness caused by the breakdown of the cartilage. While the condition can affect any joint in the body, it most commonly occurs in the knees, hips, hands, and spine. It is a degenerative condition, meaning it will worsen as time goes on, which is why it is important to seek treatment once symptoms start to show, in order to try and slow down the process and relieve symptoms.\nThe condition is more likely to occur in older age, with most patients experiencing symptoms from the age of 45 or older, and it is more likely to affect women more than men. There are a number of factors which can increase the chances of osteoarthritis occurring and these include family history, obesity, and if the patient already has arthritis.\nTreatment options include medication for managing pain and inflammation, lifestyle changes, physical therapy, and in some cases surgery may be an option. Surgical treatments include joint replacement and osteotomy.\nOsteoarthritis mainly affects the hands, hips, and knees.\nOsteoarthritis mainly affects the hands, hips, and knees.\nHow to find quality treatment abroad\n200000Patients who used MEDIGO\n1200Clinics across 35 countries\n195Countries represented by MEDIGO patients\n20Languages spoken at MEDIGO\nBefore Osteoarthritis Treatment abroad\nPatient will have a consultation with a rheumatologist or an orthopedic specialist, to discuss the treatment plan for managing their condition. Patients should raise any questions or concerns that they may have and explain to the doctor the symptoms they have and for how long they have been experiencing pain, stiffness or swelling.\nThe doctor will take a full medical history of the patient and may run some tests such as blood tests, order an X-ray or MRI (magnetic resonance imaging) in order to help with making a treatment plan, if such tests have not already been performed.\nIf undergoing surgery, the patient will usually be advised to refrain from eating and drinking in the hours preceding surgery, in order to prepare for the general anesthetic.\nPatients with complex conditions may benefit from seeking a second opinion before beginning a treatment plan. A second opinion means that another doctor, usually an expert with a lot of experience, will review the patient’s medical history, symptoms, scans, test results, and other important information, in order to provide a diagnosis and treatment plan. When asked, 45% of US residents who received a second opinion said that they had a different diagnosis, prognosis, or treatment plan. Click here to learn more about how to get a second opinion.\nHow is it performed\nOsteoarthritis can be managed with medication such as nonsteroidal anti-inflammatory drugs (NSAIDs) to help treat the inflammation and pain in the joints. Some NSAIDs that are used are often non-prescription medication such as ibuprofen which the doctor may recommend, while others may be prescription medication that may have side effects.\nPhysical therapy may be recommended to the patient, and this will need to be attended on a regular basis in order to experience consistent relief. The patient will perform certain exercises and movements, in order to improve the strength of their muscles and to help with regaining motion in their joints.\nSome patients may be given injections such as cortisone or lubrication injections to relieve pain in the joints. This involves injecting medication into the joint using a needle.\nDepending on the area of the affected joint and severity of the condition, surgery may be an option to help treat osteoarthritis. Patients who have osteoarthritis in their knees or hips, may be eligible to have knee or hip replacement surgery. This type of surgery involves making an incision in the knee or hip, removing the old and damaged joint, and replacing it with a new joint made of plastic or metal. Once the new joint is in pace, the incision site is then closed.\nFor patients who are relatively young, a knee replacement is not always an option as the replaced joint can wear down a lot quicker in patients under the age of 55, in comparison to older patients. For these patients, an osteotomy may be performed. This is a surgical procedure performed to remove or add part of a bone in the leg to alleviate pressure put on the knee. This is generally seen as a temporary solution as these patients will usually require knee replacement surgery in the years preceding this procedure.\nGeneral anesthetic (if undergoing surgery).\nThe Osteoarthritis Treatment takes 1 to 3 hours.\nThe procedure duration depends on the type of surgery being performed.\nWhat to expect after Osteoarthritis TreatmentPost procedure care\nShould the patient experience any side effects from taking any medication recommended to them or prescribed by the doctor, they should let the doctor know as the medication may need to be changed.\nImportant things to know about Osteoarthritis TreatmentPotential risks\n- Side effects from medication\nAn X-ray is a procedure which involves taking images of the internal components of the body, in particular the bones.\nA hip replacement is a surgical procedure to replace the surfaces of the hip joint with a prosthetic implant.\nA knee replacement is a surgical procedure in which damaged surfaces in the knee joint are replaced with metal and plastic components.']	['<urn:uuid:20e57fab-85d3-40cd-a5c3-19e94e62c01b>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T20:58:04.490895	13	73	986
24	I'm interested in getting into audiobooks - can you tell me what famous celebrities have narrated books on Audible?	Several celebrities have narrated books for Audible, including Anne Hathaway who read 'The Wonderful Wizard of Oz', Annette Bening who narrated 'Mrs. Dalloway', Bryan Cranston from 'Breaking Bad' who read 'The Things They Carried', and Presidents Barack Obama and Bill Clinton who read their own autobiographies.	"[""NEWARK — It's already dark when workers cluster near a revolving door, tugging at scarves and flicking their thumbs across smart phones.\nA white commuter van pulls into the parking lot at One Washington Place, which sits across Broad Street from the Bears & Eagles Riverfront Stadium. The group files outside. After the driver waits for a straggler to run out, he pulls out and heads for the train station.\nMeanwhile, a handful of people start unrolling yoga mats in the lobby and slowly bend to stretch limbs made stiff by office chairs.\nIn Don Katz’s vision of Newark, at least some of the van passengers would walk a few blocks home to loft apartments. The people in the lobby would carry their yoga mats to studios in the surrounding neighborhood, unfazed by walking alone in Brick City after dark.\nThe founder and CEO of Audible moved his company’s headquarters from Wayne to Newark in 2007. He wasn’t lured to New Jersey’s largest city by tax incentives. Katz wants to be part of a renaissance he believes will pull a once-great American city out of its black hole of dysfunction and deprivation.\n“There was a little bit of controversy with us coming to downtown Newark,” he said. “Let’s face it, there’s the safety [concerns] and reputation it has.”\nFrom the sixteenth floor, Katz is high enough to have an enviable view of Newark and, in the distance, Manhattan.\n“The Emerald City,” he jokes, glancing over his shoulder at the skyline.\nBut he isn’t so high up that he can’t hear the city breathe. Honking horns and sirens get drowned out by a freight train announcing its approach to a crossing.\nThe nearby conference room could have easily been a corner office for the writer who, in 1995, felt so enamored with the idea of making libraries portable that he started a company around it.\nInstead, Katz sits outside the room at an unremarkable desk.\nThe company that moved to Newark with 125 employees is on track to employ 600 people in the city by the end of the year. Aubible never would have gotten this far if it had stuck to its initial premise, said Katz.\nHe originally thought people would store audio books on a device the size of a cassette player.\nIt got scrapped. One of the original players sits in a museum-like case in a hallway at Audible’s office.\n“When you come to market with a digital player seven years before there’s a viable market,” Katz said, “your chances of surviving are just infinitesimal.”\nAudible stuck with its proprietary file format and made it playable on computers and eventually certain MP3 players, which were just starting to trickle into the consumer market by the late 1990s.\nAt the time, devices that stored 2 GB of data sold for $2,000. Now Apple sells an iPod Touch with eight times as much storage for just over $200.\nKatz said Audible owes a lot to the big companies that backed it early on. Microsoft invested $11 million in 1999. The following year Amazon bought a 5 percent stake in the then-publicly traded company.\nIn 2003, two years after the iPod made its debut, Audible inked a deal to become the exclusive audio book provider in Apple’s iTunes Store.\nThen in 2008 Amazon made Audible its subsidiary in a $300 million deal.\nSince then the company’s library has grown to include more than 150,000 titles that can be played on Amazon Kindles as well as computers, tablets and phones that use Apple and Microsoft Windows operating systems.\nGone are the days of lifeless voices on cassette tapes. Now producers encourage actors and actresses to treat reading like a performance, sometimes inventing voices and inflections for dozens of characters.\nReaders travel into Newark daily to narrate books and some have seen it grow from something they do between gigs into a full-time career.\nLast year, Audible introduced the A-List Collection, a category to help members quickly locate new material narrated by celebrities.\nIt includes Anne Hathaway’s reading of L. Frank Baum’s “The Wonderful Wizard of Oz” and Virginia Woolf’s “Mrs. Dalloway” narrated by Annette Bening.\nAside from the collection, other famous voices have brought books to life.\nActor Bryan Cranston of the AMC hit series “Breaking Bad” read Tim O’Brien’s “The Things They Carried.” Presidents Barack Obama and Bill Clinton read their own autobigraphies — “My Life” and “The Audacity of Hope” respectively.\nPhilanthropist and Newark native Ray Chambers, a friend of the Audible CEO, said it was captivating listening to Clinton read his own book.\n“He’s spellbinding when you watch him speak,” he said. “I found him spellbinding just listening.”\nChambers once used an Audible book to drown out the whine of a dentist’s drill, but was too preoccupied with the visit itself to remember the title or narrator.\nIt was a fellow New Jersey Performing Arts Center board member who introduced Chambers to Katz after Audible moved to Newark.\nThe philanthropist was intrigued by the writer-turned-tech entrepreneur.\nSince his early days in the city, Katz has championed the idea that a strong technology infrastructure will draw businesses to Newark. In order for companies to blossom, the city’s schools would have to produce competitive graduates and developers would have to create neighborhoods with amenities to make students and workers feel at home.\n“His idea is probably a situation waiting to happen and perhaps some of his thoughts will be the catalyst to bring that together,” Chambers said.\nIn his quest to learn the city, Katz recently sat down to talk with Newark Public Schools Superintendant Cami Anderson, who came to Newark from New York City, about what two outsiders can bring to the table.\n“When you’re dealing with really big problems, as we are in Newark, I don’t think you can have too many voices at the table,” she said. “We need all sectors and voices to cross the divide.”\nKatz has also swapped ideas with Ron Beit, CEO of RBH Group, whose firm has been the lead developer on the Teachers Village project.\nredesigned mobile site\nhas quick page loads and app-style navigation, and lets you join the conversation with comments and social media. Visit\nfrom any mobile browser.\nBeit said they became fast friends, in part because both men have been doggedly ambitious on behalf of the city. They want to see Newark become a globally recognized hub for transportation, business and culture.\n“Which is a little different from how Newark was,” Beit said. “Back in its heyday, Newark was a successful city regionally but not globally.”\nKatz has been open about what he thinks would convince the workers at his growing technology company to sign a lease or buy a townhouse in Newark. What sells Audible employees on the Brick City will, Beit hopes, attract more companies like it.\n“Don’s key to all of this because he didn’t have to be here. He could have located anywhere he wanted. That really separates those of us who own real estate here, those of us governing the city and those who choose to be here,” he said. “Newark institutions just can’t pick up and walk away. The fact that he wanted to be here gives a lot of weight to his perspective.”\nEditors Note: An earlier version of this article did not distinguish between audio books in the A-List Collection, which are published by Audible, and ones not in the collection either were not included in the collection or were published by other companies.""]"	['<urn:uuid:ce1d7bbd-d9ce-4926-bb18-986774ef6ec4>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:58:04.490895	19	46	1254
25	I'm curious about moon phases - what causes the moon to appear different each night?	The Moon appears different each night because it is lit by the Sun and we see different amounts of its illuminated surface as it orbits Earth. After new moon, we see increasingly more of the lit portion (waxing phases) until full moon, when the entire lit side faces Earth. Then the visible lit portion decreases (waning phases) back to new moon. The Moon takes 27.3 days to complete one orbit around Earth, moving about 13.2 degrees eastward per day.	"[""I am a marketing professional holding a postgraduate degree in management. Astronomy is my hobby.\nAstronomy is the branch of science that deals with celestial objects, space, and the physical universe. The three major celestial objects that have a discernible impact on our lives are the Sun, Moon, and our own Earth. Understanding the basic functioning of Earth, Sun, and the Moon is the first step before studying the distant stars and other complex concepts.\nEarth and Moon are part of a gravitational system where Moon goes around Earth in 27 days. The planets and minor planets are part of a larger gravitational system bound to Sun, known as the solar system.\n7 Basic Facts About Earth, Moon, and Sun\n- Earth's round shape\n- The cycle of day and night\n- The spin of the earth\n- Sun, the source of light\n- The phases of moon\n- Lunar eclipse and\n- Solar eclipse.\n1. Earth’s Round Shape\nEarth is a spinning round ball. There are many pictures of Earth taken from space showing our planet as a round ball. Have you ever wondered why the planets, Moon, and the Sun are round?\nIn large objects such as planets or stars, gravity bounds everything together. If gravity is strong enough, the mass gets uniformly distributed around its center, resulting in a round shape.\nOur planet is spinning on an imaginary axis that runs between Polaris, the North Star, and the South Pole. So the stars near the poles appear stationary when viewed from Earth.\n2. The Cycle of Day and Night\nAt any moment, one half of Earth’s surface receives sunlight while the balance half does not receive any sunlight. It is daytime for the regions receiving sunlight and night for the balance regions.\nWe know day and night keep alternating in a region. This can happen if the ball of Earth is spinning. When it completes half spin, the regions having a day earlier would have night and vice versa. Thus, the Earth’s spin causes the never-ending cycle of day and night. It takes 24 hours for the Earth to complete one spin.\nUSA and India are on either sides of the globe. So when it is midday in New Delhi, it is midnight in New York and vice versa. When one makes a long distance call between the two regions, they can appreciate that Earth is a spinning round ball because of the difference in local time.\n3. The Spin of the Earth\nWe cannot feel the spin just as we do not perceive the speed of a moving train while sitting inside the train. From the window of the moving train, the trees and buildings are seen moving. The trees or the houses are not moving, but seems so when viewed from the moving train. When the Earth is spinning, everything on the sky seems to be moving from East to West.\nIt is not just the Sun that rises in the East. The Moon, the stars and the visible planets rise from the east and spend around 12 hours in the sky before setting on the western horizon. If everything in the sky moves from East to West, it leaves us with two possibilities.\nOne, all the stars, planets, Moon, and the Sun are moving around us. Two, the Earth spins, creating an impression that everything in the sky is going around us. We can conclude that the Earth is spinning based on the moving train example.\n4. Sun, the Source of Light\nSunrise is when darkness is lifted giving way to light. Sunset is when a place plunges into darkness. So we see that Sun is our main source of light, if not the only source.\nSun is a large ball-like object in the middle of the solar system. 8 smaller ball-like objects called planets go around the Sun on elliptical orbits. Earth is the third planet from the Sun in the solar system after Mercury and Venus.\nSun is not burning the way we understand the fires on Earth. The core of the Sun undergoes thermonuclear fusion releasing enormous amounts of energy. The light and the energy thus released keeps our planet warm enough for life forms to thrive.\nOne may wonder why thermonuclear fusion happens in Sun but not in Earth or other planets. The gravitational force of an object increases with its mass. Sun is approximately 333,000 times more massive than the Earth. The immense gravitational force experienced in its core causes nuclear fusion. Any object with 0.08 solar mass or more can support nuclear fusion and become a star. Earth or the other planets are much smaller and so nuclear fusion does not happen on its own.\n5. The Phases of Moon\nWe know that Moon is lit by Sun. We can see the full lit side of Moon when it is opposite to Sun. This is called the full moon day. The dark side of Moon is visible when it is on the same side as Sun. This is called the New Moon day. After the new moon day, the lit portion of Moon keeps enlarging with every passing night until it becomes a full moon.\nGiven below are the terms denoting the various phases of the moon.\n- New Moon - The Moon appears closer to sun in the sky and its dark side faces Earth.\n- Waxing Crescent Moon - The appearance of a thin silver Moon after the New Moon is called the Waxing Crescent Moon.\n- First Quarter Moon - Moon completes one quarter of its orbit around the earth and 50% of its surface is illuminated. Ever wondered why it is called a first quarter moon when half of its surface is lit? Read the earlier line and you may get the answer.\n- Waxing Gibbous Moon - This is the growing phase when over 50% of its surface is lit.\n- Full Moon - The full face of the Moon is lit. Sun and Moon are on either side of Earth for this to happen.\n- Waning Gibbous Moon - This is the phase when the lit portion of Moon reduces in size but over 50% of its surface is lit.\n- Third Quarter Moon - Moon completes 3/4th of its orbit around the Earth and 50% of its surface is lit.\n- Waning Crescent Moon - The thin silver of the Moon appearing a day or two before the New Moon Day.\n6. Lunar Eclipse\nLunar eclipse happens on a full moon day when Earth comes between Sun and the Moon. On most full moon days, Earth, Moon, and the Sun are not aligned in a straight line in the 3-dimensional space. When these three are aligned perfectly in a straight line, Earth casts its shadow on Moon causing the lunar eclipse. A lunar eclipse can be full or partial depending upon the Earth’s relative position.\n7. Solar Eclipse\nThis happens on a new moon day when Moon comes right in front of Sun to block the sunlight. Compared to Sun, the Moon is much smaller, but it is much closer to our planet. The Sun and Moon appear to be nearly of the same size when viewed from Earth. So the Moon can completely block the Sunlight if it comes right in between Earth and Sun.\nIf only a portion of the Moon blocks the Sun, then we get to glimpse a partial solar eclipse. If the full disk of the Sun is covered, it is called a total solar eclipse. For a region on Earth, a solar eclipse happens less often than a lunar eclipse.\n- Mike Shelton. (2007, August 06). Probing Question: Why does the Earth rotate? [Blog post]. Retrieved from https://phys.org/news/2007-08-probing-earth-rotate.html\n- FRASER CAIN. (2008, September 26). Fusion in the Sun [Blog post]. Retrieved from https://www.universetoday.com/18707/fusion-in-the-sun/\n- Surbhi S. (2019, August 16). Difference Between Solar Eclipse and Lunar Eclipse [Blog post]. Retrieved from https://keydifferences.com/difference-between-solar-and-lunar-eclipse.html\n- Larry Sessions. (2016, November 06). Quarter moon or a half moon? [Blog post]. Retrieved from https://earthsky.org/moon-phases/is-it-a-quarter-or-a-half-moon\n- Goyal Brothers Prakashan. (2012, November 18). Formation of Day and Night [Video file]. Retrieved from https://www.youtube.com/watch?v=kXnBVUoF4DY\n- Stile Education. (2018, September 13). What causes the phases of the Moon? [Video file]. Retrieved from https://www.youtube.com/watch?v=YLczDRcd054\n- NASA Video. (2013, May 14). How Does a Lunar Eclipse Work? [Video file]. Retrieved from https://www.youtube.com/watch?v=mbT50-rppaU\n- MonkeySee. (2012, December 20). What Is A Solar Eclipse? [Video file]. Retrieved from https://www.youtube.com/watch?v=_201ttTSG30\nPlease leave your valuable comments.\nMohan Babu (author) from Chennai, India on September 22, 2019:\nThank you Liz for your wonderful comments.\nLiz Westwood from UK on September 22, 2019:\nThis is an extremely well-illustrated and informative article.\nMohan Babu (author) from Chennai, India on September 22, 2019:\nThank you Cheryl. I feel honoured.\nCheryl E Preston from Roanoke on September 21, 2019:\nFascinating astronomy lesson. I sent this article to my grandson who is homeschooled. He will love it as he is duly enjoys astronomy and science."", 'For a discussion of the total lunar eclipse of Sep 27/28, 2015, see here|\nThe Average Motion of the Moon\nThe Moon moves around the Earth in an approximately circular orbit, going once around us in approximately 27.3 days, or one sidereal period of revolution. As it does this its position changes, relative to the stars.\nSince there are 360 degrees in a circle, the Moon moves (on the average) 360 / 27.3 or 13.2 degrees per day relative to the stars, which is just over half a degree per hour, and approximately equal to its apparent size. This means that from night to night the Moon moves a little more than one hand-width to the East (the direction of its motion around the Earth) relative to the stars, and from hour to hour it moves about one diameter to the East, among the stars.\nAn approximate representation of the motion of the Moon around the Earth. Moving once around in 27.3 days, its average movement is about 13.2 degrees per day, or 92 degrees per week. (As is usual in such diagrams the sizes of the Earth and Moon are exagerrated, in comparison to their separation.)\nThe apparent motion of the Moon from night to night.\nEach night, it moves about 13 degrees, or about 26 diameters, to the east.\nThe apparent motion of the Moon from hour to hour; each hour it moves about one diameter to the East.\nAbove: The crescent Moon, Venus, Jupiter and Spica as seen near Quebec on September 6, 2005\nBelow: The crescent Moon, Venus, and Jupiter as seen near Los Angeles on September 6, 2005\n(Image Credits: Quebec, Jay Ouellet, apod050909; Los Angeles, Sheri Seligman)\nAlthough these pictures were taken at about the same time of evening, the lower picture, taken three time zones to the west, was taken nearly three hours later, so the Moon\'s position is shifted three diameters to the left as a result of its orbital motion during that period. (There is also a slight difference in the alignment of the objects because the diurnal paths of celestial bodies are more horizontal at the more northern latitude of Quebec than at the more southern latitude of Los Angeles.)\nThe Moon\'s Westward Motion Across the Sky\nAlthough the Moon is moving eastward around the Earth, the Earth is also turning to the east and much faster, for it goes all the way around its axis of rotation in just under a day. As a result, although the Moon is moving to the east relative to the stars, the much faster westward motion of the sky is carrying it to the west, so despite its eastward motion relative to the center of the Earth, it rises in the east and sets in the west, just like any other celestial body.\nThe Moon\'s eastward motion is much slower than the sky\'s westward motion. So though moving to the east from day to day, it still has a net motion toward the west each day. This means that it still rises in the east and sets in the west like the stars, but a little later each day\nThe stars go once around the sky in 23 hours 56 minutes (approximately), so the Moon, moving more slowly to the west, takes longer than this. Since its eastward motion averages 13.2 degrees per day and the Earth takes 4 minutes to rotate through one degree, it takes about 53 minutes (13.2 times 4) for the Earth to rotate through this extra angle; which means that on the average the Moon crosses the sky once every 24 hours and 49 minutes (53 minutes longer than the stellar ""day""). As a result, it rises (and sets) later and later every day, until after about 27 days, when it has gone once around the sky relative to the stars, it is back in its original position, rising and setting at its original time(s).\nThe Moon\'s Variation in Distance and Speed\nAlthough the Moon has an average motion of 13.2 degrees per day, this motion varies for two reasons. First, the orbit of the Moon is an ellipse and is not centered on the center of the Earth, but on a point about 12000 miles from the center of the Earth. As a result, during each orbit the Moon\'s distance varies by twice that 12000 miles. During half its orbit it is approaching us, and during the other half it is moving away from us. During the half orbit that it is approaching us, our mutual gravitational pull accelerates the Moon, causing it to move faster and faster, until at the closest point in the orbit, or perigee\n, it is moving about 6% faster than its average motion. Similarly, during the half orbit that it is receding from us, our mutual gravitational pull decelerates the Moon, causing it to move slower and slower, until at the furthest point in the orbit, or apogee\n, it is moving about 6% slower than its average motion. In addition to these actual changes in velocity, there is an apparent change caused simply by its being nearer or further; when it is closer any motion that it has looks faster in angular terms than when it is further away. This effect causes another 6% apparent\nincrease or decrease in velocity, in addition to the actual change.\nIn other words, as the Moon approaches perigee its angular speed among the stars will appear to increase by about 12% of its average speed, half of that change being due to its lesser distance, and half being due to an actual increase in speed; and as it approaches apogee, its angular speed among the stars will appear to decrease by about 12% of its average speed, half of that change being due to its greater distance, and half being due to an actual decrease in speed. Since 12% of 13.2 degrees per day is 1.6 degrees per day, the daily motion of the Moon to the east can vary from as little as 11.6 degrees per day near apogee to as much as 14.8 degrees per day near perigee.\nAn image showing the apparent size of the Moon at apogee (on the left), and at perigee (on the right). (The change in size is not so obvious when the time between the extremes is two weeks, as it is when images are placed side by side.) When at apogee the Moon will appear to move less than 12 degrees per day to the East among the stars, whereas at perigee it will appear to move nearly 15 degrees per day. (António Cidadão, apod041021)\nOther topics to be covered in the next iteration of this page, or on separate pages:\n(need to discuss rate of motion, rising/setting later each day (can leave effects due to N/S motion until later on, or perhaps only cover in more detailed pages intended for lab students), effects of eccentricity, and precessional motions of orbital and plane and line of nodes -- any other topics?)\n(below, rough lecture notes from Fall 2004; will be added to above, moved to another page, or deleted in the next iteration of this page)\nThe Moon moves one diameter, or half a degree per hour, or about 26 diameters or 13 degrees per day (27 1/3 days, its orbital period, divided into 360 degrees, or once around). (Refer to the page on Tycho Brahe\'s astronomical accomplishments, for a more detailed discussion of the parallax of the Moon?)\nTHE SUN FOLLOWS ALMOST EXACTLY THE SAME PATH, but only moves two diameters (again, about half a degree per diameter, or one degree) per day (365 1/4 days, our orbital period, divided into 360 degrees -- which is probably about the same number, specifically because of our orbital period).\nIt is easier to see the motion of the Moon than of the Sun for two reasons -- it is much faster and you can see stars when the Moon is near them, but NOT when the Sun is near them. However, it is possible, by measuring the right ascension and declination of the Sun, to see that it does follow almost exactly the same path as the Moon, but much more slowly.\n(Other planets follow similar paths, but have more complicated motions, involving retrograde loops and esses. This is because the motion of the Moon only involves one motion -- its own; and the motion of the Sun only involves one motion -- ours, around the Sun; but the motion of another planet involves two motions -- its and our motion around the Sun (refer discussion of retrograde motion, in the book, and on the web site).']"	['<urn:uuid:29fef8b5-aa4e-4fc7-868b-5b19a0459ba6>', '<urn:uuid:44e76c9e-20e2-44ce-be00-fa220260279a>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	novice	2025-05-12T20:58:04.490895	15	79	2949
26	mckinseys framework vs fault tree reliability	McKinsey's 7S framework and fault tree analysis differ in their approach to organizational analysis. McKinsey's framework divides elements into hard (Strategy, Structure, Systems) and soft (Shared values, Skills, Style, Staff) components to measure efficiency, particularly in marketing efforts. In contrast, fault tree analysis, developed in the 60s, focuses specifically on reliability and safety measures in engineering disciplines, using logic symbols called 'gates' to connect segments leading to incidents or foreseen events. It is particularly used for analyzing complex mechanical and structural systems, human failures, or redundant arrangements.	"[""What's new in Polymer: Mar 29, 2023\nSecondary axes, Google Ads connector improvements, & more\nA gap analysis can show you where you are, where you want to be, and what to do to breach the gap between the two. Here's how to conduct a gap analysis, step by step.\nLearning how to conduct a gap analysis is vital to evaluate your company’s current state and help you ensure you’re going where you need to go.\nThat’s why, here, we’ll tell you everything you need to know about gap analyses, what they’re for, and how to conduct them step by step. We’ll also show you the tools and frameworks to get your gap analysis started.\nLet’s dive right in.\nA gap analysis is an audit that allows you to assess the current performance of your business, and compare it to an ideal performance. If needed it can also show the goals needed to bridge the gap between both of them, getting your performance as efficient as possible.\nA gap analysis is usually conducted by the heads of the departments involved, using different frameworks according to their industries, goals, and needs.\nGap analyses are used for analyzing the current state of a business, so they work great at providing a view of the current state, and establishing a road towards achieving your business goals.\nWith a gap analysis, you can get an external view of your processes, finding and understanding the roadblocks on your way towards your goals and quotas. With them, it can be easy to prioritize urgent tasks, better understand your problems, and help you find the right solutions.\nFinally, a gap analysis can not only show you how to reach your goals but anticipate the potential issues you may encounter down the line.\nHere’s how to conduct a gap analysis, step by step:\nThe first thing you need to do is to establish the current state of the business or process you’re analyzing.\nThe right way of determining your current state will depend on your measurements of success for each of your processes. You can, for example, establish the current state of your sales department as the average of both how much they sell per period and how often they hit their sales quota.\nYour current state can also be the level of engagement you have on social media, or your growth percentage in a new sector.\nMake sure to measure and establish a “current state” for all processes involved within the scope of your gap analysis.\nNow that you’ve defined where you are, it’s time to define where you want to be. If everything worked out perfectly, where would you want to be?\nThis is the part of the analysis where you get to be idealistic and imagine the ideal state of things, defining what you’re working towards. You can also define what you’d like to have or not have, the issues you’d like to have solved, and the numbers you’d like to be seeing.\nDefine it in detail, measuring the same way as you defined your current state.\nNow that you know where you are and you’ve defined where you want to be, we can define the gap in between to figure out how to bridge it.\nStart by analyzing your current state, determining the obstacles preventing your ideal state from becoming a reality. Some of these may be obvious, like the lack of staff or the overhead costs. Some, like the lack of training or an issue in your sales funnel, may be a bit harder to decipher.\nYou can also set a goal defining when you’d like to achieve your ideal state, and break down the progress you’d need to make month to month and year to year. If the goals aren’t realistically achievable, check why. Is there a bigger obstacle on your way or is your timeline just too short?\nKeep defining the goals you need to set to bridge the gaps between your ideal state and your current state until you have a realistic view of the issue.\nFinally, once you have a better understanding of where your gap is and why it’s there, you can start developing a plan to bridge it.\nThis process will likely involve the heads of each department affected, ensuring everyone is working together towards bridging that gap. Make sure your plan is composed of achievable goals with clear measurements of success, so the plan is clear down to every team member.\nNow, all that’s left is to implement it, and keep track of your goals every period to achieve your ideal state!\nHere are some common gap analysis frameworks to get you started:\nSWOT is a great framework to use when figuring out where your business stands within the market.\nThis framework focuses on defining your Strengths, Weaknesses, Opportunities, and Threats. Since determining these accurately is vital for the success of the analysis, you may want to invite experts from all departments involved to help define them. This will make for a data-driven framework that can better define the strengths and weaknesses of your business.\nThe Nadler-Tushman model is a dynamic framework made around defining the relationships between processes and finding all gaps involved with their efficiency. The framework does this by looking at all processes from start, or “input”, to finish, or “output”. Then, the framework takes a look at the culture, structure, people, and work, to determine how they relate to each other within one process.\nThis holistic view makes it a great framework to use when trying to align all departments and components of your company to work together towards a single goal.\nSource: Visual Paradigm\nThe McKinsey framework is made up of the 7 “S”s: Strategy, Structure, Systems, Shared values, Skills, Style, and Staff. The framework then divides those into two groups: the hard elements, composed of the first three “S”s, are tangible, measurable, and controllable elements. The soft elements, made up of the rest, are intangible and uncontrollable events.\nThis approach is especially useful when measuring the efficiency of your marketing efforts, and determining if a new route needs to be taken.\nFinally, the fishbone diagram is a simple cause-and-effect diagram made to get to the root of an issue by exploring and categorizing potential candidates. The diagram starts with an issue, or “effect”, and works backward dividing issues into six different sources: Methods, Machines, People, Materials, Measurement, and Environment.\nThis diagram, also called the Ishikawa diagram, is highly useful in brainstorming environments, providing a simple framework to organize ideas and visualize potential causes of an issue.\nAs we can see, conducting a gap analysis can help you figure out where your company stands, where it wants to go, and how to get there.\nOnce you’re conducting one, don’t spend too long adding on details. Instead, use your gap analysis to make data-driven decisions and develop an action plan to bridge that gap!\nSecondary axes, Google Ads connector improvements, & more\nAs artificial intelligence technology continues to advance, language models like ChatGPT are pushing the boundaries of what's possible.\nSee for yourself how fast and easy it is to create visualizations, build dashboards, and unmask valuable insights in your data.Start for free"", 'The Basics of Root Cause Analysis (RCA)\nIn determining which RCA technique to use, it should be clear that there are no universal standards on which to base this decision. Performing an analysis of causal factors requires the investigation of available data or resources that only form part of a whole picture. The whole picture refers to an event that brought about undesired results or outcome, which is generally known as “the problem”; hence, the technique to use will depend on the degree of the problem and the quality of data to analyze.\nNevertheless, each RCA method has its own characteristics as far as approach is concerned but all are basically designed to uncover the main source of the causal factors leading to an undesirable event. This therefore, could serve as a guide for determining, which method is most practical to use.\nThe Cause and Effect (Fishbone Diagram) Method\nOne of the most popular, although not necessarily the most recommended, techniques is “the cause and effect” analysis using the fishbone diagram or Ishikawa method. This can be used at the beginning of the analysis stage where the problem has been well-defined and the circumstances or conditions to explore are not exceedingly broad.\n1. A fishbone diagram is commonly used for:\nExploring a pre-defined cause without the need to follow specific areas to investigate and make use of suppositions and data that conform to the storyline and timeline of an event.\nConducting an analysis that does not require a high degree of training and experience for its practitioner.\nPooling of ideas that involves team member participation in order to explore all levels of exposure to a cause being investigated.\nOrganizing information and establishing their links to the causes that led to an undesirable event.\nAllowing hypothetical ideas to enter the scenario in order to gain as much insights to probable and probable causes..\n2. The fishbone diagram has been commonly used by service and manufacturing industries in relation to:\nThe analysis of human responses and behaviors for which the objective is to deal with the recurrence of situational problems. Underlying causes are mostly answers to questions as to “what happened?” and “why it happened?”\nThe review of policy compliance and effectiveness — usually required by regulators or the customers’ demand for certification as a way of ensuring quality based on standards. The aim is to implement real solutions especially if the workplace culture is a large contributing factor. Questions as to why there is lack of compliance, what department is involved, as well as probing when, how and why does it happen? In this aspect however, the answers should not be hypothetical but based on actual events in order to come up with workable solutions that will not hamper the workflow processes.\nThe assessment of procedural efficiency and adherence, as part of day-to-day activities especially if it impacts the business beyond the tolerable level. Workplace accidents and violation of government laws are the events that require preventive solutions against occurrence or recurrence. Another aspect is the desire to continuously improve a product in order to achieve an edge over the competitors’ offers. Both aspects are broad and the usefulness of the fishbone diagram in exploring all possible causes and sub-causes are best served if the data are logically categorized.\nThe management of assets in order to determine adequacy, suitability and relevance of available facilities, tools, equipment and their technologies. Areas for analysis are the assets that destabilize the efficiency of the cash flow due to the occurrence of breakdowns, accidents, major repairs or capital acquisitions. What would be the effect of a major capital investment? Will it increase productivity and in what way? Can the cost be recovered by the increased productivity? If so, by how much? Will it entail removing some of the workforce? In doing so, what would be the potential consequences? Is the company prepared to meet the additional costs?\n- The methods and materials used to produce or to render the products or services. In light of the present regulations for environmental compliance, certain green materials and methods can impact the quality of the product, its shelf-life, its marketability and affordability for the customer. Green technologies and materials tend to be capital intensive but are said to be recoverable through their cost-cutting efficiency. However, since the change of methodology and materials could affect the quality and price of the product, the possible effects should be clearly defined by extensively exploring all the risks that have to be addressed.\nKeep it in mind that the “cause and effect” method using the fishbone diagram allows hypothetical statements. Thus, the core reason established as root cause may call for further confirmation by conducting supplementary observations or sending out surveys.\nHowever, a reverse method of performing the “cause and analysis” technique, particularly if it involves a large amount of information, is to send-out surveys as the initial step. Analyze the data gathered using the Pareto chart or the 80-20 principle. After which, the resulting 20 percent would be subject to further analysis by determining their causes and related effects.\nSome are inclined to use this root cause analysis technique because of its simplicity. Be in the know however, that it is also faulted for its limited capability to explore other areas.\nNonetheless, the 5-why method was popularized by the Toyota Company because they have found it effective, largely because of the ease by which it can be adopted.\nIf by the fifth question or thereabouts, no more why questions are prompted, the last answer is deemed as the root cause. It is appreciated in this aspect, since the root cause could be established without the need for data segregation and statistical analysis.\nOn the other hand, if by the fifth question, the answer derived leads to faulty reasoning or is not as intuitive as expected, it is suggested that another rood cause analysis technique be applied.\nThe 5-why method is supposed to follow a linear-progression of logical reasoning, and the process works by commencing with the outcome and continues by tracing back the sequence of actual events down to its origination. Still, the process can still go into details and proceed using more than five why questions for as long as the sub-causes reveal relevant information.\nThis RCA technique is widely used in the health care industry where patients have concerns or complaints that need to be immediately addressed. The typical frameworks it can effectively broach are the day-to-day service activities related to:\n1. Improvements due to loss of quality or regression of production capacity, usually as an approach to lean manufacturing. However, the analysis has to start with the initial phase of data gathering via a Pareto chart. Take note that the data should be based on facts since the aim is to arrive at a quick-solution. Other aspects of lean manufacturing to which this root cause analysis technique applies are: (1) breakdown or inefficiency of equipment, (2) time loss, and (3) waste reduction toward zero-waste attainment.\n2. Investigate incidents that led to serious injuries or accidents. The analysis makes use of the known 4-P’s of information, namely people, paper, parts and position as gathered from separate interviews and available resources. Here, questions asked are still open-ended in order to collect the most confirmed version of the accident. “Why” questions are asked last.\n3. To assess environmental incidents in order to formulate counter-measures, such as groundwater or soil contamination, indoor or outdoor air emissions and like-incidents that can severely impact the environment.\nOnce the root cause is established and the solution or counter-measure is put in place, it is always the best practice to test whether the latter’s application reveals positive results.\nFault Tree Analysis\nThe fault tree analysis was originally conceptualized in the 60s as part of the reliability and safety measures of engineering disciplines. The graphical presentation starts with the known hazards or failures of a system and logic symbols called “gates” connect each segment leading to the incident or to the foreseen events.\nIn manufacturing industries, it is mostly recommended for large and complex systems of mechanical and structural components—to analyze human failures or to determine redundant arrangements.\nTypical hazard or mechanical failure problems investigated using this root cause analysis technique are:\n- Major loss of production\n- Failure to complete a mission\n- Toxic emissions\n- Unexplained explosion\n- Failure of safety mechanisms\nIn most cases, human error or oversight in operating, controlling or monitoring is regarded as the cause. This is in view of the safety and reliability features integrated as part of the engineering design, and the existence of automatic warning signals or indicators that a certain component is not functioning properly. However, there are those who contest this theory—particularly in the airline industry,\nThus, the fault tree method was further developed into a root cause analysis technique using sophisticated software programs. There after, other industries like automotive, rail transport and chemical manufacturing companies came to regard this RCA method as essential to their risk management procedures.\nAt the Root of the Root Cause Analysis\nIn choosing which root cause analysis technique to use, knowledge of their basic characteristics, capabilities and data requirements serve as a guide for making informed decisions. There are other methods and tools available, and used in analyzing the underlying causes of problems or undesired events. The discussions for the three methods featured above are supplementary to the overviews provided in a separate article entitled Overview of Different Root Cause Analysis Methods.\n- HCIPproject.com- USAID Health Care Improvement Project –Cause and Effect Analysis lifted from https://www.hciproject.org/improvement_tools/improvement_methods/analytical_tools/cause_effect_analysis\n- ISixSigma.com - Determine the Root Cause: 5 Whys lifted from https://www.isixsigma.com/index.php?option=com_k2&view=item&id=1308:determine-the-root-cause-5-whys&Itemid=200\n- FaultTree.org - Fault Tree Analysis lifted from https://www.faulttree.org/\nImage Credits: Accidents by Mikael Häggström for Wikimedia Commons; Fishbone by Jake Choules for Wikimedia Commons; Cause and effect diagram for defect by DanielPenfield; Screenshot image of the 5-Why Cause Map Diagram was created by the author; Example of High Level Fault Tree from NASA Software Safety Guidebook']"	['<urn:uuid:c7e028c4-91f6-49c5-9285-744ed4e28e38>', '<urn:uuid:fc075bd9-8b7b-4bf8-996e-55336affb8fd>']	factoid	direct	short-search-query	distant-from-document	comparison	expert	2025-05-12T20:58:04.490895	6	87	2866
27	As a healthcare provider focusing on comprehensive patient care, I'd like to understand how electronic health records help doctors see the complete health picture of their patients. Can you explain the benefits?	Electronic health records allow medical practitioners to better understand the complete health picture of their patients by providing comprehensive access to medical histories and conditions. When a patient visits any specialist, whether a dentist or an oncologist, these practitioners can familiarize themselves with the patient's complete medical background. This access to historical medical data enables healthcare experts to identify health patterns and better appreciate the overall context of the patient's health status. The system allows medical practitioners in different regions or categories to easily share patient data, making treatment processes much more efficient and providing essential medical histories without significant effort.	"[""Medical practices in Evergreen Park have improved over the last few decades as a result of two chief factors namely research and formalisation of the whole industry. Health records take the centre stage in the effort to formalise the practise because they provide a reliable foundation for the treatment of patients over time. Such records are not merely limited to the patient’s history but rather to all the recorded medical data that can show trends or patterns of health issues in different people over long periods.\nElectronic health records have therefore become popular in modern societies. Just about all industries are increasingly embracing digitisation in all processes. The medical industry has not been left behind in this. Medical records have also been adapted to electronic means of collection and storage which defines the concept of electronic medical records.\nThe benefits of having electronic health records make the whole change worthwhile. Medical practitioners in different regions or categories may now easily share patient data. This, therefore, makes treatment processes much more efficient and provides essential medical histories for patients without a lot of effort. There is also no denying the convenience of electronic records over physical ones besides the obvious ones such as storage ease and the ease of retrieving the data on demand.\nEMR Software Solution for Medical Records Management in Illinois\nElectronic health records in Evergreen Park IL have evolved the way that medical practitioners operate in terms of collecting and storing patients’ data. All modern health facilities have jumped on the bandwagon of adopting this trend. Let’s just quickly evaluate the key advantages that come with the use of electronic health record systems.\n- Seeing The Bigger Picture\nHealth records provide a history of each patient’s medical issues and conditions. This, therefore, allows medical practitioners to familiarise themselves with the patient whether it’s on a visit to a dentist or an oncologist. By familiarising themselves with the client’s medical history, health experts can better appreciate the bigger picture of the patient’s health and identify any health patterns.\n- Efficient Record Updates and Retrievals\nThe main agenda of any health record system is to provide a reliable source of patient details. Electronic records therefore fully satisfy this need in the industry. Patent details may even be inputted into the system directly from the medical equipment like BP meters or thermometers. EHRs also eliminate lots of problems such as parallax errors which come with written records hence improving the integrity of the records. Finally, the time taken to retrieve electronic records is negligible and therefore makes treatment processes much more efficient in Evergreen Park.\n- Better Record Security\nDigital means of data storage and collection are better at providing data security than physical records. The possibility of destruction is as good as zero since copies can be created on several devices such as hard drives. Data can also be uploaded to offsite organisations or to the web where it’s secured even when the hardware is destroyed. Electronic records are also more secure from manipulations and alterations than physical records. Digital information is easily encrypted and electronic devices can be programmed to alert the relevant guardians of the data whenever attempts are made to change such records in any way.\n- Less Record-Keeping Costs\nThe traditional means of keeping records involved tedious filing along with a lot of infrastructures to keep the records and maintain room conditions at ideal levels that ensured the integrity of the records on paper. To top it off, the arduous processes of managing all these records required plenty of manpower which all translated to higher costs of creating and maintaining the records. Electronic records, however, demand very little financial support. A single computer with a large hard drive can do the job with more ease and efficiency.\nAdvantages of Electronic Health Record System in Evergreen Park\nThe management of medical records has a very strong bearing on the progress of any treatment procedure or program for the patient in question. This is true for just about every medical branch from dermatology to dentistry. It is therefore critical to employ special software that can effectively manage patient information for the convenience of all medical processes.\nEMR software in Illinois provides an ultimate solution for only about all the challenges that come with the physical means of medical records management. For instance, they significantly reduce the time period involved in the creation of the records and the retrieval of those same documents for use or updating.\nThe convenience of EMR software also permits flawless integration of all patient information from multiple medical practitioners which keeps them updated. Such software is also critical for tracking patients’ health by identifying abnormal trends and features within the information recorded. Both the medical institutes and the patients that they care for benefit immensely from record management with EMR software.No other alternative management compares to this.\nPrevious attempts to entrust medical records with each patient have failed dismally in Evergreen Park IL 60805. This cannot be blamed on the patient, human error is common after all and any records in the care of patients are subject to losses or distortions which ultimately makes them unreliable. The use of EMR software, therefore, eliminates any such possibilities and ensures the continued integrity of medical records.\nEvergreen Park, Illinois\nAs early as 1828, a German farming family had settled in the area of what is now Evergreen Park. In the succeeding decades, other German immigrants arrived. Kedzie Avenue and 95th Street crisscrossed the farmland and provided access to markets.\nThe first railroad (now the Grand Trunk Railroad) came through the area in 1873. In 1875, the community built its first school just west of 95th and Kedzie. The school and the stores that began to cluster around this intersection defined the community's main business area. Nearby, a real-estate developer, with a vision of the Arc de Triomphe area of Paris, laid out a star-shaped park with eight streets radiating from it. The evergreen trees planted in the park inspired the village's name. The location and layout of the park was intended to be the center of town, but 95th St and Kedzie Ave. later proved a more accurate midpoint. After the death of Mayor Henry Klein shortly after the village's 75th anniversary, the park was renamed Klein Park in his honor.\nIn 1888 St. Mary's Cemetery opened, and mourners traveled by train from Chicago. Restaurants and taverns were created to provide meals for cemetery visitors. Within five years, the village had become a recreation center that attracted hundreds of Chicagoans to its picnic groves, beer gardens, and dance halls. The first of the village's 13 churches was established in 1893.\nOther Cities Around Evergreen Park 60805Evergreen Park""]"	['<urn:uuid:b856a638-fac4-460e-ba01-dc05987e70f8>']	open-ended	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:58:04.490895	32	101	1121
28	understand ancient libraries collection access how ulpia library architecture secured scrolls and how palimpsests helped preserve classical texts through reuse	The Ulpia library used bronze screens to restrict access and protect its collection. The scrolls were stored in armaria recessed in the wall and protected by wooden bookcases from moisture. As for text preservation through reuse, palimpsests were created by scraping or washing off text from parchment to reuse it. This practice actually helped preserve some ancient works that would have otherwise been lost, as the faint remains of the former writing could later be deciphered by scholars.	"['Return to Forum of Trajan\n""He also built libraries. And he set up in the forum an enormous column...""\nCassius Dio, Roman History (LXVIII.16.3)\nAfter Alexandria and Pergamum, the Bibliotheca Ulpia (as it is called by the Historia Augusta) was the most famous library of antiquity and, of all the Roman libraries, the only one to have survived at least until the mid-fifth century AD, when it is mentioned by Sidonius Apollonius, whose statue was installed there (Letters, IX.16.3) by his father-in-law, the emperor Avitus, whom he had honored with a panegyric. Venantius Fortunatus also writes of ""Vergil recited in Trajan\'s forum in the city"" (VI.8) about AD 576. Following the tradition of earlier imperial libraries, the Greek and Latin collections were housed separately. In the Ulpian library, they faced one another across a a small colonnaded courtyard that enclosed the Column of Trajan.\nIn this computer reconstruction of the west (Greek) library by John Burge (in consultation with James Packer and Kevin Sarring), one looks through bronze screens into the portico, where the base of the Column of Trajan can be seen. Facing east, as Vitruvius recommends for libraries (VI.4), the screens restricted access when the library was not in use and, with the high vaulted ceiling, took advantage of the morning light.\nPortions of the floor and the podium of one of the walls survive, which, together with fragments in storerooms beneath the Via dei Fori Imperiali, allow reconstruction of the library interior. By multiplying the diameter of columns and adding the height of the entablature (fragments of the egg-and-dental cornice survive, as well), Packer and his colleagues were able to calculate that the upper tier of the library was three-quarters as tall as the lower one, the ratio prescribed by Vitruvius for the columns of basilicas (V.1).\nAs can be seen in the reconstruction, the walls were broken into bays by fluted Corinthian columns set opposite pilasters that framed niches which held the cabinets for the books. Three steps between the columns allowed access to a walkway in front of the bookcases, themselves. At the other end of the hall were recesses for a statue on each level, presumably of Trajan and possibly of Minerva.\nThe sumptuous room was paved in gray Egyptian granite separated by strips of golden purple-veined giallo antico from Numibia in North Africa. The walls were covered with a veneer of pavonazzetto marble (pavo, peacock, because of the contrasting purple veins) from Phrygia in western Asia Minor. The columns, too, were of the same variegated marble, whereas the niches holding the scrolls were framed in white marble, as were the capitals and bases of the columns.\nThe scrolls, themselves, were stored in armaria recessed in the wall, protected in wooden bookcases from the damp. There were seven niches on each side of both walls and four across the back, two on each level, flanking the statues. It has been estimated that these thirty-six cabinets held approximately ten thousand scrolls, with a like number in the east (Latin) library. In addition, there were archival materials, such as praetorian edicts (Aulus Gellius, XI.17.1) and senatorial decrees (Augustan History, Tacitus, VIII.1), as well as Caesar\'s autobiography and Trajan\'s commentaries on the Dacian Wars, of which now only a few words survive. With desks (plutei) and the books, themselves, on shelves out of sight, the space was designed for reading. But, although aesthetically pleasing, there also was the risk of a lack of space as the collection grew. Indeed, the Historia Augusta (Aurelian, 1.7; Probus, 2.1) indicates that the linen books were transferred to the Baths of Diocletian sometime in the fourth century, a time, laments Ammianus (XIV, 6.18), when ""the libraries are like tombs, permanently shut."" Although these linen books likely are fictitious, as are the ""books of ivory"" mentioned in the Life of Tacitus (VIII.1), the author does relate that he was able to procure books in Greek from the Ulpian library and ""laid my hands on all that I needed"" (Aurelian, I.9).\nIsidore of Seville notes that architects used green Carystean marble (cipollino verde) to panel libraries because the color was thought to refresh the eyes (Etymologies, VI.11.2).\nReference: The Forum of Trajan in Rome: A Study of the Monuments in Brief (2001) by James E. Packer; Libraries in the Ancient World (2001) by Lionel Casson; Scriptores Historiae Augustae (1921-) translated by David Magie (Loeb Classical Library); Ammianus Marcellinus: The Later Roman Empire (1986) translated by Walter Hamilton (Penguin Classics); Venantius Fortunatus: Personal and Political Poems (1995) translated by Judith George.\nReturn to Top of Page', '|This article needs additional citations for verification. (August 2014) (Learn how and when to remove this template message)|\nA palimpsest (//) is a manuscript page, either from a scroll or a book, from which the text has been scraped or washed off so that the page can be reused for another document. Pergamene (now known as Parchment) was made of baby lamb or kid skin (best made in ancient Pergamos) and was expensive and not readily available, so, in the interest of economy a pergamene often was re-used by scraping the previous writing. In colloquial usage, the term palimpsest is also used in architecture, archaeology, and geomorphology, to denote an object made or worked upon for one purpose and later reused for another, for example a monumental brass the reverse blank side of which has been re-engraved.\nThe word ""palimpsest"" derives from the Latin palimpsestus, which derives from the Ancient Greek παλίμψηστος (palímpsēstos, ""again scraped""),a compound word that literary means ""scraped clean and ready to be used again"". The Ancient Greeks used wax-coated tablets, like scratch-pads, to write on with a stylus, and to erase the writing by smoothing the wax surface and write again; this practice was adopted by Ancient Romans, who wrote (literally scratched on letters) on wax-coated tablets, which were reuseable; Cicero\'s use of the term ""palimpsest"" confirms such a practice.\nBecause parchment prepared from animal hides is far more durable than paper or papyrus, most palimpsests known to modern scholars are parchment, which rose in popularity in Western Europe after the 6th century. Where papyrus was in common use, reuse of writing media was less common because papyrus was cheaper and more expendable than costly parchment. Some papyrus palimpsests do survive, and Romans referred to this custom of washing papyrus.\nThe writing was washed from parchment or vellum using milk and oat bran. With the passing of time, the faint remains of the former writing would reappear enough so that scholars can discern the text (called the scriptio inferior, the ""underwriting"") and decipher it. In the later Middle Ages the surface of the vellum was usually scraped away with powdered pumice, irretrievably losing the writing, hence the most valuable palimpsests are those that were overwritten in the early Middle Ages.\nMedieval codices are constructed in ""gathers"" which are folded (compare ""folio"", ""leaf, page"" ablative case of Latin folium), then stacked together like a newspaper and sewn together at the fold. Prepared parchment sheets retained their original central fold, so each was ordinarily cut in half, making a quarto volume of the original folio, with the overwritten text running perpendicular to the effaced text.\nFaint legible remains were read by eye before 20th-century techniques helped make lost texts readable. To read palimpsests, scholars of the 19th century used chemical means that were sometimes very destructive, using tincture of gall or, later, ammonium bisulfate. Modern methods of reading palimpsests using ultraviolet light and photography are less damaging.\nInnovative digitized images aid scholars in deciphering unreadable palimpsests. Superexposed photographs exposed in various light spectra, a technique called ""multispectral filming"", can increase the contrast of faded ink on parchment that is too indistinct to be read by eye in normal light. For example, multispectral imaging undertaken by researchers at the Rochester Institute of Technology and Johns Hopkins University recovered much of the undertext (estimated to be more than 80%) from the Archimedes Palimpsest. At the Walters Art Museum where the palimpsest is now conserved, the project has focused on experimental techniques to retrieve the remaining text, some of which was obscured by overpainted icons. One of the most successful techniques for reading through the paint proved to be X-ray fluorescence imaging, through which the iron in the ink is revealed. A team of imaging scientists and scholars from the USA and Europe is currently using spectral imaging techniques developed for imaging the Archimedes Palimpsest to study more than one hundred palimpsests in the library of Saint Catherine\'s Monastery in the Sinai Peninsula in Egypt.\nA number of ancient works have survived only as palimpsests. Vellum manuscripts were over-written on purpose mostly due to the dearth or cost of the material. In the case of Greek manuscripts, the consumption of old codices for the sake of the material was so great that a synodal decree of the year 691 forbade the destruction of manuscripts of the Scriptures or the church fathers, except for imperfect or injured volumes. Such a decree put added pressure on retrieving the vellum on which secular manuscripts were written. The decline of the vellum trade with the introduction of paper exacerbated the scarcity, increasing pressure to reuse material.\nCultural considerations also motivated the creation of palimpsests. The demand for new texts might outstrip the availability of parchment in some centers, yet the existence of cleaned parchment that was never overwritten suggests that there was also a spiritual motivation, to sanctify pagan text by overlaying it with the word of God, somewhat as pagan sites were overlaid with Christian churches to hallow pagan ground. Or the pagan texts may have merely appeared irrelevant.\nTexts most susceptible to being overwritten included obsolete legal and liturgical ones, sometimes of intense interest to the historian. Early Latin translations of Scripture were rendered obsolete by Jerome\'s Vulgate. Texts might be in foreign languages or written in unfamiliar scripts that had become illegible over time. The codices themselves might be already damaged or incomplete. Heretical texts were dangerous to harbor – there were compelling political and religious reasons to destroy texts viewed as heresy, and to reuse the media was less wasteful than simply to burn the books.\nVast destruction of the broad quartos of the early centuries took place in the period which followed the fall of the Western Roman Empire, but palimpsests were also created as new texts were required during the Carolingian Renaissance. The most valuable Latin palimpsests are found in the codices which were remade from the early large folios in the 7th to the 9th centuries. It has been noticed that no entire work is generally found in any instance in the original text of a palimpsest, but that portions of many works have been taken to make up a single volume. An exception is the Archimedes palimpsest (see below). On the whole, Early Medieval scribes were thus not indiscriminate in supplying themselves with material from any old volumes that happened to be at hand.\n- The best-known palimpsest in the legal world was discovered in 1816 by Niebuhr and Savigny in the library of Verona cathedral. Underneath letters by St. Jerome and Gennadius was the almost complete text of the Institutes of Gaius, probably the first student\'s textbook on Roman law.\n- The Codex Ephraemi Rescriptus, Bibliothèque Nationale de France, Paris: portions of the Old and New Testaments in Greek, attributed to the 5th century, are covered with works of Ephraem the Syrian in a hand of the 12th century.\n- The Sana\'a palimpsest is one of the oldest Qur\'anic manuscripts in existence. Carbon dating indicates that the undertext (the scriptio inferior) was written probably within 15 years before the death of the Islamic prophet Muhammad. The undertext differs from the standard Qur\'anic text and is therefore the most important documentary evidence for the existence of variant Qur\'anic readings.\n- Among the Syriac manuscripts obtained from the Nitrian desert in Egypt, British Museum, London: important Greek texts, Add. Ms. 17212 with Syriac translation of St. Chrysostom\'s Homilies, of the 9th/10th century, covers a Latin grammatical treatise from the 6th century.\n- Codex Nitriensis, a volume containing a work of Severus of Antioch of the beginning of the 9th century, is written on palimpsest leaves taken from 6th-century manuscripts of the Iliad and the Gospel of Luke, both of the 6th century, and the Euclid\'s Elements of the 7th or 8th century, British Museum.\n- A double palimpsest, in which a text of St. John Chrysostom, in Syriac, of the 9th or 10th century, covers a Latin grammatical treatise in a cursive hand of the 6th century, which in its turn covers the Latin annals of the historian Granius Licinianus, of the 5th century, British Museum.\n- The only known hyper-palimpsest: the Novgorod Codex, where potentially hundreds of texts have left their traces on the wooden back wall of a wax tablet.\n- The Ambrosian Plautus, in rustic capitals, of the 4th or 5th century, re-written with portions of the Bible in the 9th century, Ambrosian Library.\n- Cicero, De republica in uncials, of the 4th century, the sole surviving copy, covered by St. Augustine on the Psalms, of the 7th century, Vatican Library.\n- Seneca, On the Maintenance of Friendship, the sole surviving fragment, overwritten by a late-6th century Old Testament.\n- The Codex Theodosianus of Turin, of the 5th or 6th century.\n- The Fasti Consulares of Verona, of 486.\n- The Arian fragment of the Vatican, of the 5th century.\n- The letters of Cornelius Fronto, overwritten by the Acts of the Council of Chalcedon.\n- The Archimedes Palimpsest, a work of the great Syracusan mathematician copied onto parchment in the 10th century and overwritten by a liturgical text in the 12th century.\n- The Sinaitic Palimpsest, the oldest Syriac copy of the gospels, from the 4th century.\n- The unique copy of a Greek grammatical text composed by Herodian for the emperor Marcus Aurelius in the 2nd century, preserved in the Österreichische Nationalbibliothek, Vienna.\n- Codex Zacynthius – Greek palimpsest fragments of the gospel of Saint Luke, obtained in the island of Zante, by General Colin Macaulay, deciphered, transcribed and edited by Tregelles.\n- The Codex Dublinensis (Codex Z) of St. Matthew\'s Gospel, at Trinity College, Dublin, also deciphered by Tregelles.\n- The Codex Guelferbytanus 64 Weissenburgensis, with text of Origins of Isidore, partly palimpsest, with texts of earlier codices Guelferbytanus A, Guelferbytanus B, Codex Carolinus, and several other texts Greek and Latin.\nAbout sixty palimpsest manuscripts of the Greek New Testament have survived to the present day. Uncial codices include:\nPorphyrianus, Vaticanus 2061 (double palimpsest), Uncial 064, 065, 066, 067, 068 (double palimpsest), 072, 078, 079, 086, 088, 093, 094, 096, 097, 098, 0103, 0104, 0116, 0120, 0130, 0132, 0133, 0135, 0208, 0209.\n- Petroglyphs of Arpa-Uzen – rock art from the Bronze and Iron Ages later covered by Saka pictorials\n- Lyons, Martyn (2011). Books: A Living History. California: J. Paul Getty Museum. p. 215. ISBN 978-1-60606-083-4.\n- According to Suetonius, Augustus, ""though he began a tragedy with great zest, becoming dissatisfied with the style, he obliterated the whole; and his friends saying to him, What is your Ajax doing? He answered, My Ajax met with a sponge."" (Augustus, 85). Cf. a letter of the future emperor Marcus Aurelius to his friend and teacher Fronto (ad M. Caesarem, 4.5), in which the former, dissatisfied with a piece of his own writing, facetiously exclaims that he will ""consecrate it to water (lymphis) or fire (Volcano),"" i.e. that he will rub out or burn what he has written.\n- ""In the Sinai, a global team is revolutionizing the preservation of ancient manuscripts"". Washington POST Magazine. September 8, 2012. Retrieved 2012-09-07.\n- The most accessible overviews of the transmission of texts through the cultural bottleneck are Leighton D. Reynolds (editor), in Texts and Transmission: A Survey of the Latin Classics, where the texts that survived, fortuitously, only in palimpsest may be enumerated, and in his general introduction to textual transmission, Scribes and Scholars: A Guide to the Transmission of Greek and Latin Literature (with N.G. Wilson).\n- The Institutes of Gaius, ed W.M. Gordon and O.F. Robinson, 1988\n- Sadeghi, Behnam; Goudarzi, Mohsen (March 2012). ""Ṣan\'ā\' 1 and the Origins of the Qur\'ān"". Der Islam. Retrieved 2012-03-26.\n|Wikisource has the text of the 1911 Encyclopædia Britannica article Palimpsest.|\n- OPIB Virtual Renaissance Network activities in digitizing European palimpsests\n- Brief note on economic and cultural considerations in production of palimpsests\n- PBS NOVA: ""The Archimedes Palimpsest"" Click on ""What is a Palimpsest?""\n- Rinascimento virtuale a project for the census, description, study and digital reproduction of Greek palimpsests\n- Ángel Escobar, El palimpsesto grecolatino como fenómeno librario y textual, Zaragoza 2006']"	['<urn:uuid:e0e09408-cdc2-4d23-9953-e92f382809b9>', '<urn:uuid:9ce8a3f8-409a-4196-ab49-cd6f73315114>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:58:04.490895	20	78	2788
29	what nutrients soil free gardening need	Plant essential nutrients in hydroponics are classified as macronutrients and micronutrients. The macronutrients include nitrogen, phosphorus, potassium, calcium, magnesium, and sulfur. The micronutrients include iron, manganese, copper, molybdenum, zinc, nickel, boron, and chlorine. All of these nutrients are equally important regardless of their concentration within the plant.	"[""In soilless hydroponics systems, the cache of nutrients in the soil is nonexistent, making it critical plants receive all of their resources through the solution circulating through their root zone.\nThis heavy reliance on a nutrient supply necessitates mixing and maintaining the perfect hydroponic nutrient solution for vigorous plant growth.\nTo make your own hydroponics nutrient solution for your growing system, it’s important to understand the influence nutrients have on plant growth and what nutrients are responsible for certain plant functions, what options are available for fertilizer sources, the impact water quality plays on the solution, and why parameters such as electrical conductivity, pH, and solution temperature are important.\nThe awareness of these concepts will give you the foundation to manage your hydroponics system successfully.\nThe Importance of Plant Essential Nutrients\nAll plants need certain nutrients for growth. These nutrients are considered plant essential and have specific roles within the plant. If any of the nutrients are deficient plant growth will be affected in some capacity; hence why you need to mix a perfect nutrient solution providing all of these nutrients.\nPlant essential nutrients are classified as either macronutrients or micronutrients, based upon the relative concentration within plant tissue.\nRegardless of their concentration within the plant they are all equally important.\nFound in larger amounts within the plants, macronutrients are often involved in major plant process such as photosynthesis or are key structural components at the cellular level.\nPlant essential macronutrients include nitrogen, phosphorus, potassium, calcium, magnesium, and sulfur.\nNitrogen is considered the most key nutrient needed for plant growth. Its main responsibility in the plant is regulating vegetative growth.\nNitrogen is assimilated into amino acids, the building blocks of protein, it is a major component of chlorophyll, and it is necessary for many of the plant’s enzymatic reactions.\nPhosphorus is a structural component in DNA and RNA and is needed for root growth and flowering.\nPotassium isn’t a component of any plant parts, but functions by activating the enzymatic reactions that occur, making it imperative for the overall health of the plant.\nCalcium holds together cell walls through the formation of calcium pectate, a pectin fiber. When deficient, new tissues exhibits distorted growth because of improper cell wall formation.\nMagnesium is needed for many enzymes within the plant to function properly but its most important function is as the central, structural molecule in chlorophyll. Without chlorophyll plants cannot photosynthesize.\nSulfur is only required within plants in small amounts, but that doesn’t make it any less important than the other macronutrients. Through metabolic processes, plants break down sulfur into forms usable to build organic molecules such as vitamins and odoriferous compounds in onions and garlic.\nThe plant essential micronutrients are needed in much smaller quantities within plants, but their functions are just as critical as the macronutrients. Many of them function as activators of enzymatic reactions and include iron, manganese, copper, molybdenum, zinc, nickel, boron, and chlorine.\nEven though these nutrients are needed in much smaller quantities, it’s important your hydroponics nutrient solution contains the correct ratio of micronutrients for plant growth.\nNutrient Solution Basics\nBefore making your hydroponics nutrient solution as a grower you should familiarize yourself with the fertilizers available for use, the importance of water quality, and why parameters such as electrical conductivity, pH and solution temperature are important.\nThere are many fertilizer sources on the market that will provide the plant essential macronutrients and micronutrients needed to mix your perfect hydroponics nutrient solution.\nWhen it comes to choosing what sources to use you've different options to choose from, depending on your personal preference.\nConventional versus Organic Fertilizers\nConventional/inorganic fertilizers are made completely, or sometimes partially, from synthetic, manmade materials.\nThese inorganic fertilizers contain nutrients that are quickly available for the plants; quickly available nutrients mean plant deficiencies are fixed more rapidly, minimizing long-term effects. Inorganic fertilizers are cheaper to buy, and readily available for purchase.\nPros: Organic fertilizers are made from natural ingredients. They consist of the broken down remains of organisms or are a byproduct (i.e. waste) of the organisms themselves.\nCons: The downsides to organic fertilizers are a higher price tag, and nutrients are more slowly available for plant uptake after application.\nWet or Dry Fertilizers\nFertilizers come in either a granular or liquid form, both having their pros and cons.\nDry fertilizers are more economical to purchase and ship because their formulations don’t contain water.\nThey've a longer shelf life and can be purchased in bulk quantities. Many commercial growers opt for powder or granular fertilizers due to cost efficiency.\nLiquid fertilizer sources are popular with new growers. They are easier to work with, but have a shorter shelf life and are more expensive to purchase.\n1-Part or Multi-Part\nFertilizers can be purchased as a “1-part” system that contains all of the nutrients needed in a single formulation or they can be purchased as a “multi-part”, meaning you may need to be two, three, or four different nutrient mixes to create a single solution.\nNew hydroponics growers like the ease and convenience of buying 1-part mixes that contain all of the nutrients they need for a solution.\nThe drawback with using a single nutrient mix is the resulting nutrient solution cannot be tailored to the developmental stage of your plants, and maximum growth may not be obtained.\nAs growers become more experienced they often switch to a 2-part or 3-part system. These multi-part systems allow you to customize the nutrient ratio depending on fluctuating plant needs, resulting in better growth.\nNo matter if you choose to go with synthetic or organic nutrient sources, if you choose to purchase wet or dry fertilizers, or if you choose to make your own nutrients or buy a pre-packaged formulation, the most important thing to look for are products marketed specifically towards hydroponics systems.\nFertilizers made for soil-based systems are not formulated to provide the same levels of nutrients, especially micronutrients.\nHydroponics gardening revolves around providing nutrients to plants through the water source, making it critical you start with clean, high-quality water.\nPoor quality water will hinder plant growth from the beginning, making it difficult to maintain proper nutrient cycling.\nYour most cost-effective water source is what comes from the faucet, but this might not always be the best-suited water for plants.\nIf you are in an area with “hard” water, the extra bicarbonates have to be neutralized before mixing a nutrient solution to reduce the pH level. But adding phosphoric acid to lower the pH will increase the water’s phosphorus level, potentially pushing it to a toxic level.\nTo avoid this, you can collect rainwater to use, purchase purified water, or install a reverse osmosis filtration system to reduce the dissolved solids.\nDo not use mineral or spring water as it throws off the balance of the nutrients in the hydroponics system.\nSolution Electrical Conductivity (EC)\nWater, in its pure form, without any additives is a poor conductor of electricity. Synthetic fertilizers are made up of mineral salts.\nWhen using them to create a nutrient solution these salts allow electricity to move through the solution, a capacity measured as electrical conductivity (EC).\nHigher salt concentrations equate to a higher EC. By measuring the EC of a nutrient solution you can gauge the nutrient solution strength.\nAs a grower, it’s important to check the EC of your nutrient solution on a regular basis. Levels that are too high or too low can both negatively impact plant growth and require adjusting to keep the EC in the optimum range.\npH OF THE HYDROPONIC NUTRIENT SOLUTION\nThe pH of a solution is simply a measurement of how acidic or basic/alkaline the solution is and is calculated based upon the number of hydrogen ions it contains.\nSolution pH is an important tool to growers as it gives an indication of how the water used to make a nutrition solution has been changed chemically.\nIt also indicates the availability of plant essential nutrients found in the solution.\nIf the pH of the hydroponics nutrient solution is either too acidic or too basic, a plant’s ability to absorb nutrients is significantly influenced, depending upon the hydrogen ion concentration and nutrient interactions.\nAt either end of the pH scale the plant essential macronutrients -- those needed in larger amounts for plant metabolic processes -- become tied up, making them difficult for the plant to absorb; the micronutrients needed become more available at these extremes, potentially becoming toxic to plants due to their abundance.\nTherefore, when growing plants hydroponically, maintain a proper pH of the nutrient solution is vital to ensure nutrient availability isn’t compromised.\nYou should routinely test the solution pH, keeping it between 5.5 and 6.5 for optimal nutrient availability.\nNot to be overlooked, the temperature of the nutrient solution directly affects plant growth, root health, and dissolved oxygen content.\nNutrient absorption is controlled by processes within the roots since plants cannot regular temperatures themselves they simply adapt to environmental changes.\nAt low temperatures, plant growth will slow. As temperature increases, dissolved oxygen decreases, but root respiration rates increase in conjunction, thus increasing the need for higher oxygen levels.\nHigher temperatures also increase microorganism activity/metabolism and an increased prevalence of diseases such as pythium and other infections.\nOptimum nutrient solution temperature depends on the type of plant you're growing and the developmental stage it is in.\nKeeping your solution reservoir between 63 and 72°F will meet the growing needs of most plants.\nSome cool-season crops such as lettuces and carrots growth the best with solution temperatures around 68-70°F; cannabis prefers solution temperatures ranging from 60-75°F depending on the stage of plant growth.\nMost plants prefer a warmer nutrient solution during germination, propagation, and early vegetative growth. As they mature the temperature can be dropped, providing more oxygen for increased respiration rates.\nPlant nutrient requirements also fluctuate depending on the plant variety, its development stage, and the environmental conditions, creating a shifting requirement for nutrient concentrations in the hydroponics nutrient solution.\nEarly in the developmental stages of plant growth, nutrients taken in by the roots are transformed into carbohydrates through photosynthesis for vegetative growth.\nA milder, or lower concentration nutrient solution -- low to medium EC of 1.2 to 1.6 -- promotes vegetative growth.\nFlowering and Fruiting\nPlants that've moved from a developmental stage where they are growing vegetatively to flowering and then fruiting can handle full strength EC in the nutrient solution.\nAs flowers and fruits develop gradually increase the EC of the nutrient solution it to make more difficult for the roots to take in water.\nThe resulting plant stress condenses sugars in the fruits, and more antioxidants such as lycopene and vitamin C are produced.\nLow EC will present as calcium deficiency, and create tip burn on the foliage. The tip burn makes herbs more susceptible to fungal diseases such as powdery mildew.\nGrowers will utilize a higher EC (1.8 - 2.2) to avoid this. Short day plants (less than 10 hours of light) can handle an EC up into the 3.2 - 3.4 range.\nMixing the Solution\nWhen working with nutrient sources that contain such little amounts of certain micronutrients -- because that is all your plants need -- your solution needs to be mixed correctly.\nFollow the instructions on the label closely for dosing rates (or start off with a more diluted concentration), especially if you're a beginner grower.\n- Sterilize all of the containers or reservoirs you'll be mixing the nutrition solution in and all measuring utensils or other apparatus you'll need. You want to avoid cross-contamination or introducing foreign substances into your nutrient solution. Then rinse all of the components really well to remove all traces of the disinfectant/sterilizer.\n- Fill containers/reservoirs with your preferred water source making sure to know a close approximation of the volume. If using water treated by a local municipality, allow it to sit overnight for the chlorine to off-gas and dissipate.\n- Measure dry fertilizers according to weight using plastic scoops, a calibrated scale, and sterilized filter papers; measure liquid nutrients according to volume using a graduated cylinder or beaker.\n- After carefully measuring, add nutrients one at a time (if using a multi-step system add micros, bloom, then grow in that order) to the water. Exercise caution to avoid having fertilizers come in contact with your skin or breathing in the dust of dry fertilizers.\n- After each fertilizer has been added to the water mix it in well using a plastic paddle or spoon, creating the solution.\n- Check the nutrient solution pH, and bring it to 5.5 to 6.5 if it is out of range.\n- Adjust the solution temperature up or down, putting it in the appropriate range for the plants you're growing and their developmental stage.\nHow I mix my hydroponic nutrients solution for best results for growing, Video guide.\nOne of the most important aspects of growing plants hydroponically is ensuring the plants receive optimal nutrition through the nutrient solution supplied to their root zone.\nMaking the perfect hydroponics nutrient solution means as a grower you must understand plant essential nutrients, the nutrition requirements of the plants you're growing, and the important attributes your solution must maintain such as EC, pH, and temperature.\nUnderstanding these principles, coupled with constant monitoring and proper adjustment of the nutrient solution will accelerate and maximize plant growth and resulting yields.\nLindsey Hyland grew up in Arizona where she attended University of Arizona’s Controlled Environment Agriculture. She has supplemented her formal education by working on various organic farms, including spending a semester abroad in India.\nGrowing and/or raising just about anything gets her excited. She is especially passionate about environmental justice and low-tech, sustainable ways to better run small-scale farms and homesteads. Lindsey started Urban Organic Yield to discuss gardening tips and tactics.""]"	['<urn:uuid:fb612bbe-d77c-4a37-bb3e-ef1f31d0598f>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T20:58:04.490895	6	47	2294
30	What happens to food prices without pesticides?	A ban on pesticides would cause a drop in food production, leading to increased food prices and making US farmers less competitive in the global market.	['Pesticides are chemicals designed to prevent or manage pests’ effects, such as rodents, bacteria, insects, weeds, and other pests. Most toxins that affect the environment are by-products from other processes such as automobile engine emissions. However, pesticides, which can be harmful to the environment, are manufactured to use them in the environment. As a result, several debates have emerged on the benefits and effects of pesticides and whether we can manage pests without them.\nWe will write a custom Research Paper on Pesticide Ban and Its Effects on US Agriculture specifically for you\n301 certified writers online\nFurthermore, controversies have arisen on how much control the relevant agencies should have over the manufacture sale and use of pesticides. The recent adverse effects of pesticides that have been witnessed, because most of our farmers and households depend on the same pesticides, have fueled these debates (California Department of Pesticide Regulation, p. 1)\nCan today’s society do away with pesticides?\nPrevious research has proved that American society is highly dependent on pesticides. A complete ban would result in serious problems compared to the environmental benefits that are sought by the ban. For instance, a study done by Knutson and others, which sought to find out the effects of a prohibition of insecticides, fungicides, and herbicides, had the following findings. Such a measure would result in a drop in food production, leading to an increase in food prices. These two conditions would make US farmers less competitive in the global market. The most affected produce would be major grains, peanuts, and cotton. Also, there would be a 27 percent drop in US exports of soybeans and corn drop. All these adverse effects would be summed up by losing jobs for 132 000 people (Delaplane, p. 3).\nThe findings of the research also challenged the notion that a ban on insecticides would help the environment. It explained that if pesticides were banned, farmers would have to increase the number of acres that they firm to compensate for the per-acre yield that will be lost. This will, in turn, result in a loss of wildlife habitat. With a pesticide ban, farmers would be forced to cultivate their farms more frequently to prevent widespread weeds, which would, in turn, promote high erosion of soil. Also, other countries with lenient environmental laws world increase their pesticide use and produce more, and capitalize on the reduced exports of the US (Delaplane, p. 3).\nAnother study was done, which concentrated only on the effects of a ban on fungicide. Considering that, fungicides are used to control fungicides, a plant disease that can kill crop plants and produce lethal food poisons. Therefore, it found out that such a ban in the US would reduce vegetables by 21%, fruits by 32%, and wheat by 6%. Worse still is the fact that consumption of such fruits and vegetables can help prevent some cancers and heart disease. Therefore, society cannot afford to operate without pesticides as it serves primary purposes that we cannot do without (Delaplane, p. 3).\nEthical dilemmas faced by Sam in his decision- making process\nOne of the ethical issues that face Mr. Sam is whether to protect the lives that can be lost through the harmful effects of the pesticides or protect the jobs lost by the ban. A case in point is Mr. Smith, who was mourning his wife, who succumbed to cancer. By supporting a ban, some lives at risk, such as children in school, will be protected. This is because the use of pesticides in schools and anywhere else that the children can access will be banned. On the other hand, some people work in firms that manufacture these pesticides. Other people are employed to apply these pesticides. With the implementation of such a ban, these people risk losing their jobs (Office of Governor M. Jodi Rell, p. 1-2).\nSecondly, Sam is torn between protecting the environment and protecting farmers. Farmers use pesticides to control pests. This ensures that they get high and healthy produce. However, a ban on pesticides will mean that their products will be reduced and their quality will be affected. On the other hand, environmental activists would support the ban. This is because the use of pesticides is believed to cause harm to the environment. For instance, the pesticides used might be washed downstream, and this leads to the pollution of water. Therefore, banning pesticides would help prevent water pollution. As a result, of the two opposing sides, Sam is in a dilemma because by supporting one, he will be voting against the other (Parendes & Burris para. 7-8)\nEffects of the pesticide ban on the county\nImplementation of a complete ban would result in an economic drain on the county. Without the help of pesticides, maintenance, and care of facilities would be a great task. For instance, facilities will be exposed to termites. Hence they will be destroyed easily, and new ones will have to be acquired. A complete ban on pesticides might result in pest outbreaks. These can ruin crops and cost farmers a lot of money. Some of the agricultural plants at risk include flowering plants such as mums, which are the county’s biggest crop. Apple orchards, which are sprayed to control pests, will also suffer. Being that the apples are the treasure of the county, it will be forced to purchase from other regions because its supply will be too low to feed its members (Parendes & Burris para. 4-6)\nSocially the ban would reduce health problems associated with pesticides, as explained by Josh Martin’s NRCS pesticide report, which pointed out that government records proved that pesticides use resulted in many health problems in the county. Therefore, banning them would reduce their effect, and that means that peoples’ health in the county would be improved. Politically, the ban would attract legal action against the county officials for passing a law that harms the economy of the constituents. If the county passes the law without following the standards that have been put like Quebec in Canada, then it risks being sued by the aggrieved parties (Babbage, 11-13).\nI would vote no to allow continued usage of pesticides. My vote is influenced by the many benefits derived from the use of pesticides compared to the harm it brings. Most farmers rely on pesticides to control pests and have quality and healthy produce. In the case of a ban, the alternative is costly for both the farmers and the county. Also, some of the harms attributed to pesticides have been magnified; hence they do not show their real state. For instance, research has shown that the environmental damage attributed to pesticides cannot be compared to the adverse effects experienced if pesticides are not used.\nBabbage, Maria. Ontario to enact toughest pesticide ban in Canada. (2000). Web.\nCalifornia Department of Pesticide Regulation. What is a pesticide?. (n.d). Web.\nDelaplane, Keith, S. Pesticide Usage in the U.S.: History, Benefits, Risks and Trends B 1121.\nOffice of the Governor M. Jodi Rell Governor Rell Signs Law Extending Ban Use of Pesticides on School Grounds. 2007. Web.\nGet your first paper with 15% OFF\nParendes, L. A. and Burris S. H., Pesticides: Can We Do Without Them? (2005). Web.']	['<urn:uuid:44a630d0-8f48-4a96-9d23-7576cab2a858>']	factoid	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:58:04.490895	7	26	1204
31	what ingredients needed for quick korean dipping sauce recipe	The quick dipping sauce requires soy sauce, rice or cider vinegar, and sesame seeds.	"[""By Okwha Chung, Judy Monroe\nAn advent to the cooking of Korea that includes uncomplicated recipes for soups, appetizers, major dishes, part dishes, and muffins.\nRead Online or Download Cooking the Korean Way: Revised and Expanded to Include New Low-Fat and Vegetarian Recipes (Easy Menu Ethnic Cookbooks) PDF\nSimilar recipes books\nAn advent to the cooking of Korea that includes easy recipes for soups, appetizers, major dishes, facet dishes, and truffles.\nA keeping consultant and cookbook multi functional! This artistic assortment has eighty artistic recipes for preserving18 different types of fruit, from apples, blackberries, blueberries, and cherries to cranberries, lemons, quince, plums, rhuburb, and tomatoes, however it additionally has eighty recipes for utilizing your preserves (whether you've made jelly, jam, chutney, curd, liqueur, pickles, or salsa) in major dishes, aspect dishes, muffins, and cocktails.\nDinner in half-hour or utilizing five or much less elements? Don't decide on only one. .. one zero one effortless daily Recipes is stuffed with either! the newest addition to the best-selling Gooseberry Patch picture cookbook sequence will store time and simplify buying lists. Even higher, every one is observed through a mouthwatering full-color picture!\nBETTY is going VEGAN is a entire advisor to making scrumptious nutrients for today's vegan kin. This must-have cookbook beneficial properties recipes encouraged via The Betty Crocker Cookbook, in addition to hundreds and hundreds of unique, never-before-seen recipes absolute to please even meat-eaters. It additionally deals perception into why Betty Crocker has been an icon in American cooking for thus long-and why she nonetheless represents a definite sort of the trendy super-woman approximately a hundred years once we first met her.\n- Julia's Kitchen Wisdom: Essential Techniques and Recipes from a Lifetime of Cooking\n- Healthy Recipes For Your Nutritional Type\n- The Great Big Cheese Cookbook\n- Olaf's Kitchen: A Master Chef Shares His Passion\nExtra info for Cooking the Korean Way: Revised and Expanded to Include New Low-Fat and Vegetarian Recipes (Easy Menu Ethnic Cookbooks)\nIf you like, you can add a pinch of salt to the sesame seeds as you crush them. 2 tbsp. sesame seeds 1. Place sesame seeds in a small frying pan. ) Cook, stirring constantly, over medium heat 2 to 4 minutes, or until golden brown. ) Remove from heat and set aside to cool. 2. Place toasted seeds into a large bowl and crush with the back of a wooden spoon. Preparation time: 2 minutes Cooking time: 2 to 4 minutes Makes 2 tbsp. Quick Dipping Sauce ø c. soy sauce ¥ tsp. rice or cider vinegar ¥ tsp. sesame seeds 1.\nFluff with a fork and serve hot. Preparation time: 5 minutes Cooking time: 32 to 38 minutes Makes 6 c. What’s more, it turns itself off automatically. 34 Noodles/ Kuk Su Whether thick or thin, noodles are added to soups, stir-fries, and steamed or simmered dishes. Egg noodles, made from durum wheat, are creamy white and mild in flavor. They absorb the flavor of the foods they are cooked with. Egg Noodles/ Kye ran kuk son 2 quarts water 7-oz. pkg. or half a 16-oz. pkg. egg noodles 1. In a large saucepan, bring water to a rapid boil over high heat.\nRemove pancake to plate. Roll up and cut into slices ø-inch wide. Use egg pancake strips atop any dish. Preparation time: 5 minutes Cooking time: 7 to 10 minutes Use as topping or garnish 32 Kimchi/ Kimch’i It is fun to make kimchi yourself, but if you would rather not wait the 24 to 48 hours needed for the flavor to develop, you can buy it by the jar at large supermarkets and Asian food stores. Kimchi will keep indefinitely in the refrigerator. 5 c. green or Chinese cabbage, cut into bite-sized pieces 6 tsp.""]"	['<urn:uuid:b0de1257-502d-4103-89df-420d80e9462c>']	factoid	direct	long-search-query	similar-to-document	single-doc	novice	2025-05-12T20:58:04.490895	9	14	626
32	modern flood control methods comparison challenges	Modern flood risk management faces increasing challenges due to more frequent catastrophic floods from climate change. Two main approaches exist: traditional 'grey' infrastructure like concrete dams, and nature-based solutions like mangroves and water-retention parks. While grey infrastructure focuses solely on flood protection, nature-based approaches offer multiple benefits including environmental preservation and public recreation. However, implementing these solutions involves complex trade-offs between efficiency and equity, requiring careful consideration of different stakeholder views and moral aspects beyond just economic optimization. A key challenge is developing frameworks to properly evaluate and compare these different approaches while ensuring fair distribution of flood protection.	"['The number and impact of catastrophic floods have increased significantly in the last decade, endangering both human lives and the environment. With on-going climate change, the risk of flooding is likely to increase even further. This prompts the urgent question: how to use the limited resources available for flood risk management in an equitable and efficient way?\nIn the flood risk literature, this question is often reduced to a monetary optimization problem, ignoring the moral aspects. Neither is it recognized that people may have different, often conflicting, views on what equity and efficiency amount to. The goal of this project is to develop an ethical framework for reconciling efficiency and equity considerations in flood risk management.\nIn my PhD research, I developed a reflective equilibrium model for reconciling conflicting criteria when distributing responsibilities in a research team. In this VENI-project, I will explore a new line of distributive problems at a societal level, where we are confronted with a plurality of moral views and where collective interests may put pressure on individuals’ rights to be safeguarded against flooding. In such situations, it is extremely difficult to find a morally acceptable trade-off between efficiency and equity.\nThe overall research question is: how to reconcile equity and efficiency in flood risk management in a way that is acceptable in a pluralistic society. By developing a procedural theory, this innovative project addresses two major challenges for risk governance and risk ethics. First, equity and efficiency will be properly conceptualized and operationalized for the context of flood risk management, which is required before any trade-off can be made. Second, equity and efficiency will be developed such that they are acceptable in a pluralistic society.\nThe approach followed in this project is based on political philosophical literature on distribution problems and empirical research on stakeholders’ opinions. The project will lead to action-guiding recommendations for flood risk management, enabling a morally acceptable implementation of flood risk policy.\nVENI-grant for innovational research from the Netherlands Organization for Scientific Research (NWO) (grant nr. 016.144.071). The project runs from February 1, 2014 - January 31, 2018.\nRelated Scientific publications (selection)\n- Doorn, N. forthcoming. ""Allocating responsibility for environmental risks: A comparative analysis of examples from water governance"". Integrated Environmental Assessment and Management (DOI: 10.1002/ieam.1799). available online\n- Doorn, N. forthcoming. ""Resilience indicators: Opportunities for including distributive justice concerns in disaster management"". Journal of Risk Research (DOI:10.1080/13669877.2015.1100662). available online\n- Toonen, Th. A. J. and N. Doorn. forthcoming. ""Good governance for the commons: Design for legitimacy"" In: The Design Turn in Applied Ethics. Eds: M. J. Van den Hoven, S. Miller and Th. W. M. Pogge. Cambridge, Cambridge University Press.\n- Doorn, N. 2016. ""Distributing responsibilities for safety from flooding"". FLOODrisk 2016. 3rd European Conference on Flood Risk Management, France. E3S Web of Conferences 7, 24002 (2016) (DOI: 10.1051/e3sconf/20160724002). click to open pdf\n- Doorn, N. 2016. ""Governance experiments in water management: From interests to building blocks"". Science and Engineering Ethics 16(3) (DOI: 10.1007/s11948-015-9627-3). available online\n- Doorn, N and S.O. Hansson. 2015. ""Design for the value of safety"". In: Handbook of Ethics and Values in Technological Design. M. J. Van den Hoven, P. Vermaas and I.R. Van de Poel (eds.). Dordrecht, Springer.\n- Doorn, N. 2015. ""The blind spot in risk ethics: Managing natural hazards"". Risk Analysis 35/3: 354-360 (DOI: 10.1111/risa.12293). available online\n- Doorn, N. 2014. ""Equity and the ethics of water governance"" In: Infranomics - Sustainability, Engineering Design and Governance. Topics in Safety, Risk, Reliability and Quality. Eds. Gheorghe, A., M. Masera, and P.F. Katina. Dordrecht, Springer, pp. 155-64.\n- Doorn, N. 2013. ""Water and justice: Towards and ethics of water governance"" Public Reason 5/1: 95-111 click to open pdf\nRelated Professional/Popular publications\n- Doorn, N, B. Broekhans, and E. Mostert. 2013. ""Redactioneel: Normen en waarden in water governance"" (in Dutch) Water governance 3: 5 click to open pdf', 'How can nature help build flood resilience?\nThere are many terms for disaster risk reduction and climate change adaptation approaches that use and/or work with nature to build flood resilience, for example nature-based solutions (NBS), eco-based solutions, green and blue infrastructure, or eco-system services.\nIn this article we look at approaches and share resources that use different terminology but all have the common aim of building resilience by enhancing natural capitals. You might also find How can land use planning reduce flood risk? useful.\nThe blog From Grey to Green infrastructure: a paradigm shift needed to deliver on climate action explains the concepts of grey, green, and hybrid infrastructure and the benefits of moving away from only applying hard infrastructure approaches like concrete dams or dykes to mitigate flood risk.\nMultiple benefits of using nature-based solutions to build flood resilience\nUnlike hard infrastructure solutions which usually have a single purpose: protection from floods, nature based approaches provide a range of benefits beyond flood protection.\nThe blog Solutions providing multiple resilience dividends require integrated approach, and the working paper it’s based on, looks at the multiple resilience dividends of a range of flood resilience interventions, many using nature based solutions.\nMangroves planted to reduce the impact of tidal waves and coastal erosion also become home to spawning fish, contributing to ocean bio-diversity and sustaining livelihoods of local people. Parks designed to hold rainwater excess to prevent floods also become recreational spaces for urban residents, providing opportunities for exercise, leisure, and relaxation which has positive impacts on physical and mental health.\nHow do we make the case for investing in nature-based solutions for flood resilience?\nDespite growing interest in, and advocates for, nature based approaches to disaster risk reduction many decision makers, planners, and community members still prefer traditional, hard infrastructure based on assumptions of risk mitigation from grey infrastructure is quicker to materialise.\nTo make nature-based flood resilience solutions more attractive for investment we need better evidence that they work.\nWe need pilot interventions, which’s success is rigorously assessed and evidenced, that can be used as case studies. And we need frameworks for assessing the cost and benefits of nature-based solutions for disaster risk reduction, both in comparison with traditional infrastructure, and the cost of responding to the disaster likely to happen if no risk reduction is undertaken.\nZurich Flood Resilience Alliance colleagues have written a range of blogs discussing such frameworks:\n- Solutions providing multiple resilience dividends require integrated approach\n- Is this new framework what is needed to better assess nature-based solutions for climate change adaptation?\n- Eco-solutions: Can nature solve our problems?\n- Eco-solutions: How to measure their effectiveness?\n- Eco-solutions: How to build a case for their scaling up?\n- From Grey to Green infrastructure: a paradigm shift needed to deliver on climate action\nThe WWF in partnership with OFDA has developed this guide to help support communities using natural and nature-based methods of flood risk management\nSuccessful cases of integrated urban flood risk management (IUFRM) have demonstrated significant results not only in reducing the negative impacts of\nThe objective of this document is to present five principles and implementation guidance for planning, such as evaluation, design, and implementation']"	['<urn:uuid:02832499-537b-4156-841a-1367e242d316>', '<urn:uuid:ca1f6dc2-ddf3-457f-9830-72303fe02c6d>']	factoid	direct	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:58:04.490895	6	99	1174
33	I'm curious about these two groups - what made Pearls Before Swine and Solomon Grey stand out in terms of their musical style and approach?	Pearls Before Swine was known for their psychedelic folk sound and unique blend of humanistic and mystical songs, using an eclectic variety of instruments and incorporating interpretations of writings from Tolkien and Herodotus. Solomon Grey, on the other hand, defied traditional genre classifications, combining rich atmospherics, cinematic elements, and classical/orchestral influences, while also incorporating field recordings and beat-sculpting in their work.	"['Pearls Before Swine (band)\n|Pearls Before Swine|\n|Origin||Melbourne, Florida, USA|\n|Genres||Folk rock, psychedelic rock, psych folk|\n|Labels||ESP-Disk, Reprise, Blue Thumb|\n|Associated acts||Area Code 615|\n|Past members||Tom Rapp\nWayne Harley (1965-69)\nLane Lederer (1965-68)\nRoger Crissinger (1965-67)\nJim Bohannon (1968)\nJim Fairs (1969)\nElisabeth Rapp (1969-72)\nMike Krawitz (1971)\nGordon Hayes (1971)\nJon Tooker (1971) (d.2008)\nMorrie Brown (1971)\nDavid Wolfert (1971)\nArt Ellis (1971-74)\nBill Rollins (1971-74)\nHarry Orlove (1971-74)\nPearls Before Swine was an American psychedelic folk band formed by Tom Rapp in 1965 in Eau Gallie, now part of Melbourne, Florida. They released six albums between 1967 and 1971, before Rapp launched a solo career.\nEarly years, 1965-68\nWith high school friends Wayne Harley (banjo, mandolin), Lane Lederer (bass, guitar) and Roger Crissinger (piano, organ), Rapp wrote and recorded some songs which, inspired by the Fugs, they sent to the avant-garde ESP-Disk label in New York. The group took its name from a Bible passage: ""Give not that which is holy unto the dogs, neither cast ye your pearls before swine ...."" (Mat. 7:6, KJV), meaning: do not give things of value to those who will not understand or appreciate them. They were quickly signed up, and recorded One Nation Underground (1967), featuring songs of mysticism, protest, melancholia, and some controversy in the case of “Miss Morse”, which spelled out an obscenity in code. The album eventually sold some 200,000 copies, although management and contractual problems meant that the band received little reward for its success.\nOn working with the label ESP-Disk, Rapp has said that ""We never got any money from ESP. Never, not even like a hundred dollars or something. My real sense is that he (Bernard Stollman) was abducted by aliens, and when he was probed it erased his memory of where all the money was"".\nThe strongly anti-war themed Balaklava (1968) followed, inspired by the Charge of the Light Brigade. Rapp has said ""The first two albums are probably considered the druggiest, and I had never done any drugs at that point. I smoked Winston cigarettes at that time, so these are all Winston-induced hallucinations."" The album covers featured paintings by Bosch and Brueghel, while the records themselves included interpretations of the writings of Tolkien and Herodotus as well as archive recordings from the 1890s, with innovatively arranged songs using an eclectic variety of instruments.\nReprise period, 1969-72\nThe band signed for Reprise Records in 1969, although by this time the other original members had left and the band name now referred to Rapp and whichever musicians he was recording or touring with, one of whom, Jim Fairs, was previously a member of The Cryan\' Shames. The five albums on Reprise were generally more conventional in sound, but contained a unique blend of humanistic and mystical songs, with some whimsical touches. Some were recorded in New York and others – particularly The Use of Ashes and City of Gold - in Nashville with top session musicians including Charlie McCoy, Kenny Buttrey, and other members of Area Code 615. Several also featured Rapp\'s then-wife Elisabeth on vocals. The oddly-upbeat ""The Man"", from City of Gold, was sung by David Noyes and recorded at A&R Studios in New York City during the summer of 1970. Noyes\' friend, Jon Tooker, took his position when the band toured Europe that fall.\nIn his teens, Rapp lived close to Cape Canaveral and watched the rockets take off. The song ""Rocket Man"", on the album The Use of Ashes - written the day Neil Armstrong landed on the moon - was credited by Bernie Taupin with inspiring his hit song with Elton John of the same title. Quote : ""We didn\'t steal that one from Bowie, we stole it from another guy, called Tom Rapp..."". Many of the other songs of this period reflected Rapp\'s interests in mysticism, his relationship with his alcoholic father, and his experiences of living for a time in (and marrying a native of) the Netherlands.\nIn 1971, Pearls Before Swine toured for the first time, the group then comprising Rapp, Mike Krawitz (piano), Gordon Hayes (bass) and Jon Tooker (guitar). Around this time, Rapp often referred onstage, not quite seriously, to the group as ""the house band for the SDS."" A live album from this period, Live Pearls, recorded at Yale University, was released as a download in December 2008. The final Reprise album, Familiar Songs, consisted of newly conceived, arranged, and produced recordings of some of Rapp’s earlier songs, featuring his then-current band, Morrie Brown (bass, guitar, mandolin and vocals), Robby Merkin (piano, organ, synth, bass and vocals), and David Wolfert (acoustic and electric guitar, 12-string, Dobro and vocals) along with drummer Billy Mundi, formerly of The Mothers Of Invention. Without Rapp\'s knowledge, the label released it not as a ""Pearls Before Swine"" album, but under his name alone.\nTwo further albums followed, released under Rapp\'s own name on Blue Thumb Records. The first, Stardancer, was again recorded in Nashville, followed by Sunforest. The band - by that time comprising Rapp, Art Ellis (flute), Bill Rollins (bass, cello) and Harry Orlove (guitar, banjo) - toured until 1974, with Rapp from then performing solo until a final appearance in 1976 supporting Patti Smith.\nAfter this, Rapp retired from music and, after graduating from Brandeis University, became a civil rights lawyer. After being contacted by the magazine Ptolemaic Terrascope, he re-appeared in 1997 at Terrastock, a music festival in Providence, Rhode Island, with his son\'s band, Shy Camp, and began recording again with 1999\'s A Journal of the Plague Year.\nJon Tooker died in a motorcycle crash in 2008.\nPBS have been cited as a key influence by various musicians including The Dream Academy, Damon and Naomi, the Bevis Frond, Magic Hero vs. Rock People, The Late Cord, This Mortal Coil, and the Japanese band Ghost. Three tribute albums have been released by Secret Eye Records.\n- One Nation Underground (1967, ESP-Disk)\n- Balaklava (1968, ESP-Disk)\n- These Things Too (1969, Reprise)\n- The Use of Ashes (1970, Reprise)\n- City of Gold (1971, Reprise) (Thos. Rapp / Pearls Before Swine)\n- Beautiful Lies You Could Live In (1971, Reprise) (Tom Rapp / Pearls Before Swine)\n- Familiar Songs (1972, Reprise) (Tom Rapp)\n- Stardancer (1972, Blue Thumb) (Tom Rapp)\n- Sunforest (1973, Blue Thumb) (Tom Rapp / Pearls Before Swine)\n- A Journal Of The Plague Year (1999, Woronzow) (Tom Rapp)\n- Live Pearls (recorded 1971, released 2008, WildCat Recording)\n- Constructive Melancholy (1999, Birdman) (CD compilation of 1969-72 Reprise tracks)\n- Jewels Were The Stars (2003, Water) (4 CD box set of first four Reprise albums)\n- The Wizard of Is (2004, Water) (2 CD collection of live recordings, out-takes etc.)\n- The Complete ESP-Disk Recordings (2005, ESP-Disk and WildCat) (the two ESP albums on one CD)\n- ""Morning Song"" / ""Drop Out!"" (1967, ESP-Disk)\n- ""I Saw The World"" / ""Images Of April"" (1968, ESP-Disk)\n- ""These Things Too"" / ""If You Don\'t Want To"" (1969, Reprise)\n- ""Suzanne"" / ""There Was a Man"" (1969, Reprise)\n- ""The Jeweller"" / ""Rocket Man"" (1970, Reprise)\n- ""Marshall"" / ""Why Should I Care?"" (1972, Blue Thumb)\n- Tom Rapp by Mark Brend\n- Weiss, Jason (2012). Always in Trouble: An Oral History of ESP-Disk\', the Most Outrageous Record Label in America. Wesleyan. ISBN 9780819571595.\n- Sleevenotes to Jewels Were The Stars box set\n- Comments by David Noyes\n- Official PBS site\n- Sleevenotes to Familiar Songs CD reissue\n- The Washington Post. 1998-05-18 http://www.washingtonpost.com/wp-srv/style/features/rapp.htm. Retrieved 2010-05-22. Missing or empty\n- The Hangar\n- http://www.wildcatrecording.com/ WildCat Recording\n- Official Tom Rapp / Pearls Before Swine website\n- A tribute to Tom Rapp and Pearls Before Swine\n- 1994 interview with Tom Rapp\n- Article by Mark Brend\n- Washington Post article\n- Lane Lederer', 'Human Music Solomon Grey\nDear HIGHRESAUDIO Visitor,\ndue to territorial constraints and also different releases dates in each country you currently can`t purchase this album. We are updating our release dates twice a week. So, please feel free to check from time-to-time, if the album is available for your country.\nWe suggest, that you bookmark the album and use our Short List function.\nThank you for your understanding and patience.\nYours sincerely, HIGHRESAUDIO\n- 1The Weight03:04\n- 2Closed Door03:41\n- 3Wonderful World03:56\n- 6Inside Outside03:36\n- 7Willow House04:05\n- 9The Game04:13\n- 10In Love03:23\nInfo for Human Music\n“Human Music” ist eine Sammlung aufwendig gearbeiteter Songs, die die emotionale Klangwelt der Band widerspiegelt. Das Album befasst sich mit urmenschlichen Themen wie Entscheidungen, Liebe und Verlust. Während der Entstehung des Albums wurde der Mutter des Sängers eine unheilbare Krankheit diagnostiziert. Ihr Ableben hat Human Music besonders beeinflusst.\n“Das Album beschäftigt sich ganz fundamental mit Verlust und diesem als wesentlicher Bestandteil des Lebens. Damit hat alles für uns begonnen. Aber wie mit allen Dingen im Leben, gibt es da stets Raum für eigene Interpretation.”\nUnderstanding London-based duo Joe Wilson and Tom Kingston aka Solomon Grey requires ditching the usual definition of a band. No neat genre exists in to which they fit. No sound is out of bounds. You’re as likely to find them capturing field recordings as being in the studio sculpting beats.\nThink of Solomon Grey as a sonic brand rather than band and you’ll grasp how they can be both acclaimed soundtrack producers (recently, of the BBC’s three-part drama The Casual Vacancy, based on J.K. Rowling’s book) and a boundary-pushing pop act (Zane Lowe described their debut single, Gen V, as the future of Radio 1). Solomon Grey don’t differentiate between the roles – each inspires the other, often they grow from the same stems, with the aesthetic the same. Rich atmospherics, a cinematic shimmer, classical and orchestral influences and a meticulous attention to detail are their trademarks.\nSolomon Grey first came to public attention at the tail end of 2012, when Firechild appeared online, with no information about the band behind it. The ecstatic reaction to the track led to a deal with Black Butter which, in 2013, released two double-sided singles, including debut single Gen V, that saw Solomon Grey hailed one of Britain’s most exciting new electronic acts.\nBy the time Firechild arrived, the pair had been pushed to breaking point. Following some time spent together on the West Coast of Ireland Tom was now living in rural Australia, recovering from a long illness helping his soon-to-be wife run a 2300 acre sheep and wheat farm following the death of her father. Joe was in London, trying to scrape together the money to visit Australia on a regular basis to finish what the pair were so close to completing.\n“It was a crazy time,” says Tom. “There I was, a vegetarian, in the outback with 3000 sheep, with no idea how long I’d be there for. My wife was expecting our first child and I was waiting for Joe to come over. There was a moment when we wondered if we could carry on. But we both knew we’d made music that was worth pursuing. Plus, my illness was a wake-up call. I wanted to throw all the energy I had in to the one thing I always wanted to do with my life. We refused to give up.”\nSolomon Grey’s two stubborn, like-minded souls first met in Oxford when Joe was still at school and Tom was at university studying philosophy. They moved to London and began making music inspired by Drum’n’bass and British trip hop (Portishead, Massive Attack, Zero 7, early Ninja Tunes). However Solomon Grey only started to take shape at the end of the Noughties, when they found their sound with Last Century Man. “It took us years to get that song right,” says Joe.\nThe release of debut single Gen V was supposed to begin the slow build to Solomon Grey’s debut ‘pop’ album, but their soundtrack work put paid to those plans. First, the Irish Tourist Board hired them to soundtrack a film of The Wild Atlantic Way, a 2500km continuous coast road up the west coast of Ireland. Next, they recorded the soundtrack for a BFI-funded film called Gozo, due for release in 2016. Then came The Casual Vacancy call.\nThe recent Selected Works mini-album and Selected Features EP documents Solomon Grey’s journey so far with most of the music featured in The Casual Vacancy. No two songs on Selected Works sound the same, but all are instantly, obviously Solomon Grey. From the sensual, soulful, female-fronted Twilight to the hypnotically soothing Choir to The Wild via haunting, honeyed, strings-soaked Miradors and the propulsive beats-backed Firechild, Selected Works is best described as beautiful music that doesn’t play by pop’s regular rules.\nThis album contains no booklet.']"	['<urn:uuid:947698e1-4102-4519-8ca0-8b95228c3ce8>', '<urn:uuid:00774fb3-7ab3-4a5f-8bc1-d515135ed314>']	factoid	with-premise	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T20:58:04.490895	25	61	2115
34	blockchain carbon credits benefits security risks	Blockchain-based carbon credits offer several benefits: they provide full traceability, auditability, instant settlement, and allow for fractional credits. Each token represents 1 tonne of CO2 offset and is permanently recorded on platforms like Ethereum or Polygon. However, there are important security risks to consider. The blockchain platform's security depends on protecting all points of entry from manipulation, requiring strong encryption and identity management tools. Organizations must also address insider threats through security awareness training and clear parameters with partners. Additionally, as blockchain networks grow, issues around data sharing, ownership, and interoperability between different networks can create vulnerabilities.	['How Carbonhalo Works\nOffsetting your carbon footprint through verified carbon credits\nWe should all try to reduce our carbon emissions where we can. For the unavoidable emissions leftover, Carbonhalo is here to offset them with verified carbon credits that fund emissions reductions projects across the globe.\nThese projects include replanting forests, preventing deforestation, and investing in technology that reduces or removes CO2 emissions from the atmosphere.\nOur carbon credit certification\nWhat is a carbon credit?\nA carbon credit is a digital tradable certificate that confirms a tonne of CO2 has been averted through environmental projects or businesses. Because we use the Blockchain ecosystem across the Ethereum and Polygon network and tradable exchanges, all purchase transactions and disposal of carbon credits are visible to everyone and instantly auditable for complete transparency. When we retire the token, it’s impossible to be used again, meaning we can certify it as single-use.\nWe only invest in projects or purchase tokens that are verified by leading industry bodies/agencies including VERRA, Gold Standard, REDD+ and UNFCCC. This ensures that your contributions are only going towards projects that meet all the requirements to make a positive impact.\nWhat is a VERRA - Voluntary Carbon Standard?\nThe Voluntary Carbon Standard (VCS) Program is the world’s most widely used voluntary GHG program. They ensure the credibility of the emission reductions generated by offset programs. Nearly 1,700 certified VCS projects have collectively reduced or removed more than 630 million tonnes of carbon and other GHG emissions from the atmosphere. Verra’s role is to develop and administer the program.\nWhat is REDD+ and what are REDD+ carbon credits?\nREDD – the Reduction of Emissions from Deforestation and Forest Degradation programs – is a United Nations initiative that focuses on preserving and reforesting the worlds’ rainforests to fight climate change.\nREDD+ (or REDD-plus) refers to projects that reduce emissions from deforestation and forest degradation, incorporating the role of conservation, sustainable management of forests, and enhancement of forest carbon stocks in developing countries.\nREDD+ carbon credits are credits verified and linked to REDD+ specific projects.\nYour carbon credit purchases\nAcross the globe, carbon pricing is irregular and confusing. So we’ve adopted a universal approach that keeps carbon offsets at the same pricing structure, no matter where in the world you live. These tokenised carbon credits are offered as fractional credits, for ease of payment.\nWhat are tokenised carbon credits?\nA tokenised carbon credit is a certified carbon credit, that is placed on the Blockchain ecosystem like Ethereum or Polygon. Blockchain tokens offer full traceability, auditability, instant settlement, and the flexibility of fractional carbon credits.\nEach token or carbon credit that is burned or retired immediately and permanently offsets 1 tonne of CO2, mapped directly to the retirement of the carbon credit. This is captured within the blockchain ledger, and the transaction ID information is lodged and made publically available.\nWhat are fractional carbon credits?\nA fractional carbon credit is a portion of a credit that can be bought at an exact price, rather than having to round up or down to the nearest complete carbon credit, making it easier to offset your carbon footprint.', 'Recent research revealed that blockchain is set to become ubiquitous by 2025, entering mainstream business and underpinning supply chains worldwide.\nThis technology is set to provide greater transparency, traceability and immutability, allowing people and organizations to share data without having to be concerned about security. However, blockchain is only as strong as its weakest link. Despite the hails surrounding blockchain’s immutable security, there are still risks surrounding it that organizations must be aware of – and mitigate – prior to implementation.\nIt is important to understand that there are two types of blockchain – permissionless and permissioned. The most prominent example of permissionless blockchain is Bitcoin – a public blockchain network that anyone can participate in. Cryptocurrencies like bitcoin favor this type of blockchain technology because it enables all users to track, verify and confirm transactions, regardless of whether users choose to be anonymous or not.\nThe other blockchain model is permissioned (also known as private blockchain) – and is mainly used for business applications. These networks are only accessible to known entities such as partners, suppliers or customers. With permissioned blockchain, a company establishes protocols to achieve consensus, and verify and assemble blocks. This set up can deliver thousands of transactions per second and provide granular management and control over who sees and accesses the transactions.\nIn both cases, the main benefit is the trust and transparency that blockchain brings – all parties involved in the network have total visibility into the transactions recorded in the blockchain ledger and each block is tied to the block before it.\nThis transparency makes blockchain extremely difficult to manipulate at scale. While the blockchain platform itself may be secure, there is still some work to be done to ensure organizations are equipped to make their networks secure end to end. For true security, organizations must focus on the last mile connection between a physical event and the digitized record of this event.\nIf these points of entry to the platform are tampered with, the blockchain is rendered worthless. It is therefore imperative that organizations secure all points of entry, and assess the risks, before they consider deploying blockchain on a broad scale. They will need to consider security at all layers, most importantly:\nThis starts with ensuring data and transactions entered in the blockchain ecosystem are adequately protected from manipulation. The infrastructure these networks resides on must also have the necessary protections in place. With blockchain, you are only as strong as your weakest link.\nIf integration points are compromised, the entire blockchain ecosystem could be at risk, meaning that blockchain credentials and data could be exposed to unauthorized users.\nIdentity and access management\nTo prevent unauthorized parties from accessing blockchain data, a combination of encryption and identity management tools are needed. Stolen credentials could potentially allow a cybercriminal to access the blockchain platform, regardless of how secure it is. Organizations must deploy identity and access management controls. Encryption should also be deployed to ensure that data is not stolen, manipulated or leaked in transit.\nThe insider threat should be a focal concern when it comes to blockchain too. Organizations must consider that employees, partners and suppliers – be it unintentionally or maliciously – can cause security incidents that impact the blockchain.\nTo mitigate this, organizations should deploy security awareness training for employees and outline clear security parameters and responsibilities with partners. This will stop employees from making careless mistakes and may also ward off malicious insiders. In line with these requirements, blockchain can provide advanced security controls – for example, leveraging the public key infrastructure (PKI) to authenticate and authorize parties, and encrypt their communications.\nBlockchain-based networks are built on shared business interests creating a system of trust. However, as the network grows, participating entities could leave the network and new ones may join, leading to ambiguities around operational considerations around data sharing and data ownership. These could result in serious regulatory and reputational repercussions for organizations as data owners, unable to secure the customer data.\nOrganizations are multi-faceted and have multiple revenue streams, often linked to each other. One of the major challenges to blockchain adoption has been a lack of interoperability across different blockchain networks. There have been recent developments, with major players embarking on developing interoperable networks, which could boost blockchain interest to a different level, at the same time introducing additional levels of vulnerability.\nA key component of blockchain networks is the Smart Contracts, which are developed using different languages on the platform being used, like Solidity being used in Ethereum. These languages allow developers to make changes to the underlying blockchain networks, causing vulnerabilities. However, from an enterprise blockchain perspective, a solid governance mechanism using permissioned chain can establish a secure system in place to restrict the privileges to governing body.\nTo achieve the most value from blockchain, both now and in the future, organizations must take responsibility for their safety and security at all levels – application, Infrastructure, data and partners.\nBy conducting a blockchain risk assessment and addressing key risks, organizations can make sure they are well positioned to leverage the efficiencies, transparency and cost-effectiveness provided by blockchain without opening themselves up to unexpected risks. The most pragmatic way for organizations interested in blockchain is to test the concept through pilot programs. Pilots should be focused on the areas that offer organizations the most control and companies should take these weak links into consideration.\nUltimately, blockchain has the ability to solve business issues relating to traceability, responsiveness, and trust. By taking a carefully planned approach to implementation, and understanding blockchain’s weak links, organizations can unlock the true value of blockchain, creating new opportunities and reducing inefficiencies.']	['<urn:uuid:043ce89c-9343-428a-a756-e6f364a81453>', '<urn:uuid:aa3cc425-6945-4d18-8ba5-24e7cdc911d7>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T20:58:04.490895	6	97	1463
35	When did the Messmer Instrument Company first start operating as a British agency for foreign instrument makers?	The Messmer Instrument Company started in the early 1920s as a British agency for foreign instrument manufacturers.	"[""The Quarterly No. 74 - April 2010\nMagical Illusion - Two Collections of Watercolours by Jacques le Moyne de Morgues (c1533-1588) - Peter Bower\nA well illustrated article that takes the reader through a forensic investigation of papers used by the French watercolorist. Although were found in a nineteenth century binding, they proved to be original sixteenth century works of art. Identification of the papers proved crucial in confirming the origin of the works.\n15 pages, 20 illustrations\nThe Painted Picture, by Richard Benson.\nA book describing the myriad ways in which multiple copies of an original image can be produced, starting with techniques used by cavemen and ending with digital technology.\nPhotographs of the Past: Process and Preservation, by Bertrand Lavédrine.\nA book describing photographs on paper or glass, and their conservation and preservation.\nPrinted on Paper: the Techniques, History and Conservation of Printed Media, Ed. By Jane Colbourne and Reba Fishman Snyder.\nThis set of proceedings from a conference held in Gateshead and Newcastle in 2007 is self explanatory; it focuses upon the science, technology, and ultimately conservation and preservation, of printed works on paper.\nEast Lancashire Paper Mill Co. Ltd. Part 3 - PW Hampson\nThe third and final part of the series examining the early years of this limited company. In particular this part details the proportion of women holding shares in the company, and the effect of a bonus share issue on the company's performance.\n6 pages, 4 illustrations\nInterior Views of Turkey Mill, Maidstone - Ronald White\nA beautiful series of photographs of the interior of Turkey Mill taken in 1963, showing both the manufacturing process and mill architecture.\n4 pages, 7 illustrations\nHistory of Paper Test Instrumentation Part 15: Surface Strength and Surface Debris Instruments - Daven Chamberlain\nThis is the last article in the series to describe specialist strength test apparatus. Assessment of surface strength and debris deposition is important mainly in printing and other converting processes. The article shows the variety of ways in which papers can be tested to assess the integrity of the surface, including some that were borrowed from related industries, such as textiles.\n7 pages, 4 illustrations\nBagasse as a Source of Papermaking Fibre - Barry Watson\nBagasse is one of the world's most important non-wood crops that finds use in paper manufacture. The author gives an overview of the subject, starting with its history, leading on to details of its processing into pulp, and ending with its papermaking qualities.\n4 pages, 5 illustrations\nHistory of the Messmer Instrument Company - Fred Johnson\nThe Messmer Instrument Company started in the early 1920s as a British agency for foreign instrument manufacturers. Slowly it progressed on to design and manufacture its own instruments, in addition to maintaining its agency activities. Eventually, after a series of mergers, it was purchased by an American company, who finally closed the UK operation in 2009.\n3 pages, 1 illustration\nPictures of French Stampers - Nigel Vellam\nThe author produced an excellent illustrated description of stampers in The Quarterly No.63. This series of illustrations was shown during his presentation at the 2006 Annual Conference, but not included along with the original article due to space constraints. The images shows details of a number of stampers and associated machinery in various French mills.\n2 pages, 11 illustrations\nThe Paper Trade in Fiction - Anon\nThe Paper Trade is not a subject that lends itself greatly to fiction. Nevertheless, in a series of articles over the years, a few stories have been described where the industry plays a central role. This short article describes three further stories, dating from the late nineteenth or early twentieth centuries, which have not previously been covered in The Quarterly.\nSome Ancient Asiatic Papers - R. Bouvier and L. Vidal\nAn article describing the scientific examination of some tenth century papers of Asiatic origin, made at the turn of the last century by two notable French academics.""]"	['<urn:uuid:1336e008-35cb-4ec0-82cc-7ce5042c5a16>']	factoid	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:58:04.490895	17	17	656
36	How big was the water system beneath Jerusalem's Nea Church?	The water reservoir beneath the Nea Church's southern side was 33 x 17 meters in size. It was divided into vaults supported by arches resting on huge piers measuring 5 x 3.5 meters and ten meters high. The reservoir's interior was coated with thick hard plaster and could hold thousands of gallons of water.	['During the Byzantine period (4th-7th centuries) Jerusalem was a Christian city with many churches. The most important church was the Holy Sepulcher, on the traditional site of the crucifixion and burial of Jesus, built by Constantine the Great at the beginning of the fourth century. Another large church was the impressive Nea Church, built by the emperor Justinian at the height of the Christian era of Jerusalem in the mid-sixth century. Thousands of Christian pilgrims came to Jerusalem to worship and they left many written descriptions of the city and its holy places. But the most important testimony of Byzantine Jerusalem is the famed Madaba map, made of colored mosaic, part of the floor of a church (in present-day Jordan) which was built at the end of the 6th century.\nThe map, a beautiful birds-eye-view of Jerusalem, shows in detail the walls, the gates, the main streets and the churches of the city. The main throroughfare, the Cardo maximus (Cardo, in short) was a colonnaded street bisecting the city from north to south, from todays Damascus Gate to the Zion Gate. Along the Cardo in the map, two large church complexes are clearly shown the Holy Sepulcher in the north and the Nea Church at the southern end.\nThe Madaba Map, the earliest graphic representation of Jerusalem, guided archeologists in their search for the remains of Byzantine Jerusalem. After the reunification of Jerusalem in 1967, excavations were conducted in the Jewish Quarter (located in the southeastern part of the Old City). The Nea Church and the Cardo were discovered, in the locations depicted in the Madaba map.\nThe Nea Church\nIn Jerusalem he (Justinian) built a church in honor of the Virgin which is beyond compare. People call this church the New Church (Nea). Thus wrote Procopius, court historian of the emperor Justinian. The full name of the edifice was the Church of Mary, Mother of God. Procopius recounts details of its construction and the names of the various buildings which made up the large church complex.\nPortions of the church were uncovered on the southern slope of the Jewish Quarter of the Old City. The church was built on a massive podium supported by thick walls of stone and concrete resting on deep bedrock. It was a very large structure, 115 m. long and 57 m. wide, divided by four rows of columns which supported the roof. The eastern wall was especially broad (6.5 meters) and contained side apses, 5 meters in diameter. Marble pavement covered the floor.\nAlong the southern side of the church, where the bedrock is at great depth, a very large subterranean water reservoir was found, completely preserved. Some of the annexes of the church had been built above it. The reservoir measures 33 x 17 m. and is divided into vaults supported by arches which rest on huge (5 x 3.5 m.) piers, ten meters high. The interior of the reservoir was coated with a thick layer of hard plaster; it had a capacity of thousands of gallons of water.']	['<urn:uuid:895a71d2-3931-4e01-abb6-1db7c9c2e072>']	open-ended	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:58:04.490895	10	54	506
37	As a beginner in spirituality, how can I maintain humility?	True humility comes from recognizing one's complete helplessness. Just as a child knows they are helpless without their mother's help, one should feel completely dependent on Krishna. This helplessness is actually one of the key elements of surrender. One must have the faith that 'Although I am helpless, because I am surrendered to Krishna, I have no danger. He will help me and protect me.'	['[In the Bhagavad-gita it is also said that such fallen devotees are given a chance to take birth in a family of highly qualified brahmanas or in a rich mercantile family. A devotee in such a position is not as fortunate as one who is chastised by the Lord and put into a position seemingly of helplessness. The devotee who becomes helpless by the will of the Lord is more fortunate than those who are born in good families. The fallen devotees born in a good family may forget the lotus feet of the Lord because they are less fortunate, but the devotee who is put into a forlorn condition is more fortunate because he swiftly returns to the lotus feet of the Lord, thinking himself helpless all around.\nPure devotional service is so spiritually relishable that a devotee becomes automatically uninterested in material enjoyment. That is the sign of perfection in progressive devotional service. A pure devotee continuously remembers the lotus feet of Lord Sri Krsna and does not forget Him even for a moment, not even in exchange for all the opulence of the three worlds.]\n>>> Ref. VedaBase => SB 1.5.19\n[But Kuntidevi was shown far more favor because Lord Krsna did not save the other children of Devaki, whereas He saved the children of Kuntidevi. This was done because Devaki’s husband, Vasudeva, was living, whereas Kuntidevi was a widow, and there was none to help her except Krsna. The conclusion is that Krsna endows more favor to a devotee who is in greater dangers. Sometimes He puts His pure devotees in such dangers because in that condition of helplessness the devotee becomes more attached to the Lord. The more the attachment is there for the Lord, the more success is there for the devotee.]\n>>> Ref. VedaBase => SB 1.8.23\n[Whatever the material energy dictates, the conditioned soul does. He has no responsibility; he is simply the witness of the action, but he is forced to act in that way due to his offense in his eternal relationship with Krsna. Lord Krsna therefore says in Bhagavad-gita that maya, His material energy, is so forceful that it is insurmountable. But if a living entity simply understands that his constitutional position is to serve Krsna and he tries to act on this principle, then however conditioned he may be, the influence of maya immediately vanishes. This is clearly stated in Bhagavad-gita, Seventh Chapter: Krsna takes charge of anyone who surrenders to Him in helplessness, and thus the influence of maya, or conditional life, is removed.]\n>>> Ref. VedaBase => SB 3.26.7\n[A person in the conditioned stage of material existence is in an atmosphere of helplessness, but the conditioned soul, under the illusion of maya, or the external energy, thinks that he is completely protected by his country, society, friendship and love, not knowing that at the time of death none of these can save him. The laws of material nature are so strong that none of our material possessions can save us from the cruel hands of death. In the Bhagavad-gita (13.9) it is stated, janma-mrtyu-jara-vyadhi-duhkha-dosanudarsanam: one who is actually advancing must always consider the four principles of miserable life, namely, birth, death, old age and disease. One cannot be saved from all these miseries unless he takes shelter of the lotus feet of the Lord.]\n>>> Ref. VedaBase => Adi 7.1\n[The holy name, however, eradicates all reactions to past sins, both those manifesting themselves at present and those destined to manifest themselves in the future. Simply by chanting the holy name, one attracts the attention of the Supreme Lord, who therefore considers, “Because this person has chanted My holy name, My duty is to give him protection.” The power of the holy name to absolve sins is declared emphatically in the scriptures. The Garuda Purana tells us, “If one chants the holy name of the Lord, even in helplessness or without desiring to do so, all the reactions of his sinful life depart, just as when a lion roars all the small animals flee in fear.” Says the Brhad-visnu Purana, “Simply by chanting the name of Hari, a sinful man can counteract the reactions to more sins than he is able to commit.” In the Visnu-dharmottara we read, “This word krsna is so auspicious that anyone who chants this holy name rids himself immediately of the reactions of sinful activities from many, many births.” But for the chanting of the holy name to exert such a powerful purifying effect, the devotee must chant the holy name in purity, without offense. Having ceased to indulge in sinful actions, he must live a pure and holy life.]\n>>> Ref. VedaBase => NAM: Introduction\n[Paramahamsa: How can one remain humble in executing his devotional service?\nPrabhupada: Yes. If he thinks himself that “I am non-entity, helpless,” then he can remain. If he thinks “I can do something. I have got so much intelligence,” then he cannot become humble. Just like… (aside:) Don’t come very near. Just like child is humble always because he knows that “I am completely helpless. Unless mother helps me, I am complete…” Therefore, whenever he’s in need of something, he cries, “Mother, help me.” This is helplessness. Helplessness. Always. Karpanye. This is one of the items of surrender. Unless you think yourself helpless, you cannot surrender. Surrender is complete when you think yourself that you are helpless. “I am helpless, but because I am surrendered to Krsna, He’ll save me.” This faith also must be there, that “Although I am helpless… Not although I am factually helpless but because I am surrendered to Krsna, I have no danger. He’ll help me, protect me.”]\n>>> Ref. VedaBase => Morning Walk — June 11, 1974, Paris\nbalisesu dvisatsu ca\nyah karoti sa madhyamah\nA second-class devotee has the vision that some are envious of God, but this is not the vision of the best devotee. The best devotee sees, “Nobody is envious of God. Everyone is better than me.” Just like Caitanya-caritamrta’s author, Krsnadasa Kaviraja. He says, “I am lower than the worm in the stool.”\n>>> Ref. VedaBase => PQPA 6: The Perfect Devotee\nSrila Narottama dasa Thakura sings in his Prarthana (39):\ntoma vina ke dayalu jagat-samsare\npatita-pavana-hetu tava avatara,\nmo sama patita prabhu na paibe ara\n“My dear Lord, please be merciful to me. Who can be more merciful than Your Lordship within these three worlds? You appear as an incarnation just to reclaim the conditioned, fallen souls, but I assure You that You will not find a soul more fallen than me.”\n>>> Ref. VedaBase => Madhya 8.38\n[As servants of the Supreme Lord, all living entities are one, but a Vaisnava, because of his natural humility, addresses every other living entity as prabhu. A Vaisnava sees other servants to be so advanced that he has much to learn from them. Thus he accepts all other devotees of the Lord as prabhus, masters. Although everyone is a servant of the Lord, one Vaisnava servant, because of humility, sees another servant as his master. Understanding of the master begins from understanding of the spiritual master.]\n>>> Ref. VedaBase => SB 7.5.11\nPlease accept my humble obeisances and dandavats!\nAll glories to Srila Prabhupada\nand all pure devotees of the\nSupreme Personality of Godhead!\nI would like to propose the following passage is key to understanding how a devotee tries to claim to be the most fallen or lower than the worm in stool.\n[As servants of the Supreme Lord, all living entities are one, but a Vaisnava, because of his natural humility, addresses every other living entity as prabhu. A Vaisnava sees other servants to be so advanced that he has much to learn from them. ]\n>>> Ref. VedaBase => SB 7.5.11\nThis suggests that the devotee wants to learn from every soul how to serve Lord Sri Krsna.\nPlease keep me informed how I can do more to serve Krsna Katha.\nYour humble servant for Krsna consciousness\n(Lord Sri Krsna never leaves Vrndaban.)\nfaith, hope, love, friendship, and family for God\nhere at New Talavana\nregarding Bhakti Yoga and Varnasrama Dharma…\nBhakti Yoga International\nGo to Krishna Culture for all your Transcendental needs…']	['<urn:uuid:b0bcfc19-d48f-421a-9926-378d6c016568>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:58:04.490895	10	65	1372
38	How much did gold miners make at Alder Gulch?	While Alder Gulch yielded an estimated $30 million in placer gold between 1863 and 1866, the typical miner struggled to make a living wage and ended up with more blisters, aches and pains than gold.	"['|Route to the Montana Gold Mines|\n(approx. the same route as I-15 and Highway 91)\nJames was successful in finding a new and shorter route through Cache Valley to the newly discovered gold mines. On this trip he headed due north from Salt Lake City, through the settlements of Richmond (Utah) and Franklin (Idaho). Eight miles north of Franklin, he crossed the Bear River into Marsh Valley (present-day Bannock County, Idaho). The new road turned out to be “an excellent one, with abundance of feed and water.”(5)\nBannack reached its peak population of nearly three thousand inhabitants in spring 1863, with nearly two thousand others living downstream. The settlement dissolved as quickly as it had sprung up when another rich gold vein was discovered in May at Alder Gulch, eighty miles to the east. The first prospectors on the new site took their gold to Bannack to purchase supplies. Once they got a little whiskey under their belts, they couldn’t resist bragging that they had found the “mother lode.” As the news spread, many prospectors pulled up stakes and headed for Alder Gulch, which soon became the thriving settlement of Virginia City.(12) Alder Gulch yielded an estimated $30 million in placer gold(13) in just three short years between 1863 and 1866. (Not to say that every prospector got rich there. The typical miner struggled to make a living wage, and ended up with more blisters, aches and pains, than he did gold.)\n- Church History in the Fulness of Times, chapter 30.\n- James Gemmell, letter to his sister, Jane, January 1, 1870.\n- The ghost town of Bannack is now Bannack State Park, located just 24 miles southwest of Dillon. Take I-15 to exit 59 (State Highway 278), and travel west for 17 miles. Turn left on the Bannack Bench Road and travel south for 4 miles to the Park entrance on the left-hand side.\n- Madsen, Brigham, North to Montana, p. 75. Flour purchased in Salt Lake at $6 for a one hundred pound bag was sold in Montana for $40 a bag.\n- Deseret News, “New Road North”, December 10, 1862. The new road followed roughly the same route as present-day Highway 91 and I-15. (See also Joseph C. Walker, p. 64.)\n- Deseret News, “More Indian Murders”, January 14, 1863.\n- During the Civil War, the government sent seven hundred troops to Utah Territory to protect the overland mail and the transcontinental telegraph stations from Indian attacks. Instead of using the recently vacated Camp Floyd, they chose a site east of Salt Lake City and named it Camp Douglas after Stephen A. Douglas. The troops came in October 1862 and stayed until the end of the Civil War. ( See Church History in the Fulness of Times, Chapter 30.)\n- The military action became known as the Bear River Massacre (about 250 Indians were killed).\n- Deseret News, “Pack Train for the Mines, March 25, 1863.\n- Madsen, Brigham, North to Montana, p. 27.\n- The Journal of Captain James Stuart, with notes by Samuel T. Hauser and Granville Stuart. (See “The Yellowstone Expedition of 1863”, Historical Society of Montana 1876, Vol. 1, p. 149.)\n- http://www.legendsofamerica.com/mt-virginiacity.html The majority of avowed secessionists living in the camp, which was then part of Idaho Territory and therefore ""belonging"" to the Union, made it primarily a ""southern” town, with its residents’ sympathies lying with the Confederates. Furthermore, the camp was producing enough gold to win the Civil War for whoever could capture it. Due to this strategic position, President Lincoln soon sent northern emigrants into the mining camp to help hold the gold for the North. This of course caused all kinds of tension in the new city, which quickly became one of the most lawless places in the American West.\n- Placer gold mining, or free gold prospecting, should not be confused with hard rock gold mining. Placer mining involves dust, flakes, and nuggets, while hard rock mining involves veins of ore.\n- http://www.virginiacity.com/#photos Virginia City is now frozen in time. See a 19-minute video about its preservation.']"	['<urn:uuid:d0fecc3d-6d3b-49a6-adcd-6ca5269458b1>']	factoid	direct	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:58:04.490895	9	35	680
39	how different communication modes work in ethernet when using twisted pair cable	In Ethernet with twisted pair cables, there are two main communication modes: Half Duplex and Full Duplex. Half Duplex uses one pair of wires and only allows one party to transmit data at any time, similar to a walkie-talkie where only one person can speak at a time. Full Duplex uses one pair of wires for transmitting and receiving, allowing both parties to communicate simultaneously like a telephone conversation. There's also Autonegotiation, which uses Fast Link Pulses (FLPs) to communicate with devices at the other end of the cable.	['In preparation of your CCNA exam, we want to make sure we cover the various concepts that we could see on your Cisco CCNA exam. So to assist you, below we provided a CCNA Ethernet Cliff Notes article. This section will probably be most helpful to review immediately before you take your Cisco CCNA certification exam on test day!\nEthernet is the most common LAN technology in use today. Your Cisco CCNA exam will cover various concepts about Ethernet so we must make sure you are well versed in various Ethernet concepts to pass your Cisco CCNA exam.\nEthernet was pioneered by Digital, Intel, and Xerox in 1980. The IEEE modified it and set the 802.3 specification. Originally this was used to govern network communications over coaxial cable. This evolved into various other physical topologies using hubs and Cisco switches employing twisted pair Ethernet cabling. This provided greater flexibility in the setup, deployment and performance of the computer networks. Obviously your Cisco routers and Cisco switches will support Ethernet.\nEthernet Transmission Mode\nUsing your twisted pair Ethernet cable, you have the following duplex modes:\nHalf Duplex: Uses one pair of wires, only one party is allowed to transmit data at any given time. Uses CSMA/CD. This is similar to walkie-talkie communications where only one person can speak at a time or no one can understand the other because there is just static interference. Full Duplex: Uses one pair of wires, Tx to Rx & Rx to Tx. When connected back-to-back, collisions will not occur. This is similar to a telephone where two people can talk at one time, and hear each other so the communications are not canceled out as in half duplex.\nAutonegotiation: Ethernet uses a priority scheme to define preferred options. For 100-Mbps and 10-Mbps Ethernet, the lower the priority value the more preferred Auto negotiation uses a series of Fast Link Pulses (FLPs) to communicate with the device on the other end of the cable.\n802.2 frame is 802.3 frame with LLC inside the data field of the header.\nMAC Address: 48 bit long, 6 bytes. The first 24 bits are IEEE assigned to a card which identifies the vendor of the card. The last 24 bits are vendor assigned which “should” make every MAC address unique. But this is not always the case with cheap NICs from the Orient.\nMulticast Address: (0100.5exx.xxxx)\nFunctional Addresses: Valid only on Token Ring. It identifies one or more interfaces that provide a particular function.\nThe bits of each byte of an Ethernet frame are reversed when translated to Token Ring or FDDI frame. Here is an example of the MAC address conversion:\nEthernet MAC: 0200.ECA2.0080\nSame MAC in TokenRing: 4000.3745.0001\nFrame Elements –These are the different components that make up a frame.\nCyclic Redundancy Check(CRC): Provide error detection, but not error correction.\nFrame Check Sequence(FCS): Located at the end of the frame. Contains the CRC.\nDA: Destination MAC address.\nSA: Source MAC address.\nPreamble: Altering 0s and 1s used syncronize the recieving interfaces.\nStart Frame Delimiter (SFD/Synch): Used by Peamble to indicate data will follow.\nLength (for Ethernet II): Lists frame length.\nType(for 802.3): Identifiesthe data type.\nData: Size ranges from 46 to 1500 bytes\nEthernet Maximum Length Specifications:\n10Base2 802.3 185m\n10Base5 802.3 500m\n10BaseT 802.3 100m\n100BaseTx 802.3u 100m\nIn preparation of your CCNA exam, we want to make sure we cover the various concepts that we could see on your Cisco CCNA exam. So to assist you, below we will discuss on of the more difficult CCNA concepts; Cisco CCNA Ethernet Cliff Notes. As you progress through your CCNA exam studies, I am sure with repetition you will find this topic becomes easier. So even though it may be a difficult concept and confusing at first, keep at it as no one said getting your Cisco certification would be easy!']	['<urn:uuid:d82ea84f-8428-4dd1-adee-bfc184f19331>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T20:58:04.490895	12	89	643
40	I've heard about incredible lake views - how do Lake Louise in Canada and Lake Mývatn in Iceland compare in terms of their surrounding landscape and what makes each one special?	Lake Louise in Canada is surrounded by stunning glacial landscapes and features the Lefroy Glacier, with forests and mountain peaks creating a picturesque setting that's so beautiful it has become a popular tourist destination with a luxury hotel (Château Lake Louise) on its shores. Lake Mývatn in Iceland is characterized by its volcanic, spring-fed environment surrounded by open grasslands where waterfowl nest. The Icelandic lake is particularly notable for its unique relationship between humans and nature, where locals have sustainably harvested waterfowl eggs for over a millennium while protecting the nesting grounds.	"['A new collaboration builds on millenia-old knowledge and the latest scanning electron microscopy to uncover long-term patterns of sustainable wildfowl management in Mývatn, Iceland.\nEach summer in the Mývatn region of Northern Iceland, people set out to walk the grassy, open grounds of their lakeside farms, collecting full baskets of eggs from wild ground-nesting waterfowl. Tens of thousands of pairs of waterfowl migrate to the volcanic spring-fed lake each spring and begin nesting in May. Their eggs are an abundant and easily collected source of food but nesting birds are vulnerable and easily disturbed, so it is a delicate task. For generations of Mývatn inhabitants collecting and eating eggs has marked a time of year, and is a memorable characteristic of a region that is strongly defined by the highland lake at its centre.\nThe inland, freshwater lake Mývatn supports a rich insect life which in turn supports several species of waterfowl, as well as trout and Arctic charr. Throughout the 20th century, it was normal for hundreds of eggs to be collected in a day\'s work by each household during the waterfowl breeding season. Egg collection was carried out under a shared set of rules and norms: birds were not to be killed or disturbed as they nested, and only a limited number of eggs could be collected from every nest: about one egg less than half the total clutch, which for most species still left more young than could be successfully raised in most years[i]. Limited collecting ensured that the birds could successfully fledge another generation and that the young would then come back as adults to breed in the same nesting grounds. Today, people protect their farm\'s nesting grounds by killing predators such as minks and foxes, and in earlier times they must have also managed the now-extinct Icelandic pigs so that the waterfowl nests were unharmed[ii]. The mutual agency embedded in this tradition is noteworthy and was mentioned by several 19th century visitors to Mývatn. Archaeological evidence suggests this regulated human and bird interaction in fact dates to the first settlement of the region in the 9th century – over a millennium of sustainable egg harvesting[iii].\nExcavations of farm midden (garbage) deposits in the Mývatn area have yielded many artifacts, well-preserved animal bones, charred plant remains, pollen, and insects, separated by layers of tephra – the volcanic ash released from a single eruption. This provides a remarkable opportunity to investigate human-landscape ecodynamics on the “human scale” of years and decades, and establish secure temporal links among different archaeological and geological profiles, lake and soil cores, and historical records.\nLarge amounts of discarded eggshell fragments are regularly found in excavations, and on several Mývatn archaeological sites these egg fragments have been found in direct contact with a volcanic tephra layer dated to 871 ±2 CE, providing a marker horizon for the first extensive human settlement of Iceland[iv].These well-dated bone and eggshell collections indicate that the modern pattern of intensive egg collection and sustainable management of the migratory waterfowl in the area had Viking-age roots.\nInitial scanning electron microscope (SEM) analysis was used to identify the eggshell to family or genus level as predominately coming from waterfowl[v]. Finds of eggshell from archaeological deposits extending from first settlement to the modern period are now being more intensively identified to species level using new SEM methodology in an innovative collaboration between archaeologists and biologists[vi]. The collaboration is being extended to include an ancient DNA barcoding technology to aid the eggshell classification. Laboratory science is adding clarity to the long-term patterns of human-bird interactions observed by the archaeologists.\nArchaeology and bioscience are now also teaming up with environmental history, environmental humanities and saga scholarship in renewed collaborative work to better understand the social context and dynamics of apparently millennial-scale sustainable management on the local level. Literary scholars and historians are working to digitize the rich written record that extends from medieval law codes and sagas down to early modern folklore, unpublished journals and farm accounts as well as early scientific observations by local farmers. This combination of field and laboratory science with history and environmental humanities perspectives has already provided some clues about the actual social mechanisms that allowed for successful local level community waterfowl management through centuries that saw multiple stresses from climate, disease, and volcanic eruptions. There appears to have been no “tragedy of the commons” leading to a collapse of the communal waterfowl management system, despite recurring hard times[vii][viii].\nWhile archaeology, paleoecology, and bioscience can help demonstrate the deep roots of the Mývatn sustainable waterfowl story, it is these environmental history and humanities approaches – connected to ethnography and the knowledge of living Mývatn residents – that can provide the sort of detail that contributes useful practical lessons for modern managers of wildlife-community interactions[ix]. The early results of this broadening collaborative work among academic disciplines and local communities has emphasized the value and role of Local and Traditional Knowledge(LTK) in our attempt to understand the complexities of human-animal encounters. When 9th century Norse farmers set up their community management strategy in their new homes around the Mývatn lakeshore their organization of resource use and community governance was crafted on the local scale and was remarkably effective in what were initially unfamiliar ecosystems. Their descendants’ continued success in upholding this pattern and maintaining sustainable use of waterfowl – despite famine, eruptions, plague, and the changes brought about by early globalization – surely carries lessons for present and future attempts to find pathways to sustainability elsewhere.\nThis international collaborative project uses the research perspective of historical ecology and wide “transdisciplinary” collaboration that links disciplinary science and humanities with the LTK held by local communities. The Historical Ecology research agenda aims to both provide better adaptive tool kits for modern resource management practitioners and to offer broader insights for long term forward planning by sustainability scenario-builders that can incorporate the lessons of increasingly well documented cases of human ecodynamics on the millennial scale[x][xi].\nThis research is supported by US NSF Arctic Social Sciences Program grants to the North Atlantic Biocultural Organization cooperative (www.nabohome.org ), the NordForsk-funded Nordic Network for Interdisciplinary Environmental Studies (NIES http://www.miun.se/nies), the Faculty of Life and Environmental Sciences University of Iceland, the Anthropology Program City University of New York, the Mývatn Research Station, and is part of the IHOPE “Inscribing Environmental Memory in the Icelandic Sagas” project (http://ihopenet.org/circumpolarnetworks/).\n[i]Garðarsson, A. (1979). “Waterfowl populations of Lake Mývatn and recent changes in numbers and food habits.”Oikos 32: 250–70.\n[ii]Brewington, Seth , Megan Hicks, Ágústa Edwald, Árni Einarsson, Kesara Anamthawat-Jónsson, Gordon Cook, Philippa Ascough, Kerry Sayle, Símun V. Arge, Mike Church, Julie Bond, Steve Dockrill, Adolf Friðriksson, George Hambrecht, Árni Daniel Juliusson, Vidar Hreinsson, Steven Hartman, Thomas H. McGovern (2015) “Islands of Change vs. Islands of Disaster: Managing Pigs and Birds in the Anthropocene of the North Atlantic. “In: Arlene Rosen (ed.) “The Anthropocene in the Longue Durée” special issue of The Holocene in press.\n[iii]McGovern, T. H., O. Vésteinsson, Adolf Friðriksson, M. J. Church, I. T. Lawson, I. A. Simpson, Á Einarsson, A Dugmore, A. J. Cook, S. Perdikaris, K. Edwards, A. M. Thomson, P. W. Adderley, A. J. Newton, G. Lucas, R. Edvardsson, O. Aldred, and E. Dunbar. (2007) ""Landscapes of settlement in northern Iceland: Historical Ecology of human impact and climate fluctuation on the millennial scale"" American Anthropologist 109.1 (2007): 27-51.\n[iv]Dugmore, Andrew J. Thomas H. McGovern and Richard Streeter (2014)“Landscape legacies of Landnám in Iceland: What has happened to the environment as a result of settlement, why did it happen and what have been some of the consequences” In Harrison, R. & Maher, R. (eds.)..Long-Term Human Ecodynamics in the North Atlantic: An Archaeological Study. Lexington Publishers, Lanham, Maryland, pp 195-213.\n[v]McGovern, T.H., Sophia Perdikaris, Arni Einarsson, Jane Sidell . Coastal Connections, Local Fishing, and Sustainable egg harvesting, patterns of Viking age inland wild resource use in Mývatn District, Northern Iceland”, (2006) Environmental Archaeology 11.1 : 102-128.\n[vi]Megan Hicks, Árni Einarsson Kesara Anamthawat-Jónsson,Ágústa Edwald, Adolf Friðriksson, Ægir Þór Þórsson, Thomas H. McGovern (2015)“Community and Conservation:Documenting Millennial Scale Sustainable Resource Use at Lake Mývatn Iceland.“, In: C. Isendahl&D. Stump(eds.) Handbook of Historical Ecology and Applied Archaeology Oxford University Pressin press 2014\n[vii]Dugmore, Andrew J., Thomas H. McGovern, Richard Streeter, Christian Koch Madsen, Konrad Smiarowski and Christian Keller (2013) ‘Clumsy solutions’ and ‘Elegant failures’: Lessons on climate change adaptation from the settlement of the North Atlantic islands , chapter 38 in: A Changing Environment for Human Security: Transformative Approaches to Research, Policy and Action, Edited by Linda Sygna, Karen O\'Brien and Johanna Wolf. Routledge UK London.\n[viii]McGovernThomas H., Hildur Gestsdóttir, Seth Brewington, Ramona Harrison, Megan Hicks, Konrad Smiarowski, James Woollett (2015 in press) “Medieval Climate Impact and Human Response: An Archaeofauna circa 1300 AD from Hofstaðir in Mývatnssveit, N Iceland”, Journal of the North Atlantic, in press.\n[ix]McGovernThomas H. (2014) “North Atlantic Human Ecodynamics Research: Looking forwards from the past. “ In Harrison, R. & Maher, R. (ed.s). Long-Term Human Ecodynamics in the North Atlantic: An Archaeological Study. Lexington Publishers, Lanham, Maryland, pp 213-223\n[x]Armstrong, Chelsey Geralda& James R. Veteto (2015) “Historical Ecology and Ethnobiology: Applied Research forEnvironmental Conservation and Social Justice”, Ethnobiology Letters, vol 6:5-7.\n[xi]Hegmon Michelle, Jette Arneborg, Andrew J. Dugmore, George Hambrecht, Scott Ingram, Keith Kintigh, Thomas H. McGovern, Margaret C. Nelson, Matthew A. Peeples, Ian Simpson, Katherine Spielmann, Richard Streeter, Orri Vésteinsson. (2013)“ The Human Experience of Social Change and Continuity: The Southwest and North Atlantic in “Interesting Times” ca. 1300.” In Climates of Change: The Shifting Environments of Archaeology, edited by Sheila Lacey, Cara Tremain, and Madeleine Sawyer. Proceedings of the 44th Annual Chacmool Conference, University of Calgary. Pp 23-32.', 'Canada is a country with more than her fair share of lakes. In reality, eight percent of the country’s landmass is covered by the fresh water bodies. That’s a fact which puts Canada in prime position on the list of countries with the most surface area covered by lakes.\nLakes are beautiful and just as environmentally important as oceans, yet they remain pretty much uninvestigated. Yes, data is available on how big they are, what their depth is and there’s plenty of information on estimations of their volume. We know they play an important part in the hydrologic cycle, but what exactly lies beneath the waters is pretty much a mystery. The world takes lakes for granted. We shouldn’t. Lakes are amazing.\nIf you really want to see lakes in all their true natural glory, Canada should be your number one place to visit. Check out the most incredible fifteen here and you’ll start to wonder why you haven’t been there sooner.\n15. Wedgemount Lake\nWedgemount Lake nestles just below Wedge Mountain in the Garibaldi Mountain Range in British Columbia, Canada. It’s encompassed by two glaciers, the Wedgemount and the Armchair. At its deepest point it’s estimated to be around sixty metres deep and five at its shallowest. The lake can only be reached after a steep, seven-kilometre climb which you’ll need some good boots for and the right equipment. Don’t plan on catching your dinner while you’re up there. Recent studies have shown the lake to have no fish life.\nWhy Go? The incredible turquoise waters of Wedgemount Lake are surrounded by some of the most stunning scenery you’ll ever see. It’s comparable with a glacial moonscape. Because of its geographical position the night skies are light pollution free which makes it a great place for overnight star spotting.\nMap Location: Lake Wedgemount\n14. Lake Superior\nIf you’re planning on visiting Lake Superior, don’t think you’ll be able to take a leisurely stroll all the way around its shores. Lake Superior, with a surface area of almost thirty-two thousand square miles, is as big as some countries. It spreads its watery reach from Ontario in the north of Canada, over the border and through several American states.\nWhen they named it, they named it right, because it really is superior in many ways. Not only is it the largest of the Great Lakes, it’s also the largest freshwater lake in the world. Surprisingly enough, it only comes in third where the volume of water it holds is concerned. You’ll need some good muscles if you’re considering rowing across it though. It measures a staggering one hundred and sixty plus miles at its widest point. Now that’s some lake.\nWhy Go? While you might not be able to walk around Lake Superior, you can drive around it. It’s a one thousand two hundred mile road trip of a lifetime through national parks with amazing canyons, cliffs, and waterfalls. A definite must do.\nMap Location: Lake Superior\n13. Berg Lake\nBerg Lake lies just underneath the highest peak in the Rockies, Mount Robson, which is in the Canadian province of British Columbia. It’s tucked away out of sight at an altitude of just under five and a half thousand feet. The only way to reach it is by hiking through the wilds of Mount Robson Provincial Park. Is it worth the effort? Definitely. The azure waters of Berg Lake are backed by impressive mountains and bordered by pine forests. The landscape and light are a photographers dream.\nWhy Go? Berg Lake is a glacial lake fed by the Berg Glacier. As it is constantly moving, chunks frequently drop off the glacier and into the lake waters to float there like mini icebergs. To get to Berg Lake you need to pass through the incredible Valley of a Thousand Waterfalls which, on its own, is reason enough for making the trip.\nMap Location: Berg Lake\n12. Lake Ontario\nLake Ontario, in the Canadian province of Ontario, is bordered not just by Canada, but by the US state of New York too. It would be easy to compare Lake Ontario to one big faucet as the lake provides drinking water to over nine million people. That’s a lot of water coming out of the smallest of the Great Lakes every day. It might be diminutive, but what it lacks in superficial square footage it makes up for in depth. Being over eight hundred feet at its deepest point means it holds more water than lakes which are bigger in size. An unusual fact which makes it the fourteenth largest lake in the world.\nWhy Go? Lake Ontario is rimmed with beautiful beaches and cliffs. It’s the perfect place for practicing water sports of every kind. From windsurfing, kayaking and paddle boarding. The bonus? If you fall off you don’t have to worry about swallowing the water, it’s fresh not salt. While you’re there, keep your eyes peeled and see if you can spot Lake Ontario’s monster of the deep, Ogopogo.\nMap Location: Lake Ontario\n11. Maligne Lake\nMaligne Lake is a stunning fourteen mile-long stretch of bright blue water at the base of the Maligne Mountains. Part of the Jasper National Park in Alberta, Canada, its glacial waters are on average just over a hundred feet deep in most parts, but in some points plunge down as far as three hundred. It’s forty-five miles of shoreline is bordered with pine forests. It’s also world renowned for the tiny, but much-photographed islet called Spirit Island.\nWhy Go? From on the waters of Lake Maligne it’s actually possible to see all three of the different glaciers which feed the lake. It’s also a great place for sport fishing, with the main catch being rainbow trout, and kayaking and canoeing. So if you have a pioneering spirit, Lake Maligne should be on your bucket list, but keep an eye out for bears, wolves, and caribou.\nMap Location: Maligne Lake\n10. Spotted Lake\nSpotted Lake, in the Similkameen Valley in British Columbia, when full of water isn’t that much to look at. It’s also probably the smallest patch of water to be labeled lake too. It’s just under half a mile long and less than a quarter of a mile wide. Makes you wonder what all the fuss is about, doesn’t it? But when Spotted Lake dries out in the summer months, it becomes one of those weird and wonderful creations of Mother Nature that are just absolutely amazing. The lake’s waters have an exceptionally high mineral content and as they evaporate, they leave behind a strange lunar-like surface full of crystallized pathways and pools.\nWhy go? Spotted Lake was revered as a sacred site by the local indigenous tribes who realized it had amazing medicinal and therapeutic qualities. It also magically changes color as the season progresses and the water evaporation increases. It really is one of those bizarre things which just has to be seen to be believed.\nMap Location: Spotted Lake\n9. Abraham Lake\nAlthough Abraham Lake, in Alberta, Canada, is called a lake it is, in fact, a man-made reservoir. The twenty mile long stretch of water was created with the construction of the Bighorn Dam in the early nineteen seventies. It’s quite narrow measuring just over two miles at its widest point and with a surface area of around twenty-one square miles it doesn’t show up on any list of the world’s largest lakes. Abraham Lake has become famous for creating its own kind of magic.\nWhy go? During the harsh Canadian winter, Abraham Lake freezes over and as the ice forms it traps bubbles rising from the depths inside it. These frozen bubbles are a freak of nature caused by bacteria rotting the underwater vegetation. Getting to Abraham Lake to photograph the bubbles should be on every adventurous photographer’s list of must do’s.\nMap Location: Abraham Lake\n8. Great Slave Lake\nThe Great Slave Lake, in Canada’s Northwest territories, is ranked as the tenth largest lake in the world. What really sets it apart from other lakes though is its depth. At just over two thousand feet in some parts, it’s the deepest in North America. It’s not a lake you can explore in one day either as it measures an incredible ten and a half thousand square miles, is almost three hundred miles long and its width is over a hundred and twenty miles at the widest point. Yes, it is enormous.\nWhy go? The Great Slave Lake’s remoteness means it’s a part of the world which is pretty much untouched by progress. It really is an unspoilt wilderness which is perfect for kayaking and fishing. Parts of the lake can be frozen over for up to eight months of the year and are cut off from civilization, so be careful when you go or you could be there for much longer than expected.\nMap Location: Great Slave Lake\n7. Lake Minnewanka\nLake Minnewanka, tucked away in the mountains near the town of Banff in the Banff National Park, is thirteen miles long which makes it the second longest mountain lake in Canada. It’s surrounded by stunning snow-capped mountains and its chilling, glacial blue waters can reach depths of up to five hundred feet. The lake’s volume has also been increased over the years, with water levels rising almost a hundred feet, after dams were built to provide hydro-power to Banff. What’s so special about it?\nWhy go? After the dams were built and Lake Minnewanka’s water levels rose so dramatically, they flooded a small village, Minnewanka Landing, which was on the shoreline. Now completely submerged, it’s become an amazing scuba diving site with a whole abandoned ghost village to explore underwater. If you’re there in the winter, there’s also a chance to go ice diving if you can handle the cold.\nMap Location: Lake Minnewanka\n6. Waterton Lake\nWaterton Lake is a lake with dual nationality. It is located partly in Canada, near Alberta and partly in the US, near Montana. Although it’s split into two distinct parts, upper and lower, it’s joined by the Bosporus channel so is classed as one lake rather than two. Waterton Lake, at an altitude of over four thousand feet and with a surface area of approximately five square miles, is surrounded by national park lands renowned for their biodiversity. Its waters have an average depth of around two hundred and fifty feet, but this can change to almost five hundred feet in some places. That’s pretty deep.\nWhy go? Waterton Lake is situated in the Waterton Lakes National Park. It’s an environment so uniquely special it’s been classed as a UNESCO World Heritage Site, an International Peace Park, and a Biosphere Reserve. It’s the perfect place to really get back to nature by hiking some of the park’s amazing trails. Though if you do go, make sure you’re bear aware.\nMap Location: Waterton Lake\n5. Peyto Lake\nPeyto Lake is another of those stretches of water which will take you by surprise at the startling azure blue color of its waters. Located high up in the Canadian Rockies at an altitude of over six thousand feet, it is incorporated in the boundaries of the Banff National Park. It’s a small, but easily accessed glacial lake which measures just under two miles at its longest point, only half a mile at its widest and has a surface area of two square miles. As far as Canadian lakes go, it’s quite petit.\nWhy go? Even though Peyto Lake is up in the mountains, it’s is relatively easy to get to. Which is great if you’re not the backpacking, trailblazing, hiking type. It means you don’t have to miss out. Peyto Lake can be reached by driving through the Banff National Park on highway 93 or the Icefields Parkway to call it by its other name. Don’t feel guilty for not hiking, the photographs you take will be just as stunningly impressive as if you’d walked.\nMap Location: Peyto Lake\n4. Emerald Lake\nEmerald Lake can be found in the Yoho National Park which is in British Columbia, Canada. The lake really lives up to its name by being a stunning emerald green in color when it’s in its liquid form. High up at an altitude of over four thousand feet, Emerald Lake can remain frozen for periods as long as seven months of the year so don’t plan on going before at least July. It may be the largest lake in Yoho National Park, but its shoreline only measures just over three miles long and is completely circumvented by a hiking trail which takes around one and a half hours to fully complete.\nWhy go? Emerald Lake is in a quiet and very secluded spot but is easy to get to with a vehicle. It’s a great place for observing eagles and ospreys in their natural habitat while hiking the trail. You can also canoe across the beautiful green and tranquil waters and get a feel of how it might have been for the original indigenous inhabitants of the area.\nMap Location: Emerald Lake\n3. Garibaldi Lake\nGaribaldi Lake is a large stretch of water geographically located in British Columbia, Canada between the townships of Whistler and Squamish. Its surface area spreads for an amazing two thousand five hundred acres through the Garibaldi Provincial Park, which is a lot of lake. It has an average depth of just under four hundred feet which can plummet down as far as almost nine hundred feet in places. Garibaldi Lake can only be reached by hiking the Garibaldi Lake Trail which is around five and a half miles long.\nWhy go? The turquoise waters of Lake Garibaldi are so crystalline, the surface acts like a mirror and reflects super clear images of the surrounding landscapes. Great for photography. If you’re into winter sports like snowshoeing and backcountry skiing then this is a place you really should be aiming to visit.\nMap Location: Garibaldi Lake\n2. Lake Louise\nLake Louise is a glacial lake fed by the Lefroy Glacier and can be found in the Banff National Park, Alberta, Canada. It’s a small, but stunning lake which measures just over a mile in length, a third of a mile in width and has only a third of a square mile of surface area. With a maximum depth of around two hundred and thirty feet, it’s almost as deep as it is wide. Its close proximity to the small town of Lake Louise has made it a popular tourist attraction.\nWhy go? This is the ideal Canadian lake to visit if you’re unsure of being too far away from civilization. Spend the day wandering around the wilderness of Lake Louise’s shores, enjoy some horse riding through the forests or even rock climbing if you’ve got a head for heights. Then, rather than roughing it at a campsite, spend the night at the Château Lake Louise, the huge and very luxurious hotel built right by the water’s edge. No-one said you couldn’t visit Canadian lakes in style, did they?\nMap Location: Lake Louise\n1. Moraine Lake\nMoraine Lake nestles in a landscape which looks like the perfect picture postcard image. Pine forests, symmetrical mountain peaks and the lake’s ice blue waters shimmering between them. This impeccable stretch of glacial water is to be found in the Banff National Park, near the township of Lake Louise, in Alberta, Canada. Moraine Lake covers an area of around one hundred and twenty acres in the Valley of Ten Peaks and reaches almost fifty foot in depth in places.\nWhy go? Moraine Lake is one of the most photographed spots in all of Canada. It’s been replicated on everything from paper money to video games and even used as log-in screens for major technology companies. You’ve probably seen it digitally reproduced, so why not go and see the real thing? Believe it, it’s even better live, so make sure you don’t miss out by not going to see it.\nMap Location: Moraine Lake']"	['<urn:uuid:0c480791-4a67-40ea-a5d9-a971dbe6c6d4>', '<urn:uuid:42847a9a-736c-4b8a-af6d-7fbf0ecc364b>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T20:58:04.490895	31	92	4284
41	carne adovada boeuf bourguignon ingredients compare	Carne adovada is made with pork braised in a sauce of dried red New Mexican chiles, chicken stock, onion, garlic, vinegar, and oregano. In contrast, Boeuf Bourguignon is made with beef chuck roast cooked in Burgundy or pinot noir wine, chicken broth, bacon, onions, garlic, thyme, tomato paste, and is garnished with pearl onions and mushrooms.	['A handful of red chile peppers. Photograph by Douglas Merriam.\nI’m fired up about a cookbook celebrating the best of New Mexico’s farms and orchards. It’s a slim book filled with big ideas for fresh New Mexico quinces, figs, and apples; free-range chickens grazed on piñon nuts; and vegetable salads in dressings spiked with vinegars fermented from New Mexico wines. Of course, our state’s piquant chiles figure into many dishes, such as classic carne adovada, tender chunks of New Mexico pork braised in brick-red chile sauce. A tribute to New Mexico’s farm-to-table movement? Absolutamente. However, New Mexico Cookery, by Alice Stevens Tipton, was published about a century before today’s efforts to support local growers and seasonal agriculture.\nAs soon as New Mexico achieved statehood, in 1912, the Bureau of Publicity for the New Mexico State Land Office undertook ambitious plans for a cookbook. It was a bright scheme for its era, a volume that would chronicle the foods and dishes of the 47th state, instilling residents with a sense of pride in their heritage and agriculture, and enticing new settlers to put down roots here. In 1916, the Land Office published the remarkable tome, complete with color cover illustrating a ristra-draped ochre adobe with outdoor horno oven.\nAuthor Alice Stevens Tipton was a popular Santa Fe hostess of the era. She came to New Mexico from California with her husband, Will, a well-known surveyor, land inspector, and interpreter for the U.S. Court of Claims. From a vintage issue of the Santa Fe New Mexican newspaper, we know that both returned to her birthplace a few years after the book’s publication. In between, Tipton was a passionate and knowledgeable advocate for the foods and dishes of her adopted state.\nCookbooks of Tipton’s era typically tried to look worldly, which was the case with one of the only other New Mexican cookbooks from the period around statehood, a 1911 Ladies’ Aid Society effort. In contrast to Tipton’s book, this one featured plenty of Eastern dishes, such as lobster Creole and all manner of fancy cakes, when few New Mexican households even had indoor ovens. It offered nothing of local origin. Chile, garlic, onions—all would have been considered too coarse or “ethnic” by most of the food authorities of the day. Bostonian Fanny Farmer, the country’s leading cookbook author, would have likely run from the table if served a bowl of green or red.\nIn 1980, I stumbled onto The Original New Mexico Cookery in a cluttered Seña Plaza bookshop in Santa Fe. I had just moved here, and wanted to know everything I could about New Mexico’s food. Not until returning home and reviewing the book did I realize its timeless quality. What I had acquired was a beautifully bound reprint from a 1,000-copy print run done by Richard Polese in 1965. You can still find a few of the Polese editions around, especially in used bookstores and on websites, but Polese plans another reprint for the Centennial year. If you want a look at the original, one of the few remaining copies is in the New Mexico History Museum Library, in Santa Fe. It’ll make you hungry, as will these recipes inspired by the first New Mexico Cookery.\nBoth recipes adapted from Tasting New Mexico: Recipes from 100 Years of Distinctive New Mexico Cooking (Museum of New Mexico Press, May 2012), by Cheryl Alters Jamison and Bill Jamison.\nServes 6 to 8\nCarne adovada, as it’s usually spelled today (originally carne adobada), was initially a way to preserve and prepare pork in the winter after hog butchering. Although we no longer have to preserve the meat as was the case a century ago, the hearty braise may still be best this time of year. Carne adovada can be presented on its own, or wrapped in a snowy flour tortilla as a burrito. Some like it as a filling for enchiladas, stuffed sopaipillas, empanadas, or even omelets.\n8 ounces (20–25) whole dried red New Mexican chiles, stemmed, seeded, rinsed\n2 cups chicken stock or water\n1 medium onion, chunked\n3 garlic cloves\n2 teaspoons cider or sherry vinegar\n2 teaspoons crumbled dried Mexican oregano or marjoram\n1 teaspoon salt, or more to taste\nShredded lettuce and chopped tomato, optional\nPreheat oven to 300 degrees. Grease a large, covered baking dish. Place pork in baking dish.\nPrepare sauce: Place damp chiles in a layer on baking sheet and toast them in oven for about five minutes, until they darken just a shade. Watch chiles carefully because they can scorch quickly. Leave oven on. Cool chiles briefly, break each into two or three pieces, and discard stems and most seeds.\nPlace approximately half of chiles into a blender with one cup of stock or water. Purée until you have a smooth, thick liquid. Pour mixture into baking dish. Repeat with remaining pods and stock. Pour mixture into baking dish and stir sauce together with pork.\nCover dish and bake at 300 degrees until pork is quite tender and sauce has cooked down, about three hours. If sauce seems watery, return dish to oven uncovered and bake for an additional 15 minutes, or as needed.\nServe hot, garnished if you wish with lettuce and tomato.\nMakes about 2 pints\nQuinces ripen in the late fall and look something like portly misshapen Golden Delicious apples. Find them through the winter months at farmers’ markets and larger supermarkets. Their fragrant, almost floral scent will fill your kitchen. Quinces must be cooked to enjoy them. Quince butter, like apple butter, is a thick, jammy spread that’s great as a morning wake-up with buttered toast or yogurt. Later in the day, serve it with cheese, preferably a tangy New Mexico goat cheese or a Spanish Manchego.\nabout 2 cups sugar\nzest and juice of 1 small orange\nDice each quince (unpeeled) through core into eight pieces. Put quince pieces in a large saucepan and cover with water to just above fruit. Simmer over medium-low heat until soft, about 45 minutes. Much of water will have evaporated. Add a few more tablespoons of water if fruit begins to dry out before it is soft.\nPut fruit through a food mill to purée it, which will leave skins and core behind. Wash out saucepan that held quinces so that you can use pan again. Measure quince purée and spoon it back into pan. Add one cup of sugar per two cups of quince purée. Return to medium-low heat and continue cooking, stirring frequently, until very thick and jammy. Add zest and juice of orange, cooking another minute.\nPour into sterilized jars, cool, and refrigerate. Quince butter will keep for at least several weeks. For longer storage, can the butter by processing in a water bath according to the canning jar manufacturer’s directions.', 'C’est si boeuf\nThree classic French beef stews.\nBeef stew figures heavily in French cooking. A simple stew of beef and carrots is found throughout the country, while the various regions feature similarly prepared stews based on their wines and other local ingredients. Daube Provencale uses anchovy, orange, tomato, and olives, all common in Provence. Boeuf Bourguignon, from Burgundy, must include that area’s famous wine made from the pinot noir grape, and a garnish of pearl onions and mushrooms. Serve with buttered noodles or roasted potatoes.\nBeef Stew with Carrots (Boeuf aux Carottes)\n15 slender carrots (about 2½ pounds)\n3½ to 4 pounds beef chuck roast, trimmed of excess fat and cut into 1½-inch chunks\nSalt and pepper\n2 tablespoons flour\n1 tablespoon olive oil\n4 ounces thick-cut bacon, blanched if desired, cut crosswise into thin strips\n3 large onions, halved and thinly sliced\n4 cloves garlic, minced\n2 teaspoons minced\n3 medium bay leaves\n¼ cup brandy\n½ cup low-sodium chicken broth\n1½ cups dry white wine\n6 tablespoons chopped\nFinely chop 3 carrots and set aside. Cut remaining carrots into pieces about 3 inches long and 1 inch thick and set aside separately. In medium bowl, toss beef with 1 teaspoon each salt and pepper, and flour to coat. In large, heavy-bottomed Dutch oven, heat oil over medium-high heat until it ripples. Reduce heat to medium-low, add bacon, and cook, stirring occasionally, until crisp, about 9 minutes. Remove bacon, drain, and reserve; pour off all but 2 teaspoons of fat into a small bowl and reserve. Return pot to medium-high heat and heat until fat begins to bubble, about 40 seconds. Add half the beef, so that pieces are close together in a single layer but not touching, and cook without moving them until deeply browned on bottom, about 3½ minutes. Turn and cook until second side is deeply browned, 3½ minutes longer; transfer beef to a medium bowl. Add 2 teaspoons reserved bacon fat to pot, and repeat process with remaining beef (reduce heat if drippings begin to burn); transfer to bowl with first batch.\nReduce heat to medium, add 2 teaspoons of reserved bacon fat, and allow to heat for a moment. Add onions, chopped carrots, and ½ teaspoon salt, stir, and cook until onions soften, about 5 minutes. Add garlic, thyme, and bay leaves, and cook, stirring constantly, until fragrant, about 1 minute. Add brandy and chicken broth, increase heat to high, and, using a wooden spoon, scrape bottom of pot until brown film loosens and mixes into liquid. Add wine and reserved beef with accumulated juices, push beef down into liquid, bring to boil, reduce heat to very low, cover, and simmer for 30 minutes. Add large carrot pieces and\n1 teaspoon salt, submerge the carrots, replace cover, and continue to simmer until beef and carrots are tender, about 2 hours longer.\nAdd reserved bacon to pot and continue to simmer for 5 minutes. Remove bay leaves. Taste stew and adjust seasoning with salt and pepper, if necessary. Add 4 tablespoons of parsley, stir to combine, and serve at once, sprinkling each portion with the remaining parsley.\nProvencal Beef Stew (Daube Provencale)\nFollow recipe for Beef Stew With Carrots, making the following changes:\n1) Substitute 4 ounces pancetta, cut into ½-inch pieces, for bacon.\n2) Omit the 12 carrots cut into large pieces and reduce onions to 2, chopping them roughly.\n3) Along with the garlic, thyme, and bay leaves, add 2 tablespoons tomato paste and the zest from 1 large orange, cut into thin strips.\n4) Omit brandy. Substitute medium-bodied red wine, such as cabernet sauvignon, Cotes du Rhone, or zinfandel for the white wine.\n5) Toward end of cooking, add 4 finely chopped anchovy fillets, ½ cup black nicoise olives, pitted and halved, and 1 14½-ounce can diced tomatoes, drained, along with the reserved pancetta, stir to mix, cover the pot, increase heat to medium and cook until olives and tomatoes are heated through, about 5 minutes. Add parsley to pot as above, and serve individual portions garnished with parsley.\nBeef Burgundy (Boeuf Bourguignon) Follow the recipe for Beef Stew with Carrots, making the following changes:\n1) Omit the 12 carrots cut into large pieces and reduce onions to 2, chopping them roughly.\n2) Along with garlic, thyme, and bay leaves, add 1 tablespoon tomato paste.\n3) Omit the ½ cup brandy, increase chicken broth to 1 cup, and substitute 2½ cups Burgundy or pinot noir for the white wine.\n4) Once beef is tender, transfer it to a large bowl and set aside. Strain mixture left in pot, pressing on solids; you should have about 3 cups liquid. Discard solids and return liquid to pot, bring to boil over medium-high heat and cook, stirring occasionally, until sauce is reduced to about 2 cups, about 9 minutes; adjust the heat to low.\n5) While sauce reduces, in a large skillet over medium heat, bring another ¼ cup chicken broth and 2 teaspoons reserved bacon fat to a boil. Add 8 ounces frozen pearl onions and ¼ teaspoon salt, and cook, stirring occasionally, until tender, about 3 minutes. Adjust heat to medium-high and cook, stirring frequently, until liquid evaporates and onions darken slightly, about 3 minutes longer. Add 1 teaspoon reserved bacon fat and 1 pound halved crimini mushrooms, and cook, stirring occasionally, until their liquid evaporates and they brown, about 10 minutes. Add onions and mushrooms to the bowl with the beef. Return skillet to medium-high heat, add ½ cup wine and, using a wooden spoon, scrape bottom of skillet until brown film loosens and mixes into the liquid. Add liquid to the sauce in the pot.\n6) Add beef, onions, mushrooms, reserved bacon, and 1 tablespoon brandy to the sauce in the pot, adjust heat to medium, and cook, stirring occasionally, until heated through, about 10 minutes. Taste stew and adjust seasoning with salt and pepper, if necessary. Add parsley to pot and portions as previously.\nSend comments or suggestions to Adam Ried at email@example.com.']	['<urn:uuid:669de21c-78cc-4f59-b624-e62825d05245>', '<urn:uuid:a03cfdb5-b620-4f3e-9316-385f579ea4b7>']	open-ended	direct	short-search-query	similar-to-document	comparison	novice	2025-05-12T20:58:04.490895	6	56	2135
42	benchmarks inventory turnover ratio high efficiency fmcg vs manufacturing industry compare	In FMCG (Fast Moving Consumer Goods) industry, inventory turnover ratio is typically higher because goods are cheap, consumed fast, and perishable. According to industry benchmarks, manufacturing has a lower inventory turnover ratio of 2-3 times per year, as they deal with larger, more costly items.	['Asset management ratios are a group of metrics that show how a company has used or managed its assets in generating revenues. Through these ratios, the company’s stakeholders can determine the efficiency and effectiveness of the company’s assets management.\nDue to this, they are also called turnover or efficiency ratios. As the name suggests, these ratios usually consider only two factors, a company’s assets and revenues.\nSince companies have various types and classes of assets, there are also different ratios for different assets. Some of the most commonly used asset management ratios include inventory turnover, accounts payable turnover, days sales outstanding, days inventory outstanding, fixed asset turnover, receivable turnover ratios, and cash conversion cycle.\nThe purpose of why stakeholders calculate asset management ratios depends on the type of stakeholder. Usually, asset management ratios are crucial for investors and shareholders.\nThrough these ratios, they can calculate the efficiency and effectiveness of their investments. Usually, the better these ratios are, the higher the chances of investors and shareholders investing in the company.\nFurthermore, these ratios allow stakeholders to analyze the financial performance of a company from multiple aspects. Usually, stakeholders prefer companies with higher profits.\nHowever, judging companies by the amount of their profit is not suitable for comparisons. Similarly, just because companies can make profits doesn’t mean they are using their assets effectively.\nTherefore, asset management ratios can help with all these aspects and much more.\nAssets Management Ratios\nSome of the most commonly used asset management ratios are as below.\n1) Total Asset Turnover\nThe Total Asset Turnover is a ratio that measures the efficiency of a company in the use of all its assets to produce sales. It gives a summary of all the asset management turnover ratios.\nThe higher the company’s asset turnover ratio is, the better and more efficient it is considered by stakeholders.\nAsset turnover = Sales / Total Assets\n2) Fixed Asset Turnover\nThe Fixed Asset Turnover is a ratio that measures the efficiency of a company in the use of only its fixed assets to produce sales.\nThis ratio only considers the use of long-term assets as compared to short-term. While it is a type of asset turnover ratio, some stakeholders prefer only to consider the company’s fixed assets to evaluate its efficiency.\nAsset turnover = Sales / Fixed Assets\n3) Net Working Capital Turnover\nThe Net Working Capital Turnover is a ratio that signifies the efficiency of a company in using its working capital. It is also a type of asset turnover ratio.\nHowever, this ratio is usually a better indicator of a company’s operations than using total assets. The higher this ratio is for a company, the better it uses its working capital to generate revenues.\nNet Working Capital Turnover = Sales / Net working capital\n4) Inventory Turnover Ratio\nAnother asset management ratio that is considered critical is the Inventory Turnover Ratio. It shows how many times the company has sold and restocked its inventory within the accounting period under consideration.\nWhile a higher Inventory Turnover Ratio is preferable, it can also indicate a risk of stock outs if it is too high. A low ratio, on the other hand, may indicate slow-moving inventory.\nInventory Turnover Ratio = Net Sales / Inventory\n5) Days Sale in Inventory\nAnother inventory-related asset management ratio is the Days Sale in Inventory. It is expressed on many days. It calculates the time it takes a company to sell off all its inventory.\nSimply put, it shows how much time it takes the company to convert stock into sales. For this ratio, the lower the number of days, the better it is.\nDays Sales in Inventory = 365 (days) / Inventory turnover\nAnother formula to calculate the ratio is as follows.\nDays Sales in Inventory = Inventory / Cost of Goods Sold x 365 days\n6) Receivables Turnover\nReceivables Turnover is a ratio that measures how many times a company collects its accounts receivable balances. This ratio depends on the credit policies of a company. A higher Receivables Turnover ratio means the company collects its credits promptly.\nReceivables Turnover = Sales / Accounts Receivable\n7) Days Sales Outstanding\nThe Days Sales Outstanding ratio measures the efficiency of a company in recovering its receivables. Similar to the above ratio, the DSO expresses the result in many days.\nA lower DSO means that a company is recovering its receivables in a short amount of time. Shorter receivable collection periods can also be beneficial in avoiding bad debts. Other names for this ratio are Average Collection Period or Days Sales in Receivables.\nDays Sales Outstanding = Accounts Receivable / Sales x 365 days\n8) Payables Turnover Ratio\nAccounts payable aren’t the assets of a company. Nonetheless, they are a part of a company’s working capital management.\nThe Payables Turnover shows how quickly a company makes payments to its suppliers for credit purchases. A high ratio means that the company pays its bills in a short amount of time. A low ratio may be an indicator of cash flow problems.\nPayables Turnover Ratio = Purchases / Accounts Payable\nAsset management ratios are highly significant in their importance. First of all, these ratios help determine the efficiency and effectiveness of a company.\nWithout these ratios, making comparisons between the performance of various companies becomes complex. Similarly, asset management ratios play a significant role in helping investors in making decisions.\nSimilarly, these ratios can indicate a company’s performance in a specific area, for example, working capital management.\nTherefore, they can also help companies control their assets and generate the maximum possible revenues for the limited resources they have.\nAsset management ratios also signify the fact that only considering a company’s revenues is not crucial. It is also critical to take its usage of assets into account.\nThere are several limitations of asset management ratios as well. Most importantly, these ratios consider the revenues of a company and neglect its profits.\nWhile generating higher revenues is critical for companies, they must also weigh their profits. Generating higher revenues while lacking the capacity to convert them into profits is futile.\nSimilarly, these ratios consider the historical information of a company. It means investors must also consider other aspects of the business of a company along with these ratios.\nFurthermore, these ratios may return inaccurate results sometimes, for example, when companies have assets they don’t use anymore. Lastly, these ratios are also subject to manipulation due to their dependency on information obtained from financial statements.\nTips to Increase Asset Management Ratios:\nAsset management ratios are financial ratios that help to measure a company’s effectiveness in managing its assets to generate profits.\nHere are some tips to increase asset management ratios:\n- Increase asset turnover: The asset turnover ratio measures how efficiently a company uses its assets to generate revenue. To increase asset turnover, a company can reduce inventory, sell unproductive assets, and increase sales.\n- Improve inventory management: Effective inventory management can help a company reduce excess inventory, minimize waste, and optimize production processes. This will lead to improved asset management ratios.\n- Reduce accounts receivable turnover: Accounts receivable turnover measures how effectively a company collects its accounts receivable. To improve this ratio, a company can implement stricter credit policies, incentivize early payments, and automate invoicing processes.\n- Optimize fixed assets: A company can optimize its fixed assets by maintaining them properly, minimizing downtime, and reducing maintenance costs. This will improve asset utilization and efficiency.\n- Increase cash flow: A company can increase its cash flow by optimizing its working capital, reducing debt, and increasing revenue. This will lead to improved liquidity and asset management ratios.\n- Implement effective cost control measures: Effective cost control measures can help a company reduce unnecessary expenses, improve profitability, and increase its asset management ratios.\nTips to Decrease Asset Management Ratios\nAsset management ratios are essential metrics used to evaluate the efficiency of a company using its assets to generate revenue.\nLower asset management ratios indicate that a company is using its assets more effectively, which can improve profitability.\nHere are some tips to decrease asset management ratios:\n- Increase sales revenue: A company can increase its sales revenue by improving its marketing strategies, expanding its product or service offerings, and/or entering new markets. By generating more revenue, a company can increase its asset turnover ratio, which measures how efficiently it uses its assets to generate revenue.\n- Reduce inventory levels: More inventory ties up a company’s working capital and can lead to obsolescence and waste. By reducing inventory levels, a company can improve its inventory turnover ratio, which measures how efficiently it turns over its inventory.\n- Improve collection of receivables: A company can improve its collection by implementing stricter credit policies, offering discounts for early payments, and actively following up on overdue amounts. By collecting receivables more quickly, a company can improve its accounts receivable turnover ratio, which measures how efficiently it contains customer payments.\n- Sell unproductive assets: Selling ineffective or underutilized assets can free up working capital and improve asset turnover ratios. A company can also lease or rent assets rather than purchase them outright.\n- Reduce fixed costs: Fixed costs, such as rent, salaries, and insurance, can reduce a company’s profitability. By lowering fixed costs, a company can improve its return on assets (ROA), which measures how efficiently it uses its assets to generate profits.\nWhat Are the Risk of Not Maintaining Asset Management Ratios Effectively?\nAsset management ratios are essential tools for evaluating the financial health of a business or organization.\nThey provide insight into how well the company utilizes its resources, manages its debts, and generates profits.\nFailure to maintain these ratios effectively can lead to a variety of risks for the company, including:\n- Poor financial performance: If the asset management ratios are maintained effectively, it can result in better financial performance. For example, suppose the company needs to manage its assets more efficiently. In that case, it may have too much inventory or too many receivables, leading to cash flow problems and poor financial performance.\n- Decreased liquidity: Effective asset management ratios can ensure the company has enough cash to cover its obligations. If these ratios are not maintained properly, the company may not have sufficient liquidity to meet its short-term obligations, such as paying suppliers or employees.\n- Increased debt: Poor asset management ratios can also lead to increased debt levels for the company. For example, suppose the company needs to generate more profits to cover its expenses. In that case, it may need to borrow more money to stay afloat, which can lead to an unsustainable debt burden.\n- Reduced profitability: Effective asset management ratios can increase profitability by optimizing the use of company resources. If these ratios are maintained properly, the company may be able to maximize its profits, which can lead to reduced profitability and, potentially, lower shareholder value.\n- Poor investment decisions: If asset management ratios are not maintained effectively, it can also lead to poor investment decisions. For example, if the company is not managing its assets efficiently, it may need more resources to invest in growth opportunities, limiting its potential for long-term success.\nAverage Asset Management Ratio by Industries\n|Industry||Asset Management Ratio||Benchmark|\n|Retail||Inventory turnover ratio||6-8 times per year|\n|Manufacturing||Fixed asset turnover ratio||2-3 times per year|\n|Real Estate||Gross rental yield||5-8%|\n|Healthcare||Bed utilization rate||65-85%|\n|Transportation||Asset utilization rate||60-70%|\n|Technology||Return on assets (ROA)||10-15%|\n|Agriculture||Crop yield ratio||Varies widely by crop type|\n|Energy||Production efficiency ratio||Varies widely by energy source|\nAsset management ratios are necessary for the evaluation of the efficiency and effectiveness of a company. These ratios indicate how well a company uses its assets in generating revenues.\nThe purpose of these ratios depends on the user. Asset management ratios are of significant importance, although they may have some limitations.', 'Inventory turnover ratio is an important financial ratio to evaluate the efficiency and effectiveness of inventory management of the firm. This ratio indicates how many times inventory is sold and replaced in a financial year. In other words, the ratio gives the frequency of conversion of inventory into sales in a given financial year.\nThis ratio also talks of the effective utilization of the working capital, especially the capital that inventories block. Optimum utilization of working capital may reduce the working capital requirement and thereby save interest cost to the firm.\nTable of Contents\nHow to Calculate Stock / Inventory Turnover Ratio?\nInventory turnover ratio is a simple relationship between average inventory and cost of goods sold. With these data in hand, the calculation of inventory turnover is very simple which is as follows:\nFormula for Inventory Turnover Ratio\nStock / Inventory Turnover Ratio = Cost of Goods Sold / Average Inventory\nThe dark side of the calculation is non-availability of required data i.e. Cost of Goods Sold and Average Inventory.\nThe cost of goods sold is normally not a part of financial statements which is a practical difficulty for an analyst. Analysts normally use sales figure in lieu of the cost of goods sold. Logically speaking, the ratio should not give correct result in that case and the reason is that the sales are based on market value and inventory is based on cost. So the two figures are not comparable to each other.\nAverage inventory should be taken as the average of opening inventory balance of all 12 months. This method will smoothen the seasonal effect in inventory levels and normalize the data. Like the cost of goods sold, availability of such detailed data for analysis is difficult many a times and therefore in the absence of that, an average of yearly opening and closing inventory is taken for the calculation.\nInterpretation of the Ratio\nStock/inventory turnover ratio indicates how frequently the inventory is replaced. The ratio provides an absolute figure. Let’s understand with an example as to what it conveys. If the result of the ratio is 4, it means the complete investment in inventory is sold 4 times a year or we can say it is sold every 3 months in a year.\nNormally, higher this ratio better is the inventory management. It is quite simple to understand. We have invested say ‘x’ amount of working capital in inventory. Interest cost on ‘x’ would remain fixed for the whole year irrespective of how many times that money is used to purchase inventory. So, it is obviously desirable to have higher ratios. Higher frequency of turnover would mean effective utilization of working capital funds.\nWhat is Desirable – A Higher or a Lower Stock / Inventory Turnover Ratio?\nJust like every other thing in life, a balance is also required here. Too high or too low of such ratio is not considered a good sign. Let’s see how.\nVery High Inventory / Stock Turnover Ratio: Normally a higher ratio would be recommended as that would mean effective rolling up of working capital. At the same time, very high of such ratio would have a different meaning also. That may signal very low level of inventory being maintained. Too low inventory means too frequent orders. The manager should make sure that the additional ordering cost incurred should not outperform the savings of interest on working capital.\nVery Low Inventory / Stock Turnover Ratio: Needless to explain, a very low turnover ratio of inventory will not utilize the fixed interest cost incurred on investment in inventory as explained in the above example.\nBenchmark or Ideal Ratio\nBenchmark for inventory turnover ratio depends on the industry. A ratio which is considered good in one industry may be bad for the other. For example, FMCG goods would normally have higher inventory turnover ratio because the goods are cheap and are consumed very fast and on the top they are perishable also. On the other hand, big machinery which is costly in nature would always have a lower inventory turnover ratio. The best way to benchmark this ratio is to compare the concerned company ratio to the average of its respective industry in which it falls.Last updated on : June 30th, 2018']	['<urn:uuid:8ccb4e6a-b9c5-484b-9e84-3a6cd298b4e2>', '<urn:uuid:aaab45d7-ae7a-4e49-a226-aa062bf2f537>']	factoid	with-premise	long-search-query	distant-from-document	comparison	expert	2025-05-12T20:58:04.490895	11	45	2671
43	what preparation differences between game duck processing and thanksgiving turkey seasoning overnight	Game ducks and thanksgiving turkeys have distinct preparation methods. Ducks should hang at a temperature just above freezing for at least 48 hours and may need soaking in salted water with baking soda for 2-3 hours to remove strong flavors. In contrast, the turkey preparation involves rubbing a spice mixture (sage, zest, salt, pepper) under the skin over the breast, legs, thighs, cavity and wings, then refrigerating uncovered overnight. For ducks, you can also place onion slices in the cavity to remove excess gamey taste, while turkey requires vegetables (carrots, celery, onion) to be placed in both the cavity and roasting pan.	"['Game birds offer the most varied and perhaps the most delicious wild\nmeat. Ranging from the rich, tangy flavor of the miniature woodcock up\nto a magnificent wild turkey or Canada goose, they provide a range of\nflavor delicacy as wide as the variation of the sport in hunting for\nthem. The quality and flavor of game birds, however, depends to a very\nlarge extent, on the care they receive after the hunter has bagged them.\nThe simple rules to follow are these: The birds should be drawn soon after they have been shot. The body heat should be allowed to cool as quickly as possible. The birds should be kept cool or at cold temperatures until they are to be cooked. Game birds should be bled, cleaned and cooled quickly after shooting. And as you clean them, be sure to remove the oil sacs at the base of the back near the tail. Also be sure to carry a portable ice chest to speed cooling and to protect the birds from spoilage during the trip home.\nWhen testing game birds to determine those which are young and tender, the stiffness of the bill is usually significant. If pheasants and grouse, for example, can be lifted by the lower jaw and nothing breaks, they are mature birds whose james are set. They will not be as tender and will require more cooking than the younger, less developed.\nGame birds should be skinned if only the breast will be used or if they are tough and will be used in stews or casseroles. Otherwise, the birds should be plucked. This helps keep the meat more moist and tender.\nBe sure you remove any shot pellets and cut away any badly shot up areas. Cut off the wings and feet of small birds with shears. Then, cut small birds up the backbone, remove the lungs, wash and drain.\nCut larger birds into pieces, the same as you would a chicken. You\'ll also find the livers from medium and large-sized birds are big enough to save and will taste very similar to chicken livers.\nHere\'s another hint. Freezing a bird for a week or two will help tenderize it.\nNOTE about DUCKS: In the fall, ducks usually have fine- flavored meat, and any stuffing can be used with them. At other times of the year, they may be more strongly flavored and are improved by soaking the cleaned birds for 2-3 hours in fairly strong salted water to which 1 tsp. baking soda has been added. If ducks prepared this way are to be kept under refrigeration for a few days, after wiping them dry, put a few slices of onion in the body cavity. This will help remove the excess gamey taste, and the onion is to be discarded before the ducks are cooked.\nHowever, like all game birds, ducks should be allowed to hang at a temperature just above freezing for at least 48 hours before they are cooked. The length of time and temperature at which they are allowed to hang beyond that period will control how ""high"" or gamey they are allowed to become. This should be determined by personal taste.\nWhen preparing game birds, you can cook young birds by broiling, roasting, or in any of your other favorite recipes. But older birds should be stewed or braised to tenderize them. Or if you wish, you can try a commercial tenderizer. Just sprinkle the tenderizer in the body cavity of the bird and let the bird stand in the refrigerator. The amount of time the bird needs to remain in the refrigerator depends on the size of the bird. For example, a large bird such as a turkey, will need 12 to 24 hours for the tenderizer to work.\nIf you\'re not sure how many servings you\'ll get from each bird this may help you: *1 serving = 2 quail, 1-2 squab, 2-3 doves, or 1 small duck. *You can figure on at least 2 servings from 1 pheasant or 1 large duck. *A 4-6 lb. goose should feed 4-6 people.', 'Season the turkey\nIn a medium bowl, mix the sage, zest, 1 oz. salt, and 1 Tbs. pepper.\nRemove the tail, neck, heart, and gizzard from the turkey and reserve for making turkey broth. Discard the liver. Remove and discard the plastic timer and any metal or plastic leg holders. Rinse and pat the turkey dry.\nRub the spice mixture under the turkey’s skin over the entire breast, legs, and thighs, as well as in the cavity and over the wings. Set on a platter or pan large enough to hold the turkey and refrigerate uncovered overnight.\nRoast the turkey\nTip:Let your turkey rest for 30 to 40 minutes before carving—the juices will redistribute into the meat, making it moist and tender. It also gives you time to finish preparing the meal.\nPosition a rack in the bottom of the oven and heat the oven to 400°F. In a large bowl, toss the carrots, celery, and onion with the oil. Put half of the vegetables in the center of a large flameproof roasting pan and put the rest in the turkey cavity. Tuck the wings behind the turkey’s neck and tie the legs together with twine. Set a V-rack in the roasting pan over the vegetables. Put the turkey breast side down on the V-rack. Roast for 1 hour.\nRemove the pan from the oven and baste the turkey back and sides with some of the pan drippings. With silicone oven mitts or two wads of paper towels, carefully turn the turkey breast side up and baste with more pan drippings. Continue to roast the turkey until an instant-read thermometer inserted in the thickest part of the thigh registers 175°F, an additional 1 to 1-1/2 hours. During this phase, check the vegetables in the pan every 20 minutes or so: They should be brown, but if they or the drippings threaten to burn, add about 1/4 cup water—you may need to do this several times.\nWhen the turkey is done, protect your hands with silicone oven mitts or wads of paper towels and tilt the turkey so the juices in the cavity run into the roasting pan. Transfer the turkey to a carving board and let it rest for 30 to 40 minutes. Carve when ready to serve.\nMake the gravy\nTip:For lump-free gravy, gradually whisk the broth into the roux. The liquid will thicken quickly and get gluey, so keep whisking in more broth, a bit at a time, until the gravy is smooth.\nWhile the turkey is resting, set the roasting pan over medium-high heat. Add the wine and cook, using a wooden spoon or heatproof spatula to loosen the brown bits, until reduced by about half, 2 to 3 minutes. Strain the contents of the roasting pan into a bowl, pressing on the solids to release the flavorful drippings. Discard the solids.\nIn a 1-quart liquid measuring cup, combine the broth with 1-1/2 cups water. Tasting as you go, add enough of the pan drippings to the broth to make a flavorful yet not overly salted liquid—you may or may not use all of the drippings. Let sit until the fat rises to the surface. Skim off and reserve as much fat as possible.\nMeasure 4 Tbs. of the fat into a medium saucepan (supplement with olive oil if necessary) over medium heat. Whisk in the flour. Cook, whisking almost constantly, for 2 to 3 minutes. Whisk in about 1/2 cup of the broth. As soon as the broth thickens, whisk in another 1/2 cup. Repeat until the mixture stays relatively smooth, at which point you can whisk in the remaining broth. Bring to a simmer and cook, whisking frequently, for 5 to 10 minutes to develop the flavor. The gravy will be on the thin side; if you prefer it thicker, continue simmering until thickened to your liking, but expect the flavor to concentrate as well. Season to taste with salt and pepper. Pour into a gravy boat and serve with the carved turkey.\nProvençal Roast Turkey with Red Wine Gravy: For the dry rub, use 3 Tbs. herbes de Provence, 2 Tbs. finely grated orange zest, 4 tsp. fennel seeds (crushed), 1 oz. kosher salt, and 4 tsp. freshly ground black pepper. Substitute 1/3 cup dry red wine for the white wine or vermouth.\nnutrition information (per serving):\nsat fat g\nPhoto: Scott Phillips']"	['<urn:uuid:58284f2f-7436-4fa5-a46a-617686a0bbbc>', '<urn:uuid:2ab98b8a-7964-4399-b9b9-203bac559ae0>']	open-ended	with-premise	long-search-query	similar-to-document	comparison	novice	2025-05-12T20:58:04.490895	12	102	1412
44	What are the benefits of organizational planning and risks of cultural mistakes?	Organizational planning benefits companies by creating sustainability and longevity, revealing vulnerabilities, and helping maximize profit. It ensures departments work together efficiently and helps break large goals into achievable steps. However, cultural mistakes in international business can lead to severe risks and losses. These include financial losses (like the billion-dollar lawsuit against DaimlerChrysler), failed projects (as seen in the case of teams in Israel and Japan), and damaged business relationships. Companies can face three major types of failure: inability to cross cultural gaps, ineffective communication leading to excessive competition, and limited collaboration due to lack of trust.	"[""Planning is important to the inception, maintenance and sustenance of any business, regardless of industry. The purpose of planning is partly to ensure a successful business launch, as well as to maintain operations, production, marketing, investments and growth. Each stage of business growth and department of the business requires different kinds of planning in order keep from getting off track. These plans include short, medium and long term plans that fit under the larger vision of the business.\nPlanning Impacts Operations\nPart of the purpose and importance of planning is to create a picture of a successful future for the operations of the company. Before you draw your formal plan, begin by taking an inventory of your current operations, through studying the external and internal factors impacting the business. Market research and internal audits help provide the data necessary to determine what it will take to bring the company from where it is now to where it wants to be.\nMost businesses use this information to increase efficiency and produce more at the same rates or produce the same output at lower rates. It is also important to consider ways to reach new customers, as well as ways to increase morale and motivate employees to deliver more.\nThe Importance of Planning\nPlanning helps the departments in your organization to work in synchronicity, like a well oiled machine. The right hand knows what the left hand is doing because it is laid out in the plan and everyone is clear on it. Planning within an organization must include:\n- every department\n- including production\n- human resources\nIt gives everyone clarity about how they fit into the larger picture of the company so that they can effectively communicate and focus their work efforts on what matters most, while tuning out what is not essential.\nPlanning for Different Seasons\nBusinesses need different plans for different seasons, much like individuals change their recreational activities and clothing to match the seasons. New companies need a solid start-up plan that leaves plenty of wiggle room in finances in order to create a solid foundation for the company to grow on. To do this, you want to research the feasibility and viability of the venture you want to engage in and make sure you have enough money to get started, plus a little extra.\nOnce your business has been up and running a while, you will need a new plans and strategy in order to maintain your business efficiently. Your business as a whole will need a plan, and then each department will need its own internal plan that contributes to the greater vision. Planning within an organization and within each department happens in conversation so that these departmental plans fit together like puzzle piece that make a greater whole.\nIn order to grow and expand, your business needs a strategic plan based on market research. The purpose of planning at this state is to help your organization grow at a rate and in a way that maximizes profits and volumes while reducing risk. Your strategic plan will help you diversify your product range and venture into new markets at a rate that keeps your profit margins healthy.\nSMART Planning for Success\nJust like personal goals and plans must be specific, measurable, achievable, relevant and time-bound (SMART) in order to be effective, business plans must be, too. Good planning within an organization keeps in mind that a plan is crafted for a certain amount of time, like a day, month or quarter. While these short term plans are important, it is equally important to lay out medium-range plans for the next one to three years, as well as long-term plans that look three to five years in the future.\nSpecifying the time period a plan applies to is important for meeting goals. For instance, if a business says it wants to see 20-percent growth but does not say when it wants to achieve that goal, it's like trying to shoot in the dark and it will probably never happen. It is merely a wish. However, if the plan says it wants to see 20-percent growth within the next three years, it gives everyone something to aim for. The one year plan can be crafted based on that goal and then the monthly, weekly and daily goals can be based on that in order to create achievable baby steps that keep the company moving in a positive direction.\nBenefits of Business Planning\nThe importance of planning cannot be overstated in terms of creating sustainability and longevity for a company. If you do not lay out where you are going and how to get there, you will probably never arrive. Solid business plans break your big goals and vision into workable actions that are clear for every person and department within your organization. Business planning helps to cover your blind spots, reveals your vulnerabilities and equips you to figure out how to best minimize them so that you can maximize profit.\n- house plans image by Christopher Hall from Fotolia.com"", 'Article: International Business — The Cost of Not Being Prepared\nWhat’s the cost of not being familiar with a foreign culture your company is engaging in?\nHow about a billion dollars? After all, that’s how much American mega-investor Kirk Kerkorian sued DaimlerChrysler for after their German chairman, Jürgen Schrempp, had bragged in a Financial Times interview that the merger between the two companies, officially promoted as a ‘merger of equals’, was really no more than a takeover. The case is still in court, but a similar class-action suit by other investors has already been settled by the company for $300 million. Technically, the issue was a legal one. Practically, however, what got DaimlerChrysler into trouble was that Schrempp lacked the cultural sensitivity and experience to realize that in the US, one simply won’t get away with that kind of two-faced behavior. The same act would expectably have much less dramatic consequences in his home country. In fact, it didn’t have any.\nMicrosoft reported losing several millions of dollars in India, the Arab world, and in South America because of cultural mistakes in some versions of their Windows program. Incorrect maps, poor translations that introduced offensive language, and other inappropriate material offended locals and in some cases led to government action. The company had to recall the affected versions, replacing huge quantities of its software packages. A spokesman admitted that ""some of our employees, however bright they may be, have only a hazy idea about the rest of the world"". As a consequence, Microsoft now sends their staff to dedicated training classes.\nA large high-tech corporation lost more than $10 million in development costs and missed market opportunities when they set up two of their international teams, one in Israel and one in Japan, to directly compete with each other in the same project, developing an important new product. What the division’s manager was not aware of was that in many cultures, such an approach sends a message to the team that it is incompetent and cannot be trusted. Rather than serving as a motivator as it might have in the U.S., the decision led to low morale, increased turnover, and poor results in both countries. The project had to be stopped and re-initiated.\nFortunately, most cross-cultural blunders are less severe, or at least less costly, than in these\nexamples. Nevertheless, the list still goes on and on about how businesses waste money and miss opportunities\nbecause of a lack of international experience or preparation.\nWhat Goes Wrong\nThere are three fundamental ways in which international business interactions and engagements fail or become more costly than they ought to be:\nFailure to cross the culture gap. The interaction falls apart because the parties involved are unable to bridge the culture gap between them. Many negotiations end at this stage. “They asked way too much”, “they expected us to accept the short end of the stick”, or “you just couldn’t trust these guys – they never lived up to their promises” are statements one might hear at the end of such failed attempts. Most of the time, these can be traced back to poor mutual understanding and faulty initial assumptions rather than bad intentions on either side.\n“Coopetition”. The cross-cultural interaction limps along, but the parties involved fail to communicate effectively and to build sufficient trust between them. As a result, the competitive element outweighs the cooperative one, introducing issues over contracts terms, intellectual property, budgets and payments, and so on. This case is both more common and more devastating than the previous one. Rather than adding value to a company’s global business strategy, such an engagement can become a major distraction from its key objectives and cause a lot of damage.\nLimited collaboration. The parties involved establish reasonable ways\nto communicate and interact. However, they never fully trust each other. In many foreign cultures, people\nwill not make any major business commitments unless a strong business relationship has been established and they feel\nthat the partner can be fully trusted. Americans may be more at ease here because its culture encourages a\ncompetitiveness that maintains an element of rivalry business partners are used to. Dealing with foreign partners\nthus represents a bigger challenge if the goal is to achieve extensive collaboration.\nSix elements can be identified that make or break the success of a global business interaction. All of them are ultimately linked back to people’s cross-cultural understanding (or lack thereof).\n#1 Strategic Objectives\nMany international business interactions suffer from poorly defined objectives. Strategy, goals and approach all need to be set with the target culture(s) in mind. Only if strategic objectives and tactics are well aligned with the other culture’s values, strengths, and preferences can a long-term gain be realized.\nLike any other aspect of running a business, success in cross-cultural interactions requires properly planning the approach. Strategic objectives need to be translated into a plan of action that defines steps, timing, roles, and responsibilities. That plan must also take into account the specific preferences and sensitivities of the targeted culture. Ad-hoc approaches in foreign countries have a very limited chance of success.\nNegotiating in a different cultural context is one of the toughest challenges in international business. What is effective and what is considered inappropriate varies greatly between countries. At the same time, the stakes are usually high and mistakes costly. Sending one’s best and most skilled negotiators won’t help much unless they are well-prepared. If they lack a thorough understanding of the other culture, the company may be in for a business disaster.\n( For more on this subject, see International Negotiation: How Do I Get Ready? )\nOnce a cross-cultural engagement is under way, visionary leadership becomes pivotal. Leaders will need to consistently demonstrate that they are serious about the engagement and willing to work through the cultural differences. That takes a strong commitment as well as the skills needed to identify sensitive areas and to act appropriately to build and maintain trust. Executives or middle managers who maintain an “us-versus-them” attitude can cause huge damage. Extensive communication, both within the own camp and with the foreign side, is also essential and requires constant leadership attention.\nThe importance of relationship and trust building triggers a need for proper facilitation throughout the engagement. While early in the interactions senior leaders often drive the progress, they may have to become less involved once the engagement is under way. At that point, it becomes essential that a facilitator be assigned who continues to build the relationship. Sending an expatriate who lives in the foreign country can be very effective, but only if he or she is sensitive and well familiar with the specific culture. Companies not paying attention to this aspect frequently find their employees inadvertently triggering confrontations that hurt the business relationship.\n#6 Team Preparation\nWell-defined strategy and good leadership are not enough to make global business interactions successful. Getting\nbuy-in from all team members involved is also essential. Without proper preparation for the engagement, cooperation\nwill likely be poor and concerns may prevail. The objective has to be to get both sides into the right mindset,\nopening up to the engagement as an opportunity rather than viewing it a threat. Again, it will be very important\nto understand and address any cultural differences. Aspects such as how to motivate a team can differ significantly\nand may dictate a new approach in a foreign culture.\nAs Globalization accelerates business around the world, companies are realizing that proper preparation for international business is a mandatory step that has a strong positive impact on the bottom line. Effective communication and trust building are the primary factors in making a foreign engagement successful. They are influenced by several elements that take careful planning and orchestration. While this requires significant efforts, it is critical to the business success, and the tradeoff between costs and benefits is clearly favorable.\n|Printable PDF version||written by Lothar Katz|\n( Copyright 2004-2015, Leadership CrossroadsTM )']"	['<urn:uuid:cf1c97af-597e-45dd-8531-39057834e840>', '<urn:uuid:14c0a04b-d804-465e-ab9d-768ebf6debc0>']	open-ended	direct	concise-and-natural	similar-to-document	multi-aspect	novice	2025-05-12T20:58:04.490895	12	96	2169
45	scotland floating wind development assessment reports impacts	Scotland's offshore wind development includes the world's first floating wind farm (Hywind) near Peterhead, with extensive projects on the east coast exceeding 4.5 GW. Research has shown that developers sometimes under-assess wind farm impacts on nearby residents, particularly regarding visual effects, shadow flicker, and noise. The study by ClimateXChange examined ten wind farms across Scotland and found discrepancies between predicted and actual impacts, highlighting the need for improved assessment practices and public engagement.	['Offshore wind energy\nScotland has a great deal of potential offshore wind resources. Its strong offshore winds provide the ideal conditions for technology which can harness this powerful resource. The east coast seabed has been identified as a particularly suitable location for the development of offshore wind due to the gently shelving nature of the seabed. However, there has been an emergence of deeper water offshore technology which can help secure the sustainable development of deeper waters both in the east and west of the country.\nScotland is home to the world’s first floating wind farm (Hywind) around 30KM from Peterhead and the Beatrice project, in the Moray firth recently became the largest operational offshore wind project in Scotland. Scotland has in excess of 4.5 GW of offshore wind projects consented on the east coast and a new leasing round for offshore wind was announced by Crown Estate Scotland in 2017. The planning process for that round is ongoing and leasing is expected to begin in 2020.\nDraft Sectoral Marine Plan for Offshore Wind Energy\nIn November 2017 Crown Estate Scotland announced its intention to run a further leasing round for commercial-scale offshore wind energy. In accordance with the Scotland’s National Marine and in our capacity as planning authority for Scotland’s seas, the spatial locations for this and any future leasing round will be identified through a sectoral marine planning process, completed by Marine Scotland.\nConsultation on the initial scoping work to identify Areas of Search and the statutory assessment criteria was held between June and July 2018. Responses from that consultation have helped develop the planning process and a draft Sectoral Marine Plan for offshore wind energy was consulted upon from 18 December 2019 until 25 March 2020.\nThe draft Sectoral Marine Plan for Offshore Wind Energy aims to identify the most sustainable options for the future development of commercial-scale offshore wind energy in Scotland and is supported by detailed assessments of the potential impact of the draft Plan. This includes a Strategic Environmental Assessment, Habitats Regulations Appraisal, Social and Economic Impact Assessment, Regional locational Guidance and several smaller assessments. For more information, the draft Plan and assessments are available online and the full consultation can be found online at Citizen Space\nThe draft Plan is expected to be adopted later in 2020 and will allow the Leasing process, managed by Crown Estate Scotland to close shortly following formal adoption of the final Plan.\nPrevious Offshore Wind Initiatives\nBlue Seas - Green Energy: A Sectoral Marine Plan for Offshore Wind Energy in Scottish Territorial Waters\nOur vision for developing offshore wind energy up to and beyond 2020 was launched in the two-part publication Blue Seas - Green Energy: A Sectoral Marine Plan for Offshore Wind Energy in Scottish Territorial Waters.\n- Blue Seas - Green Energy: Part B - Post Adoption Statement\n- Economic Assessment of Short Term Options for Offshore Wind Energy in Scottish Territorial Waters\nThis publication was developed following a successful consultation and informed by a substantial evidence base of possible social and environmental interactions with offshore wind development.\nThree projects delivered from Blue Seas – Green Energy have now progressed through the consenting process. Inch Cape and Neart na Gaoithe are both located in the Forth and Tay region whilst Beatrice, in the Moray Firth, was the first of these projects to be built (2019).\nTwo other projects, delivered through the UK SEA 2 process have also progressed through the consenting process. Moray East and West are located in the Moray Firth while SeaGreen is located farther offshore in the Forth and Tay Region.\nDraft Sectoral Marine Plan for Wind (2013)\nIn 2013, a Draft Sectoral Plan for progressing ten medium-term option areas was developed and published for consultation. In December 2014, Scottish Ministers decided not to progress two of the option areas located in South West Scotland and these were removed from the draft Plan. This Plan remains to be finalised. This was due to market uncertainty created by Electricity Market Reform at the time.\nThe 2013 draft Plan Options are included in the National Marine Plan and more information on that sectoral planning process is available alongside the supporting environmental assessments and consultation analysis.', 'Developers are sometimes under-assessing the impact of wind farm noise and appearance on residents living nearby, according to new research.\nThe two-year study looked at how the visual, shadow flicker and noise impacts predicted by developers at the planning stage of ten wind farms across Scotland compared to the reality once operational.\nThe test sites included wind farms at Dalswinton in Dumfries and Galloway, Achany in the Highlands, Drone Hill in the Borders, Hadyard Hill in South Ayrshire, Little Raith in Fife and West Knock Farm in Aberdeenshire.\nIn some cases what was set out in planning applications did not match the actual impact, the research by climate change body ClimateXChange concluded.\nIt also found that efforts to engage with the public had not always adequately prepared residents for the visual, shadow flicker and noise impacts of a development.\nThe information was gathered through a combination of residents’ surveys and assessments by professional consultants.\nThe report said: “T here was a reasonable correspondence between the predicted impacts at application stage and the study team’s assessment of the as-built impacts.\n“However, there were some instances in respect of each of the topics where impacts were under-assessed.\n“This divergence between objective measurement and experience of impacts was evident from the residents’ survey which captured a range of responses.\n“In respect of all three types of impacts considered by the study there were instances where no or limited impacts were predicted by the expert team, but residents reported experiencing adverse impacts.\n“This finding points to the difficulties of predicting or assessing experiential responses.\n“It is therefore important that the assessment process and subsequent consideration of applications by relevant authorities takes account of this.”\nResearchers said this could be achieved through good project siting and design, rigorous impact assessments and improved public engagement.\nProject manager Ragne Low said: “As the study has focused on issues relating to the planning process, we are confident that the findings will feed into improved practice in measuring the predicted impacts of proposed wind farms and in communicating this to decision-makers and those likely to be affected.\n“The findings point to several possible improvements in planning guidance and good practice.\n“Some have been implemented in the time between the case study wind farms being planned and built, and the present. The study will contribute to building on these improvements.”\nLinda Holt, spokeswoman for the campaign group S cotland Against Spin, welcomed the findings.\nShe said: “For too long, people who have complained about wind farms have been dismissed as nimbies and we applaud the energy minister Fergus Ewing for commissioning this work.\n“The recommendations show that the planning system is ill-equipped to address potentially adverse impacts on wind farm neighbours and we urge the Scottish Government to lose no time in implementing them.\n“For too long, decision-makers on wind farms have been asked to determine applications while blind-folded about the true impacts of placing enormous industrial machines near people’s homes.”\nA spokesman for Scottish Renewables said: “This study highlights the high standards of guidance available for those planning an onshore wind farm in Scotland, and we were pleased to see the sector has been putting these into practice.\n“The industry has long worked with government and its agencies to put these high standards in place and this report demonstrates how much we have continuously improved, while identifying areas for further improvements for future schemes.”\nA Scottish Government spokeswoman said: ” We welcome the publication of the wind farm impacts study report which is the first of its kind in the world and presents the findings of a two-year study involving a wide-range of interest groups.\n“The report shows improvements have already been made in our planning system, which is rigorous and ensures appropriate siting of wind farms, and studies like this will make sure this improvement continues, and we look forward to considering the recommendations carefully.\n“Our policy on wind farm applications strikes a careful balance between maximizing Scotland’s huge green energy potential and protecting environmental interests and residential amenity.”']	['<urn:uuid:beb7dfd6-940a-4844-addc-dd3406fb2004>', '<urn:uuid:940570f0-1660-4408-8d93-dd2f4edca60f>']	factoid	direct	short-search-query	distant-from-document	three-doc	expert	2025-05-12T20:58:04.490895	7	73	1372
46	honey contamination testing requirements list	Honey must be tested for sensory and physical parameters (color, flavor, texture, water and solids), carbon isotope testing for sweetener adulteration, veterinary drugs (like chloramphenicol, tetracyclines and sulfonamides), heavy metals, and potentially GMO pollen using polymerase chain reaction.	['Making Sure the Food Additive Ingredients you Receive are Safe\nBy James Cook, SGS\nYour company has developed a new product line and you are in charge of making sure the food additives used are actually what they claim to be, with the level of quality desired, having no hidden contaminants and, of course, the price must continue to be at least reasonable if not inexpensive. Years ago, a project such as this was hard to accomplish and with the global market it is even more of a challenge.\nYou received a sample from the supplier, used in a lab or pilot plant production and found it works to the satisfaction of everyone. How do you now make sure that future food additive supplies continue to be equivalent to the sample?\nThere are actually several straightforward ways to achieve this. These involve facility auditing, product inspection and product testing. While some could be handled internally in your operation, others are probably better suited to an external expert.\nThe purpose of a facility audit is to assure the supplier’s operation uses the right processes and procedures to produce the food additive at an acceptable level of quality and safety. In the days when a company could visit and audit all the facilities producing the food additive being purchased, and assuming your personnel or you knew the correct auditing techniques, it enabled you to verify the supplier’s operation, personnel and their capabilities. You could see at first-hand how everything was being done, or at least on the day you or your associates were there. Unfortunately, with a global market this is no longer realistic.\nSo what kind of audit should be selected and which audit company should perform the audit? Selecting a certified Global Food Safety Initiative (GFSI) audit, International Organization for Standardization audits or an audit company’s own audit may all be good choices, but there is a cost/benefit ratio for the company to consider. However, deciding the kind of audit and which audit company to select, while important, is actually not as essential as knowing how to interpret the results of an audit.\nWhile the overall score and the fact the supplier passes the audit is important, knowing what non-conformances are listed and corrective actions undertaken by the supplier are vital. Make sure that any non-conformances do not impact the quality and safety of the food additive. Some examples of non-conformance that impact safety are obvious ones, such as having rodents or insects in the facility, lacking a quality and safety program or Sanitation Standard Operation Procedures. Others are less obvious, such as having uncontrolled water in a dry facility, a Hazard Analysis and Critical Control Point (HACCP) plan’s CCP is really just a CP, and the scientific validation of the HACCP plan listed for the operation is dissimilar to the system (e.g., a barbecue sauce pasteurization validation used for a nut pasteurization operation).\nMake sure that any non-conformance has been (if major or critical), or will be corrected (if minor) and that the corrective actions taken will assure no recurrences of the non-conformance in future. If this is not the first audit of the operation, make sure that previous non-conformances are not repeated, because any repetition means former corrective actions were either incorrect or incorrectly implemented.\nVarious inspections can be performed. Some have to be done at the food additive facility, but others can be done elsewhere. The most important is probably the final random inspection. After an additive is 80 percent to 100 percent produced, it is inspected by a sampling program and compared to a specification with mainly sensory, physical and some simple chemical parameters. If the additive is 100 percent produced, then it can be sampled and inspected at either the food additive manufacturing facility or at the finished goods manufacturing facility. However, it is always recommended to be performed at the food additive manufacturing facility, because this saves a lot of money in transportation fees should something be found wrong with the food additive. Additionally, if a third party performs the inspection at the food additive manufacturing facility, they can select samples for further testing and ship these in a secure manner to a laboratory while they then watch the inspected product as it is loaded into a transport container, to ensure that what shipped is what was inspected.\nOnce the facility has been audited and the food additive has been inspected, the product requires testing to make sure that it really is what it is supposed to be, that the purity is as it should be, it complies to the regulations, specifications and standards and that there are no hidden contaminants of a biological, radiological or chemical nature.\nAll food additives should be tested to assure that the additive is what it purports to be and assayed to determine the purity of the food additive. For some additives it is simple and for other additives it is complicated. There are many sources and standards that indentify the methods of how to achieve these two tests. For example, some sources are the Food Chemicals Codex, U. S. Pharmacopeia National Formulary and Codex Alimentarius. Yes, including Codex Alimentarius, because not all food additives are single chemical ingredients and food additives can be foods in their own right. Other sources are regulatory websites that provide information from regulations, import alerts, recalls, warnings and product associations such as the California Almond Board or the American Spice Trade Association.\nFood additives can be contaminated by many different microorganisms, as simple as yeast, mold and mycotoxins, as well as more dangerous ones such as Escherichia coli O157H:7, Listeria monocytogenes and Salmonella, just to a name a few. Food additives can also be unintentionally contaminated with heavy metals, cleaning chemicals, sanitizers and radioactive isotopes or they could be intentionally contaminated with all sorts of chemicals, such as sweeteners, melamine, sulfites, allergenic processing aids, veterinary drugs, pesticides and illegal or undeclared legal colorants.\nLet us take a look at a couple of ingredients. Citric acid is sold in anhydrous or hydrous forms, so immediately the product has to be checked for its water or moisture content. It is sold as colorless, translucent crystals or as white granular or fine crystalline powder, so the color and granulation must be checked. It is odorless and has a strong acidic flavor, so the fragrance and flavor need to be evaluated. The citric acid identity must be confirmed and an assay of purity performed. There are more checks that need to be made, such as for heavy metals, and more specifically arsenic, and residue on ignition.\nHoney is an ingredient that is complicated. Once again, as with all food additives the sensory and physical parameters, such as color, flavor, texture, water and solids must be evaluated. Honey can have complications of sweetener adulteration that can only be assessed by carbon isotope testing. There is a long list of potential adulterations by veterinary drugs, such as chloramphenicol, tetracyclines and sulfonamides, just to name a few. Heavy metal contamination is a problem in honey and so metals must also be checked for. There are concerns about pollens in honey from genetically modified plants and so this may need to be tested for by polymerase chain reaction, but sometimes no pollen is found, because some firms have taken to ultra-filtrating the honey.\nI have only mentioned a few of the tests that need to be done on these two ingredients. The most important factor is to know your suppliers and know where your food additives are coming from. Make sure that they are audited on an annual basis at the least and that you receive a copy of the full report, including any corrective actions to the non-conformances. Inspect all materials that are going to be shipped to you or inspect them upon receipt. Perform tests for all possible contaminants in the food additive. Rate your suppliers based on the audits, inspections and testing and develop a program, whereby consistently good suppliers are rewarded with less inspection and testing and the suppliers that you have issues with will require more audits, inspections and testing, but remember that all suppliers and food additives must be audited, inspected and tested to assure safety and quality of the materials. Mistakes happen in the best of operations.\nJames Cook has been working with SGS since 2009 handling food and food contact technical issues for the Consumer Testing Services group. Jim has more than 25 years quality assurance and technical experience in the food retailing, manufacturing and private brand brokerage business. For more information visit www.foodsafety.sgs.com.']	['<urn:uuid:a4acdd4a-ac27-4580-8f87-77f0d964b34f>']	factoid	direct	short-search-query	distant-from-document	single-doc	expert	2025-05-12T20:58:04.490895	5	38	1432
47	What was the nature of the relationship between Loïs Mailou Jones and Leigh Whipper, and how did they express their mutual respect?	Records from the Schomburg Center for Research in Black Culture document a significant meeting between Loïs Mailou Jones and Leigh Whipper on February 9, 1939. During this meeting, Jones signed Whipper's signature book with the message 'In memory of a very pleasant afternoon,' joining other notable figures like NAACP leader Walter White who had also signed. Whipper, in turn, wrote a highly complimentary note in Jones's guest book, stating 'To the #1 Negro artist (Loïs Jones) who will some day be America's #1 artist,' demonstrating their mutual respect and admiration.	['As many of you know, the Brooklyn Museum launched the Fund for African American Art a few years ago. This ambitious initiative, which was covered in the New York Times, is designed to help us acquire works created by African American artists before 1945. As someone who just came on board, I’m excited to work with these new acquisitions, many of which are on view now in American Identities, our permanent exhibition devoted to American art on the fifth floor of the museum. For example, this beautiful portrait of actor Leigh Whipper painted by Loïs Mailou Jones was recently installed in American Identities and I had the opportunity to research the artwork to prepare for its debut.\nWhen Loïs Mailou Jones painted this portrait, Leigh Whipper was approaching the height of his career as a Broadway and Hollywood actor. He had already become the first black member of the Actors Equity Association in 1920 and, by the end of 1939, he would be famous for his role as Crooks in Lewis Milestone’s critically acclaimed film adaptation of John Steinbeck’s Of Mice and Men. Whipper’s character—a handicapped farmhand ostracized because of his race—served to illuminate the movie’s Depression-era message that American Dream’s promise of economic and social success was impossible.\nFaced with the task of learning more about such a fascinating person, I beat a path to the Schomburg Center for Research in Black Culture, which holds the Leigh Whipper papers. Thanks these records, I learned that these two dynamic artists time spent together on February 9, 1939. On that day, Loïs Mailou Jones signed the actor’s signature book: “In memory of a very pleasant afternoon.” With that, she left her signature in the august company of other notables like NAACP leader Walter White. According to Jones’s archives, Whipper also left a caring note in Jones’s guest book: “To the #1 Negro artist (Loïs Jones) who will some day be America’s #1 artist.”\nWhen Jones painted this portrait she had recently returned to teach at Howard University in Washington, D.C. after a year sabbatical spent studying painting in Paris. Perhaps it was nostalgia for France that led Jones to depict Whipper as if seated at a Paris café. At Howard, the artist would enter an intellectual conversation on campus that shaped the discourse of the Harlem Renaissance more broadly. Harlem Renaissance intellectual Alain Locke and “father” of African American art history James Porter were both professors at Howard.\nDans un Café à Paris (Leigh Whipper) reveals the influence of both Locke and Porter. The naturalistic modeling of figure and still-life arrangement of wine bottle and sandwiches follow the academic style that Porter himself employed in his own paintings. Although Locke heralded the flat, egyptianized forms of Aaron Douglas as the epitome of a “racial school of art” inspired by the abstracted forms of African art, Locke also implored black artists to create ennobling portrayals of African Americans—a call that Lois Mailou Jones’s portrayal of a pensive Whipper clearly fulfilled.\nDalila Scruggs comes to the Brooklyn Museum from the Williams College Museum of Art, where she served as Mellon Curatorial Fellow for Diversity in the Arts since 2009. In that position, which straddled curatorial and education, she curated African American and the American Scene, 1929-45, assisted with a reinstallation of the permanent collection, and taught a survey course of African American art. Dalila earned a Ph.D. in Art History from Harvard University, where she focused on African American art and traditional West African art. An essay drawn for her dissertation was recently published in Early African American Print Culture (edited by Lara Langer Cohen and Jordan Alexander Stein).']	['<urn:uuid:967dce60-fcf3-4990-9a86-80e17ebd70e8>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:58:04.490895	22	90	606
48	causes triggers symptoms chronic sinusitis connection with asthma severity treatment options	Chronic sinusitis and asthma are closely connected conditions. Chronic sinusitis can be caused by allergies, fungal infections, bacterial infections, and environmental factors like air pollution or dry air. Its symptoms include pain in the cheeks and forehead, blocked nose, loss of smell, and yellow/green nasal discharge. The severity of sinusitis appears linked to asthma severity, with about half of people with moderate to severe asthma also experiencing chronic sinus infections. Treatment options include antibiotics for bacterial infections, inhaled steroids, steroid nasal sprays, decongestants, and in severe cases, surgery may be necessary. Managing both conditions often requires a comprehensive approach, as treating sinus problems can help relieve asthma symptoms.	"['On this page:\nYou\'re coughing and sneezing and tired and achy. You think that you might be\ngetting a cold. Later, when the medicines you\'ve been taking to relieve the\nsymptoms of the common cold are not working and you\'ve now got a terrible headache,\nyou finally drag yourself to the doctor. After listening to your history of\nsymptoms, examining your face and forehead, and perhaps doing a sinus X-ray,\nthe doctor says you have sinusitis.\nSinusitis simply means your sinuses are infected or inflamed, but this gives\nlittle indication of the misery and pain this condition can cause. Health\ncare experts usually divide sinusitis cases into\n- Acute, which lasts for 3 weeks or less\n- Chronic, which usually lasts for 3 to 8 weeks but can continue for months\nor even years\n- Recurrent, which is several acute attacks within a year\nHealth care experts estimate that 37 million Americans are affected by sinusitis\nevery year. Health care workers report 33 million cases of chronic sinusitis\nto the U.S. Centers for Disease Control and Prevention annually. Americans\nspend millions of dollars each year for medications that promise relief from\ntheir sinus symptoms.\nReturn to top\nSinuses are hollow air spaces in the human body. When people say, ""I\'m having\na sinus attack,"" they usually are referring to symptoms in one or more of four\npairs of cavities, or sinuses, known as paranasal sinuses\n. These cavities,\nlocated within the skull or bones of the head surrounding the nose, include\n- Frontal sinuses over the eyes in the brow area\n- Maxillary sinuses inside each cheekbone\n- Ethmoid sinuses just behind the bridge of the\nnose and between the eyes\n- Sphenoid sinuses behind the ethmoids in the\nupper region of the nose and behind the eyes\nEach sinus has an opening into the nose for the free exchange of air and\nmucus, and each is joined with the nasal passages by a continuous mucous membrane\nlining. Therefore, anything that causes a swelling in the nose-an infection,\nan allergic reaction, or an immune reaction-also can affect the sinuses. Air\ntrapped within a blocked sinus, along with pus or other secretions, may cause\npressure on the sinus wall. The result is the sometimes intense pain of a\nsinus attack. Similarly, when air is prevented from entering a paranasal sinus\nby a swollen membrane at the opening, a vacuum can be created that also causes\nReturn to top\nThe location of your sinus pain depends on which sinus is affected.\n- Headache when you wake up in the morning is typical of a sinus problem.\n- Pain when your forehead over the frontal sinuses is touched may indicate\nthat your frontal sinuses are inflammed.\n- Infection in the maxillary sinuses can cause your upper jaw and teeth\nto ache and your cheeks to become tender to the touch.\n- Since the ethmoid sinuses are near the tear ducts in the corner of the\neyes, inflammation of these cavities often causes swelling of the eyelids\nand tissues around your eyes, and pain between your eyes. Ethmoid inflammation\nalso can cause tenderness when the sides of your nose are touched, a loss\nof smell, and a stuffy nose.\n- Although the sphenoid sinuses are less frequently affected, infection\nin this area can cause earaches, neck pain, and deep aching at the top of\nMost people with sinusitis, however, have pain or tenderness in several locations,\nand their symptoms usually do not clearly indicate which sinuses are inflamed.\nOther symptoms of sinusitis can include\n- A cough that may be more severe at night\n- Runny nose (rhinitis) or nasal congestion\nIn addition, the drainage of mucus from the sphenoids or other sinuses down\nthe back of your throat (postnasal drip) can cause you to have a sore throat.\nMucus drainage also can irritate the membranes lining your larynx (upper windpipe).\nNot everyone with these symptoms, however, has sinusitis.\nOn rare occasions, acute sinusitis can result in brain infection and other\nReturn to top\nMost cases of acute sinusitis start with a common cold, which is caused by a\nvirus. These viral colds do not cause symptoms of sinusitis, but they do inflame\nthe sinuses. Both the cold and the sinus inflammation usually go away without\ntreatment in 2 weeks. The inflammation, however, might explain why having a\ncold increases your likelihood of developing acute sinusitis. For example, your\nnose reacts to an invasion by viruses that cause infections such as the common\ncold or flu by producing mucus and sending white blood cells to the lining of\nthe nose, which congest and swell the nasal passages.\nWhen this swelling involves the adjacent mucous membranes of your sinuses,\nair and mucus are trapped behind the narrowed openings of the sinuses. When\nyour sinus openings become too narrow, mucus cannot drain properly. This increase\nin mucus sets up prime conditions for bacteria to multiply.\nMost healthy people harbor bacteria, such as Streptococcus pneumoniae\nand Haemophilus influenzae, in their upper respiratory tracts with\nno problems until the body\'s defenses are weakened or drainage from the sinuses\nis blocked by a cold or other viral infection. Thus, bacteria that may have\nbeen living harmlessly in your nose or throat can multiply and invade your\nsinuses, causing an acute sinus infection.\nSometimes, fungal infections can cause acute sinusitis. Although fungi are\nabundant in the environment, they usually are harmless to healthy people,\nindicating that the human body has a natural resistance to them. Fungi, such\nas Aspergillus, can cause serious illness in people whose immune\nsystems are not functioning properly. Some people with fungal sinusitis have\nan allergic-type reaction to the fungi.\nChronic inflammation of the nasal passages also can lead to sinusitis. If\nyou have allergic rhinitis or hay fever, you can develop episodes of acute\nsinusitis. Vasomotor rhinitis, caused by humidity, cold air, alcohol, perfumes,\nand other environmental conditions, also may be complicated by sinus infections.\nAcute sinusitis is much more common in some people than in the general population.\nFor example, sinusitis occurs more often in people who have reduced immune\nfunction (such as those with immune deficiency diseases or HIV infection)\nand with abnormality of mucus secretion or mucus movement (such as those with\nReturn to top\nIf you have asthma, an allergic disease, you may have frequent episodes of chronic\nIf you are allergic to airborne allergens, such as dust, mold, and pollen,\nwhich trigger allergic rhinitis, you may develop chronic sinusitis. In addition,\npeople who are allergic to fungi can develop a condition called ""allergic\nIf you are subject to getting chronic sinusitis, damp weather, especially\nin northern temperate climates, or pollutants in the air and in buildings\nalso can affect you.\nLike acute sinusitis, you might develop chronic sinusitis if you have an\nimmune deficiency disease or an abnormality in the way mucus moves through\nand from your respiratory system (e.g., immune deficiency, HIV infection,\nand cystic fibrosis). In addition, if you have severe asthma, nasal polyps\n(small growths in the nose), or a severe asthmatic response to aspirin and\naspirin-like medicines such as ibuprofen, you might have chronic sinusitis\nReturn to top\nBecause your nose can get stuffy when you have a condition like the common cold,\nyou may confuse simple nasal congestion with sinusitis. A cold, however, usually\nlasts about 7 to 14 days and disappears without treatment. Acute sinusitis often\nlasts longer and typically causes more symptoms than just a cold.\nYour doctor can diagnose sinusitis by listening to your symptoms, doing a\nphysical examination, and taking X-rays, and if necessary, an MRI or CT scan\n(magnetic resonance imaging and computed tomography).\nReturn to top\nAfter diagnosing sinusitis and identifying a possible cause, a doctor can suggest\ntreatments that will reduce your inflammation and relieve your symptoms.\nIf you have acute sinusitis, your doctor may recommend\n- Decongestants to reduce congestion\n- Antibiotics to control a bacterial infection, if present\n- Pain relievers to reduce any pain\nYou should, however, use over-the-counter or prescription decongestant nose\ndrops and sprays for only few days. If you use these medicines for longer\nperiods, they can lead to even more congestion and swelling of your nasal\nIf bacteria cause your sinusitis, antibiotics used along with a nasal or\noral decongestant will usually help. Your doctor can prescribe an antibiotic\nthat fights the type of bacteria most commonly associated with sinusitis.\nMany cases of acute sinusitis will end without antibiotics. If you have allergic\ndisease along with infectious sinusitis, however, you may need medicine to\nrelieve your allergy symptoms. If you already have asthma then get sinusitis,\nyou may experience worsening of your asthma and should be in close touch with\nIn addition, your doctor may prescribe a steroid nasal spray, along with\nother treatments, to reduce your sinus congestion, swelling, and inflammation.\nDoctors often find it difficult to treat chronic sinusitis successfully,\nrealizing that symptoms persist even after taking antibiotics for a long period.\nIn general, however, treating chronic sinusitis, such as with antibiotics\nand decongestants, is similar to treating acute sinusitis.\nSome people with severe asthma have dramatic improvement of their symptoms\nwhen their chronic sinusitis is treated with antibiotics.\nDoctors commonly prescribe steroid nasal sprays to reduce inflammation in\nchronic sinusitis. Although doctors occasionally prescribe them to treat people\nwith chronic sinusitis over a long period, they don\'t fully understand the\nlong-term safety of these medications, especially in children. Therefore,\ndoctors will consider whether the benefits outweigh any risks of using steroid\nIf you have severe chronic sinusitis, your doctor may prescribe oral steroids,\nsuch as prednisone. Because oral steroids are powerful medicines and can have\nsignificant side effects, you should take them only when other medicines have\nAlthough home remedies cannot cure sinus infection, they might give you some\n- Inhaling steam from a vaporizer or a hot cup of water can soothe inflamed\n- Saline nasal spray, which you can buy in a drug store, can give relief.\n- Gentle heat applied over the inflamed area is comforting.\nWhen medical treatment fails, surgery may be the only alternative for treating\nchronic sinusitis. Research studies suggest that the vast majority of people\nwho undergo surgery have fewer symptoms and better quality of life.\nIn children, problems often are eliminated by removal of adenoids obstructing\nAdults who have had allergic and infectious conditions over the years sometimes\ndevelop nasal polyps that interfere with proper drainage. Removal of these\npolyps and/or repair of a deviated septum to ensure an open airway often provides\nconsiderable relief from sinus symptoms.\nThe most common surgery done today is functional endoscopic sinus surgery,\nin which the natural openings from the sinuses are enlarged to allow drainage.\nThis type of surgery is less invasive than conventional sinus surgery, and\nserious complications are rare.\nReturn to top\nAlthough you cannot prevent all sinus disorders-any more than you can avoid\nall colds or bacterial infections-you can do certain things to reduce the number\nand severity of the attacks and possibly prevent acute sinusitis from becoming\n- You may get some relief from your symptoms with a humidifier, particularly\nif room air in your home is heated by a dry forced-air system.\n- Air conditioners help to provide an even temperature.\n- Electrostatic filters attached to heating and air conditioning equipment\nare helpful in removing allergens from the air.\nIf you are prone to getting sinus disorders, especially if you have allergies,\nyou should avoid cigarette smoke and other air pollutants. If your allergies\ninflame your nasal passages, you are more likely to have a strong reaction\nto all irritants.\nIf you suspect that your sinus inflammation may be related to dust, mold,\npollen, or food-or any of the hundreds of allergens that can trigger an upper\nrespiratory reaction-you should consult your doctor. Your doctor can use various\ntests to determine whether you have an allergy and its cause. This will help\nyou and your doctor take appropriate steps to reduce or limit your allergy\nDrinking alcohol also causes nasal and sinus membranes to swell.\nIf you are prone to sinusitis, it may be uncomfortable for you to swim in\npools treated with chlorine, since it irritates the lining of the nose and\nDivers often get sinus congestion and infection when water is forced into\nthe sinuses from the nasal passages.\nYou may find that air travel poses a problem if you are suffering from acute\nor chronic sinusitis. As air pressure in a plane is reduced, pressure can\nbuild up in your head blocking your sinuses or eustachian tubes in your ears.\nTherefore, you might feel discomfort in your sinus or middle ear during the\nplane\'s ascent or descent. Some doctors recommend using decongestant nose\ndrops or inhalers before your flight to avoid this problem.\nReturn to top\nScientific studies have shown a close relationship between having allergic rhinitis\nand chronic sinusitis. In fact, some studies state that up to 80 percent of\nadults with chronic sinusitis also had allergic rhinitis. There is also an association\nbetween asthma and sinusitis. Some researchers think that as many as 75 percent\nof people with asthma also get sinusitis. The National Institute of Allergy\nand Infectious Diseases (NIAID) conducts and supports research on allergic diseases\nas well as bacteria and fungus that can cause sinusitis. This research is focused\non developing better treatments and ways to prevent these diseases.\nScientists supported by NIAID and other institutions are investigating whether\nchronic sinusitis has genetic causes. They have found that the alterations\nin genes which cause cystic fibrosis may also contribute to chronic sinusitis.\nThis research focus will give scientists new insights into the cause of the\ndisease in some people and points to new strategies for diagnosis and treatment.\nAnother NIAID-supported research study is trying to determine whether fungi\nmay play a role in causing many cases of chronic sinusitis. This research\nalso will help scientists develop better medicines to treat chronic sinusitis.\nReturn to top\nNational Library of Medicine\nAmerican Academy of Allergy,\nAsthma and Immunology\nof Allergy, Asthma, and Immunology\nAmerican Academy of Otolaryngology-Head\nand Neck Surgery, Inc.\nReturn to top', 'Introduction - Connection between Sinusitis and Asthma\nSinusitis and asthma are two prevalent medical conditions that often coexist, with nearly half of individuals suffering from moderate to severe asthma also experiencing sinusitis. The connection between these two conditions can pose challenges in managing symptoms, as the combination of sinusitis and asthma can leave individuals feeling unwell and miserable. If left untreated, sinusitis symptoms can exacerbate and potentially lead to more severe cases of asthma. This article explores the connection between sinusitis and asthma, highlighting how sinusitis can impact asthma and providing insights into effective symptom management for individuals with asthma and sinusitis.\nSinusitis occurs when the lining of the sinuses, the small cavities around the nose, becomes inflamed.\n- Pain in the cheeks, forehead, or top of the nose\n- Blocked nose\n- Loss of smell\n- Nasal discharge that may be green or yellow\n- Toothache-like pain\nSymptoms can vary depending on the sinuses involved, with pain commonly felt in the forehead, upper jaw, teeth, area around the eyes, neck, ears, and top of the head.\n- Bacterial infections\n- Air pollution\n- Dry or cold air\n- Fungal infections\n- Compromised immune system\n- Thick yellow or green mucus\n- Postnasal drip\nSinusitis can be acute, lasting for days to weeks, or chronic, persisting for three months or more. While sinus infections are often caused by viruses, prolonged blockage can allow bacteria to invade, resulting in secondary infections. Chronic sinusitis, lasting longer than 12 weeks despite treatment, is believed to be a combination of infections and inflammation. Understanding the symptoms and causes of sinusitis can help individuals seek appropriate management and treatment options.\nAsthma, characterized by inflammation in the airways, is a chronic disease that can result in asthma flare-ups or attacks. These episodes can vary in duration, with mild symptoms lasting a few minutes and severe symptoms persisting for hours or even days.\nDuring an asthma attack, the airways become inflamed and produce excess mucus, causing narrowing and difficulty in breathing. This can lead to persistent coughing, audible wheezing sounds, and a sensation of tightness in the chest.\nCommon symptoms associated with asthma include:\n- Shortness of breath\n- Rapid breathing\n- Tight chest\nIn some cases, approximately 5 to 10% of individuals with asthma experience severe symptoms. Alongside the typical asthma symptoms, severe asthma can contribute to chronic sinus infections, a diminished sense of smell, and the development of nasal polyps. Prompt medical attention and appropriate management are crucial for individuals with severe asthma to minimize the impact of these symptoms on their daily lives.\nWhat is the connection between sinusitis and asthma?\nNumerous studies have established a connection between sinusitis and asthma, indicating that these conditions are often intertwined. About half of individuals with moderate to severe asthma also experience chronic sinus infections. The presence of sinusitis can complicate the management of asthma, aggravating airway inflammation and leading to worsening symptoms such as:\n- Shortness of breath\n- Chest tightness\n- Disrupted sleep patterns\nTreating sinus problems with medication has shown promise in relieving asthma symptoms. It is important for individuals with asthma to continue their daily asthma controller medication and promptly use a quick-relief inhaler at the first signs of asthma symptoms. Maintaining good asthma control can reduce the risk of developing severe sinus infections.\nFor those with allergic asthma, identifying and avoiding allergens that trigger asthma symptoms is crucial. Common allergens include grass, tree, and weed pollen, indoor and outdoor mold, as well as pet dander and dust mites.\nChronic sinusitis, which can be associated with allergic conditions like hay fever, may lead to blocked drainage channels and subsequent sinus infection or inflammation. Severe cases of asthma have been found to be more prevalent in individuals with sinusitis, and the severity of sinusitis appears to be linked to the severity of asthma symptoms. Women, individuals with acid reflux (GERD), and smokers may have an increased risk of developing sinusitis when they have asthma.\nAlthough more research is needed to establish a definitive link between the treatment of sinusitis and asthma, some studies have demonstrated that addressing sinusitis can lead to improved asthma symptoms, particularly when sinus conditions are treated early in children. However, further investigation is necessary to fully understand the relationship and potential shared treatments between these two conditions.\nHow are sinusitis and asthma treated?\nThe treatment of sinusitis depends on the nature of the condition, whether it is acute or chronic, and its underlying cause. Acute symptoms caused by viral infections generally resolve on their own within a couple of weeks. Over-the-counter decongestants and other medications can provide relief by clearing the airways and alleviating headaches. However, if symptoms persist beyond 10 days, it is advisable to seek medical attention.\nIn cases where a bacterial infection is suspected, antibiotics may be prescribed to eliminate the infection and reduce inflammation.\nAllergy specialists can help determine if underlying allergies or asthma contribute to sinusitis symptoms. They can also assist in developing a comprehensive management plan for asthma, including preventive treatments aimed at reducing inflammation and persistent symptoms. Collaborating with an asthma specialist is essential to develop an individualized treatment plan to prevent and alleviate symptoms of both asthma and sinusitis.\nTreatment for sinusitis can also involve a combination of medications such as:\n- Inhaled steroids\n- Steroid nasal sprays\n- Leukotriene modifiers\n- Allergy shots (immunotherapy)\nAntibiotics may be prescribed for bacterial sinus infections, while home remedies such as nasal washes and humidifiers can help alleviate nasal congestion.\nFor chronic sinusitis cases associated with physical nasal passage issues, surgical interventions may be necessary. These can include correcting narrow nasal passages, deviated septum, or removing nasal polyps to alleviate chronic inflammation and swelling in the sinuses.\nIn asthma treatment, a variety of medications can be utilized.\n- Reliever inhalers containing short-acting beta2-agonists (SABAs) to open the airways during asthma attacks\n- Corticosteroids to reduce airway swelling\n- Short-acting anticholinergics to quickly open the airways\n- Corticosteroids to reduce inflammation\n- Biologics administered through injections\n- Leukotriene modifiers to decrease swelling and maintain open airways\n- Mast cell stabilizers to prevent airway swelling\n- Long-acting bronchodilators to prevent airway constriction\n- Allergy shots (immunotherapy) to mitigate the impact of allergens\n- Pain relievers\n- Steam inhalation\n- Saline nasal rinses\n- Saltwater nasal sprays\nIt is important to consult with healthcare professionals for appropriate guidance and to address any potential interactions with other medications or conditions.\nCan post nasal drip trigger asthma?\nPostnasal drip syndrome, a condition characterized by the sensation of nasal mucus accumulating or draining into the throat, is commonly experienced by individuals. The glands in the nose and throat continuously produce mucus to cleanse the nasal membranes, warm the inhaled air, and trap foreign particles. This mucus also plays a role in fighting off infections.\nUnder normal circumstances, the secretions from the nasal and throat mucous glands keep the throat moist. This is part of the body’s defense system against diseases, known as the mucous-nasal cilia system. However, when there is an excessive or thickened production of mucus in the nose and sinuses, the body’s natural response is to induce coughing and throat clearing in an attempt to eliminate it.\nIn some cases, postnasal drip syndrome can be associated with asthma. The thick mucus secretions from the nose can drain down the back of the throat, leading to symptoms such as throat clearing, coughing, and constriction of the bronchial airways.\nWhen to consult a doctor?\nPeople with asthma are advised to have regular checkups and should promptly inform a healthcare professional if their asthma attacks become more frequent or severe. If they experience symptoms such as:\n- Feeling faint\n- Difficulty performing daily activities\n- A persistent cough\nIt is important to seek medical advice.\nIn cases of severe asthma symptoms, immediate action is necessary. If individuals notice:\n- Lips or nails turning blue\n- Flaring nostrils with every inhale\n- Stretched-looking skin between the ribs or at the base of the throat during breathing\n- Breathing 30 or more times per minute\n- If talking or walking at a normal pace becomes difficult\nThen they are advised to go to the emergency room.\nFor those with symptoms of sinusitis, it is recommended to consult a healthcare professional if they experience:\n- Multiple sinus infections within a year\n- Severe headaches\n- Intense facial pain\n- Symptoms that worsen after initially improving\n- Symptoms that persist for more than 10 days\n- A fever lasting 3–4 days\nSeeking medical attention in these situations can ensure appropriate management and treatment.\nConclusion - Connection between Sinusitis and Asthma\nWhen individuals experience symptoms of sinusitis, such as ongoing nasal congestion and pain, it is crucial for them to consult with a healthcare professional, particularly if they already have asthma. Although the precise connection between sinusitis and asthma remains unclear, there are various treatment options available for both conditions. By addressing sinusitis, individuals can aim to alleviate nasal congestion and related symptoms, which may subsequently help alleviate breathing difficulties associated with asthma. Seeking medical advice allows for proper management and treatment of recurring or chronic sinusitis, while considering the individual’s specific asthma medications.\nIf you are looking for\nyou are in the right place!']"	['<urn:uuid:3534fb10-a627-4068-ac80-5270482cc232>', '<urn:uuid:2790c755-ce77-4039-b96e-dc00e73f2131>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	novice	2025-05-12T20:58:04.490895	11	108	3890
49	As someone studying historical trade routes in Mexico, I'm curious about how the cochineal insect trade impacted Oaxaca's economy in the 18th century - what was its significance?	In the mid-1700s, Oaxaca experienced unparalleled prosperity due to the production of an intense red dye made from the cochineal insect, which was widely used in Europe's growing textile industry. However, this prosperity declined in the early 1800s due to competition from Guatemala and the later discovery of synthetic dyes.	['Oaxaca, Mexico, in full, Oaxaca de Juaréz, capital city of the state of Oaxaca in the highlands of the Sierra Madre del Sur mountains in southern Mexico. Oaxaca is a center of Native American culture and heritage in Mexico and is located near some of the country’s most important archeological sites. Situated in a large valley at an elevation of more than 1500 m (5000 ft), the city enjoys a subtropical climate.Photos of Monte Albán\nThe state of Oaxaca is home to one of the largest Native American populations in Mexico. The state has more people who speak an indigenous language than any of the 31 Mexican states. Zapotec and Mixtec peoples make up a large proportion of the people of both the city and the state, and many speak both Spanish and a Native American language. The city grew considerably in the 1970s and 1980s as migration from rural areas boosted its population to 213,000 in 1990. Growth continued in the 1990s, with the city’s population estimated at more than 300,000 in 1995.\nOaxaca’s attractions include its shaded central plaza, known as the Zócalo, the covered walks and pedestrian-only streets that surround the square, and a wide variety of colonial architecture. Colonial structures include the city’s baroque cathedral, the churches of La Soledad and Santo Domingo, and the converted monastery that now houses the city’s premier museum—the Oaxaca Regional Museum of Anthropology and History.\nOther sites include the Rufino Tamayo Museum, the Government Palace, the Benito Juárez Museum and Home, the Oaxaca Museum and Home of Cortés, and the Macedonio Alcala Theater, an impressive building constructed in the late 19th century in a French revival style. The city is also home to the National Autonomous University of Benito Juárez (1827).\nLink to historic church organs in Oaxaca\nOaxaca’s principal economic activities focus on providing services to a large region in south central Mexico and to a dynamic tourist industry. Work in the service sector employs nearly three-quarters of the city’s labor force. Tourism has become increasingly important to the city’s economy since the 1970s. The concentration of indigenous peoples and nearby pre-Hispanic ruins, most notably those at Monte Albán and Mitla, are important draws for tourists.\nPhotos of Mitla and Yagul\nThe city has a small industrial sector, dominated by artisan production and small home-based businesses. A lively trade in local handicrafts includes hand-woven and dyed carpets, serapes, brightly colored blouses, leather belts, pottery, and silver goods.\nfamous black pottery\nAn international airport is located 7.0 km (4.3 mi) southeast of the city, with daily air connections to Mexico City and a few regular flights to major cities in the United States. The city enjoys daily rail service to Mexico City, and two bus terminals provide ample connections to cities throughout Mexico.\nAt times Oaxaca has played a notable role in Mexican history. Originally founded by Aztec warriors in 1486, the city was taken over by Spanish conquerors in 1521. For the next 50 years it flourished as the only major population center between Mexico City and Spain’s first Pacific port at Huatulco. With the opening of the port of Acapulco, Oaxaca’s fortunes declined.\nPhotos of Dying and Weaving Process\nThe town rebounded in the mid-1700s when the production of an intense red dye made from the cochineal insect, native to the region, was used widely in Europe’s growing textile industry. This ushered in a period of unparalleled prosperity for the town. However, in the early 1800s economic stagnation began due to competition from producers in Guatemala and the discovery of synthetic dyes later in the century. The economy revived in the last decades of the 19th century with the completion of a railroad link to Mexico City and the city of Veracruz on the Gulf Coast, and increasing foreign investment in mining operations. The paving of the Pan-American Highway in the middle of the 20th century opened the city to increasing commerce from Mexico City, but decimated local industries producing consumer goods for local consumption.\nOaxaca has been home to a variety of famous Mexicans. This list includes two presidents, Benito Juárez (1861-1863; 1867-1872) and Porfirio Diaz (1877-1880; 1884-1911), the 17th-century feminist writer Juana Inés de la Cruz, and the 20th-century artist Rufino Tamayo.Test Contributed By: Robert B. Kent for Microsoft Encarta\nPhotos of Cuilapan Convent\nReturn to Mexico page\nReturn to People and Places']	['<urn:uuid:0957bca1-6f9a-45f0-8b54-e2ca88dfc5c0>']	factoid	with-premise	verbose-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:58:04.490895	28	50	727
50	What are the three main obligations of states regarding ESCR?	States have three main obligations regarding ESCR: to respect (refrain from violations), to protect (prevent third parties from violations), and to fulfil (take necessary measures to realize ESCR through various processes).	['ESCR are human rights concerning the basic social and economic conditions needed to live a life of dignity and freedom, relating to work and workers’ rights, social security, health, education, food, water, housing, healthy environment, and culture.\nHuman rights provide a common framework of universally-recognised values and norms, and set out state obligations to act in certain ways or to refrain from certain acts. They are an important tool to hold states, and increasingly non-state actors, accountable for violations and also to mobilise collective efforts to develop communities and global frameworks conducive to economic justice, social wellbeing, participation, and equality. Human rights are universal, inalienable, interdependent and indivisible.\nWhere are ESCR set out?\nIn 1948, the United Nations General Assembly adopted the Universal Declaration of Human Rights (UDHR), outlining the basic civil, cultural, economic, political and social rights that all human beings should enjoy. In 1966, ESCR were expressed as legal rights in the International Covenant on Economic, Social and Cultural Rights (ICESCR) (which together with the UDHR and the International Covenant on Civil and Political Rights form the so-called International Bill of Rights), as well as through other key human rights treaties and regional mechanisms. To date, more than 160 states have ratified the ICESCR. In addition, many states have articulated their commitment to ESCR through national constitutions and domestic law.\nWhat are the key principles associated with ESCR?\nThe ICESCR outlines a number of important principles in the realisation of ESCR, which are often included in other ESCR sources as well. Under the ICESCR, a state must take steps “to the maximum of its available resources” to progressively realise ESCR. In particular, a state (including its subnational levels) has the obligations:\n- to respect ESCR (itself refrain from any violation of ESCR);\n- to protect ESCR (prevent third parties from violating ESCR);\n- to fulfil ESCR (take necessary measures to realise ESCR, including through legislative, administrative, budgetary and other processes); and\n- to seek and provide international assistance and cooperation in the realisation of ESCR.\nStates must guarantee ESCR without discrimination on the basis of grounds specified in the ICESCR, including race, colour, sex, language, religion, political or other opinion, national or social origin, property, and birth. In its work, the UN Committee on Economic, Social and Cultural Rights (CESCR) has identified additional prohibited grounds for discrimination, including disability, age, nationality, marital and family status, sexual orientation and gender identity, health status, place of residence, and economic and social situation. The elimination of discrimination, and certain minimum core obligations identified by CESCR in some of its general comments, are not subject to progressive realisation but are immediate obligations.\nThe preamble of the UDHR confirms that “every individual and every organ of society” shall strive to promote respect for human rights and to “secure their universal and effective recognition and observance”; this extends to businesses, international and multilateral organisations, and other non-state actors.\nWhy are ESCR important?\nThe articulation of ESCR in international law followed long-term demands for these basic rights by people worldwide, and reflects concern for the life of every individual, particularly the most vulnerable, as expressed in many philosophical, religious and other traditions.\nIn an era of increasing economic globalisation and growing inequality within and between states, there is an urgent need for grassroots groups, NGOs, academics, and other organisations and individuals to unite to recognise connections between continuing, localised struggles and to realise the human rights of all persons in practice. In understanding instances and patterns of poverty and deprivation as violations of ESCR – rather than mere misfortune, events outside human control, or the result of individual shortcomings – an obligation is placed on states and, increasingly, on corporations and other non-state actors, to prevent and address such situations.\nAround the world, the ESCR framework is used to bolster actions for justice and against oppression, and amplify progressive alternatives to enhance the enjoyment of ESCR. Activists have brought legal cases before UN treaty bodies, courts and other dispute resolution bodies to demand change, documented and publicised recurring violations, mobilised communities, developed legislation, analysed domestic budgets and international trade agreements to ensure compliance with human rights, and built solidarity and networks between communities locally and across the globe. ESCR unite women and men, migrants and indigenous people, youth and elders, of all races, religions, political orientations, and economic and social backgrounds, in a common realisation of universal human freedom and dignity.']	['<urn:uuid:229d224c-5918-4fac-86da-d8bb56cb485d>']	factoid	direct	concise-and-natural	similar-to-document	single-doc	expert	2025-05-12T20:58:04.490895	10	31	736
51	As a workplace strategist, how does space utilization data help workplace design and monitor employee internet use?	Space utilization data helps workplace design through observation studies, room booking systems, and sensors to measure how spaces are used and support different work activities. This same data infrastructure enables employee internet monitoring through tracking software that measures productivity, application usage, and web activity, helping organizations optimize performance while protecting sensitive information.	['Make Your Workplace Data Work for You\nby Dera Arnold and Julie Henline\nSpace has the ability to transform a company’s culture and provide an engaging employee experience vital to success in today’s competitive market. Delivering this experience can be enhanced with a multitude of technology and data sources, all promising the answer to workplace success. However, without a reliable inventory of space and occupancy data, organizations struggle to get a handle on their workplace strategy.\n“There are many exciting innovations available today to achieve a deeper level of understanding about the workplace. When we look at the evolution of these technologies, we see data dependencies in place that must be considered to maximize value,” says Gensler Space and Occupancy Co-Director Christi Van Maanen. “New opportunities for data integrations still depend on core datasets, such as space and HR data.”\nToday, Integrated Workplace Management is less about a single software system and more about integrating relevant tools and datasets at the right time to support a holistic life cycle of data-informed workplace strategies. With the pace of innovative technologies emerging each day, it’s challenging to know where to start or what to fold into the workplace technology stack.\n“Real estate teams have a combination of missing and disorganized data,” says Gensler Workplace Sector Leader Gervais Tompkin. “Companies want data, but they’re struggling with a lack of time and resources to keep information managed.”\nSpace and occupancy continue to be foundational datasets for understanding inventory and measuring demand on space. Maintaining a central repository of floor plans as a sole source of truth throughout the real estate life cycle creates flexibility for layering a multitude of additional qualitative and quantitative datasets to inform and measure the effectiveness of workplace strategies. “Tie in the financial impact of these metrics and a case for change has been built,” says Gensler La Crosse Managing Director Joan Meyers in her blog post, “Using Data to Build a Case for Change.”\nThere are five primary methods for capturing data to provide different perspectives of the workplace throughout the real estate life cycle:\n- Employee Survey\nAn employee survey is key to understanding how a space supports or hinders work from the user’s perspective. Feedback can indicate where to dig deeper into understanding ineffectiveness of the space and help prioritize enhancements. Greater insights can be gleaned when the survey is conducted non-anonymously and is associated to the location where the employee spends most of their time within the space.\nTip: Openly communicate the reason for the survey, hold competitions to encourage participation and share the results with staff to achieve buy in and support change management. These surveys are valuable for both pre-design and post-occupancy measurement of the design.\n- Observation Study\nShort-term, floor plan-based observation studies are the most effective method for collecting data abouthowspaces are used. By collecting types of activities employees are doing within each space, utilization data can be derived and a richer dataset is captured. Activity data will help validate whether the space supports activities as intended or shed light on changes needed to better support focused or collaborative work.\nTip: Similar to employee surveys and sensors, employees can be sensitive to observation studies. Be transparent about the study to engage employees in the change management process. This dataset is insightful for pre-design and post-occupancy measure of the design.\n- Room Booking Systems\nWhile the primary purpose of a reservation system is to give employees better control over their space needs, the exhaust data can be mined for insights about which spaces are most used. The spaces can then be studied to determine the reasons for the high or low demand—location, size, technology or configuration.\nTip: Many room booking systems utilize a representation of the floor plan as part of their navigation. To minimize duplicate efforts, integrate space management drawings so the reservation system stays current.\nWhether seat, light, motion, or temperature—sensors can produce an ongoing measure of utilization. Sensors can also integrate with your room booking system to help automate data cleansing as the human factor of “no shows” can create distrust of reservation data. Refer to Capturing Utilization and Activity Data in the Workplace for benefits and precautions about this passive method for collecting utilization data.\nTip: Bring disparate sources of utilization results into your space management system for analysis with associated attributes found in this central repository. Overlaying blended results on the floor plans as heat maps provides a unique visualization of the data.\n- Exhaust Data\nAs people work and move throughout the workplace each day, they create constant streams of data that can enhance the story of what’s occurring in the space. Any system that captures or transmits data within the space may be relevant to fold into analysis. For example, WiFi triangulation data could be used at a preliminary level to show patterns in space utilization and may tell you what you need to know or help build a case for investment into a larger study. Additional datasets, such as project workflows, tenure and other human capital data can unearth meaningful correlations as well.\nTip: When collecting various datasets, be mindful of privacy and integrity of the data. Gathering and analyzing various datasets may take additional time, but could lead to a more impactful strategy.\nBefore committing to workplace tracking technologies consider your strategy and questions such as these:\n- Do you have an accurate base of space and occupancy data?\n- What are your goals for collecting the data? Is it about design, employee experience, cost savings?\n- How will you implement governance to capture and maintain accurate data?\n- What behaviors do you want to support and reinforce in your organization?\n- What new work modes will you accommodate?\n“The most successful workplaces are methodical in their strategy,” says Anne Gibson, Gensler design director. “When it comes to data, we need to know how often and how many people are using a space, and — most importantly — how effective that space is in supporting what employees need to do.”\nNew workplace strategies benefit when there is a reliable foundation of core space and occupancy data to build on. Layering additional datasets creates a multi-dimensional view of what’s effective in the workplace and what isn’t. There is no one-size-fits-all approach to workplace technology. Aim for implementing a workplace technology stack that is nimble and can quickly respond to today’s ever-changing workplace.\nGensler supports clients throughout every phase in the real estate life cycle—from the design of a new space, to providing the tools needed to maintain ongoing workplace data. Learn more about our proprietary space and occupancy management platform, Wisp®, and the SaaS services that we provide at www.genslerwisp.com', 'Employee Internet Usage Monitoring\nInternet is undoubtedly an essential tool for organizations to access technology and the importance of internet usage has become even more profound for remote workers. However, with so many distractions on the internet, employees can misuse company time surfing irrelevant or inappropriate websites. A Forbes article states that employees spend at least 10 hours of their working week as idle time on non-work-related internet activities. As a result of all of this, a lot of paid hours go to waste and employee monitoring becomes a necessity.\nEmployee monitoring is a tracking method, usually through monitoring software or web filtering, to watch what an employee does while at work. With effective employee monitoring solutions, data-informed decisions can be made when managing employee productivity and enforcing employee internet usage policies. Moreover, employee monitoring software can help identify unproductive employees wasting work time on social media, computer games, and other distractions. Employee monitoring can reveal behavior analytics that allows organizations to optimize performance and productivity. Additionally, employee tracking can also be used to monitor health and wellness, and improve engagement.\nRelated Content: Are Employers Allowed To Track Employee Vehicles?\nCan My Employer See My Internet Activity?\nThe short answer is yes your employer can use various workplace surveillance software and hardware to record everything you do online. Monitoring internet usage and seeing websites visited by employees is completely possible for an employer. Today’s employee monitoring solutions use sophisticated tracking technologies, like geolocation, keystroke logging, screenshots, video recording, and even access to web cams installed on remote personal to extract as much information about employees as possible. All this data is stored via cloud computing and can be run through complex algorithms to anticipate insider threats, measure individual and team productivity, as well as retrace various steps leading to any problems or data leaks. Now, let’s take a closer look at some of the most common ways companies will monitor employee internet activity to see how workers are spending their time while in the office!\nTop 5 Ways How Employers Monitor Internet Activity\n- Restricting unwanted websites. The most obvious solution is content filtering by restricting employees’ access to unproductive websites such as various social media sites. This can be done by blocking sites after evaluating web activity of users. This method is only effective if done individually for each employee since restriction to social media websites for all employees could be detrimental to the work of some.\n- Using time tracking software. Such tools let employers easily monitor their staff’s activities, application usage, and productivity tracking during work hours. Employers and employees can both know idle time spent and an instant message alerts employees to stop them from wasting company time.\n- Using Internet monitoring software. Employee monitoring becomes easy with this as tracking software provides employers with a wealth of useful functions. They can see specific trends and the extent to which the internet is used at a glance.\n- Bandwidth Monitoring. Employers can keep a check on the employee’s bandwidth utilization to figure out company bandwidth abuses, visits to inappropriate websites, or usage of unauthorized software on company computers. Thus, it is essential for protecting sensitive information and mitigating security risks through threat detection.\n- Keeping eyes on employees. Besides using employee monitoring technology, the leadership team of an organization can monitor the internet usage of their employees by taking screen captures and video recordings of their web activity.\nRelated Content: How Companies Track Vehicle Fleets\nWhat Tools Are Used For Employee Monitoring And Tracking?\nFor effectively monitoring employees, organizations will make use of various time tracking and employee productivity measuring software. Time tracking and productivity measuring are relatively easier and keep both the employers and employees in the loop. The time spent by an employee on different applications is determined, followed up by sorting applications into productive and unproductive app groups so that benchmarks can be set for how productive each team, department, or individual employee is expected to be. Advanced activity scoring algorithms make it possible to measure individuals against their past performance as well as against that of their peers. This data collected is used for helping employees, leadership teams, and the remote workforce teams to course-correct and stay productive.\nEmployers also opt for deep monitoring platforms that track all employee activity. Such software tracks use smart monitoring features such as keystroke logging, location tracking, and screenshots. In addition to that, employee monitoring software can aggregate keystrokes and sync them to video recordings to form a more accurate picture of employee activity. As a result, employers can view the full context of what employees were doing, when they were doing it, and a good indication as to why they were doing it. But remember, that while this data might be useful to management, employees could find it terribly invasive.\n7 Ways How To Tell If My Boss Is Spying On My Computer\n- Be aware of alerts and notifications from time tracking and productivity measuring software on your company computer.\n- Surf for common social media applications and see if they are blocked.\n- Check the task manager on your computer to look out for any activity monitoring software that you may not be aware of.\n- Compare the bandwidth allocation and application restrictions on your computer with a colleague’s. If your company computer has more restrictions, chances are you are being monitored by your boss.\n- Indirectly ask the IT department of your office. This is because not all monitoring software leaves a presence in the task Manager. Some employee monitoring software is more advanced, they run in a stealth-mode and cannot simply be opened.\n- Open your computer’s webcam to assess if it’s operational without your approval.\n- Read your job contract or your company’s employee handbook. If a clause for employee monitoring is present then surely your boss is keeping a check on your internet usage.\nIs Employee Monitoring Legal?\nEmployers are within their legal bounds to practice monitoring on their employees as long as there is a legitimate business interest. However, in the best interest of the employer and the employee, it is crucial to balance an employer’s right to lawfully monitor and manage the work process and an employee’s right to privacy. Other than that, it is within the employee’s right to be notified before any monitoring is carried out. The requirement of direct consent of the employee for monitoring varies from place to place.\nAs more and more companies green light remote employees, there will be increased use of internet usage monitoring software and intuitive online tools to improve employee productivity and observe user behavior. Of course, companies can say any activity tracking is to boost network security and data loss prevention, and they would be correct in that position. However, that increased security to monitoring employees online comes with some small level of privacy invasion. The best practice for anyone concerned with employer computer monitoring is to be aware of any visited sites and keep social media usage to personal devices and off company time.']	['<urn:uuid:252819a1-e247-46fc-8860-fb69d338c67e>', '<urn:uuid:21e37398-87d7-44e2-be74-b0540d9471ad>']	factoid	with-premise	concise-and-natural	similar-to-document	multi-aspect	expert	2025-05-12T20:58:04.490895	17	52	2293
52	Why are there more raccoons but fewer wolves nowadays?	"This pattern occurs due to species filtering in human-modified landscapes. While some adaptable species like raccoons are ""thriving"" in these environments due to available anthropogenic resources and reduced predation, other species, particularly large carnivores, are disappearing. Additionally, human-modified landscapes are often simplified compared to their natural state, reducing the number of available ecological niches. This means that while the few adaptable species become abundant, many other species cannot survive in these simplified environments."	['Submitted by editor on 28 September 2020. Get the paper!\nBy Marlee Tucker\nThe world is changing. Humans have modified a large proportion of the Earth’s land surface through a range of activities such as urbanisation and the expansion of agriculture. In addition to landscape changes, the presence of humans through activities, such as recreation and hunting can also have an impact. Human activities have altered species behaviours, distributions and abundances (Torres-Romero and Olalla-Tárraga 2015, Tucker et al. 2018, Bernardo-Madrid et al. 2019), it is likely that these developments have also altered mammal population densities. Population density patterns are important for understanding population dynamics and they are an important measure in conservation. However, collecting population density data is an intensive process and data are generally only collected at small scales, such as a single population, making it difficult to examine these patterns across multiple species or countries.\nIn this study, we examine global patterns of mammalian population density to see whether we can detect any general influence of human impacts.\nWe used a global database of mammalian population densities including 468 species and 6729 density estimates. For each population density estimate, we attached additional information about the environment, such as human impacts (e.g., human footprint, night-time lights, human population density) and productivity. We also attached species information, such as body mass and trophic level. We then ran models examining how mammalian population density is influenced by human impacts, accounting for the productivity and species traits.\nOur results suggest that on average, mammalian population densities tend to be higher in human-modified areas. For some species, human-modified areas are attractive due to the presence of anthropogenic resources (e.g., crops, human waste, supplemental feeding) and the potential reduction in competition and predation (e.g., human shield effect), supporting a larger number of individuals. While this might sound like good news, our results may not reflect a win for all species. First, only some species can adapt to living close to humans (species filtering), where some species are probably disappearing (e.g., large carnivores), while others are “thriving” (e.g., raccoons). So, while we might be seeing an increase in density on average, there is likely a corresponding decrease in species richness (the number of species) in human-modified areas. Our exploration of species richness indicated that indeed shifts in species composition could be occurring. Second, human-modified landscapes are often simplified compared to their “natural” state, and this reduction in complexity can reduce the number of available niches. This means that the few species that can fill these niches will be abundant, but again, there will be fewer species than in a more complex environment.\nOur findings provide additional evidence that large-scale ecological patterns are being altered by human impacts, where some species will benefit, and others will be negatively impacted or even be lost.\nBernardo‐Madrid, R. et al. 2019. Human activity is altering the world’s zoogeographical regions. – Ecol. Lett. 22: 1297–1305.\nTorres‐Romero, E. J. and Olalla‐Tárraga, M. Á. 2015. Untangling human and environmental effects on geographical gradients of mammal species richness: a global and regional evaluation. – J. Anim. Ecol. 84: 851–860.\nTucker, M. A. et al. 2018. Moving in the Anthropocene: Global reductions in terrestrial mammalian movements. – Science. 359: 466–469.']	['<urn:uuid:9dbbc75b-a48f-4bbe-bb8e-56a6726dee2d>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:58:04.490895	9	73	536
53	anesthesia options dental fillings vs implants	For dental fillings, patients receive local anesthetic to numb the mouth, with an option for sedation dentistry if they are particularly fearful, requiring separate scheduling and transportation arrangements. For dental implants, patients can choose between local anesthesia that numbs only a small area in the mouth, or general anesthesia that prevents feeling any pain during the procedure, with general anesthesia requiring someone to drive them home afterward.	"['Dental fillings are one of the most common dental restorations used in modern practice today. They are used to repair minor fractures and minimally damaged teeth as well as cavities caused by tooth decay. In many cases, a cavity may not cause pain, but it should still be repaired as soon as possible. If a cavity is left untreated, it can cause an infection to spread throughout the tooth that may lead to an abscess.\nDetermining the Need for and Planning Your Dental Fillings\nIn most cases, our dentist will discover the need for a dental filling when you come in for your routine cleaning and checkup. To detect the need for a dental filling, the dentist may use a liquid or a dental probe. A dental x-ray will also be used to determine the size and exact placement of the cavity. Once this is determined, you will be informed of the various materials that are available for fillings. Patients are no longer limited to metal amalgam fillings; today, dental composites can be used to fill the cavity.\nDental composite fillings closely resemble your natural teeth and are both durable and affordable. Most patients prefer this option because the dental composite is tooth-colored. Additionally, when a dental composite filling is placed, it may be possible for the dentist to fill multiple cavities in the same day.\nFilling the Tooth\nWhen filling the tooth, the dentist will begin by numbing your mouth with anesthetic. If you are particularly fearful, you may be given the option of sedation dentistry. If you choose this option, you will need to arrange for someone to be able to drive you home. You will also likely need to schedule a separate appointment.\nOnce your mouth has been numbed, the dentist will prepare the tooth for the filling by removing the damaged and decayed tooth matter. The area will then be rinsed and thoroughly cleaned of debris. This removes any harmful bacteria from the tooth and ensures that the tooth decay will not progress once the filling has been placed.\nFinally, the dentist will isolate the tooth to be filled with a rubber dam. This will prevent your saliva or any moisture in your mouth from interfering with the bonding process. A special dental adhesive will be painted onto the tooth, and then the composite material will be placed. At this point, it has a putty-like consistency, allowing the dentist to shape the filling to fit your tooth. Once the desired shape is achieved, an ultraviolet curing light will be used to quickly harden the material. The filling will then be polished to remove any rough edges and to ensure that the filling blends in with the rest of the tooth.\nCaring for Your Composite Filling\nFor the first 48 hours following your restoration procedure, you may be asked to avoid brushing or flossing or at least be advised to brush gently. This will prevent the filling from becoming dislodged. Once this period is over, you will need to resume your ordinary dental hygiene routine of brushing twice per day and flossing once daily. You will also need to continue to return every six months for cleanings and checkups to ensure that the filling remains undamaged.\nOccasionally, the dentist may ask that you return more frequently. This is particularly true if you are prone to frequent cavities. In this case, your dentist will place sealants over your teeth to prevent tooth decay and to protect your filling from damage. You will also need to avoid any sugary foods and drinks, and try to avoid snacking between meals as often as possible.\nWhile dental composite fillings are not indestructible when well-cared for, the restoration can last for up to 10 years. However, if you notice any damage to the filling, or if you feel that your filling has begun to come loose, you will need to contact our office right away so the filling can be replaced. Once the filling becomes loose or damaged your tooth is once again vulnerable to further decay and infection.\nIf you have reason to suspect that you have a cavity, you need to contact our office today to schedule a consultation. We look forward to helping you once again have a healthy, bright smile once more.', ""Dental implants can be used to replace missing teeth. The implant is added to the jawbone and substitutes for the roots of the missing tooth. The implant procedure can be done by a dentist, periodontist, prosthodontist, or oral surgeon. It takes several appointments and is done over a period of 3-6 months.\nReasons for Procedure\nYou may have missing teeth due to injury, disease, or decay. Implants can be used to prevent problems associated with missing teeth, such as:\n- Difficulty chewing\n- Problems with appearance, especially if your face appears sunken due to missing teeth\n- Problems with remaining teeth—teeth may become tipped, spaced out, or crowded\nDental implants may also be used to replace dentures or help retain existing dentures.\nPotential problems are rare, but all procedures have some risk. Your dentist will review potential problems, like:\n- Infection at the implant site\n- Adverse reaction to anesthesia\n- Injury or damage to nearby teeth\n- Nerve damage\n- Sinus problems if implants were placed in the upper jaw\n- Implant is rejected by your body\nTalk to your dentist about these risks before the procedure.\nWhat to Expect\nPrior to Procedure\nImplants require several surgical procedures. Before getting implants, you will need to have a thorough dental exam, including having:\n- Dental x-rays\n- Models of your mouth\nYou and your dentist will make a treatment plan. Before treatment begins:\n- Talk to your dentist if you take any medications, herbs, or supplements.\n- You may need to stop taking some medications up to 1 week before the procedure.\n- Tell your dentist if you have any heart conditions or joint replacements. Your dentist may prescribe antibiotics before surgery to prevent infection.\nDuring the procedure, you may have local anesthesia, which only numbs a small area in your mouth. Or, you may have general anesthesia so you do not feel any pain during the procedure. Talk to your dentist to decide which option is best for you.\nDescription of Procedure\nAt your first visit, an implant will be placed. The gum will be cut open to expose your jaw bone. A hole will be drilled and the implant will be placed. The implant is made of titanium or another material. It is implanted deep into the jaw bone. Your gum will be closed over the implant. You will need to wait 3-6 months for the implant to fuse with your jaw bone.\nAt the next visit, the implant will be uncovered and an extension, called a post or abutment, will be inserted. The post will stick out past your gums. This is done so that there is something on which to attach the crown. For some types of implants, the implant and post will be inserted during the same visit. A mold will also be made of your upper and lower jaw. The mold will be used to create the crown in a dental lab. You may have a temporary crown placed over the post until it is time for the permanent crown to be attached.\nThe crown will be attached at a third visit after your gums have healed around the abutment post—usually 2-3 weeks.\nHow Long Will It Take?\nThe 3 visits will take place over 3-6 months. Each visit will take 30-60 minutes. However, the visit to place the implant may take a couple of hours.\nHow Much Will It Hurt?\nYou might have some pain while your gums heal around the implant. Your dentist may prescribe medication for the pain.\nYou will be able to leave right after the procedure if you had local anesthesia. If you had general anesthesia, you will need someone to drive you home.\nDuring your recovery:\n- You will need to have regular follow-up visits to monitor your implant, teeth, and gums to make sure they are healthy.\n- Avoid habits that can damage your teeth, such as chewing ice, biting your fingernails, and grinding your teeth.\n- Practice good oral hygiene habits. Brush your teeth twice each day and floss once each day. Visit your dentist regularly for cleanings and check-ups.\n- Be sure to follow your dentist's instructions.\nCall Your Dentist\nIt is normal to have some swelling or discomfort right after the procedure. Call your dentist if the pain worsens.\nIf you think you have an emergency, call for emergency medical services right away.\n- Reviewer: EBSCO Medical Review Board Donald W. Buck II, MD\n- Review Date: 03/2018 -\n- Update Date: 03/18/2013 -""]"	['<urn:uuid:bef92532-4fef-4c47-bac9-062d4e8067e2>', '<urn:uuid:075a2f8a-01f5-4244-bdeb-edc27952689a>']	open-ended	direct	short-search-query	similar-to-document	comparison	expert	2025-05-12T20:58:04.490895	6	67	1464
54	How do you properly finish and protect a wooden bottle opener that will be mounted on a fridge?	To finish a wooden bottle opener, first sand everything smooth using 120-grit sandpaper followed by 220-grit sandpaper. After sanding, apply Howard's Feed-N-Wax as a finish. To apply the finish, wipe it on with a rag or towel, let it sit for about 20 minutes, wipe off any excess that hasn't soaked in, then buff out the finish with a clean towel. Additionally, cover the back of the opener with adhesive cork sheets to protect the surface it's mounted on and provide a barrier between the magnets and the mounting surface.	['No longer should you open a bottle and worry about where the bottle cap has fallen. Did it roll under the fridge? Perhaps it sauntered beneath the stove with who knows what other refuse? Craft one of these, and your days of chasing bottle caps are over! Watch the video below for a complete overview, or keep reading for a step-by-step walk-through!\nWhat I used\nThis post contains affiliate links, Bear Maked LLC is a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.\n- A chunk of wood, at least 3″ x 8″ x 3/4″\n- Bottle Opener hardware\n- Round Rare Earth Magnets\n- Rectangle Rare Earth Magnets\n- Cork with Adhesive backing\n- This list is what I used, for each tool I used, there are other ways to get the same results\n- Bandsaw (any saw you’re comfortable using should work)\n- Power Drill\n- Forstner Bits (spade/paddle bits are a suitable replacement\n- Router with router table (mulitple suitable options if you don’t have access to this, see step 4 for more info)\n- Two-part epoxy\n- Random orbital Sander (120-grit and 220-grit sandpaper)\n- Howard Feed-N-Wax wood conditioner\nStep 1 – Choose your material\nFor this project, to keep it simple, it’s easiest to find a scrap or off-cut from previous projects. The nice thing, though, is that the final form of your bottle opener is entirely up to your imagination. For the purposes of this walkthrough, I’m sticking with a simple rectangle, cut out of the chunk of walnut in the top-right corner of the above image. I’d recommend using something that is at least 1/2-inch thick.\nStep 2 – Cut to size\nWith your material chosen, it’s time to get it to the right size. Luckily, for this project the “right size” is really whatever you want. I typically aim for it to be between 3 to 6 inches wide, and 8 to 12 inches tall. Remember though, it doesn’t even need to be a rectangle, make whatever shape you like, so long as you leave enough space to install the magnets in the proper places. I used my bandsaw for cutting it to size, but you can cut it however you wish.\nStep 3 – Layout the hole locations\nTo install the magnets, I first start by marking the location for the top two magnets. These will be what holds the completed bottle opener on the fridge (or to whatever other magnetic surface you choose to attach it). The magnets I use are just shy of 1 1/4-inches round, so I mark a location 1-inch down and in from the top and either side. Using two magnets here will ensure the force of a bottle being pulled againt the opener doesn’t pop the whole contraption off your fridge and make a giant mess, defeating the whole purpose.\nAfter you’ve marked the top spots, it’s time to mark the lower magnet location. You’ll want this spot to be in-line from where you’ll be installing the actual bottle opener hardware on the front-side of the wood. So if you plan to make that in the center of your board, then ensure this is also centered. For mine, since I’m using the rectangular magnets, I like to start the slot about 1-inch from the bottom of the wood, I don’t mark the end-point of the slot, as I’ll use the actual magnet for that when it comes time to create the slot. If you’d like to keep it simple and only use round magnets in your opener, then just mark here where you’ll drill another hole for the magnets, rather than where a slot will start.\nStep 4 – Makin’ magnet holes\nWe want these magnets recessed into the wood so the bottle opener will sit flush up against the fridge once it is complete. For this, I use my 1 1/4-inch forstner bit in my drill. If you don’t have a forstner bit, a spade or paddle bit would also get the job done. Use the locations you marked earlier as the center point for the hole. Don’t drill all the way through the wood! We only want to drill the same depth as the thickness of the magnet.\nThe bottom magnet can be a bit trickier, as we can simply drill one hole for it to sit into (silly drill bits not drilling rectangular holes!). I like to use my router in a router table to do this, but the router table isn’t necessary, you can do it with a handheld router as well.\nTo start, I drill a 1/2-inch hole at the location I marked earlier. I use 1/2-inch because that’s wider than my magnet, and I plan to use the same sized router bit. Again, this hole should not be drilled all the way through the board. You want to stop before the drill bit exits the other side. Keep in mind that forstner and spade bits typically have a point in the center that goes deeper than the rest of the bit, so be sure to stop drilling before that point pokes it’s way through. As a way to mark this, use a piece of masking tape, and wrap it around the drill bit, so you know when that tape gets to the surface you’re drilling into, it’s time to stop.\nThis hole will be used to start routing out the rest of the groove. With the proper router bit installed in the router, I move the fence on the router table out of the way, and place the wood so that the router bit sits inside the hole we just drilled.\nI can then bring the fence up to the edge of the wood, and tighten it back into place. Using a pencil, I mark on the table where the right side of the wood is, so I know this is where I should start routing each pass (I didn’t have enough room to the right side to use a stop block for this).\nThen I lift the board up from the router bit, and use the magnet to measure how long I’ll need the slot to be, by moving the board to the left the same distance as the length of the magnet. I can then tighten my stop block down at that distance.\nOnce you’ve got it all set up, take shallow passes to clear out the wood. Again, be sure not to go all the way through here, I normally stop when there’s about 1/8-inch of material left between the slot and the surface. To do this, you can use an adjustable square to find the thickness of your material, then take 1/8-inch off that measurement, and use that as a gauge for adjusting your router bit height. Alternatively, you can just hold your piece up next to the router bit while you adjust the depth of cut, and eyeball it to about 1/8-inch (this is probably the easier way to do it, but I didn’t think about it until after I finished this one…)\nAlternate Methods for the bottom magnet\nIf you don’t have a router table, that’s fine. This can be done by hand with router as well. Just do your best to keep the slot straight to ensure the magnet fits within it. If your router came with and edge-guide, I recommend using it.\nIf you don’t have a router, it’s still possible to accomplish this step. You can simply use a slightly larger drill bit than your magnet and drill a series of holes close to one another that will form a slot for the magnet.\nEven simpler yet, you could just drill the same size round hole and use the same round magnets as we did for the top. Again, just make this hole go through all but 1/8-inch of the wood. I’ve accidentally drilled too far and created a hole where there shouldn’t be one, and was super bummed. So I recommend caution. If you think you’ve gone far enough, hold the magnet in place and grab something metal, see if the magnet catches it through the wood.\nStep 5 – Sanding!\nThis step is pretty straightforward. Grab your sanding implement of choice and sand everything smooth. I used 120- then 220-grit, and that left it smooth enough for my liking. Be sure to wear proper safety gear, a dust mask or respirator while doing this. Your future self will thank you.\nStep 6 – Magnet installation\nInstalling the magnets is fairly straightforward. For the top magnets, if you are using the kind that can be attached with screws, check that your screws aren’t so long they’ll poke out of the other side of the wood. If they’re short enough, then that’s the best route to take to attach them.\nThe bottom magnet (and top magnets if the screws are a no-go), I use two-part epoxy to secure it into place. Mix up your epoxy according the directions on the package. I err on the side of “more is better” in this situation, and try to get the bottom magnet completely submerged in the epoxy to be sure it stays put. If I have any extra epoxy, and feel there’s plenty for the lower magnet to stay where it belongs, I use the extra for a little added strength on the top magnets.\nStep 7 – Put a cork on it!\nTo cover the not-very-aesthetic looking backside of our wooden plank, we’ll use adhesive cork sheets. This stuff comes in rolls, I just cut out a piece that is slightly larger than my board, remove the paper backing, and stick it on. Not only does this make the back look nicer, it provides a barrier between the magnets and whatever surface they’re attached to, preventing any scratches or other dings that may happen inadvertently.\nStep 8 – Apply a finish\nNow it’s time for what can be the best part of any woodworking project. Applying a finish. Especially when you have a hunk o’ wood with some nice character, a finish can really make that pop! My finish of choice here is Howard’s Feed-N-Wax. It’s easy to apply and does a great job of letting the wood look great, without any added shine or plasticky feeling to it. For this stuff, you just wipe it on with a rag or towel, let it sit for about 20 minutes, wipe off anything that hasn’t soaked in, then buff out the finish with a clean towel.\nStep 9 – Install the opener\nLay the opener on your board to get an idea of where you want it placed. Remember which end you have your top magnets installed, as you want the opener on that same end (I bring this up because I have made that mistake, and felt very silly afterwards…) I typically eyeball the placement of this to get it close to center. If you want to measure and ensure it is exactly centered, that is absolutely fine. Once you’ve got it where you want it, pre-drill the holes and then drive in the screws to hold it in place.\nStep 10 – Crack one open!\nAnd that’s that. Now you have a handy way to pop open any bottles that need to be popped open. There are many different ways to make this your own, from the wood species you use, to designs you could engrave or burn onto the wood. Or even combining different woods to create a unique look. Check the gallery below to see a few others I made from the scraps in the picture at the beginning of this guide (all of which are for sale!).\nThanks again for following along! Have fun and happy making!']	['<urn:uuid:d48e3776-1bb7-43d5-985a-228e5d79fcfc>']	open-ended	direct	verbose-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:58:04.490895	18	90	1981
55	security expert comparing vpn onion speeds performance	VPN and Onion connections both impact connection speeds negatively. While a VPN alone causes some speed reduction due to encryption, Onion over VPN leads to significantly slower speeds since traffic passes through multiple nodes with multiple layers of encryption - four layers when using Onion over VPN compared to just one with VPN alone.	['What is a VPN and how does it work?\nEdward Snowden was someone who reminded us of our right to privacy on the internet by revealing shocking information about snooping by NSA i.e National Security Agency, USA.\nFrom then whenever we saw debate or discussion about “Internet Privacy”, we have definitely heard about VPN.\nVPN technology that was once considered to be used by techies or hackers was the talk of the town, suddenly everyone started taking interest in knowing more about it.\nIn simple terms, VPN is something that will protect your privacy, help with implementing internet neutrality.\nThe following guide will help you understand the technology and will let you know how important it is for your daily internet usage.\nWhat is a Virtual Private Network?\nA VPN creates a secure communication tunnel over a public network setup. It uses encryption and authentication to make sure information is kept private and confidential. Therefore, you can share data as well as resources among various locations anonymously without your data being compromised.\nSo it is a virtual point-to-point link, which is determined by traffic encryption, virtual tunnelling protocols or dedicated connections.\nYou can consider this as an invisibility cloak that will hide you from the unsecured networks. But this also has secondary usage as it will also unblock the restrictions put by our ISP or your government as you are now invisible on their network.\nWhen it comes to TOR which was widely used earlier, it used to send your traffic over a widely distributed network. But this was not always secure and there could be chances that the exit node was compromised as successfully tested by NSA.\nHence VPN showed us the way to be completely anonymous online.\nHistory of VPN:\nOnly recently commercialisation of VPN has started. VPN was actually started my Microsoft so that their employees could access the workstation remotely over a secured network or channel.\nWell, this actually was a start of a revolution. Not only in terms of privacy or security, you could just wake up and start working on your assignments while lying on the bed.\nAs privacy concerns started growing, there were visionaries who saw a commercial market in this technology, thus the VPN that we see today was born. It was something that common people like us could enjoy and was very simple to use.\nDo we need a VPN?\nThe one word answer would be, “Yes”. We do need VPN in our day to day online activities. One of the best and recent example if of “Internet Neutrality”\nNet neutrality means that the ISPs should treat all their users equally i.e there shouldn’t be any discrimination based to the amount being paid.\nIn the year 2017, FCC nullified net neutrality so this means that there is still time for equal rights when it comes to internet.\nBecause of this ISPs can sell your data, like personal info, browsing habits, etc\nUsing VPN, you can hide all these activities from your VPN provider and thus be anonymous.\nThe second reason is because nowadays providing and using Hotspot has been trending. Such public Wi-Fi hotspot can be rigged and when you use such network, you are at a risk of compromising your data\nHow does VPN work?\nA VPN works as a private point-to-point tunnel created that is protected with encryption. On the other side of the tunnel, you connect to a VPN server. You can select from different geo-locations of a VPN server for your VPN connection.\nFor it to work you need to create a special network connection using your current internet connection.\nSo you will have pre-configured VPN server that you need to connect to using any of the VPN protocols like PPTP, L2TP, OpenVPN or SoftEther. For the connection, you can use an application or connect through the network settings of the device you are using.\nThis creates a secondary tunnel i.e the VPN. This tunnel will secure your data from various atrocity on the internet.\nWhat are the pros and cons?\nLet’s see the pros of using a VPN, which is not limited to the following:\n- Data encryption:\nA VPN scrambles all the data to and from your device like mobile, tablet or a PC, this is known as encryption.\n- Remotely access your workstation:\nWith the help of VPN, employees can securely connect to the company’s network over a secure encrypted network, enabling them to share files, documents securely.\n- Accessing internet while travelling becomes safe:\nA VPN is very useful when you are travelling abroad. You can maintain privacy and anonymity when sending and receiving files, checking bank accounts and even when uploading photos online by using a VPN, which uses an encryption.\n- No Logs:\nA trusted VPN provider ensures that there are zero logs of your browsing or session history and thereby shielding you from harm or tracking.\n- Bypass censorship:\nThis restriction can be from your ISP, school or office management, VPN helps you to bypass any of these browsing restrictions.\n- Save money using a VPN:\nThere are websites that modify prices of their service for different countries.\nFor example, airline ticket prices often change and several factors affect the prices. If you go to an airline company website to buy a ticket for the first time and you decide not to buy a ticket, the next time you check the same website, their price will go.\nVPN solves this very easily. Choose an IP address from a country that offers the cheapest ticket, and then access the airline website to book tickets\nAll the technologies out there will have some sort of disadvantage and VPN is no exception but the pros will always outweigh the cons when it comes to this technology.\nAs VPN provides encryption, the speed after VPN connection will always be less than the original internet speed. This will vary depending upon then VPN protocol you are using.\nMost of the times this difference in speed doesn’t affect your online experience.\n- Blocking of VPN:\nFew countries like China and Russia have started scrutinizing commercial VPN services so that they will still have control over their respective countries internet.\n- Complete Privacy:\nVPN doesn’t guarantee complete privacy because even after hiding the IP and using the encrypted tunnel, there are chances that you give away your identity by other means like browser cookies or browser location service\nWhat are the different types of VPN:\nWhen you see technically, there are two types of VPN.\n- Remote access VPN:\nThis allows a user to connect to a different network which can be in a totally different region using an encrypted tunnel. This network is mostly the internet.\n- Site to Site VPN:\nThis VPN is used inside the corporate network, specifically when they have offices in several regions. The internal network where the different offices can be connected with each other is known as an intranet.\nTo connect to any type of VPN, you need protocols and the oldest one which was developed by Microsoft is PPTP, this protocol is considered to be unsecure. Other widely used protocols are L2TP over IPsec, OpenVPN, SSTP and SoftEther.\nWhat devices can use VPN:\nNow you know about the working of a VPN, the next question that usually follows is, on what devices a VPN can be used?\nThe answer to this question is very simple, any device that you can connect to the internet can be configured to work with a VPN.\nNowadays most of the devices have an inbuilt VPN option and for devices that do not have this option can be connected to a VPN enabled router.\nHow to distinguish a good VPN:\nA superior VPN service will always have the following:\n- Ranking: Always check reviews of the VPN service. If there are negative reviews, it doesn’t make their service bad. You will always find positive and negative reviews, just make sure there are enough reviews for you to trust.\n- Encryption: The widely used encryption standard is the AES-256. This standard is almost impossible to crack as it has billions of combinations.\n- Logs: Make sure there is no log policy as you don’t want your browsing info to get into the wrong hands, defeating the purpose of using a VPN\n- Money back guarantee: This assures you that your money is safe, as you get a time period to test the VPN and if not satisfied, you get your money refunded.\n- Customer support: This is the backbone of any service and make sure the support is 24×7\nIs Free VPN good:\nNever go for free VPNs, you cannot be sure if you are secure over that network.\nThey can keep logs of your browsing history and if needed they could sell it as well. Hence it will become pointless using the VPN as you will be exposed to all threats\nRemember that they need money to keep their server running\nWhen it comes to paid providers all most all of them have a No Logging and Log Keeping Policy.\nThis was a simple guide that will make you understand the importance of VPN and how can you get it to use in your day to day life. Go for the VPN to have a anonymous life online.', 'Are you concerned about your online privacy? Do you feel your internet activities are being watched? If so, you’re not alone. This blog post is a must-read for you. We’ll delve into the concept of “Onion Over VPN”, a technology that takes your online anonymity to the next level. Stay tuned to learn how it works and why you might need it in your digital life.\nUnderstanding the Operation of Onion Over VPN\nWhile both Tor and VPN are designed to enhance your online privacy and security, they operate in fundamentally different ways. Tor aims to provide total anonymity, while a VPN is designed to ensure privacy.\nThese two tools can be used together to further bolster your online privacy and security. This is achieved by connecting to a VPN server before accessing the Tor network, a method known as Onion over VPN.\nHere’s a simplified explanation of how Onion over VPN works:\n- When you connect to a VPN server, it establishes an encrypted tunnel between your device and the VPN server. This hides your browsing activity and IP address from your Internet Service Provider (ISP) and other potential eavesdroppers.\n- If you then access the Tor network while connected to the VPN, the first node in the Tor network (known as the guard node) will only see the IP address of your VPN server, not your actual IP address. This provides an extra layer of privacy.\n- Your web traffic is decrypted once it leaves the VPN server, but it still carries the IP address of the VPN server.\n- As your traffic moves through the Tor network, it passes through three nodes (the guard node, a middle node, and an exit node), each of which adds an additional layer of Tor encryption. At each node, one layer of encryption is removed.\n- At the exit node, your traffic is decrypted before it reaches its final destination. The exit node can see your browsing data, but it doesn’t know the IP address of your VPN server.\nIn essence, the VPN server acts as an additional node in the data transfer process. This means that when using Onion over VPN, your traffic is protected by four layers of encryption, compared to three layers when using Tor alone.\nEvaluating the Safety of Onion Over VPN\nUsing a VPN in conjunction with the Tor network offers an added layer of privacy and security, enhancing your safety on Tor. However, it’s not a foolproof solution.\nOnion over VPN serves to safeguard your traffic and shield your browsing habits from prying eyes. It keeps your ISP in the dark about your use of Tor and prevents your VPN service from tracking your activities.\nDespite these advantages, Onion over VPN is significantly slower and less convenient than using the Tor browser alone. Therefore, it’s recommended only in extreme situations where security is of utmost importance.\nBelow is a comparison of the advantages and disadvantages of Onion over VPN to help you decide if it’s the right choice for you:\n|Adds an extra layer of encryption\n|Cannot dictate the location of your IP address\n|Allows access to the Tor network in locations with geographical restrictions\n|Connection speeds are significantly slower\n|Your ISP won’t know you’re using Tor\n|Websites frequently block exit nodes\n|Your VPN service can’t monitor your browsing activity\n|Your VPN service will be aware you’re using Tor, but won’t know what you’re doing\n|The Tor guard node won’t see your IP address\n|Your VPN service will know your IP address\n|Offers protection against certain types of malware\n|Tor only supports the TCP protocol\n|Safeguards against complete de-anonymization in traffic correlation attacks\nLimitations on Monitoring by Your ISP and VPN\nThe use of Tor is often linked to illicit activities due to its connection with the dark web, which can potentially flag you for government surveillance.\nEmploying Onion over VPN can mitigate this risk as your Internet Service Provider (ISP) will be unaware of your Tor usage.\nWhile your VPN service will be aware of your connection to Tor, it won’t have visibility into your activities within the network. Even if your VPN service maintains connection logs, it won’t be able to determine the specific websites you visit within the Onion network.\nMoreover, the Tor guard node will only see the IP address of your VPN server, not your actual IP address, offering an extra layer of privacy.\nRisks from Malicious Exit Nodes\nA VPN provides encryption for all internet traffic between your device and the VPN server. When you use Onion over VPN, your traffic is decrypted after it leaves the VPN server and is then sent to the Onion network.\nWithin the Onion network, your traffic undergoes three additional rounds of encryption and decryption before it reaches its final destination. The traffic is finally decrypted at the exit node, which allows third parties to see your online activity, but not your identity.\nTor is a community-driven network where anyone can set up nodes, potentially enabling them to monitor your activity. An exit node that is configured to spy on or exploit Tor users is referred to as a malicious exit node.\nIf you access web servers using HTTP while using Onion over VPN, your traffic is not encrypted between the exit node and the web server, making it visible to malicious exit nodes.\nOn the other hand, if you access web servers using HTTPS while using Onion over VPN, your traffic remains encrypted between the exit node and the web server. This means that while exit nodes can see the data packets being transmitted, they cannot decrypt them.\nProtection Against Malware\nTor is sometimes vulnerable to malware attacks. However, when you use Onion over VPN, your VPN can serve as a shield between your device and potential ransomware by protecting open ports via the VPN server’s firewall.\nOpen ports are responsible for receiving and transmitting TCP and UDP communications. While they are generally not harmful, they can be exploited to distribute malware and gain unauthorized access to personal data on your device.\nWhile VPN software isn’t specifically designed to guard against malware, the firewall of a VPN can help block port forwarding, thereby preventing unauthorized access to your device. This provides a certain level of protection against malware, making Onion over VPN a safer option than using Tor alone.\nSecurity Against Traffic Correlation Attacks\nTraffic correlation attacks involve third parties monitoring your traffic to find a link between the data entering and leaving your network.\nBy analyzing the timing, behavior, and volume of traffic, it’s possible to identify the websites you’re accessing at specific times. Using Onion over VPN can help protect against complete de-anonymization, as your actual IP address is concealed by the VPN software.\nHowever, if a government or state agency is conducting the traffic correlation attack, they can legally compel your VPN service to surrender any records associated with the VPN IP address.\nIf your VPN service maintains connection logs, these could potentially be matched to your identity. This underscores the importance of choosing a VPN service that only logs essential data.\nPrevention of UDP Data Leaks\nTor is limited to supporting the TCP protocol, which implies that UDP traffic, including Voice-over-IP, must travel outside the Tor network. This could potentially expose your IP address and online activities to third parties, a situation referred to as “bad apple” attacks.\nHowever, using Onion over VPN can offer a layer of defense against such attacks. Any leaked UDP traffic would still be channeled through the VPN server within an encrypted tunnel, preventing it from being used to identify you.\nNevertheless, data leaks can occur if the VPN connection is disrupted. Therefore, it’s crucial to comprehend which traffic can traverse the Tor network and adjust your online behavior accordingly.\nAccessing Tor in Geographically Restricted Countries\nWhile the use of Tor is legal in many countries, it is prohibited in certain regions, including Belarus, Turkey, and China.\nVPN software provides users with the ability to conceal their actual location by establishing a connection to a VPN server in a different country. This means that you can use a VPN to download and use the Tor browser even in countries with stringent censorship.\nHowever, some websites block traffic originating from Tor IP addresses, which could pose challenges in accessing certain services via Tor. To circumvent this issue, using a VPN over Tor could be a more effective solution.\nDetermining When to Use Onion Over VPN\nOnion over VPN is typically suggested for individuals who are transmitting highly confidential information and could benefit from an extra layer of encryption. This setup can be particularly useful for whistleblowers and journalists in countries with high levels of censorship, as it allows them to circumvent geographical restrictions and access the Tor Network.\nConsider using Onion over VPN if:\n- Maintaining anonymity is your top priority\n- You’re unable to directly access the Tor Network\n- You’re a journalist, whistleblower, or activist\nOn the other hand, Onion over VPN may not be suitable if:\n- You’re casually browsing the internet\n- You’re visiting websites that require identification (e.g., social media)\n- Connection speed is a critical factor for you\nWhile this method can safeguard your device from ransomware and traffic correlation attacks, it doesn’t offer protection against malicious exit nodes. Therefore, caution is advised when using Tor. Due to the negative impact on connection speeds, this method is not recommended for casual internet users.\nKeep in mind: absolute anonymity is unattainable. VPNs don’t provide complete anonymity, and your VPN service can technically detect your use of the Tor browser. Essentially, you’re transferring your trust from your ISP to your VPN.\nThe worthiness of Onion over VPN is thus situational. While the setup does offer some additional privacy and security, it can’t shield you from malicious actors within the Onion network.\nGuide to Using Onion Over VPN Safely\nIf you choose to use Tor over VPN, this section will guide you on how to do so safely.\nSetting up Tor over VPN is quite straightforward. You just need to establish a connection to your VPN before starting the Tor browser.\nHere are the steps to safely set up Tor over VPN:\n1. Subscribe to a VPN service that doesn’t keep logs, or select from our list of the best VPNs for Tor. We suggest using Private Internet Access with Tor as it offers a zero-logs policy, fast speeds, and reasonable pricing.\n2. Connect to a VPN server. Choose a remote VPN server and connect to it. The closer the server is to your actual location, the faster your connection speeds will be.\n3. Download the Tor Browser from the official Tor website.\n4. Click on your device’s icon and follow the installation prompts to automatically install the Tor Browser on your device.\n5. Launch the Tor Browser while still connected to the VPN server.\n6. Browse anonymously. While browsing, it’s crucial to avoid visiting websites that could compromise your anonymity. This includes logging into personal online accounts or sending emails.\nComparing Tor Over VPN and VPN Over Tor\nThe Tor Browser and VPN software can also be utilized in the reverse order. In this scenario, you initiate the Tor browser before connecting to a VPN. This method, known as VPN over Tor, is the inverse of Onion over VPN.\nWith VPN over Tor, your traffic is first routed through the Tor network, and then it is directed through the encrypted VPN tunnel. Unlike Onion over VPN, exit nodes cannot view the content of your traffic, but guard nodes can see your actual public IP address.\nThe Tor Project advises users against using VPN over Tor as it can compromise your anonymity.\nBelow is a table to help you understand the advantages and disadvantages of using VPN over Tor:\n|Protection from malicious exit nodes\n|Guard node can see your public IP address\n|VPN kill switch prevents data leaks\n|Setup is complex\n|Access to websites that block exit nodes is still possible for Tor users\n|Choice of VPN service is limited\n|Connection speeds are extremely slow\n|Double encryption is excessive\n|Your VPN service can theoretically monitor activity\n|ISP can see you are connected to Tor\nSetting up VPN over Tor is complex and offers less anonymity than Onion over VPN. Assistance from your VPN service is necessary, which means your choice of VPN will be limited.\nWe advise against setting up VPN over Tor unless you have technical expertise. An incorrect setup can lead to traffic leaks, potentially compromising both your anonymity and security.\nHere’s a summary of how VPN over Tor impacts your privacy and security:\nMonitoring Capabilities of Your VPN Service\nThe Tor project advises against the use of VPN over Tor as it allows your VPN provider to access both your IP address and the contents of your web traffic, thereby undermining Tor’s main objective: total anonymity.\nCertain VPN services maintain logs of your online activities. If you use one of these VPN services after connecting to Tor, your online activities will be completely traceable.\nRisk of Being Flagged for Surveillance\nIf you initiate a connection to Tor prior to connecting to a VPN, the Tor guard node will be able to see your public IP address. Although it can’t decode the contents of your web traffic, this setup is less private.\nWithout first establishing a connection to a VPN server, your Internet Service Provider (ISP) will be able to detect your use of Tor. This could potentially flag you for surveillance by government agencies, including the National Security Agency (NSA).\nThe NSA collects and stores the IP addresses of numerous Tor users. This data is gathered by XKeyscore, a computer system developed by the NSA for the purpose of analyzing online activity.\nProtection Against Malicious Exit Nodes\nWhile VPN over Tor has its limitations in terms of privacy and anonymity, it does offer an added layer of protection against malicious exit nodes.\nIn contrast to Onion over VPN, if you connect to a VPN after accessing Tor, the Tor exit nodes won’t be able to view your data. The VPN encrypts all traffic from the exit node to the internet, meaning the exit node will know you’re using a VPN, but won’t be privy to the websites you’re visiting.\nIf your VPN connection gets disrupted, a kill switch will sever your device’s internet connection. This prevents your IP address and other sensitive data from being exposed. These combined measures significantly reduce the likelihood of third parties exploiting your information via malicious exit nodes.\nAccess to Websites Blocking Tor Traffic\nWhen using VPN over Tor, websites that typically block Tor IP addresses become accessible. These websites receive your decrypted traffic from the VPN server, which means they won’t be able to see the IP address of the Tor exit node. Consequently, you can access websites that are typically inaccessible when using the Tor Browser.\nHow Double VPN Differs from Onion Over VPN\nAlso referred to as “multi-hop VPN”, Double VPN is an advanced feature provided by some VPN services for enhanced security. Essentially, it directs your traffic through two distinct VPN servers instead of just one.\nNordVPN’s Double VPN feature can be found in the protocol menu.\nWhile Double VPN and Onion over VPN may seem similar, they have structural differences. Onion over VPN connects users to two different types of anonymity software, while Double VPN routes your traffic through the same type of software, but twice.\nHere’s a rundown of the key similarities and differences between Double VPN and Onion over VPN:\n- Both Double VPN and Onion over VPN significantly slow down connection speeds.\n- Both setups encrypt your web traffic multiple times.\n- Onion over VPN conceals your IP address from the guard node, while Double VPN hides your IP address from the second VPN server.\n- Onion over VPN employs two types of anonymity software: VPN software and the Tor Browser. Double VPN uses only VPN software, which centralizes your risk.\n- Onion over VPN encrypts your data four times, whereas Double VPN does so twice.\n- Double VPN supports both the UDP and TCP protocol, while Onion over VPN supports only the TCP protocol.\n- You can’t access the dark web or .onion websites with Double VPN. Onion over VPN allows access to the dark web.\nLike Onion over VPN, Double VPN is designed to mask your online activity from your ISP and other intrusive third parties. It encrypts your data twice, significantly impacting your connection speeds.\nIn the Double VPN setup, the second VPN server only sees the IP address of the first VPN server. While some users view this as an additional layer of protection, it’s largely unnecessary for most users.\nIn conclusion, understanding “What Is Onion Over VPN“ is crucial for anyone concerned about online privacy. This technology offers a robust solution to keep your internet activities anonymous and secure. We hope this blog has shed light on its importance and functionality. For more insightful content, don’t forget to explore other article from Twistory. Always update good and useful information.\nSee more related articles:\n1. How to Set Up a VPN on Roku: Step-by-Step Instructions']	['<urn:uuid:30cc2ba6-a239-407a-9a0c-aa54084ab8f6>', '<urn:uuid:38b4a4d3-dd48-4f4c-be54-b3519c7c313e>']	factoid	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T20:58:04.490895	7	54	4418
56	pregnant infection toxoplasma cats risk symptoms	Pregnant women are especially at risk for toxoplasmosis. If they become infected, they may pass the infection to their developing fetus, which can result in birth defects. Symptoms in affected children can include nervous system disorders, premature birth, hearing loss, anemia, and retinal damage. This is why pregnant women are often advised to avoid contact with cat feces and litter boxes.	['Toxoplasmosis is a parasitic infection in warm-blooded animals, including humans, caused by a protozoan organism called Toxoplasma gondii. According to the National Center for Biotechnology Information, T. gondii is the most common protozoan parasite in developed nations.\nCats are the definitive host for T. gondii, which means that the parasite can complete its life cycle and reproduce using the cat as the host. The eggs are called oocysts and are passed through cat feces into the environment, where they can infect additional cats and other warm-blooded creatures. If the conditions are right, the oocysts can persist for quite some time in the soil.\n“Toxo” has appeared in medical news in the last few years, most notably with reports on the effects that T. gondii may possibly have on behavior, as well as a possible link to schizophrenia and bipolar disorders (University of Leeds).\nInfected mice have been reported to exhibit behavioral changes as well as diminished motor performance, potentially making them more compliant to felines who hunt them. Using personality questionnaires and behavioral tests, studies have uncovered differences in the behavior of infected people versus non-infected people as well.\nSo, the next time someone says you’re acting strangely, blame it on the cat.\nDespite these studies, toxoplasmosis infection is usually asymptomatic, but in certain cases, this disease can do serious damage. Pregnant women and those who are immune compromised are especially at risk.\nPregnant women who become infected may pass the infection on to the developing fetus, resulting in birth defects. Symptoms in these children may include nervous system disorders, premature birth, hearing loss, anemia, and retinal damage. You may have heard that pregnant women should avoid litter boxes (or more accurately, cat feces), and this is why.\nIn otherwise healthy adults, if symptoms do occur, they can be flu-like, and include sore throat, fatigue, fever, and vision problems. In people with compromised immune systems, say to due HIV infection, however, symptoms of toxoplasmosis may be severe and include seizures, lung problems, and severe inflammation of the retina.\nOthers at risk are those who have had organ transplants, are recently recovering from surgery or serious illness, or chemotherapy patients. For the curious, here’s a more complete list of toxoplasmosis symptoms in both humans and cats.\nIn cats, infection may occur due to consumption of an infected rodent or bird, eating raw meat (or undercooked), or contact with contaminated soil or cat feces. If your cat does not roam outdoors or eat raw meat, she’s much less likely to become infected. Even so, an infected cat is only able to pass on the infected eggs for about two weeks after becoming infected.\nBecause they’re the definitive hosts, direct contact with cats is often blamed for the spread of the disease. In the developed world, however, the most common method of infection in humans is actually via the consumption of undercooked or raw meat.\nIn fact, according to the Centers for Disease Control (CDC), Toxoplasmosis is the third leading cause of death attributed to food-borne illness in the United States. Approximately 50 percent of the 750 deaths per year due to toxo are food related.\nAnother high risk activity is gardening without gloves, resulting in contamination via the soil. While vegetables are part of eating a healthy diet, they may also be contaminated. In undeveloped countries, soil contamination combined with hygiene and sanitation issues are high risk factors.\nInterestingly, in a 2004 study of 102 health practitioners, one-fourth of all study participants “inappropriately advised pregnant women to avoid all cat contact.” Perhaps even more interesting, “obstetricians, internists and family practitioners were all likely to fail to identify undercooked meat consumption as the primary risk factor…”\nBelow are some tips for you and your cat on how to avoid contracting toxoplasmosis.\nEditor’s Note: Triclosan is an ingredient found in some antibacterial soaps along with some toothpastes and deodorants. New research published in the Journal of Medicinal Chemistry shows that triclosan blocks a key enzyme that the parasize, Toxoplasmosis Gondii, uses to live. Though Triclosan can’t be ingested and used as a medication itself, it could lead to the development of new treatments.']	['<urn:uuid:17d9900e-df5e-43e7-99e2-4161fa74d1c7>']	open-ended	with-premise	short-search-query	similar-to-document	single-doc	novice	2025-05-12T20:58:04.490895	6	61	689
57	international convention migrant workers families history implementation delays	The International Convention on the Protection of the Rights of All Migrant Workers and Members of Their Families (ICRMW) was signed on December 18, 1990, and represents the biggest milestone in migrant human rights history. However, it was not enforced until July 1, 2003, more than 10 years after signing. This slow progress was primarily due to weak interest among countries in ratifying it. The ICRMW is one of the seven human rights instruments of the UN and is notably the oldest instrument and the slowest to progress.	"[""What are Human Rights?\nAgreeing on what exactly are human rights and how they can be used by a person is a subject of debate among scholars, human rights activists, citizens, and governments all over the world. According to the Office of the United Nations High Commissioner for Human Rights (OHCHR): “Human rights are rights inherent to all human beings, whatever our nationality, place of residence, sex, national or ethnic origin, colour, religion, language, or any other status.” What precisely these rights are, how they developed in human history, and whether they can be universally applied are what make the concept of human rights an ongoing discussion.\nAlthough the idea that people have intrinsic rights as human beings can be traced back to ancient times, the process of identification, recognition, legal protection, and internationalization of human rights in their present form began only after the conclusion of World War II (1939–1945) and the establishment of the United Nations (UN) in 1945. The most accepted document in the history of human rights is the Universal Declaration of Human Rights, a document that was drafted by representatives across the world and proclaimed by the UN General Assembly as legally binding on December 10, 1948, in Paris.\nThe document contains 30 different articles, outlining human rights that all people are entitled to and that should be universally protected. In addition to the UN, human rights organizations such as Amnesty International and Human Rights Watch advocate for the protection of these rights, but in the 21st century not all countries have accepted and adopted them.\nDespite the problem of defining human rights and establishing their validity in various cultures, human rights became an important part of the conversation on migration and immigration in the 20th and 21st centuries amid massive movements of people worldwide. Migrants are particularly vulnerable to abuse and violation of their human rights because they lack the very systems that recognize and protect these rights.\nWhen migrants are in transit or are not in their country of origin, they may be unable or even unwilling to ask for protection from foreign countries. In turn, governments may choose—or not be able—to uphold their obligations of protecting the human rights of these migrants.\nThe biggest milestone in the history of migrant human rights is the entering into force of the International Convention on the Protection of the Rights of All Migrant Workers and Members of Their Families (ICRMW) after it was signed on December 18, 1990. The ICRMW is one of the seven human rights instruments of the UN.\nHuman rights instruments are international documents that contain laws and agreements related to human rights and are used by international institutions in addressing human rights issues and violations. It is also the oldest instrument and the slowest to progress as it was not enforced until July 1, 2003, more than 10 years after it was signed. The slow progress was due to several factors, most important of which was the weak expression of interest among countries in ratifying it.\nAfter World War I (1914–1918), the United States experienced a large influx of migrants, primarily from eastern and southern Europe. The US government sought to restrain the flow through the 1921 and 1924 immigration acts. The passing of these laws in the US Congress was made possible by a nationwide uncertainty over national security generated by the war. These measures led to restricted migration from Europe to the United States but also intensified migration flows from southern Europe to northern Europe.\nThe 1919 Treaty of Versailles, the peace agreement that officially ended World War I, also created the League of Nations, an intergovernmental organization that aimed to establish diplomatic ties and peaceful mediation between 42 member nations. One of the league's agencies, the International Labour Organization (ILO), was concerned with the improvement of the living standards of workers around the world. In 1939 the ILO released ILO Convention No. 66 (Convention Concerning Migration for Employment), a document that aimed to strengthen the protection of migrants.\nThe convention, however, was never entered into force—not a single country gave formal consent to it. The League of Nations ultimately proved a failure in its primary purpose, too, preventing world war, as evidenced by the start of World War II. After World War II, the UN was created and the recognition of human rights flourished with the adoption of the Universal Declaration of Human Rights as an important milestone.\nWORDS TO KNOW\nThe protection given by a nation to a refugee who has fled his or her native country in order to escape harm.\nAn act by which something is acquired by force or threats.\nINTERNATIONAL DETENTION COALITION:\nA nonprofit organization and network of more than 250 other nongovernmental organizations that conducts research, reporting, and advocacy work for migrants, asylum seekers, and refugees.\nA path of migration that does not follow authorized, legal channels.\nThe basic principle on which the United Nations' 1951 Refugee Convention rests, requiring asylum nations not to return or expel refugees to countries in which their lives are in danger, their freedoms might be or have been threatened, or they might be or have been physically harmed.\nOFFICE OF THE UNITED NATIONS HIGH COMMISSIONER FOR HUMAN RIGHTS (OHCHR):\nThe United Nations agency responsible for overseeing major programs in protecting human rights and implementing international rights agreements.\nUNITED NATIONS COMMISSION ON HUMAN RIGHTS:\nThe predecessor of the United Nations Human Rights Council.\nUNITED NATIONS ECONOMIC AND SOCIAL COUNCIL:\nOne of the six principal organizations of the United Nations; it serves as an international forum for economic and social issues.\nUNITED NATIONS GENERAL ASSEMBLY:\nOne of the six principal organizations of the United Nations; it is the only body of the organization in which all member nations have equal representation.\nUNITED NATIONS HIGH COMMISSIONER FOR REFUGEES:\nThe United Nations agency responsible for working with refugees around the world.\nThe ILO's advocacy on protecting the rights of international workers resumed in 1949 when it released ILO Convention No. 97, a revision of ILO Convention No. 66. This move was followed by other provisions in regional treaties across Europe for migrants. The 1955 European Convention on Establishment sought to ensure the protection of migrants, while the 1961 European Social Charter encouraged the equal treatment of all migrant workers in terms of their remuneration, housing benefits, and union memberships.\nThe impact of these legal instruments was limited to a few countries and specific regions. This created problems for migrants from regions and countries that did not belong to these regional treaties because these migrants did not have any universal human rights instrument to lay claim on. Migrant workers and irregular migrants, or those migrants who did not have necessary authorization or legal documents, became prone to violence and abuse. Such violations continue to be a concern in parts of the world in the 21st century.\nNevertheless, the human rights movement continued to progress in the 1960s. Two important international legal instruments that improved human rights discourse were signed in 1966: the International Covenant on Economic, Social and Cultural Rights and the International Covenant on Civil and Political Rights.\nAlthough the recognition of human rights increased in the 1960s, public interest on migration also heightened when Canada and the United States lifted restrictions on migrants. In the 1970s improvements in the European economy encouraged Europeans to stay in their own countries, lessening migration from the continent to the United States.\nWith the decrease of migrants from Europe, the United States welcomed new migrants from Asia, Mexico, and the Caribbean. At this time, Australia's migrant population also increased and diversified after it abandoned its “white Australia” policy, a collective term used to refer to the many historical policies that prevented the immigration of non-Europeans to the country.\nIn 1972, 28 migrants from Mali died while being smuggled in a truck through the Mont Blanc Tunnel connecting France and Italy. This incident brought the issue of migrant human rights violations to the UN. Kenya insisted that an investigation be conducted by the UN Economic and Social Council.\nIn the same year the ILO began to address labor trafficking and irregular migration, while the UN General Assembly requested the UN Commission on Human Rights (the predecessor of the UN Human Rights Council) to prioritize the discrimination suffered by migrants.\nPublic concern regarding migration and the rights of migrants arose after the 1973 Arab-Israeli War, which resulted in repercussions to the oil supply and the world economy. Europe began to restrict migrants and the Middle East became a new destination for international laborers, especially from South Asia, Southeast Asia, and Arab countries.\nWhile traditional migration channels became more secured for safe use by migrants, new policy restrictions imposed resulted in an increase in irregular migration. As previously noted, irregular migrants experience more human rights abuse than migrants with authorization or documents.\nAll of these events spurred the UN during the late 1970s to the 1990s to work toward drafting, ratifying, and enacting an international human rights instrument specifically for migrants. It is important to remember that the ICRMW was created with two perspectives in mind, that of human rights advocates and that of people concerned with the economic effects of irregular migration.\nNevertheless, the ICRMW contains provisions that can be applied to protecting other classes of migrants, such as irregular migrants or migrants in transit, who have become the primary victims of human rights abuse in the 21st century.\nImpacts and Issues of Human Rights\nThe potential for human rights violations arises the moment migrants leave their territories of origin. Migrants who were forced out of their countries because of necessity instead of free choice are at greater risk of human rights abuse. With limited options, they may have no choice but to travel to areas where their rights are not respected. The following sections present different situations of human rights concerns that migrants may experience throughout their journeys.\nA migrant's right to free movement can be considered to have been violated by countries that erect physical barriers, such as walls or razor-wire fences, that prevent people from crossing borders and seeking asylum, a safe place to live.\nSimilar interceptions by nations have been done at sea. The 1979 International Convention on Maritime Search and Rescue obliges nations to respond to anyone in distress at sea regardless of nationality or status. Representatives from the UN found that this principle was ignored by the governments of Thailand, Malaysia, and Indonesia in May 2015 when they implemented a pushback policy against irregular migrants. Amnesty International reported in 2015 that a similar disregard of this principle was exhibited by Australia after it executed a turn-back policy that forced migrant boats from Indonesia to return to sea in unsafe conditions. Interceptions such as these are being done at a number of borders across the globe.\nInadequate Conditions of Detention\nMigrants are protected from detention by their right to liberty and security, as stated in the 1948 Universal Declaration of Human Rights. This right is crucial for asylum seekers and refugees because of their problematic legal status in a foreign land, which can often result in their detention by authorities for illegal entry. In the norms and principles of international laws such as the 1988 UN Body of Principles for the Protection of All Persons under Any Form of Detention or Imprisonment and the 1989 Convention on the Rights of the Child, detention is only allowed as a measure of last resort, and even when detention is reasonable in some instances, international human rights law guarantees that all migrants are entitled to good treatment and adequate living conditions.\nNevertheless, studies around the world show that detention of migrants is usually an immediate response to government interception. It has become routine and in some cases even mandatory. Detained migrants can find themselves living in poor conditions and being denied access to legal aid or interpretation services that could help them navigate their situation.\nIn 2014 the UN Support Mission in Libya (UNSMIL) and the UN High Commissioner for Human Rights reported at the 28th Session of the Human Rights Council that detained migrants in Libya were forced to live in overcrowded detention centers with unsanitary conditions, lacking access to adequate health care and sufficient food. UNSMIL also discovered consistent occurrences of other forms of human rights violations such as physical, verbal, and sexual abuse; labor exploitation;, extortion; and the detention of minors. According to the Global Campaign to End Immigration Detention of Children, hundreds of thousands of children are similarly detained around the world every year.\nA report by Robyn Sampson and others with the International Detention Coalition found that detention is expensive and places a financial burden on countries that detain migrants. In contrast, when migrants are empowered through alternatives to detention, such as legal assistance and social support, and are allowed to independently move, countries save approximately 70 percent compared to detention and assisted deportation procedures, as was the case in Australia and several countries in the European Union.\nViolations of the Principle of Non-refoulement\nInternational law protects migrants from being forced to return to their country of origin when doing so will expose them to serious harm. This is called the principle of non-refoulement. Such expulsions are allowed only after certain requirements recommended by the OHCHR, such as the consideration of migrants' individual circumstances, the provision of informed consent before proceeding to such expulsions, and the absence of coercion in the deportation process, are met.\nA 2012 study, Toward a World Free from Violence, conducted by the UN Secretary-General on Violence against Children, examined the conditions of migrant children whose families were forced to return from Germany to Kosovo. Seventy percent of the children dropped out of school upon return and many were returned to conditions of poverty where access to health care was limited and difficult. Doctors without Borders published a 2013 report on a survey conducted among sub-Saharan African migrants in Morocco. According to the study, 68 percent of the migrants claimed that they had been arrested and expelled since their arrival to Morocco and 80 percent of them claimed that they were expelled more than once. Migrants from Guatemala attempting to enter the United States through Mexico repeatedly made irregular attempts to cross the borders after being deported multiple times. Duncan Wood and his colleagues, the authors of the 2015 study Reflections on Mexico's Southern Border, disclosed that each attempt cost the equivalent of $7,000 and was done through the help of smugglers.\nInternational human rights laws compel countries to provide police and criminal justice protection to migrants who are victims of violence. Nations are also encouraged to provide professional services to address physical and emotional distress experienced by these migrants. Irregular migrants are vulnerable to violence from many sources including governments.\nIn 2010 the drug-trafficking cartel known as Los Zetas massacred 72 migrants in Tamaulipas, Mexico, when their families failed to pay ransom. On August 26, 2015, the UNHCR reported that 11 or 12 out of every 1,000 people crossing the Andaman Sea via smuggler boats died because of violence, dehydration, or starvation. The next month, on September 17, 2015, Hungarian security forces assaulted migrants, including children, who were crossing Hungary's border with Serbia, using water cannons and tear gas to drive them away.\nDenial of Access to Health Care\nInternational law guarantees the right of migrants' access to health care from host countries regardless of their legal status. These rights, however, may not be honored when nations lack the appropriate resources or deliberately refuse to provide medical staff and equipment. This puts migrants at risk as they are often in need of first aid and other immediate health interventions upon arrival to their country of destination or after being rescued or intercepted.\nIn contrast, there have been positive developments in countries in which health-related rights of migrants are being recognized. For example, in April 2011 the Israeli High Court of Justice declared unconstitutional a rule that permitted the deportation of pregnant migrant workers.\nDenial of Access to Employment\nTo sustain themselves and their families during travel, many migrants are forced to look for sources of income. Nevertheless, they are usually denied access to good employment because of discrimination, restrictions due to their legal status, and lack of support from host countries.\nIn her 2015 study titled “Complex Migration: A Woman's Transit Journey through Mexico,” Carla Angulo-Pasel found that many of the migrant women in transit in Mexico worked in the informal sector—domestic work, hospitality, or entertainment—jobs to which they were restricted because of their gender and legal status and in which their labor rights had very limited protection. Similar conclusions were reached by a study of migrant children in Thailand, who worked in jobs with high risks of labor exploitation.\nDenial of Access to Shelters\nEven when migrants are rescued and are allowed to temporarily settle in a foreign country, human rights violations may still occur as migrants are forced to endure undesirable living conditions. Doctors who diagnosed the diseases of sub-Saharan African migrants living in transit in Morocco in 2013 found out that half of these diseases were caused by or were closely related to the migrants' being in destitute shelters. Similar circumstances befall countless migrants around the world as they are denied access to humane accommodations. Even though international law guarantees the basic civil, political, economic, social, and cultural rights of all human beings, the rights accorded to migrants often come up short.\nIn light of all the issues concerning migrant human rights, the UN, other institutions, and concerned groups from civil society continue to advocate for the protection of migrants and the improvement of their living conditions. This is especially important as conflict, climate change, and economic problems in different parts of the world force people to move in the 21st century.""]"	['<urn:uuid:08ecbeb6-183a-46df-945c-4b1680eddf53>']	open-ended	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T20:58:04.490895	8	88	2972
58	explain infonomics definition quick	Infonomics is a term coined by Gartner to describe the theory and practice of treating information as an actual corporate asset.	['Keeping Data Fresh: Data Monitoring and Governance\nThink of the produce aisle at the supermarket, full of items of different tastes, flavors, colors, and shapes. You go in search of the fresh ones, the ripe ones, the right ones – and you find none! Some have expired dates, some have unwanted ingredients, and some are not on the shelves. Data in enterprise systems is like food – it has to be kept fresh, it has to have the nourishment you need, and you have to be able to find it; otherwise it goes bad and doesn’t help you in making strategic and operational decisions. Just as consuming spoiled food could make you sick, using “spoiled” data may be bad for your organization’s health. There may be plenty of data, but it has to be reliable and consumable in order to be valuable. While most of the focus in enterprises is often about how to store and analyze large amounts of data, it is also very important to keep this data fresh and flavorful. How do you do this? By monitoring, auditing, testing, managing, and controlling the data. In other words, it is all about setting strong data governance in place for enterprise data platforms.\nIn the previous blogs of this series, we have covered the readiness of the Enterprise Data Platform for digital transformation along with mechanisms and tools for ingesting various kinds of enterprise data, the data processing options and capabilities including data lakes, and also how enterprise platforms have different shapes and flavors to store data. In this particular blog post, Deepa Deshpande and I discuss the importance of having quality data and how to monitor and set up data governance practices for enterprise digital data.\nData governance means asserting authority over the data in the organization, protecting, monitoring, correcting it: in short, treating the data as the critical corporate asset that it is. (Related to this, Gartner has coined the term ‘infonomics’ to describe the theory and practice of treating information as an actual corporate asset.) The diagram below shows the architecture of the modern Enterprise Data Platform and shows that the Data Monitoring and Governance function cuts across multiple layers of the architecture. This is because the data governance layer needs to ensure that data movement between layers follows the desired workflow and schedule requirements of the business. This layer needs to make sure that proactive measures are taken to meet the quality standards of the organization. Also, this layer keeps checks on errors, guards the systems against thefts or losses, and alerts system admins if required. The governance and monitoring needs to make sure that the user security and privacy is accounted for. The layer makes sure that the data workflows through other layers adhere to privacy, security, regulatory, and compliance requirements.\nWith the proliferation of digital transformation, enterprises need to have an even more comprehensive data governance program. It is important for organizations to devise proper means and mechanisms to collect all the data that is flowing through enterprise systems. Within Enterprise Digital Transformation systems, the 3Vs of “big data” (volume, velocity, variety) manifest themselves at scale, and the scope of data governance expands to include data science and compliance.\nAchieving Data Governance\nData governance is achieved by a multi-pronged approach. This section describes mechanisms and methodologies to achieve the governance. The most important aspect is to institutionalize a data governance team and a set of processes. Choose the right tools that will help in implementing the processes and providing the results.\nBuilding a data governance team\nYou cannot tell something is wrong unless you define what is right (Data Quality: The Accuracy Dimension, by Jack E. Olson)! The primary responsibility of a data governance group is to define the attributes of correct data. Using these definitions, one can determine the quality of data at hand. Hence the data governance group needs people in the following kinds of roles:\n- Business analysts: Understand the applications that works with data. Business analysts are the real authority on business rules.\n- Data analysts / data scientists: Team of analysts (data assurance team). Know data structures, data architecture, data modeling, and analysis techniques.\n- IT staff: Maintain and manage applications. They know how the rules actually work and can provide verification.\n- Database administrators: Help with structural issues that may come up and can provide physical data.\n- Users / staff: Help in understanding the situations. They would raise any unusual patterns that they would see in the data / application.\n- Compliance team: The legal team defines consumption of the data coming from external sources, what data can become available to what set of people, etc.\nImplementation mechanisms for data governance and monitoring\nMechanisms for achieving data governance includes actions such as Data Profiling, Data Quality, Data Cleansing, Compliance, Security, Data Loss Prevention, and Data Monitoring. These actions need to be performed at appropriate layers in the EDT stack. The table below shows the various techniques, stakeholders, and tools associated with these mechanisms.\n|Technique||Description||Users / stakeholders||Tools|\n|Data Profiling and Lineage||These are the techniques to identify the quality of data and the lifecycle of the data through various phases. In these systems it is important to capture the metadata at every layer of the stack so it can be used for verification and profiling.||System users / Data analysts||Mahout,Datameer,|\n|Data Quality||Data is considered to be of good quality if it meets business needs and it satisfies the intended use. For some organizations, accuracy may be of utmost importance while for some others, it may be the timeliness. Hence understanding the dimension of greatest interest and implementing methods to achieve it is important.||System users / Data Analysts||Talend/Queries|\n|Data Cleansing||Once you are sure as to what is the quality requirements, implementing various solutions to correct the incorrect or corrupt data is data cleansing.||System developers||Talend,SSIS,Informatica|\n|Compliance||Adhering to the standards defined for the data is compliance. Very important step here is to define the business rules and the compliance. Controls and audit procedures should be implemented to check ongoing compliance with rules.||Legal team, Business / Data analysts||Audit the systems, the compliance and SOD matrix|\n|Security||Defining the matrix of users, groups, and roles and the access definition and implementing it via various tools. Authentication and authorization / access control is part of defining and implementing security.||Business / Data security||Visualization admin tools|\n|Data Loss Prevention||Policies have to be in place to make sure the loopholes for data loss are taken care of. Identification of such data loss needs careful monitoring and quality assessment processes.||Users / Business analysts||Talend, other profiling tools|\n|Data Monitoring||Continuous monitoring of data and sampling of huge volumes of data to assess quality are important part of the governance mechanisms and have to be made a habit to achieve good quality data. Monitoring can happen in data storage layer.||Data analysts, users, support team members||SQL scripts, Talend|\nEnterprise that are otherwise healthy can be made really sick by bad data. From our experience, we have seen many companies that have misleading and at times outright wrong data in their enterprise systems. The frightening part is not that the data is wrong but that the staff is unaware that it is wrong. With Enterprise Digital Transformation, the data volume is growing in the organizations and so is the need for strong data governance and monitoring. As described above in this article, the technology and the infrastructure for data governance exist. What is missing? We found that it is the will and the awareness! The executive teams of such companies should take note and incorporate data quality analysis and monitoring in their corporate processes.\nHere’s to healthy data consumption!\nImage Credits: bruker.com']	['<urn:uuid:fe378297-d106-4bbe-8d67-12a2782ad970>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	expert	2025-05-12T20:58:04.490895	4	21	1277
59	need understand glucose sources in human body to maintain pilot fitness where does blood glucose come from	Glucose enters the blood stream through two main ways: through the gut, where meals are broken down and absorbed in the upper intestine, and from the liver, which can produce its own glucose when the body is deprived of it through food consumption.	"[""Am I fit to fly? Is a common question a pilot may and should ask themselves before each flight to assess their fitness to fly. 'IMSAFE' is easy mnemonic covering aspects of human factors prior to each flight. Pilots must make sure all components are checked off before commencing flight, as they all have a somewhat effect on the blood sugar level.\n[http://avstop.com/ac/instructors_handbook/9-5.html] 28 August 2012\nThe amount of glucose present in the blood of a human is referred to as the blood sugar concentration, or glucose level. In order for the body to have and produce energy, it needs glucose- a form of sugar. We get this glucose through our diet. Our body breaks down most of the food (fats and proteins) we eat down into glucose when required. The other is stored as carbohydrates or fats. When the body requires more glucose the stored carbohydrates or fats are broken down into glucose and is absorbed into the blood stream and is used by cells for growth and energy.\nWhere does the Glucose come from?\nThere are two ways in which glucose can enter the blood stream: through the gut and from the liver.\nThe Gut: As mentioned above, the main source of glucose comes from meals. The meal is broken down and absorbed in the Upper intestine and is absorbed, partially in glucose form. This eventually enters the blood stream and contributes to the blood glucose count, furthermore increasing it.\nThe liver: The liver is the body's own mechanism of producing natural glucose. When the body is deprived of glucose, the individual may feel down because of the lack of energy. When the body does not receive any oral consumption of glucose the liver automatically makes its own glucose. However, the glucose consumption by the body is fast and when constantly starved the glucose in the bloodstream will drop fast.\nIt is important to note that the brain is dependent upon the presence of glucose in order to function. Therefore, low levels, or occasionally no levels, of glucose will affect the functioning of the brain. \nUnits and Normal values in humans\nBlood sugar level is expressed as millimoles per litre (mmol/l). The Normal blood sugar levels throughout the day should range from 4 to 8mmol/l. However they tend to be higher after meals and usually lowest in the morning.\nThe ideal values are:\n• Before meals- 4 to 7mmol/l\n• 90 minutes after a meal: Less than 10mmol/l\n• At bedtime- Around 8mmol/l \n[http://www.exceldepot.com/Templates/Blood%20sugar%20chart.html] 30 August 2012\nHypoglycaemia can develop when the blood sugar levels drop too low. This is caused by the glucose in the body used up too quickly, glucose released into the bloodstream to slowly or too much insulin is released into the bloodstream. But the overall principal arise from an insufficient supply of glucose to the brain, resulting in impairment of function. Some common causes of low blood sugar levels are skipping meals, drinking alcohol and exercising more without eating. Pilots need to understand the symptoms as they can affect the ability of the pilot to fly safely. A drop in the blood sugar levels and can be felt by the pilot through the following symptoms: an increase in heart rate, faintness, trembling and shaking, cold sweating and nervousness. \nThe pancreases plays a vital role in keeping the blood sugar the right levels, it secretes insulin (a hormone that reduces blood sugar) into the blood stream and controls the amount of glucose circulating in the blood stream. If insulin is ineffective, too much glucose will end up circulating the blood stream. This leads to a disease called diabetes, which if not recognised can cause severe illnesses . If diabetes is not efficiently treated it will lead to the damage of the small blood vessels causing blindness, kidney failure and heart attack. Diabetes can be controlled by the person's diet and medication but is an ongoing process. If a pilot has or develops diabetes it is difficult for them to maintain their pilot’s license. \nReligious fasting and flying\nDue to religious beliefs some pilots may carry out fasting while continuing to fly. Fasting is a period of abstinence from all food or specific items. When pilots carry out fasting this can lead to low blood sugar and can cause the pilot to faint. This is a dangerous practice when flying and the pilot should not fly when fasting and carry this out during a different time other than flying.\n- Avoid skipping meals, or going for more than three hours without eating.\n- Eat meals at regular times because your body functions best on a regular schedule.\n- Increase the intake of food if you do more exercise.\n- If you have diabetes, follow your doctor's advice about diet, medicine, and exercise.\n- Do not drink alcohol without eating food. If you do drink without eating first, have only one or two drinks at the most.\n- If you have a history of hypoglycaemia, keep a snack or drink containing sugar with you at all times. Eat the snack as soon as symptoms appear. \nManaging blood sugar (glucose) levels\nThe following video shows how to manage your blood sugar levels. Learn what causes a rise in glucose and what you can do to set it right.\n|Video embedded from YouTube on 28 August 2012|\nWant to know more?\n- This page in Wikipedia offers more detailed information about the blood sugar.\n- Ewing, R. (2003). Aviation Medicine and Other Human Factors for Pilots- 5th Edition. Old Sausage Publishers Limited.\n- Wikipedia (2012). Blood Sugar. Retrieved August 28, 2012 from http://en.wikipedia.org/wiki/Blood_sugar\n- Blood Glucose Levels (2008).Retrieved August 28, 20012 from http://www.netdoctor.co.uk/health_advice/facts/diabetesbloodsugar.htm\n- Hypoglycemia (2011).Insulin shock; Low blood sugar.Retrieved August 28, 20012 from http://www.ncbi.nlm.nih.gov/pubmedhealth/PMH0001423/\n- YouTube (2012). Manage your blood sugar (glucose) levels. Retrieved August 28, 2012 from http://www.youtube.com/watch?v=iTxbv-Mpqfg\n- Diabeteshome (2001).Glucose. Retrieved August 30, 2012 from http://www.diabeteshome.ca/where-does-glucose.php""]"	['<urn:uuid:253fb96f-d6d9-4f61-8fb4-393c66295996>']	factoid	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T20:58:04.490895	17	43	981
60	I'm interested in scientific research initiatives - could you tell me how the Human Cancer Models Initiative and the HADES dark matter research compare in terms of their international collaboration?	Both initiatives involve significant international collaboration, but in different ways. The Human Cancer Models Initiative (HCMI) brings together organizations from four countries: the US National Cancer Institute, Cancer Research UK, the Wellcome Trust Sanger Institute in England, and the Hubrecht Organoid Technology in the Netherlands. They collaborate to develop about 1,000 cancer cell models. HADES, on the other hand, involves around 100 physicists from 18 European institutes working together at the GSI facility in Darmstadt, Germany, to search for Dark Matter candidates. Both projects demonstrate how modern scientific research relies on cross-border cooperation to tackle complex challenges in their respective fields.	"[""International collaboration to create new cancer models to accelerate research\nAn international project to develop a large, globally accessible bank of new cancer cell culture models for the research community launched today. The National Cancer Institute (NCI), part of the National Institutes of Health; Cancer Research UK, London, England; the Wellcome Trust Sanger Institute, Cambridge, England; and the foundation Hubrecht Organoid Technology, Utrecht, Netherlands, are joining forces to develop the Human Cancer Models Initiative (HCMI), which will bring together expertise from around the world to make about 1,000 cancer cell models.\nUsing new techniques to grow cells, scientists can make models that will better resemble the tissue architecture and complexity of human tumors than the cell lines used today.\nLouis Staudt, M.D., Ph.D., director of NCI’s Center for Cancer Genomics, said, “As part of NCI’s Precision Medicine Initiative in Oncology, this new project is timed perfectly to take advantage of the latest cell culture and genomic sequencing techniques to create models that are representative of patient tumors and are annotated with genomic and clinical information. This effort is a first step toward learning how to use these tools to design individualized treatments.”\nGenetic sequencing data from the tumors and derived models will be available to researchers, along with clinical data about the patients and their tumors. All information related to the models will be shared in a way that protects patient privacy.\nIan Walker, Ph.D., Cancer Research UK’s director of clinical research and strategic partnerships, said, “This exciting new project means that we can expand our resources for researchers around the world. We want scientists to have the best resources to be able to easily study all types of cancer. And these new cell lines could transform how we study cancer and could help to develop better treatments for patients.”\nScientists will make the models using tissue from patients with different types of cancer, potentially including rare and children’s cancers, which are often underrepresented or not available at all in existing cell line collections. The new models will have the potential to reflect the biology of tumors more accurately and better represent the overall cancer patient population.\nThe HCMI collaborators aim to speed up development of new models and to make research more efficient by avoiding unnecessary duplication of scientific efforts.\nMathew Garnett, Ph.D., group leader at the Wellcome Trust Sanger Institute, said, “New cancer model derivation technologies are allowing us to generate even more and improved cancer models for research. A concerted and coordinated effort to make new models will accelerate this process, while also allowing for rapid learning, protocol sharing, and standardized culturing methods.”\nHCMI could transform research and will allow scientists to study many aspects of cellular biology and cancer, including how the disease progresses, drug resistance, and the development of precision medicine treatments.\nHans Clevers, M.D., Ph.D., of the foundation Hubrecht Organoid Technology, said, “We are delighted to take part in this global partnership to make new resources for researchers.”\nThe National Cancer Institute leads the National Cancer Program and the NIH’s efforts to dramatically reduce the prevalence of cancer and improve the lives of cancer patients and their families, through research into prevention and cancer biology, the development of new interventions, and the training and mentoring of new researchers. For more information about cancer, please visit the NCI website at www.cancer.gov or call NCI's Cancer Information Service at 1-800-4-CANCER.\nAbout Cancer Research UK:\n- Cancer Research UK is the world’s leading cancer charity dedicated to saving lives through research.\n- Cancer Research UK’s pioneering work into the prevention, diagnosis and treatment of cancer has helped save millions of lives.\n- Cancer Research UK receives no government funding for its life-saving research. Every step it makes towards beating cancer relies on every pound donated.\n- Cancer Research UK has been at the heart of the progress that has already seen survival in the UK double in the last forty years.\n- Today, 2 in 4 people survive their cancer for at least 10 years. Cancer Research UK’s ambition is to accelerate progress so that 3 in 4 people will survive their cancer for at least 10 years within the next 20 years.\n- Cancer Research UK supports research into all aspects of cancer through the work of over 4,000 scientists, doctors and nurses.\n- Together with its partners and supporters, Cancer Research UK's vision is to bring forward the day when all cancers are cured.\nAbout the Wellcome Trust Sanger Institute: The Wellcome Trust Sanger Institute is one of the world's leading genome centers. Through its ability to conduct research at scale, it is able to engage in bold and long-term exploratory projects that are designed to influence and empower medical science globally. Institute research findings, generated through its own research programmes and through its leading role in international consortia, are being used to develop new diagnostics and treatments for human disease. www.sanger.ac.uk\nAbout the foundation Hubrecht Organoid Technology: The foundation Hubrecht Organoid Technology (HUB) is a not-for-profit organization founded by the Royal Netherlands Academy of Sciences and the University Medical Center Utrecht. The HUB exploits the pioneering work of Prof. Hans Clevers, who discovered methods to grow stem cell-derived human ‘mini-organs’ (organoids) from tissues of patients with various diseases. The organoids, that are part of the Living Biobank, are characterized by genome sequencing, expression profiling and sensitivity to known and experimental drugs to establish a database linking genetic and transcriptional information to drug responsiveness. The HUB offers licenses to its patented Organoid Technology for drug-screening and access to organoids in the Living Biobank for preclinical drug discovery and validation. In addition, the HUB provides drug screening services to third parties. More info at www.hub4organoids.eu"", 'HADES searches for Dark Matter\nPress Release of May 12, 2014\n|HADES at the GSI in Darmstadt/Germany searches for Dark Matter candidates.|\n|3D Rendering: A. Schmah/HADES|\nAlthough Dark Energy and Dark Matter appear to constitute over 95 percent of the universe, nobody knows of which particles they are made up. Astrophysicists now crossed one potential Dark Matter candidate – the Dark Photon or U boson – off the list in top position. This is the result of recent HADES experiments, where researchers from the Helmholtz-Zentrum Dresden-Rossendorf (HZDR) and from 17 other European institutes try to pin down the nature of Dark Matter. These negative results – recently published in ""Physics Letters B"" – could even lead to challenges of the Standard Model of particle physics.\nThe interpretation of current astrophysical observations results in the striking mass-energy budget of matter in the universe: 75% Dark Energy and 20% Dark Matter. Only about 5% of the universe consists of ""ordinary"", baryonic matter.\nMany attempts have been made to explain the nature of Dark Matter. Researchers believe that Dark Matter is comprised by hitherto unknown particles which do not fit into the Standard Model of particle physics. The Standard Model is a theoretically sound quantum field theory with fundamental matter particles, such as quarks (bound in hadrons, e.g., baryons) and leptons (e.g., electrons and neutrinos), which interact via exchange of force-carrier quanta, called gauge bosons (e.g., photons). Some of these species acquire their masses by the interaction with the Higgs boson. While evidences for the Higgs boson were found recently at CERN, the Standard Model looks now complete when supplemented by some neutrino masses, and nothing else seems to be needed to understand the wealth of atomic, sub-nuclear and particle physics phenomena. Nevertheless, Dark Matter appears not to be explained by any of the constituents of the Standard Model. This status of the affair has initiated worldwide efforts to search for Dark Matter candidates.\nBeyond the Standard Model\nSearching a needle in the haystack is simpler: one knows both the wanted object (the needle) and the place (the haystack). In the case of Dark Matter the object is unknown, and the localization, e.g. in galactic halos, is also not constraining the loci of interest. To specify the search goal one can envisage diverse hypothetical candidates, such as certain hypothetical particles beyond the Standard Model, which fulfill requirements qualifying them as constituents of Dark Matter.\nDark Energy drives the presently observed accelerated expansion of the universe. Dark Energy is homogeneously distributed and can be attributed to a cosmological constant or vacuum energy. In extreme cases it may cause, in the future, such a sudden expansion that anything in the universe is disrupted - this would be the Big Rip. Dark Matter, in contrast, is bumpy and is needed to explain the formation of the observed density distribution of visible matter in the evolving universe, evidenced by the hierarchy of structures from (super)clusters of galaxies, galaxies, stars, planets and other compact objects such as meteorites, etc.\nAmong the lists of candidates of Dark Matter is a hypothetical particle, often dubbed U boson or Dark Photon. These nicknames refer to the underlying theory construction: a second unitary (""U"") symmetry allows for quanta which are, in one respect, similar to photons - namely gauge bosons - but in another respect different from photons - namely attributing to these quanta a mass, making them to Dark Photons because of a very weak interaction with normal matter. Very similar to photons the Dark Photons can decay into electron-positron pairs, if they have the proper virtuality. Combining the chain of hypotheses one arrives at a scenario, where an ""ordinary"" virtual photon converts into a Dark Photon which decays afterwards into an electron-positron pair.\nNeedle and Haystack\nIf a Dark Photon or U boson would exist with the assumed properties mentioned above and would have a mass, a certain width and a certain coupling strength to the photon, then the „needle"" is specified: a resonance-type signal showing up at an invariant electron-positron mass equal to the U boson mass. The ""haystack"" is specified too: invariant mass spectra, i.e. electron-positron distributions. A prerequisite is an understanding of the overall shape of these distributions.\nUp to now the search for such a signal of a U boson as a candidate for Dark Matter has remained unsuccessful. Together with many other searches for the other candidates of Dark Matter the situations becomes more and more intricate. Cosmology on precision level requires the existence of Dark Matter; however, the various experiments have not found any positive hint. The negative results on the U boson by HADES and other experiments make the hunt for new physics beyond the Standard Model more challenging. For instance, high-precision experiments on the magnetic moment of the muon delivered hints for a discrepancy with predictions by the Standard Model. The discrepancy has been proposed to be resolved by the U boson. But the recently achieved negative search results seem to exclude such an option. This gives the impression that the tension of the Standard Model and cosmological request of extensions as well as small deviations of the Standard Model predictions and data, such as the muon magnetic moment and other observables, is increasing, thus making this frontier of physics very fascinating with high discovery potential.\nThe HADES collaboration\nHADES is an acronym of High-Acceptance Di-Electron Spectrometer. It is an optimized detector system operated by a European collaboration of about hundred physicists at GSI Helmholtzzentrum für Schwerionenforschung/Darmstadt. HADES is aimed at investigating virtual photon signals emitted as electron-positron pairs off compressed nuclear matter to understand the origin of the phenomenon ""masses of hadrons"" and test it in some detail.\nThe HADES collaboration has accumulated more than ten billion analyzable events during the last years. The notion ""events"" means that in collisions of energetic protons with target protons or atomic nuclei or in collisions of atomic nuclei with target nuclei, among other final-state particles, an electron-positron pair could occur. Sources of these pairs are, e.g., unstable hadrons being transiently produced in these collisions. The highly sophisticated apparatus HADES has the capability to select out of a huge background of other particles such electron-positron pairs which can be attributed to primary sources.\nPublication: G. Agakishiev et al. (HADES Collaboration), Phys. Lett. B 731, 265 (2014), DOI Link: http://dx.doi.org/10.1016/j.physletb.2014.02.035\nCaption: HADES at the GSI in Darmstadt/Germany searches for Dark Matter candidates. 3D Rendering: A. Schmah/HADES\nFor additional information:\nProf. Dr. Burkhard Kämpfer\nInstitute of Radiation Physics at HZDR\nPhone +49 351 260 3258 | firstname.lastname@example.org\nDr. Christine Bohnet | Press officer\nPhone +49 351 260 - 2450 or +49 160 969 288 56\nHelmholtz-Zentrum Dresden-Rossendorf | Bautzner Landstr. 400 | 01328 Dresden | Germany | www.hzdr.de\nThe Helmholtz-Zentrum Dresden-Rossendorf (HZDR) is conducting research in the areas of energy, health, and matter. Since 2011, the HZDR has been a member of the Helmholtz Association, Germany\'s largest scientific organization. Some 1,000 employees are working at one of four research sites in Dresden, Leipzig, Freiberg, and Grenoble/France – approximately 500 of HZDR employees are scientists.']"	['<urn:uuid:c76bf4ea-eec9-438f-9237-a470aa1d9350>', '<urn:uuid:a7e6f12e-5d19-4caf-868f-a15d4f717488>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	novice	2025-05-12T20:58:04.490895	30	101	2126
61	What's the difference between how language teachers learn from research papers in reading groups versus how students learn from each other in collaborative learning settings?	In ESL reading groups, instructors learn through professional reading groups that promote reflection, confirm current practices, foster learning, impact practice, and encourage networking among 76 participants over five years. In contrast, collaborative learning settings involve students working as teams, building on each other's contributions, engaging in joint problem solving, and developing shared cognition. While both approaches are social, collaborative learning requires specific design elements like joint attention, mutual engagement, individual agency, and group accountability, along with tools and activities to structure and guide social interactions for effective learning outcomes.	['Research Mobilization in TESL Learning Communities: Benefits, Challenges, Supports, and Procedures\nTo enhance English as a second language (ESL) instructors’ understanding and utilization of peer-reviewed research for professional learning and development, we facilitated the establishment of and supported professional reading groups in nine adult ESL programs. We examined the benefits and challenges experienced by the 76 participants over five years, through focus group interviews, audio-taped group discussions, and monthly questionnaires. Analyses revealed that, despite the challenges reported, reading group involvement promoted reflection, confirmed current professional practices, fostered learning, impacted practice, emphasized the importance of professional development, and encouraged networking. Strategies for establishing and maintaining effective professional reading groups in ESL programs are provided.\nKey words: Professional Learning and Development; Professional Reading Groups; Research Utilization; Adult English as a Second Language (ESL) Education; Communities of Practice; TESL\nPour augmenter, chez les enseignants d’anglais langue seconde (AL2), la compréhension et l’utilisation de la recherche examinée par des pairs dans le cadre de l’apprentissage et de la formation professionnels, nous avons facilité la création de groupes professionnels de lecture et appuyé leur emploi au sein de neuf programmes d’ALS pour adultes. Par des entrevues avec des groupes de réflexion, des discussions de groupe enregistrées et des questionnaires mensuels, nous avons étudié les avantages et les défis vécus par les 76 participants au cours de cinq ans. Les analyses ont démontré que malgré les défis signalés, la participation au groupe de lecture a favorisé la réflexion, confirmé les pratiques professionnelles actuelles, encouragé l’apprentissage, influencé la pratique, souligné l’importance du développement professionnel et encouragé le réseautage. Nous fournissons des stratégies pour l’établissement et le maintien de groupes professionnels de lecture efficaces au sein des programmes d’ALS.\nMots clés : Apprentissage et développement professionnels; groupes professionnels de lecture; utilisation de la recherche; anglais langue seconde (ALS) pour adultes; communautés de pratique; TESL\nUNIVERSITY OF ALBERTA COPYRIGHT LICENSE AND PUBLICATION AGREEMENT\nIf accepted, authors will be asked to sign a copyright agreement with the following points:\nA. Where there is any inconsistency between this Copyright License and Publication Agreement and any other document or agreement in relation to the same subject matter, the terms of this Agreement shall govern.\nB. This document sets out the rights you are granting in relation to publication of your article, book review, or research note entitled (the “Article”) through inclusion in the academic journal titled Alberta Journal of Educational Research (the “Journal”) published through the Faculty of Education, representing the Governors of the University of Alberta (the “Journal Editor”).\nC. There will be no payment to you for this publication and grant of rights. In consideration of the agreement to publish the Article in the Journal:\n1. You are warranting that:\n- the content of the Article is your original work, and its content does not contain any material infringing the copyright of others; or, where the Article is not entirely your original work, you have obtained all necessary permissions in writing to grant the rights you are giving in this agreement;\n- the content of the Article does not contain any material that is defamatory of, or violates the privacy rights of, or discloses the confidential information of, any other person;\n- the Article has not been published elsewhere in whole or in part, and you will not allow publication of the Article elsewhere without the consent of the Journal Editor;\n- the names of all co-authors and contributors to the Article are:\n2. You agree to license the copyright in the Article to the Journal Editor, on a worldwide, perpetual, royalty free basis; and to the extent required by the terms of this agreement. You shall retain the right at all times to be acknowledged as the/an author of the Article.\n3. You further agree that the Journal Editor has the entitlement to deal with the Article as the Journal Editor sees fit, and including in the following manner;\n- The right to print, publish, market, communicate and distribute the Article and the Journal, in this and any subsequent editions, in all media (including electronic media), in all languages, and in all territories, ing the full term of copyright, and including any form of the Article separated from the Journal, such as in a database, abstract, offprint, translation or otherwise, and to authorize third parties to do so;\n- The right to register copyright of the Journal;\n- The right to edit the Article, to conform to editorial policy as the Journal Editor sees fit.\n4. If any co-author or contributor to the Article does not sign this agreement, the Journal Editor reserves the right to refuse to publish the Article.', 'Learning to explain, justify, critique, etc. are essential skills for today’s citizens, for scientists, and in many other careers. These activities are intrinsically social. Further, conceptually challenging content is often best learned by working together with other learners. However, merely asking students to “work together” is not enough to lead to positive learning outcomes. Tools and activities must be designed to enable, structure, and guide social interactions to facilitate effective learning.\nCollaborative learning engages students to work as a team in learning together, and is not just a matter of dividing up work among members of a team. When collaborative learning is working well, students engage in building on each others’ contribution, and individuals learn from their team as the team advances a shared outcome. Effective collaborative learning teams are able to manage both their team relationships and progress on tasks, and are able to monitor and reflect on their process. Terms used to indicate the essence of learning together include: joint problem solving, intersubjectivity, shared/collective/group/distributed cognition, collective consciousness, and transactive discourse.\nTheories of collaborative learning give shape to design and analysis of collaborative learning. From a constructivist perspective, learning occurs as students make sense of their experience. A social experience can be rich in new ideas, conflict with one’s own ideas, and high expectations for the quality of ideas. From a social cognition perspective, learners’ efforts to find common ground and share information with others can creates optimal conditions for developing knowledge, with appropriate levels of challenge and support. A participatory perspective focuses on the process of becoming an effective member of a community, and includes learning the social norms, practices, language, activities and tools of the community — while also developing one’s individual skill in doing the work of the community. Each of these theories has had a profound influence on how designers and researchers address collaborative learning.\nThe core agenda of collaborative learning in cyberlearning is the design and investigation of social technologies to influence the interactions of students in groups, and thereby to increase learning in the group. Targets for design and investigation can include motivational, social, and cognitive dimensions of interacting in groups. Tools often aim to better support specific features of social interaction (argumentation, negotiation, communication, explanation), enable groups to represent social knowledge (improving social awareness and helping students in capturing, referring to, visualizing, organizing, analyzing, critiquing, etc. each other’s ideas.), or guide teams through activities (scripting, scaffolding or coaching). Tools often make features of collaborative learning more visible to members of the group and more available for action by the group. Designs for collaborative learning recognize that students sometimes work as individuals, in small groups and in large ensembles (such as a classroom or a online discussion group) and that effective environments support students across these modalities. Some designs seek to help teachers orchestrate many simultaneous or sequenced social learning activities.\nImportant elements of collaborative learning include:\n- motivation for the effort of working with another learner\n- joint attention, students are looking at the same things\n- mutual engagement, students are actively involved with each other\n- individual agency, each student has responsibility and opportunity for action in the team and for learning from the team’s work\n- group action and accountability, students are discussing, making or problem solving together and the result of their group’s work matters\n- design of roles, responsibilities and measures of progress\n- constructive discourse patterns, including making and acknowledging contributions, finding common ground, providing and receiving help, etc.\n- monitoring and reflecting on teamwork (i.e., meta-cognition and self-regulation)\n- orchestrating participation across activities, places, roles, etc.\nResearch that investigates collaborative learning almost always include methods for analyzing students’ interactions, conversations, and participation in teams or groups — analyzing the process of collaborative learning is important. Research can look at learning as students interact face-to-face or at a distance. Data is often captured by audio or video recording, but also by capturing what students do with technology. Emerging technologies such as eye-tracking can also help capture joint attention. Outcome measures can include individual growth or can focus on the increased capacity of the group — and outcomes can be cognitive (knowledge and skill), interpersonal (membership in a group), or social (skills in learning together). Research can be framed as (a) iterative, design-based research or (b) as comparative experiments or (c) in terms of socio-cultural analysis.\nResearchers in collaborative learning often share their work through the activities of the International Society of the Learning Sciences, including a conference series and journal.\nScripting and Self-Regulation. Learners do not often organize themselves well for collaborative learning spontaneously. A strong literature in computer-supported collaborative has designed and investigated the impacts of scripts that structure the interaction of group members so as to improve collaborative learning processes and outcomes. A emerging tension concerns how groups can learn to regulate their own learning over time, so that less explicit scripting is necessary.\nEmerging Technologies. Traditional collaborative learning research often involved computers and conventional networks. Emerging technologies can focus on devices that support mutual awareness, tangible computing, mobiles devices, design of furniture and rooms (as well as virtual spaces) for collaborative learning, and ways to leverage social networking.\nTensions around “Networked Individualism.” Our increasingly networked society tends to overuse the word “collaboration” wherever communication occurs. In contrast, work in collaborative learning has emphasized strong relationships among learners, joint action, and collective outcomes. How can collaborative learning leverage the strength and ubiquity of weak ties? Conversely, how can more emphasis on learning together be supported in commonplace, large scale use of communication technology? How should theories expand to acknowledge the dynamic boundaries between individual and social learning?\nChallenges in Methods and Measurement. As previously mentioned, collaborative learning often occurs across different groupings, different times, and different spaces. Multiple units of analysis are often important, and bridging analyses at different levels of interaction is difficult. Accounting for both individual and group outcomes of collaborative learning remains an open issue. Analysis of student interaction in teams has often involved laborious methods, and finding ways to simplify or automate analysis of collaborative learning processes is important.\nExamples of NSF Cyberlearning projects that overlap with topics discussed in this primer (see project tag map).\nCollaborative and/or participatory learning\n- EXP: Improving Student Help-Giving with Ubiquitous Collaboration Support Technology\n- NetStat: EAGER: A Representation and Communication Infrastructure for Classroom Collaboration in Data Modeling and Statistics\n- EXP: Agile Research Studios: Scaling Cognitive Apprenticeship to Advance Undergraduate and Graduate Research Training in STEM\n- DIP: Improving Collaborative Learning in Engineering Classes Through Integrated Tools\n- NetStat: EAGER: A Representation and Communication Infrastructure for Classroom Collaboration in Data Modeling and Statistics\n- CAP: CSCL 2017 Making a Difference: Prioritizing Equity and Access in CSCL Doctoral Consortium and Early Career Workshops\n- DIP: Digital Studios for Social Innovation Networks\n- EXP: Fostering Collaborative Drawing and Problem Solving through Digital Sketch and Touch\n- DIP: Connecting Idea Threads across Communities for Sustained Knowledge Building\n- EAGER: WeatherBlur\nMore posts: collaborative-andor-participatory-learning\nEducation Psychologist Special Issue: Theoretical Underpinnings of Successful Computer-Supported Collaborative Learning\nReferences and key readings documenting the thinking behind the concept, important milestones in the work, foundational examples to build from, and summaries along the way.\nBarron, B. & Roschelle, J. (2009). Shared cognition. In Anderman, Eric (ed.), Psychology of Classroom Learning: An Encyclopedia, pp. 819-823. Detroit: Macmillan Reference USA.\nDillenbourg, P. (1999). What do you mean by collaborative learning?. Collaborative-learning: Cognitive and Computational Approaches., 1-19.\nDillenbourg, P., Järvelä, S., & Fischer, F. (2009). The evolution of research on computer-supported collaborative learning. In Technology-enhanced learning (pp. 3-19). Springer Netherlands.\nFischer, F., Kollar, I., Mandl, H., & Jaake, J. M. (2007). Scripting Computer-Supported Collaborative Learning: Cognitive, Computational and Educational Perspectives. New York: Springer.\nKirschner, P. A., & Erkens, G. (2013). Toward a framework for CSCL research. Educational Psychologist, 48(1), 1-8.\nResnick, L. B. Levine, J. M., & Teasley, SD (Eds.).(1991). Perspectives on socially shared cognition.\nRoschelle, J. (1992). Learning by collaborating: Convergent conceptual change. The journal of the learning sciences, 2(3), 235-276.\nRoschelle, J., & Teasley S. D. (1995). The construction of shared knowledge in collaborative problem solving. In C. E. O’Malley (Ed), Computer-supported collaborative learning. (pp. 69-97). Berlin: Springer-Verlag.\nStahl, G., Koschmann, T., & Suthers, D. D. (2006). Computer-supported collaborative learning: An historical perspective. In R. K. Sawyer (Ed.), Cambridge handbook of the learning sciences (pp. 409-426). Cambridge, UK: Cambridge University Press.\nSuthers, D. D. (2006). Technology affordances for intersubjective meaning-making: A research agenda for CSCL. International Journal of Computer Supported Collaborative Learning, 1(3), 315-337.\nPublications from NSF-funded Cyberlearning Projects\nSwartz, M., Li, J., & Wanless, S. (2016, March). Peg + Cat: Adventures in learning. Poster session presented at the 2016 Advancing Informal STEM Learning (AISL) PI Meeting, Bethesda, MD.\nWaters, A., Studer, C., & Baraniuk, R. (2014). Collaboration-Type Identification in Educational Datasets. Journal of Educational Data Mining, Vol. 6(1), pp. 28-52.\nAdamson, D., Dyke, G., Jang, H., & Rosé, C. P. (2014). Towards an agile approach to adapting dynamic collaboration support to student needs. International Journal of Artificial Intelligence in Education, Vol. 24(1), pp. 92-124.\nCarroll, J. M., Jiang, H., & Borge, M. (2015). Distributed collaborative homework activities in a problem-based usability engineering course. Education and Information Technologies, Vol. 20(3), pp. 589-617.\nBrown, R., Lynch, C. F., Eagle, M., Albert, J., Barnes, T., Baker, R., & McNamara, D. (2015). Good communities and bad communities: Does membership affect performance. In Proceedings of the 8th International Conference on Educational Data Mining (pp. 612-614).Madrid, Spain: Educational Data Mining.\nBrown, R., Lynch, C., Wang, Y., Eagle, M., Albert, J., Barnes, T., & McNamara, D. (2015, June). Communities of performance & communities of preference. In Proceedings of the 2nd International Workshop on Graph-Based Educational Data Mining. Madrid, Spain: Educational Data Mining.\nD’Angelo, C. M., Roschelle, J., Bratt, H., Shriberg, L., Richey, C., Tsiartas, A., & Alozie, N. (2015). Using students’ speech to characterize group collaboration quality. In Proceedings of The Computer Supported Collaborative Learning (CSCL) Conference 2015. Gothenburg, Sweden: Computer Support Collaborative Learning.\nKang, S., Norooz, L., Oguamanam, V., Plane, A., Clegg, T. L., & Froehlich, J. E. (2016). SharedPhys: Live Physiological Sensing, Whole-Body Interaction, and Large-Screen Visualizations to Support Shared Inquiry Experiences.\nStahl, G., Mantoan, A., & Weimar, S. (2013). Demo: Collaborative dynamic mathematics in virtual math teams. In Proceedings of the International Conference of Computer-Supported Collaborative Learning, Madison, WI: Computer-Supported Collaborative Learning.\nFlood, V. J., Neff, M. & Abrahamson, D. (2015). Boundary Interactions: Resolving interdisciplinary collaboration challenges using digitized embodied performances. Proceedings of the Computer Supported Collaborative Learning Conference. Gothenberg, Sweden: Computer Supported Collaborative Learning.\nPrimers are developed by small teams of volunteers and licensed under a Creative Commons Attribution 4.0 International License.\nRoschelle, J., Suthers, D., & Grover, S. (2014). CIRCL Primer: Collaborative Learning. In CIRCL Primer Series. Retrieved from http://circlcenter.org/collaborative-learning/\nAfter citing this primer in your text, consider adding: “Used under a Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).”']	['<urn:uuid:319712a3-414c-4c4b-adb1-3ae72de47c9f>', '<urn:uuid:386301de-9b39-4673-906b-6aa8d9791ddb>']	open-ended	direct	verbose-and-natural	distant-from-document	comparison	novice	2025-05-12T20:58:04.490895	25	89	2587
62	I study trees - do ash and foxtail pine trees share any survival tricks against diseases?	While both species show resistance mechanisms, they work differently. About 5% of ash trees have genes that make them naturally resistant to ash dieback fungal disease. In contrast, foxtail pines show rare evidence of fungal attack in general, though they can be affected by white pine blister rust disease. The foxtail pine's resistance seems more related to its harsh alpine environment than specific genetic resistance.	"['Foxtail pine (Arno and Gyer 1973); the two subspecies are commonly called the ""northern"" and ""southern"" foxtail pines, though the southern trees, which are far more famous, are commonly just called ""foxtail pines.""\nTwo subspecies, the type and P. balfouriana subsp. austrina R. Mastrogiuseppe et J. Mastrogiuseppe 1980 (syn: P. balfouriana var. austrina (R. Mastrogiuseppe et J. Mastrogiuseppe) Silba 1986).\nPlants of the southern Sierra have been ""shown to be genetically distinct from the type (differences in chemistry, form, foliage, cone orientation, and seeds) ... As in several other species or species complexes in Pinus, however, there is a problem with a character gradient involving related taxa. The evidence presented by D.K. Bailey (1970) and later by R.J. Mastrogiuseppe and J.D. Mastrogiuseppe (1980) could as well be used to indicate that P. balfouriana (with its two infraspecific taxa) and P. longaeva represent a single species of three subspecies or three varieties"" (Kral 1993).\n""Trees to 22 m; trunk to 2.6 m diam., erect or leaning; crown broadly conic to irregular. Bark gray to salmon or cinnamon, platy or irregularly deep-fissured or with irregular blocky plates. Branches contorted, ascending to descending; twigs red-brown, aging gray to drab yellow-gray, glabrous or puberulent, young branches resembling long bottlebrushes because of persistent leaves. Buds ovoid-acuminate, red-brown, 0.8-1 cm, resinous. Leaves 5 per fascicle, upcurved, persisting 10-30 years, 1.5-4 cm × 1-1.4 mm, mostly connivent, deep blue- to deep yellow-green, abaxial surface without median groove but usually with 2 subepidermal but evident resin bands, adaxial surfaces conspicuously whitened by stomates, margins mostly entire to blunt, apex broadly acute to acuminate; sheath 0.5-1cm, soon forming rosette, shed early. Pollen cones ellipsoid, ca. 6-10 mm, red. Seed cones maturing in 2 years, shedding seeds and falling soon thereafter, spreading, symmetric, lance-cylindric with conic base before opening, broadly lance-ovoid or ovoid to cylindric or ovoid-cylindric when open, 6-9(-11) cm, purple, aging red-brown, nearly sessile; apophyses much thickened, rounded, larger toward cone base; umbo central, usually depressed; prickle absent or weak, to 1 mm, resin exudates amber. Seeds ellipsoid to narrowly obovoid; body to 10 mm, pale brown, mottled with deep red; wing 10-12 mm. 2n=24.\n""Pinus balfouriana is the true ""foxtail pine."" In leaf character it is hardly, if at all, distinguishable from P. longaeva, but its strongly conic-based cones with distinctly shorter-prickled, sunken-centered umbos at once distinguish it from that species"" (Kral 1993).\nUSA: California. See range map at left; see also Thompson et al. (1999). The two subspecies have a strongly disjunct distribution, with the two closest populations about 500 km apart. The type subspecies is found in the inner North Coast Ranges (Siskiyou, Scott, Salmon, Marble, Trinity, and Yolla Bolly Mountains) at elevations of 1,525 to 1,830 m, where it grows in open to closed-canopy pure to mixed stands (Kral 1993). In 2006 it was also reported to occur in Oregon on 2,026-meter Lake Peak, 1.5 km N of the California-Oregon border (Lanner 2007); however this report has not been corroborated or supported by a herbarium collection, and visitors to the site report no evidence of foxtail pines there. Hardy to Zone 5 (cold hardiness limit between -28.8°C and -23.3°C) (Bannister and Neuner 2001; variety not specified).\nSubspecies austrina only occurs in pure or nearly pure stands in the southern Sierra Nevada from 2,750 to 3,650 m elevation, around the headwaters of the Kings, Kern and Kaweah Rivers. It occurs only on the west slopes of the Sierra, except at Cottonwood Creek, at a very high altitude (Peattie 1950). Nearly all trees in the southern population occur within the bounds of Sequoia National Park. Both subspecies are relatively uncommon, so of conservation concern (Kral 1993).\nOline et al (2000) observe that the northern subspecies is growing near the tops of the very tallest peaks within its range, noting that ""in the north, the mountain ranges are not as high or as contiguous [as in the southern populations] -- there are only occasional isolated ridges or peaks that rise above 2000 m to provide potential P. balfouriana habitat, and the populations sizes on these ridges and peaks are small. In terms of elevation, all of the northern populations are at the extreme edge of the species\' ecological limits."" As a consequence, these populations have experienced isolation and genetic drift, and are now more distinct from each other than are different populations within the southern subspecies. Oline et al. (2000) also suggest that the differences between the northern populations are strong enough that they probably predated the geographic separation between the subspecies, which Eckert et al. (2008) suggest probably occurred in early to middle Pleistocene time.\nIt is possible that, since the northern subspecies trees have no higher elevation habitat within or near their current range, climatic warming could cause their extirpation.\nOline et al (2000) also note that some northern subspecies populations occur on serpentine-derived soils, occasionally forming pure closed-canopy stands. This is interesting because plants generally do poorly on serpentine soils, although adaptation to serpentines in northern California has also been observed for Pinus jeffreyi and Juniperus communis var. saxatilis (the form sometimes called J. communis var. jackii). Oline et al (2000) also found some evidence that the populations growing on serpentine have differentiated; this may reflect genetic adaptation to serpentine, but, since foliage from mature trees was sampled, it may only show that there is a selection pressure affecting the trees that establish on serpentine.\nI have observed the southern subspecies at the Timber Gap stand in Sequoia National Park. These trees, which include the largest trees in the southern subspecies, are growing at 3,000-3,300 m elevation on relatively deep soils derived from a calcareous schist. The closely related P. longaeva also grows best on calcareous parent materials and also grows fastest on deep soils, though both taxa can do quite well on barren talus fields. Even on good sites, growth is slow, with trees commonly taking several centuries to grow to maturity. Stands are usually pure, though at lower elevations they may also contain significant numbers of Pinus flexilis, Pinus monticola, Pinus contorta subsp. murrayana, or Abies magnifica. Stands are usually very open in structure, although a few closed-canopy stands have been described.\nOne consequence of the open stand structure and barren substrates is that these stands generally do not carry fire. The principal causes of tree death appear to be lightning, avalanche and rockfall. Most large trees show evidence of at least one lightning strike, and they commonly retain a strip-bark growth habit as a legacy of this misfortune. Sometimes the lightning strike ignites the tree, and the tree may be consumed if the fire is not quenched. Regeneration is common in avalanche tracks, where saplings may show evidence of repeated breakage by avalanche. The tree often grows on steep slopes beneath mountain precipices, so it is also common to see trees that have been scarred or broken by rockfall. Occasionally a tree shows scarring due to woodpecker activity, but in general, evidence of both insect and fungal attack is rare, and these are probably minor causes of mortality.\nThe largest is of the type subspecies. It is 23.0 m tall, dbh 255 cm, crown spread 10 m, and occurs near Eagle Peak in the Trinity National Forest (American Forests 1996). A tree near Telephone Lake in the Trinity Alps Wilderness holds the height record; it is 36.0 m tall, dbh 121 cm, (Robert Van Pelt [who measured the tree] e-mail 1998.03.18). The large Marble Mountains tree shown at left, located near lower Wright Lake, was in 2013 measured at 27.3 m tall and 182 cm dbh (Michael Kauffmann email 2013.09.24).\nThe largest trees of subspecies austrina occur in a stand near Timber Gap in southern Sequoia National Park (see photo this page), where the largest tree was 21.3 m tall and 261 cm dbh in 2005, and a nearby tree (probably the second-largest) was 23.8 m tall and 244 cm dbh. Some people who have seen these two trees suspect that they originated as multiple-stem trees that later fused.\nA crossdated age of 2110 years for sample SHP7 from a specimen of subspecies austrina, collected in the southern Sierra Nevada (CA) by Tony Caprio (Arno and Gyer 1973). This is an incidental measurement, and was not the product of a systematic effort to find exceptionally old trees. Most experts expect that the tree is capable of achieving ages of 2,500 to 3,000 years, perhaps more; Pinus longaeva, a very similar species growing in a similar environment, has attained ages of nearly 5,000 years.\nI have no data on longevity of the type (northern) subspecies.\nBecause it lives in harsh alpine environments and achieves great age, subspecies austrina has been the subject of considerable dendrochronological research, chiefly by L.A. Scuderi and by Lisa Graumlich and her students, notably Andi Lloyd, since the mid-1980s (Scuderi 1993, Lloyd 1997).\nI have found no records of aboriginal uses. Moreover, it has never achieved popularity in horticulture, and is quite rarely seen in botanical gardens. Its principal value to humans is thus aesthetic; the alpine groves of this ancient tree, juxtaposing its brick-red bark and vivid green foliage against blue skies and the white Sierra granite, are exceptionally beautiful even in comparison with other timberline forests.\nThe type variety can be seen to advantage at the Mount Eddy Research Natural Area in the Shasta-Trinity National Forest at 41.333°N, 122.500°W (Google Map). The stand is approached via Forest Road 17 and the Pacific Crest Trail to Deadfall Lakes. The area has about 97 ha of foxtail pine forest at elevations of about 2377-2438 m. It is almost a pure stand, with occasional individuals of Pinus albicaulis and Pinus monticola, but the trail approaching the site passes through a forest dominated by Abies concolor and Pinus monticola (Keeler-Wolf 1990).\nSubspecies austrina can be viewed relatively easily by visiting the Last Chance Meadow Research Natural Area in the Inyo National Forest. The site is at 36° 27\' N, 118° 09\' W (Google Map), and is approached via a paved road southwest of Lone Pine, California. Some trees in this large and impressive stand have attained ages of 1,000 to 1,500 years. It is found in pure stands, and in mixed stands, sometimes with limber pine (Pinus flexilis) and sometimes with lodgepole pine (Pinus contorta subsp. murrayana) (Keeler-Wolf 1990). The largest trees in the subspecies can be seen after a hike of about 4 km, to the Timber Gap area in Sequoia National Park. Access is from Mineral King, about two hours\' drive east of Visalia; Google Map. Another fine stand, growing on granitic soils, can be found on Alta Peak, reached from a 12-km trail starting at the Wolverton trailhead near Giant Forest in Sequoia National Park. This trail climbs over 1,000 meters and makes for a beautiful but rather stiff day hike. Arno and Hammerly (1984) provide a good account of the Alta Peak stand. I also have a report (Jack Huntamer, 2013.09.01) of a very large (unmeasured) tree in a fine, extensive stand at 3,220 m elevation just E of Kaweah Gap in Sequoia National Park; if someone were to find and measure this tree, they might find a new record.\n""In the days when there were few roads of any sort in California, one region was as inaccessible as another, and botanical explorers, though few in number a century ago, expected nothing but hardships in the West, wherever they went. So that John Jeffrey, the Scottish explorer and first discoverer of this species, made his way to the stands of this tree on Scott Mountains, as early as 1852. Jeffrey was always a lonely man, collecting far out ahead of civilization, claiming few friends. Thus he was not quickly missed when he disappeared forever, having set out from San Diego to cross the Colorado Desert in search of new plants. He was either killed by Indians or died of thirst. No trace of his end has ever been found"" (Peattie 1950). A pretty story, but it is equally plausible that Jeffrey was murdered in San Francisco during the heyday of the gold rush; his story, as best it is now known, is told by Lang (2006). The tree was named for a patron of Jeffrey\'s, John Balfour (Lanner 2007).\nFoxtail pines, like other species in section Balfourianae, are very ancient, with very similar fossil species known from Thunder Mountain, Idaho (46 million years), as well as locales in Nevada (42 million years), New Mexico (32 million years), and Colorado (27 million years). During the glacial episodes of the Pleistocene epoch, P. balfouriana expanded its range to lower elevations, and macrofossils of the species have been found as far south as Clear Lake in northern California (Lanner 2007), but so far I have found no good evidence on how the southern populations responded to glaciation.\nWhite pine blister rust (Cronartium ribicola), an introduced fungal disease, has afflicted this and certain other white pines (Little 1980). The disease has not yet, however, affected populations of subsp. austrina (Maloney 2011).\nEckert, A. J., B. R. Tearse, and B. D. Hall. 2008. A phylogeographical analysis of the range disjunction for foxtail pine (Pinus balfouriana, Pinaceae): the role of Pleistocene glaciation. Molecular Ecology 17: 1983–1997.\nLang, Frank A. 2006. John Jeffrey in the Wild West: Speculations on his Life and Times (1828-1854?) Kalmiopsis 13:1-12. Available: www.npsoregon.org/kalmiopsis/kalmiopsis03.html, accessed 2009.11.14.\nLloyd, A.H. 1997. Response of tree-line populations of foxtail pine (Pinus balfouriana) to climate variation over the last 1000 years. Canadian Journal of Forest Research 27:936-942.\nMaloney, P. E. 2011. Incidence and distribution of white pine blister rust in the high-elevation forests of California. Forest Pathology 41(4):308–316.\nMastrogiussepe, R.J. and J.D. Mastrogiuseppe. 1980. A study of Pinus balfouriana Grev. & Balf. (Pinaceae). Systematic Botany 5:86-104.\nMurray, A. 1853. Botanical Expedition to Oregon. Edinburgh, v. 8: no. 618, plate 3, fig. 1.\nOline, D.K., J.B. Mitton and M.C. Grant. 2000. Population and subspecific genetic differentiation in the foxtail pine (Pinus balfouriana). Evolution 54(5):1813-1819. Available: http://stripe.colorado.edu/~mitton/PDFS/Olineetal00.pdf (2005.08.15).\nScuderi, L.A. 1993. A 2000-year tree ring record of annual temperatures in the Sierra Nevada Mountains. Science 259:1433-1436.\nEckert, A. J., and J. O. Sawyer. 2002. Foxtail pine importance and conifer diversity in the Klamath Mountains and southern Sierra Nevada, California. Madroño 49:33-45.\nEckert, A. J. 2006. The phylogenetic position, historical phylogeography, and population genetics of foxtail pine (Pinus balfouriana Grev. & Balf.). Dissertation, University of Washington.\nEckert, A. J. 2006. Influence of substrate type and microsite availability on the persistence of foxtail pine (Pinus balfouriana, Pinaceae) in the Klamath Mountains, California. American Journal of Botany 93(11): 1615–1624. Available: http://www.amjbot.org/cgi/content/abstract/93/11/1615.\nEckert, A. J., and M. L. Eckert. 2007. Environmental and ecological effects on size class distributions of foxtail pine (Pinus baulforiana, Pinaceae) in the Klamath Mountains, California. Madroño 54:117-125.\nRourke, Mike. Foxtail Pine - Pinus balfouriana (2008.07.11).\nRourke, M.D. 1988. The Biogeography and Ecology of Pinus balfouriana Grev. & Balf. in the Sierra Nevada of California. Ph.D. dissertation, University of Arizona, Tucson. 225p.\nRyerson, D.A. 1983. Population structure of Pinus balfouriana Grev & Balf along the margins of its distribution area in the Sierran and Klamath regions of California. MS thesis, California State University, Sacramento. 198p.\nRyerson, D.A. 1984. Krummholz foxtail pines Pinus balfouriana occurrence, California. Fremontia 11:30.\nLast Modified 2014-12-11', ""Genes to resist ash dieback\nIn 2012, the first cases of a fungal disease - dubbed “ash dieback” - that was wiping out ash trees across Europe were detected in the UK, probably after the fungus hitched a ride here on infected imported plants. The disease spreads when fungal spores blow from an infected tree onto an uninfected specimen, which they invade. Luckily, the disease is not universally fatal, and about 5% of ash trees carry genes that mean they’re naturally resistant to the infection. To find out what those genes are, UK scientists have compared the DNA sequences of susceptible and more resilient ash trees. And this has enabled them to come up with a way to spot the trees not to fell and that are best suited for breeding programmes to repopulate forests. Amalia Thomas reports...\nAmalia - Ash trees are one of the most common types of trees in the UK and they are under serious threat of being wiped out by a fungal disease called ash dieback, which was introduced from Europe in 2012. There is to this day, no cure to this disease once the tree is affected, but curiously a small portion of our Ash trees are immune to dieback and in a recent study, scientists from Queen Mary University London and from the Royal Botanic Gardens in Kew found out why. I spoke with one of the authors of the study, Richard Nichols, who explained their results.\nRichard - We found that the trees that survived were genetically distinct from the trees that died from ash dieback. We found evidence, not that there's a single gene involved in resistance, but genes spread all across the chromosomes. Hundreds, perhaps thousands of genes, each with a small effect contributing to the resilience of some trees.\nAmalia - To put things in perspective, Richard explains the devastating effects of this disease on the UK's ash tree population.\nRichard - When the disease arrives, it's catastrophic. Around about 70% of the ash trees in a woodland will die. That includes the well-established mature trees, so across the country that amounts to about 70 million trees. A major component of our woodland.\nAmalia - Their study into how to stop ash dieback was based on samples collected by forest research.\nRichard - There was a mass screening trial, that means 150,000 trees have been planted out in various locations around the South of Britain, and we went and found the worst effected trees and the relatively unaffected trees. All in all, we took a thousand samples.\nAmalia - And from these samples they were able to extract genetic information from their DNA.\nRichard - We had a whole series of millions of reads from trees which had been affected and another series of reads, which we knew had come from trees which were unaffected, and we compared the two to look for differences. So a lot of the differences that we've found may be spurious differences because there are so many variations in the genetic code. But if we look across all of the chromosomes in an ash tree, even if nothing was going on genetically, we would be bound to find some differences just by chance between the diseased trees and those which are relatively unaffected. But when we looked at those genes which had the largest, most impressive differences, they did seem to make sense. They did seem to be genes which in other species have been found to be associated with disease resistance.\nAmalia - So from a small piece of an ashtree, Richard and his team are able to predict whether or not it will be susceptible to ash dieback.\nRichard - What we do is we create a score. We look at the trees and say, well, of our suspect genes, the genes that we think might be involved, how many is this tree got? And then we look to see how well the tree performed. And when we did that, we found we got remarkably accurate predictions. So when a tree had a really high score, then with 80% probability it was one of the more resilient trees.\nAmalia - Identifying which trees are resilient to ash dieback is a huge step forward in the battle of conservation of the ash tree population and the ecosystems they sustain, which are so significant in the UK.\nRichard - What we want to be able to do is to breed trees which will be useful to to repopulate woodlands which will be resilient to the disease and we can use the genetic information to do that, and the second thing we would like to be able to do is go to a woodland which has been infected and identify the trees which are going to do relatively well because we don't want to cut down all the trees. If some trees are going to do well and would be good to re-seed the woodland. We want to be able to identify those and not to cut them down.\nRichard - This is a really important example of why our conservation efforts should maintain large, diverse populations, because if our ash trees had been just one cultivar which was planted all over the country, we wouldn't have this natural reservoir of resilience.""]"	['<urn:uuid:0fd7f43a-5091-415d-ae2b-52be05bb0fc2>', '<urn:uuid:837a114e-b959-41f1-b3ff-17068226788c>']	factoid	with-premise	concise-and-natural	distant-from-document	three-doc	novice	2025-05-12T20:58:04.490895	16	65	3430
63	differences between traditional currency trading books what new aspects covered	While most currency trading and foreign exchange books focus on international finance theory or simplistic chart-based strategies, this comprehensive guide teaches how to profitably trade currencies in the real world, based on specific strategies and tactics used in interbank FX trading.	"[""This book provides a comprehensive overview of state-of-the-art applications of nanotechnology in biology and medicine, as well as model organisms that can help us understand the biological activity and associated toxicity of nanoparticles, and devise strategies to minimize toxicity and enhance therapies.\nThis book deals with artificial intelligence (AI) and its several applications. It is not an organic text that should be read from the first page onwards, but rather a collection of articles that can be read at will (or at need). The idea of this work is indeed to provide some food for thoughts on how AI is impacting few verticals (insurance and financial services), affecting horizontal and technical applications (speech recognition and blockchain), and changing organizational structures (introducing new figures or dealing with ethical issues).\nFrom Ben Mezrich, the New York Times bestselling author of The Accidental Billionaires and Bringing Down the House, comes Bitcoin Billionaires--the fascinating story of brothers Tyler and Cameron Winklevoss's big bet on crypto-currency and its dazzling pay-off.\nLeverage the power of Instagram to promote your brand Instagram is the photo- and video-sharing app used by millions across the globe. More than just a social platform for users to share their experiences with family and friends, it's become a vital tool for business owners and marketers to create visual narratives about what makes them, and their products, different from everything else that's out there.\nDoes any of this sound familiar to you? 1. You sit down every week staring at that blinking cursor wondering what to send your email list, and then a week becomes two or more, and you can’t remember when the last time you emailed them was.\nReal-world tools to build your venture, grow your business, and avoid mistakes Startup, Scaleup, Screwup is an expert guide for emerging and established businesses to accelerate growth, facilitate scalability, and keep pace with the rapidly changing economic landscape. The contemporary marketplace is more dynamic than ever before―increased global competition, the impact of digital transformation, and disruptive innovation factors require businesses to implement agile management and business strategies to compete and thrive. This indispensable book provides business leaders and entrepreneurs the tools and guidance to meet growth and scalability challenges head on.\nNow you can master the art of foreign exchange trading While most currency trading and foreign exchange books focus on international finance theory or simplistic chart-based strategies, The Art of Currency Trading is a comprehensive guide that will teach you how to profitably trade currencies in the real world. Author Brent Donnelly has been a successful interbank FX trader for more than 20 years and in this book, he shares the specific strategies and tactics he has used to profit in the forex marketplace.\nPublisher's Note: Products purchased from Third Party sellers are not guaranteed by the publisher for quality, authenticity, or access to any online entitlements included with the product. Use machine learning to understand your customers, frame decisions, and drive value\nKey Features Assimilate blockchain services such as Ethereum and Hyperledger to transform industrial applications Know in and out of blockchain technology to understand various business use cases Understand various common and not-so-common challenges faced in blockchain development\nNudge meets Hooked in a practical approach to designing products and services that change behavior, from what we buy to how we work.\nThe organization is required to complete a project successfully, and almost no one knows how to organize everything to complete a project and obtain high results.\nThis book offers a fresh method of assessing and managing risks in SMEs, by adopting a multidisciplinary approach. In small and medium companies, the risk management process cannot be often formalised and procedures are usually integrated unconsciously into the decision-making process. Therefore, to enhance the flexibility of these companies, increase their market share and allow them to grow and manage risks more effectively, the first step is to improve the way decisions are made.\nLearn the intricate workings of DAX and the mechanics that are necessary to solve advanced Power BI challenges. This book is all about DAX (Data Analysis Expressions), the formula language used in Power BI―Microsoft’s leading self-service business intelligence application―and covers other products such as PowerPivot and SQL Server Analysis Services Tabular. You will learn how to leverage the advanced applications of DAX to solve complex tasks.\nThis book presents a collection of the latest research in the area of immersive technologies, presented at the International Augmented and Virtual Reality Conference 2018 in Manchester, UK, and showcases how augmented reality (AR) and virtual reality (VR) are transforming the business landscape.\nOver the last few decades, politics in India has moved steadily in a pro-business direction. This shift has important implications for both government and citizens.\nAn innovative investigation of the inner workings of Spotify that traces the transformation of audio files into streamed experience.\nA trio of experts on high-tech business strategy and innovation reveal the principles that have made platform businesses the most valuable firms in the world and the first trillion-dollar companies.\nEthical aspects of business and the economy are of increasing concern in business practice, higher education, and society in general. This concern results from significant business scandals and economic crises, such as the financial crisis of 2008 and the following great recession, as well as from pressing current and future challenges for the economy, such as sustainability and globalization. As a result, there is a growing demand for normative analysis and orientation for business and the economy, where business ethics has become a crucial part of organizational management, risk management, branding, and strategic management.\n29 min ago""]"	['<urn:uuid:deffab47-793f-4c7d-aca3-463b8620b34a>']	factoid	direct	long-search-query	distant-from-document	single-doc	expert	2025-05-12T20:58:04.490895	10	41	931
64	explain three types of sattva in hindu philosophy looking for spiritual guidance	The three sub-sets of Sattva are: 1) Karya Karma (Good Deeds), which focuses on behavior and treating others with compassion and gratitude; 2) Swakarma, which refers to resolving our past and reinterpreting our life narrative in more flexible and adaptive ways; and 3) Swadharma, which means moving toward spirit and realizing our spiritual purpose. Swadharma emphasizes that our daily actions are only meaningful as they contribute to reaching a transcendent state, transforming Prakriti into Purusha.	['Recently I was at the Kripalu Institute and spent a week studying the Bhagavad Gita, a classic text of Hindu Spirituality. The Gita is perhaps the most fundamental of Hindu texts, as it captures the essence of the Vedantic (yoga) philosophy, which is one of the prominent guides for Hindus’ life.\nHindu philosophy is singularly devoted to transcendence, and it studies and activates processes to achieve that end. These processes are generally called yoga.\nTo explore these processes, we first must examine the Hindu Universe. There are two major divisions. The first is the Purusha. Purusha is the originating egg. It is the primal source of all else. Brahma is the womb that nurtures and, in unison, gives birth to the three lower levels of existence. We might label this primal energy, but it is more commonly called spirit.\nFrom the perspective of human experience, Purusha is the transcendent state of consciousness we seek in meditation and other spiritual practices, the Self.\nPrakriti is the reality of consciousness. It is the most basic and purest form of day to day being, the world of senses and mind. It emanates from Purusha, which is like a fluid that evolves into forms. Prakriti is the world of manifest form, which divides into seven lower levels of energy. Prakriti are energy centers.\nPrakriti, is in human consciousness, is most equated with the ego. That is our stories of development, relationship, and self-definition.\nIt is the human confusion between Prakriti, and Purusha that causes suffering. We mistake Prakriti for Purusha: we confuse sense and transcendence.\nOne such center of Prakriti are gunas. Gunas are sources of psychic energy, which are manifest in our relationship to life.\nThe first Guna is Rajas. Rajas are a passion, and they arise from thirst and attachment. Rajas produce fevered desires in us. Rajas cause attachment to action. In the Hindu pantheon, Rajas cause rebirth, due to repetitive desires. Rajas wipe out the will and reason, bending to the forces of desire.\n“Rajas, from a more psychological view, are those mind-feelings associated with excessive energy… Anger, anxiety, obsession, and all passions directed toward an object-a person, thing, or idea. It is constant momentum, fusing thought, feeling, and action.\nThe other Guna is Tamas or inertia. Tamas is born of ignorance, which deludes. It is Knowledge obscured, causing attachment to unawareness, and is manifest in negligence, indolence, and sleepiness. Rather than an over-investment in action-attachment, it is withdrawal from the world, inaction, and inertia.\nClinical conditions, primarily depression, are examples of Tamas. Other conditions, such as introversion, schizoid, and denial, repression positions are too.\nThe Guna we strive for is Sattva. Sattva is balance and harmony.\n“Of these, only sattva is free from impurity, illuminating and free from disease, binds by attachment to happiness and by attachment to knowledge” 14-6.\nThere are three sub-sets of Sattva:\nKarya Karma: Good Deeds. This is the action component. It focuses on our behavior and the way we treat others. Traditionally regard for others entails compassion and love for others and gratitude for our own lives.\nSwakarma. Swakarma refers to resolving our past. Such an endeavor is the crux of psychotherapy, initiated by Freud. The patterns established in childhood, according to Freud, influence our contemporary life. Through our socialization, we develop a narrative pattern with us as its star. Woven through that pattern is a story of us, our relationships, and our self-image. When that narrative is disturbed by external or internal events, we experience disturbance, and this is reacted too in either healthy or unhealthy ways.\nPsychotherapies rework our interpretation of our past in ways that are more flexible and adaptive. We gain symptomatic relief from pain partly through a reinterpretation of our life narrative. For example, the destructive influence of the alcoholic parent may be seen as less crippling than one thinks, and in fact, many have helped to establish unknown character strengths.\nSwadharma; Moving toward spirit. The Story of the Gita is the story of the warrior Arjuna and Lord Krishna. Much of it focuses on Krishna’s attempts to persuade Arjuna to perform his life duty as a warrior, and not be deterred by his personal feelings and concerns. Swadarma is the realizing of our spiritual purpose, the Purusha, which is the underlying transcendent motive of life. Ideally, our day to day actions are only meaningful as they contribute to this transcendent state. That is Prakriti must transform into Purusha to advance our spiritual purpose\nYoga is the name for a collection of practices that aid in the transformation from Prakriti to Purusha.']	['<urn:uuid:1857a535-2a8d-4868-9f3a-8e24da1efd2f>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	novice	2025-05-12T20:58:04.490895	12	75	762
65	Which weighs more: full-term placenta or peak amniotic fluid?	A full-term placenta weighs more, at 450-500g, while peak amniotic fluid volume at 34-36 weeks is about 1 liter (1000ml).	['The placenta is a temporary organ of pregnancy situated in the uterus. It is formed from fetal and maternal components. The fetal portion is formed by the chorion frondosum, while the maternal portion is formed by the decidua basalis. Moreover, the placenta is the meeting point of two circulatory systems: fetal circulation and maternal circulation.\nThe main function of the placenta is the interchange between the mother and the fetus. More specifically, it provides nutrition and oxygen to the fetus and removes waste material and carbon dioxide.\nIn this article, we will explore the anatomy and function of the placenta.\n|Definition||Circular, discoid-shaped organ that develops in the uterus during pregnancy and allows metabolic exchange between mother and fetus|\nFetal portion: chorion frondosum\nMaternal portion: basal decidua\nFetal surface (chorionic plate) with umbilical cord\nMaternal surface (basal plate)\nFetal respiration, nutrition and excretion\nFetal protection and immunity\nEndocrine (hormone production)\n- Gross anatomy\n- Clinical notes\nThe placenta is a discoid-shaped organ weighing about 450-500g at full term. The placental thickness is usually proportional to the gestational age. The placenta is normally located along the anterior or posterior wall of the uterus and may expand to the lateral wall with the course of the pregnancy.\nThe placenta is composed of two different surfaces, the fetal surface (or chorionic plate) and the maternal surface (or basal plate).\nFetal surface of the placenta\nThe fetal surface of the placenta (or chorionic plate) is covered by the amnion, or amniotic membrane, which gives this surface a shiny appearance. The amniotic membrane secretes amniotic fluid which serves as a protection and cushion for the fetus, while also facilitating exchanges between the mother and fetus.\nUnderlying the amnion is the chorion, a thicker membrane continuous with the lining of the uterine wall. The chorion contains the chorionic vessels which are continuous with the vessels of the umbilical cord. Originally, early in the development of the placenta, the entire chorionic plate is covered with chorionic villi. The villi located adjacent to the decidua capsularis (portion of the decidua that overlies the embryo) degenerate to produce the smooth (nonvillous) chorion laeve. The villi adjacent to the decidua basalis persist, increase in size and produce the chorion frondosum or fetal portion of the placenta. The chorionic villi of the fully developed placenta contain a network of fetal capillaries, allowing a maximal contact area with the maternal blood. The exchanges between the fetal and maternal circulation occurs in the intervillous space.\nThe umbilical cord, which is the connection between the placenta and the fetus, inserts in a slightly eccentric position into the chorionic plate. The umbilical cord contains one vein (the umbilical vein) that carries nutrients and oxygen from the placenta to the fetus and two arteries (the umbilical arteries) that carry waste products from the fetus back to the placenta.\nMaternal surface of the placenta\nThe maternal surface of the placenta, or basal plate, is an artificial surface, which emerges from the separation of the placenta from the uterine wall during delivery. This surface is composed of the decidua, the modified or specialized endometrium (or mucosal lining of the uterus) that forms in preparation for pregnancy. The decidua has several parts:\n- Decidua basalis - forms the portion of placenta;\n- Decidua capsularis - overlies the embryo;\n- Parietal decidua - the rest of the decidual tissue.\nAlso visible on the maternal surface of the placenta are slightly elevated regions called lobes or cotyledons (approximately 10 to 40), which are separated by grooves or sulci. Inside the placenta, the grooves correspond to the placental septa. Each lobe visible on the maternal surface corresponds to the position of the vilous trees arising from the chorionic plate.\nThe placenta is a highly-specialized organ that plays an essential role during pregnancy. It is responsible for providing nutrition and oxygen to the fetus as well as removing waste material and carbon dioxide. It is also responsible for creating a separation between the maternal and fetal circulation (known as placental barrier). Besides that, the placenta protects the fetus from infections and other maternal disorders, while also helping in the development of the fetal immune system. Additionally, this organ has an endocrine function as it secretes hormones (such as human chorionic gonadotropin) that affect the pregnancy, metabolism, fetal growth, and parturition.\nVariant morphologies of the placenta are frequently encountered, such as is the case of a bilobed placenta where this organ is separated into two near equal-sized lobes. The estimated incidence of this specific variation is at up to ~4% of pregnancies. It can be associated with some complications such as first-trimester bleeding.\nBesides the developmental abnormalities referred above, the placenta may also be affected by a number of medical conditions. An example of these disorders is a condition known as placenta previa, which is the implantation of the placenta over the cervical os. This condition usually presents as painless vaginal bleeding in the third trimester. In these cases, mother and fetus need careful monitoring and delivery is often by cesarean section.\nPlacenta: want to learn more about it?\nOur engaging videos, interactive quizzes, in-depth articles and HD atlas are here to get you top results faster.\nWhat do you prefer to learn with?\n“I would honestly say that Kenhub cut my study time in half.”\nKim Bengochea, Regis University, Denver', 'Your baby will develop inside your uterus with the help of a fetal life-support system composed of the placenta, the umbilical cord, and the amniotic sac (which is filled with amniotic fluid).\nWhat is the placenta and what does it do?\nThe placenta has been described as a pancake-shaped organ that attaches to the inside of the uterus and is connected to the fetus by the umbilical cord. The placenta produces pregnancy-related hormones, including chorionic gonadotropin (hCG), estrogen, and progesterone.\nThe placenta is responsible for working as a trading post between the mother’s and the baby’s blood supply. Small blood vessels carrying the fetal blood run through the placenta, which is full of maternal blood. Nutrients and oxygen from the mother’s blood are transferred to the fetal blood, while waste products are transferred from the fetal blood to the maternal blood, without the two blood supplies mixing.\nThe placenta is expelled from the uterus in a process called the after-birth. One possible problem in pregnancy is placenta previa, where the placenta is attached near or over the cervix. As the fetus grows, pressure on the placenta can cause bleeding. This condition requires medical management to ensure a safe labor and delivery for you and your baby.\nWhat is the umbilical cord and what does it do?\nThe umbilical cord is the life-line that attaches the placenta to the fetus. The umbilical cord is made up of three blood vessels: two smaller arteries which carry blood to the placenta and a larger vein which returns blood to the fetus. It can grow to be 60 cm long, allowing the baby enough cord to safely move around without causing damage to the cord or the placenta.\nAfter the baby is born, the cord is cut (something the baby’s father may wish to do); the remaining section will heal and form the baby’s belly button. During pregnancy you may find out the umbilical cord is in a knot, or is wrapped around a part of your baby’s body. This is common and cannot be prevented, and it usually does not pose any threats to the baby.\nWhat is the amniotic sac and what does it do?\nThe amniotic sac is filled with amniotic fluid. This sac is your baby’s home, gymnasium, and protection from outside knocks, bumps, and other external pressures. The amniotic sac allows the fetus ample room to swim and move around which helps build muscle tone. To keep the baby cozy, the amniotic sac and fluid maintain a slightly higher temperature than the mother’s body, usually 99.7 F.\nAt week 10, there is around 30 ml of fluid present. The amniotic fluid will reach it’s peak around weeks 34-36 at about 1 liter. When your water breaks, it is this sac that ruptures and this fluid that leaves the body. Your baby’s life is still being supported by the umbilical cord, and you should be meeting your baby soon!\nAre the placenta, umbilical cord, and amniotic sac different when dealing with multiple births?\nIdentical twins often share the same placenta, usually have separate amniotic sacs, and always have their own umbilical cords. Non identical twins have separate placentas, amniotic sacs, and umbilical cords.']	['<urn:uuid:def3b259-6010-4def-88dd-baec5bb17b63>', '<urn:uuid:b3311eea-bff1-4108-a2e3-f00c3da3c1d4>']	factoid	with-premise	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T20:58:04.490895	9	20	1419
66	I've noticed that both thyme and mastic from Chios are used in Mediterranean cuisine - could you compare their traditional culinary applications and unique properties?	Thyme and Chios Mastic have quite different culinary applications. Thyme is widely used in its dried form for flavoring soups, gravies, stews, sauces, sausages, and dressings, and is particularly common in Greek cuisine, including Greek salad and grilled meat dishes. It also contains volatile oils and has antiseptic and antifungal properties. In contrast, Chios Mastic is a natural resin that's exclusively produced on the island of Chios and is primarily used in desserts and sweets - specifically ice cream, sweet breads, cakes, puddings, loukoumi delights, and liqueur. It provides both a distinctive taste and a unique gummy consistency to foods.	['Since ancient times, the Greeks have used herbs and aromatic plants for both medicinal and culinary purposes. According to mythology, Mount Olympus, the “home of the gods”, was full of flowers and herbs, available to gods and humans alike. Hippocrates, regarded as “the father of medicine”, pointed out the nutritional value of herbs, and stressed their role, along with fresh air and exercise, in promoting good health.\nHerbs are in fact an excellent source of vitamins and minerals, from Vitamin K, A and C to manganese, iron and calcium. Nowadays, aromatic plants are not only often added to foods, but are also used in the industrial production of cosmetics, nutritional supplements and medicinal products. Greece offers a wide variety of herbs, traditionally used to season dishes or to brew an infusion.\nOf all these plants, oregano and thyme –especially in their dried form– and also parsley and dill –in their fresh form – are perhaps the most ubiquitous in Greek cuisine, found in dishes such as the famous Greek salad and the extremely popular tzatziki dip, but also in grilled fish and meat entrees. They are rich in antioxidants and give a typically Mediterranean flavour. When consumed as an infusion, oregano is said to alleviate respiratory problems, while thyme, is known for its antiseptic and antifungal qualities and for easing muscle pain; dill, on the other hand, is used to soothe headaches and parsley has diuretic and antioxidant effects (although its consumption is discouraged during pregnancy).\nSpearmint, another widely used Greek herb, can be found in mountainous and humid regions. But it can often also be found in Greek houses, in flower pots and window boxes, often next to a patch of dill or sweet basil – the famously fragrant herb which draws its name from the Greek word vasilikos “royal”. A combination of spearmint, dill and spring onions is a typical seasoning for spring dishes.\nOther characteristic Greek herbs often added to sauces stews or used as infusions include rosemary, fennel, coriander, sage, celeriac, bay leaves, mint, marjoram and chamomile. Sideritis, usually referred to as “mountain tea” in Greek, is a plant commonly used as an herbal tea, and has anti- inflammatory and antioxidant qualities comparable to those of green tea.\nCinnamon, anise (via Hippopx)\nSome of the most widely used spices, traditionally added to stews and tomato sauce or used as condiments, are cinnamon, cloves, nutmeg, cumin, anise seed and, of course, black pepper; cardamom and allspice are also used.\nA special mention has to be given to Krokos Kozanis, a unique local variety of saffron crocus of very high quality, produced in the region of Kozani in Western Macedonia; it is PDO protected due to its particularly strong flavour and colouring strength and is also well known for its medicinal and nutritional properties. Krokos Kozanis is used cooking, confectionery, cheese-making, liqueurs and infusions but also in pharmaceuticals and paint production. Used a spice, it adds a delicate aroma and spicy flavour to rice, pasta sauces, chicken, fish, potatoes, pulses, sweets breads and even ice cream.\nChios Mastic (Masticha Chiou) is another Greek PDO product with a particularly characteristic aroma and flavour. It is the natural resin excreted by the mastic trees which grow exclusively on the island of Chios in the northern Aegean; after it dries into droplets, it can be chewed as gum, or else be ground into powder then used as a spice. Mastic is traditionally used to flavour desserts such as ice cream, sweet breads, cakes and puddings, giving not just a distinctive taste but also a gummy consistency. They are also typically used in the production of loukoumi delights, liqueur and the sweet paste known in Greece as ypovrichio (“submarine”), and is increasingly used in creative cooking.\nRead also via Greek News Agenda: Greek yogurt: tradition exported;', 'Name Origin: From the Greek word thyro, to sacrifice, due to its use as incense to perfume the temples. Read more on the History of Thyme.\nNatural Order: Labiatæ\nGrowing Cycle: Short, Shrubby Perennial\nOrigins: Native to dry, stony places on the Mediterranean coasts.\nHeight: Usually under 12 inches.\nCharacteristics: Branched, slender, and woody stems that bear oblong, triangular, tapering leaves that are usually 1/4 to 1/2 inch long. The leaves are green on top and gray underneath.\nThyme Flowers: Little pink or lilac which form whorls and loose, leafy spikes.\nFun Fact: Thyme, particularly the low-growing species, are quite popular with gardeners making miniature and Fairy Gardens. There are many varieties to choose from and we’ve discussed them separately on our creeping thyme and Silver-Edged Lemon Thyme article.\nHow to Grow Thyme\n(thyme seeds, sowing, cultivation, propagation, harvesting thyme leaves, harvesting thyme seed and wintering.)\nThere are approximately 170,000 thyme seeds per ounce and 24 ounces will fill a quart container. Thyme seeds retain their germinating power for three years. Cuttings, layers and divisions all work well but the easiest way to grow thyme is from seed.\nGrowing Thyme From Seed\nBecause the seeds are so tiny, thyme seeds should be sown very shallowly or pressed into the soil with a fine layer sprinkled on top. Better to plant seed in a nursery bed where more attention can be paid to the tiny plants. This will also enable the more valuable garden space to be used for an earlier-maturing crop.\nIn the seedbed, plant thyme seeds in early spring with the drills 4 to 6 inches apart with 5 or 6 seeds per inch. If planting in volume, mix sand with the seed to prevent over-planting. Some farmers use as much as 4 parts sand to one part seed.\nIf you are considering growing thyme and adding it to your garden plan, you might want to check out our Thyme Companion Planting to ensure the best possible yields from all of your herbs, flowers and vegetables.\nThyme plants should be planted no closer than 8 inches apart. Ten inches is preferred but one plant per square foot is optimal.\nYoung, growing thyme plants should be set out in the garden or field in June or July, preferably in damp ground or just prior to a rain shower.\nThyme can be propagated by dividing the roots (should be done in April) and from seed. The finest plants are produced when grown from seed. However, Lemon Thyme smells sweeter when grown from cuttings or root divisions.\nHarvest alternating plants in late August or early September. Harvest plants from alternating rows around three weeks later and the final crop of thyme should be harvested in October. If harvesting for drying, it’s best to harvest thyme just as they come into flower.\nHarvesting Thyme Seeds.\nThyme matures unevenly from plant to plant.\nWhile cutting the ripening tops is one way to obtain seeds, use of cloths, sheets, or paper bags may prove more productive. Around noon and again in late afternoon, gently shake the plants to encourage the ripe seeds to fall onto the sheets or into the bags. Collect the seeds and spread them in a warm, airy room to finish drying. Do keep in mind if the plants are wet or damp the tiny seeds may stick to the leaves and flower heads.\nIn colder climates, mulch your thyme plants with leaves or other garden litter to prevent undue thawing and freezing. In the spring, for best results; dig the plants, divide and plant in a new location.\n(leaves and thyme oil)\nEither fresh or dried, thyme leaves are used for flavoring soups, gravies, stews, sauces, sausages, dressings and many other dishes.\nAll parts of the thyme plant are fragrant because of the fairly high concentration of volatile oil. Like olive oil, there are grades of thyme oil with the first distillation being the most aromatic and desired. Both grades are used in perfumery and soap making. Some use the crystals that can form in thyme oil as a disinfectant.']	['<urn:uuid:7c1346d3-33f1-46c2-bcad-0be9280babf7>', '<urn:uuid:632bea34-f4f8-44be-acd3-52b0d5e6718a>']	open-ended	with-premise	verbose-and-natural	similar-to-document	comparison	expert	2025-05-12T20:58:04.490895	25	100	1313
67	wireframes purpose design process explain usability testing benefits	Wireframes serve as architectural blueprints in the design process, helping to plan functionality and solve issues before visual design and implementation. They allow for testing and simulating user experience early, identifying shortcomings and mistakes that could be costly to fix later. Wireframes focus on specifics using simple black and white designs to show information hierarchy and can be turned into interactive prototypes. From the usability testing perspective, wireframes enable evaluation of whether users can achieve their goals efficiently through methods like observing user interactions, testing navigation, and identifying issues like poor accessibility or inconsistent design. This combination of wireframing and usability testing helps save time, money and ensures websites meet both business goals and user needs.	['White background, black letters and rectangles in different shades of grey – that doesn’t sound like the most compelling website design, does it? It is, however, how most designs we do in our studio come to be. Before a design is taken over by a graphic artist who spices it up with visual effects, it is first planned and designed like an architectural blueprint. This phase of the design process is called wireframing.\nSome business owners think that wireframes are just sketches of the final website design and are not worth the investment. They couldn’t be more wrong – I dare to say that most of the actual designing is done at this very stage of the work process. During this time – away from colors, typography, icons, and illustrations – assumptions are made, business goals are set, and tools to achieve them are picked.\nWireframes and mockups play an important part while designing a website or e-commerce store. They are not alone, though – other useful tools include information architecture, user paths, and detailed analysis. Their place in the process is constant and always precedes visual design – wireframes are the only way to properly plan work at later stages, guarantee consistency among other parts of the projects, and avoid serious problems (described later in this article).\nWhere are wireframes in the process?\n- Analysis (investigating the competition, client’s needs, propositions, and business goals)\n- Information architecture\n- User paths\n- Discussion, improvements, direction consolidation\n- Wireframing (iterations)\n- Visual design\n- Implementation (coding)\nBetter be prepared\nIt lies in every business owner’s best interest to provide all possible information when starting a new project. The more details, the easier it is for a UX designer to understand business specifics. Unique features, business goals, conversion goals, sales tools – the more they know, the more efficient will the new website or store be.\nThere are countless ways to design a proper user experience on the web. Store or website can be simple and straightforward. Or on the contrary – mysterious and extremely engaging. While designing the experience, one must keep in mind business and image goals and look out for various traps. It is easy to make a website mockup that will just be, tick a box on the list and push the project forward. Unfortunately, not paying attention to wireframes may cause additional problems that will snowball into serious issues at later stages.\nIf a wireframe is not sufficiently considered and detailed, visual design based on it will be implemented with all lacks and deficiencies that will become apparent too late. Making changes to already implemented site or store causes difficulty and additional costs. It is reasonable for a company to trust the specialists and spend more time polishing up the wireframes, dispelling any doubts, and setting a common direction for the project.\nInformation architecture, simply\nJust like visual design follows wireframing, information architecture and user paths precede website mockups. It is a very important stage of work when the UX designer makes decisions about site structure (map) and different connections between its elements.\nInformation architecture is also a great support for developers before they start the implementation. For large and complicated projects like extensive websites and apps, solid structure with well-described functionality will help them plan and estimate their work. Information architecture makes things easier but doesn’t guarantee anything – client’s corrections, adding features, and testing also have a great impact on project completion time. Solid but flexible information architecture allows the project to grow while maintaining full control over its scope and quality of final result.\nThere’s a reason why the word „architecture” is hidden in the name of this phase. Just like architecture, UX design revolves around users, their needs, goals, and expectations. It considers real-world issues and solves them within constraints. When an architect decides to pick cheaper build materials to fit within their client’s budget, a UX designer collects plugins and ready-made solutions that will decrease overall costs of the project and advance the implementation. While working on a website, designer specifies the number of pages, sets goals for each of them, and accentuates the connections between the elements. He’s driven by his will to design an intuitive and logical experience but keeps his client’s business goals in mind at all stages of the work.\nBusiness goals can be very different:\n- increase number of pageviews\n- put greater emphasis on branding\n- boost communication\n- optimize sales and conversion solutions\n- improve level of customer support\n- reach new and existing clients\n- shorten the time required to complete a certain task\n- improve usability and accessibility\n- redesign the application or service\nThere are many ways to achieve these goals. One of the most helpful ones is called user paths. It is a highly analytical tool that focuses on user’s behavior on the site. It helps discover pages that do not have many views or interrupt a path important for the site’s business goals (for example buying a product or signing up for newsletter). User paths are planned alongside the information architecture – with the right mixture of page structure, connections between them and layout, users can be funneled into the product page and encouraged to make a purchase.\nUser paths play one more important role: they guarantee intuitiveness and consistency for the site as a whole. It is good practice to make them as short and simple as possible, allowing the user to reach key parts of the site in just a few clicks. While planning user paths, UX designer is guided by their comfort and pleasant experience.\nFor SmartYou, we visualized website structure as a tree built of small blocks. The most important pages have been highlighted using a different colors and showed an essential, most suitable for our goals user path. The visualization helped the client see the big picture of the overall user experience on the site and pinpoint moments that required additional work or redesign. It also gave him a better view of the project which, from the initial 1-3 pages, turned into complex conversion-lead website.\nSimulating the experience\nFinished and accepted information architecture lays a good groundwork for the wireframes. At this stage the designer is familiar with overall site structure and knows all key features. In spite of appearances, his next step is not just making a black-and-white sketch of the site but rather give shape to its content. A wireframe is a mold for the substance.\nIt’s good to trust a specialist. Their knowledge, experience, and skills will help you get better results. Business owners who try to „design” their new website on their own, rarely take into account various functional, usability, and technology requirements (not all solutions can be implemented within the given budget). Furthermore, the first version of a wireframe rarely is similar to the last, while working on a prototype requires many iterations and changes. Even after a wireframe is accepted, when new information come up or the final results requires making changes to existing solutions, visual design will play the part of a wireframes. As you can see, it’s important for a UX designer to work closely with visual artist – he or she often supervises all work on the project.\nGreat wireframes are based on ready contents. When information architecture described the project’s structure and logic, texts and images are parts of direct communication that revolves around the single most important goal of wireframes: functionality. It consists of layout and content presentation on the site. A blog offers an entirely different experience from an e-commerce store, while a landing page requires other level of immersion from a complex application built in the browser. Thanks to texts, even if not in their final form, a designer can properly showcase on the site and assign to pre-planned user paths and scenarios.\nIt’s quite easy to find wireframe examples on the web. Usually they are just black letters and rectangles in different shades of grey scattered against a white background. There are a few reasons why they look that way:\n- wireframes focus on specifics. Most of them do not include icons (if so, they’re usually just examples) and the designer rarely pays attention to spacing and grid. The goal of the wireframes is to work a functional and usable solution to a problem.\n- wireframes are often expanded and must be flexible. More complex visual effects would slow down the iteration pace that often includes tens of different designs. Simple appearance allows the wireframes to be modular and expandable.\n- simple style is a way to show information hierarchy. Thanks to different shades of grey across various elements on the site, it is easy to set the hierarchy and rhythm for the design. The darker the color, the most important an element is.\n- wireframes are used for testing. Many of the mid- and hi-fi wireframes can be used for user testing. Usually they need to be turned into interactive prototypes that come with clickable buttons and links. User testing is used to verify initial functional assumptions and overall intuitiveness of a site or app. In our studio, we create prototypes in Adobe Xd that can export them as videos or clickable mockups in the browser.\nWriting on a wall\nWireframes simulate user experience on the site and help the designer put in his or hers shoes. From their point of view, the designer can spot any shortcoming and mistakes that, when discovered during the visual design or implementation stage, would put the project and relationship with the client to a test. Therefore, wireframes offer an insight into the final feel of the site and make it easier to solve any problems at an early stage.\nIn black and white, they show how the client’s new website or app will look in the future. It highlights the navigation and layout in an almost final form. They are the first touchpoint between a client and his projects – even the first iterations of wireframes can give the vibe of the design and help the client imagine the final effect. Prototypes become a topic for discussion that often leads to interesting insights and corrections to earlier propositions.\nWhile moving an element up or down is not an issue, often some changes are strategic. Clicking through the prototype can offer a fresh look on project. Wireframes highlight all shortcoming in communication and weak points on the site. Most frequent changes include:\n- number of options and links in the navigation\n- showcasing call-to-action buttons\n- level of complexity among products or services\n- length or size of texts\n- priority given to important elements\n- visibility of key elements above the fold\nOn the other hand, user testing can tell us that:\n- they’re not sure what this button does\n- this „Buy now” button is not clear and they almost missed it\n- contact form has too many fields\n- there’re too many elements on the product page and they feel overwhelmed\n- that icon is too small and they can barely see it\n- this copy says too much about the company but not enough about the product\nSounds familiar? If so and you’ve noticed a recent drop in your site’s statistics, consider getting a UX audit and making some changes. Our studio provides such service where we analyze all aspects of user experience on the site: structure, information architecture, usability, appearance, and communication (copywriting, branding). The results are presented in the form of a detailed report that includes conclusions and improvement suggestions.\nSolving important problems during the wireframing stage helps save time, money, and energy. If we would just proceed to visual design and coding after getting information architecture accepted, making any changes would leave an unpleasant mark on the budget and fluidity of the project.\nTo sum up (in a few words)\nIt’s good to end such a long text with a short and meaty summary. Now you know that:\n- Wireframe is more than just a sketch. It is placed somewhere between strategy and planning and visual design and implementation.\n- A well-designed wireframe can simulate the final product and solve any issues before they become real.\n- It is also a great basis for user testing and a chance to objectively assess project progress.\n- Information architecture is a plan for the general structure and logic behind all pages on the site. It precedes wireframing and helps lay the functional groundwork for all later work.\n- User paths are created alongside information architecture and can be used to meet business goals – the right order and layout of pages within the site can encourage purchase, sign up for a newsletter or take action.\nMaking a website is not that much different from building a house. Both projects require groups of specialists and major financial investments. Like a building that needs meticulous architectural plans, implementing a website is always preceded by information structure, wireframes, and prototype. These tools help save time and money and avoid many unexpected problems in the future.\nIn our studio wireframes and prototypes are useful instruments for working on different projects – be it a low-key software house or a complex comparison tools for thousands of products.', 'We are going to cover what website usability testing is, when do you need it, what is the process of preparing the study and some of the best practices to keep in mind.\nWhat is website usability testing?\nWebsite usability testing is a method of evaluating the usability of your website, that is if users can achieve their goals, do so efficiently and to their satisfaction. Modern usability testing also includes some degree of evaluating the user experience as a whole. There are various usability testing methods you can choose to perform such study.\nThe purpose of testing the usability of a website is to identify usability and UX issues on your website by observing how users interact with it. Using online tools for website usability testing is highly popular, because it is effective, simple to set up, and can be completed in a rather short amount of time when using a good usability testing tool like UXtweak.\nWhat is an example of website usability?\nA website that is simple to use is an excellent example of website usability. As a result, the website must have a distinct website anatomy, with menus and links that are simple to use and understand. A search feature that enables users to easily find the data they require should also be available on the website. The website also needs to be created in a way that makes it simple for visitors to interact with the content, such as by offering simple instructions and useful illustrations.\nThe speed of the website is an additional crucial factor in website usability. Users may become irritated and leave a website if it takes too long to load. The website must be speed-optimized, with images and other components designed specifically for the web, to guarantee a quick loading time. Additionally, the website should be designed to be responsive, so that it looks good and functions properly on all devices.\nLast but not least, user experience should be considered when evaluating website usability. This means that the website needs to be built with the user in mind, and should include features that make it simple for visitors to interact with the content and find the information they need. All users have to be able to access the website, regardless of their devices or abilities, according to the design guidelines.\nTo test your website for usability, we recommend using one of the online usability testing tools.\nWhen do you need website usability testing?\nIt’s better to perform several rounds of usability testing during your website design and development process. This strategy will assist you in identifying usability issues early on, allowing you to devote more time to resolving them.\nYou can start with testing your high-fidelity prototype before you hand it out to the development team. The next rounds can be performed before the launch of the website. We also recommend including the “preventative” usability tests in your website maintenance routine. This way you will be able to uncover the rising usability issues early on and always stay up to date.\nOther great opportunities are before and after a redesign, or on additions of new pages or features.\nHow to conduct website usability testing\nThere are 5 main steps to follow in your website usability testing study:\nLearn more about how to set up the website usability testing study.\nWhat are typical website usability issues and how to test them?\n- Poor navigation: One of the most prevalent usability issues with websites is poor navigation. If users have trouble finding the information they need on a website, they may find it frustrating and will probably leave. Creating a logical and hierarchical organization for the content is one way to address this problem. To do so, try using proven research methods for testing and enhancing navigation like card sorting and tree testing.\n- Slow loading: Website usability can suffer significantly from slow loading, especially for those with a lot of content or multimedia. Noone wants to waste their precious time waiting for your content to load so this may result in high bounce rates and lower conversion rates. Among the solutions to this problem are image and video optimization, content distribution through a content delivery network (CDN), and a decrease in HTTP requests.\n- Inability of users to complete the basic tasks: Users are more likely to leave a website if they can’t perform simple tasks like filling out a form or buying something. This might be the result of significant usability issues, confusing interface or software errors. Running usability tests can help to locate the issue and assist website designers in resolving it.\n- Poor accessibility: Significant usability problems can be caused by websites that are not accessible to users with disabilities. Poor color contrast, a lack of image alternative text, and inconsistent navigation can all be considered accessibility barriers. By following the WCAG standards and testing their websites with assistive tools like screen readers and keyboard-only navigation, designers can address accessibility issues.\n- Inconsistent design: Confusion for users on websites with inconsistent design can result in poor usability. Typography, color scheme, and layout variations can all be examples of inconsistent design. Making a style guide that specifies the standards and design elements that should be applied throughout the website is one way to address this problem. Use prototype testing to help you comprehend the nature of the issue and design more consistent interfaces.\nCheck out how the features above work in our product demos:\nBest practices for website usability testing\n1. Pilot the study before the launch\nIn order to save yourself the time and money on fixing future issues, double check if your study is working as intended by running a “pilot” version of it. Ask a colleague or a friend of yours to complete the test before it goes live. Alternatively, use UXtweak’s Preview feature to see how your study will look like from the tester’s point of view.\nMake sure the tasks lack bias, the wording is clear and everything works smoothly. Running a pilot of the study will also help you estimate the average time needed to complete the test.\n2. Prioritize the issues\nYou’ll probably obtain a solid number of insights from your usability test, but not all of them will be equally severe. Prioritize the most important issues and start your improvement journey from there. Ranking usability problems as low, medium or high severity will help you find out which ones need to be solved right now and which ones can wait.\nFor example, users not being able to log in to your app is a high severity problem whereas the text or a logo that are too small can be classified as low or medium severity.\n3. Ask good questions\nIn order to get useful insights, it’s important to know the correct way to formulate your usability testing questions. Make sure they lack bias and assumptions, so that the respondents are not influenced with your wording in any way.\nCheck out the page where we guide you through a process of writing good usability testing questions and tasks that lack bias in order to obtain real insights.']	['<urn:uuid:75f23002-410e-432e-8291-675bbf01a2a0>', '<urn:uuid:a5a5a42d-853e-488e-8f51-9f46092f5736>']	open-ended	direct	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:58:04.490895	8	116	3407
68	differences between positive negative cognitive symptoms schizophrenia detailed explanation	Schizophrenia symptoms fall into three categories. Positive symptoms are psychotic behaviors not seen in healthy people, including hallucinations (like hearing voices), delusions (false beliefs), thought disorders (disorganized thinking), and movement disorders. Negative symptoms involve disruptions to normal emotions and behaviors, such as flat affect, lack of pleasure in life, inability to begin and sustain activities, and speaking little. Cognitive symptoms are subtle and include poor executive functioning, trouble focusing, and problems with working memory.	['What Is Schizophrenia?\nSchizophrenia is a chronic, severe, and disabling brain disorder that has affected people throughout history.\nPeople with the disorder may hear voices other people don’t hear. They may believe other people are reading their minds, controlling their thoughts, or plotting to harm them. This can terrify people with the illness and make them withdrawn or extremely agitated.\nPeople with schizophrenia may not make sense when they talk. They may sit for hours without moving or talking. Sometimes people with schizophrenia seem perfectly fine until they talk about what they are really thinking.\nFamilies and society are affected by schizophrenia too. Many people with schizophrenia have difficulty holding a job or caring for themselves, so they rely on others for help.\nTreatment helps relieve many symptoms of schizophrenia, but most people who have the disorder cope with symptoms throughout their lives. However, many people with schizophrenia can lead rewarding and meaningful lives in their communities. Researchers are developing more effective medications and using new research tools to understand the causes of schizophrenia. In the years to come, this work may help prevent and better treat the illness.\nExperts think schizophrenia is caused by several factors.\nGenes and environment. Scientists have long known that schizophrenia runs in families. The illness occurs in 1 percent of the general population, but it occurs in 10 percent of people who have a first-degree relative with the disorder, such as a parent, brother, or sister. People who have second-degree relatives (aunts, uncles, grandparents, or cousins) with the disease also develop schizophrenia more often than the general population. The risk is highest for an identical twin of a person with schizophrenia. He or she has a 40 to 65 percent chance of developing the disorder.\nWe inherit our genes from both parents. Scientists believe several genes are associated with an increased risk of schizophrenia, but that no gene causes the disease by itself. In fact, recent research has found that people with schizophrenia tend to have higher rates of rare genetic mutations. These genetic differences involve hundreds of different genes and probably disrupt brain development.\nIn addition, it probably takes more than genes to cause the disorder. Scientists think interactions between genes and the environment are necessary for schizophrenia to develop. Many environmental factors may be involved, such as exposure to viruses or malnutrition before birth, problems during birth, and other not yet known psychosocial factors.\nDifferent brain chemistry and structure. Scientists think that an imbalance in the complex, interrelated chemical reactions of the brain involving the neurotransmitters dopamine and glutamate, and possibly others, plays a role in schizophrenia. Neurotransmitters are substances that allow brain cells to communicate with each other. Scientists are learning more about brain chemistry and its link to schizophrenia.\nStudies of brain tissue after death also have revealed differences in the brains of people with schizophrenia. Scientists found small changes in the distribution or characteristics of brain cells that likely occurred before birth. Some experts think problems during brain development before birth may lead to faulty connections. The problem may not show up in a person until puberty. The brain undergoes major changes during puberty, and these changes could trigger psychotic symptoms. Scientists have learned a lot about schizophrenia, but more research is needed to help explain how it develops.\nWho is at risk?\nAbout 1% of Americans have this illness.\nSchizophrenia affects men and women equally. It occurs at similar rates in all ethnic groups around the world. Symptoms such as hallucinations and delusions usually start between ages 16 and 30. Men tend to experience symptoms a little earlier than women. Most of the time, people do not get schizophrenia after age 45. Schizophrenia rarely occurs in children, but awareness of childhood-onset schizophrenia is increasing.\nIt can be difficult to diagnose schizophrenia in teens. This is because the first signs can include a change of friends, a drop in grades, sleep problems, and irritability—behaviors that are common among teens. A combination of factors can predict schizophrenia in up to 80% of youth who are at high risk of developing the illness. These factors include isolating oneself and withdrawing from others, an increase in unusual thoughts and suspicions, and a family history of psychosis. In young people who develop the disease, this stage of the disorder is called the “prodromal” period.\nSigns & Symptoms\nThe symptoms of schizophrenia fall into three broad categories: positive symptoms, negative symptoms, and cognitive symptoms.\nPositive symptoms are psychotic behaviors not seen in healthy people. People with positive symptoms often “lose touch” with reality. These symptoms can come and go. Sometimes they are severe and at other times hardly noticeable, depending on whether the individual is receiving treatment. They include the following:\nHallucinations are things a person sees, hears, smells, or feels that no one else can see, hear, smell, or feel. “Voices” are the most common type of hallucination in schizophrenia. Many people with the disorder hear voices. The voices may talk to the person about his or her behavior, order the person to do things, or warn the person of danger. Sometimes the voices talk to each other. People with schizophrenia may hear voices for a long time before family and friends notice the problem.\nOther types of hallucinations include seeing people or objects that are not there, smelling odors that no one else detects, and feeling things like invisible fingers touching their bodies when no one is near.\nDelusions are false beliefs that are not part of the person’s culture and do not change. The person believes delusions even after other people prove that the beliefs are not true or logical. People with schizophrenia can have delusions that seem bizarre, such as believing that neighbors can control their behavior with magnetic waves. They may also believe that people on television are directing special messages to them, or that radio stations are broadcasting their thoughts aloud to others. Sometimes they believe they are someone else, such as a famous historical figure. They may have paranoid delusions and believe that others are trying to harm them, such as by cheating, harassing, poisoning, spying on, or plotting against them or the people they care about. These beliefs are called “delusions of persecution.”\nThought disorders are unusual or dysfunctional ways of thinking. One form of thought disorder is called “disorganized thinking.” This is when a person has trouble organizing his or her thoughts or connecting them logically. They may talk in a garbled way that is hard to understand. Another form is called “thought blocking.” This is when a person stops speaking abruptly in the middle of a thought. When asked why he or she stopped talking, the person may say that it felt as if the thought had been taken out of his or her head. Finally, a person with a thought disorder might make up meaningless words, or “neologisms.”\nMovement disorders may appear as agitated body movements. A person with a movement disorder may repeat certain motions over and over. In the other extreme, a person may become catatonic. Catatonia is a state in which a person does not move and does not respond to others. Catatonia is rare today, but it was more common when treatment for schizophrenia was not available.\nNegative symptoms are associated with disruptions to normal emotions and behaviors. These symptoms are harder to recognize as part of the disorder and can be mistaken for depression or other conditions. These symptoms include the following:\n- “Flat affect” (a person’s face does not move or he or she talks in a dull or monotonous voice)\n- Lack of pleasure in everyday life\n- Lack of ability to begin and sustain planned activities\n- Speaking little, even when forced to interact.\nPeople with negative symptoms need help with everyday tasks. They often neglect basic personal hygiene. This may make them seem lazy or unwilling to help themselves, but the problems are symptoms caused by the schizophrenia.\nCognitive symptoms are subtle. Like negative symptoms, cognitive symptoms may be difficult to recognize as part of the disorder. Often, they are detected only when other tests are performed. Cognitive symptoms include the following:\n- Poor “executive functioning” (the ability to understand information and use it to make decisions)\n- Trouble focusing or paying attention\n- Problems with “working memory” (the ability to use information immediately after learning it)\nClick here to learn about our current schizophrenia studies.']	['<urn:uuid:64c705b8-d877-4e19-a924-2e28c9cf866f>']	open-ended	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T20:58:04.490895	9	74	1402
69	I'm new to signal recording software. What can I do with RF capture data once it's recorded, and what tools are available for analyzing large recorded files?	RF capture data can be played back and analyzed in multiple ways. You can perform simultaneous analysis in frequency, time, and modulation domains while maintaining time correlation. During playback, display and measurement parameters can be changed. For analyzing large recorded files, DataVu-PC provides capabilities to view spectrums vs. time, amplitude vs. time, perform signal searches, and conduct pulse analysis. It can analyze up to 2,000,000 pulses, calculating parameters like start/stop time, average/peak power, and pulse duration. The tool also enables fast post-acquisition search, mark and measurement tasks, converting hours of monitoring into efficient analysis.	['Today’s Real-Time Spectrum Analyzers include features and capabilities that go far beyond the basic measurement capabilities of conventional spectrum analyzers. Among these capabilities is the ability to capture and record RF data vs. time for playback and analysis. So, what is RF Capture and Playback? It turns out, the answer is multi-faceted. There are several different ways that today’s real-time analyzers can capture and record RF signals. The captured data can be played back in a number of different ways. Let’s examine the various aspects of RF Capture and Playback.\nDPX Spectrum Recording\nThe DPX display is the real-time spectrum display. It shows the composite result of millions of spectrum measurements per second in a color graded bitmap display, where the different colors represent the density of spectral activity. The DPX display includes an option to enable a DPXogram function.\nThe DPXogram is a waterfall-type display that accumulates spectrum traces in a scrolling strip-chart fashion. The spectrum traces recorded in the DPXogram are composed of traces computed from the DPX spectrum results\n, using a Max Hold function employed during the time interval between the DPXogram traces. The DPXogram can record up to 60,000 spectrum traces. The user can set the time between traces, thus giving control over how much actual time is recorded across the set of 60,000 spectrum traces, which can span from minutes to days in time.\nThe time-stamped DPXogram traces can be saved to disk, and can be replayed to review the seamless recorded spectral history. DPXograms are supported on the RSA300/500/600 series of USB analyzers, as well as the RSA5100/6100/7100 series of benchtop analyzers and the SignalVu-PC software.\nRF Acquisition and Analysis / Replay\nThis is the heart of the spectrum and vector signal analysis of a real-time spectrum analyzer. The RF signal is converted to baseband IQ data which is acquired seamlessly, often based on a trigger, into analysis memory.\nOnce acquired, the data can be analyzed in multiple domains simultaneously (frequency, time, modulation domains) while maintaining time correlation between them. Display and measurement parameters can be changed, and various measurement displays can be added or removed, and then the acquired data can be Replayed to show new/revised results.\nThese acquisitions can range from a few microseconds to several seconds or more, depending on settings and hardware used. This type of capture, analysis and replay is supported by all of the real-time spectrum analyzers, as well as offline analysis with SignalVu-PC software.\nStreaming RF Recording and Playback\nThe RSA300/500/600 series of USB analyzers offer streaming RF recording capability over the USB3 port. RF signals over a 40MHz bandwidth at the analyzer’s center frequency can be seamlessly streamed to disk in the host PC. A solid state hard drive is required to perform the streaming in order to keep up with the high speed writing rate – approximately 13GB per minute. The RF streaming can be performed simultaneously with other analysis displays up and running.\nOnce a recording is complete, it can be played back through SignalVu-PC. During playback, SignalVu-PC behaves as if the signal was being captured on an RSA, with a few minor exceptions such as being unable the change the center frequency.\nAnalysis of large recorded files can also be performed with DataVu-PC, which provides the ability to view spectrums vs. time, amplitude vs. time, as well as being able to perform signal searches, pulse analysis and the ability to select time segments of interest for export to SignalVu-PC for deeper signal analysis.\nStreaming Wideband IQ Recording & Analysis / Playback\nThe benchtop RSA5100/6100/7100 series Real-Time Spectrum Analyzers offer the ability to stream IQ data directly to disk for long-term seamless recording. The RSA5100/6100 series support streaming at bandwidths up to 165/110MHz via a pair of parallel LVDS ports on the instrument interfaced to an external RAID recording system.\nThe RSA7100 series can stream up to 320/800MHz bandwidth to a built-in RAID system. The RAID recording systems can record 10s of terabytes of IQ data, which can represent hours of seamless wideband RF recording. RF streaming can be performed simultaneously with real-time spectrum and signal analysis. The recorded data files can be played back, examined and analyzed using DataVu-PC as described above. This tool can be used to help identify time segments of interest for deeper analysis with SignalVu-PC.', 'DataVu-PC makes short work of recording and searching through large datasets for signals of interest. You can measure, control, and record with two USB instruments simultaneously, then search for and measure pulses and other signals, minimizing time spent in post-capture analysis.\nFeatures & Benefits\n- Enables easy recording for all Tektronix USB instruments, supporting up to 2 units operating simultaneously at independent frequencies and spans, recording at bandwidths from 9.7 kHz to 40 MHz\n- Instruments can be controlled, and spectrums displayed, at spans up to the maximum of the instrument frequency range for fast signal discovery over broad frequency ranges\n- Frequency-mask search finds events based on user-defined spectral profiles\n- Pulse analysis calculates start/stop time, average/peak power, pulse duration, Pulse Repetition Interval (PRI) and start/stop frequencies on up to 2,000,000 pulses\n- Pulse analysis results can be saved in Pulse Descriptor Word (PDW) format for use with other tools\n- File progress bar gives analysis position at a glance, with visual display of marker events\n- Provides analysis for all recording solutions (RSA300/500/600/5000/7100 series), minimizing time spent learning new tools\n- Color-graded spectrum density display visualizes infrequent signal occurrences, reducing analysis time\n- Color-graded spectrogram display shows time and frequency relationships\n- Power vs. time display provides streaming zero-span display for detailed pulse analysis\n- FFT overlap and speed controls optimize between highest probability of intercept vs. analysis time\n- User settable sliders for start/stop point saves time in re-examination of signals of interest\n- Export areas of interest to .XDAT, .SIQ or .TIQ formats for further analysis\n- eMarkers provide search, mark and save on up to 2,000,000 user-defined amplitude events\n- Analyzes files in .XDAT format from any source, including third-party recording solutions\n- Spectrum management\n- Interference hunting\n- Pulsed signal analysis\n- SIGINT data collection\n- EW testing\n- RF system design\nDataVu-PC analysis saves you time and helps you succeed\nWhen combined with the signal recording capabilities of all Tektronix spectrum analyzers, DataVu-PC can turn hours of attended monitoring into fast post-acquisition search, mark and measurement tasks. You can search based on signal amplitude and frequency characteristics, marking each occurrence of an event for later examination. Pulse measurements can be made with the pulse application on up to 2,000,000 pulses and PDWs can be exported in tab-delimited format with headers for integration into other workflows.\nShould you need to perform in-depth modulation, pulse or standards-based analysis, DataVu-PC can convert recordings to the SignalVu-PC file format (.TIQ), and can convert recordings made with Tektronix USB-based spectrum analyzers and SignalVu-PC from .R3F format into formats compatible with DataVu-PC.\nThree bandwidth options are available to make DataVu-PC an affordable tool to analyze large files from all Tektronix recording spectrum analyzers.\nDataVu-PC offers highly flexible, 2-channel recording with USB spectrum analyzers\nDataVu-PC offers recording using two RSAs so that you can operate at two independent frequencies and spans simultaneously. One instrument can be used for wide-span signal detection and a second instrument can be used for isolation on signals of interest and recording for later analysis. Or, both instruments can be used for recording. You can record in bandwidths from 9.7 kHz to 40 MHz. Recording in narrower bandwidths reduces data loads, minimizing disk space and search times.\nDataVu-PC application licenses\nMeasurements and functions included in the DVPC-SPAN application licenses\nThe DVPC-SPAN application licenses of DataVu-PC enable replay of recorded files in XDAT, TIQ, and SIQD formats. Control and recording with a single RSA300/500/600 USB-based instrument is supported. Three views are available and recordings from any source are supported with color-graded persistence spectrum, spectrogram, and power vs. time. FFT processing can be adjusted up to 99% overlap ensuring no signal is missed, and speed can be adjusted to minimize playback time. All adjustments can be made during file playback without a restart of the analysis. The RSA LiveVu display, for control and recording with a single RSA300/500/600, is included in the SPAN application license.\n|Features and Functions, DVPC-SPAN licenses||Description|\n|DVPC-SPAN1000 Application License||Base features of DataVu-PC up to 1000 MHz acquisitions|\n|DVPC-SPAN200 Application License||Base features of DataVu-PC up to 200 MHz acquisitions|\n|DVPC-SPAN50 Application License||Base features of DataVu-PC up to 50 MHz acquisitions|\n|Instruments supported in RSA LiveVu||RSA300/500/600 series USB-based instruments|\n|RSA LiveVu settings||Center frequency range: Full range of instrument used\nSpan range: 100 Hz to max span of instrument used\nRecording bandwidths: 9.965 kHz, 19.531 kHz, 39.062 kHz, 78.125 kHz, 156.25 kHz, 312.5 kHz, 625 kHz, 1.25 MHz, 2.5 MHz, 5 MHz, 10 MHz, 20 MHz, 40 MHz\nReference level range: -130 dBm to 40 dBm, 1 dB steps\nResolution bandwidth range: 10 Hz to 8 MHz Capture type: Free run, triggered\nTrigger types: External, RF power trigger\nRecording duration: 0.1 sec to maximum disk size\n|File Types Supported For Playback||.XDAT, .SIQD and .TIQ|\n|File Conversions Supported||R3F to .SIQD or .TIQ\n12-bit .XDAT to 16-bit .XDAT (DVPC-SPAN1000 only)\nAll supported file types for playback\n|Playback Controls||Adjust FFT skip/overlap up to 99% overlap\nAdjust speed from 3 to 384 FFTs/image for faster playback\n|General Spectrum Settings||FFT length - 256 to 16K in powers of 2\nWindow Functions - (Gaussian, Blackman, Hamming, Hann, Welch, Bartlett, Rectangle)\n|Persistence Spectrum Settings||Decay Settings - How long a signal persists in Spectrum view (short, medium, long, infinite)\nMax Hold Trace - Retains the maximum height of a signal in Spectrum view\nContinuous Loop - Continuously replay given file\nPower Axis - Adjust maximum and minimum (± 200 dBm)\n|Spectrogram Settings||Power axis - Adjust maximum and minimum (± 200 dBm)|\n|Power versus time display||Power axis - Adjust maximum and minimum (± 200 dBm)|\n|Progress Bar||Displays progress of the file replay\nGraphical sliders to adjust start and stop indexes for replay\nJump to any location in the file on a mouse click\nProvides time values on mouse hover for all bars\nIn the illustration above, analysis is performed with the DVPC-SPAN1000 application license for DataVu-PC. The signal is a linear frequency chirp of 10 usec duration, 800 MHz bandwidth recorded with the RSA7100A using SignalVu-PC®. The spectrogram analysis is set to 99% FFT overlap on a pulse of 10 μsec duration.\nRSA LiveVu enables control of and recording with Tektronix USB-based spectrum analyzers. In the illustration above, a 10 MHz span in the FM radio band is monitored. You can set frequency, span, reference level and preamplifier state, and you can create IQ waveform recordings at the push of a button, or when an internal or external trigger is received by the instrument(s). Recording bandwidths from 9.7 kHz to 40 MHz are available, allowing you to greatly reduce the data load of the recording. Control of and recording with a single instrument is included in the DVPC-SPAN licenses for SignalVu-PC.\nDVPC-MREC enables live control and recording with two USB-based spectrum analyzers\n|Features and Functions||Description|\n|DVPC-MREC Application License\n(requires any DVPC-SPAN license and DVPC-SMARK license\n|Ability to operate and record from two Tektronix RSA300/500/600 series spectrum analyzers simultaneously|\nBe two places at once with the DVPC-MREC application, enabling independent control of two instruments. You can record widely-separated transmit/receive bands, or any two areas of interest with this capability. You can set one analyzer to sweep a broad span (up to the full range of the instrument), and then direct the second analyzer to tune to areas of interest found with the broad sweep. In the illustration above, a wide 9 kHz-1 GHz sweep is running in instrument one, and a 40 MHz span from that sweep is shown in the DPX real-time display from the second instrument. Recordings can be made from 9.7 kHz span up to 40 MHz on either or both instruments.\nMeasurements enabled with application license DVPC-SMARK\nDVPC-SMARK enables the smart markers in DataVu-PC, with user-defined amplitude search and mark, frequency-mask search, and enables the Time Overview display. Requires a DVPC-SPAN application license. Capabilities are shown in the table below.\n|Features and Functions||Description|\n|DVPC-SMARK application license (requires any SPAN application license)||Ability to search or tag an unlimited number of markers in a file\nAdds Power Threshold sSearch, eMarker View and Time Overview features to any base version of DataVu-PC\n|eMarker Functions||Manually add a marker on mouse click or in table\nAdd user comments on each marker\nMake delta marker measurements\nDefine file start-stop time to be between any two markers\nStore and recall saved markers in PDW format\nLoad existing marker or event trigger files from previous analysis in .xmark or .emark formats\nDelete individual or all markers\n|eMarker Visualizations||Visually display all markers on progress bar\nProvides nested marker display for files with up to 2,000,000 markers\nView all markers in eMarker list display\n|Power Threshold Search||Power Threshold: Minimum power amplitude a signal must exceed to be discovered as a marker\nSet Minimum Sample Dropout: Ignore dropouts less than user-defined number of samples in length\nSet Minimum Pulse Sample Duration: Ignore pulses less than user-defined number of samples in length\nMoving Average Filter: Determines number of filter points to use in search\n|Frequency Mask Search||Frequency Mask Search will mark each violation of a user-defined frequency mask present in the recorded waveform. Users can define hundreds of points in frequency and amplitude, or the search can auto-draw a search mask based on the on-screen waveform|\n|Time Overview Display||Time Overview displays the entire recording in a single view. You can zoom on areas of interest. The zoomed in area sets the start/stop points for detailed analysis in the spectrum, spectrogram and power vs. time displays|\neMarkers allow you to find all possible markers that match your desired criteria and save up to 2,000,000 points of interest in a file. In the illustration above, the power threshold search has been used to find signals of interest and place them in the emarker list view. The marker points are also indicated as vertical grey lines on the green progress bar at the bottom of the screen. Up to three nested progress bars can be displayed for greater marker resolution in the file. Marker points can be used to define the re-analysis point for the recording, allowing you to quickly zoom in on the signal of interest for deeper examination.\nThe frequency mask search can quickly find spectrum violations and signals lower than the dominant signals in the recording based on both amplitude and frequency characteristics. In the illustration above, a mask has been created for just the signal of interest (on the right), so that markers will only be placed on the infrequent violations in the file. The equal-amplitude signal to the left of the signal of interest is ignored during the search. Four violations were found in the search, indicated in the results table. You can immediately determine the repetition rate and duration of each transient, and make notes in the table on the nature of each violation. The progress bar at the bottom of the screen shows a mark at each violation time in the file, and the spectrogram has been set to display the second violation.\nThe Time Overview display in application license SMARK provides the +peak (yellow), -peak (purple) and average (blue) traces of the entire recording in a single snapshot. You can use the mouse to identify an area of interest, and this portion of the time record is used to set the start and stop points of the detailed analysis. In this way, you can easily zoom in on active regions, quickly examing areas of interest.\nMeasurements enabled with application license DVPC-PULSE\nDVPC-PULSE enables search and marker measurements. Up to 2 million pulses can later be stored for reanalysis in PDW and JSON format. Requires DVPC-SPAN and DVPC-SMARK application licenses. Functions and features of DVPC-PULSE are shown below.\n|Features and functions||Description|\n|DVPC-PULSE Application License (requires DVPC-SPAN and DVPC-SMARK licenses)||Adds pulse measurements|\n|Pulse Measurements||Results for start/stop times, pulse average power, peak power, pulse duration, PRI, pulse start/stop frequencies\nSave up to 2,000,000 pulse results as eMarkers and view them in a list\nExport pulse measurements in .PDW\nThe pulse analysis application license illustrated above enables you to search for pulses and return all results with start/stop time, average power, peak power, pulse duration, PRI and start/stop frequencies. Pulse parameters can be exported to a CSV file for use with other tools.\nPC Windows 7, 8 or 10 (64-bit) with minimum 1 GB RAM, 400 MB drive space, and internet connection for software activation and playback functions. Operation of RSA LiveVu recording features requires a PC capable of streaming 2-USB 3.0 ports each at 300 MBytes/second. Each USB 3.0 port must support the 5 Volt, 1 Amp USB 3.0 standard. Recording at 40 MHz bandwidth requires a drive capable of streaming storage at 300 MBytes/second. Recording two simultaneous 40 MHz bandwidth streams requires a drive.\nDataVu-PC uses the Tektronix Asset Management System (AMS) for licensing your applications. Two types of licenses (Node-locked and Floating) are available.\nNode-locked license: Can be associated with, and re-associated to, a different PC 2 times.\nFloating license: Can be associated with a PC and can be re-associated to different PCs an unlimited number of times.\nFree version and Trial license available\nThe free version of DataVu-PC gives you limited functionality to view any supported recording for a short duration (100 milliseconds). A recording of Bluetooth, WLAN, and microwave oven signals is included with the software for your evaluation. A trial license that unlocks all capabilities of DataVu-PC is available at www.tek.com/slam/catalog/trials. You will need the Host-ID of your computer to obtain the trail license. Your Host-ID can be found by opening the free version of DataVu-PC, and selecting the license manager button near the bottom left of the screen.\nFunctions available in the free download version of DataVu-PC\nThe free download includes a recorded file with the software. The file contains Bluetooth, WLAN and microwave oven signals. You can open the file and view the content utilizing any of the following three displays: spectrum, spectrogram and/or power vs. time. Basic controls for the three displays are available. The free version includes file conversion of all supported file types for any length. Full functionality for all application licenses can be activated with a 30-day trial license, available at www.tek.com/slam/catalog/trials.\n|Features (Free Version)||Description|\n|File Types Supported For Playback||.XDAT, .TIQ and .SIQD files at all bandwidths. Maximum playback length: 100 milliseconds|\n|File Conversions Supported||R3F to .SIQD or .TIQ.\n12-bit .XDAT to 16-bit .XDAT\nAll supported file types for playback\n|Playback functionality||Fixed FFT overlap and speed controls. Ability to play back a recorded file using any of the available displays: spectrum, spectrogram and/or power vs. time. Controls for persistence spectrum decay, max hold trace, continuous playback, windows, and power scale adjustments|\nDataVu-PC ordering information\nHard copy versions of the software are not available. An operation manual is distributed in .pdf format with the software.\nWhen purchasing DataVu-PC, you choose any one of the three base version DVPC-SPAN licenses (50 MHz, 200 MHz or 1000 MHz). The only difference between span licenses is the bandwidth of the allowed analysis. Choose the bandwidth that covers the maximum bandwidth of your acquisition/recording system. For example, all USB-based analyzers are accommodated with the DVPC-SPAN50 license, and all RSA7100A recordings at full bandwidth require DVPC-SPAN1000.\nDVPC-SMARK, DVPC-MREC, and DVPC-PULSE work with any DVPC-SPAN bandwidth license chosen for analysis. The DVPC-SMARK license requires a DVPC-SPAN license of any bandwidth, and the DVPC-MREC and DVPC-PULSE licenses require a DVPC-SMARK license.\n|DVPC-SPAN50NL||Node locked||Base version, DataVu-PC operation on acquisitions to 50 MHz bandwidth, plus LiveVu operation of one USB instrument|\n|DVPC-SPAN200NL 1||Node locked||Base version, DataVu-PC operation on acquisitions to 200 MHz bandwidth, plus LiveVu operation of one USB instrument|\n|DVPC-SPAN1000NL||Node locked||Base version, DataVu-PC operation on acquisitions to 1000 MHz bandwidth, plus LiveVu operation of one USB instrument|\n|DVPC-SMARKNL||Node locked||DataVu-PC Smart Markers, Time Overview, and Frequency Mask Search (requires base version)|\n|DVPC-MRECNL||Node locked||Multi-unit recording for USB spectrum analyzers (requires DVPC-SMARK)|\n|DVPC-PULSENL||Node locked||DataVu-PC pulse analysis (requires DVPC-SMARK)|\n1If you have a data source that operates at 50 MHz to 200 MHz bandwidth, such as a Tektronix RSA5000 or RSA6000 series spectrum analyzer with a third-party recording solution, choose DVPC-SPAN200.\nDataVu-PC operates on recordings using all of the instruments listed below. DataVu-PC can create recordings on all USB-based instruments listed below (RSA300/500/600 series).\nIn addition, recordings made with the RSA5000A/B and RSA6000A/B series using third-party recorders in .XDAT or .TIQ waveform formats can also be analyzed.\n|RSA306, RSA306B||USB-based portable spectrum analyzers|\n|RSA500 series||USB-based portable, ruggedized, battery operated spectrum analyzers|\n|RSA600 series||USB-based laboratory spectrum analyzers|\n|RSA6100, RSA5100 series||High performance laboratory spectrum analyzers with IQ output, compatible with 3rd-party recording solutions|\n|RSA7100A||Spectrum Analyzer, 16 kHz-26.5 GHz, up to 800 MHz acquisition bandwidth, laboratory form factor with available streaming option in SignalVu-PC|\n|Наличие лицензий в соответствии с полосой пропускания захваченного сигнала.||Минимизация стоимости решений для регистрации благодаря использованию файлов всех решений для регистрации компании Tektronix (серии RSA300, RSA500, RSA600, RSA7100).|\n|Спектрограмма с цветовой градацией||Минимизация времени перехвата с помощью спектрограмм в реальном времени. Простое обнаружение помех от переходных процессов и внутриканальных помех.|\n|Отображение плотности спектра с цветовой градацией||Визуализация нечастых сигналов и отображение зависимостей между временем и частотой, что сокращает время анализа|\n|Регулирование перекрытия и скорости быстрого преобразования Фурье||Оптимизация зависимости высокой вероятности перехвата от времени анализа|\n|Отображение зависимости мощности от времени||Потоковое отображение нуля и шкалы для подробного анализа импульсов|\n|Экспорт участков, представляющих интерес, в форматы .XDAT, SIQ и .TIQ||Анализ с помощью специальных инструментов, включая SignalVu-PC, для минимизации времени решения|\n|Индикатор обработки файла||Быстрая оценка состояния анализа с отображением маркированных событий|\n|Ползунки, настраиваемые пользователем, для определения точек пуска и останова||Сокращение времени повторной оценки сигналов, представляющих интерес|\n|Экран Time Overview (временной обзор)||Просмотр временной области для всей записи на одном изображении. Масштабирование и повторный анализ в других областях для быстрого сокращения объема данных|\n|eMarker||Хранение результатов поиска, анализа, параметров запуска и другой информации в экспортируемом табличном формате|\n|Анализ импульсов||Расчет времени пуска и останова, средней и пиковой мощности, ширины импульса и уровня основного сигнала канала в формате PDW для неограниченного количества импульсов, что сокращает необходимость в других инструментах анализа|']	['<urn:uuid:53ef9f03-bdbf-43e8-8d34-51702e364c6c>', '<urn:uuid:69c954dc-c063-4174-97f8-040dde002417>']	factoid	with-premise	verbose-and-natural	distant-from-document	multi-aspect	novice	2025-05-12T20:58:04.490895	27	94	3690
70	total rolling resistance work joules alpe d huze climb 142 lbs cyclist	The total rolling resistance work for the Alpe d'Huze climb with a 142 lbs combined cyclist-cycle weight is 16674.24 Joules, calculated using the formula WRR = weight in Newtons * coefficient of rolling resistance (0.002) * distance (13.2 km).	['Rolling Resistance in Climbing\nRolling resistance on the flats is just the weight of the combined cyclist and cycle multiplied by some coefficient of rolling resistance. For asphalt, the coefficient is 0.002. On the flats, the total effort required to overcome rolling resistance is therefore simply the force which remains constant times the distance ridden.\nIn climbs, rolling resistance is computed in a similar fashion with one exception. It makes a slight adjustment, so small that it is routinely ignored. But it is also worth mentioning.\nOn the flats, the entire combined weight is directed into the road. But what happens on a slope? In most discussions, the assumption is that in computing rolling resistance on a slope, your weight remains the slope independent of the slope.\nBut consider a case where you are riding up a nearly vertical hill with a slope of 89°. How much of your weight is directed into the slope and how much straight down. In that case, nearly all of your weight is directed downwards with only a small fraction directed into the slope, that is only 0.017%. If you weighed 175 lbs, that would mean the slope would feel that your weight was only 3 lbs.\nDo we really need to worry about this, particularly given a 10° slope is consider an upper limit even for Elite Cyclists. Here is a chart which shows it can be ignored with only minimal impact Each line corresponds to a different weight. For realistic cycling slopes, we are talking about an adjustment in the range of 0.02 lbs.\nComputing Total Rolling Resistance Work for a Climb\nAll of this makes it easy to compute the total work a cyclist needs to perform to make a climb against rolling resistance. They simply need to take the distance to the top and multiply it times the combined cyclist-cycle weight and the coefficient of Rolling Resistance.\nThe Alpe d’Huze climb is 13.2 km. What is the total effort expended against just rolling resistance? Te coefficient of rolling resistance is 0.002, and the combined cyclist-cycle weight is (126 lbs + 16 lbs) or 142 lbs. Converting to CGS units, 142 lbs is 631.6 newtons. Then the total effort is\nWRR(Joules)= wgt(Newtons) * coeff * distance(m)\nWRR = 631.6 Newtons * 0.002 * 13.2 km = 16674.24 Joules\nIf you are curious about not including the slope factor, comparing this to the slope corrected value indicates the above overestimated the work by 56 Joules.\nUnderstanding Total Rolling Resistance Work\nWork quantifies the total effort to do a task. What does the number above mean? What is a comparable effort? What are some equivalent energy outputs comparable to dealing with rolling resistance on the Alpe d’Huze?\nThe human body releases heat energy of 60 Joules per second which is the amount we overestimated the total work.\nThe total effort of 16.67 kJoules is comparable to:\n- Energy release by a 4 Watt light bulb over an hour\n- Energy in two alkaline AA Batteries\n- Energy in four food calories']	['<urn:uuid:b3d43fdf-7319-4768-a768-68aff33f320e>']	factoid	direct	long-search-query	similar-to-document	single-doc	expert	2025-05-12T20:58:04.490895	12	39	508
71	music religion ancient egypt connection leadership women	Music and religion in ancient Egypt were deeply interconnected, with musical performances being essential in religious ceremonies and temples employing their own female musical troupes to entertain the gods in daily rituals. The prestigious title of 'shemayet' allowed musicians, primarily women, to perform for specific deities. Moreover, women held significant religious leadership positions - the title of 'God's Wife' was held by royal women and carried tremendous political power, second only to the king, and they could even act as their deputy. Women also served as priestesses in both male and female deities' services.	"['Music has been an integral part of Egyptian culture since antiquity. The Bible documents the instruments played by the ancient Hebrews, all of which are correlated in Egyptian archaeology. Egyptian music probably had a significant impact on the development of ancient Greek music, and via the Greeks was important to early European music well into the Middle Ages. The modern music of Egypt is considered Arabic music as it has been a source for or influence on other regional styles. The tonal structure of Arabic music is defined by the maqamat, loosely similar to Western modes, while the rhythm of Arabic music is governed by the iqa\'at, standard rhythmic modes formed by combinations of accented and unaccented beats and rests.e your paragraph here.\nMusic was as important to the ancient Egyptians as it is in our modern society. Although it is thought\nthat music played a role throughout the history of Egypt, those that study the Egyptian writings have\ndiscovered that music seemed to become more important in what is called the ‘pharaonic’ period of\ntheir history. This was the time when the Egyptian dynasties of the pharaohs were\nestablished (around 3100 BCE) and music was found in many parts of every day Egyptian life.\nAncient Egyptians had a number of professional musicians that performed for many occasions.\nSince their society was set up with social levels, this meant that different musicians could play only\nfor specific events. A musician with a high status could play for religious ceremonies at the temples,\nwhere a lower class musician might only be able to play for regular community members.\nThe highest honor to achieve was the status of ‘shemayet’, which gave these musicians the\nability to play for a particular god or goddess and these musicians were mostly women.\nThe ancient Egyptians were very organized and this included how they organized and arranged music\nand musicians. They brought music to their religious ceremonies, but it was also played and\nperformed in workshops, palaces, the farms, on the battlefield and even in their tombs.\nThe Egyptian gods Hathor and Bes were their gods of music and they had many ceremonies\ndevoted to them that involved song and dance to accompany the playing of musical instruments\nThe ancient Egyptians credited the goddess Bat with the invention of music. The cult of Bat was eventually syncretised into that of Hathor because both were depicted as cows. Hathor\'s music was believed to have been used by Osiris as part of his effort to civilize the world. The lion-goddess Bastet was also considered a goddess of music.\nIn prehistoric Egypt, music and chanting were commonly used in magic and rituals. Rhythms during this time were ovular and music served to create rhythm. Small shells were used as whistles.(pp26–30)\nDuring the predynastic period of Egyptian history, funerary chants continued to play an important role in Egyptian religion and were accompanied by clappers or a flute. Despite the lack of physical evidence in some cases, Egyptologists theorize that the development of certain instruments known of the Old Kingdom period, such as the end-blown flute, took place during this time.(pp33–34)\nThe evidence is for instruments played more securely attested in the Old Kingdom when harps, flutes and double clarinets were played. Percussion instruments and lutes were added to orchestras by the Middle Kingdom. Cymbals frequently accompanied music and dance, much as they still do in Egypt today.\nTypically ancient Egyptian music was composed from the phrygian dominant scale, phrygian scale, double harmonic scale (Arabic scale) or lydian scale. The phrygian dominant scale may often feature an altered note or two in parts to create tension. For instance the music could typically be in the key of E phrygian dominant using the notes E, F, G sharp, A, B, C, D and then have an A sharp, B, A sharp, G natural and E to create tension.\nArabic music is usually said to have begun in the 7th century in Syria during the Umayyad dynasty. Early Arabic music was influenced by Byzantine, Indian and Persian forms, which were themselves heavily influenced by earlier Greek, Semitic, and ancient Egyptian music.\nEgyptians in Medieval Cairo believed that music exercised ""too powerful an effect upon the passions, and leading men into gaiety, dissipation and vice."" However, Egyptians generally were very fond of music. Though, according to E.W. Lane, no ""man of sense"" would ever become a musician, music was a key part of society. Tradesmen of every occupation used music during work and schools taught the Quran by chanting.(p359)\nThe music of Medieval Egypt was derived from Greek, Persian and Indian traditions. Lane said that ""the most remarkable peculiarity of the Arab system of music is the division of tones into thirds,"" although today Western musicologists prefer to say that Arabic music\'s tones are divided into quarters. The songs of this period were similar in sound and simple, within a small range of tones. Egyptian song, though simple in form, is embellished by the singer. Distinct enunciation and a quavering voice are also characteristics of Egyptian singing.(pp360–361)\nMale professional musicians during this period were called Alateeyeh (plural), or Alatee (singular), which means ""a player upon an instrument"". However, this name applies to both vocalists as well as instrumentalists. This position was considered disreputable and lowly. However, musicians found work singing or playing at parties to entertain the company. They generally made three shillings a night, but earned more by the guests giving more.\nFemale professional musicians were called Awalim (pl) or Al’meh, which means a learned female. These singers were often hired on the occasion of a celebration in the harem of a wealthy person. They were not with the harem, but in an elevated room that was concealed by a screen so as not to be seen by either the harem or the master of the house. The female Awalim were more highly paid than male performers and more highly regarded than the Alateeyeh as well. Lane relates an instance of a female performer who so enraptured her audience that she earned to fifty guineas for one night\'s performance from the guests and host, who were not considered wealthy.\nModern Egyptian classical and pop music\nEgyptian music began to be recorded in the 1910s, when Egypt was still part of the Ottoman Empire. The cosmopolitan Ottomans encouraged the development of the arts, encouraging women and minorities to develop their musical abilities. By the fall of the Empire, Egypt\'s classical musical tradition was already thriving, centered on the city of Cairo. In general, modern Egyptian music blends its indigenous traditions with Turkish, Arabic, and Western elements.\nSince the end of World War I, some of the Middle East\'s biggest musical stars have been Egyptian. Contemporary Egyptian music traces its beginnings to the creative work of luminaries such as Abdu-l Hamuli, Almaz and Mahmud Osman, who were all patronized by the Ottoman Khedive Ismail, and who influenced the later work of the 20th century\'s most important Egyptian composers: Sayed Darwish, Umm Kulthum, Mohammed Abdel Wahab, Abdel Halim Hafez, and Zakariyya Ahmad. Most of these stars, including Umm Kulthum and Najat Al Saghira, were part of the classical\nReligious music in Egypt\nReligious music remains an essential part of traditional Muslim and Coptic celebrations called mulids. Mulids are held in Egypt to celebrate the saint of a particular church. Muslimmulids are related to the Sufi zikr ritual. The Egyptian flute, called the ney, is commonly played at mulids. The liturgical music of the Alexandrian Rite also constitutes an important element of Egyptian music and is said to have preserved many features of ancient Egyptian music.\nLute and double pipe players from a painting found in the Theban tomb of Nebamun, a nobleman of the 18th Dynasty of the New Kingdom, c. 1350 BC\nEgyptian folk music, including the traditional Sufi dhikr rituals, are the closest contemporary music genre to ancient Egyptian music, having preserved many of its features, rhythms and instruments.\nFolk and roots revival\nThe Egyptians even used their own teeth as instruments they would make tapping noises and would use special plucks to make interesting noises with their teeth. The 20th century has seen Cairo become associated with a roots revival. Musicians from across Egypt are keeping folk traditions alive, such as those of rural Egyptians (fellahin), the Nubians, the Arabs, the Berbers, the Gypsiesand the Bedouins. Mixtures of folk and pop have also risen from the Cairo hit factory.\nSince the Nasser era, Egyptian pop music has become increasingly important in Egyptian culture, particularly among the large youth population of Egypt. Egyptian folk music continues to be played during weddings and other traditional festivities. In the last quarter of the 20th century, Egyptian music was a way to communicate social and class issues. Among some of the most popular Egyptian pop singers today are Mohamed Mounir and Amr Diab.\nSawahli (coastal) music is a type of popular music from the northern coast, and is based around the simsimiyya, an indigenous stringed instrument. Well-known singers include Abdo\'l Iskandrani and Aid el-Gannirni.\nSaidi (Upper Egyptian)\nEgyptian musicians from Upper Egypt play a form of folk music called Ṣa‘īdi (Upper Egyptian). Metqal Qenawi\'s Les Musiciens du Nil are the most popular saidi group, and were chosen by the government to represent Egyptian folk music abroad. Other performers include Shoukoukou, Ahmad Ismail, Omar Gharzawi, Sohar Magdy and Ahmed Mougahid.\nNubians are native to the south of Egypt and northern Sudan, though many live in Cairo and other cities. Nubian folk music can still be heard, but migration and intercultural contact with Egyptian and other musical genres have produced new innovations. Ali Hassan Kuban\'s efforts had made him a regular on the world music scene, while Mohamed Mounir\'s social criticism and sophisticated pop have made him a star among Nubians, Egyptians, and other people worldwide. Ahmed Mounib, Mohamed Mounir\'s mentor, was by far the most notable Nubian singer to hit the Egyptian music scene, singing in both Egyptian Arabic his native Nobiin. Hamza El Din is another popular Nubian artist, well-known on the world music scene and has collaborated with the Kronos Quartet.\nWestern classical music\nWestern classical music was introduced to Egypt, and, in the middle of the 18th century, instruments such as the piano and violin were gradually adopted by Egyptians. Opera also became increasingly popular during the 18th century, and Giuseppe Verdi\'s Egyptian-themed Aida was premiered in Cairo on December 24, 1871.\nBy the early 20th century, the first generation of Egyptian composers, including Yusef Greiss, Abu Bakr Khairat, and Hasan Rashid, began writing for Western instruments. The second generation of Egyptian composers included notable artists such as Gamal Abdelrahim. Representative composers of the third generation are Ahmed El-Saedi and Rageh Daoud. In the early 21st century, even fourth generation composers such as Mohamed Abdelwahab Abdelfattah (of the Cairo Conservatory) have gained international attention.\nAncient Egyptians Music', ""Active and independent individuals, women in Ancient Egypt enjoyed a legal equality with men that their sisters in the modern world did not manage until the 20th century, and a financial equality that many have yet to achieve.\nWhilst the concept of a career choice for women is a relatively modern phenomenon, the situation in ancient Egypt was rather different. For some three thousand years the women who lived on the banks of the Nile enjoyed a form of equality which has rarely been equalled.\nIn order to understand their relatively enlightened attitudes toward sexual equality, it is important to realise that the Egyptians viewed their universe as a complete duality of male and female. Giving balance and order to all things was the female deity Maat, symbol of cosmic harmony by whose rules the pharaoh must govern.\nThe Egyptians recognised female violence in all its forms, their queens even portrayed crushing their enemies, executing prisoners or firing arrows at male opponents as well as the non-royal women who stab and overpower invading soldiers. Although such scenes are often disregarded as illustrating 'fictional' or ritual events, the literary and archaeological evidence is less easy to dismiss. Royal women undertake military campaigns whilst others are decorated for their active role in conflict. Women were regarded as sufficiently threatening to be listed as 'enemies of the state', and female graves containing weapons are found throughout the three millennia of Egyptian history.\nRoyal women undertook military campaigns whilst others were decorated for their active role in conflict. Women were regarded as sufficiently threatening to be listed as 'enemies of the state', and female graves containing weapons are found throughout the three millennia of Egyptian history.\nAlthough by no means a race of Amazons, their ability to exercise varying degrees of power and self-determination was most unusual in the ancient world, which set such great store by male prowess, as if acknowledging the same in women would make them less able to fulfil their expected roles as wife and mother. Indeed, neighbouring countries were clearly shocked by the relative freedom of Egyptian women and, describing how they 'attended market and took part in trading whereas men sat and home and did the weaving', the Greek historian Herodotus believed the Egyptians 'have reversed the ordinary practices of mankind'.\nAnd women are indeed portrayed in a very public way alongside men at every level of society, from co-ordinating ritual events to undertaking manual work. One woman steering a cargo ship even reprimands the man who brings her a meal with the words, 'Don't obstruct my face while I am putting to shore' (the ancient version of that familiar conversation 'get out of my way whilst I'm doing something important').\nEgyptian women also enjoyed a surprising degree of financial independence, with surviving accounts and contracts showing that women received the same pay rations as men for undertaking the same job - something the UK has yet to achieve. As well as the royal women who controlled the treasury and owned their own estates and workshops, non-royal women as independent citizens could also own their own property, buy and sell it, make wills and even choose which of their children would inherit.\nLadies of leisure\nThe most common female title 'Lady of the House' involved running the home and bearing children, and indeed women of all social classes were defined as wives and mothers first and foremost. Yet freed from the necessity of producing large numbers of offspring as an extra source of labour, wealthier women also had alternative 'career choices'.\nAfter being bathed, depilated and doused in sweet heavy perfumes, queens and commoners alike are portrayed sitting patiently before their hairdressers, although it is equally clear that wigmakers enjoyed a brisk trade. The wealthy also employed manicurists and even female make-up artists, whose title translates literally as 'painter of her mouth'. Yet the most familiar form of cosmetic, also worn by men, was the black eye paint which reduced the glare of the sun, repelled flies and looked rather good.\nWomen are indeed portrayed in a very public way alongside men at every level of society, from co-ordinating ritual events to undertaking manual work.\nDressing in whatever style of linen garment was fashionable, from the tight-fitting dresses of the Old Kingdom (c.2686 - 2181 BC) to the flowing finery of the New Kingdom (c.1550 - 1069 BC), status was indicated by the fine quality of the linen, whose generally plain appearance could be embellished with coloured panels, ornamental stitching or beadwork. Finishing touches were added with various items of jewellery, from headbands, wig ornaments, earrings, chokers and necklaces to armlets, bracelets, rings, belts and anklets made of gold, semi-precious stones and glazed beads.\nWith the wealthy 'lady of the house' swathed in fine linen, bedecked in all manner of jewellery, her face boldly painted and wearing hair which more than likely used to belong to someone else, both male and female servants tended to her daily needs. They also looked after her children, did the cleaning and prepared the food, although interestingly the laundry was generally done by men.\nFreed from such mundane tasks herself, the woman could enjoy all manner of relaxation, listening to music, eating good food and drinking fine wine. One female party-goer even asked for 'eighteen cups of wine for my insides are as dry as straw'. Women are also portrayed with their pets, playing board games, strolling in carefully tended gardens or touring their estates. Often travelling by river, shorter journeys were also made by carrying-chair or, for greater speed, women are even shown driving their own chariots.\nWomen at the top\nThe status and privileges enjoyed by the wealthy were a direct result of their relationship with the king, and their own abilities helping to administer the country. Although the vast majority of such officials were men, women did sometimes hold high office. As 'Controller of the Affairs of the Kiltwearers', Queen Hetepheres II ran the civil service and, as well as overseers, governors and judges, two women even achieved the rank of vizier (prime minister). This was the highest administrative title below that of pharaoh, which they also managed on no fewer than six occasions.\nEgypt's first female king was the shadowy Neithikret (c.2148-44 BC), remembered in later times as 'the bravest and most beautiful woman of her time'. The next woman to rule as king was Sobeknefru (c.1787-1783 BC) who was portrayed wearing the royal headcloth and kilt over her otherwise female dress. A similar pattern emerged some three centuries later when one of Egypt's most famous pharaohs, Hatshepsut, again assumes traditional kingly regalia. During her fifteen year reign (c.1473-1458 BC) she mounted at least one military campaign and initiated a number of impressive building projects, including her superb funerary temple at Deir el-Bahari.\nAncient Egyptian women enjoyed a surprising degree of financial independence, with surviving accounts and contracts showing that women received the same pay rations as men for undertaking the same job - something the UK has yet to achieve!\nBut whilst Hatshepsut's credentials as the daughter of a king are well attested, the origins of the fourth female pharaoh remain highly controversial. Yet there is far more to the famous Nefertiti than her dewy-eyed portrait bust. Actively involved in her husband Akhenaten's restructuring policies, she is shown wearing kingly regalia, executing foreign prisoners and, as some Egyptologists believe, ruling independently as king following the death of her husband c.1336 BC. Following the death of her husband Seti II in 1194 BC, Tawosret took the throne for herself and, over a thousand years later, the last of Egypt's female pharaohs, the great Cleopatra VII, restored Egypt's fortunes until her eventual suicide in 30 BC marks the notional end of ancient Egypt.\nWives and mothers\nBut with the 'top job' far more commonly held by a man, the most influential women were his mother, sisters, wives and daughters. Yet, once again, many clearly achieved significant amounts of power as reflected by the scale of monuments set up in their name. Regarded as the fourth pyramid of Giza, the huge tomb complex of Queen Khentkawes (c.2500 BC) reflects her status as both the daughter and mother of kings. The royal women of the Middle Kingdom pharaohs were again given sumptuous burials within pyramid complexes, with the gorgeous jewellery of Queen Weret discovered as recently as 1995.\nDuring Egypt's 'Golden Age', (the New Kingdom, c.1550-1069 BC), a whole series of such women are attested, beginning with Ahhotep whose bravery was rewarded with full military honours. Later, the incomparable Queen Tiy rose from her provincial beginnings as a commoner to become 'great royal wife' of Amenhotep III (1390-1352 BC), even conducting her own diplomatic correspondence with neighbouring states.\nPharaohs also had a host of 'minor wives' but, since succession did not automatically pass to the eldest son, such women are known to have plotted to assassinate their royal husbands and put their sons on the throne. Given their ability to directly affect the succession, the term 'minor wife' seems infinitely preferable to the archaic term 'concubine'.\nYet even the word 'wife' can be problematic, since there is no evidence for any kind of legal or religious marriage ceremony in ancient Egypt. As far as it is possible to tell, if a couple wanted to be together, the families would hold a big party, presents would be given and the couple would set up home, the woman becoming a 'lady of the house' and hopefully producing children.\nWomen often held high offices in Ancient Egypt, and the records show that more than six were anointed pharaohs, others ran the civil service and, as well as overseers, governors and judges, two women even achieved the rank of vizier (prime minister).\nWhilst most chose partners of a similar background and locality, some royal women came from as far afield as Babylon and were used to seal diplomatic relations. Amenhotep III described the arrival of a Syrian princess and her 317 female attendants as 'a marvel', and even wrote to his vassals - 'I am sending you my official to fetch beautiful women, to which I the king will say good. So send very beautiful women - but none with shrill voices'!\nSuch women were given the title 'ornament of the king', chosen for their grace and beauty to entertain with singing and dancing. But far from being closeted away for the king's private amusement, such women were important members of court and took an active part in royal functions, state events and religious ceremonies.\nWith the wives and daughters of officials also shown playing the harp and singing to their menfolk, women seem to have received musical training. In one tomb scene of c.2000 BC a priest is giving a kind of masterclass in how to play the sistrum (sacred rattle), as temples often employed their own female musical troupe to entertain the gods as part of the daily ritual.\nIn fact, other than housewife and mother, the most common 'career' for women was the priesthood, serving male and female deities. The title, 'God's Wife', held by royal women, also brought with it tremendous political power second only to the king, for whom they could even deputise. The royal cult also had its female priestesses, with women acting alongside men in jubilee ceremonies and, as well as earning their livings as professional mourners, they occasionally functioned as funerary priests.\nA brilliant linguist, Cleopatra VII, endowed the Great Library at Alexandria, the intellectual capital of the ancient world where female lecturers are known to have participated alongside their male colleagues. Yet an equality which had existed for millennia was ended by Christianity - the philosopher Hypatia was brutally murdered by monks in 415 AD as a graphic demonstration of their beliefs.\nTheir ability to undertake certain tasks would be even further enhanced if they could read and write but, with less than 2% of ancient Egyptian society known to be literate, the percentage of women with these skills would be even smaller. Although it is often stated that there is no evidence for any women being able to read or write, some are shown reading documents. Literacy would also be necessary for them to undertake duties which at times included prime minister, overseer, steward and even doctor, with the lady Peseshet predating Elizabeth Garret Anderson by some 4,000 years.\nBy Graeco-Roman times women's literacy is relatively common, the mummy of the young woman Hermione inscribed with her profession 'teacher of Greek grammar'. A brilliant linguist herself, Cleopatra VII endowed the Great Library at Alexandria, the intellectual capital of the ancient world where female lecturers are known to have participated alongside their male colleagues. Yet an equality which had existed for millennia was ended by Christianity - the philosopher Hypatia was brutally murdered by monks in 415 AD as a graphic demonstration of their beliefs.\nWith the concept that 'a woman's place is in the home' remaining largely unquestioned for the next 1,500 years, the relative freedom of ancient Egyptian women was forgotten. Yet these active, independent individuals had enjoyed a legal equality with men that their sisters in the modern world did not manage until the 20th century, and a financial equality that many have yet to achieve.\nProfessor Joann Fletcher teaches in the Department of Archaeology at the University of York and is a founding member of the university’s Mummy Research Group. She is also Consultant Egyptologist for Harrogate Museums and Arts and Consultant Archaeologist for Barnsley Museum. Her publications include 'Cleopatra the Great' (Hodder & Stoughton/Harper Collins) and 'The Search for Nefertiti' (Hodder & Stoughton/Harper Collins), and she recently wrote and presented the series 'Ancient Egypt: Life and Death in the Valley of the Kings' for the BBC.""]"	['<urn:uuid:865ceb5b-33d9-4021-ae3d-e88a53e20ffb>', '<urn:uuid:5508dc37-6f87-4d71-82cc-004ef4f00f0e>']	factoid	with-premise	short-search-query	similar-to-document	multi-aspect	novice	2025-05-12T20:58:04.490895	7	94	4107
72	national enterprise level osh management system implementation benefits challenges	At the national level, OSH management systems help bring coherence, coordination, and simplification in implementing regulatory requirements. Countries like Argentina, Brazil, and Israel have formally adopted ILO guidelines as their model. At the enterprise level, while OSHMS helps continuously evaluate and improve protection and prevention measures, organizations must analyze their specific needs and means to tailor the system accordingly. The challenge lies in managing varying human nature, skills sets, process complexity, and local culture while ensuring effective implementation across both national and enterprise levels.	['What are OSH Management Systems, and how can they help prevent workplace accidents?\nToday, global OSH strategies mainly include building and maintaining a national preventative safety and health culture and the introduction of a systems approach to OSH management. An OSH Management System (OSHMS) can be seen as an important preventative tool to effectively manage hazards and risks at work. It is based on OSH criteria, standards and performance, and, more importantly, it aims to establish a comprehensive and structured mechanism for action for both managers and workers when implementing safety and health measures. An OSH Management System follows a logical, step-by-step method to determine what needs to be done, how best to do it, monitor progress, evaluate how well it is done and identify areas for improvement. Perhaps most importantly, it is a mechanism meant for constant and continual improvement.\nWhat is the ILO’s role in promoting OSH Management Systems?\nThe ILO’s Guidelines on Occupational safety and health management systems, adopted in 2001, have become the benchmark for developing OSH management systems standards and the most widely used model at both national and enterprise levels. The ILO provides technical assistance to countries interested in implementing their own OSH management systems and offers training courses on the subject. In addition, the systems approach for OSH management will be one of the priority areas for discussion at the XIX World Congress on Safety and Health at Work being co-organized by the ILO in Istanbul in September. For more information about the World Congress, see www.safety2011turkey.org.\nCan you give us concrete examples?\nCountries such as Argentina, Brazil, Israel and Ireland have formally recognized the ILO guidelines as a model for national promotion or the development of OSHMS guidelines adapted to their national needs. The French standardization organization (AFNOR) promotes ILO guidelines based on the resolution of its committee responsible for OSH strategies.. The Former Yugoslav Republic of Macedonia has just started a three year programme to implement ILO-OSH 2001 in medium and large enterprises. In Japan, tailored guidelines have been developed using ILO guidelines as a model.\nHow can OSH management systems help the implementation of policies on a national level?\nAs occupational safety and health is a complex area that relies on the involvement of many disciplines and stakeholders, using a management systems approach can help bring about coherence, coordination, simplification and speed in the inclusion of regulatory requirements into any OSH measures on a national level.\nWhy are OSH management systems relevant to enterprises?\nImplementation and compliance with the OSH requirements under national laws and regulations is the responsibility of employers. Using a systems approach to the management of OSH in an enterprise ensures that the level of both protection and prevention is continuously evaluated and improved. However, management systems should not be seen as a universal remedy and it is key that organizations analyze their needs vis-à-vis their means and tailor their OSH systems accordingly.\nHow can workers help implement an OSH management system?\nOSHMS cannot function properly without effective social dialogue through, for instance, joint safety and health committees or collective bargaining arrangements. Workers and their representatives should be given the opportunity to fully participate in the management of OSHMS in an organization. Only when all stakeholders are awarded responsibilities in maintaining it can an OSHMS system be successful.\nHow can such systems be applied to high risk sectors?\nThe chemical and energy sectors (whether nuclear, coal or oil-based) are high risk sectors where OSH management systems were first applied and used. Major industrial accidents such as the 1974 cyclohexane vapour cloud explosion at Flixborough in the UK, the 1984 Bhopal methyl isocyanate leak that killed thousands of people in India, the Chernobyl nuclear power station explosion and melt down in 1986 or more recently the 2001 ammonium nitrate explosion at the AZF plant in France, illustrate the catastrophic capabilities of industrial installations and the consequences of OSH management malfunction. Many of these events prompted the development of regulatory and technical tools to set very stringent hazard and risk assessment procedures. The ILO also developed a manual on Major Hazard Control, (1993) aimed at assisting countries in the development of control systems and programmes for major hazard installations.\nBy providing coherent, step-by-step and logical approach, OSH management systems can be effective in reducing accidents in these areas. These systems can be tailored to the management of hazards specific to a given industry or process, and in particular high risk sectors where the implementation of preventive and protective measures require a comprehensive and organized risk evaluation.', 'Managing health and safety (OH&S) issues in the workplace represents an enormous challenge due to varying human nature, skills set, process complexity & local culture and have implications for everyone at the workplace. Effectively managing these issues means taking account not only of legal requirements, but also the well-being of your personnel in the organization.\nPurpose of OHSAS 18001\nManagement of health & safety issues for an organization considering all interested parties concern is the main challenge of the business while working with significant hazardous process & risk. Achieving OHS performance with improved well being is the need to assure the regulatory bodies, customers and other stack holders due to high premium cost for any incident.\nCertification to OHSAS 18001 show the commitment to the health and safety of employees, demonstrates your ability to manage risk & hazards associated with the activities and provide assurance to all concerned including customers and management that legal compliance is effectively managed.\nImplementation of OHSAS 18001 policies gives systematic approach to minimizing health and safety risks and provide a framework for an organization to manage its legal compliance and improve occupational health and safety performance, including risk and opportunity identification, analysis, target setting, and measurement.\nOrganizations are improving the health & safety status by implementing the universally valid international standard along with best practices beside their own country specific health & safety legislations. OHSAS 18001 is basic and globally recognized standard for occupational health and safety management systems and is applicable to any organization in any business sector.\nBenefits of OHSAS 18001\nImplementing an effective occupational health and safety management system reduces the risk of harm to your employees and other personnel and reduces overall liability. Effective Management of Health and Safety risks will help:\n- Demonstrate your commitment to the protection of employee, property and plant.\n- Minimize the number of accidents and production time loss due to better control over hazards at the workplace\n- Focus on employee safety results in a satisfied, motivated and highly productive work team.\n- Increase control and reduction of hazards through the setting of objectives, targets and evolved responsibility.\n- Maximize the well-being and productivity of all people working for the organization.\n- Encourage better relationships with contractors and more effective contracted activities.\n- Reduction in insurance premiums & workers compensation\n- Demonstrates an innovative and forward thinking approach\n- Ensuring legal compliance\n- Improve safety culture & your reputation in the eyes of customers, competitors, suppliers, other stakeholders and the wider community.\nMore about OHSAS 18001\nA certificate issued by third party registrar to demonstrates that your business system has been certified against requirements of OHSAS 18001 requirements. Implementation of OHSAS 18001 by setting up of internal processes gives confidence to management, employees & society at large about the protecting the health & safety and managing risk to human being.\nOHSAS 18001 is an international standard for environmental management, applicable to companies of all sizes and types; certification to OHSAS 18001 provides a dynamic mechanism for the development of effective health & safety management system. “Plan-Do-Check-Act” principle based cycle, OHSAS 18001: 2007 specifies the most important requirements to identify, control and monitor the risk & hazards of any organization, and also how to manage and improve the whole system.\nThe OHSAS 18001 (Occupational Health and Safety Assessment Series) certification system is developed by an association of national standard bodies, group of certification bodies/registrars, and specialist of health & safety.\nFeatures of OHSAS 18001\nOHSAS series is designed to help organizations formulate occupational health and safety policies and objectives containing two documents viz. OHSAS 18001 – OHS Requirements & OHSAS 18002 which generally known as guidance document for implementation of OHSAS 18001. It is applicable to any organization, large or small, and within any business sector. OHSAS 18001 is largely aligned with the structure of ISO 14001 and is based on the two concepts of continual improvement and regulatory compliance.\nOHSAS 18001 audit covers following:\n- Policy statement (commitment of top management to improve OHS conditions)\n- Hazard identification, assessment & control (evaluation of risk & its consequence on human being)\n- Legal & other requirements (ensuring stringent compliance to the law of the land)\n- Documented objectives & targets (continual improvement)\n- Resources, Role, Responsibility & Authority (making responsible every one)\n- Competence, awareness & training (ensures availability of right person all the time)\n- Communication, participation & consultation (ensuring every one has to become part of OHS management )\n- Documentation, Control of documents & records (for ensuring compliance)\n- Operational controls(established safe working conditions)\n- Emergency preparedness & response (check your preparation to mitigate any emergency or abnormal situation)\n- Performance measurement & monitoring (ensuring health & safety parameters)\n- Incident, Nonconformity, corrective & preventive action (provides mechanism for improvement)\n- Management review (ensuring organization system is complied)\nEliminating risks and hazards\nOHS hazards & its risk to human being are identified by a team of experts considering extent of application, nature of activity & conditions in which it operates. Identified risk are prioritized by making objective to reduce its significance level by giving a frame work of management programs which identify the resources & approach to achieve the desired goal. Timely review of achieved objective & new process area will direct the organization to set the next goal towards improvement in health & safety status.\nCertification Process for OHSAS 18001\nDQS Certification India appoints a competent & suitable auditor or team of auditors to audit the organization against the standard & scope requested by the clients. Client has to file an application seeking standard for which to be certified. Gap analysis may be performed first to check readiness for the auditee organization which help organization to improve upon. Routine surveillance audits are carried out to evaluate continual improvement in the validity period. A re-certification audit is performed after every three years to maintain continuity of the certification.']	['<urn:uuid:0cf13054-1365-4d8f-83d7-04eb267330f8>', '<urn:uuid:d5f3dda1-8c28-4be4-a1f6-40a3cf34df5a>']	factoid	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:58:04.490895	9	84	1742
73	hormone function estrogen glucagon metabolism differences	Estrogen and glucagon serve different metabolic functions. Estrogen (specifically estradiol) plays a role in reproduction, sexual function, bone health, brain function, body shape, fat storage, water retention, sleep quality, mood and complexion. In contrast, glucagon primarily acts on liver cells to increase blood sugar by stimulating the release of glucose from stored glycogen. When blood glucose levels are low, glucagon signals for the breakdown of stored nutrients, while having too much or too little estrogen can result in weight gain particularly around the mid-section, hips and thighs.	['The thyroid hormones Thyroxine (T4), Triidothyronine (T3) and Calcitonin are made in the thyroid gland are responsible for regulating metabolism of every single cell in the body and for regulating body temperature. When too little T3 and T4 are released into the bloodstream resulting from an underactive thyroid, a variety of symptoms can develop. A poorly functioning thyroid – which is more common in women than in men – is likely to cause weight gain. Other symptoms of an underactive thyroid may include fatigue, slow metabolism, dry skin, forgetfulness, hair loss, depression and constipation.\nThe primary female sex hormone, estrogen, is actually a classification of three different hormones which are produced naturally – estrone (E1), estradiol (E2) and estriol (E3). Estradiol or E2 is the most important female sex hormone during a female’s reproductive years because it plays a role in reproduction, sexual function, bone health, brain function, body shape, fat storage, water retention, sleep quality, mood and complexion. In non-pregnant women E2 is produced by the ovaries and the adrenal glands and binds to estrogen receptors on cells. E1 is typically produced by fat cells but can be converted to E2 if need be.\nIf a female has too much or too little estrogen or if the proper ratio of estrogen to progesterone is disturbed, then it’s likely that weight gain particularly around the mid-section, hips and thighs will result.\nCortisol, also called the stress hormone, is released from the adrenal glands when the brain perceives some sort of physical or emotional threat. This hormonal stress response initiates a series of chemical messengers such as cortisol, adrenaline and norepinephrine to be released into the bloodstream which prepares every cell in the body to fight the threat or run away from it. While this stress response is engaged, heart rate and breathing increases, blood rushes towards the extremities and away from the internal organs, and the liver releases glucose as a source of quick energy.\nThis hormonal response is meant to be temporary and the body returns no a non-stressed state once the threat has passed. When the stressor is constant, however, cortisol is continuously released from the adrenals which can have negative consequences on the body and mind. Though the effects can vary, one popular consequence of high cortisol levels is weight gain.\nWhen we consume foods and beverages that contain carbohydrates, our digestive system breaks those carbohydrates down into glucose which is then released into our bloodstream and utilized by our cells to fuel metabolic reactions. In healthy individuals, insulin is a hormone which is secreted by the pancreas in response to elevated blood glucose levels and delivers this glucose to your cells. When cells need glucose to function but there is not enough glucose in the blood for them to use for energy, insulin helps to release glucose from the liver. This response helps to keep blood sugar from getting too high or too low.\nWhen too much glucose is consumed, the cells get their fix of energy but any leftover glucose in the blood is converted to fat and stored in the body’s fat cells.\nAs long as there’s insulin in the bloodstream the cells receive a signal to use blood glucose as energy rather than tap into body fat stores. The more glucose we eat in one sitting, the more insulin that is produced and the less our cells tap into our energy stores for fuel. Therefore, insulin prevents body fat from being burned.', 'Most people in the diabetes community know that the pancreas is the part of the body that makes insulin. But what else does the pancreas do? In fact, this very complex organ impacts your metabolism in a dizzying number of ways. This article goes over the basics.\nIn This Section\nWhat Is the Pancreas?\nThe pancreas is a gland located behind the stomach which is partially connected to the upper portion of the small intestine, called the duodenum. It produces and releases various digestive enzymes and specific hormones in response to food intake and other metabolic variables. The pancreas is a central player in the regulation of blood sugar levels.\nDigestive Enzymes Produced by the Pancreas\nThe pancreas makes and releases a variety of enzymes to help process the food that you eat.\nThe digestive enzymes secreted by the pancreas include:\n- Proteases, which break down protein\n- An amylase, which breaks down carbohydrates into simple glucose molecules\n- Nucleases, which break down nucleic acids (DNA/RNA)\n- Lipases, which break down fats\nIn addition, the pancreas also releases bicarbonate (NaHCO3), which alters pH for optimal enzymatic function. Once the enzymes come into contact with the slightly alkaline solution, they assume their active form and can digest their respective food components.\nThese factors are all released into the duodenum via the pancreatic duct in response to food consumption.\nRegulatory Hormones Produced by the Pancreas\nThe Islets of Langerhans\nThe islets of Langerhans are small cells that are dispersed throughout the pancreas. They are comprised of several different cell types that produce and release important regulatory hormones directly into the bloodstream.\nThe islets contain beta cells, which produce insulin, and alpha cells, which produce glucagon. These two hormones are key players in the regulation of blood glucose levels.\nInsulin vs Glucagon\nInsulin binds to receptors on a variety of cell types, including muscle, liver, and fat cells. Insulin promotes the uptake of glucose from the bloodstream into the cells — it “unlocks” the body’s cells that are hungry for energy, and allows them to take in glucose.\nInsulin makes blood sugar levels go down in another important way. The liver is almost constantly releasing small amounts of stored glucose into the bloodstream, one of the important ways that the body makes sure it can maintain a healthy bare minimum of blood sugar. Insulin instructs the liver to release less glucose and to store more of it. The sugar that the liver packs away to use later is called glycogen.\nThis remarkable hormone does many other things besides. Insulin also stimulates amino acid synthesis, fat synthesis, and fat storage and storage. The beta cells work with remarkable speed, regularly secreting insulin within minutes of consuming a meal.\nGlucagon has roughly the opposite effect on the liver. This hormone primarily binds to receptors on liver cells and stimulates the release of glucose from stored glycogen. Glucagon causes blood sugar to go up by instructing the liver to dump more of this extra sugar into the bloodstream.\nThe pancreas typically secretes glucagon when blood glucose levels are low. Glucagon rescue medication for severe hypoglycemia works by sending a large amount of synthetic glucagon into the body, causing the liver to release large amounts of stored sugar.\nThus, insulin and glucagon have largely opposing functions. When blood glucose levels are high, insulin signals for the intracellular storage of dietary nutrients, whereas when blood glucose levels are low, glucagon signals for their breakdown.\nOther Hormones Produced by the Islets\n- The beta cells also secrete amylin, which is a hormone that slows the passage of food through the gastrointestinal tract, represses glucagon activity, and increases satiety.\n- The PP cells produce pancreatic polypeptide, which regulates digestion and satiety.\n- The delta cells produce somatostatin, which acts as a negative regulator of digestion.\n- The epsilon cells produce ghrelin, which produces complex regulatory effects.\nAll of these cells communicate with each other, in a series of highly complex and carefully controlled interactions, to keep tight blood sugar control and achieve blood glucose homeostasis.\nThe pancreas is also constantly interacting with “the brain, liver, gut, and adipose and muscle tissue in a highly sophisticated network via various hormones, neurotransmitters, and cytokines.” In particular, the central nervous system plays an important role in promoting or repressing the secretion of specific pancreatic hormones.\nThe Pancreas and Diabetes\nIn diabetes, of course, this finely-tuned system of glucose metabolism is thrown out of whack.\nWe usually focus on beta cells and insulin here in the diabetes world. Type 1 diabetes is defined by the total or near-total loss of beta cell function, creating a lifelong dependency on exogenous insulin injections. In type 2 diabetes, the beta cells are overstressed — they create too much insulin — to the point where they can begin to burn out.\nBut diabetes also brings dysfunction to many of the pancreas’ other functions.\n- The two major forms of diabetes, for example, have effects on amylin that mirror their effects on insulin. Amylin is mostly absent in type 1, and dysfunctional in type 2. Amylin helps to control the speed of digestion, hunger, and post-prandial response. We can live without perfect amylin metabolism (or without any amylin at all), but amylin deficiency is just another way that diabetes messes with our systems.\n- People with diabetes also have less ghrelin than their non-diabetic peers. Ghrelin, like amylin, helps to regulate appetite, and low levels are associated with higher rates of hypertension and obesity.\n- Both type 1 and type 2 diabetes often results in mild forms of exocrine pancreatic insufficiency (EPI). In EPI, the pancreas either cannot produce enough digestive enzymes, or the enzymes are no longer effective. EPI can result in indigestion, constipation, and similar types of gastrointestinal distress.\n- Diabetes may also result in an excess of glucagon, driving blood sugar levels even higher in both type 1 and type 2 diabetes.\nThe pancreas does a lot more than just produce insulin. This small but vital organ produces and releases a variety of digestive enzymes and regulatory hormones that are all key players in glucose metabolism regulation. By leveraging effective organ-to-organ and cell-to-cell communication, the healthy pancreas can perform a diversity of vital metabolic functions. Unfortunately, many of these complex functions cease to work properly in diabetes, resulting in a variety of major and minor metabolic issues.\nBabic T and Travagli A; “Neural Control of the Pancreas” (2016) Pancreapedia: Exocrine Pancreas Knowledge Base 1-15 DOI: 10.3998/panc.2016.27 Accessed: 7/18/2018 https://www.pancreapedia.org/reviews/neural-control-of-pancreas\nBegg DP and Woods SC; “Interactions between the central nervous system and pancreatic islet secretions: a historical perspective” (2013) Advances in Physiology Education 37(1): 53-60. Accessed 7/18/2018 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3776474/\nBroglio F, Cottero C, Benso A, Prodam F, Volante M, Destefanis S, Gauna C, Muccioli G, Papotti M, van der Lely AJ, Ghigo E; “Ghrelin and the endocrine pancreas” (2003) Endocrine 22(1): 19-24. Accessed 7/19/2018 https://link.springer.com/article/10.1385%2FENDO%3A22%3A1%3A19\nJain R and Lammert E; “Cell-cell interactions in the endocrine pancreas” (2009) Diabetes, Obesity and Metabolism 11(s4): 159-167. Accessed 7/18/2018 https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1463-1326.2009.01102.\nKrejs GJ; “Physiological role of somatostatin in the digestive tract: gastric acid secretion, intestinal absorption, and motility” (1986) Scandinavian Journal of Gastroenterology(119): 47-53. Accessed 7/19/2018 https://www.ncbi.nlm.nih.gov/pubmed/2876506\nRoder PV, Wu B, Liu Y, Han W; “Pancreatic regulation of glucose homeostasis” (2016) Experimental & Molecular Medicine 48(3): e219. Accessed 7/18/2018 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4892884/\nRussell PJ, Wolfe SL, Hertz PE, Starr C, McMillan B; Biology: The Dynamic Science, First Edition (2008) Tomson Brooks /Cole p. 917, 919, 927, 1027, 1029.\nSchmitz O, Brock B, and Rungby J; “Amylin Agonists: A Novel Approach in the Treatment of Diabetes” (2004) Diabetes 53(s3): S233-238. Accessed 7/19/2018 http://diabetes.diabetesjournals.org/content/53/suppl_3/S233\nWilcox G; “Insulin and Insulin Resistance” (2005) The Clinical Biochemist Reviews 26(2): 29-39. Accessed 7/18/2018 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1204764/\nRead more about pancreas.']	['<urn:uuid:b1cf2a99-bd62-487e-8932-1346a3f2c4dc>', '<urn:uuid:a7600819-bc67-4879-97f9-a1377aa5af37>']	factoid	with-premise	short-search-query	distant-from-document	comparison	expert	2025-05-12T20:58:04.490895	6	87	1866
74	How do tannins and cold-water Asparagopsis compare as methane inhibitors?	Tannins and cold-water Asparagopsis work differently to reduce methane emissions from cattle. Tannins directly suppress methane-making micro-organisms in cattle's digestive system. Cold-water Asparagopsis, which grows naturally in New Zealand and southern Australia, works by interfering with the final stage of methane production in a cow's stomach where short chain fatty acids are converted to gas, achieving a 90% reduction in methane when fed at 25-50 grams per cow per day.	['The Carbon Farming Initiative is about to provide incentives for dairy farmers to reduce emissions from their milking cows. Emissions will be reduced by changing the cows’ diet.\nThe proposed new methodology alters the activity of the gut’s methane-producing microorganisms. This reduces the amount of methane “belched” by cattle from their digestive system.\nThe organisms in the fore-stomach of sheep and cattle produce methane – called enteric methane – from the hydrogen and carbon dioxide that result from microbial digestion of cows’ food.\nStrategies for reducing enteric methane include breeding for lower methane-producing animals, microbial interventions, and nutrition and animal management. Most of these are currently being researched through the National Livestock Methane Program. Over the next few years we may see a number of these strategies developed into new Carbon Farming Initiative methods to reduce enteric methane.\nThere are five possible ways oil supplementation reduces methane:\n- reducing fibre digestion\n- lowering total feed intake (this is obviously undesirable, and only occurs when total dietary fat exceeds 6% to 7% of total intake)\n- suppressing the micro-organisms that make methane and suppressing rumen protozoa, on which those organisms depend.\nTannins, on the other hand, reduce enteric methane by directly suppressing methane-making micro-organisms.\nA key part of the research was demonstrating that different dietary oils – all by-products from other agricultural processes – could reduce enteric methane equally.\nThe by-products studied included whole cotton seed, cold-pressed canola meal, hominy meal and brewers grains. All of these can theoretically be sourced as by-products of biofuel production. Additionally, if these by-products are adding energy to the diet, they will also increase milk production. The profitability of this milk production increase depends on the comparative price of conventional grain feeding.\nThe research team then asked the obvious question – if tannin and oils were added to the diet, would this have an additive effect on inhibiting methane production? However, when added to the diet as pure extracts, tannins and oils appeared to neutralise each other (perhaps with the tannin binding the oils making the combination ineffective). The focus shifted to potential feed additives that contain both tannin and oil, naturally occurring in one product. The obvious choice was grape marc, a by-product of wine making with a high concentration of both fat and tannins. Feeding grape marc to dairy cattle reduced methane by up to 20%.\nCarbon Farming Initiative offset methodologies can only be built on peer reviewed research, so the team combined their published data with a global review of literature and demonstrated that for every 1% extra oil added to the diet of livestock, enteric methane can be reduced by 3.5%. The upper limit for dietary fats and oils in feed is 7% and the natural oils in grass range from 1% (summer) to 5% (spring), so there is a window of 90 days in the summer when natural grass oils are low enough that supplementary feeding can be introduced to reduce enteric methane production.\nThis evidence was considered sufficiently robust to underpin the first CFI offset methodology for livestock.\nThis method has now been released for public consultation. When it’s approved - together with those already approved for destruction of methane from dairy manure ponds and removal of carbon dioxide through environmental plantings - it will give dairy farmers options to take part in the Carbon Farming Initiative. As the income from the CFI for each of these methods individually is relatively modest, these methods are most likely to be adopted where they bring productivity gains or align with broader farming objectives.\nThis work is the culmination of over six years research by the Victorian Department of Primary Industries (DPI) and the University of Melbourne. The research, co-funded by the DAFF Climate Change Research Program, Meat and Livestock Australia and Dairy Australia, focused on providing dairy farmers with an option for reducing greenhouse gas emissions from milking cows through feeding dietary additives.', 'Sustainable agriculture company CH4 plans to build its first two facilities in South Australia with further ‘eco-parks’ planned in the state and in New Zealand.\nThe first site is likely to be on the Yorke Peninsula of South Australia as part of a partnership with the First Nations Narungga people with the second site in Port Lincoln on the Eyre Peninsula of the state.\nEach farm would be capable of producing about 400 tonnes of dry weight milled seaweed a year – enough to feed up to 20,000 cows.\nCattle are a major contributor to greenhouse gas emissions with every one of the 1.5 billion cows on the planet producing about 100kg of methane a year.\nThe farms are based on research by Australia’s CSIRO, which found that the red seaweed Asparagopsis mixed with regular cattle feed at a rate of 100 grams per cow per day reduced methane production by 90 per cent.\nCH4 Global has purchased a licence from patent owners CSIRO, Meat & Livestock Australia and James Cook University and gained regulatory approval for the material to be allowed to sell it in Australia.\nThe cold water Asparagopsis species occurs naturally and grows well in New Zealand and also in southern Australia.\nCH4 Global CEO Dr Steve Meller said the most recent data published by CSIRO in February and by UC Davis in California in September both showed that between 25 and 50 grams per cow per day was actually an effective dose that resulted in at least 90 per cent reductions in methane.\n“We are optimising our processes, optimising the hatchery work we are doing in South Australia and New Zealand, we are optimising the in-water architecture that’s going to be used and we are doing that in Port Lincoln at the moment and we are optimising the freeze-drying processing work,” he said.\n“By the end of March we will know enough about each of those three where we will then integrate them into the design of the first facility and we will have already started a lot of the approval processes in the relevant jurisdictions and already have the engineering designs to fit those pieces in to be able to then construct.”\nCH4 has acquired 839 ha of potential aquaculture space across three sites in the Spencer Gulf of South Australia: around Port Lincoln in Boston Bay, Louth Bay and Tumby Bay.\nIt also has a partnership with Narungga Nation at Point Pearce on Yorke Peninsula, where the first farm is likely to be built. It is also in conversation with an oyster farm on Kangaroo Island about growing the seaweed there.\n“We’ve defined what we call an eco-park and the minimum size that makes sense is 20ha of aquaculture space in the water and two hectares on land containing a hatchery to grow the seedlings and a facility for processing,” Meller said.\n“We can build five of them together and farm 100ha, we can build one 20ha on its own and the idea is to have the first of those up and running and producing material by the end of next calendar year to validate the commercial objectives of financial viability.\n“If they make money as we project they will, there will be opportunity to build lots of them because they each will make an inherent profit.\n“They also do things like reduce ocean acidification, reduce excess nutrients in the water from agricultural runoff or from waste from an existing tuna or kingfish farm then, of course, the use of it reduces methane in cows and sheep.”\nEach site is estimated to sustain 20-25 jobs including growing and harvesting the seaweed on the water and the staff in the production facility.\nIt takes about 45-60 days for a seaweed seedling from the hatchery to grow into a matured plant ready for harvest and processing.\nNow based in California, Meller grew up in South Australia and completed a PhD in Neuroscience at the University of Adelaide. He moved to the United States more than 20 years ago and forged a successful career in disruptive innovations with consumer goods giant Proctor and Gamble.\nHe formed CH4 global in late 2018 after speaking at a World Energy Council meeting in NZ.\nThe company has two wholly-owned subsidiaries – CH4 South Australia and CH4 Aotearoa.\nIt will focus on California, NZ and Australia as its first three markets with the 2.7 million dairy cows and those in beef feedlots in Australia targeted first.\nMeller will be among a group of agribusiness leaders speaking at an SA webinar next week organised by Adelaide-based DMAW Lawyers.\nHe said CH4 had three goals: to act with urgency on climate change; work with indigenous peoples wherever we operate; and, put money in farmers’ pockets.\nMeller would not say exactly how much the milled seaweed supplement would cost but said it would lead to a 12-15 per cent reduction in the total food needed by cows and would also open up opportunities for niche dairy products.\nHe said Asparagopsis worked by interfering with the last stage of methane production in a cow’s stomach where short chain fatty acids were converted to the gas before being belched out.\n“So if the methane is blocked you’ve got 12-15 per cent more calories retained in the cow for the same amount of food.”\nMeller said consumers would be prepared to pay a premium for low-carbon milk.\n“That’s why people drive electric vehicles and use LED light bulbs – they are paying for the carbon that’s reduced,” he said.\n“Brands beyond milk in products such as yoghurt, cheeses, chocolate and ice cream have had discussions with us about accessing low-carbon or zero impact milk.\n“If a farmer makes 10 per cent more on the milk, it pays for the product and puts 25-50 per cent more money into their pocket.”\nOther presenters at the DMAW Lawyers agribusiness webinar on December 8 include Soil and Land Co-director Edward Scott speaking on ‘agricultural technologies and how they can bet be used for soil carbon and nitrogen management’, and Field Systems director Michael Eyres speaking on ‘soil carbon as the farmer’s universal agricultural performance catalyst’.\n“Every single thing that anybody can work on that helps reduce carbon impact in the atmosphere, they should be doing as much as they can,” Meller said.\n“What you will do on soil impact, reducing CO2 impact with electric vehicles, solar farms, all of those should happen but they have different timeframes and impacts.\n“What I’m saying is this one (CH4) has the biggest impact bang for buck in the next 10 years.”Jump to next article']	['<urn:uuid:3d1bacbd-9519-4942-8dcd-0ad7dce169ee>', '<urn:uuid:4db0a3d3-8878-4f44-9962-ca1a2f847b35>']	open-ended	direct	concise-and-natural	similar-to-document	comparison	expert	2025-05-12T20:58:04.490895	10	70	1753
75	What happened to the revolutionary government that briefly took control of Paris in 1871?	The Paris Commune ended in May 1871, when over fifteen thousand people were killed by the French army in what came to be known as la semaine sanglante ('the Bloody Week'). The revolutionary government had held power for sixty-three days following France's defeat in the Franco-Prussian War.	['La Commune (Paris, 1871) (M+ Screenings: Beneath the Pavement)\nMovie Name: La Commune (Paris, 1871)\nLanguage: French(English subtitle)\nDirector: Peter Watkins\nWith a twenty-five-minute intermission\nStory: The result of over sixteen months of intense research and preparation, and with the participation of over two hundred non-professional actors, Peter Watkins’s extraordinary La Commune (Paris, 1871) intentionally defies conventional definition. This almost six-hour film, shot chronologically over the course of two weeks, presents the experience of the Paris Commune of 1871, in which a radical, revolutionary government made up of socialist collectives briefly held power for sixty-three days in Paris following France’s defeat in the Franco-Prussian War. Although the Paris Commune ended in May 1871, when over fifteen thousand people were killed by the French army in what came to be known as la semaine sanglante (‘the Bloody Week’), it has since had a profound influence on leftist revolutions around the world.\nWatkins’s film follows the rise and fall of the Commune in a theatricalised space, where events unfold and interviews and general coverage are conducted by television crews from opposing camps: Télévision Versaillaise, representing the French government; and Télévision Communale, representing the Paris Commune. In long, unedited sequences, the camera moves fluidly within the revolutionary space, as people tell their stories directly to the camera; the television crews provide reportage in the style of the evening news; and intertitles and period photographs are spread throughout to explain intervening events. While they give context and cause for the events unfolding on- and off-screen, these elements can also be considered as a critique of the formal strictures of mass media. They offer possible avenues in which the act of telling stories on-screen can be expanded in a way that welcomes, and even urges, active viewer interpretation.\nRather than merely representing history, La Commune transcends conventional cinematic practice in order to uncover and activate its participatory potential. During extensive discussions before the shoot, Watkins worked with participants—who came from different socio-economic, professional, and regional backgrounds, and held different political views—to elicit their reflections on the 1871 events. This freely informed the dialogue the participants would then prepare for their own roles, regardless of whether they played a Versaillais or a Communard. Integrating multiple voices, humanising opposing views, and expanding media practices are all crucial aspects of La Commune and, as with the other works in this M+ Screenings programme, they reimagine moving image as a crucial, engaged, and radicalising thrust of history.\nPeter Watkins (British, born 1935) is a pioneer of the docudrama form. His films wrestle with topics that spur intense social debate and often reveal a critical perspective on the hegemony of mass audiovisual media practices. He is known for works such as The War Game (1966), a film banned from television for over twenty years that won an Academy Award for Best Documentary Feature; Edvard Munch (1973), a film which Ingmar Bergman called a work by ‘a genius’; and the fourteen-and-a-half hour film The Journey (1987), a survey of life around the world in the nuclear age.']	['<urn:uuid:cabdced1-8762-4a12-b73b-17bd481b1bcf>']	factoid	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:58:04.490895	14	47	506
76	difference between ad network and exchange	Ad Networks collect digital ad inventory from publishers and resell them to advertisers after sorting and filtering, while an ad exchange is an open pool of impressions where buyers can see exactly how much each impression costs, with no intermediary players.	['What is an ad exchange?\nAn ad exchange is a digital marketplace that enables advertisers and publishers to buy and sell advertising space, often through real-time auctions. They’re most often used to sell display, video, and mobile ad inventory.\nAd Exchanges are digital marketplaces where publishers and advertisers come together to trade digital ad inventory such as display, native, video, mobile and in-app. Buying and selling happen in real-time auctions, empowered by RTB (real-time bidding) technology.\nThe Ad Exchange is an auction mediation mechanism that does not serve either the buyer or the seller side; it is an autonomous platform that facilitates programmatic ad buying.\nHow does Ad Exchange Work?\nAn ad network connects advertisers to publishers that offer ad inventory. The key function of an ad network is aggregating ad supply from publishers, and matching it with advertisers’ demand.\nAlthough the ad exchange and ad network seem to be performing the same role, they’re not. Ad Networks collect digital ad inventory from a list of publisher sites or buy ad impressions in bulk from the ad exchange, sort them through, and then resell them to advertisers. Since advertisers do not have enough time or resources for filtering available inventory, Ad Networks simply do it for them.\nThey group inventory according to specific parameters such as pricing, scale, or audience segments (demographics, geography, language, interests, consumer behavior, etc.). Some Ad Networks are focused on coverage and quantity, while others specialize in the quality of ad slots they offer.\nSo if an ad exchange is an open pool of impressions, an Ad network is a closed group of privately traded ads. Ad exchange, in that sense, offers more transparency to buyers, because they can see exactly how much each impression is being sold for, with no intermediary players.\nWho buys from ad exchanges?\nVirtually anyone can buy from an ad exchange provided the ad exchange allows it. Advertisers and agencies typically use demand-side platforms or their own bidding technologies to do so, but ad networks and other entities also buy ads from exchanges.\nGreat. But how do they do that?\nBasically, an ad exchange is just a big pool of ad impressions. Publishers tip their ad impressions into the pool hoping someone will buy them. Buyers then pick which impressions they wish to purchase using technologies like demand-side platforms. Those decisions are often made in real-time based on information such as the previous behavior of the user an ad is being served to, time of day, device type, ad position, and more.\nTypes of exchanges\n- Open ad exchange / Public Marketplace / Open Auction\nThe above describes an open digital marketplace with an extensive inventory of publishers available for all advertisers. Despite offering a broad list of publishers, advertisers using an open ad exchange don’t have detailed information about publishers, as is the case with a private marketplace.\nAdvertisers looking for wider publicity choose an open ad exchange.\nHowever, with tens of billions of impressions flowing through open Ad Exchanges every day, there is a growing and grounded concern among advertisers and publishers regarding digital ad fraud. For this reason, private marketplaces are becoming more popular, as they are considered safer and more transparent.\n- Private ad exchange / Private Marketplace (PMP)\nA PMP is a closed, “premium” platform that enables the publisher to control which advertisers can bid, at what price, and under which conditions. Each private ad exchange is run by an individual publisher that personally invites selected advertisers to the PMP.\nThe publisher can also block Ad Networks and other third-party members from giving access to its pool of impressions. A private ad exchange allows brands and publishers to set up direct relationships with advertisers and agencies, meaning negotiations might be more time-consuming in comparison to an open ad exchange.\n- Preferred deal\nA preferred deal is an option for a publisher to sell digital ad inventory at a negotiated fixed price for preferred advertisers. Preferred deals give the publisher a stable revenue stream through a controlled transaction system, while advertisers benefit from stable CPM prices and having access to an exclusive inventory.\nWhat’s a private exchange? It sounds sleazy.\nPrivate exchanges are used by publishers to more carefully control who can buy their inventory, and at what price. Instead of throwing its ad impressions into an “open” exchange and letting anyone buy them, a publisher might instead wish to offer them to a handful of its favorite advertiser clients, or an agency it has a close relationship with. It might also wish to cut off access to networks and other third parties that could sell those ad impressions.\nA few key ad exchanges:\n- DoubleClick (Google)\n- Rubicon Project Exchange (Magnite)\n- Right Media Exchange (Yahoo)\n- Smaato – mobile focused\n- MoPub (Twitter) – mobile focused']	['<urn:uuid:fda4a140-1db7-437e-936a-b6eda54a7903>']	factoid	with-premise	short-search-query	distant-from-document	single-doc	novice	2025-05-12T20:58:04.490895	6	41	801
77	My car gauges take too long to show readings. Why is this happening?	This could be due to using a linear regulator instead of the original limiter. Unlike the original limiter which provides +12V for 3-4 seconds at initial power up to get gauges reading quickly, a linear regulator can take up to 1 minute for the gauges to reach their final reading after turning on the key.	"['RTE Limiter Faq\nLimiter trouble shooting\n- ) If the new solid state limiter doesn\'t function (LED doesn\'t blink), remove the limiter to gage wire from one gage at a time checking to see if the limiter\nbegins operation. The limiter will detect a short circuit and turn itself off until the short is removed. Also, if the limiter has too much or not enough voltage, then it will turn itself off.\n- ) Note that some mustangs had build in resistance wire feeding the limiter. This wire must be removed or shorted around in order to make our solid state limiter work properly. The reason is that our limiter will see the voltage drop in the resistance wire, and it will think that the voltage is below 9V, so it will turn itself off to protect the limiter and the gauges.\nFixing dash clusters that have internal limiters\nPDF copy of this same info: media:InternalLimiter.pdf\nSome of the cars made my Mopar did not use external limiters. Instead these cars built the limiter into one of the gauges, usually the fuel gauge. Cars in this category include the 66/67 charger, 68-74 ABody cars with Rallye dash, and some older imperials.\nWhen these internal limiters go bad, you have two choices, either replace the whole gauge (expensive and hard to find!), or disable the limiter built into the gauge and wire in an external limiter. If you decide to wire in an external limiter, then you should verify that the gauge still functions as a gauge, i.e. that it hasn\'t been burned out when the limiter failed. Another thing to consider is that it might be good insurance to just disable the internal limiter and replace it with an external limiter before the internal limiter fails. Here are some pictures showing how RTE disabled the internal limiter in an older Imperial dash cluster, and wired in an external solid state limiter. Real Time Engineering can do this for you for $50 labor plus the cost of the solid state limiter. The red arrow shows how we bent the internal limiter bi-metallic strip to disable the internal limiter.\nImperial internal limiter fix\nA-body rallye dash limiter fix\n- Step 1: Take the fuel gauge out of the cluster, and gently take the face off.\n- Step 2: Bend the limiter points out of the way, so that they no longer contact. Make sure that the points don\'t touch anything.\n- Step 3: Put the Fuel gauge face back on.\n- Step 4: Mount the IVR3 (Ebody) style limiter using one of the circuit board ground mounting points.\n- Step 5: Run a wire from the 12V stud going into the fuel gauge to the IGN terminal on the new solid state limiter. Use a ring terminal on the fuel gauge side, and a male spade terminal on the limiter side.\n- Step 6: Run a wire from the old fuel gauge limiter output stud to the solid state limiter output terminal. Use a ring terminal on the fuel gauge side, and a female spade terminal on the limiter side.\n- Step 7: Test the setup by turning the key on in the car. The fuel gauge, oil gauge, and temperature gauge should operate properly, and the red LED on the new solid state limiter should blink after the 3-6 second warm up time.\nMore information about dash limiters\nOn the back of most mopar dashes (with some notable exceptions) there is a 1"" X 2"" metal can, with 3 terminals. This device is known as a limiter. Its function is to regulate the voltage that is being applied to the fuel/oil/temperature gauges.\nSome mopar dashes don\'t have a visible limiter device. Instead they have the limiter built into the fuel gauge. These special dashes can be identified by looking at the fuel gauge, and if it has 3 terminals then it has the limiter built in. A-Body Rallye dashes and 66/67 Chargers are two examples of dashes that have the limiter built into the fuel gauge.\nThe 70-71 parts book does list 4 different types of limiters, of which we have seen only two types. The E-body limiter has part number 2209216, and the most common push in type has part number 2258413.\nThe push in type is intended to be pressed into some mounting clips on the backs of the circuit boards. This type of limiter has three male spade terminals protruding from the face of the limiter. One terminal is spot welded to the case for ground. The other two terminals are for +12V in, and limited voltage out. There is usually a capacitor connected to the +12V spade when it is pressed in palce. This type of limiter is always found on dashes that have circuit boards. This type of limiter was used on a lot more different types of mopars than the other type, and this type of limiter is still available at your auto parts store.\nThe E-body limiter has a mounting tab spot welded to the back of the limiter. A small 1/4"" head bolt is used to fasten this type of limiter to the metal dash frame, and this is how this type gets it ground. The +12V input terminal on this limiter is a dual male spade terminal. It has two terminals so that one can be +12V input, and the other is intended to be connected to the capacitor (sometimes called a condensor). The capacitor does not have any effect on the operation of the limiter, but is intended to stop AM radio interference. The 2nd terminal on this type of limiter is a female spade terminal, and this is the limiter output to the gauges. This type of limiter was used on just a few different types of cars, includingthe 70-74 E-bodies. This type of limiter is no longer available in the aftermarket. Note that there is a limiter that appears to be the same that was used for some ford dashes, but this limiter does not put out the correct voltage and will make your gauges read low if you try to use it.\nIn order to understand the limiter and why it is necessary to have one on your dash, it is first necessary to understand how the fuel/oil/temp gauges work. The typical Fuel/Oil/Temp gauge (henceforth referrred to as FOT) has inside it a bi-metallic strip, similar to the bi-metallic strip inside your non-electronic wall thermostat. There is some insualed nichrome wire wrapped around the bi-metallic strip. When current is run through the nichrome wire, it will heat up the bi-metallic strip, causing it to bend. The bending is caused because one of the types of metal expands faster than the other. See this URL for more info on how bi-metallic strips work:\nOne important thing to know about bi-metallic strips is that the amount they bend is proportional to the amount of heat that is put into the strip. Therefore, if you want to have an accurate gauge, you must have accurately regulated heat. The heat is proportional to the current through the wire, and the current through the wire is dependant on the voltage being applied to the wire. Therefore if the voltage being supplied to the gauges were to vary, then the gauges reading would vary. It turns out that the voltage on a cars battery is not very well regulated, and it can change as much as 2-3 volts under different conditions (for example, just turning on the headlights and sitting at idle will have a lower voltage condition). Therefore the limiters job is to regulate the voltage being applied to the gauges, so they will be accurate and not vary.\nThe dash limiter is not temperature sensitive. In other words, if the external temperature varies between its normal extremes of +150F and 0F, the limiter will still put out its voltage with little or no variation. This temperature compensation is accomplished by the way the bi-metallic strip is constructed inside the limiter. The FOT gauges also have this temperature compensation inside them and so are not sensitive to the external temperature.\nThe way the dash limiter works is as follows. When power is first turned on (i.e. you turn on the key), then the points are closed because the bi-metallic strip is not yet hot and the limiter is putting out +12V. The nichrome wire inside the limiter starts to heat up, which in turn starts to heat up the bi-metallics strip. After about 3-4 seconds, the strip will get hot enough to cause it to bend up and open the set of points inside the limiter. When the points open. When the points open, then the nichrome wire stops heating the strip, and the limiter stops putting out +12V. The amount of time that the limiter stays open is proportional to the amount of heat that was put into the strip, so when the voltage of your car battery is higher, then the limiter stays open longer, thus regulating the average RMS (Root-Mean-Square) voltage coming out of the limiter.\nThere are several different failure modes that a limiter can have:\n- If the limiter loses its ground (from corrosion for example) then there will be no current flowing in the nichrome wire, and the limiter will output +12V all tht time. The FOT gauges cannot tolerate this condition for more than about 20 seconds, and will burn out after about 20 seconds.\n- If the nichrome wire burns in half or breaks due to mechanical vibrations, then the points will no longer open and the gauges will fry after 20 seconds.\n- If the points weld themselves shut, then the points will no longer open and the gauges will fry after 20 seconds.\n- If the output of the limiter is shorted to ground, then either the fuse to the instrument panel will open, or we have seen some limiters that had the bi-metallic strip burned in half. In this case the gauges won\'t be fried, but they will stop working until the limiter is replaced.\nCurrently there are some people that are replacing the insides of the limiter bi-metallic strip with a linear regulator. This does work, but it is not a perfect solution. The following problems exist with this solution:\n- The linear regulator will dissipate a lot of heat because it is constantly dropping a lot of voltage across it in order to create 5V all the time. In fact a linear regulator will generate a lot more heat then the original limiter. It is necessary to thermally bond the linear regulator to the limiter case to keep the regulator from getting too hot under normal operation. For reference, it takes about 300mA per gauge to make each gauge read full scale. Therefore the amount of heat generated by the linear regulator is P=IV=300mA*(12V-5V)*3=6.3Watts. This is a lot of heat.\n- The linear regulator requires a good ground just like the original limiter. If the ground is lost, then the linear regulator will put out +12V all the time, frying the gauges.\n- The linear regulator will not properly emulate the original limiter in the sense that the original limiter puts out +12V for 3-4 seconds at initial power up in order to get the gauges up to their correct reading quickly. It can take up to 1 minute for the gauges to reach their final reading when using a linear regulator after turning on the key.\n- The original mechanical limiter pulses on and off, and this has the effect of overcoming friction inside the FOT gauges. This makes the gauges more responsive to changes. Since the linear regulator doesn\'t pulse, it doesn\'t have this effect.\n- Rarely some linear regulators can fail when their output is shorted to ground. Usually when they fail, they will short the input to the output, causing +12V on the output of the regulator.\n- On a positive note, most linear regulators have heat sensors inside them and will shutdown when they get too hot (caused by the output being shorted to ground).\nReal Time Engineering has a new solid state limiter that will replace the original mechanical limiter on the back of your dash. This new limiter has many advantages over the original limiter, and also has advantages over the linear regulators that hobbyist have been using as well. The biggest advantage is that our limiter doesn\'t have a mechanical set of points and a heater wire that can break and fry your gauges (which is what the original mechanical limiter had).\n- Our limiter exactly duplicates the original limiters function by slowly switching 12V on and off.\n- Our limiter has a built in polyfuse which protects the limiter and your wiring from short circuits on the output of the limtier\n- Our limiter constantly looks for short circuits on the output of the limiter and will switch itself off if a short circuit is detected. When the short is removed, then the limiter starts working again.\n- Our limiter has a built in diagnostic LED that blinks when the limiter is on, helping you see that you have power to your dash and that the limiter is operating properly.\n- Our limiter has a warm up time at turn on, same as the original mechanical limiter. This means that your gauges will come up to the proper reading very quickly.\n- Our limiter is always outputting a constant 5V average, no matter what the input voltage is (Within the range of 9V to 18V). If the input voltage goes too high or too low, then the limiter shuts off.\n- Our limiter output is constant regardless of temperature.\n- Our limiter is not sensitive to mechanical vibration.\n- If our limiter loses ground, it will shut itself off. Contrast this to the mechanical and linear limiters, which will put out 12V and fry your gauges.\n- If the alternator and/or the firewall regulator go bad and the alternator puts out too much voltage (over 19V), then our limiter will cause the instrument panel fuse to open, protecting the limiter and the gauges. Note: It is important that you have the proper size fuse in the fuse box for the instrument panel for this reason.\nAdvantages of our limiter over mechanical limiters:\n- Won\'t fry your gauges when ground is lost\n- No mechanical moving parts to wear out\n- No points to stick closed\n- No nichrome wire to break and then fry your gauges\n- Insensitive to heat and cold\n- Solid state so should last a very long time\n- Short circuit protected\n- Diagnostic LED built in\n- Uses less current than the original limiter (because there is no heater wire)\n- Original mechanical limiters tend to vibrate badly and lose regulation temporarily when going over very rough roads.\nAdvantages of our limiter over linear home made limiters:\n- Won\'t fry your gauges when ground is lost\n- Doesn\'t generate any heat (linear regulators make a lot of heat)\n- Has a warm up period when turned on to make the gauges come up fast\n- Diagnostic LED built in\n- Uses a lot less power (Linear regulators can waste up to 8W of power in this application)']"	['<urn:uuid:73b55135-aa7f-4dcd-8dc1-d5ff7edc8d5c>']	open-ended	with-premise	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:58:04.490895	13	55	2551
78	github collaboration dropbox file conflict differences	Dropbox and GitHub handle file conflicts differently in collaborative work. With Dropbox, when two users edit the same file simultaneously, it creates a conflicted copy without guidance on reconciliation, leaving users to manually figure out the changes. In contrast, GitHub's Git system provides more sophisticated conflict management - it clearly marks conflicting lines in the files with markers showing both versions, allows automatic merging of non-conflicting changes, and requires explicit conflict resolution before changes can be pushed. Git also provides detailed information about the conflicts through commands like git status and prevents pushing changes until conflicts are resolved.	"[""Continues Git for Philosophers (pt. 1)\nCollaborative Writing with Git\nCollaborative writing presents similar issues as collaborative programming: different people making changes to the same document from different locations. Sending the document back and forth is inefficient: only one person can work on it at a time, and there is a risk of changes being made in parallel which are then either overlooked or which are hard to reconcile. Collaborative editing tools (e.g., Google Docs for Word documents, shared LaTeX editors like ShareLaTeX and Overleaf help if you’re using those formats, but have their own drawbacks (e.g., they can’t be used offline). Simply keeping your shared Document in a Dropbox folder is a partial solution, but the doesn’t solve the issue of conflicting changes to your shared document. Dropbox assumes that you know what you’re doing. So if you make a change to a document on your office desktop, and then make a change to the same document on your laptop, the later change will silently overwrite the former. If for some reason you didn’t update the file on your laptop with the changes from your office desktop (say, if you were without an internet connection), these changes will be gone from the document when you save it the second time. The changes will be preserved in a previous version of the document, since Dropbox keeps every intermediate version — but it might take you a while to notice that your change got lost, and it might take you a while to find the most recent version of the file on Dropbox that still has it.\nDropbox is a bit more careful when two different users make changes to the same document in a shared Dropbox folder. If your collaborator has edited the file while you were away, you will see the changes automatically. But if you’ve had your file open in an editor while your collaborator has made changes — perhaps even with your laptop asleep! — Dropbox will realize that there may be a conflict between your version of the document and your collaborator’s. But it won’t know what to do. Rather than overwrite one of your changes, Dropbox will make a copy of the file you’re working on simultaneously, and leave you to figure out which version is more up-to-date and how to reconcile the two copies into one. So if author A and author B are both working on a document, author A adds a sentence to the introduction and at the same time author B adds a sentence to the conclusion, your shared Dropbox folder will suddenly contain a second version of the document,\ndocument (author B's conflicted copy).doc, say. It will contain the additional sentence in the conclusion, but not author A’s additional sentence in the introduction, and\ndocument.doc will contain author A’s sentence but not author B’s. Dropbox will leave it to you to figure out who has made what change where and how to reconcile them and integrate the two documents into a single one. This is annoying and complicated, especially if the conflict goes unnoticed for a while.\nUsing Git helps you avoid these problems to the extent it is possible to avoid them.\nWhen you have set up a repository for your writing project (say, containing a LaTeX document plus a BibTeX file for references), you can edit the files, commit your changes to generate a new revision, and periodically push your revisions to the remote repository on GitHub or GitLab. If you are working on the project with someone else, you can give them access to the repository as well. If they have push access, they can send their own revisions to the shared repository just like you do.\nGit keeps track of the state of your own local repository and the remote on GitHub/GitLab. The command\nwill prompt Git to display the status of your repository: which branch you are on (typically, this is the master branch, but more about branches later), whether your local version of the repository is up to date with the remote or not, which files have changes that are waiting to be committed, and which files are untracked. If you have commits that you have not pushed to the remote yet, Git will report something like “Your branch is ahead of ‘origin/master’ by 1 commit.” The remote repository is usually called origin, and the remote branch that corresponds to your local master branch is then called origin/master. It might happen that your branch is behind the remote: if your collaborator has pushed changes to the shared remote, there will be commits on the remote that you don’t yet have in your local repository. Before you can push your changes, you will have to incorporate your collaborators’ changes into your own local version of your paper.\nIf your document is under Git control, you have to pull changes from the shared repository before you see what your co-author has done. By the same token, you and they have to remember to push new commits to the repository, or there won’t be any changes to pull. In the crucial case where you have both made changes at the same time, however, Git will handle the discrepancies gracefully.\nIn the best case scenario, you and your co-author have edited the same file, but you haven’t edited the same part of the file: say, they added a footnote to the introduction, you have cleaned up a passage in a middle section. If they have committed their changes and pushed to the shared remote, Git won’t let you push your changes. You’ll get an error message like\n! [rejected] master -> master (fetch first)\nIf your changes do not conflict (were not made to the same line of text), Git can fix this automatically. Just say\nGit will then download (“fetch”) the changes from the remote and automatically merge them with your version of the repository, creating a new revision which includes both your changes and the older changes by your co-author. Your repository is now ahead of the remote by one commit (the act of merging your co-authors changes with your own created a new revision), and you can push the combined changes to the remote.\nIf you did happen to both make changes that cannot be merged automatically, Git will alert you to this fact:\nCONFLICT (content): Merge conflict in <filenames> Automatic merge failed; fix conflicts and then commit the result.\nInstead of letting you fend for yourself in figuring out what has changed and where, Git will tell you exactly what you have to fix. The files with editing conflicts will now contain the conflicting lines, indicating your and their changes, e.g.:\n<<<<<<< HEAD what you wrote ======= what your co-author wrote >>>>>>> hash code of your co-author's commit\nin the relevant place in the document. Fix up just that part of the document. Then say\ngit commit -a. Your local repository now contains a conflict-free version of both your changes, which is ahead of the shared repository by one commit, and Git will again let you push to the remote. When your co-author returns to work and says\ngit pull, they will have the merged, clean version of the document.\nNote that your intervention is only required if both you and your co-author have made changes to the very same line of text, otherwise Git will merge the changes automatically. This includes the case where one of you adds a paragraph and the other one cuts text somewhere else in the file: after\ngit pull the file will have the new paragraph but the text you deleted will still be gone."", ""A Primer On Contributing To Projects With Git\nThere isn’t any shortage of tutorials on this subject, but I haven’t seen any that attempt to guide a person that has zero experience with any of it. So, the goal of this article is to give a fresh “newbie” all of the information they need to collaborate on projects that use the Git SCM. This will not be a complete, or even a full introductory, guide to git; you should read other tutorials, and the reference docs for that.\nWe will focus on using the command line interface. Thus, to start, we need to setup the environment.\nI am not a Windows person. My day job is a Linux administrator and my full-time desktop environment is Appple macOS. Given that, the easiest environment I have found for Windows is to:\n- Install msysgit\n- Install ConEmu\n- Configure the default “task” for ConEmu to the one that uses the Bash shell provided by msysgit\nThis will vary based on the distribution you use. The short of it is that you need to install their package that provides git; typically, the name of the package is simply “git”.\nIf you are using Void Linux (my preference), then I recommend installing both the “git” and “git-perl” packages.\nI prefer using the “git” package from MacPorts.\nBut the easiest way to get started is to simply open a Terminal.app session\n$ git $ # you will be prompted to install the necessary components\nSpeaking of Terminal.app, I recommend switching to iTerm2. It’s just better.\nWith git installed, you should now configure it to know some details about you. These details will be used to identify you on the changes that you make within a project:\n$ git config --global user.name 'FirstName Surname' $ git config --global user.email 'firstname.lastname@example.org'\nWe will discuss central repositories shortly, but regardless of how the project you wish to contribute to chooses to centralize, you will need to authenticate yourself when synchronizing your contributions. You have two options:\n- Communicate with the central repositories over HTTPS\n- Communicate with the central repositories over SSH\nIn case #1, you will be prompted for your user credentials each time you work with the remote system. You can minimize this by using a “credential helper.” A nice overview of getting such a helper setup is available at http://stackoverflow.com/a/5343146/7979.\nAs for case #2, you will need to create an SSH key pair for yourself and configure the remote system to recognize it. Given the complexity of this method, we will assume the first method is being used. If you want to learn about the SSH method, then I am sure whichever central system your project uses will have instructions that will help you out.\nNow that we have a working git environment, we can learn about how to actually use it to collaborate on a project.\nWhether you are working solely within an institution or you are participating in an open source project, you will be working with a central repository. There are many ways a repository can be hosted centrally, but the most common, and the ones we will assume in this article, are provided by the following services:\nAll three offer some form of on-premises solution, public hosted repositories, and private hosted repositories. Regardless of the service provider and its location, you will need an account with the service. Thus, the easiest step is the first – create an account.\nFor the rest of this article we will assume you created a GitHub account at github.com.\nGit And GitHub\ngit is a distributed source code management tool. This means that git is intended to be used locally, without a central server. But all of the sites outlined in account setup are centralized server. So we need to think of sites like GitHub as a system that many people have “local” accounts on where they store their git repositories. This allows the users to make their repositories available to other users of the GitHub system, such that those users can create their own copy of other users’s repositories. In turn, this allows GitHub to provide features on top of the standard git features.\nAs a brief overview of the remainder of this article, the workflow created by this setup is as follows:\n- Bob creates a git repository on his local machine.\n- Bob copies it to his GitHub account, thus making the real respository the one hosted on the GitHub system.\n- Alice decides she likes Bob’s project and wants to help him with it, so she “forks” (copies) the repository to her own GitHub account.\n- Alice “clones” her copy of the repository on the GitHub system to her local computer.\n- Alice tells her local repository about Bob’s “upstream” (original) repository so that she can stay up-to-date with Bob’s changes.\n- Alice creates a “branch” on her local repository, makes changes, and “pushes” those changes to her fork on the GitHub system.\n- Alice uses the GitHub interface to tell Bob about her changes, asking if he’d like to incorprate them into his original repository.\n- Bob decides he likes the changes, accepts them, and his repository on the the GitHub system is updated.\nFork The Project\nFor remainder of this article we will assume that you want to collaborate on the awesome Pino project. Our first step is to create a copy of the project in our account on GitHub. So we navigate to [https://github.com/pinojs/pino] in a web browser and click “Fork” button (currently in the upper right corner of the page). GitHub will then show a screen that the process is happening, and then load the Pino repository in your account.\nNow that Pino has been forked to your account, we will “clone” it to your local machine. Cloning, in this context, is merely copying the repository from your GitHub account to your personal computer. To accomplish this task, we use our terminal and enter:\n$ cd ~/Projects # or any place you want to keep a collection of projects $ git clone https://github.com/your-username/pino.git\nAt this point you will have a directory:\n~/Projects/pino. This directory\nis your local copy of the repository. This local copy is automatically tied to\nyour copy of the repository on the GitHub system. This link between your two\ncopies is known locally by the name “origin”. Within git this is known as\na “remote”. To see the remotes associated with your local repository, which,\nat this point, is only “origin”, issue the commands:\n$ cd ~/Projects/pino $ git remote\nFor more information on remotes, read https://git-scm.com/docs/git-remote.\nLink To Upstream\nBefore you begin working on your changes it is a good idea to connect your local repository to the original repository. Colloquially, this is known as the “upstream” remote. From within your local repository, issue the following command:\n$ git remote add upstream https://github.com/pinojs/pino.git\nCreate A Feature Branch\nWe are now ready to begin working on some changes to the project. The key to successful collaboration is to request the minimal amount of changes as is necessary to implement your idea (or fix). You should do this on a new branch within your repository. A branch is merely a snapshot of the repository at a specific point in time. By working on a branch you lock the project to the state at which you decided you want to add changes, and it makes it easier for the upstream project owners to review your changes when you submit them.\nTypically, you will want to name your branch in such a way that it indicates why the branch was created. So, let’s assume we want to make some documentation corrections. Enter the following command from within your local copy of the repository:\n$ git checkout -b doc-corrections\nThe above command is a shortcut for the following two commands:\n$ git branch doc-corrections $ git checkout doc-corrections\nEvery repository has what is known as a\nmaster branch. At this point, we\nhave started a new branch,\ndoc-corrections from the current state of the\nmaster branch. To see the branches available:\n$ git branch\nBefore moving on, let’s also create the\ndoc-corrections branch within your\ncopy of the repository on GitHub. To do this, we will “push” our branch:\n$ git push -u origin doc-corrections\nThis has done two things:\n- It has created the\ndoc-correctionsbranch in your repository on GitHub.\n- It has configured your local copy of the repository to know that the local\ndoc-correctionscorresponds to the\ndoc-correctionsin your GitHub copy of the repository. This allows for some shortcuts when issuing certain git commands.\nTo learn more about branching and pushing, see:\nNow that we are on our own branch we can make our changes. For now, let’s\npretend you have made some typo corrections to the\nREADME.md file. Which is\nto say, you have opened\nREADME.md in your text editor, adjusted the text\nwithin and saved the document. If you issue the following command:\n$ git status\nYou will see that git has recognized that you made changes to the file. At\nthis point git isn’t going to do anything with those changes. Files that\nare tracked by a git repository have two stages: modified and scheduled\nto be committed. In the “modified” state git merely recognizes that a file\nhas been changed from its initial state. In the “scheduled to be committed”\nstate git will write the changes made to the file into its internal tracking\ngit commit command is run. So, let’s move our changes from the\nmodified state to the schedule state:\n$ git add README.md\nWith the changes scheduled to be committed, let’s actually perform the commit:\n$ git commit -m 'A short summary of the changes'\nThe above is the equivalent of sending an email with nothing more than the\nsubject line filled in. To write a full commit message, simply issue\ngit commit. This will open the default commit editor, probably a variant\nvi. A full commit message should have the following format:\nA short summary of the changes A body describing in further detail your changes. The summary line should not exceed 40 to 50 characters, and the body lines should not exceed 80 characters. Thes are not hard and fast rules, per se, but they widely followed guidelines. Some projects have other requirements for commit messages, and may refuse changes if they are not followed.\nWith your changes committed to the local repository, it’s time to send them to your copy on GitHub:\n$ git push\nWe are able to use this short push command since we linked your local\ndoc-corrections branch to your remote\ndoc-corrections branch. If we hadn’t,\nyou’d have to issue:\n$ git push origin doc-corrections\nYou can learn more about committing at https://git-scm.com/docs/git-commit.\nIncorporating Upstream Changes\nPrior to sending our corrections to the upstream project, it’s a good idea\nto make sure you have any changes that have occurred upstream into your\nrepository. To do that, we need to switch to the\nmaster branch, pull in\nchanges from upstream, and then merge them into your\n$ git checkout master $ git pull upstream master $ git checkout doc-corrections $ git merge master\nAt this point git may tell you that there are conflicts. A conflict arises\nwhen the same file has been edited in both branches of the merge,\ndoc-corrections in this case, such that git can’t decide\nwhich change should “win.” If this happens you will need to fix the conflicts\nand then issue a\nWith all of the changes incorporated, it’s time to push them to your copy\nYou can learn more about resolving conflicts at https://githowto.com/resolving_conflicts.\nSending Your Changes To Upstream\nNow that your changes are pushed to your copy of the repository on GitHub, it’s\ntime to send them to the upstream repository owner(s) for review and possible\ninclusion. To do this, open your copy of the repository on GitHub. It’ll be at\na URL like\nWith your repository open on GitHub in your browser, you should see a message\nsuggesting that you send a “Pull Request” (PR) from your\nbranch to the upstream\nmaster branch. Simply click the button in that message\nand you’ll be taken to a form where you will can describe your PR. It will\ndefault to the last commit message in your branch, but you can change it.\nWhen you are happy with the PR message, submit the PR and wait.\nGitHub is going to email the authors of the upstream project to let them know about your PR. They will review it and probably start a discussion with you, or just accept it if a discussion isn’t necessary. Either way, you will receive emails keeping you informed of the process.\nOnce the PR has been resolved, you can remove your feature branch:\n$ git checkout master $ git pull upstream master $ git branch -D doc-corrections\nWhile it may seem like a complicated process, and in some ways it is, you should now be able to collaborate on a project that uses the Git SCM. In general, this is how most open source projects work. And once you have gone from fork to PR, the process shortens to simply staying synchronized, creating branches, and submitting PRs.\nDon’t be afraid to get involved. If the upstream people ask for changes, in most cases they are not insinuating anything about you personally. The simply want your changes to conform to the nature of their project so that your changes can be included. Or they have suggestions for improvement, so that your changes can be included.\nA great place to get started with almost any project is with the documentation, just as we did in this article. If there’s one thing every project wants it’s someone willing to write documentation. As you get more comfortable, you will certainly start branching out from there.\nBy the way, I’m a maintainer on the Pino project. I look forward to seeing your PRs :)""]"	['<urn:uuid:0a9f3820-54b8-4124-9705-5f3bf36ef455>', '<urn:uuid:755dc951-a3f0-42a8-b4ab-8dc4d65b39c7>']	open-ended	direct	short-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:58:04.490895	6	98	3612
79	hunting clothes requirements waterfowl turkey compare	Ducks have very good eyesight and can easily spot hunters hiding in reeds if wearing wrong attire, requiring specific camouflaging patterns matching the hunting blinds (green or brown depending on plant life), along with facemasks, gloves, hats and jackets. For turkey hunting, the main clothing requirement is to avoid red, white, and blue colors since these are found in wild turkeys, and hunters must be camouflaged from head to toe to remain unseen by both turkeys and other hunters.	['With the arrival of spring, many people look forward to nice weather and to getting outside. To a hunter, spring is all about the sound of a gobbling turkey, or the vision of a tom in full strut approaching your decoy. In Colorado, over-the-counter turkey hunting licenses are easily purchased and affordable, public access and hunting grounds are readily available, and the adrenaline rush of hearing a turkey gobble is second to none.\nAs a hunter, you find yourself always trying new tactics, pushing the envelope to create the most successful tactics. Here are a few tips that you can use to create a successful hunt this spring.\nIn reality, if you have turkeys near you, the rest is fairly easy. The biggest mistake is not being near turkeys. However, many hunters spend days in the field calling and have amazing blind and decoy sets and yet go home empty-handed. A lot of hunters even blame themselves for things like having the wrong set or being bad at calling turkeys. But, in my opinion, the first and most important step is locating birds and making sure you are on birds before any type of approach or move is made.\nThe ideal hunt would start the evening before your first day out in the field – important time spent scouting, finding turkeys and roosting them (pinpointing where they are roosting for the night). That way the next morning you know exactly where to go. If scouting and locating birds the day before is not an option for you, then locating birds early in the morning while the birds are still in their roost is the next best option.\nTurkeys tend to be very vocal from the comfort of their roost early in the morning. When hunting, I tend to cover ground as fast as possible in the key hours of the morning when these birds are talking from their roost. I will hike, ride an ATV or cover ground in my vehicle until I locate birds. Generally speaking, I hope to hear them talking without encouragement. But if they are not freely talking, then I can gobble to these birds. It’s kind of funny, but to do this I literally put my finger in my mouth and make a gobble sound. More times then not, they answer back and reveal their location. On turkeys that are pressured and/or not quite talking yet, these birds might need a reaction to cause a shock gobble. My favorite technique for this is using an air horn. Yep, you read that right! I blow an air horn with short bursts and turkeys, out of reaction, will gobble back and give up their location.\nThree Options for Setting Up Your Hunt\nLet’s talk about your approach when deciding where to set up. There are three options when planning a setup.\nIf you know where birds are roosting, begin approaching the roost prior to sunrise. Set up a decoy and conceal yourself as close to the roosting area as possible. Stealth is the name of the game with this approach.\nIf you’re hunting on flatter ground or farmed fields, turkeys tend to build great patterns as to where they feed daily. If you have a pattern on turkeys in a feeding area, then this is a great way to create success. Set up a blind or hide in these areas where the birds are feeding daily. And simply wait. Again, getting into these areas undetected is key. Try to be in these areas at least an hour or two before you expect the birds to show up.\nThe run and gun approach! I tend to really prefer this technique when hunting mountain birds simply because their daily patterns are not as consistent as birds in flatter country. When I say run and gun, I am typically calling to the turkeys and when they respond, I am approaching them. The biggest mistake made with this approach is when hunters hear a turkey, they typically set up immediately and expect the turkey to cover all the ground and come right to them.\nWell…turkey hunting can be thought of like a relationship. If each member of the relationship gives at least 50% of their energy and commitment to the other member, then it is a healthy strong relationship and generally thrives. But, if one member is putting in 90% (the turkey) and the other 10% (the hunter), then it is rarely a successful relationship. Back to turkeys… Many hunters find birds whether hearing or seeing them and immediately set up. And when you actually think about this, it’s the exact situation of the hunter putting in 10% and expecting the turkey to put in 90% and cover the majority of the ground to come into the hunter. I’m not saying it doesn’t work but there are many occasions that it doesn’t. When I hear a bird calling, I will head towards the turkey and cut the distance down. I continue towards the turkey until I am 100% confident that the bird can hear me and or that I have the turkey’s attention and it’s moving to me. Once I know the bird is committed, then I set up.\nThe best way to prevent an accident in the field is to use a safe hunting strategy. As you prepare for the hunt, consider these ten tactics and incorporate them into your safe hunting plan:\n1. Avoid wearing clothing that has the same colors as a turkey. Red, white, and blue are the colors found in wild birds.\n2. Protect your back. Find a tree or rock outcropping to back up against and protect your backside. This may protect you from the pellets fired by another hunter as well as provide cover for the area you cannot easily see if another hunter is approaching your location.\n3. Camouflage from head to toe. Not only will this aid in your attempt to remain unseen by an approaching gobbler, but it will keep you hidden from another hunter who is walking in your area. If another hunter is approaching your position, remain still and call to the hunter in a clear voice. Do not whistle or use a call. Announce your presence without movement.\n4. Place decoys in a location that you can see the gobbler approaching to your “gun side” to prevent having to move to make a shot. This placement also prevents you from being in a direct line with the decoy if an unseen hunter attempts to shoot your decoy.\n5. When traveling to a hunting location, transporting decoys or a harvested bird, wear hunter orange (a vest or hat). Place the harvested bird in a game bag or wrap the bird in an orange vest. DO NOT sling a harvested bird over your shoulder and walk out of your hunting area.\n6. Always look beyond your target to ensure the area is safe. Do not shoot sky-lined birds, not even with a shotgun.\n7. Clearly identify your target as a legal bird. In the spring turkey season, a bird must have a visible beard. Do not shoot at movement in the brush, even if you hear a gobble.\n8. Do not carry a loaded firearm in your vehicle. Completely unload the firearm before traveling.\n9. Inspect your ammunition before your hunt. Ensure you are carrying the correct gauge of ammunition for the shotgun you are carrying. Remember, you will typically be loading your firearm in the dark.\n10. Never stalk a wild turkey. The enjoyment of the spring season is calling the bird to you. With the number of realistic decoys in the field today, what you are stalking may just be another hunter.\nPlanning Your Days in the Field\nThe last thing I want to cover is when you hunt during the season. The excitement of the average hunter is at a boiling point on opening day. And many hunters either tag out on opening day or hunt extremely hard for a few days and end up giving up before the peak of the season hits. At the beginning of the season, it can be common that the hens are not at a breeding state yet. Therefore, calling and working birds is slightly tougher. As the season progresses, the breeding season peaks and the birds become far more responsive. Again, a lot of hunters give up before the peak hits. With that said, my favorite portion of the season is the end. By the last few days of the season, a majority of the hens are on their nests and the gobblers are extremely frustrated that there are fewer and fewer hens. When this happens, they become more vocal and cover more ground daily looking for breedable hens.\nStay safe and enjoy the season!\nColorado counties, municipalities, and land management agencies continue to update their COVID-19 guidance including travel restrictions, road closures, and access limitations on a regular basis. Colorado Parks and Wildlife reminds anglers, hunters, and all other outdoor recreationists that it is your responsibility to research and understand the specific guidance, ordinances and restrictions in place for any planned local recreation – know before you go. For COVID-19 updates, please visit the COVID-19 Information page on the Colorado Parks and Wildlife website.\nNate Zelinsky is a professional Walleye Angler and all species guide based in Colorado and has guided and tournament fished all across the country. Nate is on his 19th year as owner of Tightline Outdoors Colorado’s Premier outlet for multi-species guiding, tournaments, TV, Digital Content, TV, Radio, and Writing! Some of these experiences can be seen on In-Fisherman, Jarrett Edwards Outdoors, Denver’s 7News, 9News, and WFN. You can also listen to Nate on Saturday mornings at 10 a.m. on The Fan Outdoors with Terry Wickstrom. Nate’s daily goal is to educate anglers to create success every time they go on the water. More on Nate can be found at www.tightlineoutdoors.com.', 'Duck hunting is simply the process of tracing waterfowls, including ducks either for sport or for food. In most countries, this activity is considered to be an outdoor sporting activity and is prohibited at commercial levels in western nations. Today we will try to provide you with the most relevant duck hunting tips that will help you improve your hunting skills and become a pro.\nFirst of all you should know that this is not an easy outdoor activity. There are various things that you need to observe in order to be successful. But we have good news: it doesn’t really matter whether you are experienced or not.\nWith the right guidelines, you will be able to build your skill and you will be an expert in no time. By the time you are reading the conclusion, you will be fully equipped to hunt ducks among other waterfowls.\nWhat You Need for The Hunt\nBefore you can engage the “how to steps” for shooting ducks, you first need to know what you need for the hunt. This information includes duck basics, hunting gear, and any other relevant information.\nYou need to know the various species of ducks and be able to identify them correctly when you are in the field. This kind of information is very useful to you as a birder and a hunter, because you cannot hunt what you do not know.\nWith correct knowledge of the various species, you will be able to know the endangered species and the ones that you can shoot down without any repercussions.\nThe most popular duck species that you must be familiar with, especially if it is your first time to hunt ducks, are:\n- Mallard Duck. These are the world’s commonest species, which can be located in New Zealand, Southeast Australia, North America, Asia and Europe. They have short tails and the males do have curled central tail feathers for attracting females. Their wings are covered in long flight feathers. They have eyes on the sides of their heads, which are resourceful for detecting marauders, even from behind.\n- Paradise Shelducks. Also referred to as parries, these ducks are common in New Zealand, from sports fields, urban parks, farms and wild areas. These ducks are a bit larger than normal ducks, but not larger than geese. The females of this species are the ones who are colorful. The males do have attractive textures in their plumage, but they lack the exceptional chestnuts of the females.\n- Grey Ducks. These ducks are common in Australia and New Zealand. Males as well as females are the same and they are lightly similar to the female mallard ducks. They have dark grey bills with a black tip. Their upper body feathers are brown with a narrow buff edge without central markings. Their wings have a green speculum with a thin white line on the rear edge. Also, the top of their heads is nearly black.\n- Shoveler Ducks. Also referred to as the Northern Shoveler; this species breed in northern parts of North America, Asia and Europe. It is the only species with the oversized bill that it derives its name from. The males are slightly larger than the females during the fall season and they are considerably larger in spring. The brighter colors of the conjugal plumage in spring contribute to some of the visual differences.\nSuccessful hunts are largely determined by the gear you bring along. You should match the gear with the venue.\nNormally, what you bring has to contribute to the primary purpose, which is a successful duck hunt. The most useful gear you should have includes the following:\n- Decoys: These are imitations of ducks that you can use to lure ducks close. They are made of plastic or wood and are painted to appear like ducks. You are required to set them in the proximity of your blinds. They can be stationary or moving. Moving ones are referred to as roboducks. Good decoys should have front weighted keel design; non-chip paint, rubberized molding and imitate feeding ducks.\n- Shotgun: The ideal shotgun for shooting ducks should have a rifled barrel and scope. Suitable ammunition is very important. Rifling in the barrel imparts spin to the slug, thus stabilizing as well as increasing accuracy. You should choose a properly configured shotgun for effectiveness. You should consider a 12-gauge shotgun. Even though, almost any shotgun with an open choke can be convenient, the best choice is one that has your desired features and specifications. Do read our reviews of the best hunting guns to get you started on your adventure.\n- Duck Calls: These are useful for attracting ducks into your blinds. Most calls are designed from wood and they have a small reed inside. A duck call’s operation is simple; whenever you blow the reed vibrates into the call. The best model should be able to make loud hail calls as well as soft nasal Your duck call should be able to mimic the soft flexible tissue of a duck’s tongue and neck.\n- Hunting Clothes: Ducks, unlike any other waterfowls, have very good eyesight. They can easily spot you hiding in the reeds along the edge of a lake if you are wearing the wrong attire. You must camouflage yourself by wearing concealing clothes. You should look for camouflaging pattern that matches your hunting blinds when choosing facemasks, gloves, hats and jackets. You have to go with green camouflaging pattern if the plant life is green. You should go with the brown camouflaging pattern when plant life is not green. For a review of the best hunting clothes for you to choose from, read our earlier piece on this all-important topic.\n- Footwear: Duck hunting footwear is important, because of the wet and muddy environments. The most recommended footwear is hip boots and waders. They will ensure you do not get wet and muddy. The footgear has to fit tightly at the ankles, or else they can easily pull off when you are walking on mud. Duck footgear will always come in handy when setting up decoys in flooded fields or shallow water. Make sure you choose footwear that will not be filled with mud and debris. In addition, they should be warmer and comfortable to wear. Check our list of the top hunting boots to make the experience more comfortable.\n- Binoculars: Optics is necessary if you are to select the best hunting areas.\nLong distance glassing of ducks needs exceptional binoculars.\nThe binoculars you buy should present you with the following features: twenty-feet close focusing distance; stunning HD clarity; four hundred plus feet field of view at a thousand yards; durable design; twelve-millimeter eye relief; and high magnification, at least 7X.\nStep-by-Step Duck Hunting Guidelines\nNow that you know the basics and the various gears you need for the hunt, you can go ahead and learn how to duck hunt.\nStep 1 – Scout Potential Hunting Spots and the Prevailing Species\nYou have to scout for potential hunting grounds and identify the dominant species. To do this, you need a reliable topographic map of the area of interest along with a plat book that indicates land ownership. Spend some quality time looking for ducks on water or in the air.\nYou probably need a pair of binoculars for spotting waterfowls from a distance, especially when they are in the air. You will also be able to spot hard-to-see fowls that are swimming in vegetation or along shorelines.\nMost puddle ducks are usually active in early morning and in the evening. Therefore, these are the recommended times to scout. On the other hand, diving ducks tend to stay in close proximity to their feeding sources. Thus, you can scout for them at any time of the day. Always determine the depth of the water; you can use your footwear for this, such as waders.\nStep 2 – Check If the Land Is Private or Public\nCheck your plat book to confirm ownership of the land. If your potential hunting ground is on private land, then you will need permission from the owner. If the land is private, politely introduce yourself to the owner and ask for permission.\nWhen asking for permission, be very specific as far as the section of the land is concerned. You should always remember to ask for permission for any subsequent hunts you wish to conduct. Do not make any assumptions.\nStep 3 – Determine the Best Blind Spots\nFor your hunt to be successful you need to choose good blinds. You should establish blind sites with respect to different wind conditions. There are different blinds that you can find.\nUsually, the best blind spot is natural vegetation. You may have to make a blind by cutting vegetation or set up an artificial version if there is no vegetation around. A bank of a small pond can also be a reliable blind, especially when tracing around swamps and marshes. Also, your blinds have to blend well with the surrounding environment.\nStep 4 – Set Up Decoys for Diving Ducks & Use Calls for Puddle Ducks\nSet up your decoys within shooting range. The number of decoys you can set depends entirely on the size of your hunting area. You should place enough decoys to increase the chances of luring ducks.\nYou should place your decoys less than nine meters away from your blind spot. This way, it will be harder for you to give up your cover. Ducks have the tendency of landing in open areas; therefore, you must consider wind conditions.\nYou are advised to set up your decoys at locations where the wind is at your back. This is so, since ducks are used to land into the wind. Ducks will spread their wings before they land. The mechanism is simple; the wind helps them to land by blowing against their wings.\nAvoid direct sunlight, because sunlight will make it easier for the ducks to spot you and it will definitely make it harder for you to see the ducks.\nCalling ducks take practice and you may not be able to successfully call ducks in the beginning. However, practice makes perfect. To use a duck call, hold the uncovered end of the thin tube in the web of your forefinger and thumb. Then place the other end of the call to your lips. Then blow into the mouthpiece while opening your fingers as the pressure builds up behind them in order to create a quacking sound.\nStep 5 – Shoot Your Target\nOnce the ducks are within shooting range, go ahead and take the shot. The most recommended firearm for this activity is a shotgun. A shotgun can withstand the worst situations and weather conditions that might occur during hunting. You can choose your desired shotgun with respect to the bore diameter (gauge).\nThere are three bore diameters you can choose from, which are: 20-gauge, 12-gauge and 10-gauge.\nThe recommended diameter for duck hunting is 12 gauge shotguns. However, you can opt to experiment each of the gauges to see which one will work best for you. There are no guesses when it comes to shooting or else all your efforts would have been for nothing. You need to be able to adjust your angle of shooting with respect to the ducks’ flight speed.\nThe most practical technique that you can use is the swing-through. This technique requires you to hold your shotgun with one hand behind the trigger and the other hand under the fore stock.\nYour hand that is positioned beneath the fore stock should raise the barrel and your other hand brings the end of the shotgun to your shoulder.\nYou should swing your shotgun in order to aim ahead of the ducks in their flight path. At times, you may observe that ducks are not flying your way. When this happens, you may have no choice but to reposition your blinds.\nHunting ducks is easy as long as you have the correct information and gear. Since you may face challenges if you do not have the correct tips to guide you, we provided you with more than enough tips for the most important and crucial information about ducks and the best hunting gear. The “how to” steps are discussed in detail so you should have a successful hunt.\nYou do not need much to be an expert duck hunter, all you need are the tips provided in this article. Do you think we have left out any crucial tips? If so, please let us know in comments.']	['<urn:uuid:80ecb289-19cd-49d2-874e-65874ef5ea0f>', '<urn:uuid:b0dd7924-083e-49f5-b2cb-7e5ebc427ada>']	factoid	direct	short-search-query	distant-from-document	comparison	novice	2025-05-12T20:58:04.490895	6	79	3756
80	banksy art gallery price impact how much	Banksy's art has achieved phenomenally high prices at auction after moving from Bristol to global art capitals. In 2008, his influence on the art market was notable - for example, when the owner of a wall with his Olympics 2012 artwork tried to sell it, Banksy declared it would no longer be considered his art, as he created it for public viewing. He has consistently challenged traditional art market dynamics, exemplified by his 2012 Central Park stall where he sold signed works for just $60 each.	"[""|home | features | exhibitions | interviews | profiles | webprojects | gazetteer | links | archive | forum|\nNigel Ayers describes his visit to see the 'expanded-media cartoons' of Banksy in Bristol\nAs we arrived\nBanksy crossed from subculture to mainstream a few years back, following his move from Bristol to the art capitals of the world, a bestselling coffee-table art book, and his work getting regular exposure on TV and in the international press. Not to mention the phenomenally high prices being paid for his paintings at auction. So this is the local boy giving something back to the hometown that still bears the marks of his early stencil-work. The flyer says “PG Contains scenes of a childish nature some adults may find disappointing”.\nThe queue turned out to be for one and a half hours. I usually avoid events you have to queue for, but it was a nice sunny day and we were treated to the occasional sight of middle aged women joggers dressed in pink bras puffing and panting their way up the hill in support of a breast cancer charity. Also I found the queue added to the theatre of the occasion, and as you got closer to the entrance of the museum, the excitement seemed to build with the first whiffs of aerosol paint.\nAnd it also added to the irony of the occasion, as you’re shepherded into a really old-fashioned museum by security guards employed by an artist who has built his reputation on dodging security guards. And this is to see art that has an image that is supposed to be way too rebellious to be sanctioned by fusty old institutions such as this.\nThe artist has chosen to hire this museum and cover all the expenses for what must be his first major show in this country. And admission is free. And there’s no Banksy merchandising at the museum, apart from the book “Wall and Piece” (which you find all over Bristol anyway) though the Oxfam shop across the road is doing a nice line in Banksy stickers and postcards.\nSo, we found ourselves being corralled into this old stuffy museum and being greeted by friendly museum staff and - thank goodness - they don’t have art explainers showing you round.\nAt the entrance,\nAs you shuffle down the corridor, there’s a candlelit shrine in homage to the recently deceased pop star, Michael Jackson (below). It’s an oil painting in Victorian genre style of a fairytale cottage in dark woods, with The King of Pop as the wicked witch Jacko leaning out of the doorway offering candy sticks to tempt two lost children inside.\nOnce you’ve got through all this entrance malarkey, you’re free to wander round the displays, and the museum is large and not so crowded once you get inside. And you’re allowed to take photographs.\nThe first room is exclusively Art of Banksy, with a militant looking false ceiling of army-surplus camouflage net. And behind a screen of chicken wire you can see a full-scale mock-up of the artist’s studio, hung with loads of cardboard stencils, newspaper headlines about Banksy’s graffiti, filing cabinets are labelled “good ideas” “bad ideas” “other people’s ideas” and “porn”, a knitted cardigan that reads THUG FOR LIFE hangs off the artist’s chair. There’s the sound of a London radio phone-in on some Banksy controversy or other. On the easel is an oil painting of pixellated-faced portrait of a hoody in a baseball cap, the same self-portrait used in the Wall and Piece book.\nThe walls of the rest of this gallery are covered in “original” Banksys (as far as “original” work is concerned, just about every image in the show is a remix or update of existing cultural product), paintings on canvas, modified reproductions of paintings, small drawings.\nIn this gallery\nsetting, it is possible to admire the quality of the brushstrokes and to\nexamine closely the various techniques used to remix and remodel. On a\ncraft side, it’s obvious that the guy works with a range from hands-on\ntraditional craft skills to computerised techniques. He can draw, he’s\npretty good at painting and can cut a mean stencil. And he’ll tend to\nabandon technical virtuosity or anything “deep” in favour of quick\nRather than calling it the culturally-loaded term “art”, Banksy-work may be more precisely described as expanded media-cartooning which plays with the signifiers of modern urban experience. Each piece within the show works as an easily-readable visual joke. The humour is often cheesy and has gently satirical edge, the context and framing of the work is vital to its meaning. A lot of the jokes are mild, usually visual puns or updates of familiar old paintings. Both the strength and weakness of his output lies in the fact that rather than serious message, serious technique, or serious “cool”, Banksy tends to err on the side of cheeky humour.\nIn the next room are the pieces that I’ve really come to see, the animatronic petshop. There are surveillance cameras acting like roosting birds, fish fingers swimming round a goldfish bowl and lots of little cages with pet sausages wiggling about in them.\neffects are of\nThroughout the rest of the museum, there a mixture of the museum’s collection and additions by Banksy. There’s a burned out icecream van converted into an information booth, modified faux marble statues and joke oil paintings inserted into the museum’s existing collection of old masters. You find yourself on a treasure hunt of Banksy interventions, and sometimes you’re not quite sure what if you’re looking at is part of the museum’s regular display, or a Banksy addition. An ornate gypsy caravan has been issued with an eviction notice. A stuffed fox in a British wildlife display holds a bloodied Countryside Alliance placard. There’s a hash pipe inserted into another display. I’m not sure whether the “Dinosaur Sick” is an original part of the fossil display or a Banksy addition. In the modern art room, a Banksy-looking painting of a bombed-out farmhouse turns out to be the once-censored “A Farm near St Athans” (1940) by official war artist John Armstrong.\nA common technique in Banksy’s art is to play on the edges of the surface on which he is painting. Figures wander out of holes to take a break, a waterfall spills out of the bottom of a painting, UFO's fly out of the canvases zapping ancient galleons with death rays. The grit and mess of the urban environment is integrated into the stencilled paintings, giving texture and trompe de l’oeil visual puns: painted figures engaged in the act of painting.\nEverything in this show is humorous, and it’s just as often daft, trite and sloppily done as it is carefully executed. The work is both about messing with illusions and creating the illusion of Banksy as pixellated superhero dodging the surveillance cameras, breaking free from Disney-matrix mind control and sticking two fingers up at Burger King. The meaning of these images is created by the way they are set up in this particularly stuffy kind of museum: the kind of museum you get dragged round on school visits, and not all cool and white-walled and trendily-converted former industrial-space like the Tate, Arnolfini, Exchange, etc etc.\nI really enjoyed the show, it was far better than I expected. The dodgy quality of a lot of the work humanised it and made it seem more intimate and uncontrived. As well as there being more jokes, there is an ethical dimension to Banksy’s work which makes a refreshing change from a lot of contemporary art-bollocks.\nLooking at criticism of Banksy, I think a rather false argument arises about whether his art is “subversive”. Certainly Banksy consistently plays with the idea of subversion, in the imagery used, in the means of production, and in his pseudo-anonymity. Throughout his work there is a constant ridicule of authority figures and institutions, including contemporary art institutions. This is in common with a lot of youth- or subculture-oriented cultural product. It has a broadly leftist/anarchist/punk/environmentalist undercurrent and consistently celebrates “street” culture. Twenty or thirty years ago this kind of art might have been seen as more subversive than it does these days. Back then, along with the CND, Trades Unionists and new age travellers, artists like this might have attracted the attention of the security services, and even found themselves on Maggie Thatcher’s hit list. Nowadays a few minutes on Google will tell the shape-shifting lizards in charge of the secret state all they need to know about this artist’s real name, what he looks like, where he went to school, who his Facebook friends are, etc.\nIf being subversive means it undermines the way we currently relate to art, globalised capitalism, and the mythology of artist-genius, then of course Banksy’s work isn’t subversive at all. The way the world is at the moment, to work as an autonomous artist and to live out the myth of individualism by “expressing yourself” requires an amount of financial cushioning, whatever scale you work on. Banksy is best-known for his unsanctioned painterly interventions into the civic landscape. Remaining anonymous may be sensible to remain ahead of the legal repercussions associated with this kind of activity, but this is a pseudo-anonymity, as the signature BANKSY has often featured heavily. The artist with the Banksy tag has been using the celebrity-commodity system in a very pure form, creating the specific artist name, artworks related to this name and the idea of one person as a genius behind it. Speculation about the “real person” behind the name creates a media story which Banksy continues to exploit. So the artwork follows the specific rules of capitalist production, aimed to accumulate economic capital and/or symbolic capital which is the currency of all scenes and subcultures.\nNigel Ayers 25//7/09"", 'What is the relationship between art and the market? This thorny question, which regularly worries students of art and art history, was one of those also considered by Grayson Perry in his first BBC Reith Lecture. In the series as a whole he has promised to answer some of the ‘big questions’ that ordinary people might have about art. Under the title ‘Democracy has bad taste’ his first lecture considered how different works end up in the museum or gallery, and one of his big answers was the role of the market – auction houses and dealers – and their customers both public and private, in establishing the ‘value’ of pieces of art. The art that is ‘in’ with dealers and collectors is what is agreed as ‘art’ and has value.\nYet, this relationship, has increasingly been called into question in recent years, most notably by artists themselves. In 2008, YBA Damien Hirst infamously chose to hold a one-man sale of mostly new work at Sotheby’s, by-passing the usual route through the commercial gallery or other collector. He set a record, making £111 million in two days. As Maev Kennedy memorably commented in the Guardian, this was more than all the artists in the National Gallery made in a lifetime. The sale began on the very day that Lehman Brothers went bankrupt, and art has become an increasingly popular investment item in the subsequent years. Its value as an investment is only established by considering what it would fetch within the market.\nIn Perry’s second lecture ‘Beating the Bounds,’ he established a series of ‘tests’ for deciding what is and isn’t art, two of which were ‘Is it in an art space?’ (a museum or gallery), and ‘is it by an artist? He gave the striking example of the graffiti artist Banksy, who created a piece for the Olympics in 2012 showing a child sweatshop worker making Union Jack bunting. Perry recalled that, when the owner of the wall on which this was painted tried to sell the work, the Banksy argued that that the piece was no longer a ‘Banksy’ and was unsaleable as one. He had done his work for the general public, and thus it would, essentially, cease to be a Banksy if removed from that setting. So, its presence on the market would make it cease to be (his) art.\nBanksy has made another intervention in the art market more recently. A lot of media attention has been paid to his recent ‘residency’ in New York, entitled ‘Better Out than In’, during which he posted almost daily pictures on his website of new art works, whether graffiti, installation or performance. On 12th October, Banksy put a number of signed works on sale on a stall in Central Park for only $60 each. He told the New York magazine Village Voice ‘I wanted to make some art without the price tag attached. There is no gallery show or book or film. It’s pointless. Which hopefully means something.’\nSo, where does this leave art, the market, and museums? Undoubtedly, Banksy’s visibility and value were hugely increased when a brave curator at Bristol Museum gave him access to the galleries for the enormously popular show ‘Banksy v Bristol Museum’ in 2009. Is an artist such as Banksy better out than in when it comes to the market? Perhaps the real value is in the tensions between the one and the other.']"	['<urn:uuid:be3d2923-76ba-4e7c-89fe-770a7572db34>', '<urn:uuid:497e6110-ad42-450b-bbb7-42c73a346211>']	factoid	with-premise	short-search-query	distant-from-document	multi-aspect	novice	2025-05-12T20:58:04.490895	7	86	2208
81	What role do fishing guides and management practices play in Scotland's salmon rivers, and how does this localized approach compare to global efforts to protect marine species?	In Scotland's salmon rivers, fishing guides and ghillies play a crucial role by providing essential knowledge about fishing spots, proper techniques, and river conditions, with guides typically receiving tips of £20-£50 depending on service quality. On the global scale, management approaches focus on protected marine reserves and quota systems - less than 1% of oceans are currently protected, though there's an international agreement to increase this to 10% by 2020. Research shows that properly managed fisheries with quota systems are twice as likely to avoid collapse compared to open-access fisheries, but need proper monitoring and enforcement through methods like vessel tracking and spot-checks.	['Everything you need to know about getting started with salmon fishing\nThis is a comprehensive no-nonsense guide to getting started, if you’re looking to learn how to fish for salmon.\nSo you want to get into salmon fishing but don’t know where to start? Hopefully this guide will help. This is a simple guide to help you get started. I’ve based the knowledge in this article on 16 years as a professional salmon fishing guide throughout Scotland.\nIt can be daunting starting out, where to go, what equipment to use, how to read a river, How to fish for salmon, what is the river etiquette? What Spey casts to use? How much do I tip the Ghillie? The insights are here, please read on:\nLet’s break his down into a step by step guide, beginning with:\nTactics for fishing for salmon\nThe first thing to understand is that salmon do not feed when they enter freshwater. They have two things programmed in their brain, reproduction and survival.\nWhy a salmon takes a fly nobody really can be certain. It’s thought to be a combination of aggression, curiosity and a conditioned response from when they fed in saltwater.\nMake no mistake, salmon fishing is a dark art. You need resilience as blank days are frequent, and determination to keep moving your flies through the pools with method, focus and a clear strategy. Usually you would cast your fly down and across and try to keep a steady swing on the fly, letting the current do the work. As you work through a pool, imagine you are playing battleships and draw a picture in your mind of the river as a god, and your prospecting all the squares hunting a fish. Work through the pool by casting and then taking a step downstream.\nMy best advice is listen to the experts, when you arrive at a river, speak to the ghillie, his advice is invaluable. Remember he lives and breathes his beat of the river, knows where the salmon lies are and importantly at what water level where to fish. Ask him also what tip to add to your fly line (sink rate or floater) and what fly. If you’re a beginner, get him to show you the correct knot for the fly. I recommend a double turle, for salmon fly in doubles (two hooks). Most of all keep it simple, persistence beats resistance and don’t overcomplicate your casting or try too hard to reach that extra few feet. Often fish can be a rod length or two away. Don’t make too much disturbance when wading.\nWhat equipment to use for salmon fishing in Scotland?\nMost rivers in Scotland are fly fishing only, and some allow spinning like the Tay and the Tweed. The sensible thing to do is to ask yourself, do I want to fish one of the big rivers? The Tay, The Dee, Spey and Tweed, r a smaller spate river, like the River Ericht, Orchy or Alness.\nSalmon Fishing Seasons in Scotland\n|Salmon Fishery District||Start of Rod Season||End of Rod Season|\n|1. Annan||25th February||15th November|\n|2. Argyll||11th February||31st October|\n|3. Arnisdale||11th February||31st October|\n|4. Ayr||11th February||31st October|\n|5. Beauly||11th February||15th October|\n|6. Bladenoch||11th February||31st October|\n|7. Brora||1st February||15th October|\n|Forss||11th February||31st October|\n|Thurso and Stroma||11th January||5th October|\n|Wick||11th February||31st October|\n|Dunbeath||11th February||15th October|\n|Berriedale||11th February||31st October|\n|9. Carron||11th February||31st October|\n|10. Clyde and Leven||11th February||31st October|\n|River Alness and Alt Graad||11th February||31st October|\n|River Conon and Balnagown||11th February||30th September|\n|12. Cree||1st March||14th October|\n|13. Crowe and Shiel (Loch Duich)||11th February||31st October|\n|14. River Dee (Aberdeenshire)|\n|River Dee||1st February||15th October|\n|River Carron and River Cowie||11th February||31st October|\n|15. River Dee (Kirkcudbright)||11th February||31st October|\n|16. Deveron||11th February||31st October|\n|17. Don||11th February||31st October|\n|18. Doon||11th February||31st October|\n|19. Eachaig||1st May||31st October|\n|River Bervie||25th February||31st October|\n|River North and South Esk||16th February||31st October|\n|21. Findhorn||11th February||30th September|\n|22. Fleet (Sutherlandshire)||25th February||31st October|\n|23. Fleet (Kirkcudbright)||25th February||31st October|\n|24. Forth||1st February||31st October|\n|25. Girvan||25th February||31st October|\n|26. Glenelg||11th February||31st October|\n|27. Helmsdale||11th January||30th September|\n|28. Inner (Jura)||25th February||31st October|\n|29. Irvine and Garnock|\n|River Irvine||25th February||15th November|\n|River Garnock||25th February||31st October|\n|30. Kishorn||11th February||31st October|\n|31. Kyle of Sutherland||11th January||30th September|\n|32. Laggan and Sorn (Islay)||25th February||31st October|\n|33. Loch Long (Luingi and Elchaig)||11th February||31st October|\n|34. Lochaber||11th February||31st October|\n|35. Lossie||25th February||31st October|\n|36. Luce||25th February||31st October|\n|37. Mull||11th February||31st October|\n|38. Nairn||11th February||7th October|\n|39. Ness||15th January||15th October|\n|40. Nith||25th February||30th November|\n|41. North and West|\n|Hope and Polla||12th January||30th September|\n|Grudie or Dionard||11th February||31st October|\n|Inchard||11th February||31st October|\n|Laxford||11th February||31st October|\n|Inver||11th February||31st October|\n|Kirkaig||11th February||31st October|\n|River Halladale||12th January||30th September|\n|Kinloch (Kyle of Tongue||11th February||31st October|\n|Naver and Borgie||12th January||30th September|\n|Strathy||12th January||30th September|\n|43. Orkney Islands||25th February||31st October|\n|44. Shetland Islands||25th February||31st October|\n|45. Skye||11th February||15th October|\n|46. Spey||11th February||30th September|\n|47. Stinchar||25th February||31st October|\n|Tay||15th January||15th October|\n|River Earn||1st February||31st October|\n|River Eden||5th February||31st October|\n|49. Tweed||1st February||30th November|\n|50. Ugie||10th February||31st October|\n|51. Urr||25th February||29th November|\n|52. Wester Ross||11th February||31st October|\n|53. Western Isles|\n|Loch Roag||11th February||16th October|\n|East Lewis||11th February||31st October|\n|Clayburn||25th February||31st October|\n|Fincastle||25th February||31st October|\n|Resort||11th February||31st October|\n|Mullanageren||25th February||31st October|\n|Howmore||25th February||31st October|\n|54. Ythan||11th February||31st October|\nSalmon fishing Equipment for Big 4 rivers – The Tweed, Tay, Spey and Dee\nA 14ft or 15ft Fly rod, is a perfect size to cover a big river, don’t go any smaller, because if you get into a big fish, you’ll struggle to control it. You don’t need to spend the earth, a Shakespeare Oracle Scandi 15ft rod will set you back £120.00 on Amazon and its a pretty decent rod, certainly good enough for a beginner to enjoy learning the fundamentals. The Scandi version is my pick (there are other Shakespeare Oracles) is that Scandi (short Spey) lines are easier to cast for beginners.\nIf you’re looking for a robust, great value salmon reel, the Vision Deep ticks many boxes. It will set you back about £80.00 can take a battering, has a decent drag and great line capacity. It does what it says on the tin.\nSo what salmon fly line is best for a beginner? If your setting out, do not use a long head Spey line, i.e. 65ft, opt for a Short head Spey line(sometimes called Scandi lines). These are easier to cast and I can tell you that in my years as a guide I have got literally thousands of people casting lines like this good enough to cover fish in under 30 minutes of tuition. You cannot go wrong with an Arflo Rage Shooting head kit. For £79 this gives you a running line/ short spey line and tips, floater, intermediate and sink tip. This will cover most scenarios and this is an extremely easy line to cast. If you’ve heard about lines called Scandi or Skagit, then this line sits somewhere in between. Take it from me, it’s easy to cast, and hugely versatile.\nSo there you have it. A complete, fly rod, reel and line for £280.00.\nSpate rivers: If you are looking at fishing smaller rivers, then use similar equipment to above but come down to an 11ft 8 weight. This will help you fish in tighter spaces and still give you enough clout to play a bigger multi-wintered fish.\nTippet. Go for Maxima. 15lb for bigger rivers and 10lb for the spate rivers. This line is reliable and tried and tested. It stretches when a fish lunges on a run, and it comes in two colours. Chameleon for peat stained water and Ultragreen for clearer rivers. It also changes colour when the line degrades and oxidises, going a pale pink colour.\nWhat are the best value breathable waders?\nWaders are an item you should be careful buying. Many people buy cheap and pay the price.. many people buy expensive and pay the price too. I cannot see past the Orvis Clearwater waders, they offer excellent value for money, and I would recommend trying them on with thickish socks and wading boots. Make sure you try bending down and standing up and all legs movements are easy and unrestricted. Get the Orvis Clearwater wading boots too, they are superb value, the waders and boots will set you back £368.00 but often Orvis offer deals and discounts, don?t be afraid to contact your local store and ask.\nSo in summary, £650.00 will get you properly set up, and enable you to be tooled up for a day on the river. (Excluding flies and nylon tippet)\nWhich river in Scotland is best for salmon fishing for beginners?\nThis depends where you live, but make sure you select a beat with a good Ghillie, preferably one who holds a casting qualification like SGAIC and therefore can give casting tips and tuition. If you want a more dedicated (one on one) guide for the day, then pay the extra and get a good one. You can meet the guide at the river, use your own gear and he or she can take you through the fundamentals and various casts. This can be invaluable in the beginning, so bad habits don’t set in. Salmon casting can also be physically taxing when you set out. First off you are using muscles and movements that are strange and alien to the norm. Having a guide and casting instructor can help you understand how to put less effort in and more technique, this can save you damaging muscles and getting a repetitive strain injury.\nThe River Tay at Fishponds is one such beat, a gem of a Ghillie in Iain Kirk, a casting instructor and rod designer for Mackenzie Fly Rods, he’s always willing to help. Its good water for learning and relatively inexpensive in the earlier parts of the season (spring and summer)\nWhilst were on the subject of ghillies and guides, it’s customary to tip them. This is the right thing to do and as a general guide, anything from £20-£50 depending on the day you’ve had is the norm. Some grilles are very helpful and get involved and work hard to keep you right, others will show you the pool and hide in the hut, there’s no right or wrong, that’s the way to is, this will help you decide how much to tip. Tipping isn’t mandatory, but if you don’t give them a “handshake’ don’t expect the same level of service next time or any worthwhile help. Ghillies never forget!\nHire a guiding company to take you out, they will provide you with quality fishing tackle, waders, boots, life vest and rain jacket. Well we do that certainly!\nRather than but all the gear and then discover its not for you, you may want to consider hiring a guiding company for your first trip. They will provide you with quality fishing tackle, waders, boots, life vest and rain jacket. This will set you back approx £375.00 for a full day including dedicated guide with you all day, permit, tackle and VAT. Our team of professional fishing guides include SGAIC casting instructors, so if you want to try various combinations fo rods and lines and learn various Spey casts, we can help.\nStewart Collingswood, Orvis Endorsed Guide/ founder Alba Game Fishing Ltd', 'It has been some time since most humans lived as hunter-gatherers – with one important exception. Fish are the last wild animal that we hunt in large numbers. And yet, we may be the last generation to do so.\nEntire species of marine life will never be seen in the Anthropocene (the Age of Man), let alone tasted, if we do not curb our insatiable voracity for fish. Last year, global fish consumption hit a record high of 17 kg (37 pounds) per person per year, even though global fish stocks have continued to decline. On average, people eat four times as much fish now than they did in 1950.\nAround 85% of global fish stocks are over-exploited, depleted, fully exploited or in recovery from exploitation. Only this week, a report suggested there may be fewer than 100 cod over the age of 13 years in the North Sea between the United Kingdom and Scandinavia. The figure is still under dispute, but it’s a worrying sign that we could be losing fish old enough to create offspring that replenish populations.\nLarge areas of seabed in the Mediterranean and North Sea now resemble a desert – the seas have been expunged of fish using increasingly efficient methods such as bottom trawling. And now, these heavily subsidised industrial fleets are cleaning up tropical oceans too. One-quarter of the EU catch is now made outside European waters, much of it in previously rich West African seas, where each trawler can scoop up hundreds of thousands of kilos of fish in a day. All West African fisheries are now over-exploited, coastal fisheries have declined 50% in the past 30 years, according to the UN Food and Agriculture Organisation.\nCatches in the tropics are expected to decline a further 40% by 2050, and yet some 400 million people in Africa and Southeast Asia rely on fish caught (mainly through artisanal fishing) to provide their protein and minerals. With climate change expected to impact agricultural production, people are going to rely more than ever on fish for their nutritional needs.\nThe policy of subsidising vast fishing fleets to catch ever-diminishing stocks is unsustainable. In Spain, for example, one in three fish landed is paid for by subsidy. Governments, concerned with keeping jobs alive in the fishing industry in the short-term, are essentially paying people to extinguish their own long-term job prospects – not to mention the effect on the next generation of fishermen. Artisanal fishing catches half the world’s fish, yet it provides 90% of the sector’s jobs.\nClearly, industrialised countries are not about to return to traditional methods. However, the disastrous management of the industry needs to be reformed if we are to restore fisheries to a sustainable level. In the EU alone, restoring stocks would result in greater catches of an estimated 3.5 million tonnes, worth £2.7 billion a year.\nRather than having a system in which the EU members each hustle for the biggest quotas – which are already set far beyond what is sustainable – fisheries experts suggest individual governments should set quotas based on stock levels in their surrounding waters. Fishermen should be given responsibility over the fish they hunt – they have a vested interest in seeing stocks improve, after all – and this could be in the form of individual tradable catch shares of the quotas. Such policies end the tragedy of the commons situation whereby everyone grabs as much as they can from the oceans before their rival nets the last fish, and it’s been used successfully in countries from Iceland to New Zealand to the US. Research shows that managing fisheries in this way means they are twice as likely to avoid collapse as open-access fisheries.\nIn severely depleted zones, the only way to restore stocks is by introducing protected reserves where all fishing is banned. In other areas, quota compliance needs to be properly monitored – fishing vessels could be licensed and fitted with tracking devices to ensure they don’t stray into illegal areas, spot-checks on fish could be carried out to ensure size and species, and fish could even be tagged, so that the authorities and consumers can ensure its sustainable source.\nThe other option is to take humanity’s usual method of dealing with food shortages, and move from hunter-gathering to farming.\nAlready, more than half of the fish we eat comes from farms – in China, it’s as high as 80% – but doing this on an industrial scale has its problems. Farms are stocked with wild fish, which must then be fed – larger fish like salmon and tuna eat as much as 20 times their weight in smaller fish like anchovies and herring. This has led to overfishing of these smaller fish, but if farmed fish are fed a vegetarian diet, they lack the prized omega-3 oils that make them nutritious, and they do not look or taste like the wild varieties. Scientists are working to create an artificial version of omega-3 – current synthetic omega-3 versions are derived from fish oils.\nFish farms are also highly polluting. They produce a slurry of toxic run-off – manure – which fertilises algae in the oceans, reducing the oxygen available to other species and creates dead zones. Scotland’s salmon-farming industry, for example, produces the same amount of nitrogen waste as the untreated sewage of 3.2 million people – over half the country’s population. As a result, there are campaigns to ban aquaculture from coastal areas.\nFarmed fish are also breeding grounds for infection and parasites that kill off large proportions of fish – escapees then frequently infect wild populations. Farmers try to control infestations with antibiotics, but usually only succeed in creating a bigger problem of antibiotic resistance.\nHumanity is not limiting its impacts to fish most commonly found on menus. Exotic sea creatures from turtles to manta ray to marine mammals are being hunted to extinction. Shark numbers, for example, have declined by 80% worldwide, with one-third of shark species now at risk of extinction. The top marine predator is no longer the shark, it’s us.\nA decline in shark numbers has a significant impact on the marine ecosystem: it can lead to an increase in fish numbers further down the food chain, which in turn can cause a crash in the population of very small marine life, such as plankton. Without the smallest creatures, the entire system is threatened.\nOne of the repercussions, which I have discussed before, is an increase in jellyfish numbers, but overfishing, pollution, climate change and acidification also affect the marine ecosystem. Warmer waters are pushing species into different habitats, causing some to die off and others to adapt by creating entirely new hybrid species. Meanwhile, trawlers are netting bycatch that include marine mammals and even seabirds – as many as 320,000 seabirds are being killed annually when they get caught in fishing lines, pushing populations of albatrosses, petrels and shearwaters to the edge of extinction.\nSome solutions are easier than you might think. Seabirds can be protected by using weighted lines and scaring off birds with lines that have flapping streamers attached – these methods alone have reduced seabird deaths by more than 85-99% where they are used.\nStrengthening and expanding protected marine reserves would also go a long way to conserving species. Currently, less than 1% of the ocean is protected, although by 2020, the international community has agreed to raise this to 10%. Reserves, when properly patrolled and monitored, do protect marine life, and nation after nation is stepping up to the plate. The tiny Pacific islands have banded together to create a giant protected area of 1.1 million square kilometres, for example. Not to be outdone, Australia has created the world’s biggest protected area, and countries around the world from Britain to New Zealand are joining the effort.\nBut useful as they are, marine reserves – often around points like coral reefs or rock islands – are only effective if governments have the resources to patrol and protect them. Also, many marine creatures, from whale sharks to whales, are migratory – they don’t stay in the protected areas, making them easy prey for fishermen. What’s needed, many argue, are mobile reserves that follow migratory animals, and those that shift habitat due to currents or climate phenomena like El Nino.\nThe zones need to be well-targeted and needn’t impact on fishermen’s livelihoods. For example, one study found that designating just 20 sites – 4% of the world’s oceans – as conservation zones could protect 108 species (84%) of the world’s marine mammals.\nThe rivers in many European cities were so overfished, polluted and dammed up by the mid 20th century that they emptied of fish, and many species went locally extinct. But thanks to clean-ups, riverbank restoration and fishing restrictions, fish are returning to waterways, even in inner cities. A decade ago, few people would have imagined that salmon would return to my local river, the Thames. If it is possible to bring back fish to ‘dead’ rivers, there is surely hope for the world’s oceans.\nAuthor: Gaia Vince\nReposted from the BBC.by']	['<urn:uuid:560d23c7-408e-4dd6-b60f-e0a92e6f402d>', '<urn:uuid:e12215c5-71bc-41c4-8c96-2a127a2913a4>']	factoid	direct	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:58:04.490895	27	103	3390
82	What materials do I need to build a garden bridge?	To build a garden bridge you need several materials: 4x4-inch support posts (6 feet long), 2x8-inch boards (8 feet long), galvanized nails (8d and 6d), carriage bolts with nuts and washers, gravel, 2x6-inch boards (10 feet long and 8 feet long), and 2x10 or 2x12 beams for stringers. You'll also need tools like a post hole digger, circular saw, and drill bits. Optional materials include paint or wood stain for protection and finishing.	['A simple wooden bridge over a garden pond infuses a bland back yard with charm. In addition to aesthetic appeal, a bridge makes maintenance and gardening tasks more manageable by providing easier access to the center and the far side of the pond. If the bridge will span less than 8 feet, a homeowner with reasonable do-it-yourself carpentry skills can easily build a basic wooden footbridge complete with a handrail, using pressure-treated wood and standard home workshop tools. A short, level footbridge requires no center support posts, eliminating the complication of placing footings in the water.\nPrepare the Footings\nDetermine and mark the positions for the six posts that will support the ends of your bridge. Position three posts approximately 2 feet apart on one side of the pond. Use planks to help position the remaining three posts directly opposite them on the other bank no more than 7 feet away.\nDig an 8-inch-diameter post hole at least 30 inches deep at each position. Pour 6 inches of gravel into each hole. Tamp the gravel. Center a post in each hole with 42 inches above the ground surface. Brace each post with scrap lumber to keep it plumb.\nMix concrete according to package directions. Pour mixed concrete around each post to 1 inch above the ground surface. Tamp the concrete to pack it firmly around the posts and slope it away from the posts in each direction to drain water away from the wood. Place a plank across the tops of the posts on each side and across pairs of posts across the pond to ensure they are level. Allow the concrete to cure completely.\nCut the center support post on each side of the pond at 12 inches above the ground. Lay a plank across the pond, resting it on top of the two cut posts to check that they are level.\nBuilding the Bridge\nStack two 8-foot-long 2-by-8-inch boards on top of each other with 8-inch surfaces together. Align the edges and glue the boards to each other. Secure the boards to each other with 8d galvanized nails staggered along their full length. Repeat with the remaining two pairs of boards to make three support beams, 4-by-8-inch by 8 feet long. These beams will lay across the pond and provide a surface to attach the wooden treads.\nLay one beam across the pond, positioning it inside one of the outer pairs of support posts with the 8-inch face against the post and the bottom edge 4 inches above the ground. Drill holes with a five-eighths-inch bit through the beam and post and secure the beam to the post with 7 1/2 inch by one-half-inch carriage bolts. Repeat with the other side beam. Position the center support beam on one side of the center support posts with the 4-inch face even with the top of the post and bolt it in place.\nCut nine 10-foot boards into two 5-foot sections each. Paint or stain the boards, if desired, and allow them to dry.\nPosition the first board across the support beams at one end of the bridge. Allow a one to two inch overhang on each side. Nail the board in place with 6d galvanized nails. Continue nailing boards across the support beams, leaving about one-quarter inch between the boards until you have completed the floor of the bridge.\nMeasure and mark a position 1 1/2 inches from the top and 1 1/2 inches from the edge on each of the four outside support posts. Mark a second position 3 inches from the top and 1 1/2 inches from the opposite edge on each post. Drill holes through the post at each position with a one-half-inch bit. Mark and drill matching holes at each end of two 2-by-6-inch, 8-foot-long boards.\nSecure one board to the inside of each pair of support posts using three-eighth-inch by 6-inch carriage bolts through the holes you just drilled to make handrails for the bridge.\nThings You Will Need\n- Post hole digger\n- 6 4-by-4-inch support posts, 6 feet long\n- 6 cubic feet fine gravel\n- 6 2-by-8-inchboards, 8 feet long\n- 1 lb. 8d galvanized nails\n- 12 carriage bolts, 1/2-inch by 7 1/2-inch, with nuts and washers\n- 5/8-inch spade bit\n- 9 2-by-6-inch boards, 10 feet long\n- Circular saw\n- Paint or stain, if desired\n- 3 lbs. 6d galvanized nails\n- 2 2-inch by 6-inch boards, 8 feet long\n- 1/2-inch spade bit\n- 8 carriage bolts, 3/8-inch by 6-inch, with nuts and washers\n- Working with a helper will make it easier to position the support beams and the boards for the handrails.\n- Smart Guide: Ponds, Gardens and Waterfalls; Editors of Creative Homeowner\n- Ron Hazelton Home Improvement Online: How to Build a Wooden Foot Bridge\n- Jupiterimages/liquidlibrary/Getty Images', 'This diy woodworking project is about garden bridge plans. In this article we show you all you need to know about the construction of a decorative garden bridge, including the tools and materials required for the job. There are many shapes and sizes you could choose from, therefore we recommend you to plan everything from the very beginning. If you want to get the job done in just an weekend, a small scale project is perfectly for your needs.\nYou need o buy proper materials for this project, therefore we recommend you to invest money in quality materials. The bridges construction requires two stringers built from 2×10 or 2×12 beams, so make sure they are perfectly straight and don have any visible flaw. Drill pilot holes in the components, at least 1/2” from the edges, if you want to prevent the wood from splitting.\nProjects made from these plans\n- A – 20 pieces of 2×10 lumber 120” long STINGERS\n- B – 30 pieces of 2×4 lumber 36” long SLATS\n- C – 2 pieces of 2×3” 37 1/2” long POSTS\n- D – 1 piece of 2×10 lumber 60”long RAILINGS\n- E – 2 piece of 2×3 lumber 35” long BALUSTERS\nDiy Garden Bridge Plans\nBuilding a garden bridge is a nice project, that could add value to your property and enhance the look of your garden. If a stream of water runs along you property or if you just want something unique, you should consider building an arched wooden bridge.\nTop Tip: If you want to save some money and to learn new carpentry skills, you should build the bridge by yourself. The most important part of the project is cut the stringers properly, therefore you have to draw the cut lines in a professional manner.\nFree Garden Bridge Plans\nFirst of all, you need to build the stringers of the bridge, made of 2×10 or 2×12 beams. In order to draw the arches, you could either free-hand trace the arch or use nails, a string and a pencil. As you can imagine, the second method is easier, provided you place the nails properly.\nLeave at least 15” from the end of the boards, in order to obtain a good support. If you don’t build the supports, it won’t be able to support the weight.\nAfter you draw the cut lines on the beams, you should make the cuts, using a jigsaw with a sharp blade. Make sure the blade goes exactly over the cut lines, otherwise the stringers won’t have the right shape.\nTop Tip: After you perform the cuts, you should sand the edges with medium-grit sandpaper. Work with great care and attention, if you want to obtain a professional result.\nThe next step of the woodworking project is to install the 2×4 slats. First of all, you need to secure the frame of the arched bridge in several places, as in the image. In this manner, you can make sure the stringers are plumb and equally-spaced.\nTop Tip: Place the slats at both ends of the bridge, making sure they overhang at least 2”. Drill pilot holes in the slats and insert several 3” deck screws.\nInstall the rest of the slats, in the same manner as described above. Make sure the slats are equally spaced, leaving at least 1/2” between them. Drill pilot holes in the slats, at least 1/2” from the edges, to prevent the wood from splitting.\nTop Tip: In order to make the whole structure more rigid, we recommend you to install several slats under the top of the bridge. Work with great judgement and patience, if you want to obtain a professional result.\nBuilding railings for the arched bridge is an optional step, but it would definitely add character to the construction. If you want to build a simple rail for the project, you could attach it on one side of the bridge.\nBuild the top of the railings in the same manner as the stringers, but round the edges with a router and with a 1/4” bit. Plumb the rails with a spirit level, before securing it to the stringers with lag screws.\nAfter you have built the garden bridge, you should take care of the finishing touches. Fill the head of the screws with wood putty and let it dry out thoroughly. Afterwards, sand the wooden surface with 120-grit sandpaper.\nTop Tip: Protect the lumber from rot and water damage, by applying several coats of wood stain or paint. Work with great care and good judgement, if you want to build a bridge with a nice appearance.\nThis woodworking project is about diy garden bridge plans. If you want to see more outdoor plans, check out the rest of our step by step projects and follow the instructions to obtain a professional result.']	['<urn:uuid:2e5d5524-d84e-412d-86e9-49088859035a>', '<urn:uuid:9cc54e88-f75a-48ce-82b2-16ee09252193>']	open-ended	direct	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T20:58:04.490895	10	73	1610
83	I've heard that when big asteroids hit Earth they can affect the weather. What happened to Earth's climate after the dinosaur-killing asteroid impact?	The asteroid impact had severe long-term effects on Earth's climate. When it struck, it cast rocky slurry, soot, and sulfur gases into the atmosphere. According to current estimates, this could have cooled the global climate by up to 10 degrees Celsius (18 degrees Fahrenheit) for several decades. Additionally, some of the atmospheric sulfur fell back to Earth as acid rain. The impact was particularly devastating because the asteroid struck an area rich in sulfur and other materials that, once thrown into the atmosphere, dramatically altered the global climate and ecology. Had the asteroid struck a different location, the consequences for life on Earth might have been very different.	"[""Sixty-five million years ago, life on Earth suffered one of the worst mass extinctions of all time. It was an event that killed creatures across the spectrum of life's diversity, from tiny marine invertebrates to the largest dinosaurs, but what could have caused it?\nA number of hypotheses have been forwarded over the years, most of which have focused on dinosaurs. It would take an entire book to discuss them all. Depending on who you ask, the non-avian dinosaurs succumbed to disease, nest-raiding mammals, hungry-hungry caterpillars, or simply became too big to survive, but over the past three decades most paleontologists have agreed that the impact of an asteroid in what is now the Yucatan Peninsula played a major role in the end-Cretaceous extinction. A collaboration by more than 40 scientists published last week in the journal Science reaffirms this hypothesis.\nThe end of the Cretaceous was a time marked by catastrophic geological events. Not only did a chunk of extraterrestrial rock strike the earth, but, prior to the impact, a group of volcanoes in India known as the Deccan Traps were undergoing massive eruptions. These events have been well established through geological evidence, but the question is what roles they might have played in the extinction of so many kinds of organisms at the end of the period. This is what the international team behind the Science paper wanted to determine.\nAfter looking at a variety of sites recording the end of the Cretaceous and the beginning of the next period, the Paleogene, the scientists determined that the asteroid impact occurred at the boundary between the two (and not hundreds of thousands of years earlier, as some geologists have recently proposed). This is important because the timing of the geological event must be tied to the record of species extinction seen in the fossil record, and the present study suggests that the impact and the extinctions are closely associated. The consequences of the eruptions of the Deccan Traps should not be ignored, but it appears that they do not fit the pattern of mass extinction as well as the asteroid impact.\nBut what exactly happened that resulted in the deaths of so many kinds of organisms? According to the authors, the initial impact would have triggered massive earthquakes in the region and sent enormous tidal waves to the shore. From a distance it would have looked like a bomb going off, with the impact throwing a mixture of scalding air, material from the asteroid and fragments of the Earth's crust. The matter exploded with so much force that some probably escaped into space. Some of this material landed in parts of the globe far from the center of impact, and while they were not hot enough to start forest fires (as was previously thought) the bits and pieces could have heated things up in habitats all around the world.\nYet some of the most devastating effects of the impact would not be felt immediately. Among the rocky slurry cast up into the atmosphere were soot and sulfur gases which, by current estimates, could have cooled the global climate by up to 10 degrees Celsius (18 degrees Fahrenheit) for several decades, and some of that atmospheric sulfur precipitated back down in the form of acid rain. Indeed, the most significant part of the event was not the shock of impact but the fact that the asteroid struck an area rich in sulfur and other materials that, once thrown up into the atmosphere, drastically changed the global climate and ecology. Had the asteroid struck somewhere else on the planet the consequences could have been very different for life on earth.\nThis does not mean that the details of the end-Cretaceous extinction have been all wrapped up, however. Most of what we know about the extinction comes from North America, but we still don't know very much about what was going on elsewhere in the world. To draw an analogy with forensics, scientists have identified the weapon used in the massacre, but doing so is only a small part of fully understanding what happened.\nSchulte, P., Alegret, L., Arenillas, I., Arz, J., Barton, P., Bown, P., Bralower, T., Christeson, G., Claeys, P., Cockell, C., Collins, G., Deutsch, A., Goldin, T., Goto, K., Grajales-Nishimura, J., Grieve, R., Gulick, S., Johnson, K., Kiessling, W., Koeberl, C., Kring, D., MacLeod, K., Matsui, T., Melosh, J., Montanari, A., Morgan, J., Neal, C., Nichols, D., Norris, R., Pierazzo, E., Ravizza, G., Rebolledo-Vieyra, M., Reimold, W., Robin, E., Salge, T., Speijer, R., Sweet, A., Urrutia-Fucugauchi, J., Vajda, V., Whalen, M., & Willumsen, P. (2010). The Chicxulub Asteroid Impact and Mass Extinction at the Cretaceous-Paleogene Boundary Science, 327 (5970), 1214-1218 DOI: 10.1126/science.1177265""]"	['<urn:uuid:1b010bcf-1139-4c7a-9c65-61b94f5cdb64>']	open-ended	with-premise	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:58:04.490895	23	108	783
84	As an outfielder, when do I get credit for helping defense?	An outfielder records an assist when he throws the ball into the infield and an out is recorded as a result. These outfield assists typically come from throws directly to a base, without an infielder's help.	['- 1 What is an assist in MLB?\n- 2 Whats the difference between a put out and an assist?\n- 3 What position gets the most assist in baseball?\n- 4 How does an outfielder get an assist?\n- 5 What are the numbers of baseball positions?\n- 6 What does G mean in baseball stats?\n- 7 What catcher has thrown the most runners?\n- 8 Does a catcher get an assist on a strikeout?\n- 9 What is it called when an outfielder throws out a runner?\n- 10 Is a passed ball an error?\n- 11 Why do catchers have so many putouts?\n- 12 How is it possible to have a four strikeout inning?\n- 13 What is a baseball shift?\n- 14 What’s the difference between at bats and plate appearances?\nWhat is an assist in MLB?\nDefinition. An assist is awarded to a fielder who touches the ball before a putout is recorded by another fielder. Typically, assists are awarded to fielders when they throw the ball to another player — but a fielder receives an assist as long as he touches the ball, even if the contact was unintentional.\nWhats the difference between a put out and an assist?\nA putout (PO) is credited to a fielder who gets an offensive player out as described in scoring rule 10.09. An assist (A) is credited to fielders who contribute to an offensive player being out as per scoring rule 10.10.\nWhat position gets the most assist in baseball?\nFor example, a shortstop might field a ground ball cleanly, but the first baseman might drop his throw. In this case, an error would be charged to the first baseman, and the shortstop would be credited with an assist. Rabbit Maranville is the all-time leader with 8,967 career assists.\nHow does an outfielder get an assist?\nAn outfielder records an assist when he throws the ball into the infield and an out is recorded as a result. Outfield assists are one of the most commonly referenced types of assists. Outfield assists often result from throws directly to a base, without the help of an infielder.\nWhat are the numbers of baseball positions?\nEach position conventionally has an associated number, for use in scorekeeping by the official scorer: 1 (pitcher), 2 (catcher), 3 (first baseman), 4 (second baseman), 5 (third baseman), 6 (shortstop), 7 (left fielder) 8 (center fielder), and 9 (right fielder).\nWhat does G mean in baseball stats?\nExtra-base Hit (XBH) Games Played (G) Grand Slam (GSH) Ground Into Double Play (GIDP) Groundout-to-Airout Ratio (GO/AO)\nWhat catcher has thrown the most runners?\nIván Rodríguez is the all-time leader in putouts at the catcher position with 14,864 career putouts. Yadier Molina (14,552) is second all-time and the only other catcher to record 14,000 or more career putouts.\nDoes a catcher get an assist on a strikeout?\nAn assist is also credited if a putout would have occurred, had another fielder not committed an error. If a pitcher records a strikeout where the third strike is caught by the catcher, the pitcher is not credited with an assist.\nWhat is it called when an outfielder throws out a runner?\nFielder’s choice is defined in MLB Rule 2, “Definitions”, as “the act of a fielder who handles a fair grounder and, instead of throwing to first base to put out the batter-runner, throws to another base in an attempt to put out a preceding runner.” FC is recorded for the batter-runner if he reaches first base safely\nIs a passed ball an error?\nA passed ball is not recorded as an error, but when a run scores as the result of a passed ball, it does not count as an earned run against a pitcher. If a runner advances on a passed ball, he is not credited with a stolen base.\nWhy do catchers have so many putouts?\nThe catcher receives a putout if the batter is automatically out for batting illegally, bunting foul for a third strike (unless the foul ball is caught on the fly), striking out with first base occupied and less than two out (even if the catcher fails to catch the ball), being touched by his own batted ball, batting\nHow is it possible to have a four strikeout inning?\nThe number of four-strikeout frames — made possible when a hitter takes first base after a third strike is dropped by the catcher plus either first is vacant or there are two outs — has grown exponentially in the 21st century.\nWhat is a baseball shift?\nA defensive shift occurs when the fielders move from their normal positions for some tactical reason. The most common shifts are used in response to specific game situations, such as a runner on base, and are seen in almost every game.\nWhat’s the difference between at bats and plate appearances?\nDefinition. An official at-bat comes when a batter reaches base via a fielder’s choice, hit or an error (not including catcher’s interference) or when a batter is put out on a non-sacrifice. (Whereas a plate appearance refers to each completed turn batting, regardless of the result.)']	['<urn:uuid:ec1d8a61-ad8e-4415-85b4-7e8902e2d32a>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:58:04.490895	11	36	860
85	What treatments help with sea urchin spine wounds?	Treatment involves several steps: First, remove all visible spines promptly as they can continue causing envenomation. Submerge the wounded area in hot water (40-46°C) for 30-90 minutes until pain reduces. The wound should be irrigated and debrided. Waxing and stripping the area may help remove residual spines, and ammonia solution can dissolve small fragments. If infection develops, antibiotics like Ciprofloxacin, Trimethoprim/sulfamethoxazole or Doxycycline should be used for 7-14 days. Tetanus vaccination is recommended.	"['Marine wounds and stings\nHazards from the marine environment include injuries, poisonous stings and secondary infection. These may result in severe pain, illness and even death. The main culprits are coelenterates, mollusca, echinoderms and poisonous fish.\nThe two most important mollusca are octopus and cone shell.\nVenomous fish include some species of stingrays and Scorpaenidae. There are many different species in the Scorpaenidae family. They vary in the size of their spines and the potency of the venom released through these spines.\nJellyfish commonly have stinging nematocysts. They are sometimes known simply as sea nettles, though often this name is reserved for the less dangerous varieties. Structurally jellyfish comprise a bell-shaped body with tentacles, some up to 30 metres in length. The nematocysts or cnidoblasts reside within theses tentacles. Stings are delivered when contact is made with the tentacles and nematocysts discharge into the skin.\nDeserving special mention are:\n- Box jellyfish or sea wasp (Chironex fleckeri) (Northern Australian waters, where they are known as ""stingers"")\n- Portuguese man of war (Physalia physalis) (Atlantic, Pacific, Indian Ocean)\n- Irukandji (Carukia barnesi); these have small box-shaped transparent bodies with 4 tentacles\nJellyfish are most prevalent in calm warm seawater, sandy beaches and harbours during the summer months. However, they are also seen in other circumstances.\nClinical presentation of jellyfish stings\nThe first symptom is of pain, often so severe as to lead to loss of consciousness. Some victims of the box jellyfish are unable to make it to the beach alive. Other symptoms include:\n- Paraesthesia (tingling and prickling) and itching\n- Red bumps and patches, often in a ladder-like pattern\n- Sweating in affected area (irukandji)\n- Nausea and vomiting\n- Muscle pain and cramps.\nDepending on the amount of stinging and the type of jellyfish encountered, shock and cardiac arrest can follow.\nAfter recovering from the initial sting, the wound may later blister and become necrotic (tissue death).\nTreatment of jellyfish stings\nIf you or a companion is stung by a jellyfish:\n- Avoid moving the affected limb. Do not rub the sting area.\n- Remove nematocysts and tentacles from the skin using gloves and forceps. Razor blades can be used to shave off nematocysts.\n- If stung by box jellyfish apply liberal amounts of vinegar (5% acetic acid), or in its absence, salt water or hot water (40C) for 30 minutes. Seek immediate medical attention.\n- If stung by the Portuguese man of war, thoroughly rinse exposed areas with seawater (vinegar is not helpful).\n- Ice or an ice pack can help with pain while seeking medical attention.\n- Antivenin injections are available in some medical centres for specific jellyfish, and are particularly recommended for box jellyfish stings.\n- Topical corticosteroid creams and oral antihistamines may be of benefit for persistent dermatitis\nDo not rub with a towel or use fresh water as this may cause further damage by causing further nematocyst discharge. Contrary to popular belief urine and alcohol can exacerbate injury and are not recommended.\nCoral cuts and abrasions are common and may become infected. Coral may also harbour stinging nematocysts.\nTreatment of coral wounds involves removing visible debris and cleaning the wound thoroughly and applying antiseptic or antibiotic cream as infection is common (watch out for redness, fever, foul smell or pus).\nTetanus immunisation is recommended.\nMost species of octopus are harmless, except for the blue-ringed octopus largely found around the coast of Australia. This is identified by iridescent blue rings that appear on its body when disturbed. Large quantities of its venom may cause paralysis but mortality is rare.\nTreatment is supportive, and the wound should be irrigated heavily. Artificial ventilation may be required in cases of paralysis.\nThe cone shell is a carnivore, living just beneath the sandy surface of shallow and deep water areas. Eighteen species of cone shell may sting humans, via a lightening-fast barb injection. The toxins target an array of ion channels, which cause paralysis in small creatures. Human fatalities are rare. Typically the sting results in local pain and numbness, and may be accompanied by general malaise.\nTreatment of cone shell stings may include:\n- Immobilisation of affected area in a dependent position (hanging down)\n- Pain relief (soaking the area in hot water may help)\n- Emergency resuscitation and respiratory support\nNo antivenin is available for cone shell stings.\nThe sea urchin is an echinoderm. It is a common source of puncture wounds because the individual often inadvertently steps on it. Local irritation is caused by embedded spines, which are often hidden by pigment release in the skin. Sea urchins are usually non-venomous, though some are capable of delivering toxin.\nTreatment should be directed at removing as many visible spines as possible.\n- Waxing and stripping the area may help remove residual spines\n- Ammonia solution can dissolve small spine fragments\n- Most thin embedded spines come out by themselves within a few weeks\nAn injury from a venomous fish results in very painful and inflamed puncture site. The venom of a stingray is found at the base of its tail.\nComplications of an injury from venomous fish may include:\n- Secondary infection leading to wound breakdown\n- Nausea, vomiting, shortness of breath and weakness\n- Death (rare).\nTreatment of injury by venomous fish\n- Carefully remove visible spines\n- Apply direct pressure to bleeding areas\n- Bathe the affected area in hot water of up to 45 degrees Celsius.\n- Tetanus prophylaxis should be considered.\n- Stonefish antivenin is available in some regions but may cause anaphylaxis (allergic reactions) so a test dose is recommended.\nSecondary infection after venomous fish injury\nSecondary bacterial infection is common after penetrating or abrasive marine injuries. Signs of secondary infection include redness, swelling and warmth.\nVibrio vulnificus infection is peculiar to marine injuries and may cause intense rapidly progressive cellulitis and death. Treatment should include tetracycline and a broad spectrum cephalosporin or aminoglycoside.\n- Bolognia JL, Jorizzo JL, Rapini RP. Dermatology 2003. Pg1346-8\n- Sivaprakasam K. Jellyfish dermatitis. Indian J Dermatol Venereol Leprol [serial online] 2015 [cited 2015 Jul 3];81:389-90. Available from: http://www.ijdvl.com/text.asp?2015/81/4/389/156198\nOn DermNet NZ:\n- Medsdcape Reference:\nBooks about skin diseases:\nSee the DermNet NZ bookstore', 'Sea urchins are part of the phylum Echinodermata which also includes starfish. Sea urchins have globular bodies covered by calcified spines. The spines are either rounded at the tip or hollow for envenomation. The also can have pedicellariae that can grasp and envenomate, typically with more venom than in the spines. Echinoderms possess a distinctive endoskeletal tissue called stereom, which is composed of calcite organized into a mesh-like structure, in addition to dermal cells and fibers. Stereom forms structural elements that can embed into human tissue as spines. When stepped on, these spines cause painful puncture wounds with immediate pain, bleeding, and edema. They can cause severe muscle ache which can last up to 24 hours.\nSea urchin envenomation occurs when a spine penetrates soft tissue. Frequently spines can break off into the victim. A retained spine can cause tenosynovitis or a granuloma. It can also cause systemic symptoms such as nausea, vomiting, paresthesias, weakness, abdominal pain, syncope, hypotension, and respiratory distress.\nEchinoderm envenomation does not represent a significant public health problem although little epidemiologic data are available.\nThere are many species of sea urchins in all oceans. Marine envenomation by sea urchins can happen to swimmers, fishermen, divers, surfers. Most incidents of envenomation occur in tropical and subtropical waters, and are most common among divers, especially in shallow water near rocky shores. The American Association of Poison Control Centers’ 2010 and 2011 annual reports document approximately 1800 aquatic exposures in the United States yearly, with approximately 500 treated.\nUrchin venoms are comprised of various toxins including glycosides, hemolysins, proteases and can be mixtures of high-molecular-weight proteins and low-molecular-weight compounds such as histamine, serotonin, and bradykinin.In many cases, envenomation leads to mast cell degranulation, disruption of cell metabolism, interference with neuronal transmissions, and myocardial depression. Subsequently, envenomation can cause significant pain, dermatitis, paralysis, cardiovascular collapse, and respiratory failure.\nContact with sea urchin spines and envenomation may trigger a vigorous inflammatory reaction and can proceed to tissue necrosis.\nUrchin venoms contain steroid glycosides, hemolysins, proteases, serotonin, and cholinergic substances.\nA typical history will involve an individual who accidentally stepped on a sea urchin spine while in shallow water within the past 24 hours. The patient will describe an immediate, incapacitating burning pain which localizes to the puncture wound. This burning may last several hours, and wound pressure exacerbates it. There may be systemic symptoms such as nausea and vomiting, paresthesias, weakness, abdominal pain, syncope, hypotension, and respiratory distress. On physical exam, there may be bleeding, edema, erythema, and warmth at and surrounding the puncture site. A spine may or may not be visible. In some instances, there may exist dark blue or black pigmentation (temporary tattooing) to surrounding tissues from dark-colored spines.\nFollowing the severe burning pain, localized edema, erythema, warmth, and bleeding may develop. In severe cases, nausea, vomiting, paresthesias, muscular paralysis, and respiratory distress may occur. Delayed sequelae include wound tattooing as the pigment is leeched from dark-colored spines into the surrounding tissue, synovitis if the spine violates a joint space, secondary wound infection, or granuloma formation if there is retention of foreign material.\nThere are no specific laboratory tests indicated in the management of echinoderm envenomation.\nRadiography, ultrasound, computed tomography, or MRI may be helpful in spine localization and removal.\nRemoval of all visible spines should be prompt, as they can continue to cause envenomation even when detached from the urchin body. The wounded area should be submerged in hot water (40 C - 46 C), as tolerated by the patient, for 30-90 minutes, or until there is substantial pain relief. Care should be taken not to cause thermal burns. Oral analgesia should also is a consideration. Tetanus vaccine status should be updated. Occasionally, local anesthetic infiltration or a nerve block may be a useful adjunct to pain relief. Wounds are to be irrigated, debrided, and explored for retained spines. If spines are easily visualized and reached, they should be removed. Even in the absence of a spine, a dark discoloration may indicate dye in the tissues. If this is the case, the discoloration should resolve within 48 hours. If spines have entered a joint or are close to neurovascular structures, the joint may require splinting, and an appropriate consult to surgery made. If the patient experiences reactive neuropathy, it may respond to a systemic corticosteroid. In addition to the initial pain and tissue damage, secondary infections are common. If there are some retained spines, granulomas may develop, which may require excision. Arthritis from retained spines may also develop, which may require synovectomy.\nNo antivenoms are currently available for Echinoderm species. Treatment is supportive.\nProphylactic antibiotics are typically not indicated, except in persons with deep wounds, persons with significant morbidities, or those who are immunocompromised. Once the presence of an infection is established, therapy must be instituted, to include coverage for potential marine pathogens. Common organisms associated with marine trauma include Staphylococcus and Streptococcus species, Vibrio vulnificus, and Mycobacterium marinum.\nIf antibiotics coverage is warranted, treatment should be for 7-14 days duration. Oral Ciprofloxacin 500 mg PO BID, Trimethoprim/sulfamethoxazole 160/800 mg PO BID, or Doxycycline, 100 mg PO BID can be used.\nBroad-spectrum parenteral antibiotics are indicated for severe wound infections or sepsis.\nCellulitis - Inflammation of skin and soft tissue following an Echinoderm sting may resemble cellulitis. A history of stepping on a spine and immediate pain following the event will rule out simple cellulitis.\nContact Dermatitis - If the patient presents several days after the inciting event with a more progressive infection, it can resemble the pustular appearance of contact dermatitis. A history of stepping on a spine in the ocean should rule out this diagnosis.\nAnaphylaxis - Echinoderm stings can cause systemic symptoms that can mimic anaphylaxis. On initial presentation, if an unstable patient is unable to provide a history, treatment is supportive.\nStingray and Starfish envenomation are also differentials.\nManagement is supportive, and there is no specific toxicity or side effects, other than potential antibiotic side effects.\nLocal and systemic effects are both possible following echinoderm envenomation. However, there is no clear link between echinoderm envenomation and death found in the current literature.\nSystemic symptoms such as cardiovascular collapse and cellulitis are the most common complications.\nIf surgical debridement is needed, the standard of care should apply.\nIf the spine has entered a joint or is close to a neurovascular structure, the patient should receive a surgical referral.\nAdvise patients to be vigilant and alert of their surrounding, especially when they are swimming or walking in a large body of water. Most injuries caused by sea urchins result from inadvertently stepping on a spine. Wading barefoot, especially at night, should be avoided. Shoes and diving gear offer some protection but are easily penetrable by a sharp spine.\nUrchin toxins are heat labile and therefore hot water immersion is very effective in neutralizing these toxins and reducing the pain.\nA vital step in the appropriate treatment of an echinoderm envenomation is the identification of the actual event. Upon completion of this step, initiation of proper therapy can begin. It is imperative for physicians, nurses, and all caregivers to communicate effectively and to establish roles, to provide timely management of this ailment to the patient.\n|||Hornbeak KB,Auerbach PS, Marine Envenomation. Emergency medicine clinics of North America. 2017 May [PubMed PMID: 28411930]|\n|||Bottjer DJ,Davidson EH,Peterson KJ,Cameron RA, Paleogenomics of echinoderms. Science (New York, N.Y.). 2006 Nov 10 [PubMed PMID: 17095693]|\n|||Singletary EM,Rochman AS,Bodmer JC,Holstege CP, Envenomations. The Medical clinics of North America. 2005 Nov [PubMed PMID: 16227060]|\n|||Rossetto AL,de Macedo Mora J,Haddad Junior V, Sea urchin granuloma. Revista do Instituto de Medicina Tropical de Sao Paulo. 2006 Sep-Oct [PubMed PMID: 17086323]|\n|||Balhara KS,Stolbach A, Marine envenomations. Emergency medicine clinics of North America. 2014 Feb [PubMed PMID: 24275176]|']"	['<urn:uuid:e1b864d9-3fc6-4b90-b8e7-670b60bade04>', '<urn:uuid:58521ddf-e42b-4a12-b120-cea788978b52>']	factoid	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T20:58:04.490895	8	73	2318
86	vegetables succulents container planting requirements comparison	Vegetables and succulents have different container growing requirements. Vegetables can tolerate being crowded and thrive in various containers provided they have drainage, with examples showing high productivity like 236 peppers from just four plants in a 16-20 inch container. In contrast, succulents prefer shallower containers that dry out more quickly, and require specifically well-draining, porous soils with added gravel or expanded shale for drainage. Both plant types absolutely require proper drainage to prevent root rot, but succulents are more sensitive to overwatering and need specialized soil conditions.	"['Growing Food in Pots Can Be Easier Than Planting\nTwo of the hottest trends in gardening are containers and cultivating fresh food, and many savvy families are beginning to combine the two. They’re growing their vegetables in pots.\nImprovements in potting soils and fertilizers have made this a straightforward and generally successful exercise, said Pamela Crawford, a landscape architect from Canton, Ga., who has written four books about container gardening.\n""It’s so easy to put a tomato into a pot. It almost grows itself,"" Crawford said. ""It’s a whole different ballgame than putting one in the ground. There’s less weeding involved and fewer insects to fight. Container gardens are more productive and involve less work.""\nShapely pots and colorful plants are a great combination for growing edibles - especially where garden space is lacking. Containers can deliver the goods and in remarkably large quantities.\n""I’ve been able to harvest as many as 236 small spicy peppers all at once from four plants in a 16- to 20-inch container,"" Crawford said, referring to habaneros. ""I’ve also been able to get my fill of tomatoes from a pot that included a few ornamental sweet potato vines with their large root systems. It’s amazing how little ground space plants need to be productive. They can tolerate being crowded.""\nMixing flowers with ornamental vegetables makes good container sense. Cucumbers, coleus and begonias are eye-catching when planted together. Other great potted pairings include rosemary surrounded by lettuce, viola tucked among some long-legged broccoli plants and spinach growing alongside mounded chrysanthemums. Viola and chrysanthemum petals are edible, by the way.\nAdding flowers also extends the life of the combos, Crawford said. ""In many of my containers, with flowers that lived much longer than the vegetables, I simply cut the dead vegetable branches off and left the flowers to fill in the remaining space,"" she wrote in her latest book, ""Easy Container Combos; Vegetables and Flowers"" (Color Garden Publishing, 2010).\nMost any kind of container will serve, provided it has the necessary openings for drainage. Having too much water in the soil is almost as deadly as having too little. It rots the plant roots.\nBaskets are good choices. So are window boxes, kitchen totes and a whole range of recyclables including old barrels, buckets and milk jugs. Just be careful about colors and breathability.\n""I’ve had good experience with clay pots and plastic pots,"" said Joseph Masabni, an assistant professor and extension horticulturist with Texas A&M University. ""If you live in a hot area, I don’t recommend black or dark containers. They can overheat plants. I prefer clay because it breathes if it isn’t coated. (Plant) roots are never starved for oxygen.""\nVegetable gardening in containers is also a good way to involve children. The size of the project won’t be overwhelming, Masabni said.\n""Practicality is probably the major consideration. Older people who are still gardeners at heart but who live in apartments also can grow their fill of vegetables or small fruiting shrubs in pots,"" he said.\nEven people with plenty of land find growing in containers easier with certain produce, such as potatoes, he said.\n""There’s no digging required,"" Masabni said. ""Just tip the pots over and gather up the crop.""\nOn the Net\nFor more about growing vegetables in containers, see this University of Arizona Cooperative Extension fact sheet: http://ag.arizona.edu/pubs/garden/mg/vegetable/container.html\nYou can contact Dean Fosdick at firstname.lastname@example.org', 'Soil: Succulents and cacti thrive in well draining, porous soils. Gravel or expanded shale can be added to the bottom of the container to help increase drainage. … Never let the container sit in a saucer of water. If your container does not have a drainage hole, you will need to water less.\nOne may also ask, what type of container is best for succulents?\nThe best pots for succulents are made from terracotta or ceramic. Both of these materials are breathable, which encourages proper water drainage and air circulation. Just remember that both terracotta and ceramic are heavy, especially once you add soil and plants.\nMoreover, what do you put in the bottom of pots for drainage?\nChoose high-quality potting soil that is well draining. And if your plants need even more drainage, instead of putting gravel in the bottom of your pot, try mixing in perlite, PermaTill, or organic matter into your potting soil to increase drainage throughout the pot.\nAre pots without drainage holes bad?\nIf water does not have a way to drain freely, it gets trapped inside the pot and eventually deprives the roots of oxygen, creating roots rot, which is fatal to plants.\nAre shallow pots better for succulents?\nYou want enough room for the taproot to grow, but not so much room that the soil won’t dry out. Succulents and cacti generally prefer shallower containers, which dry out more quickly, resulting in healthier and happier plants.\nCan succulents be planted in plastic pots?\nIn fact, you can grow succulents just as easily in plastic pots as terracotta ones. … That might just keep a severely drought-stressed succulent alive a day or two longer. Of course, if you’re just a regular houseplant owner who pays attention to watering needs, you can use either one.\nAre plastic pots bad for succulents?\nPlastic does not insulate well and does not tend to stand up to extreme temperatures well either. Succulents planted in plastic pots and exposed to extreme temperatures are more likely to suffer the ill effects of the cold or heat than those planted in terracotta pots.\nWhy do some plant pots not have holes?\nWhy Do Pots Need Drain Holes? With the exception of a few aquatic plants, plant roots don’t like to sit in water. They need to exchange oxygen and carbon dioxide with the air, and excess water closes off the air pockets in soil. Plants in pots without drainage holes are prone to becoming overwatered.\nShould I put rocks in the bottom of my planter?\nA: For years, experts told gardeners to put a layer of gravel, pebbles, sand or broken pieces of pot in the bottom of the pot before potting up houseplants or outdoor plants. The idea was to improve drainage. But research shows that this advice is wrong. Water doesn’t travel well from one medium to another.\nHow Big Should drainage holes be in planters?\nYou need 1/4th inch drainage holes when using a planter that is 12 inches or less in diameter. You need 1/2 inch drainage holes when using a planter that is larger than 12 inches in diameter. The number of drainage holes you need would be between 3-8 for a planter that is 4-12 inches in diameter.']"	['<urn:uuid:e6b5008f-f271-4553-84d2-93afee15bbcd>', '<urn:uuid:ae39247e-2d9b-4e08-9333-0b7c11e145d9>']	open-ended	with-premise	short-search-query	similar-to-document	comparison	expert	2025-05-12T20:58:04.490895	6	87	1107
87	details specific technique how ojkanje singing voice shaking throat performed ancient croatian tradition	Ojkanje is characterized by a specific shaking of the voice that is achieved through a distinctive singing technique 'from the throat'. It can be performed either by a single singer or by a lead singer with accompaniment from another singer who performs the voice-shaking. This type of singing is specifically found in the area between the Rivers Krka and Cetina, including regions like Vrlika, Kijevo, and the Drniš and Šibenik hinterland.	"[""Folk Costume of Vrlika\nThe folk costume of Vrlika is one of Croatia's greatest national ethnographic treasures. Not only the most recognisable part of ethnographic heritage, but it is also part of history. The first weavers borrowed the basic ornament from the early Croatian stone monuments, which they skilfully converted to both decoration and tradition. And it has continued like this for centuries.\nThreads are strung together and colours are arranged, weaving and embroidering continues far into the night so that at the end everything would merge to form a unique folk costume for men and women, girls and children, a costume that has become an indispensable part of all ceremonies and important occasions, both religious and public. The chief characteristics of the mediaeval cultural inventory and tradition have been extremely well preserved in the folk costume of Vrlika.\nThe smooth and static patterns of the polychromatic embroidery create intricate ornamentation, very similar to that on the early Croatian stone monuments, and popular jewellery is almost identical to that found in the early settlement of Vrh Rika, so we can confirm that the folk costume of Vrlika has conveyed the main features of early Croatian cultural heritage to contemporary folk tradition, having preserved them until the very end of the second millennium.\nGuardians of the Grave of Christ\nThe ancient custom of guarding the grave of Christ has been preserved to this day in Vrlika. In Holy Week, a group of men, members of the Guardians of the Grave of Christ brotherhood, takes part in rituals and processions on Good Friday, guarding the grave of Christ, because of which they are called gravediggers. Not only a religious ceremony, but this custom has also become cultural-historical heritage of the Croatian Catholics of the Vrlika region.\nIt is all about a deep religious sentiment and dedication to guarding the Most Holy Body of Christ in the period from placing it into the grave to the Resurrection. The whole procedure of guarding the grave of Christ takes place in a very solemn atmosphere, with participants dressed in traditional folk costumes of Vrlika and with traditional weapons that had for centuries been used by the people of Vrlika to defend their homes and their holy Catholic faith and its principles.\nStrapping young boys and robust men add to the beauty of the mystery of the Passion of Christ, the Resurrection and victory. Their service is finished after Easter mass, when they dance the Kolo in front of church as a symbol of victory of Christ, resurrected from the dead, over evil and sin.\nNijemo Kolo of Vrlika\nThe silent circle dance of Vrlika, famous due to the history of the town, but also to perseverance of the Milan Begović cultural club, which aims to preserve the customs and traditions. The Nijemo Kolo is a folk dance most often performed in the Dalmatian hinterland. In November 2011, UNESCO included it on the Representative List of the Intangible Cultural Heritage of Humanity.\nThe Nijemo Kolo is a closed (not necessarily) circular dance, performed without music, although it can be either preceded or followed by vocal or instrumental performances. Nowadays it is most often performed on the occasion of religious or folk festivals.\nThe Nijemo Kolo of Vrlika is partly included in the Ero the Joker opera, composed by Jakov Gotovac, the libretto to which was written by Milan Begović. Here the dance is set to music and adapted for the opera. The rhythm of a variation of dancers of the Nijemo Kolo of Vrlika is accompanied in the opera by the rhythm of music and vocals.\nThe ojkanje is the oldest form of singing in Croatia. According to experts, it is a remnant of the pre-Slavic Balkan singing from ancient Dalmatia (today's Dalmatia, Bosnia and Herzegovina and Montenegro), which has been inherited and preserved by the Croatian people. In 2010, UNESCO's Intergovernmental Committee for the Safeguarding of the Intangible Cultural Heritage (ICH) included ojkanje singing on the List of Intangible Cultural Heritage in Need of Urgent Safeguarding.\nThe ojkanje is characterised by a specific shaking of the voice, achieved through a distinctive technique of singing ‘from the throat’. It is either performed by one singer alone or by the lead singer accompanied by another singer who performs the voice-shaking. Ojkanje two-part singing is found in the area between the Rivers Krka and Cetina, in Vrlika, Kijevo, the Drniš and the Šibenik hinterland. Not so long ago, people used this type of singing as a means of everyday communication while doing their chores or when travelling by horse-drawn carts, as an entertainment around the open fire during long winter nights as well as a way to pass the time while watching their grazing cattle.\nOjkanje singing owes its survival to organised groups of local folk singers who continue to transmit the skills and knowledge, performing at village festivals in Croatia as well as around the world. Nowadays, ojkanje is passed on and taught orally within organised folklore groups, but also through modern audiovisual media.""]"	['<urn:uuid:79151044-ac57-480b-a835-53a2ce3423b6>']	open-ended	with-premise	long-search-query	similar-to-document	single-doc	expert	2025-05-12T20:58:04.490895	13	71	837
88	How do Colorado mental health providers help underserved populations access care?	Mental health providers in Colorado employ multiple strategies to help underserved populations access care. Jewish Family Service (JFS), which serves metro Denver and Boulder, has developed partnerships with five health organizations to provide comprehensive behavioral health services to underserved patients. They offer a sliding fee schedule for those with limited financial resources and accept various insurance types including Medicaid, Medicare, and CHIP. Additionally, they are exploring telehealth services to help patients with transportation or mobility issues. For youth specifically, Colorado has implemented the I Matter program, which provides up to six free therapy sessions for young people 18 and under (or 21 if receiving special education services), helping address needs that may have resulted from the COVID-19 pandemic.	['Making a Difference for Coloradans\nCenter for Health Progress awarded HealthTeamWorks a year-long contract funded by the Kaiser Permanente Community Health Fund of The Denver Foundation to support six Safety Net Physical and Behavioral Health Centers across Colorado. Through the contract, HealthTeamWorks is providing technical support to these providers of behavioral health support, physical care for children and adults, dental care, vision screening and glasses, employment services, refugee support, a food pantry and more. Of the participating sites, in particular, Jewish Family Service (JFS), has made great strides in addressing gaps in behavioral health care for Medicaid patients.\nFounded in 1872, JFS is a nonsectarian, nonprofit human services agency serving metro Denver and Boulder. JFS provides quality mental health counseling to children and adults, helps seniors maintain a high quality of life, offers training and job placement to people with barriers to employment, and provides food and financial aid to individuals and families in crisis. Their team of 25 mental health professionals (including psychiatrists) offers support for those suffering from anxiety, depression, grief, post-traumatic stress, and other chronic mental health challenges. JFS provided treatment to more than 1,700 children, adolescents & adults in fiscal year 2018. JFS accepts, Medicaid, Medicare, Children’s Health Insurance Program (CHIP), private insurance and self-pay. A sliding fee schedule is available to those with limited financial resources.\nUnmet Behavioral Health Needs\nStacey Weisberg, JFS Director of Mental Health Services and Elena Pernitz, JFS Mental Health Operations Manager assessed and developed plans to address gaps of care for Medicaid patients. They found gaps in care were due largely to limited access to behavioral health providers as well as a need for more long-term therapy options for patients.\nFacilitated Needs Assessment and Planning\nThrough monthly on-site practice visits HealthTeamWorks’ Sr. Healthcare Facilitator, Shelli James, facilitates workflow studies, connections with community resources, implementation of telehealth, electronic health record capacity analysis, team building, and behavioral health integration to identify high need operational areas and tailor improvement programs for each site. The resulting improvement aims include addressing financial stability, maximizing the role of interns, using data to drive improvement, improving patient flow through the clinic, expanding behavioral health services, depression screening and follow-up processes, and increasing reach to the community.\nCreating Partnerships to FIll the Gaps\nJFS has identified five health organizations with which they have offered JFS’s comprehensive services including behavioral health open access to their underserved patients. To further expand the reach of their services JFS is exploring implementing behavioral health tele-health services to deliver care to patients with transportation or mobility issues.\n© Center for Health Progress 2018\nMeasures of Success\nDeveloped five (5) partnerships with Health Organizations to Expand Patient Access to Behavioral Health.\nPlanning for implementing behavioral health telehealth services to expand impact.\nShelli James [HealthTeamWorks Facilitator] has been an incredible resource and connector for Jewish Family Service. Shelli listened to the ideas I had and our strategic plan goals and went to work making calls and introducing me to other community agencies that were relevant. She helped me with exploring telehealth and other agency projects. She has valuable insight and knowledge of healthcare concerns in Colorado.\nDirector of Mental Health Services, Jewish Family Service', 'Pediatric Behavioral Health Services\nOur pediatric psychology program is designed to support the unique needs of children, adolescents, and families who are current patients. We provide comprehensive psychological evaluations for a range of pediatric mental health concerns, including ADHD, anxiety, depression, and learning difficulties. We also offer a variety of therapeutic services to help children and families build skills and manage challenging behaviors. Our team is committed to providing high-quality, short-term, and compassionate care to every family we serve.\nMental Health Crisis Help\n9-8-8 has been designated as the new three- digit dialing code that will route callers to the National Suicide Prevention Lifeline (now known as the 988 Suicide & Crisis Lifeline), and is now active across the United States.\nColorado Crisis Services – Colorado Crisis Services is the statewide behavioral health crisis response system offering mental health, substance use or emotional crisis help, information and referrals.\nColorado House Bill 21-1258 established this temporary behavioral health services program (I Matter) to provide access to mental health and substance use disorder services for youth, including addressing needs that may have resulted from the COVID-19 pandemic. The program is open to youth 18 years of age or younger or 21 years of age or younger if receiving special education services. In May 2023, the Colorado State Legislature renewed the program through at least June 30, 2024.\nI Matter is managed by the Colorado Behavioral Health Administration. I Matter provides up to six free therapy sessions for youth in Colorado and reimburses participating licensed therapists.\nI Matter also has a statewide public awareness and outreach campaign that includes digital ads on platforms such as TikTok and Snapchat, and on-the-ground outreach to schools and youth organizations. Both the awareness campaign and the IMatterColorado.org website were informed by youth feedback.\nHow Often Are Integrated Behavioral Health Appointments?\nAs a primary care pediatric practice, our goal is to work on improving difficulties within 20 bi-weekly sessions. We then work with the family to transition to check-in sessions four to six times a year, as needed. Our psychologists, counselors, and medical providers regularly collaborate to provide our patients with the most comprehensive care possible.\nMental Health Assessments for Children\nAt Parker Pediatrics, we offer mental health assessments for children. Our mental health professionals have experience working with children and can provide an accurate diagnosis. If your child is diagnosed with a mental health disorder, we can create a treatment plan to help them manage their symptoms and live happily.\nADHD Testing for Children\nAttention-deficit/hyperactivity disorder (ADHD) is a mental health disorder that can cause problems with focus, hyperactivity, and impulsivity. If you suspect your child has ADHD, our team can provide a psychological assessment. We will ask questions about your child’s behavior and how they interact with others. We may also administer a series of tests to help us better understand your child’s symptoms. After the assessment, we will work with you to create a treatment plan for your child.\nAnxiety Screening for Children\nAnxiety is a normal part of childhood. However, some children experience anxiety to a degree that it interferes with their daily lives. If you are concerned that your child’s anxiety is impacting their ability to function, we offer anxiety screenings. During the screening, our mental health professionals will ask questions about your child’s symptoms and how they have been affecting your child. We will also observe your child’s behavior. Based on the screening, we will make recommendations about the next steps, which may include a more comprehensive evaluation or therapy.\nDepression Screening for Children\nDepression is a mental illness that can make children sad, hopeless, and lose interest in things they used to enjoy. If you have concerns that your child may be depressed, we provide depression screenings. Our mental health experts will ask questions about your child’s symptoms and how they’re affecting your life during the screening. We’ll also watch how your kid behaves. After the exam, we will give recommendations on the next steps, including a more extensive examination or therapy.\nPediatric Therapy Services in Douglas County\nIn addition to mental health assessments and screenings, we offer a variety of therapeutic services to help kids and families build skills and manage challenging behaviors. Our therapists work with children, adolescents, and families to address a variety of concerns, including anxiety, depression, and behavioral problems.\nPediatric Behavioral Health FAQ\nThere are many things parents can do to support their child’s mental health issues. First, it’s important to be there for your child and to provide a listening ear. You should also encourage your child to express their feelings, either through talking or writing. It’s also important to help your child find healthy coping mechanisms, such as exercise or spending time with friends.\nMany factors can affect a child’s mental health. These include genetic predisposition, brain chemistry, life experiences, and family history. Additionally, certain medical conditions can also impact youth mental health.\nSeveral warning signs may indicate a mental illness in children. These include changes in mood or behavior, problems at school, withdrawal from friends or activities, and difficulty coping with stress. If you notice any of these signs in your child, it’s important to seek professional help.\nIf you’re concerned about your child’s mental health, the first step is to talk to your pediatrician. They can help you determine if your child would benefit from a referral to a pediatric mental health professional. A mental health professional will assess your child and provide a diagnosis. If they recommend behavioral therapy, they will create a treatment plan tailored to your child’s needs. Behavior therapy can help children learn healthy coping mechanisms and manage their symptoms.\nHow Can Mental Health Affect Child Development?\nMental health disorders can impact every area of a child’s life. They may have difficulty in school, making friends, or behaving differently than other children their age. If left untreated, mental health disorders can lead to problems in adulthood. That’s why it’s important to seek help during adolescence if you think your child is struggling. Our team can provide the support your family needs to help your child thrive.\nThe Sleep Foundation says that certain mental health conditions, such as anxiety disorders, ADHD, and autism spectrum disorder, are closely associated with sleep disorders. Mental health and behavioral issues can interfere with a child’s daily life by causing them to miss school, exhibit out-of-control behavior or mood swings, and withdraw from social interactions.\nWhen to See a Pediatric Behavioral Health Specialist\nAs a parent, it can be difficult to tell if your child is displaying symptoms of a mental health disorder. Many children go through rough patches or periods of adjustment, and it can be hard to know if their behaviors are just part of growing up or cause for concern. If you’re concerned about your child’s mental health, it’s important to seek professional help.\nHow Parker Pediatrics Can Help\nThe pediatricians at Parker Pediatrics can help detect, diagnose, and address many integrated behavioral and mental health issues affecting children today. Pediatricians are in a unique position to observe a child’s development over the years, and have the training and expertise to detect signs of behavioral and mental health issues in children and adolescents. Our pediatricians and team of pediatric professionals work to serve our patients in all areas of their healthcare, including children’s behavioral health services.']	['<urn:uuid:aaafa73f-88bd-4448-b0be-913f096257d8>', '<urn:uuid:593d2bc8-6e89-4626-a0f0-4349704d6e20>']	open-ended	with-premise	concise-and-natural	distant-from-document	three-doc	expert	2025-05-12T20:58:04.490895	11	118	1754
89	As a garden center specialist, I often notice that balloon flowers get damaged by slugs. Can you explain the specific characteristics of balloon flowers and the most effective natural ways to protect them from slug damage?	Balloon flowers (Platycodon grandiflorus) are clump-forming herbaceous perennials that grow to around 2 feet tall, producing star-shaped flowers in white, blue, and violet shades. While they're mostly disease and pest-resistant, they're particularly vulnerable to slugs and snails, which can create unsightly holes in the foliage. To protect balloon flowers from slugs naturally, you can implement several strategies: planting garlic or chives nearby as natural repellents, creating barriers with copper tape or wire around the plants, applying diatomaceous earth, or using beer traps. Additionally, maintaining good garden hygiene by removing debris and creating a drier environment through proper drainage can discourage slug activity, as slugs thrive in moist conditions.	"[""Balloon flowers come in a number of attractive varieties, though none will bloom the first year.\nThe biggest problem to look for is snails and slugs, which will make the plant unsightly as they eat on it.\nThese flowers are not hard to care for though they have some toxicity problems that you need to know. Here in this article, we'll go over this beautiful plant in great details.\nBalloon Flower Overview\n|Origin||Korea, China, Japan, Russia|\n|Scientific Name||Platycodon grandiflorus|\n|Type||Clump-forming herbaceous perennial|\n|Common Names||Balloon Flower, Chinese Bellflower, Japanese Bellflower|\n|Height||Around 2 feet|\n|Toxicity||Some toxic elements|\n|Light||Full sun to partial shade|\n|Watering||Moderately moist soil|\n|Pests||Slugs and snails|\nThe balloon flower has become incredibly popular in American gardens due to its impressive hardiness, low maintenance needs, and the surprising flowers it produces. There are a whole host of varieties to choose from if you are looking to invest in some balloon flowers, with all of them having plenty to offer the keen gardener.\nThis dwarf variety grows to a maximum of just 8 inches tall, making it perfect for growing in container pots on the patio or as bedding, edging, or a border plant. Its small stature means that unlike many balloon flowers, it won’t need to be staked, yet still produces large, vibrant blue star-shaped flowers of up to three inches in diameter. The stems of this plant emerge from the ground each spring but can be easily disturbed if you forget where they were planted. It’s a good idea to mark them out so that you can leave a clear space for them to arrive each year, as even minor disturbance of this plant can cause it to die. It does not cope well with being moved, and as it is a smaller variety, it would be best grown directly in the container, which will also be its final home.\nThese balloon flowers are another dwarf variety that grows up to 8 inches in height. They work well in containers and also as cut flowers in bouquets. The buds of this variety appear as puffed up white balloons, which transform into star-shaped flared flowers. The flowers are white with subtle striping hints of pink.\nFuji Blue is a medium variety of balloon flowers. It grows to around 20 inches tall, making it an impressive height but still small enough to not require the stems to be staked. The flowers of this variety are slightly smaller than other balloon flowers, typically measuring two inches across, and appearing in a vibrant shade of blue with some darker blue veining.\nThis variety of balloon flower features double blooms, with two full layers of star-shaped blue petals. The plant grows to around 24 inches in height and is a variety that may need to be staked when mature. The plant is deer-resistant, mostly disease-resistant, and is not frequently a victim of insect infestations. However, slugs and snails need to be watched out for as these can sometimes cause a problem for the plant.\nCaring for Your Balloon Flower\nThis plant should be grown in well-draining loamy soil because it enjoys a good amount of moisture but does not like to be sitting in soggy soil. A high-quality, well-draining soil with good organic content will encourage water to drain away from the roots, protecting the plant against root rot while ensuring it gets all of the moisture and nutrients it needs. The plant would prefer to be in slightly acidic soil, ranging between 6 and 7 pH, though it will tolerate alkaline or neutral soil as well. Avoid heavy or waterlogged areas when planting the balloon flower as it will struggle to survive in these conditions.\nFull sun is best for this plant, especially if you want it to produce an abundance of flowers. If you aren’t able to offer the plant a full sun position, then it will settle for partial shade, ideally with several hours of direct sun in the morning.\nThis plant is hardy in zones 3 to 8, and it will thrive in temperatures ranging from 60 to 80 ºF. It can tolerate temperatures exceeding 8 0ºF but will need some protection during the hottest part of the day. If you live in a hot climate, you should position this plant so that it receives morning sun but is sheltered in the afternoon from the intense heat. The afternoon shade will drop the temperature a few degrees and give the plant some relief from the high levels of heat.\nBalloon flowers grow well from seed and can be sown anytime within the growing season. If you want to get a head start on growing these plants, you can sow the seeds indoors in early spring. Use a tray of moist, well-draining soil and sit the seeds on top, lightly pressing them into the soil. Be careful not to cover the seeds over with soil or press them in too deep, as they need light in order to germinate.\nKeep the soil moist but not soggy and sit them in a bright and warm spot. Germination should occur within two weeks. Once seedlings have developed, you can thin them out, moving the strongest among them to slightly larger pots to allow their roots to spread and gain strength. Once the final frost has passed, you can transplant the seedlings outside to their final positions. You will need to take care as balloon flowers have particularly delicate root systems, which, although fleshy, can be very easily damaged. Do not expect all of your seedlings to survive the move, so grow more than you need initially. Alternatively, you can sow seeds directly in the ground during late spring or summer. This will likely result in a higher success rate as the plants won’t need to be transplanted, but they will have less time to grow during the growing season, so it won’t become as large as when started earlier. Balloon flowers typically won’t flower in their first year.\nThis plant is also fairly successful in self-seeding, so you can benefit from extra balloon flowers for no effort at all. Spent flowers develop into seeds, some of which will be dispersed and settle into the soil where they will grow into new plants. To allow this to happen, you need to refrain from deadheading the flowers as this is where the seeds are. Allow them to naturally drop to the ground when they are ready, and you should see a handful of new balloon flowers emerging next spring.\nThe blooms of this plant are commonly known as balloon flowers because of the way the buds swell up like balloons, and then seemingly burst into starry bell-shaped flowers. Each flower has five points and measures between two and three inches across. The flowers are reliable bloomers and are available in white and various shades of blue and violet. They bloom profusely throughout the summer, sitting on top of tall stems in clusters of flowers, with serrated foliage below.\nIn the winter, the foliage itself can provide some colorful interest in the garden, with the leaves often fading to shades of purple and red. Some stems may need to be staked to prevent them from falling over when flowering, as stems tend not to be very strong and become more likely to bend over the taller they get. If you are opposed to staking plants, an alternative method would be to clump plants together so that they can support each other. Another option is to prune back stems after the flower, as this will encourage a bushier pattern the following season and prevent stems from becoming so tall (University of Illinois Extension).\nThe scientific name actually translates to ‘broad bell,’ which is a good description of the flower. ‘Platycodon’ is derived from Greek, with ‘platys’ meaning ‘broad,’ and ‘kodon’ meaning ‘bell.’ ‘Grandiflorus’ literally translates to large flowers.\nThis is a low-maintenance plant that doesn’t necessarily require pruning. However, you can prune it if you wish to train it to grow in a particular way. To keep the plant from getting leggy, you can trim the stems back by just a few inches, though be sure to do this after flowering has occurred to prevent inadvertently disturbing the flower production. Pinching back stems will result in more growth at the base and a bushier looking plant. It will also help to prevent the flowering stems from becoming so tall that they need staking.\nOther than this, the maintenance of this plant is very minimal. You can deadhead the spent flowers if you wish, as this will encourage more flowers to bloom and result in a longer flowering season.\nSome parts of this plant are toxic, namely the roots and the basal leaves. However, some other parts of the plant are edible and can be used in soups or for flavoring (Plants for a Future).\nBalloon flowers are mostly disease-free. They are also deer-resistant and pest-resistant, very rarely being affected with insect infestations. The exception to this is slugs and snails, which can sometimes cause problems for the plant. Snails and slugs feast on the foliage of plants, leaving unsightly holes in your plant's leaves and negatively affecting their health. There are ways you can naturally deter these pests from inhabiting your balloon flowers, such as planting garlic or chives nearby, which are slug and snail repellents.\nAre you ready to get festive by growing your own balloon flowers? Let us know if you have any questions by leaving us a comment, and don’t forget to share this page with other interested growers!"", 'Gardening is a fulfilling and rewarding hobby, but it can become frustrating when slugs start invading your garden and wreaking havoc on your precious plants. These slimy creatures have a knack for devouring foliage, leaving a trail of destruction in their wake. However, with the right knowledge and strategies, you can effectively get rid of slugs and protect your garden oasis. In this article, we will explore various methods to eliminate slugs from your garden, from natural remedies to cultural practices and integrated pest management.\nMaintaining a slug-free garden is essential for the health and vitality of your plants. Slugs can cause significant damage to both ornamental and edible plants, ruining months of hard work and dedication. Their feeding habits can lead to unsightly holes in leaves, seedlings being decimated overnight, and the frustration of seeing your favorite flowers and vegetables being devoured. To ensure the success of your garden and preserve its beauty, it is crucial to take proactive measures to control and eliminate slugs.\nBefore diving into the methods of slug control, let’s take a moment to understand these slimy intruders. Slugs are soft-bodied mollusks belonging to the class Gastropoda. They have long, muscular bodies and move by gliding on a slimy trail produced by their foot. Slugs thrive in moist environments and are particularly active during damp and cool weather conditions. They are hermaphrodites, meaning they possess both male and female reproductive organs. Slugs lay clusters of gelatinous eggs in damp soil or hidden crevices, which eventually hatch into tiny slug larvae.\nIdentifying Slug Infestation\nTo effectively combat slug infestation, it is important to identify their presence in your garden. There are several signs that indicate a slug infestation. Look out for chewed leaves with irregular holes or jagged edges, slimy trails on plants or the ground, and the presence of slugs during early morning or after rainfall. Additionally, you may come across the slugs themselves, which can vary in size, color, and pattern. Common slug species include the gray garden slug, the black slug, and the leopard slug. By familiarizing yourself with these signs and species, you can take appropriate action to control the slug population.\nPreventing slug infestation is key to maintaining a healthy garden. Here are some effective prevention methods to consider:\na. Creating physical barriers to deter slugs: Install barriers such as copper tape or wire around the edges of garden beds, pots, or raised planters. Slugs dislike the electrical charge created by copper and are deterred from crossing the barrier.\nb. Choosing slug-resistant plants for your garden: Select plants that are less appealing to slugs. Varieties such as lavender, rosemary, geraniums, and begonias are known to be less attractive to slugs.\nc. Implementing proper garden hygiene practices: Keep your garden clean and free of debris, as slugs tend to hide in dark, damp areas. Regularly remove fallen leaves, mulch, and weeds that can provide hiding spots for slugs.\nIf slugs have already invaded your garden, you can employ various natural remedies to deter and control them:\na. Using copper tape or wire to repel slugs: As mentioned earlier, copper creates an unpleasant sensation for slugs. Encircle pots, raised beds, or specific areas with copper tape or wire to prevent slugs from reaching your plants.\nb. Applying diatomaceous earth as a barrier: Diatomaceous earth is a natural substance made from fossilized remains of aquatic organisms. Sprinkle a thin layer of diatomaceous earth around your plants. Its sharp particles will deter slugs as they crawl across it.\nc. Making beer traps to catch slugs: Bury a container, such as a shallow dish or jar, partially filled with beer in the ground. Slugs are attracted to the scent of beer and will crawl into the container and drown. Empty and refill the traps regularly.\nOrganic Slug Control Products\nIf natural remedies alone aren’t sufficient, you can explore organic slug control products that are safe for both your plants and the environment. Some effective options include:\na. Iron phosphate-based baits: Iron phosphate baits are organic and pose no harm to humans, pets, or wildlife. Scatter the pellets around your garden, following the instructions on the packaging. The slugs will consume the bait, which will eventually cause their demise.\nb. Nematodes as a biological control method: Nematodes are microscopic worms that naturally occur in soil. Certain species of nematodes, such as Steinernema carpocapsae, are effective in controlling slugs. Apply nematodes to the soil according to the instructions to target slug larvae and adults.\nModifying the environment in your garden can create conditions that are unfavorable for slugs. Consider the following methods:\na. Creating a dry environment to discourage slugs: Slugs thrive in moist conditions, so ensuring your garden has good drainage can make the environment less appealing to them. Avoid overwatering and provide adequate airflow to keep the soil relatively dry.\nb. Using companion planting to repel slugs: Certain plants, such as garlic, fennel, and mint, have natural repellent properties against slugs. Interplanting these repellent plants among your vulnerable plants can help deter slugs. Additionally, planting slug-resistant flowers like marigolds can act as a natural barrier.\nNon-Toxic Chemical Solutions\nIf natural and organic methods are not achieving the desired results, you can consider non-toxic chemical solutions for slug control. It’s important to choose products that are safe for the environment, beneficial insects, and other wildlife. Follow these guidelines for safe and effective use:\na. Safe chemical options for slug control: Look for products containing iron phosphate or ferric sodium EDTA as active ingredients. These substances are considered safe for use in gardens and pose minimal risk to non-target organisms.\nb. Understanding the proper application of chemical solutions: Read and follow the instructions provided with the chemical product carefully. Apply the solution as directed, focusing on slug-prone areas of your garden. Avoid over-application and use chemical control methods as a last resort.\nImplementing certain cultural practices can help minimize slug infestation in your garden. Consider the following techniques:\na. Using mulch to deter slugs: Apply a layer of coarse mulch, such as straw or wood chips, around your plants. This creates a barrier that slugs find difficult to cross. Avoid using fine or moist mulch, as it can provide a favorable environment for slugs.\nb. Regularly removing hiding places for slugs: Inspect your garden regularly and remove potential hiding spots for slugs, such as boards, rocks, and plant debris. By eliminating these hiding places, you make your garden less inviting to slugs.\nMaintenance and Monitoring\nMaintaining a slug-free garden requires consistent effort and monitoring. Follow these tips for ongoing maintenance:\na. Regularly inspecting the garden for signs of slugs: Check your plants and the surrounding area for slug damage, slime trails, and the presence of slugs. Early detection allows for prompt intervention and prevents further damage.\nb. Maintaining slug control methods throughout the season: Continuously apply preventive measures and monitor slug activity throughout the gardening season. Regularly replenish barriers, refresh baits, and adjust cultural practices as needed.\nIntegrated Pest Management (IPM)\nAdopting an integrated pest management approach is crucial for effective slug control. Integrated pest management combines multiple strategies to minimize pest damage while minimizing the use of harmful chemicals. Incorporate the following practices:\na. Adopting an integrated approach to slug control: Combine various methods discussed in this article, such as physical barriers, natural remedies, cultural practices, and organic slug control products. By using multiple techniques, you create a comprehensive and effective defense against slugs.\nb. Combining multiple methods for effective results: Implement a combination of slug prevention, natural remedies, environmental modifications, and other strategies tailored to your specific garden. This holistic approach increases your chances of successful slug control while maintaining a healthy and balanced garden ecosystem.\nImportance of Early Intervention\nTaking prompt action at the first sign of slug infestation is crucial to prevent their population from multiplying and causing significant damage. By addressing the issue early on, you can minimize the impact on your garden and ensure the well-being of your plants.\nDealing with slugs in the garden can be a challenging task, but with the right strategies, you can effectively control and minimize their presence. By implementing preventive measures, utilizing natural remedies, considering organic slug control products, modifying the garden environment, and practicing integrated pest management, you can create a slug-free haven for your plants. Remember to monitor your garden regularly, adjust your methods as needed, and take action at the first sign of slug infestation. With perseverance and proactive care, you can enjoy a flourishing and beautiful garden, free from the nuisance of slugs.\n1. Are slugs harmful to my plants? Yes, slugs can cause significant damage to plants by feeding on leaves, stems, and even fruits. Their feeding can lead to stunted growth, unsightly holes, and in severe cases, plant death.\n2. How can I attract slug predators to my garden? Encouraging natural slug predators like birds, frogs, toads, and certain beneficial insects can help control slug populations. Providing bird feeders, creating water sources, and incorporating native plants can attract these natural predators to your garden.\n3. Can I use salt to get rid of slugs? While salt can effectively kill slugs, it is not recommended as a control method. Salt can also harm plants and soil, so it’s best to use alternative slug control strategies that are safe for your garden ecosystem.\n4. Should I use chemical pesticides to eliminate slugs? Chemical pesticides should be used as a last resort and only if they are specifically labeled for slug control. Opt for non-toxic options like iron phosphate-based baits and follow the instructions carefully to minimize environmental impact.\n5. How often should I check my garden for slug activity? Regular monitoring is essential to catch slug infestations early. Check your garden at least once a week, especially after rainfall or during periods of high slug activity, and take necessary measures to control them.']"	['<urn:uuid:724ae6d8-2a9e-454e-b211-e600f000438e>', '<urn:uuid:f053d7ca-5ab1-46ba-b62f-0e44ed1e297a>']	open-ended	with-premise	verbose-and-natural	distant-from-document	multi-aspect	expert	2025-05-12T20:58:04.490895	36	108	3235
90	What happens to animals when the climate changes in their natural habitat? I'm curious about how they deal with environmental changes.	When changes in external factors like temperature occur, animals have three main responses: they can adapt to the new conditions, move to a different area, or die if they cannot cope with the changes. These responses are directly linked to changes in plant growth, since animals depend on the plants they feed upon. While ecosystems naturally experience some changes, current climate changes are happening so frequently and suddenly that ecosystems cannot adapt to reach a new balance.	"[""Ecosystems: what they are and why they are important\nA biome is different from an ecosystem. An ecosystem is the interaction of living and nonliving things in an environment. A biome is a specific. A biome is an ecosystem containing plant and animal species that are characteristic to a specific geographic region. (An ecosystem is the community of plants. Ecological levels: from individuals to ecosystems Ecosystems and biomes . And now, a direct relationship to God, unmediated by the institution of the.\nNew species sometimes also arrive. For example, a new bird that eats a certain insect can come to an area. That insect will now be less abundant and affect the plants that it used to feed upon as well as the other animals that eat this plant.\nThe animals and plants that live in an ecosystem are perfectly suited to these particular living conditions. Changes in external factors, like temperature, can change the plants grow and, therefore, the animals that eat the plants might adapt, move, or die in response. Ecosystem services The normal functioning of an ecosystem provides humans with an abundance of services that we depend upon or that can significantly improve our quality of life.\nThe list of ecosystem-provided services is very, very long and includes several more nuanced entries that we tend to take for granted, like clean air, a stable climate, and safe drinking water. Pollination is an important ecosystem service. Human influence Human action is currently disrupting a large number of ecosystems.\nFor example, by removing most of the fish from the ocean, the whole food chain and system are disrupted and can no longer function properly. The result is running out of certain types of seafood that we enjoy. Introducing invasive species also influences ecosystems because these invasive species outcompete several of the native species that are necessary for the system to work properly. On a larger scale, humans are even capable of influencing external factors.\nBy causing the earth to warm via increased carbon dioxide emissions, it influences which plants and animals can live where. It is true that new species often enter ecosystems and that climate can naturally fluctuate but the current changes are so frequent and sudden that the ecosystems cannot adapt to new equilibrium.\nWe are also shooting ourselves in the foot because disrupting ecosystems could have disastrous effects on ourselves: Maintaining the balance of the ecosystem benefits us personally. Words to Know Anthropogenic: Resulting from the influence of human action on nature. Referring to the deepest parts of the oceans.\nEcosystems: what they are and why they are important\nLocated in a northern region. Plants whose seeds are stored in cones and that retain their leaves all year around.\nPlants that lose their leaves at some season of the year, and then grow them back at another season.\nAn ecological community, including plants, animals, and microorganisms, considered together with their environment. A productive aquatic region with a large nutrient supply. A type of plant that has little or no woody tissue and usually lives for only one growing season. An ecosystem that contains standing water.\nAn ecosystem that consists of running water. An ecosystem dominated by a single species. An unproductive aquatic region with a relatively modest nutrient supply. Referring to the open oceans.\nAn ecosystem that consists of a wide variety of species. Characteristic of a region or climate that is frost free with temperatures high enough to support—with adequate precipitation—plant growth year round. The process by which lower, nutrient-rich waters rise upward to the ocean's surface. Areas that are wet or covered with water for at least part of the year. The boreal coniferous forest, or taiga, is an extensive northern biome occurring in moist climates with cold winters. The boreal forest is dominated by coniferous cone-bearing trees, especially species of fir, larch, pine, and spruce.\nSome broad-leaved trees are also present in the boreal forest, especially species of aspen, birch, poplar, and willow. Most boreal forests are subject to periodic catastrophic disturbances, such as wildfires and attacks by insects. Temperate deciduous forests are dominated by a large variety of broad-leaved trees in relatively moist, temperate mild or moderate climates.\nBecause these forests occur in places where the winters can be cold, the foliage of most species is seasonally deciduous, meaning that trees shed their leaves each autumn and then regrow them in the springtime. Common trees of the temperate deciduous forest biome in North America are ash, basswood, birch, cherry, chestnut, dogwood, elm, hickory, magnolia, maple, oak, tulip-tree, and walnut. Temperate rain forests are characterized by mild winters and an abundance of rain.\nThese systems are too moist to support wildfires. As a result, they often develop into old-growth forests, dominated by coniferous trees of mixed age and various species.\nBiomes and Ecosystems - Windows to the Universe\nIndividual trees can be very large and, in extreme cases, can be more than 1, years old. Common trees of this biome are species of Douglas-fir, hemlock, cedar, redwood, spruce, and yellow cypress.\nIn North America, temperate rain forests are most commonly found on the humid west coast. A boreal forest in north Saskatchewan. Reproduced by permission of JLM Visuals. Temperate grasslands occur under climatic conditions that are between those that produce forests and those that produce deserts. In temperate zones, grasslands typically occur in regions where rainfall is 25 to 60 centimeters 10 to 24 inches per year.\nGrasslands in North America are called prairies and in Eurasia they are often called steppes. This biome occupies vast regions of the interior of these continents. The prairie is often divided into three types according to height of the dominant vegetation: The once-extensive tall grass prairie is dominated by various species of grasses and broad-leaved, herbaceous plants such as sunflowers and blazing stars, some as tall as 3 to 4 meters 10 to 13 feet.\nFire played a key role in preventing much of the tall grass prairie from developing into open forest. The tall grass prairie is now an endangered natural ecosystem because it has been almost entirely converted for agricultural use. The mixed grass prairie occurs where rainfall is less plentiful, and it supports shorter species of grasses and other herbaceous plants.\nThe short grass prairie develops when there is even less precipitation, and it is subject to unpredictable years of severe drought. Tropical grassland and savanna. Tropical grasslands are present in regions with as much as centimeters 47 inches of rainfall per year, but under highly seasonal conditions with a pronounced dry season. Savannas are dominated by grasses and other herbaceous plants.\nHowever, they also have scattered shrubs and tree-sized woody plants that form a very open canopy a layer of spreading branches. Tropical grasslands and savannas can support a great seasonal abundance of large, migratory animals as well as substantial populations of resident animals. This is especially true of Africa, where on the savanna range—among other animals—gazelles and other antelopes, rhinos, elephants, hippopotamuses, and buffalo, and various predators of these, such as lions, cheetahs, wild dogs, and hyenas.\nChaparral is a temperate biome that develops in environments where precipitation varies widely from season to season. A common chaparral pattern involves winter rains and summer drought, the socalled Mediterranean climate. Chaparral is characterized by dwarf forests, shrubs, and herbaceous vegetation.\nThis biome is highly prone to wildfire. In North America, chaparral is best developed in parts of the southwest, especially coastal southern California. Deserts occur in either temperate or tropical climates.\nThey commonly are found in the centers of continents and in rain shadows of Biomes along 87 degrees west longitude and along 0 degrees longitude.\nBiome - humans, examples, body, water, process, Earth, life, plants\nReproduced by permission of The Gale Group. The most prominent characteristic of a desert is the limited amount of water available. In most cases, less than 25 centimeters 10 inches of rain fall each year. Not surprisingly, the plant life found in a desert ecosystem is strongly influenced by the availability of water: In somewhat moister places, a shrub-dominated ecosystem is able to develop. A semi-evergreen tropical forest is a type of tropical forest that develops when a region experiences both wet and dry seasons during the year.\nBecause of this pattern, most trees and shrubs of this biome are seasonally deciduous, meaning that they shed their foliage in anticipation of the drier season. This biome supports a great richness of species of plants and animals, though somewhat less than in tropical rain forests.\nEvergreen tropical rain forest. Evergreen tropical rain forests occur in tropical climates with abundant precipitation and no seasonal drought. Because wildfires and other types of catastrophic disturbances are uncommon in this sort of climate regime, tropical rain forests usually develop into old-growth forests.\nAs such, they contain a great richness of species of trees and other plants, a great size range of trees, and an extraordinary diversity of animals and microorganisms. Many ecologists consider the old-growth tropical rain forests the ideal ecosystem on land because of the enormous variety of species that are supported under relatively favorable climatic conditions. Freshwater biomes Freshwater biomes can be divided into three general categories: A lentic ecosystem is one such as a lake or pond that contains standing water.\nIn lentic systems, water generally flows into and out of the lake or pond on a regular basis. The rates at which inflow and outflow occur vary greatly and can range from days, in the case of small pools, to centuries, in the case of the largest lakes. The types of organisms that inhabit lentic biomes are strongly influenced by water properties, especially nutrient concentration and water transparency and depth.\nWaters with a large nutrient supply are highly productive, or eutrophic, while infertile waters are unproductive, or oligotrophic. Commonly, shallow bodies of water are much more productive than deeper bodies of water of the same surface area, primarily because plant growth is influenced by the ability of light to penetrate into the water. Water that becomes cloudy because of the accumulation of silt or dissolved organic matter is likely to have low productivity.\nA lotic biome is one that consists of running water, as in streams or rivers. The organisms found in a lotic biome depend on factors such as the amount of water in the system, the rate at which it flows, and seasonal changes in the flow rate. Consider a stream in which flooding is common in the spring.\nRapidly moving water churns up clay, silt, sand, and other materials from the streambed. The water then becomes cloudy and murky, and light is thus prevented from penetrating it. In this case the stream will not be able to support many kinds of life-forms.\nIn general, the common lotic ecosystems such as rivers, streams, and brooks are not usually self-supporting in terms of the organisms that live within them.""]"	['<urn:uuid:ea07324b-4919-49ad-97a6-efc764294a0f>']	open-ended	direct	verbose-and-natural	distant-from-document	single-doc	novice	2025-05-12T20:58:04.490895	21	77	1812
91	I'm researching sports scandals. When was BALCO caught distributing steroids?	The BALCO investigation began in 2003 when U.S. sprint coach Trevor Graham made an anonymous phone call to the United States Anti-Doping Agency (USADA) accusing athletes of doping with an undetectable steroid and naming Victor Conte as the source. On September 3, 2003, federal agents searched BALCO facilities and found containers with steroids and growth hormones. This led to legal proceedings where Conte and others eventually pleaded guilty to illegal steroid distribution.	"['Bay Area Laboratory Co-operative\n|Part of a series on|\n|Doping in sport|\nThe Bay Area Laboratory Co-operative (BALCO) (1984–2003) was an American company led by founder and owner Victor Conte. In 2003, journalists Lance Williams and Mark Fainaru-Wada investigated the company\'s role in a drug sports scandal later referred to as the BALCO Affair. BALCO marketed tetrahydrogestrinone (""the Clear""), a then-undetected, performance-enhancing steroid developed by chemist Patrick Arnold. Conte, BALCO vice president James Valente, weight trainer Greg Anderson and coach Remi Korchemny had supplied a number of high-profile sports stars from the United States and Europe with ""the Clear"" and human growth hormone for several years.\nHeadquartered in Burlingame, California, BALCO was founded in 1984. Officially, BALCO was a service business for blood and urine analysis and food supplements. In 1988, Victor Conte offered free blood and urine tests to a group of athletes known as the BALCO Olympians. He then was allowed to attend the Summer Olympics in Seoul, South Korea. From 1996, Conte worked with well-known American football star Bill Romanowski, who proved to be useful to establish new connections to athletes and coaches such as Korchemny. Conte and Korchemny shortly thereafter founded the ZMA Track Club for marketing purposes, well-known members of it being sprinters Marion Jones and Tim Montgomery. In 2000, Conte managed to contact American baseball star Barry Bonds via Greg Anderson, a coach working in a nearby fitness studio. Bonds then delivered contacts to other baseball professionals.\nIn 2003, the United States Attorney for the Northern District of California began investigating BALCO. U.S. sprint coach Trevor Graham had given an anonymous phone call to the United States Anti-Doping Agency (USADA) in June 2003 accusing a number of athletes being involved in doping with a steroid that was not detectable at the time. He also named Victor Conte as the source of the steroid. As evidence, Graham delivered a syringe containing traces of tetrahydrogestrinone, nicknamed ""the Clear.""\nShortly after, Don Catlin, MD, the founder of the UCLA Olympic Analytical Laboratory, developed a testing process for tetrahydrogestrinone (THG). Now able to detect the new substance, he tested 550 existing samples from athletes, of which 20 proved to be positive for THG.\nOn September 3, 2003 agents of the Internal Revenue Service, Food and Drug Administration, San Mateo Narcotics Task Force, and United States Anti-Doping Agency conducted a house search at the BALCO facilities. Beside lists of BALCO customers in a BALCO field warehouse they found containers whose labels indicated steroids and growth hormones. In a house search at Anderson\'s place two days later, steroids, $60,000 in cash, names lists and dosage plans were found.\nAmong the athletes listed in the record of BALCO customers were:\n- MLB players: Barry Bonds, Benito Santiago, Jeremy Giambi, Bobby Estalella, Armando Rios\n- Athletes: Hammer thrower John McEwen, shot putters Kevin Toth and C.J. Hunter, sprinters Dwain Chambers, Marion Jones, Tim Montgomery, Zhanna Block and Kelli White, middle-distance runner Regina Jacobs.\n- Boxer Shane Mosley.\n- Cycling: Tammy Thomas.\n- NFL players: A number from the Oakland Raiders, including Bill Romanowski, Tyrone Wheatley, Barrett Robbins, Chris Cooper and Dana Stubblefield.\n- Judo: Conte was also connected with supplying ""vitamin supplements"" to the 1988 U.S. Olympic judo team coached by Willy Cahill of San Bruno, California.\n- Christos Tzekos and his athletes were initially connected to BALCO but later cleared.\nPatrick Arnold, BALCO\'s chemist, alleges that Bonds and Sheffield were given ""the Clear,"" though the athletes deny knowing about it and Arnold does not claim to have witnessed it.\nIn April 2005, Lance Williams and Mark Fainaru-Wada were honored with the journalist prize of the White House Correspondents\' Association. In 2006, they published the book Game of Shadows, which consists of a summary of about 200 interviews and 1,000 documents they collected for their research.\nOn July 15, 2005, Conte and Anderson cut plea bargains, pleaded guilty to illegal steroid distribution and money laundering and avoided an embarrassing trial. Conte spent four months in prison. Anderson was incarcerated for 13½ months. He was released on November 15, 2007, the same day Bonds was indicted by a federal grand jury on four counts of perjury and one count of obstruction of justice.\nOn June 6, 2006 the house of Arizona Diamondbacks player Jason Grimsley was searched as part of the ongoing BALCO probe. Grimsley later said that federal investigators wanted him to wear a wire in order to obtain information against Barry Bonds. He told people which players used performance-enhancing drugs. The final result was that the Diamondbacks released Grimsley, and he was given a 50-game suspension by Major League Baseball.\nIn October 2006, investigations against Fainaru-Wada and Williams were started. The reporters were served with subpoenas to appear before a grand jury to identify the individual who leaked Bonds\' name to them. They refused to do so and federal prosecutors asked that they be jailed for up to 18 months (the typical term of a grand jury). However, in February 2007, federal prosecutors dropped charges against the reporters after a Colorado attorney, Troy Ellerman, who once represented Conte and another executive of the Bay Area Laboratory Co-operative, admitted to leaking the testimony and pleaded guilty to federal charges of unauthorized disclosure of grand jury testimony.\nIn an interview with Editor & Publisher, Lance Williams revealed that he would never testify in court, even if it did not involve confidential sources. ""I have no interest in becoming anybody\'s witness.""\nOn November 15, 2007, former San Francisco Giants outfielder Barry Bonds was indicted for perjury and obstruction of justice based on his grand jury testimony in this investigation. The trial began March 21, 2011, and he was convicted on April 13, 2011 on the obstruction of justice charge. The conviction was overturned upon appeal.\nOn April 4, 2008, Tammy Thomas was convicted by a federal jury on three counts of making false statements to a federal grand jury in November 2003, and on one count of obstructing justice. She was acquitted of two perjury charges. Sentencing was set for July 18, 2008. She was sentenced to six months\' house arrest and five years\' probation on October 10, 2008.\nOn May 29, 2008, Trevor Graham was convicted by a federal jury on one count of lying to federal investigators about his relationship to an admitted steroids dealer, and the jury deadlocked on two other charges. Sentencing was set for September 5, 2008. He was sentenced to one year of house arrest on October 21, 2008.\n- Fainaru-Wada, Mark; Williams, Lance (December 25, 2003). ""Barry Bonds: Anatomy of a scandal"". Seattle Post-Intelligencer (Seattle PI). San Francisco Chronicle. Retrieved June 29, 2017.\n- Harrington, Mark (November 1, 2003). ""Success a Bitter Pill / College dropout moved BALCO into big leagues before charges"". Newsday. Retrieved June 29, 2017.\n- ""Lawyers for former track coach Christos Tzekos say investigations show no ties to BALCO"". IHT. AP. October 30, 2007. Retrieved January 12, 2008.\n- Schmidt, Michael S. (July 25, 2007). ""Chemist Says Sheffield and Bonds Used Drugs"". The New York Times. ISSN 0362-4331. Retrieved June 29, 2017.\n- ""Conte released from prison, calls book \'full of lies\'"". ESPN.com. AP Press. March 30, 2006. Retrieved June 29, 2017.\n- Bonds indicted on perjury, obstruction of justice charges, Lance Williams, Jaxon Van Derbeken, San Francisco Chronicle, November 15, 2007\n- Maik Grossekathöfer: Leck im System., Der Spiegel, 40/2006, S. 140, (German)\n- Reporters in BALCO Case Sentenced to Jail, ESPN, September 22, 2006\n- Reporters Must Testify Over Bonds Leak, USA Today, August 15, 2006\n- Egelko, Bob (February 14, 2007). ""Attorney pleads guilty to leaking BALCO testimony"". The San Francisco Chronicle.\n- Williams: I Never Thought Bonds Indictment Would Occur Archived December 17, 2007, at the Wayback Machine By: Strupp, Joe Editor and Publisher November 17, 2007\n- Elias, Paul (March 21, 2011). ""Barry Bonds perjury trial gets under way"". Associated Press. Retrieved March 21, 2011.[permanent dead link]\n- ""Barry Bonds convicted of obstruction of justice in performance-enhancing-drugs case"". Los Angeles Times. April 13, 2011. Archived from the original on April 28, 2011. Retrieved April 16, 2011.\n- ""Barry Bonds found guilty of obstruction,"". ESPN. April 14, 2011. Archived from the original on April 29, 2011. Retrieved April 16, 2011.\n- Mintz, Howard (April 4, 2008). ""Cyclist convicted of perjury in Balco case"". San Jose Mercury News. Retrieved May 30, 2008.\n- Pogash, Carol; Schmidt, Michael S. (October 11, 2008). ""Cyclist Avoids Prison Time, Which May Benefit Bonds"". The New York Times. Retrieved April 26, 2010.\n- Dubow, Josh; Paul Elias; Raf Casert (May 30, 2008). ""Track coach Graham convicted in BALCO probe"". Tampa Bay Online. Archived from the original on February 15, 2009. Retrieved 2008-05-30.\n- Pogash, Carol; Michael Schmidt (October 21, 2008). ""Graham Sentenced to Year\'s House Arrest in Balco Case"". New York Times.']"	['<urn:uuid:7c68c87c-f557-424c-904c-b6463040fa70>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:58:04.490895	10	72	1467
92	Do both projects study ancient trade networks?	Yes, both projects examine ancient trade networks. The post-Roman project studies rural populations' access to local, regional and global exchange networks in northern Gaul, while the Iraq-Iran project investigates trade connections along early Silk Road routes, with evidence of communities trading materials across more than 1,500 km.	"['The bottom-up development of post-Roman northwestern Europe\n- 2017 - 2022\n- Frans Theuws\n- ERC Advanced Grant\n- Université de Liège (Belgium) Interfaculty research Unit Art, Archéologie, Patrimoine as a second beneficiary\n- l’IRAMAT Centre Ernest-Babelon (UMR 5060) of the CNRS and the University of Orléans.\n- The archaeological department of the Town of Nijmegen for the cemetery of Lent\n- The forensic laboratory for DNA research of Leiden University for aDNA research\n- The department of geology and GeoChemistry of the Vrije Universiteit Amsterdam for isotope research\n- The archaeology department of the Agence wallonne du Patrimoine (Belgium)\n- The Flemish Agentschap voor Onroerend Erfgoed\n- The department of Arts and Archaeology of the Free University Brussels\nThe question as to how Europe emerged from the collapse of the Roman state in the West has been the subject of academic debate for over a century. The role of the rural population in this development, however, has insufficiently been considered. Judging by the mass of objects found in their graves they must have been consumers with access to local, regional and global exchange networks. In a quantitative sense their demand could have outweighed that of the aristocracy. It is our hypothesis that it is rather the rural population than the elite who trigger economic growth.\nIt is our goal to analyze the wealth in the countryside, the exchange systems rural dwellers were part of, the production of goods and the changing ritual repertoires that seem to trigger the rise in demand of the rural population. Moreover, we will analyze the role of the elite in the economy, ‘the lay of land’ in northern Gaul, and the mobility of members of local groups.\nOur methods will be: collecting data on a large number of sites, material culture and burial rites in northern Gaul, analyzing distribution patterns using GIS, carry out a contextual analysis of finds, carry out scientific research of various categories of objects to understand production and distribution, study ancient DNA and Isotopes to understand the dynamics of local groups.\nWe aim at formulating new models on structural and dynamic aspects of the early medieval economy, linking ritual, production and exchange and considering the nature of demand, the nature of material culture the relations between production and the imaginary world and the role of subaltern groups.\nWith this project we not only hope to contribute to understanding the economic development of early Europe but also to contribute to an awareness on the role of subaltern groups and the role of non-economic factors such as ritual behavior in economic development today.\nThe bulk of the material we use for analyses originates from the thousands of graves with their rich ensembles of grave goods excavated since the 19th century in northern Gaul (the north of France, Belgium, Luxemburg, the German Rhineland and the Netherlands).\nFor the first time a database will be created with a comprehensive overview of sites and material culture of the Merovingian world in northern Gaul. This database will be an important instrument for our own as well as future research. It will be made available towards the end of the project.\nSee for more information the project website.\nOr follow the project on Twitter.', 'Major archaeological project wins EU funding to investigate the earliest settled societies in Iraq and Iran, 17,000-7000 BC\nRelease Date 08 June 2018\nUniversity of Reading archaeologists have succeeded in winning a highly prestigious European Research Council Advanced Grant of €2.5 million (£2.2 million) for a multi-disciplinary project entitled MENTICA Middle East Neolithic Transition – Integrated Community Approaches.\nThe five-year grant will support six work packages involving archaeological excavations and scientific analysis of early agricultural communities in the Zagros mountains of Iraq and Iran, one of the core regions where human societies first made the transition from mobile hunter-gatherers to settled farmers, founding the basis for the cities and civilisations that followed.\nThe study is one of the largest of its kind ever undertaken to provide the scale needed to investigate the local and regional development of communities along a major ancient routeway (later the Silk Road). It is led by Professor Roger Matthews with Dr Wendy Matthews, both in the Department of Archaeology, University of Reading. All work will be in full collaboration with colleagues in universities and state antiquities departments in Iran and Iraq.\nProfessor Roger Matthews said: “This generous grant from the ERC will allow us to highlight the unique archaeological significance of this region of the Middle East for one of the most significant episodes of change in the history of humanity. Our project is also seeking to establish a sense of community ownership of heritage sites, through engagement with local and national communities, which is vital for their protection in conflict-affected regions.”\nThe team of researchers and excavators will excavate at the important heritage sites Zarzi and Bestansur in Iraq, and Sheikh-e Abad in Iran which span these transformations from 17,000 to 7000 BC. These key sites lie within the under-researched area of eastern Fertile Crescent in the Middle East where communities were first able to settle and farm on fertile land in the biodiverse uplands east of the Tigris and Euphrates rivers.\nThe team will explore how ancient communities in the highlands and lowlands of Iran and Iraq were connected with each other and with other contemporary communities across the Fertile Crescent and beyond, following the routes later known as the Silk Road. Evidence previously uncovered at Sheikh-e Abad and Bestansur suggests people traded in materials across more than 1,500 km and that they were amongst the first communities to domesticate plants and animals, such as cereals and goats.\n""Prehistoric people in this region faced many challenges to their new community lifestyle which they had not experienced before"" - Dr Wendy Matthews, University of Reading\nPrevious excavations by Roger and Wendy in this region, supported by grants from the Arts and Humanities Research Council, the British Academy, National Geographic Society and other bodies, have uncovered evidence of Neolithic buildings constructed by prehistoric communities to support farming and trading lifestyles. The elaborate buildings and the artefacts discovered within them are being studied using a range of state-of-the art techniques.\nDr Wendy Matthews said: “Human societies have always had to adapt to change. Prehistoric people in this region faced many challenges to their new community lifestyle which they had not experienced before, such as living in larger more densely populated settlements and diseases that affected them, their crops and livestock. We hope to learn lessons from the past about how humans addressed these global challenges and climate change, allowing more complex societies to thrive.”\nRoger and Wendy are the only archaeologists currently directing archaeological research in both eastern Iraq and western Iran in collaboration with Iraqi and Iranian colleagues. Together with these colleagues they are providing unique insights into one of the core regions in the origins of agriculture and settled life, which are the foundation of communities today. As President of RASHID International, Professor Matthews works closely with local communities and authorities to preserve and promote awareness of Iraq’s cultural heritage. The organisation collects information on damage sustained at the sites and lobbies for international laws and agreements to protect them.']"	['<urn:uuid:23d38184-408c-4732-bebc-1719bdc1c29f>', '<urn:uuid:ac206501-fd05-4f28-a93c-69b62256b806>']	factoid	direct	concise-and-natural	distant-from-document	comparison	expert	2025-05-12T20:58:04.490895	7	47	1203
93	bedside screening benefits downsides community settings	Bedside screening in community settings offers several benefits: it enables reaching populations at risk who might not visit hospitals, allows using portable and easy-to-operate diagnostic tools, and can be conducted by trained community health workers to address healthcare provider shortages. However, there are important downsides to consider: the need for clear guidelines to prevent over-diagnosis, quality control challenges outside laboratory settings, and the requirement for proper training and safety protocols when handling biological samples. The screening must also ensure treatment availability and affordability for those who test positive to avoid unnecessary psychological stress.	"['Preventive Screening of Non-Communicable Diseases in sub-Saharan Africa\nOne of the recent major public health concerns in low and middle-income countries, especially in sub-Saharan Africa, is the escalating burden on non-communicable diseases (NCDs). It is predicted that by the year 2020, non-communicable diseases, of greatest interest being cardiovascular diseases, diabetes and cancer, will cause seven out of every ten deaths in developing countries (Boutayeb, 2006).\nThis trend is particularly concerning for low and middle-income countries because a majority of these countries are still battling with infectious diseases such as malaria, AIDS, and TB. The resulting ‘double burden’ of disease would be too high especially for low income countries that are already struggling economically and are characterized by suboptimal health systems.\nAnother alarming aspect of non-communicable diseases in developing nations is that the majority of suffers from the increased cases are expected to be relatively young i.e. the working population (WHO, 2009). This is concerning because most African countries are experiencing a demographic transition where the majority of its people fall within the working-age population, and there’s thus a great potential for these countries to experience a demographic dividend. Demographic dividend is the accelerated economic growth that can result from improved reproductive health, a rapid decline in fertility, and the subsequent shift in population age structure (Bill & Melinda Gates Institute for Population and Reproductive Health at the Johns Hopkins Bloomberg School of Public Health; Population Reference Bureau, 2017). I therefore believe that timely and effective interventions to reverse the trend of non-communicable diseases will not only save the countries from a high burden of disease, but will also be of great economic benefit.\nBorrowing lessons from developed countries, it is clear that the treatment and management of non-communicable diseases is extremely costly. In addition to the high economic burden of treating and managing NCDs at the country level, a heavy financial burden would also be experienced at the household level. Given the limited insurance coverage for NCDs, families would have to spend money on medicine, as well as deal with costs related to loss of income-generating opportunities because of the decreased productivity of ill family members. The best strategy for developing nations is prevention, before the situation becomes unmanageable. One way to achieve this is through preventive screening.\nWith the increased technological advances in diagnostics and preventive screening, we now have available cost-effective, rapid and easy to use tools and techniques for the diagnosis of a number of diseases. Examples include rapid blood glucoses tests (for risk of type 2 diabetes), Lipid and Blood Pressure tests (for risk of hypertension), PSA tests for the screening of prostate cancer and Clinical Breast Exams for breast cancer screening (a technique, not tool).\nMost of the rapid diagnostic tools are portable, use replaceable batteries and are easy to operate; therefore, screening services can even be provided in remote and otherwise difficult to reach places on the continent. The diagnostic tools have to be purchased, and health care providers have to be trained to use them, both of which are costs to be incurred. I, however, believe that if the governments and relevant stakeholders invest in this, given the current state and performance of our health systems, in the long run, the cost of prevention would be much less than that of treatment and management of NCDs. Also, we cannot screen for every single non-communicable disease.\nPriority should be given to diseases with the highest DALYs or highest prevalence, which in most African countries would be cardiovascular diseases, diabetes and cancer. Another advantage of preventive screening is that it provides us with data and evidence for which diseases are most prevalent and where; and what the differences are in prevalence between sub populations e.g. men vs women, urban vs rural dwellers, young vs older people. With this data, policy makers can make informed decisions on best ways to intervene and thus ensure the best use of resources.\nOne way that preventing screening can be done is through incorporating it into primary care, like the way it is done in developed countries such as the United States. However, within the African context, there would be two major challenges to this strategy. First, culturally, most people do not attend the clinic or go to the hospital unless they are sick. Therefore, screening at hospitals will miss a significant number of people who may be at risk for NCDs. Also, the screening results may give us a false sense of disease prevalence because considering the people who attend the clinics are already unwell, albeit, from other diseases, they may also be at greater risks of diseases for which we are screening given their compromised immunities. Secondly, screening at hospitals may not be feasible given the shortage of healthcare providers and the overwhelmingly large number of patients in these settings.\nI, therefore, think that the best action plan would be through mass screening campaigns in communities using trained community health workers. Mass screening in communities, say by setting mobile clinics, will ensure that the population at risk is reached, which would most likely not be achieved by screening at the hospitals. Community health workers address two challenges; first they are a solution to the shortage of healthcare providers to do the screening, and secondly, since they are usually known and trusted members of the community, they are in the best position to persuade community members to participate in the screening campaigns.\nLessons can be drawn from the successes and failures of HIV screening, one important lesson being the understanding of the community’s perception of the diseases (or if they have any knowledge of it), perceived susceptibility, severity and benefits of screening. It is very crucial that education campaigns are done first because if people do not understand the benefits of the screening, i.e. they do not think that they are susceptible or they are unaware of the severity of the conditions, they will not participate. Another strategy to be borrowed from successful HIV screening campaigns is that to ensure high participation, the tests should be offered at no cost to the community. Again, this is another cost to be incurred by the government and stakeholders, but the long terms benefits would outweigh the costs.\nAs we proceed with preventive screening, we have to be wary of over diagnosis. There have to be clear guidelines for which conditions we are screening for, who should be screened (who is really at risk of the disease), and who should be referred to treatment. We have to ensure that treatment is available and affordable for those who will need it as a result of screening, so as to avoid unnecessary psychological stress on the participants. Therefore, counselling services should also be an important component of the screening campaigns.\nScreening is only the first step to prevention. Following it, there will be cases that need treatment and should be referred to the clinic, but most importantly, the population at risk would require either behavioural or systemic interventions. Effective interventions for NCDs are not discussed in this article, as the topic deserves its own entry.\nBill & Melinda Gates Institute for Population and Reproductive Health at the Johns Hopkins Bloomberg School of Public Health; Population Reference Bureau. (2017, November 16). Retrieved from Demographic Dividend - Investing in Human Capital : http://www.demographicdividend.org/\nBoutayeb, A. (2006). The double burden of communicable and non-communicable diseases in developing countries. Transactions of the Royal Society of Tropical Medicine and Hygiene , 191-199.\nWHO. (2009). Noncommunicable Diseases, Poverty and the Development Agenda. ECOSOC High-level Segment 2009 .\nKwinoja Kapiteni, from Tanzania, is a first-year MPH student at the University of California, Berkeley. Kwinojas research interests in chronic diseases were influenced by her experiences working at a Health and Demographic Surveillance Site in Dar es Salaam, where NCDs were the focus area.', ""Point of care instrument, test strips and IT solution developed by Roche for critical- and primary care settings – for health care professionals. The development of lab-on-a-chip technology and its applications in biochemical and biomedical analyses has, during the last two decades, led to the potential realisation of portable and on-site detection devices, the so-called point-of-care (PoC) detection systems. neous use of regular laboratory procedures. bedside, physician’s Point-of-care testing. makes it difficult to isolate the cost of these procedures. Point of Care Testing (PoCT) means pathology testing performed near or at the site of the individual by a PoCT Operator at the time of the consultation or encounter. Although statistical and clinical heterogeneity is evident and only a small number of studies were included in the meta-analysis, our results suggest that POCTs might lead to faster discharge decisions. What is Point of Care Testing? The first phase involved collec-. No data are available that de-, scribe the elements of an effective disease management, involving this testing must be individualized for the setting, and environment where the device will be used. Widening the debate and informing the debaters will enhance the chances of making choices that achieve the best health for the most people at the best cost. There is a. great need for additional investigation into the use of point-of-care testing in patient care. The management strategy used for heparin and protamine added accuracy and precision, which was associated with improved hemostasis. Key to this demonstration pro-, ject was increased accessibility of laboratory data to phar-, rated into this project that was designed to evaluate the dis-, the patients reached and maintained their National Choles-, terol Education Program lipid goal at the end of the pro-, ject, with observed rates of persistence and compliance, with therapy of 93.6% and 90.1%, respectively, no control or matched cohort in this study, with patients not receiving this type of care cannot be, made. of CLIA-waived tests are available at http://cms.hhs.gov/clia. See also page 126, DOI 10.1345/aph.1D230. Methods For example, Jobes et al. Some 12,000 deaths and 15,000 hospitalizations due to adverse drug reactions (ADRs) were reported to the FDA in 1987, and many went unreported. PharmaD and MPharm in pharmacy practice/clinical pharmacy has focus on patient-oriented services The implementation process occurred in, 4 phases for 216 patients. Education/consultation and medication/therapy management were the most commonly evaluated types of pharmaceutical care services throughout the studied groups. Washington, DC: Institute of Medicine, The Effect of Intensive Treatment of Diabetes on the Development and Progression of Long-Term Complications in Insulin-Dependent Diabetes Mellitus. Drug-related morbidity and mortality are often preventable, and pharmaceutical services can reduce the number of ADRs, the length of hospital stays, and the cost of care. In several studies, levels, as well as blood gases and coagulation, are greater, are related to the loss of efficiency associated with batching, and high volume of tests in a central laboratory, pensive reagents and test strips, and, perhaps, increased la-, bor costs per test. Supplemental protamine was given twice as often to control patients and frequently when no heparin was detectable (retrospectively). Results: No model considered differences in access to tests. Although few studies have evaluated the impact of, sibility to laboratory testing reduces the amount of time, necessary to obtain important results. We present a real-time blood coagulation monitoring by using an externally vibrated, self-sensing piezoresistive microcantilever sensor for disposable point-of-care coagulation device. Detailed comparison of patients treated with digoxin during 1970 to 1972 at Massachusetts General Hospital and Peter Bent Brigham Hospital, 2 very similar Harvard Medical School teaching hospitals, showed the risk o f digitoxicity to be 2.12 times greater at the latter where serum digoxin assays were not commonly performed. Adult patients having primary cardiac operations were prospectively randomized into two groups. Studies that reported differences by race/ethnicity and studies where most participants were from multiracial/ethnic minorities were included. 2006 AACC Press ISBN: 1-59425-051-0. Motor and sensory nerve conduction velocities were faster in intensively treated subjects. point of care, although samples may be taken outside of the lab. Both analyzers provided precise results of pH, PaCO2, PaO2, and HCT, meeting CV% quality requirement values. This group was chosen due to a high use of blood analysis, a great possibility of benefit from rapid tur, ratory tests, a large patient population, and a lar, associated with this procedure. This role may, be expanded through new legislation on prescription drug, coverage for Medicare that provides payment for therapeu-, MEDLINE English literature searches using PubMed, and economics. To better define subclinical coagulation in man, we measured plasma fibrinopeptide A concentrations before, during, and after cardiopulmonary bypass. The CAP definition fails to recognize devices designed to, be used in the home by a patient or caregiver or to be used, technology is making many of these devices available for. Objectives: – The CLIA Certificate of Waiver application includes testing site … iv. It will be necessary to set new practice standards, establish cooperative relationships with other health-care professions, and determine strategies for marketing pharmaceutical care. achieving these improvements may involve additional, costs, regulations, technical problems, and administrative, issues. Promotion of safe, effective and rational use of medicines, patient counseling and monitoring of disease management through pharmaceutical care aspects were introduced from 1st and/or 2nd year programs in USA, Finland and Denmark A, comparison of techniques with measurement of subclinical plasma coag-. This study was undertaken to learn whether more precise control of drug variables and patient response would affect blood loss and transfusion requirements. Our findings reaffirm the clinical importance of rapidly achieving therapeutic levels of heparin. Devices that are CLIA waived vary from state to state, so lo-, Operator and public safety is also a concer, blood or other body fluids are used by the testing device, and, at times, the testing device requires chemicals that, the Occupational Safety and Health Administration, pathogens require the laboratory to establish and maintain, a written control plan that addresses the procedures to be, followed if an exposure occurs. The diagnosis of intra-amniotic inflammation can be easily made by detecting an elevated concentration of the cytokine interleukin (IL)-6 or the enzyme neutrophil collagenase, also known as matrix metalloproteinase (MMP)-8. These articles will provide a snapshot of a, ing it difficult for anyone to be truly current. designed to efficiently and properly use the results and de-, only demonstrated improved outcomes when associated, with disease management. Need of Quality Control in Point of Care Glucose Monitoring Devices, Pupillometry and neurotrauma: point-of-care technologies versus point-of-care techniques, Impact of point-of-care panel tests in ambulatory care: A systematic review and meta-analysis, Model Based Economic Evaluations of Diagnostic Point of Care Tests Were Rarely Fit for Purpose, Point-of-Care Devices for Pathogen Detections: The Three Most Important Factors to Realize towards Commercialization, New Biomarkers of Sepsis with Clinical Relevance, Comparison of Enterprise Point-of-Care and Nova Biomedical Critical Care Xpress analyzers for determination of arterial pH, blood gas, and electrolyte values in canine and equine blood, Comparison of Rapid MMP-8 and Interleukin-6 Point-of-Care Tests to Identify Intra-amniotic Inflammation/Infection and Impending Preterm Delivery in Patients With Preterm Labor and Intact Membranes, An oscillated, self-sensing piezoresistive microcantilever sensor with fast fourier transform analysis for point-of-care blood coagulation monitoring, Self-monitoring of blood glucose levels using glucometers in Riverina NSW: Patients' perspectives, Opportunities and Responsibilities in Pharmaceutical Care, To Err is Human. With such a self-sensing cantilever sensor technique and FFT analysis system, the prothrombin time (PT) result measured by the self-sensing microcantilever technique exhibits a strong correlation with that of clinically used prothrombin time measured by the commercial instrument of mechanical detection. Am J Clin Pathol 1995;104(suppl 1):S1, diac markers: point of care testing. The newer POCT devices have an advanced level of connectivity with laboratory information system (LIS). Comparative studies, demonstration project reports, and systematic reviews were selected. Bland‐Altman plots demonstrated varying degrees of bias, but good agreement between the 2 analyzers was seen when arterial blood gases and electrolytes were measured, except for PaCO2 and Cl−. Management strategies emphasizing the demand side of the market are more empowering to providers and patients and, given the increasing knowledge and accountability of these stakeholders, are increasingly feasible. The guidelines promote consensus priorities, multidisciplinary teamwork, fiscal coordination, and collaborative practice during this phase of rapid change. GPP guidelines adopted by India and the Vision 2020 emphasize highest professional and ethical standards of pharmacy. Careful exploration, of published data and recommendations is essential to devel-, crease accessibility to laboratory testing, make testing results, more rapidly available, improve the quality of therapeu, decision making, and improve treatment outcomes in pa-, positively impact patient care without being joined with, administrative systems designed to effectively use data, of the most effective programs incorporating point-, istics probably revolve around the specific test used, the, nature of the disease treated, and the details of the environ-, lutions to these problems have been proposed, but few of, the currently available devices include them as part of the, proposed, none has been thoroughly evaluated with docu-, mentation of results. Only results, from traditional laboratory procedures were used in deci-, sion making. Pharmacy programs offered in, Healthcare remains a dominant issue for Canadians. Amniotic fluid white blood cell (WBC) counts were determined using a hemocytometer chamber. Of the respondents 40% taught themselves while 60% were taught by health staff to use glucometers. This study also supports previous findings that the costs of STAT blood analysis are more personnel-related than equipment-related. Regarding the question of economics with point-, testing, 2 items must be considered: the cost of the test per, patient compared with standard laboratory procedures and, the economic impact on patient care and total healthcare, ing costs of this testing, it is difficult to determine its econom-. Personnel-Related than equipment-related an elevated amniotic fluid white blood cell ( WBC ) counts determined! Observation is valid, the profession of pharmacy degree has replaced the Bachelor of Science as! Social responsibility to reduce preventable drug-related morbidity and mortality is explored however, ease of use of serum determinations! Between pharmaceutical care services throughout the studied populations we present a real-time blood coagulation reaction, the, seen these. Instruments, which may adversely affect your experience on the accuracy of testing! • a CLIA certificate is required for each testing site with some specific exceptions and compared POCT laboratory. Tool is the dynamic tension among the value, accessibility and affordability of.... Faster in intensively treated subjects time or CABG patient volume clinic, or transportable devices each testing with... Content introduction to POCT principles Interpretation treatment literature NUTH experience major adverse with. Could be demonstrated therapy ) and poor disposal of biological waste were significant.! With health care apothecary role but has not yet been restored to its utilization in hands! Cardiac surgery, ing it difficult for anyone to be using incognito/private browsing mode or an blocker! Micro-Electro-Mechanical systems, are also denoted as micromachines, … iii whether more control! And systematic reviews were selected simply using point-, of-care testing for therapeutic decisions before during! On point-of-care testing devices and technology are increasingly used in the sec-, group. Protamine by conventional methods v. government regulations and legislation impact the use of a POCGMD its. Management and patient outcomes may not oc-, truly assess the economic and 37 % addressed the point of care testing pdf of! No difference between point-, of-care testing for therapeutic decisions these data, other resources such. Do not push or pull the test immediately to the patient for immediate testing that... Individual devices have an advanced level of connectivity with laboratory testing clearly improve patient outcomes balance of quality.. Edition published in 2015 the care of patients, cant increase in the current advances Lab-on-a-chip. Frequency of adverse reactions to digoxin in 2,425 digoxin-treated patients ranged from 13.1 % to 19.4 % supports... Poct principles Interpretation treatment literature NUTH experience controlled than previously as useful tests have become available for point-of-care determination either... And intact membranes, those with intra-amniotic inflammation have worse obstetrical and outcomes! The best balance of quality, access and costs patient or other caregiver 2000 ; 46: heparin protamine... 5 years of the EPOC analyzer provides consistent, predictable protocols that are beneficial of protamine than control. Velocities were faster in intensively treated subjects assess the economic impact a mean 7.4! Join ResearchGate to find the people and research you need to help your work accuracy various. % quality requirement values inclusion criteria evaluated only 1 type of patient outcome, primarily clinical outcomes predominantly. Digoxin toxicity referred to as ‘ near patient, bedside, or transportable.., improved clinical outcomes testing ’ culture results ( aerobic and anaerobic bacteria as as... Overall cost of health care in America, Institute of Medicine practice settings are also.. Testing that occurs at the point of care Urinalysis Testing.doc rev will be addressed in future studies and project. Settings, not just in healthcare institutions diagnostic tool for clinicians and medical practitioners also! Early in the current health care provider dose adjustment ) is the most cost-effective option, feasible... • a CLIA certificate is required for POCT devices facilitate ways to improve the quality and outcomes overall of! System ( LIS ) cost and length of stay ( LOS ) at the point of,... Does not improve patient outcomes found by testing d. place on instrument test table studied populations devices facilitate ways improve... Not statistically significant, sion making dose and heparin concentration during cardiopulmonary but! The ACT exceeded 400 seconds multiracial/ethnic minorities were included denoted as micromachines, … iii degree has the... Were prospectively randomized into two groups trials and before-after studies expenditures increase for adjustment f! Higher equipment costs, regulations, were included convenient handheld, portable, or laboratory! Compared POCT with laboratory information system ( LIS ) among the value, accessibility and affordability of drugs to preventable. ; point-of-care testing in patient outcomes to the patient or other caregiver is valid, the profession of has! Abstract point of care: in a community phar, betes care program 97:222-3. ing a! Tendency for interprofessional collaboration in the intensive care unit experience the major adverse with. Laboratory walls some problems occur that were not problems within point of care testing pdf laboratory some. An increased tendency for interprofessional collaboration in the delivery of care at fixed frequency and fixed amplitude of especially... Re-, sults between the Enterprise Point‐of‐care ( EPOC ) and correctly predicted the effective heparin dose Pharm 2003. Important to determine the clinical reliability of the respondents 40 % taught themselves while 60 % were taught health... With this move outside the laboratory walls some problems occur that were not problems within the laboratory walls some occur..., developed a decision analysis model to evaluate the eco-, although samples be... Intensive therapy was a nearly threefold increase of severe hypoglycemia may adversely affect your experience on site!, were included termed disposition decision ( DD ) time, easy-to-handle systems, offering sample-to-answer! Heparin therapy ) and correctly predicted the effective heparin dose we measured plasma fibrinopeptide values. 11. point of care Urinalysis Testing.doc rev has not yet been restored to its erst-while importance in care. Which are either hand carried or transported to the vicinity testing for therapeutic decisions services throughout the populations... Clinicians and medical practitioners loss and transfusion requirements the control patients not problems within the laboratory walls some occur... Were taught by health staff to use glucometers, demonstration project reports, and improved QC protocols are. Designs, which may adversely affect your experience on the accuracy of various testing devices in comparison, with laboratory! Reliability of the studies meeting inclusion criteria evaluated only 1 type of patient,., surement of coagulation after cardiac surgery, ing for critically ill:... Of blood glucose concentrations be impressive enough to contin- field is point of care testing pdf need of whole-blood standards, harmonization among,. The patient via convenient handheld, portable, or near the patient immediate., innovative drugs improve health and economic outcomes for individuals and populations reduce preventable drug-related morbidity and mortality pharmacists at! At an increased tendency for interprofessional collaboration in the inpatient setting, HMO found these data, healthcare. Cp, St John a, comparison of techniques with measurement of subclinical plasma coag- fixed and... Used for heparin and protamine added accuracy and precision, which make it difficult isolate! Patient self-testing with health care resources, such as testing in patient care immunosorbent assay ( ELISA ) tests available! Between point-, conventional testing to decision regarding disposition that is, admission/referral termed decision... And/Or humanistic, were included to apply basic pharmacologic principles and establishment of consistent, predictable protocols are! For point-of-care determination of either point of care testing pdf or IL-6 sensed when the ACT exceeded 400.. Properly used or incorrect decisions made that do not push or pull the test immediately to vicinity... But has not yet been restored to its erst-while importance in medical care, agreed point of care testing pdf these,.""]"	['<urn:uuid:36e4f3d6-b4d8-44c2-8fe9-299894a69733>', '<urn:uuid:de23d774-ae04-4a10-b97e-16f90e0d6571>']	open-ended	with-premise	short-search-query	distant-from-document	multi-aspect	expert	2025-05-12T20:58:04.490895	6	93	4097
94	How do companies measure their social and environmental impact?	Companies measure their impact through the '3Ps' approach (Profit, Planet, People) and environmental justice metrics. The Planet aspect measures environmental practices, while People involves community impact and labor practices. Companies also use tools like EJSCREEN to track their impact on environmentally overburdened communities, conduct environmental compliance inspections, and engage with local communities through meetings and outreach to local organizations.	['Corporate social responsibility still matters28 Nov, 2019\nIndian companies are focusing exclusively on community engagement, corporate governance, and environmental issues.\nThis is evident through annual releases of the company’s CSR (Corporate Social Responsibility) reports.\nLet’s look at what makes the CSR report a powerful medium to raise awareness and tackle social issues.\nWhat is the meaning of a CSR report?\nThe CSR report has evolved into a business model for Indian companies to present themselves as a responsible organization.\nIt is a systematically constructed report that enhances a corporation’s reputation as a responsible business. Every CSR report comes with an internal and an external perspective.\nThrough a well-designed CSR report, a company’s economic, social and environmental performance are presented to its external and internal stakeholders. A successful CSR program aligns business strategy with its CSR strategy.\nSome companies combine environmental management and employee relationships with the same CSR report. Others, address them individually.\nAlthough a CSR project report is expanding in reach, they are either not strategically constructed or poorly communicated.\nAn impactful CSR report is one that is designed strategically (preferably with the help of a report design agency). A combination of inputs from the R&D team, marketing team, company branding department, talent management, and operations team is necessary.\nHow can you measure your CSR performance?\nThere is an increase in Indian companies incorporating social responsibility within their company operations.\nCompanies are not just focusing on building a brand, driving business or generating profit. The 3Ps or ‘triple bottom line’, is what they use to evaluate complete business performance.\nThe 3Ps are: Profit, Planet and People\n- Profit - is the economic value that the company generates. The cost of input & capital invested govern this metric\n- Planet - is the environmental impact of the company. This is measured through the usage of healthy environmental practices\n- People - involves the community. This also includes labour practices and employees\nIf the above 3 metrics are monitored well, a company’s CSR strategy can be constructed and conveyed to employees well. Company goals, employee relationship, and social endeavours are all parts of a successful CSR strategy.\nWhat are the key aspects of a CSR report?\nAs mentioned earlier, every element included within a CSR report needs to be accompanied by an internal and external aspect.\n1. CSR strategy\nThis is the first element that needs your time and resources. With a CSR strategy in place, defining future company goals becomes easier.\nThe internal aspect of a CSR strategy would be to gain support and participation of high-level executives. Speak to them, gather their inputs and seek approval over the strategy.\nThe external aspect involves gaining the confidence of external stakeholders. This can be done by preparing a strategy that will highlight your company’s vision about reaching environmental goals.\n2. CSR management\nOnce the CSR strategy is in place, work on setting up a CSR management system. This phase includes laying down pointers to address what your company’s operational tasks would be.\nThis internal aspect here includes gathering feedback as to how the strategy is received. Based on this feedback along with individual departmental inputs, build a management plan. The management system needs to be customized as per your company’s organizational context.\nThe external aspect of a successful CSR management system would be to use it as a medium to receive external recognition. Engage with other organizations during events where you can learn about their CSR strategies.\nIdentifying the people to work with is key to achieve CSR-related results. Locate the community where you feel your company can make the greatest impact. Speak to NGO’s and understand what you can do differently to make a difference.\n3. CSR reporting\nWith the strategy set, and the management system in place along with successful completion of community-based activities, it’s time to report the same. CSR reports are published on an annual basis. The report is an attempt to share the company’s CSR activities with company stakeholders.\nA CSR report tests your reporting team’s coordination and management skills. The internal aspect here is to be on the same page when it comes to the layout, content, and elements.\nThe external aspect is the report release. The report needs to stand out. It needs to be visually appealing and tell a relevant story as to how your company impacted the environment or made a substantial contribution to the community.', 'ESG and Environmental Justice\n“ESG” — referring to environmental, social, and governance factors that influence decision making — has moved quickly from the fringes of policy to the center of corporate decision making, with the US Securities and Exchange Commission (SEC), the Federal Trade Commission (FTC), and other governmental entities regularly weighing in on what businesses need to report and what they can say advertising in this area. While “E” in ESG stands for “environmental,” “S” stands for “social,” meaning how businesses interact with their employees and — more particularly — the communities in which they operate.\n“Environmental justice” sits at the interplay between “environmental” and “social” factors which can be evaluated under the ESG umbrella. The US Environmental Protection Agency (EPA) defines “environmental justice” as “the fair treatment and meaningful involvement of all people regardless of race, color, national origin, or income with respect to the development, implementation and enforcement of environmental laws, regulations and policies.”\nBelow, we will unpack what EJ is, how it relates to ESG, and outline steps businesses can use to evaluate ESG risks and opportunities relative to EJ concerns.\nThe Rise of Environmental Justice\nWhile “environmental justice” as a concept dates to the Civil Rights era, consciousness of the effects of social status – including race – on public health date back far longer.\nThough past decades have seen significant progress in the environmental space, it has not been uniformly distributed. There is broad acceptance that the benefits of environmental progress, and the burdens of environmentally intensive industries, are not uniformly dispersed among Americans. The Biden Administration’s EPA has made addressing EJ concerns a primary focus by integrating EJ concerns into ESG initiatives at the federal level, including working to increase equity in the environmental space through programming and funding, requiring equitable considerations while advancing the clean energy transition, and inching toward more substantive changes to how federal and state regulators administer and enforce federal environmental laws such as the Clean Air Act and the Clean Water Act.\nKey steps EPA has taken to promote equity in the environmental space include:\n- Actively working to build capacity of local organizations to engage in the environmental space. (See Actions 2 & 4 in EPA’s Equity Action Plan, discussed in greater detail here.)\n- Increasing environmental compliance inspections in “environmentally overburdened” communities to the point that more than 50% of all inspections will occur in such communities. (For more on this, see here.)\n- Allocating significant funding for community lead projects, most recently, $100 million in January 2023 (See our discussion here.)\n- Redoubling EPA’s focus on civil rights issues in terms of program administration (see here for a discussion of how this plays out in practice).\nIn terms of substantive changes, the primary substantive changes stem from increased prominence of civil rights issues and assessment of “cumulative impact” in the environmental space.\n- Civil Rights Issues. For most projects, state or federal regulators have focused narrowly on individual projects. Which projects can be and are permitted may depend on shifting regulatory perspectives that determine whether a project meets regulatory limits and, in combination with planned or pre-existing factors, has the potential negatively affect a community. The past year has seen EPA repeatedly evaluate civil rights issues in connection with projects fairly far along in the permitting process, The two foremost examples are its actions related to the relocation of a recycling facility in Chicago and a petrochemical plant in Louisiana.\n- “Cumulative Impacts.” Environmental laws in the United States have focused on permitting and process. In general, regulators are narrowly focused on a project’s effects versus pre-existing regulatory limits. We have previously examined this issue as applied in regard to the Ninth Circuit’s decision in Center for Community Action v. FAA; and more generally in the context of federal and state efforts to formalize assessment of “cumulative impacts.”\nEJ and ESG\nEJ falls at the intersection of the ‘E’ and ‘S’ of ESG, and good governance is required to manage exposure to EJ risks. Assessment of ESG issues frequently involves conducting a quantitative and qualitative assessment of environmental, social, and governance issues. As we have discussed, this has often involved assessments of both environmental and social concerns when addressing issues like plans to reduce carbon emissions.\nEJ is similar some other environmental issues in that quantitative metrics are — to some degree — available. For instance, federal environmental permits often require extensive data collection and reporting. (See here, for an example.) In terms of understanding how corporate operations interplay with “environmentally overburdened” communities, various databases exist which track whether communities fall into this category. EJSCREEN, EPA’s primary tool, is discussed in greater detail here. Corporations are also engaging with the communities in which they operate by holding community meetings and reaching out to local organizations.\nWell-positioned Companies Understand Corporate Exposure on EJ Issues\nKnowing whether business operations are in “environmentally overburdened” communities is a good starting point. When businesses operate in such communities, ensuring consistent community engagement is the first step in minimizing exposure. Additional steps:\n- Consistent community engagement. Consistent community engagement is a keystone to managing EJ issues. If community leaders are only in contact with a business when the business has a need to report an environmental issue, tensions are likely to be higher and community leaders may reach out to regulators as a first step to raise concerns, instead of relying on pre-existing relationships with at the business. As EPA has stated that it intends to redouble efforts to engage in EJ communities and to use tools like increased monitoring to better assess and address community concerns in real time, having resilient relationships to begin with is a must.\n- Making sure consciousness of EJ-associated risks is factored into the corporation’s governance strategy and overall risk profile. To state the obvious, every business’s risk profile is different. As we have indicated in a recent post, energy intensive businesses currently face a heightened risk of litigation in the ESG space, whereas a software or marketing company might have no meaningful exposure in the EJ space. Of relevance here, businesses seeking to relocate or re-permit operations in EJ communities may need to be strategic about what EJ-related disclosures are made and when. Some of the “asks” EPA has recently made, such as asking a petrochemical company to relocate a school as a potential solution to resolve concerns related to facility expansion, can pose significant challenges in terms of ESG reporting. And EPA’s “asks” in this space can come tied – as they did in Louisiana –to America’s long history of racial discrimination, a topic with which every business would want to avoid any association.\n- Clarity in EJ-related sustainability disclosures or corporate reports. Businesses need to evaluate where and when EJ-related ESG disclosures are made in the context of prior disclosures. As we discussed above, EJ disclosures both have an “environmental” and a “social” aspect to them and would be made in a context where the underlying law is rapidly evolving. As with other business disclosures, corporate ESG disclosures or sustainability reports in the EJ space need to rely on verifiable data and strike an appropriate balance between aspiration and reality. Broad disclosures like “the Company intends to comply with the letter and the spirit of all environmental laws” may not be appropriate given the underlying legal uncertainty.\nIn an upcoming post, we will provide greater detail on how environmental justice issues have the potential to lead to litigation in the ESG space.\n- Related Practices']	['<urn:uuid:4bed4fa6-9f83-48c7-9aeb-09761fceee5d>', '<urn:uuid:4c8c77b4-d2c2-4bc5-a94e-4e952d0ad1cf>']	factoid	with-premise	concise-and-natural	similar-to-document	three-doc	novice	2025-05-12T20:58:04.490895	9	59	1987
95	What makes frogs able to jump so far?	Tendons allow frogs to jump great distances. While strong muscles are needed to propel against gravity, muscle power alone cannot explain the speed and distance that frogs achieve.	"[""How can we replace the beautiful Panamanian golden frog. Well, we can breed them in zoos or laboratories. Or go out and find a new golden species to conserve. Take your pick.\nFrom cougars to tree frogs and tiger to elephant, we protect the wild from many threats. It is not only orangutans that are affected by lack of planning and knowledge in wildlife reintroductions. The situation on the ground and in the labs that unearth genetic mistakes is made clear with painstaking research. The future could leave us with little wildlife in Africa, SE Asia or in fact, anywhere, unless the planning is logical and forward-looking.\nWhen threatened species face both competition and hybridisation from a relative, the best techniques for assessment are needed. The giant salamanders stand apart as unique endemics in NE Asia, but now the Chinese species has begun invading the rivers in which the other member of its genus lives. This could be curtains for Andrias japonicus if the IAS manages to gain a foothold and interbreed.\nHow often do we discover a new species, only to lose it? Many of the genus of these animals have already disappeared, as the dreaded fungal infections continue to decimate our amphibian around the world. Maybe the conditions of its natural habitat will enable this beautiful new species to survive. We can but hope!\nThere are so many new species out there waiting to be found and it is essential we find them before so many become extinct. Here is a wonderful island that will suffer the fate of the rest of its nation, if we cant stop the habitat destruction with which Asia has replaced its uniquely biodiverse forests.\nYou would have thought we could have worked out how the amphibians move around by now. But no, we have not, until two scientists carefully X-rayed the mechanics of the frogs leg. The French have been eating them for so long, you would have thought they would have noticed the\ndynamic catch mechanism. But no, you never think about the amazing and delicately adjusted mechanism disappearing into your maw.\nHow much research flows out of Madagascar, on the lemurs, chameleons and frogs alone. We have to preserve this island and sustain its people in their struggles with nature, including climate change. The age of introspection is over. This is one planet and we all are one with it just investigate the biodiversity and the climate change conferences mushrooming in response to popular demands.\nSome carnivorous toads find it useful to breed when theyre starving. The herbivorous tadpoles make a fine snack! Here is another possibility in the beautiful poison dart frog, Dendrobates tinctorius.\nThe life of the Yasuni Reserve in the Amazon is not as we know it. With 44 times the biodiversity of the whole of one country in one hectare, it exhibits a show of insect, amphibian, bird and even mammalian species that we cant imagine. Now replace them all with oil rigs.\nThe tree frog, like the gecko requires extraordinary adhesion from its toes.\nAround one-third of Salamander larvae vanished from sites in North Carolina, USA, during severe drought, according to new research.\nWhen a dead red eyed tree frog's egg is used as food by fly larvae, that is an important part of the decomposition of living material. Frog flies tend to choose dead eggs but can lay on others that are healthy nearby.\nEcology of coastal giant salamanders, (Dicamptodon tenebrosus). Research into the genetic structure and history of giant salamander populations in the United States and Canada.\nSave the Frogs Day 2012 is on April 28th. An amphibian conservation charity has organised an extensive website and a 'Save The Frogs' day, now in its 4th year of frog conservation awareness.\nNew family of tailless burrowing caecilian tetrapod amphibians has been discovered, but Chikilidae have no legs.\nMammals, birds, amphibians and many other groups are exposed to development or climate change threats in isolated parts of the Andes in Amazonian Peru and Bolivia.\nTwo new species of frog discovered in New Guinea, one the smallest vertebrate yet. One small step for Paedophryne amauensis is a giant step for the evolution of the tiniest vertebrate footprints anywhere.\nThe University of Florida has co-authored a new study on the endangered Ozark Hellbender giant salamander. The study aimed to observe and look deeper into the declining health and habitat of the salamander and can also be used as a measure of how our ecosystems are changing and affecting amphibians across the world.\nA study recently published looks at the possibility that toads can predict earthquakes. A toad breeding lake emptied a few days before the much-reported Italian M6.3 earthquake on April 6th 2009 and was only revisited by its former occupants after a series of aftershocks.\nThere is nothing more delightful than waking in South East Asian forests to the gibbon dawn chorus, but South America Howlers and African Colobus seem to compete more loudly. In an intriguing and difficult investigation by Anne Schel and Klaus Zuberbuhler, communication in animals reports a dawn chorus from insect, amphibian and bird.\nA combination of threats has severely threatened the survival of amphibians around the world. New research underlines the need for proactive conservation efforts to avoid extinction for many of the worlds amphibians.\nResearch has shown that tendons allow frogs to jump great distances. A jump requires strong muscles to propel an animal's body against the pull of gravity and muscle power alone would not explain the speed and distance that frogs are able to achieve.\nThe sudden appearance of a fungal epidemic that is wiping out amphibian species worldwide. Amphibians are currently engaged in a battle for survival - and losing.\nAustralian researchers have found that green tree frogs Litoria spp. use condensation in the same way as windows on frosty mornings. During the dry season from June to September, Ozzy water is a precious commodity. These enterprising Amphibia expose themselves in such a way as to gather the air's excess moisture when temperatures plummet.\nIsraeli beetles turning the tables on five spp. of Amphibian. The classic instance of predators stalking their live and not-so-innocent food supply is reversed with the prey waiting to be stalked by the naive predator. They then turn the tables by killing the villain - but just where would the observers' sympathies lie?\nChytridiomycosis, a fungal infection which has caused 200 amphibian species to disappear is coming under renewed attack from scientists looking at combining previously used methods to tackle it.\nMadagascan species at risk of extinction are commonly found on sale in Thai markets says a WWF-backed body investigating the trade in endangered species. TRAFFIC surveyed sales for 15 days in Bangkok and eight of Thailand's provinces and found 591 Madagascan reptiles and amphibians available on sale.\nA new species of large, land-based, carnivorous crabs has been described in toady's PloS ONE - found all over the Hawaiian Islands. The catch is that they have been extinct for a thousand years - tipped into an early demise by newly arrived Polynesian colonists.\nUnusual hybrid frog reveals importance of considering genetics in amphibian conservation. The hybrid frog is providing scientists with exclusive insights into the genetic make-up of different amphibian populations - with important bearing on future conservation measures. The unique hybrid was produced by scientists at the University of Manchester, who allowed two entirely different, critically endangered species of Central American leaf frogs to interbreed\nFive species of frog, some unseen for 136 years, have been rediscovered in India. Amphibians of India aims to rediscover 50 species of Indian amphibians that are considered to be actually, or potentially, extinct in the wild. These are the so-called 'lost' species. Launched in December 2010, it is an ambitious campaign led by the University of Delhi in conjunction with the IUCN and Conservation International.\nHuman intervention is causing a serious decline in the world turtle population. 2011 has been designated 'The Year of the Turtle' by Partners in Amphibian and Reptile Conservation (PARC). Turtles are currently disappearing from the planet faster than any other kind of animal and although they have been around for about 220 million years.\nColombian rainforests under threat due to an increase in production of coca to meet world demand for cocaine. More than 1,821 species of birds, 623 species of amphibians, 467 species of mammals, 518 species of reptiles and 3,200 species of fish are found, mainly in the country's vast tracts of tropical forest.\nA critically endangered species, the La Loma tree frog, Hyloscirtus colymba, has been successfully bred for the first time as part of a project involving the Smithsonian Institute's National Zoo. The scheme, involving nine partners as part of the Panama Amphibian Rescue and Conservation Project offers hope, not just for the survival of the La Loma tree frog but other endangered amphibians from the region.""]"	['<urn:uuid:eeca751c-10f8-4d67-accb-19685e2ddce1>']	open-ended	direct	concise-and-natural	similar-to-document	single-doc	novice	2025-05-12T20:58:04.490895	8	28	1480
96	Which tank is larger: Standhill Farm's greenhouse rainwater tank or Underground Garden System?	The Underground Garden System tank is larger with a 5200 litre capacity, while Standhill Farm's greenhouse uses two 1100 m3 rainwater storage tanks.	['Until the 1980’s Scotland had a thriving tomato industry in the Clyde Valley region, near Lanark. These relatively small family businesses supplied the local Scottish markets in nearby Glasgow and Edinburgh, but the buying power of supermarkets, increasing fuel prices and cheap competition from Europe gradually undermined their viability and one by one they were either forced to grow another crop or cease trading. Eventually, none were left.\nSo, some might be surprised to learn that tomatoes are in production in Scotland again, but this time with a business plan that is set to change the old economics.\nJim Shanks, whose family have been dairy farmers at Standhill Farm near Hawick in the Scottish Borders for over 60 years, started to look at renewable energy more than 4 years ago. The first step was to install an Anaerobic Digestion plant to recycle farm waste and create biogas to fuel a CHP system. Initially, the CHP was used to generate all the electrical power for the dairy operation, export surplus power to the grid and use the heat to dry woodchip for biomass, but Jim also felt that building a glasshouse heated by biomass would complete the renewable energy cycle.\nAfter much research, Jim chose turnkey specialist Certhon and their UK partners CMW Horticulture to design and build the project. ‘Certhon and CMW had a different approach than other potential suppliers – from the outset, they were keen to focus on the financial viability of the project and gave me great confidence that it could work out’ says Jim. That was particularly important because the technical requirements of successful modern tomato production were completely new ground for him.\nCerthon and CMW supplied and installed everything required for a long season tomato crop: Glasshouse, heating, screens, irrigation and water treatment, ready for planting in January 2017.\nThe completed 16,000 m2 glasshouse with 6m gutter height and diffused glass consists of two compartments and a work area/irrigation room controlled by a Priva Connext computer. Heating is provided by two RHI compliant 1MW Herz biomass boilers connected to two 121 m3 horizontal heat storage tanks. Notwithstanding the revenue generated by the boilers through the RHI, energy saving is a high priority, so Svensson 1347 FR overhead screens and Bonar Phormium Phormitex Crystal V gable screens have been installed.\nThe focus on sustainability at Standhill Farm doesn’t end with heating and electrical energy, as rainwater collected from the greenhouse roof and stored in two 1,100 m3 water tanks is the main water source, with a further two 90 m3 tanks to store water from the farm’s borehole.\nThe crop is grown in Cultilene rockwool on Formflex hanging gutters with Priva air tubes for air circulation through the crop underneath, and all the drain water is collected, recycled and sterilised by a Priva C-Line Vialux UV system. ‘The Vialux has been crucial, as it massively reduces the volume of water needed’ says manager Mark Wilkinson. We aim for 28-30% drain and by using the Vialux we always have sufficient for the crop, we know it’s safe to re-use, and at the same time our fertiliser cost is substantially reduced. The nutrient solution is precisely controlled by a Priva Nutriflex.', '5200 Litres Premium Underground Rainwater Harvesting Garden System\n|Length - mm||2700|\n|Width - mm||2700|\n|Height - mm||1725|\n|Capacity - Litres||5200|\n|Capacity - Gallons||1143|\n|Suitable for Drinking Water (Potable)||No|\nThis 5200 Litres Premium Underground Rainwater Harvesting Garden System has all of the features of the Economy system but adds several extras to increase usability and efficiency. The garden system is ideal for collecting and storing rainwater to use in the garden for a variety of applications. These include:\n- Garden irrigation\n- Washing the car\n- Cleaning windows\n- Power washing driveways and patios\n- Outside taps and hosepipes\nThe 5200 litre system is suitable for medium sized applications and will provide an excellent supply of water for use in your garden.\nThis system includes:\n- 5200 litre polyethylene tank\n- Pedestrian rated telescopic neck and lid\n- P Series stainless steel submersible pump with float switch\n- Rodent guard\n- Calmed inlet drainage pack with rainwater filter\n- Pump turret\n- Pump lifting chain\n- Floating intake\n5200 Litre Tank\nThe 5200 litre tank is rotationally moulded from high strength polyethylene and complies with EU and UK regulations. Designed with strength in mind, it is shorter in height compared to other tanks on the market today. This means that a shallower hole is needed to accommodate the tank, and this can be by as much as 500mm less.\n- 5200 Litre / 1143 Gallons capacity\n- Made in the UK\n- Made from high strength polyethylene\n- Ribbed and curved design provides added strength and efficiency\n- Low profile means a shallower dig is required\n- Large 600mm diameter access allows inspection and maintenance\n- Easy to install with environmentally friendly pea gravel backfill\n- Ideal for areas with a high water table\n- 10 year warranty\nThe ribbed design provides added strength and efficiency, while also allowing for an environmentally friendly pea-gravel backfill, eliminating the need for a time consuming and high cost concrete base and backfill to be used. The curved element of the design also enables all water, even from the bottom of the tank to be pumped out.\nThese tanks are amongst the most dependable on the market and as such are supplied with an industry leading 10 year warranty. With the correct installation and general on-going care, these tanks have a long lifetime ahead of them with very little or no maintenance required.\nIn-Tank Filter with calmed inlet\nAs standard, a PF filter is pre-installed into the neck of the tank. The filter removes leaves, grit and other debris from the rainwater passing through. The filter requires very little maintenance, usually about 3-4 times each year and is very quick and easy to clean with no consumables, just lift, rinse and replace.\nThe calmed inlet slows down the speed the water enters the tank and introduces the water into the lower part of the tank. As a result of this controlled entry, the water does not disturb any sediment at the base and replenishes the supply of oxygenated water throughout the tank, not solely at the top. This keeps the water inside the tank fresher for longer.\nThis system is supplied with a P Series stainless steel submersible pump which sits in the bottom of the tank and has sufficient power to service most garden based water requirements such as hoses and irrigation.\nThe pump combines with a float switch to help protect the pump from running dry and potentially causing damage.\nPump Lifting Chain\nThis allows you to quickly and easily pull the pump up towards the top of the tank for maintenance or inspection purposes.\nThe pump turret is a flexible plastic connection between the pump and access port which allows smooth flow of the pumped water out towards its point of use.\nA floating intake ensures that no water from either the surface or bottom of the tank is taken into the pump. This means that only clean water passes through the system and is pumped to the point of use.\nThe rodent guard protects the water inside the tank from rodents and large debris which could contaminate the water by entering via the overflow.\nImportant Delivery Information\nPlease note the price of this tank includes delivery to an address in England or Wales. Deliveries to Scotland, Northern Ireland and offshore islands may incur a surcharge. Please contact us for delivery charges to any of these locations.\nThe delivery will be unloaded from the vehicle by the delivery personnel; however, responsibility then passes on to the customer to move the tank into position.']	['<urn:uuid:cdb3f478-114a-4441-bf11-3aeeeba56c08>', '<urn:uuid:fce4a4a4-fdb0-4dc8-a875-2dff2189b68e>']	factoid	direct	concise-and-natural	similar-to-document	comparison	novice	2025-05-12T20:58:04.490895	13	23	1294
97	splunk data consolidation architecture incident review investigation workflow capabilities	Splunk's data consolidation architecture typically involves universal forwarders sending unparsed data from workstations or production servers to a central Splunk Enterprise instance for consolidation and indexing. This architecture supports comprehensive incident review capabilities where analysts can drill down from graphical elements to raw data and wire data captures. The incident review workflow includes unique actions that allow pivoting on common information to rapidly develop threat context, bulk event reassignment, and changes in status and criticality classification. All analyst activities are logged for auditing purposes, and the platform uses data signing to maintain chain-of-custody and detect any alterations to the original log and event data.	"['Forwarder deployment topologies\nYou can deploy forwarders in a wide variety of scenarios. This topic provides an overview of some of the most useful topologies that you can create with forwarders. For detailed information on how to configure various deployment topologies, see Consolidate data from multiple hosts.\nData consolidation is one of the most common topologies, with multiple forwarders sending data to a single Splunk instance. The scenario typically involves universal forwarders forwarding unparsed data from workstations or production servers to a central Splunk Enterprise instance for consolidation and indexing. In other scenarios, heavy forwarders can send parsed data to a central Splunk indexer.\nHere, three universal forwarders are sending data to a single indexer:\nFor more information on data consolidation, read Consolidate data from multiple hosts.\nLoad balancing simplifies the process of distributing data across several indexers to handle considerations such as high data volume, horizontal scaling for enhanced search performance, and fault tolerance. In load balancing, the forwarder routes data sequentially to different indexers at specified intervals.\nForwarders perform automatic load balancing, in which the forwarder switches receivers at set time intervals. If parsing is turned on (for a heavy forwarder), the switching will occur at event boundaries.\nIn this diagram, three universal forwarders perform load balancing between two indexers:\nFor more information on load balancing, read ""Set up load balancing"".\nRouting and filtering\nIn data routing, a forwarder routes events to specific hosts, based on criteria such as source, source type, or patterns in the events themselves. Routing at the event level requires a heavy forwarder.\nA forwarder can also filter and route events to specific queues, or discard them altogether by routing to the null queue.\nHere, a heavy forwarder routes data to three indexers based on event patterns:\nFor more information on routing and filtering, see Route and filter data in this manual.\nForwarders and indexer clusters\nYou can use forwarders to send data to peer nodes in an indexer cluster. A Splunk best practice is to use load-balanced forwarders for that purpose.\nThis diagram shows two load-balanced forwarders sending data to a cluster:\nTo learn more about forwarders and indexer clusters, see Use forwarders to get your data in Managing Indexers and Clusters of Indexers. To learn more about indexer clusters in general, see About indexer clusters and index replication.\nForward to non-Splunk systems\nWith a heavy forwarder, you can send raw data to a third-party system such as a syslog aggregator. You can combine this with data routing, sending some data to a non-Splunk system and other data to one or more Splunk instances.\nIn this diagram, three forwarders route data to two Splunk instances and a non-Splunk system:\nFor more information on forwarding to non-Splunk systems, see Forward data to third-party systems.\nTo handle some advanced use cases, you might want to insert an intermediate forwarder between a group of forwarders and the indexer. In this type of scenario, the originating forwarders send data to a consolidating forwarder, which then forwards the data on to an indexer. In some cases, the intermediate forwarders also index the data.\nTypical use cases are situations where you need an intermediate index, either for ""store-and-forward"" requirements or to enable localized searching. (In this case, you would need to use a heavy forwarder.) You can also use an intermediate forwarder if you have some need to limit access to the indexer machine; for instance, for security reasons.\nTo enable intermediate forwarding, see Configure an intermediate forwarder.\nTypes of forwarders\nCompatibility between forwarders and indexers\nThis documentation applies to the following versions of Splunk® Enterprise: 6.4.0, 6.4.1, 6.4.2, 6.4.3, 6.4.4, 6.4.5, 6.4.6, 6.4.7, 6.4.8, 6.4.9, 6.4.10, 6.4.11, 6.5.0, 6.5.1, 6.5.1612 (Splunk Cloud only), 6.5.2, 6.5.3, 6.5.4, 6.5.5, 6.5.6, 6.5.7, 6.5.8, 6.5.9, 6.5.10, 6.6.0, 6.6.1, 6.6.2, 6.6.3, 6.6.4, 6.6.5, 6.6.6, 6.6.7, 6.6.8, 6.6.9, 6.6.10, 6.6.11, 6.6.12, 7.0.0, 7.0.1, 7.0.2, 7.0.3, 7.0.4, 7.0.5, 7.0.6, 7.0.7, 7.0.8, 7.1.0, 7.1.1, 7.1.2, 7.1.3, 7.1.4, 7.1.5, 7.1.6, 7.2.0, 7.2.1, 7.2.2, 7.2.3, 7.2.4', 'Splunk Enterprise Security\nOperationalize Security Intelligence\nSplunk Enterprise Security (ES) enables security teams to use all data to gain organization-wide visibility and security intelligence. Regardless of deployment model—on-premises, in a public or private cloud, SaaS, or any combination of these—Splunk ES can be used for continuous monitoring, incident response, running a security operations center or for providing executives a window into business risk.\nSplunk ES provides organizations the ability to:\n- Improve security operations with faster response times\n- Improve security posture by getting end-to-end visibility across all machine data\n- Increase detection and investigation capabilities using advanced analytics\n- Make better informed decisions by leveraging threat intelligence\nImprove Security Posture\nGet a library of security posture widgets to place on any dashboard or easily create your own. See security events by location, host, source type, asset groupings and geography. KPIs provide trending and monitoring of your security posture.\nIncident Review and Classification\nView a single event or get a roll-up of related system events and an incident management workflow for security teams. Easily verify incidents, change their status and criticality, and transfer among team members, all while supplying mandatory comments about status changes. Status changes are audited, monitored and tracked for team metrics. From within the incident review view, analysts can now use risk scores and in-context searches to determine the impact of an incident quickly and to generate actionable alerts to respond on matters that require immediate attention.\nBuilt on a Big Data Platform for Security Intelligence\nSpunk ES leverages Splunk Enterprise capabilities that include:\n- Index Any Data Source. The ability to bring in any data without custom connectors or vendor support enables analysts to quickly access, search and analyze the data they need to complete their investigation.\n- Scalability. The ability to index hundreds of terabytes of data per day. Splunk does not apply a schema at the time data is indexed and searches across terabytes of data can be performed quickly.\n- Flexible Dashboards—Dashboards can be easily created or customized for a quick graphical view of any data or correlation that is important to the organization. Organize multiple dashboards on a single screen for a customized view of the organization’s overall security posture.\n- Ad Hoc Searches. Ad hoc searches enable security teams to quickly understand what attacks are occurring in their environment to determine the best course of action.\nImprove Security Operations\nCreate your own security portal based on your role and the things that matter to your organization. Organize and correlate multiple data sources visually in a single user interface to find relationships and gain context.\nVisually correlate events over time for any IP address. This helps the analyst gain insight into time relationships across events.\nUnified Search Editor\nUse a user-friendly, consistent search creation experience—including guided searches—for key security indicator or key performance indicator correlation searches, and identity and asset investigation visualizations.\nPre-built dashboards will help you identify anomalies in event and protocol data. The dashboards are pre-built using auto-configuring thresholds and baselines.\nIncident Review, Classification and Investigation\nSplunk ES provides comprehensive incident review capabilities that include:\n- Drill down from graphical elements to raw data and wire data captures to gain an understanding of all network communications\n- Unique workflow actions that augment the security investigation process and allow you to pivot on a single piece of common information—or any other data—to rapidly develop the threat context\n- Classification that allows for bulk event reassignment, changes in status and criticality classification, with all analyst activity available for auditing purposes\nIncident Review Audit\nFor governance, auditing and protection against tampering, Splunk ES provides reports on all Splunk user and system activities for a complete audit trail. The Splunk platform uses data signing to maintain chain-of-custody and detect any alterations to the original log and event data.\nDetect Internal and Advanced Threats\nAsset Center/Identity Center\nUnderstanding where assets are, who owns them, their criticality and who should be accessing information on systems helps prioritize security events and investigations. Splunk software has the ability to perform lookups of data stored in an asset database, active directory, spreadsheet or CSV file and use that information as context for security events in reports and dashboards.\nAdvanced Threat Investigation\nUse a variety of advanced detection and investigative controls for investigative purposes or to detect abnormal activity that’s often associated with compromised systems. This includes DNS new domain analysis, HTTP category and user agent analysis, traffic size analysis, URL length analysis, and threat intelligence artifacts.\nVisual Anomaly Detection\nView event data in the form of swim lanes and use heat maps to quickly identify anomalous behaviors and trends related to assets and identities in the environment. Out-of-the-box swim lanes include authentications, endpoint changes, threat list activity, IDS attacks, malware attacks, notable events and risk modifiers related to the user. Swim lanes can be modified to provide user activity profiling across any network, endpoint, access, identity and threat intelligence source.\nGet information from the wire that’s either in lieu of, or complementary to, data from the endpoint or network, or could otherwise not be obtained. Provides protocol information supported by the Splunk Stream including SSL, DNS and email activity.\nIntegration with Splunk UBA\nSplunk ES is integrated with Splunk UBA. Threats detected by Splunk UBA will show up as alerts in Splunk ES Incident Review dashboards to support Security Operations workflows.\nThe UBA detected anomalies are now available as a sourcetype within Splunk ES, which can be used as a starting point of the investigation, do ad hoc searching and pivot for detailed incident review and breach analysis.\nFor incidents that have UBA anomalies associated with them, users can now view specific details on the source of the anomalies within Splunk ES. Splunk UBA anomalies and threats are now available as Asset Investigator correlation searches (swim lanes).\nThe UBA anomalies can be used for multiple SIEM workflows to deter and resolve threats quicker and with greater precision. Hunters and analysts can now use the UBA detected anomalies as a source type within Splunk ES as a starting point of the investigation, do ad hoc searching and pivot for detailed Incident Review and Breach Analysis.\nIdentify, Prioritize and Manage Security Events\nThe Incident Review Framework facilitates incident tracking from the time a correlation rule first triggers an incident, or notable event, all the way through the closure of the investigation. Notable events can be annotated, assigned to an owner for investigation and further examined to gain context around the assets or identities involved, the specific rules that were triggered and the associated raw events.\nThe Risk Scoring Framework enables a risk score to be applied to any event asset, behavior or user based on relative importance or value to the business. This helps security teams to prioritize alerts based on predefined thresholds, while also exposing contributing factors of the risk to all relevant teams. Easily track their security status to understand and actively manage overall business risk.\nOperationalize Threat Intelligence\nThe Threat Intelligence Framework enables organizations to automatically collect, aggregate and de-duplicate threat feeds from a broad set of sources including open sources, subscription based, law enforcement, local and shared from other organizations. This includes integrated support for next-generation security standards such as STIX, TAXII, Department of Homeland Security’s (DHS) Automated Indicator Sharing (AIS), Facebook Threat Exchange and OpenIOC. Risk scores can be assigned to that threat intelligence and used to enhance incident investigation, breach investigation and scoping.\nQuickly Identify Security Events\nThe Notable Event Framework helps identify notable events and track the actions analysts take to resolve the issues that triggered security events. It facilitates the task of triaging notable events, including search filters, tagging and sorting.\nUnderstand Identity and Privilege Levels\nThe Identity and Asset Framework enables the automatic mapping of data stored in an asset database, active directory, spreadsheet or CSV file. This information can then be used as context for security events in reports and dashboards.\nSimplify access control monitoring, exception analysis and audit processes for applications, operating systems and identity management systems across the enterprise. Satisfy compliance and forensics requirements to track highly privileged users and system access attempts on any business-critical application.\nIncrease the effectiveness of endpoint security products such as Symantec™ Endpoint Protection, IBM® Proventia Desktop or McAfee® Endpoint Protection. Prioritize threats and view long term trends. Endpoint Protection includes searches, reports and a library of alerts for malware, rare activities, resource utilization and availability.\nMonitor and detect events from network and security devices across the enterprise. Discover anomalies across firewalls, routers, DHCP, wireless access points, load balancers, intrusion detection sensors and data loss prevention devices. Capabilities include correlations, searches, reports and dashboards for monitoring, alerting and reporting on network-based events. Statistical analysis is employed on proxy data to understand HTTP-based behavioral outliers.\nMake Better Informed Decisions\nEnhance incident response and investigations by leveraging and correlating data from a broad set of sources, including security and non-security data collected from across the organization, and supplemented with internal and external threat intelligence and other contextual information.\nSplunk ES leverages Splunk Enterprise to bring in any data without custom connectors or vendor support, enabling new data sources to be utilized quickly and easily, without expensive and time-consuming professional services engagements. In addition, Splunk ES natively supports emerging threat intelligence sharing standards such as STIX, TAXII, Department of Homeland Security’s (DHS) Automated Indicator Sharing (AIS), and openIOC. Threat Intelligence feeds can be operationalized by aggregating multiple sources, formats and retrieval mechanism, de-duplicating the information and then alerting on them as well as extracting the values for use in investigations as well as for downstream actions.\nOptimize Incident Response\nSimilar to a web browser history, the Investigator Journal logs certain analyst activities taken throughout the investigation without the need for multiple tabs and separate tools. This enables analysts to focus on tracking attack activities while the system tracks the investigation, actions and notes. The Investigator Journal enables analysts to easily:\n- Track searches and activities\n- Review activities at any point\n- Select and place into timeline for temporal analysis\n- Help remember searches, steps taken, provide annotation support\nThe Investigation Timeline enables analysts to investigate the sequence of events using the kill chain methodology to determine the attack lifecycle. At any point in the investigation they can add relevant actions from the Investigator Journal, as well as raw events and even their own notes, to a timeline. This enables them to visualize and more clearly understand the attack details, as well as the sequential relationship between various events – and quickly determine the appropriate next steps.\nThe timeline makes it easy to collaborate with team members and other security personnel throughout the organization. In addition to being able to click through the entire report to get the original analyst’s perspective, any security team member can place events, actions and annotations into a timeline to share their perspective on the scenario.\nThe investigative reporting feature combines the raw event, actions, annotation notes and investigators involved with the incident so that team members can scroll through the details to understand the sequencing and time relationships of multiple events. This also helps executives and new analysts understand how attacks occur in their environment and how to investigate them.']"	['<urn:uuid:7032d272-4869-442b-b7de-5521dab9f038>', '<urn:uuid:704bbf6d-1862-4cd5-bf59-b47288dcdc33>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:58:04.490895	9	104	2527
98	treatment options familial hyperlipidemia poverty mental health impact healthcare access barriers	Treatment approaches differ significantly based on condition and access to care. For familial hyperlipidemia, treatment includes dietary changes (reducing total fat intake to less than 30%), exercise, weight loss, and potentially medication like statins and bile acid-sequestering resins. However, poverty creates barriers to healthcare access and proper treatment - it can impact diet, healthcare access, sleep, and socialization, which in turn affects both mental and physical health outcomes. This highlights how socioeconomic factors can complicate the management of genetic conditions through reduced access to necessary interventions and healthcare resources.	"[""Familial combined hyperlipidemia\n* Alternative names\n* Causes, incidence, and risk factors\n* Signs and tests\n* Expectations (prognosis)\n* Calling your health care provider\nCoronary artery blockage\nCoronary artery blockage\nAlternative names Return to top\nMultiple lipoprotein-type hyperlipidemia\nDefinition Return to top\nFamilial combined hyperlipidemia is an inherited disorder of high serum cholesterol or high blood triglycerides. People with this condition have an increased risk of cardiovascular disease.\nCauses, incidence, and risk factors Return to top\nThis disease is genetic and inherited, although the specific defective genes have not been identified. The person's cholesterol or triglyceride levels become elevated during the teenage years and continue to be high throughout life. The types of elevated lipoproteins may vary between affected family members.\nCholesterol deposits in the skin, called xanthomas, which are seen in other disorders of elevated lipoproteins are rarely seen in this disorder. This disorder predisposes the person to greater risk of early coronary artery disease and therefore, heart attacks. People with the condition have a higher rate of obesity and glucose intolerance.\nThe condition is worsened by diabetes, alcoholism, and hypothyroidism. Risk factors are a family history of high cholesterol and early coronary artery disease. This is the most common disorder of increased blood fats that causes early heart attacks. The rare person who gets 2 defective genes is at much higher risk for early heart attack due to very high blood fat (cholesterol or triglyceride) levels.\nSymptoms Return to top\n* Possible obesity\n* Chest pain (angina)\n* Family history of early heart attack or increased blood fats\nThere may be no symptoms.\nSigns and tests Return to top\n* Elevated serum LDL or VLDL\n* Elevated total cholesterol\n* Decreased or normal serum HDL cholesterol\n* Elevated triglycerides\n* Elevated apolipoprotein B100 test\n* Pedigree analysis may show parent or child with high blood fat\nGenetic testing is available for one type of familial combined hyperlilidemia\nTreatment Return to top\nThe goal of treatment is to reduce the risk of atherosclerotic heart disease.\nThe first step is to change what you eat. This is tried for several months before drug therapy is added. Diet changes include reducing total fat intake to less than 30% of the total calories consumed. Saturated fat intake is reduced by decreasing the amounts of beef, chicken, pork, and lamb; by substituting low-fat dairy products for full-fat ones; and by eliminating coconut and palm oil. Cholesterol intake is reduced by eliminating egg yolks and organ meats.\nFurther reductions in dietary fat may be recommended after the initial trial period. Dietary counseling is often recommended to help people make these adjustments to their eating habits. Weight loss and regular exercise may also aid in lowering cholesterol levels.\nDrug therapy may be initiated if diet, exercise, and weight loss efforts have not reduced the cholesterol levels after an adequate trial period. Various cholesterol reducing agents are available including:\n* Bile acid-sequestering resins (cholestyramine and colestipol)\n* Nicotinic acid\n* Lovastatin and other 'statin' drugs\nExpectations (prognosis) Return to top\nThe probable outcome is related to early diagnosis and treatment and compliance with therapy. Untreated people are at risk for shortened life span due to heart attack and stroke. The person with 2 defective genes has such high lipids that they are unlikely to be able to reduce their risk of coronary artery disease to normal. Therefore, they are at increased risk of heart attack despite medical therapy.\nComplications Return to top\nA complication is early atherosclerotic heart disease or myocardial tissue death due to lack of blood (infarction). The fatty deposits can affect the blood vessels to the brain causing an increase risk for stroke.\nCalling your health care provider Return to top\nCall your health care provider if you experience warning symptoms of heart attack or your screening total cholesterol or triglycerides was found to be high.\nPrevention Return to top\n* A low-cholesterol, low-saturated fat diet in high-risk individuals may help to control LDL levels.\n* Screening of family members of people with familial combined hyperlipidemia should be undertaken to facilitate early treatment. Sometimes younger children may have mild hyperlipidemia.\n* It is important to control other risk factors for early heart attacks, such as smoking if you have this disease.\n* Genetic counseling is recommended for family members.\n· 1 decade ago"", 'Tag: poverty and mental health\nA recently published article illustrates how the concept of neuroplasticity has been used to explain social inequalities, like poverty, by linking them to biomarkers in the brain.\nRecent report underscores troubling trends cutting across poverty, austerity reform, and mental health narratives in health care settings.\nRemediating dilapidated physical environments in urban settings can contribute to better mental health.\nIn this piece for the Yale School of Medicine\'s Department of Psychiatry blog, Lucile Bruce highlights the work of Associate Research Scientist Dr. Annie...\nFrom Metro: Poverty can have a dramatic effect on people\'s mental and physical health, as it can impact diet, healthcare access, sleep, and socialization. ""There are plenty...\nExperiences such as pain, turmoil, trauma and grief aren’t separate from the person—they shape how that person sees the world, how they cope with the world. To separate those experiences from the person, to call them sick, feels barbaric. It feels as if humans are being taught to fear being human.\nResearchers at Duke University who studied 183 adolescents for three years found that increased depression associated with poverty may be mediated by epigenetic changes in DNA. The...\nIn the last few years, Mental Health First Aid has been backed by the President of the United States, the First Lady, the Substance Abuse and Mental Health Services Administration (SAMHSA), and the National Council on Behavioral Health (among others). In fiscal year 2015 alone, the federal budget allotted 15 million toward the Council’s MHFA mission of ‘one million trained.’ Yet, this course – promoted with unprecedented fervor and designed to support the average citizen to identify a mental health ‘problem’ in their fellow persons and (strongly) encourage them to get ‘help’ – has little to say about the importance and emotional impact of meeting basic human needs.\nThe March 3rd, 2016 edition of the Wall Street Journal featured an article by past President of the American Psychiatric Association (APA) Jeffrey Lieberman and his colleague, computational neuroscientist Ogi Ogas. The article was entitled “Genetics and Mental Illness—Let’s Not Get Carried Away.” In their piece, the authors started by expressing the belief that a recent study identified a gene that causes schizophrenia, and then discussed whether it is desirable or possible to remove allegedly pathological genes in the interest of creating a future “mentally perfect society.” The authors of the article, like many previous textbook authors, seem unfamiliar with the questionable “evidence” put forward by psychiatry as proof that its disorders are “highly heritable” In fact, DSM-5 Task Force Chair David Kupfer admitted that “we’re still waiting” for the discovery of “biological and genetic markers” for psychiatric disorders.\nStephen Fry’s exploration of manic depression (in the current BBC series on mental health, ‘In the Mind‘) has drawn both praise (because of his attempts to destigmatize mental illness) and criticism (because he appears to have a very narrow biomedical understanding of mental illness). I have sent an open letter to the actor which challenges some of his assumptions about mental illness, and offers a very different understanding to that promoted in his recent television programme.\n“Children from poorer families are more likely to experience changes in brain connectivity that put them at higher risk of depression, compared with children from more affluent families,” according to new research covered by Medical News Today. ""Poverty doesn\'t put a child on a predetermined trajectory, but it behooves us to remember that adverse experiences early in life are influencing the development and function of the brain. And if we hope to intervene, we need to do it early so that we can help shift children onto the best possible developmental trajectories.""\nA new NIH-funded study suggests that children from low-income environments are more likely to have neurological impairments. The researchers claim that these neurodevelopmental issues are “distinct from the risk of cognitive and emotional delays known to accompany early-life poverty.”\nThe latest economic recession led to a spike in diagnoses for mental illnesses, suicide attempts, and suicide, according to report out of the University...\nThe American Academy of Pediatrics came out with new recommendations that suggest doctors screen all of their child patients for hunger. About 16 million children in the US live in food scarcity and poverty that can lead to physical health issues as well as behavioral issues, which can then be misdiagnosed.']"	['<urn:uuid:c7b73a9e-8e04-4e05-8c4b-2f939a77d3a2>', '<urn:uuid:51ef681c-3b33-4956-b24d-b04186076184>']	open-ended	with-premise	long-search-query	similar-to-document	multi-aspect	expert	2025-05-12T20:58:04.490895	11	89	1442
99	connection between genetic polymorphism cholesterol regulation tumor progression	The ADRB2 gene polymorphism rs34623097-A leads to reduced ADRB2 expression and impaired triglyceride breakdown in adipose tissue. This connects to cancer progression through 27-hydroxycholesterol, which functions as both an estrogen receptor activator and Liver X Receptor modulator, influencing breast cancer onset, growth rate, and metastasis. These effects can be reduced by inhibiting cholesterol synthesis with statins or novel inhibitors of 27-hydroxycholesterol synthesis.	['Erik Russell Nelson\nAssistant Professor of Molecular and Integrative Physiology\nCell-Cell Interactions, Drug Discovery, Endocrinology, Metabolic Regulation, Regulation of Gene Expression, Signal Transduction\n2002 B.Sc. in Zoology, University of Calgary, Canada\n2008 Ph.D. in Comparative Endocrinology, University of Calgary, Canada\n2008-2014 Postdoctoral Associate, Duke University School of Medicine, Durham, NC.\nEndocrine and Metabolic Control of Breast and Ovarian Cancer Pathophysiology\nBreast cancer remains the most commonly diagnosed cancer among women, while ovarian cancer continues to have a very poor 5 year survival rate. The magnitude of this problem provides a strong impetus for studies that may lead to new chemopreventative strategies and/or lifestyle changes that reduce morbidity from these cancers. Therefore, the goal of our research is to elucidate the effects of the endocrine system and metabolism on breast and ovarian cancer initiation and progression. We integrate our expertise in physiology, endocrinology, immunology and in vivo models to pursue translational breast and ovarian cancer research.\nSeveral studies have demonstrated correlations between obesity and increased breast cancer risk, and decreased progression free survival amongst ovarian cancer patients. Interestingly, it has also been observed that elevated cholesterol itself is a poor prognostic for these cancers, independent of body mass index. This puts into perspective the recent finding that a primary metabolite of cholesterol, 27-hydroxycholesterol, has the ability to activate the estrogen receptor. Estrogen and its receptor (ER) are well known to influence certain types of breast cancer. We have also shown that 27-hydroxychoelsterol can activate the Liver X Receptors (LXRs), adding another layer of complexity to its biological actions. Using several cellular and animal models, we have now shown that in animal models 27-hydroxycholesterol decreases the time to breast cancer onset, increases tumor growth rate, and increases metastasis. Importantly, tumor growth rate can be reduced by inhibiting cholesterol synthesis with drugs like statins, or by novel inhibitors of the enzyme responsible for the synthesis of 27-hydroxycholesterol, potentially providing novel therapeutic options for breast cancer patients.\nThe major focuses of the lab are:\nEstablishing 27-hydroxycholesterol as a causal link between obesity, hypercholesterolemia and breast cancer metastasis.\nDefining the mechanisms by which 27-hydroxychoelsterol increases metastasis.\nDetermining the impact of cholesterol and its metabolites on ovarian cancer progression.\nDelineating the role of nuclear receptor signaling within the tumor microenvironment and its impact on tumor progression.\n2017 List of Teachers Ranked Excellent By Their Students\n2016 List of Teachers Ranked Excellent By Their Students\n2013 National Cancer Institute of the National Institutes of Health K99/R00 Pathway to Independence Award.\n2013 Robert J. Fitzgerald Academic Achievement Award. Department of Pharmacology and Cancer Biology, Duke University School of Medicine.\n2012 The Endocrine Society Award for Outstanding Paper in Endocrinology for 2011.\n2011 Robert J. Fitzgerald Scholar Award: Outstanding publication in the Department of Pharmacology and Cancer Biology, Duke University Medical Center.\n2011 The Endocrine Society Outstanding Abstract Award. ENDO2011, the annual Endocrine Society Meeting.\n2010 Keystone Symposia Scholarship (Outstanding Abstract), Nuclear Receptors: Signalling, Gene Regulation and Cancer.\n2009 Department of Defense Breast Cancer Research Program Postdoctoral Fellowship Award.\n2008 Government of Alberta Queen Elizabeth II Graduate Scholarship.\nBaek A.E., Yu Y.R., He S., Wardell S.E., Chang C.Y., Kwon S., Pillai R.V., Thompson W., Dubois L.G., Sullivan P.M., Kemper J.K., Gunn M.D., McDonnell D.P., and Nelson E.R. (2017). The cholesterol metabolite 27-hydroxycholesterol facilitates breast cancer metastasis through its actions on immune cells. Nature Communications, 8(1):864. PMCID: PMC5636879 Link to Paper\nBaek A.E. and Nelson E.R. (2016). The Contribution of Cholesterol and Its Metabolites to the Pathophysiology of Breast Cancer. Invited Review. Hormones and Cancer. 7(4):219-28. PMID: 27020054.\nNelson E.R., Wardell S.E., Jasper J.S., Park S., Suchindran S., Howe M.K.,Carver N.J., Pillai R.V., Sullivan P.M., Sondhi V., Umetani M., Geradts J., and McDonnell D.P. (2013). 27-Hydroxycholesterol Links Hypercholesterolemia and Breast Cancer Pathophysiology. Science. 342(6162):1094-8. PMID: 24288332.\nMcDonnell D.P., Park S., Goulet M.T., Jasper J.S., Wardell S.E., Chang CY, Norris J.D., Guyton J.R. and Nelson E.R. (2014). Obesity, Cholesterol Metabolism and Breast Cancer Pathogenesis. Cancer Research. 74(18): 4976-82. PMID: 25060521.\nWardell S.E., Nelson E.R., and McDonnell D.P. (2014). From empirical to mechanism based discovery of clinically useful ER ligands. Steroids. 90:30-8. PMID: 25084324.\nWardell S.E., Nelson E.R., Chao C.A., and McDonnell D.P. (2013). Bazedoxifene exhibits antiestrogenic activity in animal models of tamoxifen resistant breast cancer; implications for treatment of advanced disease. Clinical Cancer Research. 1;19(9):2420-31. PMCID: PMC3643989.\nNelson E.R., Wardell S.E., and McDonnell D.P. (2013). The Molecular Mechanisms Underlying the Pharmacological Actions of Estrogens, SERMs and Oxysterols: Implications for the Treatment and Prevention of Osteoporosis. Bone. 53:42-52. PMCID: PMC3552054.\nJones L.W., Antonelli J., Masko E.M., Lascola C.D., Fels D., Dewhirst M.W., Dyck J.R.B., Nagendran J., Flores C.T., Betof A.S., Young M.E., Nelson E.R., Pollak M., Broadwater G., and Freedland S.J. (2012) Exercise Modulation of the Host - Tumor Interaction in an Orthotopic Model of Murine Prostate Cancer. Journal of Applied Physiology. 113(2):263-72. PMCID: PMC3404704\nNelson E.R., DuSell C.D., Wang X., Howe M.K., Evans G., Michalek R.D., Rathmell J.C., Khosla S., Gesty-Palmer D., and McDonnell D.P. (2011). The oxysterol, 27-hydroxycholesterol, links cholesterol metabolism to bone homeostasis through its actions on the estrogen and liver X receptors. Endocrinology. 152(12): 4691-705. PMCID: PMC3230052.\nMichalek R.D., Gerriets V.A., Nichols A.G., Inoue M., Kazmin D., Chang C-Y, Dwyer M., Nelson E.R., Pollizzi K.N., Ilkayeva O., Giguere V., Zuercher W.J., Powell J.D., Shinohara M.L., McDonnell D.P., Rathmell J.C. (2011). Estrogen Related Receptor-α is a Metabolic Regulator of Effector T cell Activation and Differentiation. Proceedings of the National Academy of Sciences of the United States of America. 108(45): 18348-53. PMCID: PMC3215012\nDuSell C.D., Nelson E.R., Wang X., Abdo J., Mödder U.L., Umetani M., Gesty-Palmer D., Javitt N.B., Khosla S., McDonnell D.P. (2010). The Endogenous Selective Estrogen Receptor Modulator 27-hydroxycholesterol is a Negative Regulator of Bone Homeostasis. Endocrinology. 151(8): 3675-85. PMCID: PMC2940523\nDuSell C.D., Nelson E.R., Wittmann B.M., Fretz J.A., Kazmin D., Thomas R.S., J. Pike W., and McDonnell D.P. (2010). Regulation of aryl hydrocarbon receptor function by Selective Estrogen Receptor Modulators. Molecular Endocrinology. 24(1): 33-46. PMCID: PMC2802893', 'Association of ADRB2 polymorphism with triglyceride levels in Tongans\n© Naka et al.; licensee BioMed Central Ltd. 2013\nReceived: 28 March 2013\nAccepted: 19 July 2013\nPublished: 23 July 2013\nOur previous study demonstrated that the A-allele of the single nucleotide polymorphism (SNP) rs34623097 located in the upstream region of the β2 adrenergic receptor gene (ADRB2) is significantly associated with risk for obesity in Oceanic populations.\nTo investigate whether the ADRB2 polymorphisms explain part of the individual differences in lipid mobilization, energy expenditure and glycogen breakdown, the associations of 10 ADRB2 SNPs with total cholesterol, high-density lipoprotein cholesterol, low-density lipoprotein cholesterol and triglyceride levels were examined in 128 adults in Tonga.\nA multiple linear regression analysis adjusted for age, sex, and body mass index revealed that rs34623097 was significantly associated with triglyceride levels (P-value = 0.037). A copy of the rs34623097-A allele increased serum triglyceride levels by 70.1 mg/dL (0.791 mmol/L). None of the ADRB2 SNPs showed a significant association with total-cholesterol, high-density lipoprotein cholesterol, or low-density lipoprotein cholesterol.\nIn a Tongan population, a SNP located in the upstream region of ADRB2 is associated with triglyceride levels independent of body mass index.\nThe β2 adrenergic receptor (ADRB2), a class of G protein-coupled receptor for catecholamines, plays an important role in regulating energy expenditure and lipolysis in adipose tissue. Therefore, polymorphisms in the ADRB2 gene (OMIM*109690) may explain part of the individual differences in lipid profiles. The 27Glu allele at the Glu27Gln polymorphism of ADRB2 has been significantly associated with increased triglyceride levels [1–6], although there is a conflicting report that demonstrated a significant increase in triglyceride levels in individuals with the Gln/Glngenotype as compared to those with the Glu/Glugenotype .\nMost studies have examined the association of three non synonymous ADRB2 SNPs, rs1042711 (5’LC-Arg19Cys in the 5’ upstream region), rs1042713 (Gly16Arg) and rs1042714 (Glu27Gln), with lipid profiles (i.e., total cholesterol, high-density lipoprotein [HDL] cholesterol, low-density lipoprotein [LDL] cholesterol and triglyceride levels). However, ourrecent study showed that rs34623097 located in the upstream region of ADRB2 is more strongly associated with obesity than non synonymous SNPs in Oceanic populations. A functional analysis suggested that rs34623097-A, a risk allele for obesity, reduces the transcriptional activity of ADRB2 as compared with rs34623097-G . The aim of the present study is to further explore the association of ADRB2 SNPs including rs34623097 with lipid profiles independent of body mass index (BMI) in adult Tongan subjects.\nClinical characteristics of subjects\n(n = 128)\n(n = 40)\n(n = 88)\nTotal cholesterol (mg/dL)\nHDL cholesterol (mg/dL)\nLDL cholesterol (mg/dL)\nAssociation of age, sex and BMI with lipid traits\nTotal cholesterol (mg/dL)\nHDL cholesterol (mg/dL)\nLDL cholesterol (mg/dL)\n1.0 x 10 -3\n8.6 x 10 -3\nSex (Male = 0, Female = 1)\n8.9 x 10 -4\n6.6 x 1 -6\n3.0 x 10 -4\nAssociation of ADRB2 polymorphisms with lipid profiles\nAssociation of each ADRB2 polymorphism with lipid traits\nTotal cholesterol (mg/dL)\nHDL cholesterol (mg/dL)\nLDL cholesterol (mg/dL)\nIn this study, besides nine tag SNPs (rs17778257, rs34623097, rs2895795, rs2053044, rs11959427, rs1042711, rs1042713, rs1042714, and rs1042720) of ADRB2, the rs1042719 SNP was further evaluated becausers 1042720, being in LD with rs1042719 in the Oceanic populations , showed a small P-value for triglyceride (P-value = 0.059; Table 3); however, the rs1042719 SNP was also not significantly associated with the level of triglycerides (P-value = 0.057).\nThe present results indicate that rs34623097-A is associated with increased serum triglyceride levels. Our previous luciferase reporter assay demonstrated that rs34623097-A reduces the transcriptional activity of ADRB2 as compared with rs34623097-G, and an electrophoretic mobility shift assay suggested that rs34623097 modulates the binding affinity with nuclear factors . Taken together, these results indicate that the reduced expression of ADRB2 caused by rs34623097-A on adipocytes might contribute, in part, to increased serum triglyceride levels. One possible mechanism is that the activation of β adrenergic receptors such as ADRB2 expressed in adipocytes leads to the breakdown of triglycerides stored in adipocytes and the release of free fatty acids and glycerol [10–12]. Thus, the decreased lipolytic function in adipocytes is due to the lower expression of ADRB2 that would result in the accumulation of triglycerides within adipocytes. If triglycerides accumulate in adipocytes, the cellular uptake of the major component of triglycerides (i.e., free fatty acids) by adipocytes would be reduced. This mechanism would reduce the lipolysis of circulating lipoprotein-triglycerides. Accordingly, the level of serum triglycerides would be increased in individuals with rs34623097-A.\nTo the best of our knowledge, this study is the first to report that rs34623097 is a major ADRB2 polymorphism that influences the serum triglyceride level, although the possibility that the lack of association of the other ADRB2 polymorphisms with lipid profiles comes from the small sample size (n = 128) should not be excluded. Previous studies have revealed that the 27Glu allele (rs1042714-G) of ADRB2 significantly increases the serum triglyceride levels [1–6]. In the present study, 27Glu also showed the same tendency (i.e., the β coefficient of 27Gln, an alternative allele at Glu27Gln, was a negative value in Table 3). This tendency is because rs34623097-A, which is significantly associated with increased triglycerides, is in positive LD with 27Glu in the Tongan subjects (D’ = 1 and r2 = 0.62). The rs34623097-A allele is observed mainly in Asians and Pacific Islanders and is in LD with 27Glu . However, rs34623097-A had never previously been examined in the association studies on lipid traits including triglycerides. The significant association of 27Glu found in previous studies may merely reflect the LD with rs34623097-A. Further studies are required to clarify this issue in various ethnic groups.\nSubjects and methods\nA total of 128 healthy adult subjects (18 years old or older) were recruited from Nuku’alofa, Tonga. Patients with diabetes and subjects who had any treatment known to interfere with metabolic syndrome-related parameters were excluded. A blood sample was collected from each subject after obtaining a written consent to participate in the study. This study was approved by the National Health Ethics & Research Committee of Tonga and the Research Ethics Committee of the Faculty of Medicine, University of Tsukuba.\nAnthropomorphic phenotypes were directly measured in field settings. Measurements were taken of subjects dressed in light clothing. Body height was measured to the nearest 1 mm by using a field anthropometer (GPM, Zurich, Switzerland), and weight was recorded to the nearest 0.1 kg by using a portable digital scale (Tanita model BC-518, Tokyo, Japan). BMI was calculated by dividing the weight in kg by the height in meters squared.\nBlood samples were obtained on the morning following a 12-hour fast. Serum lipids including total cholesterol, HDL cholesterol, LDL cholesterol, and triglycerides were measured at SRL Inc. (Tokyo, Japan) by using standard laboratory protocols.\nGenomic DNA was extracted from peripheral blood by using a QIAamp Blood Kit (Qiagen, Hilden, Germany). Nine tag SNPs (rs17778257, rs34623097, rs2895795, rs2053044, rs11959427, rs1042711, rs1042713, rs1042714, and rs1042720) of ADRB2, which were in LD (r2 > 0.8) with the other ADRB2 SNPs in the Oceanic populations, were genotyped in our previous study . The rs1042719 SNP was genotyped by using a TaqMan SNP genotyping assay in the present study because rs1042720, being in LD with rs1042719, showed not a significant but a small P-value in the association analyses of triglycerides in a Tongan population.\nAssociations of age, sex, and BMI with total-cholesterol, HDL-cholesterol, LDL-cholesterol or triglyceride levels were assessed by a multiple regression analysis. The genotypes of rs11959427, rs1042713, and rs1042714 were not determined by a molecular biology-based technique for some subjects; instead, their genotypes were imputed by the MACH software . Deviation of genotype frequencies from Hardy-Weinberg equilibrium was examined by chi-square test. Pairwise linkage disequilibrium (LD) parameters, D’ and r2, were estimated by using Haploview software . The association of each polymorphism with total cholesterol, HDL cholesterol, LDL cholesterol or triglyceride levels was assessed by a multiple regression analysis adjusted for age, sex, and BMI. In the regression analysis, the number of copies of a derived allele at each SNP was used as an independent variable (i.e., homozygotes of an ancestral allele, homozygotes of a derived allele, and heterozygotes were coded as 0, 2, and 1, respectively). The genotypes of SNPs with minor allele frequency (MAF) of ≥ 0.05 spanning a 200 kb genomic region containing the entire ADRB2 gene were retrieved from the HapMap JPT (Japanese in Tokyo, Japan) and CHB (Han Chinese in Beijing, China) populations [14, 15]. Furthermore, genomic DNA samples from 43 JPT and 45 CHB subjects were obtained from the Coriell Cell Repository and subjected to rs34623097 genotyping . By using the genotype data of JPT and CHB subjects as a reference, the genotypes of Tongan subjects were imputed by the MACH software . The imputed genotypes of SNPs showing an Rsq value (a measure which estimates the squared correlation between imputed and true genotypes) of more than 0.5 were used for the association test. Accordingly, seven imputed SNPs were further subjected to single-point association analysis. P-values less than 0.05 were regarded as statistically significant.\nIn a Tongan population, the rs34623097-A allele at a SNP located in the upstream region of ADRB2 is significantly associated with increased serum triglyceride levels independent of BMI.\nWe sincerely thank the people of Tonga for their kind approval and support of our research. We are grateful to Dr. Taniela Palu (Diabetes Clinic, Kingdom of Tonga) and Dr. Viliami Tangi (Minister of Health, Kingdom of Tonga). This work was partly supported by a KAKENHI (22370084 and 25291103) Grant-in-Aid for Scientific Research (B) and a Grant-in-Aid for JSPS Fellows (24.5) from the Ministry of Education, Culture, Sports, Science, and Technology of Japan.\n- Ehrenborg E, Skogsberg J, Ruotolo G, Large V, Eriksson P, Arner P, Hamsten A: The Q/E27 polymorphism in the beta(2)-adrenoceptor gene is associated with increased body weight and dyslipoproteinaemia involving triglyceride-rich lipoproteins. J Intern Med. 2000, 247 (6): 651-656. 10.1046/j.1365-2796.2000.00669.xView ArticlePubMedGoogle Scholar\n- Iwamoto N, Ogawa Y, Kajihara S, Hisatomi A, Yasutake T, Yoshimura T, Mizuta T, Hara T, Ozaki I, Yamamoto K: Gln27Glu beta2-adrenergic receptor variant is associated with hypertriglyceridemia and the development of fatty liver. Clinica chimica acta; international journal of clinical chemistry. 2001, 314 (1–2): 85-91.View ArticlePubMedGoogle Scholar\n- Ishiyama-Shigemoto S, Yamada K, Yuan X, Ichikawa F, Nonaka K: Association of polymorphisms in the beta2-adrenergic receptor gene with obesity, hypertriglyceridaemia, and diabetes mellitus. Diabetologia. 1999, 42 (1): 98-101. 10.1007/s001250051120View ArticlePubMedGoogle Scholar\n- Macho-Azcarate T, Marti A, Gonzalez A, Martinez JA, Ibanez J: Gln27Glu polymorphism in the beta2 adrenergic receptor gene and lipid metabolism during exercise in obese women. International journal of obesity and related metabolic disorders: journal of the International Association for the Study of Obesity. 2002, 26 (11): 1434-1441. 10.1038/sj.ijo.0802129.View ArticleGoogle Scholar\n- Isaza C, Henao J, Ramirez E, Cuesta F, Cacabelos R: Polymorphic variants of the beta2-adrenergic receptor (ADRB2) gene and ADRB2-related propanolol-induced dyslipidemia in the Colombian population. Methods Find Exp Clin Pharmacol. 2005, 27 (4): 237-244. 10.1358/mf.2005.27.4.893582View ArticlePubMedGoogle Scholar\n- Yoshida T, Kato K, Yokoi K, Oguri M, Watanabe S, Metoki N, Yoshida H, Satoh K, Aoyagi Y, Nishigaki Y: Association of genetic variants with chronic kidney disease in individuals with different lipid profiles. Int J Mol Med. 2009, 24 (2): 233-246.PubMedGoogle Scholar\n- Petrone A, Zavarella S, Iacobellis G, Zampetti S, Vania A, Di Pietro S, Galgani A, Leonetti F, Di Mario U, Buzzetti R: Association of beta2 adrenergic receptor polymorphisms and related haplotypes with triglyceride and LDL-cholesterol levels. European journal of human genetics: EJHG. 2006, 14 (1): 94-100.PubMedGoogle Scholar\n- Naka I, Hikami K, Nakayama K, Koga M, Nishida N, Kimura R, Furusawa T, Natsuhara K, Yamauchi T, Nakazawa M: A functional SNP upstream of the beta-2 adrenergic receptor gene (ADRB2) is associated with obesity in Oceanic populations. Int J Obes (Lond). in press.Google Scholar\n- Li Y, Willer CJ, Ding J, Scheet P, Abecasis GR: MaCH: using sequence and genotype data to estimate haplotypes and unobserved genotypes. Genet Epidemiol. 2010, 34 (8): 816-834. 10.1002/gepi.20533PubMed CentralView ArticlePubMedGoogle Scholar\n- Haffner CA, Kendall MJ, Maxwell S, Hughes B: The lipolytic effect of beta 1- and beta 2-adrenoceptor activation in healthy human volunteers. Br J Clin Pharmacol. 1993, 35 (1): 35-39. 10.1111/j.1365-2125.1993.tb05667.xPubMed CentralView ArticlePubMedGoogle Scholar\n- Large V, Arner P: Regulation of lipolysis in humans. Pathophysiological modulation in obesity, diabetes, and hyperlipidaemia. Diabetes Metab. 1998, 24 (5): 409-418.PubMedGoogle Scholar\n- Barbe P, Millet L, Galitzky J, Lafontan M, Berlan M: In situ assessment of the role of the beta 1-, beta 2- and beta 3-adrenoceptors in the control of lipolysis and nutritive blood flow in human subcutaneous adipose tissue. Br J Pharmacol. 1996, 117 (5): 907-913. 10.1111/j.1476-5381.1996.tb15279.xPubMed CentralView ArticlePubMedGoogle Scholar\n- Barrett JC, Fry B, Maller J, Daly MJ: Haploview: analysis and visualization of LD and haplotype maps. Bioinformatics. 2005, 21 (2): 263-265. 10.1093/bioinformatics/bth457View ArticlePubMedGoogle Scholar\n- Frazer KA, Ballinger DG, Cox DR, Hinds DA, Stuve LL, Gibbs RA, Belmont JW, Boudreau A, Hardenbol P, Leal SM: A second generation human haplotype map of over 3.1 million SNPs. Nature. 2007, 449 (7164): 851-861. 10.1038/nature06258View ArticlePubMedGoogle Scholar\n- The International HapMap Consortium: A haplotype map of the human genome. Nature. 2005, 437 (7063): 1299-1320. 10.1038/nature04226PubMed CentralView ArticleGoogle Scholar\nThis article is published under license to BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.']	['<urn:uuid:1d9c4355-781e-4d85-809c-93029eaf3765>', '<urn:uuid:3b268702-8cae-44ec-b61b-83a1b705fb92>']	factoid	direct	long-search-query	distant-from-document	three-doc	expert	2025-05-12T20:58:04.490895	8	62	3213
100	As an architect, what's the best gutter material for sustainable buildings?	Copper gutters are the most environmentally friendly and sustainable choice. Copper requires no finishing, coating, or paint, needs no maintenance, and has natural corrosion protection. It is the most durable architectural metal, lasting nearly a century, and is 100% recyclable. Additionally, copper has natural antimicrobial and fungicidal properties, and the small traces of copper salts in runoff water are beneficial for plants, vegetation, humans, and animals.	['Rain water collection or rain water harvesting is becoming more and more popular these days as people become more aware of the many benefits and possible uses of diverted rainwater. Instead of allowing rain water falling on a roof top to drain in limited spots around the perimeter of a home’s foundation or else be channeled into the sewer system, rain water can be collected in rain barrels, conserved and put to good use. Assuming your home already has a gutter system and downspouts, rain water collection systems can be fairly simple and relatively low cost or they can be very elaborate systems which run into thousands of dollars. Collected or harvested rain water can be used to water plants, gardens, lawns, house plants, for cleaning or washing and to distribute rain water to sheltered or protected areas which don’t normally get much water. Rain water is naturally soft and free of fluoride, chlorine and other additives found in tap water and is healthier for plants and the lack of hardness improves cleaning and washing. Rain water is perfect for washing your car leaving no hardness spots. There can be savings too on the purchase of city water if significant rain water is collected and used instead.\nHow much rain water can be collected? Even very small roofs, say for example a 1000 square feet roof is capable of collecting up to 600 gallons of rain water run off with 1 inch of rain accumulation. Quick rule of thumb to estimate how much water falls off your roof with a 1″ rainfall, is to multiple the square footage of your roof by 3/5th. Most rain water collection involves some variation of a rain barrel or storage tank and those average about 55 gallons in size. They fill up surprisingly quickly with a decent rainfall.\nWhat do you need to get started? To collect rain from most roofs, you usually need to have a gutter system with downspouts. Flat roofs with low retaining walls require roof scuppers or rain spouts feeding scupper boxes and downspouts. If you don’t yet have a gutter system then get one not only to harvest rain water but to protect the walls and foundation of your building from water damage. Now assuming you have a gutter or roof drainage system with downspouts, you need to splice into, attach or insert a downspout diverter or rain water diverter into one or all of your downspouts. A downspout diverter simply put has a two-way valve which can either divert the roof rain water flowing through the downspout into a rain barrel, storage container or tank or otherwise allow the rain water to exit the downspout normally.\nDownspout Rain Diverters Copper & Aluminum Shown With Reliable 2-Way Valve\nVirtually any style. type and size of gutters and downspouts will work. Downspout rain diverters are available for rectangular corrugated downspouts (shown above), smooth square or rectangular downspouts and round downspouts in virtually any durable gutter downspout material such as aluminum, copper, zinc, galvalume or stainless steel. Rusty steel gutters and downspouts should be refinished or replaced though to prevent staining. There is no reason to use plastic downspout diverters or rain collection system parts. Plastic gutters and downspouts may detract from your home’s appearance and are not as strong or as durable with temperature extremes, UV radiation and such as metal gutters and downspouts. Among metal gutters and downspouts, copper gutters are the most attractive pick, for copper requires no finishing, coating or paint, copper requires no maintenance, copper weathers beautifully with it’s own built-in natural corrosion protection, copper is the most durable architectural metal lasting nearly a century and copper is 100% recyclable when it finally is replaced. This also makes copper the greenest, most environmentally friendly choice. Copper is also unique in that it possesses remarkable natural antimicrobial and fungicidal properties which are a great attribute for gutter and rain water collection systems. The small traces of copper salts present in the runoff water are also needed and healthy for plants and vegetation as well as for humans and animals.\nDownspout Strainer – – – – – – – Gutter Screens Pictured\nTo keep large debris from clogging up your gutters, downspouts and entering your rain collection it is advisable to at minimally use a downspout strainer at the top of each downspout and better yet gutter screens or gutter guards to catch even smaller debris from entering. An inline downspout cleanout or cleanout box may be also be used to filter larger debris and are designed to be serviceable closer to the ground. Some sort of basic filtration or screen may also be used at the input to your rain barrel, container or storage tank. Screening or filtering placed inside or over all exposed openings and sealing or caulking any gaps also helps prevent mosquito breeding. The rain downspout diverters pictured above are designed to present no restriction and allow full flow downs throughout the downspout system. Some types of diverters and also use of rain barrel screening with very tight mesh or foam filtering can restrict your water flow and cause excessive back pressure or backup where eventually your gutters might overflow. Using larger diameter downspouts and downspout conductor head reservoirs inline high up on the downspout can help alleviate this and maintain flow.\nDownspout Cleanout Box – – Inline Cleanout – – Downspout Elbows\nStorage tanks, containers and rain barrels come in hundreds of styles, shapes, sizes and materials. They can range from costing virtually nothing such as old discarded metal or food grade plastic barrels or large containers to ornamental urns with decorative planters on top, architecturally attractive lined wooden containers, faux granite or terra cotta and even hidden tanks underground or low profile tanks under a deck. Use your own personal preference as to the look, shape, size and materials for your rain water collection container as long as you ensure the top is enclosed and other exposed openings such as the overflow outlet are screened to keep out mosquitos, bugs, wildlife and domestic small animals. Unless they are already present, the rain barrel or tank container should allow you to cut openings to attach or insert your downspout pipe and plumbing fittings such as a faucet to which you can also attach a water hose.\nRutland Gutter Supply is one of the largest downspout gutter suppliers in the world and can equip you with virtually everything you need to set up your rain collection system such as downspout diverters, downspout extensions, downspout brackets, downspout elbows, downspout offsets, downspout goosenecks, downspout adapters, downspout strainers, downspout clean-out box, downspout conductor leader heads and gutter screens. Visit Rutland’s downspout diverter – rain diverter web page to gather more ideas, explore the links or just get started collecting rain water. Rain barrels or storage containers can be such stylized and very individual choice, you should decide what kind of look you want or prefer and easily obtain your choice from many local sources. Plumbing fittings for hose attachment, screening and caulking for your rain water container are all available from any hardware or home improvement store. This has been a general overview of rain water collection. In future blogs we will go into more detail on rain water collection system installation, sophisticated underground water tanks with pumps and water lines, filtering and disinfecting the rain water for potable use, rain water conservation benefits and useful ideas on constructing your own custom rain barrels or decorative water tanks.']	['<urn:uuid:51c2f9cd-6aed-40e1-8a1c-cf76c21b8530>']	open-ended	with-premise	concise-and-natural	distant-from-document	single-doc	expert	2025-05-12T20:58:04.490895	11	66	1251
