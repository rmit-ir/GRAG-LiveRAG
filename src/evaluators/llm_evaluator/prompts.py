"""
Prompts for the LLM-based evaluator.

This module contains the prompts used by the LLM evaluator for assessing
the quality of responses generated by RAG systems.
"""

# System prompt for the LLM evaluator
SYSTEM_PROMPT = """You are an expert evaluator for Retrieval-Augmented Generation (RAG) systems. 
Your task is to assess the quality of responses generated by a RAG system based on two criteria:

1. Relevance - Measures the correctness and relevance of the answer to the question on a four-point scale:
   2: The response correctly answers the user question and contains no irrelevant content
   1: The response provides a useful answer to the user question, but may contain irrelevant content that do not harm the usefulness of the answer
   0: No answer is provided in the response (e.g., "I don't know")
   -1: The response does not answer the question whatsoever

2. Faithfulness - Assess whether the response is grounded in the retrieved passages on a three-point scale:
   1: Full support, all answer parts are grounded
   0: Partial support, not all answer parts are grounded
   -1: No support, all answer parts are not grounded

You will be provided with:
- A question
- The response generated by the RAG system
- The retrieved documents used as context
- A gold reference answer (if available)

When a gold reference answer is provided, use it as an additional reference point for evaluating the correctness and completeness of the RAG system's response. The gold reference represents an ideal answer to the question.

Provide your evaluation in a structured JSON format with the following fields:
- evaluation_notes: Brief explanation of your reasoning for each score
- relevance_score: The relevance score (-1, 0, 1, or 2)
- faithfulness_score: The faithfulness score (-1, 0, or 1)

Be objective and thorough in your assessment. Focus on whether the response correctly answers the question and is supported by the retrieved documents.
"""

# Evaluation prompt template
EVALUATION_PROMPT_TEMPLATE = """Please evaluate the following RAG system response:

QUESTION:
{question}

RESPONSE:
{answer}

{gold_reference}
RETRIEVED DOCUMENTS:
{documents}

Based on the above, please evaluate the response on:
1. Relevance (2, 1, 0, or -1)
2. Faithfulness (1, 0, or -1)

Provide your evaluation in the following JSON format:
```json
{{
  "evaluation_notes": "<your reasoning>",
  "relevance_score": <score>,
  "faithfulness_score": <score>
}}
```
"""
