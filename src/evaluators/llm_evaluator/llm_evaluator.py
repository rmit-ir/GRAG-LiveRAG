"""
LLM-based evaluator for RAG system assessment.

This module implements an evaluator that uses Claude Sonnet 3.5 through the Bedrock client
to assess the quality of responses generated by a RAG system based on relevance and faithfulness.

uv run scripts/evaluate.py --evaluator evaluators.llm_evaluator.llm_evaluator.LLMEvaluator --help
"""
from typing import List, Dict, Any, Optional
import json
import re
import time
from statistics import mean
import concurrent.futures
from threading import Lock

from evaluators.evaluator_interface import EvaluatorInterface
from evaluators.evaluation_result import EvaluationResult, EvaluationResultRow, SystemAnalysis
from systems.rag_result import RAGResult
from services.ds_data_morgana import QAPair
from services.llms.bedrock_client import BedrockClient
from utils.logging_utils import get_logger
from evaluators.llm_evaluator.prompts import (
    SUPPORT_PROMPT_TEMPLATE,
    SYSTEM_PROMPT_CORRECTNESS,
    CORRECTNESS_PROMPT_TEMPLATE,
    SUMMARY_SYSTEM_PROMPT,
    SUMMARY_PROMPT_TEMPLATE,
    SYSTEM_PROMPT_SUPPORT
)


class LLMEvaluator(EvaluatorInterface):
    """
    LLM-based evaluator for RAG system assessment.

    This evaluator uses LLM through the Bedrock client to assess
    the quality of responses generated by a RAG system based on:

    1. Relevance - Measures the correctness and relevance of the answer to the question
       on a four-point scale:
       2: The response correctly answers the user question and contains no irrelevant content
       1: The response provides a useful answer to the user question, but may contain irrelevant
          content that do not harm the usefulness of the answer
       0: No answer is provided in the response (e.g., "I don't know")
       -1: The response does not answer the question whatsoever

    2. Faithfulness - Assess whether the response is grounded in the retrieved passages
       on a three-point scale:
       1: Full support, all answer parts are grounded
       0: Partial support, not all answer parts are grounded
       -1: No support, all answer parts are not grounded
    """

    def __init__(
        self,
        model_id: str = "anthropic.claude-3-5-sonnet-20241022-v2:0",
        temperature: float = 0.0,
        max_tokens: int = 2048,
        use_gold_references: bool = True,
        silent_errors: bool = True,
        num_threads: int = 1,
        perform_system_analysis: bool = True,
        num_samples_for_analysis: int = 40
    ):
        """
        Initialize the LLM evaluator.

        Args:
            model_id: The model ID to use for evaluation
            temperature: The temperature parameter for generation (lower for more deterministic results)
            max_tokens: Maximum number of tokens to generate
            use_gold_references: Whether to include gold reference answers in the evaluation prompt
            silent_errors: Whether to silently handle errors by returning default scores (True) or raise exceptions (False)
            num_threads: Number of threads to use for parallel evaluation (>1 for parallel, 1 for sequential), note this only speed up API calls
            perform_system_analysis: Whether to automatically perform system analysis during evaluation
            num_samples_for_analysis: Total number of samples to analyze in system analysis (divided equally between lowest relevance and lowest faithfulness)
        """
        self.logger = get_logger("llm_evaluator")
        self.logger.info(f"Initializing LLM evaluator with model: {model_id}")

        # Initialize the Bedrock client with Claude Sonnet 3.5
        self.client = BedrockClient(
            model_id=model_id,
            temperature=temperature,
            max_tokens=max_tokens
        )

        self.evaluator_name = "llm_evaluator"
        self.use_gold_references = use_gold_references
        self.silent_errors = silent_errors
        self.num_threads = max(1, num_threads)
        self.perform_system_analysis = perform_system_analysis

        # Set the number of samples for each analysis type (divide total by 2)
        self.num_lowest_relevance = num_samples_for_analysis // 2
        self.num_lowest_faithfulness = num_samples_for_analysis // 2

    def _create_evaluation_prompt(self, prompt_template: str, rag_result: RAGResult, reference: Optional[QAPair] = None) -> str:
        """
        Create the evaluation prompt for a single RAG result.

        Args:
            rag_result: The RAG result to evaluate
            reference: Optional reference QA pair with gold standard answer

        Returns:
            The evaluation prompt as a string
        """
        # Format the documents
        documents = ""
        for i, doc in enumerate(rag_result.context, 1):
            documents += f"\nDocument {i}:\n{doc}\n"

        # Format the gold reference section
        gold_reference = ""
        if self.use_gold_references and reference is not None:
            gold_reference = f"""GOLD REFERENCE ANSWER:
{reference.answer}

"""

        # Fill in the template
        prompt = prompt_template.format(
            question=rag_result.question,
            answer=rag_result.answer,
            gold_reference=gold_reference,
            documents=documents
        )

        return prompt

    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """
        Parse the LLM response to extract the evaluation scores.

        Args:
            response: The LLM response as a string

        Returns:
            A dictionary containing the evaluation scores and notes
        """
        # Try to extract JSON using regex
        json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # If no code block, try to find JSON directly
            json_match = re.search(r'(\{.*\})', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                self.logger.error(
                    f"Failed to extract JSON from LLM response: {response}")
                return {
                    "relevance_score": 0,
                    "faithfulness_score": 0,
                    "evaluation_notes": "Failed to parse LLM response"
                }

        try:
            return json.loads(json_str)
            # # Ensure the required fields are present
            # if "relevance_score" not in result or "faithfulness_score" not in result:
            #     self.logger.error(
            #         f"Missing required fields in parsed JSON: {result}")
            #     return {
            #         "relevance_score": 0,
            #         "faithfulness_score": 0,
            #         "evaluation_notes": "Missing required fields in LLM response"
            #     }
            # return result
        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to parse JSON: {e}")
            return {
                "relevance_score": 0,
                "faithfulness_score": 0,
                "evaluation_notes": f"JSON parsing error: {str(e)}"
            }

    def _evaluate_single(self, rag_result: RAGResult, reference: Optional[QAPair] = None) -> Dict[str, Any]:
        """
        Evaluate a single RAG result.

        Args:
            rag_result: The RAG result to evaluate
            reference: Optional reference QA pair with gold standard answer

        Returns:
            A dictionary containing the evaluation metrics
        """
        try:
            # Create the evaluation prompt
            prompt_correctness = self._create_evaluation_prompt(CORRECTNESS_PROMPT_TEMPLATE,
                                                                rag_result, reference)
            prompt_support = self._create_evaluation_prompt(SUPPORT_PROMPT_TEMPLATE,
                                                            rag_result, reference)

            # Send the prompt to the LLM
            resp_correctness, raw_resp_correctness = self.client.complete_chat_once(
                prompt_correctness, SYSTEM_PROMPT_CORRECTNESS)
            resp_support, raw_resp_support = self.client.complete_chat_once(
                prompt_support, SYSTEM_PROMPT_SUPPORT)

            # Parse the LLM response
            eval_correctness = self._parse_llm_response(resp_correctness)
            eval_support = self._parse_llm_response(resp_support)

            # Add token usage and cost information if available
            if hasattr(raw_resp_correctness, "token_usage"):
                eval_correctness["token_usage"] = raw_resp_correctness.token_usage
            if hasattr(raw_resp_support, "token_usage"):
                eval_support["token_usage"] = raw_resp_support.token_usage
            if hasattr(raw_resp_correctness, "cost_usd"):
                eval_correctness["cost_usd"] = raw_resp_correctness.cost_usd
            if hasattr(raw_resp_support, "cost_usd"):
                eval_support["cost_usd"] = raw_resp_support.cost_usd

            # merge two evaluation fields
            evaluation = {
                'evaluation_notes': f"{eval_correctness.get('evaluation_notes', '')}\n\n{eval_support.get('evaluation_notes', '')}",
                'relevance_score': eval_correctness.get("relevance_score", 0),
                'faithfulness_score': eval_support.get("faithfulness_score", 0),
                'token_usage': {
                    'input_tokens': eval_correctness.get("token_usage", {}).get("input_tokens", 0) + eval_support.get("token_usage", {}).get("input_tokens", 0),
                    'output_tokens': eval_correctness.get("token_usage", {}).get("output_tokens", 0) + eval_support.get("token_usage", {}).get("output_tokens", 0),
                    'total_tokens': eval_correctness.get("token_usage", {}).get("total_tokens", 0) + eval_support.get("token_usage", {}).get("total_tokens", 0)
                },
                'cost_usd': eval_correctness.get("cost_usd", 0) + eval_support.get("cost_usd", 0)
            }

            # Log the evaluation
            self.logger.info(
                f"Evaluated RAG result",
                qid=rag_result.qid,
                relevance_score=evaluation["relevance_score"],
                faithfulness_score=evaluation["faithfulness_score"],
                cost_usd=evaluation.get("cost_usd", None),
                token_usage=evaluation.get("token_usage", None),
            )

            return evaluation
        except Exception as e:
            if self.silent_errors:
                self.logger.error(f"Error during evaluation: {e}")
                return {
                    "relevance_score": 0,
                    "faithfulness_score": 0,
                    "evaluation_notes": f"Evaluation error: {str(e)}"
                }
            else:
                self.logger.exception(e)
                raise e

    def _evaluate_with_threads(self, rag_results: List[RAGResult], references_by_qid: Dict[str, QAPair]) -> List[EvaluationResultRow]:
        """
        Evaluate RAG results using multiple threads.

        Args:
            rag_results: List of RAG results to evaluate
            references_by_qid: Dictionary mapping qid to reference QA pair

        Returns:
            List of EvaluationResultRow objects
        """
        # Filter out results with no qid
        valid_results = [r for r in rag_results if r.qid is not None]

        # Create a lock for thread-safe operations
        lock = Lock()
        rows = []

        def evaluate_and_collect(rag_result):
            # Get the reference QA pair if available
            reference = references_by_qid.get(
                rag_result.qid) if self.use_gold_references else None

            # Evaluate the RAG result
            evaluation = self._evaluate_single(rag_result, reference)

            # Create row-level result
            metrics = {
                "relevance_score": evaluation.get("relevance_score", 0),
                "faithfulness_score": evaluation.get("faithfulness_score", 0),
                "evaluation_notes": evaluation.get("evaluation_notes", "")
            }

            # Add token usage and cost to metrics if available
            if "token_usage" in evaluation:
                metrics["token_usage"] = evaluation["token_usage"]

            if "cost_usd" in evaluation:
                metrics["cost_usd"] = evaluation["cost_usd"]

            row = EvaluationResultRow(
                qid=rag_result.qid,
                metrics=metrics,
                evaluator_name=self.evaluator_name
            )

            # Thread-safe append to rows list
            with lock:
                rows.append(row)

            return row

        # Use ThreadPoolExecutor to parallelize evaluation
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_threads) as executor:
            # Submit all tasks and wait for them to complete
            list(executor.map(evaluate_and_collect, valid_results))

        return rows

    def evaluate(self, rag_results: List[RAGResult], references: List[QAPair]) -> EvaluationResult:
        """
        Evaluate a list of RAG results against a list of reference QA pairs.

        Args:
            rag_results: List of RAG results to evaluate
            references: List of reference QA pairs to compare against

        Returns:
            An EvaluationResult containing evaluation metrics and row-level results
        """
        start_time = time.time()
        self.logger.info(
            f"Evaluating {len(rag_results)} RAG results with {self.num_threads} threads")

        # Create a mapping of QA pairs by qid for easier lookup
        references_by_qid = {
            qa.qid: qa for qa in references if qa.qid is not None}

        # Skip results with no qid
        for rag_result in rag_results:
            if rag_result.qid is None:
                self.logger.warning("Skipping RAG result with no qid")

        # Evaluate using threads if num_threads > 1, otherwise use sequential evaluation
        if self.num_threads > 1:
            rows = self._evaluate_with_threads(rag_results, references_by_qid)
        else:
            # Sequential evaluation
            rows = []
            for rag_result in rag_results:
                # Skip if qid is None
                if rag_result.qid is None:
                    continue

                # Get the reference QA pair if available
                reference = references_by_qid.get(
                    rag_result.qid) if self.use_gold_references else None

                # Evaluate the RAG result
                evaluation = self._evaluate_single(rag_result, reference)

                # Create row-level result
                metrics = {
                    "relevance_score": evaluation.get("relevance_score", 0),
                    "faithfulness_score": evaluation.get("faithfulness_score", 0),
                    "evaluation_notes": evaluation.get("evaluation_notes", "")
                }

                # Add token usage and cost to metrics if available
                if "token_usage" in evaluation:
                    metrics["token_usage"] = evaluation["token_usage"]

                if "cost_usd" in evaluation:
                    metrics["cost_usd"] = evaluation["cost_usd"]

                row = EvaluationResultRow(
                    qid=rag_result.qid,
                    metrics=metrics,
                    evaluator_name=self.evaluator_name
                )
                rows.append(row)

        # Extract scores for aggregation
        relevance_scores = [row.metrics["relevance_score"] for row in rows]
        faithfulness_scores = [
            row.metrics["faithfulness_score"] for row in rows]

        # Calculate aggregate metrics
        metrics = {
            "avg_relevance_score": mean(relevance_scores) if relevance_scores else 0,
            "avg_faithfulness_score": mean(faithfulness_scores) if faithfulness_scores else 0,
            "count": len(rows)
        }

        # Calculate total cost if available
        total_cost = 0.0
        cost_available = False
        for row in rows:
            if "cost_usd" in row.metrics:
                total_cost += row.metrics["cost_usd"]
                cost_available = True

        # Add total cost to metrics if available
        if cost_available:
            metrics["total_cost"] = total_cost

        # Calculate total processing time
        total_time_ms = (time.time() - start_time) * 1000

        # Create the evaluation result
        result = EvaluationResult(
            metrics=metrics,
            evaluator_name=self.evaluator_name,
            sample_count=len(rows),
            system_name=rag_results[0].system_name if rag_results else None,
            rows=rows,
            total_time_ms=total_time_ms,
            total_cost=total_cost if cost_available else None
        )

        # Perform system analysis if requested and there are enough samples
        if self.perform_system_analysis and len(rows) >= 2:
            self.logger.info("Performing system analysis")
            system_analysis = self.summarize_evaluation(
                result,
                references,
                num_lowest_relevance=self.num_lowest_relevance,
                num_lowest_faithfulness=self.num_lowest_faithfulness
            )

            # Add system analysis to the evaluation result
            result.system_analysis = system_analysis

            # Add system analysis cost to total cost if available
            if cost_available and system_analysis.cost_usd is not None:
                result.total_cost += system_analysis.cost_usd
                result.metrics["total_cost"] = result.total_cost

        self.logger.info(
            f"Evaluation complete",
            avg_relevance_score=metrics["avg_relevance_score"],
            avg_faithfulness_score=metrics["avg_faithfulness_score"],
            count=metrics["count"],
            total_time_ms=total_time_ms,
            total_cost=total_cost if cost_available else None
        )

        return result

    def summarize_evaluation(
        self,
        evaluation_result: EvaluationResult,
        references: List[QAPair],
        num_lowest_relevance: int = 20,
        num_lowest_faithfulness: int = 20,
        model_id: str = "anthropic.claude-3-5-sonnet-20241022-v2:0",
        temperature: float = 0.0,
        max_tokens: int = 4096
    ) -> SystemAnalysis:
        """
        Analyze the evaluation results to identify patterns and issues in the RAG system.

        This method samples the lowest-scoring responses based on relevance and faithfulness,
        then uses an LLM to analyze the patterns and provide recommendations for improvement.

        Args:
            evaluation_result: The evaluation result to analyze
            references: List of reference QA pairs for additional context
            num_lowest_relevance: Number of lowest relevance score samples to include
            num_lowest_faithfulness: Number of lowest faithfulness score samples to include
            model_id: The model ID to use for the analysis
            temperature: The temperature parameter for generation
            max_tokens: Maximum number of tokens to generate

        Returns:
            SystemAnalysis object containing the analysis results
        """
        self.logger.info(
            f"Summarizing evaluation results",
            num_lowest_relevance=num_lowest_relevance,
            num_lowest_faithfulness=num_lowest_faithfulness
        )

        # Create a mapping of QA pairs by qid for easier lookup
        references_by_qid = {
            qa.qid: qa for qa in references if qa.qid is not None}

        # Check if there are enough rows to analyze
        if not evaluation_result.rows or len(evaluation_result.rows) < 2:
            self.logger.warning("Not enough evaluation rows to analyze")
            return SystemAnalysis(
                analysis="Unable to perform analysis due to insufficient data",
                samples_analyzed=0
            )

        # Get the lowest relevance score samples
        relevance_samples = []
        for row in evaluation_result.rows:
            relevance_score = row.metrics.get("relevance_score", 0)
            relevance_samples.append((relevance_score, row))

        # Sort by relevance score (ascending) and take the lowest N
        relevance_samples.sort(key=lambda x: x[0])
        lowest_relevance = relevance_samples[:num_lowest_relevance]

        # Get the lowest faithfulness score samples
        faithfulness_samples = []
        for row in evaluation_result.rows:
            faithfulness_score = row.metrics.get("faithfulness_score", 0)
            faithfulness_samples.append((faithfulness_score, row))

        # Sort by faithfulness score (ascending) and take the lowest N
        faithfulness_samples.sort(key=lambda x: x[0])
        lowest_faithfulness = faithfulness_samples[:num_lowest_faithfulness]

        # Combine the samples (may have duplicates if a response scored poorly on both metrics)
        combined_samples = lowest_relevance + lowest_faithfulness

        # Remove duplicates by qid
        unique_samples = {}
        for score, row in combined_samples:
            if row.qid not in unique_samples:
                unique_samples[row.qid] = (score, row)

        # Format the samples for the prompt
        formatted_samples = []
        for qid, (score, row) in unique_samples.items():
            # Get the reference QA pair if available
            reference = references_by_qid.get(qid)

            # Format user categories and question categories
            user_categories = []
            question_categories = []

            if reference:
                # Format user categories
                for category in reference.user_categories:
                    cat_str = f"{category.get('categorization_name', '')}: {category.get('category_name', '')}"
                    user_categories.append(cat_str)

                # Format question categories
                for category in reference.question_categories:
                    cat_str = f"{category.get('categorization_name', '')}: {category.get('category_name', '')}"
                    question_categories.append(cat_str)

            # Format the sample
            sample = f"""
SAMPLE {len(formatted_samples) + 1}:
QID: {qid}
Relevance Score: {row.metrics.get('relevance_score', 'N/A')}
Faithfulness Score: {row.metrics.get('faithfulness_score', 'N/A')}
Evaluation Notes: {row.metrics.get('evaluation_notes', 'N/A')}
User Categories: {', '.join(user_categories) if user_categories else 'N/A'}
Question Categories: {', '.join(question_categories) if question_categories else 'N/A'}
"""
            formatted_samples.append(sample)

        # Create the prompt
        prompt = SUMMARY_PROMPT_TEMPLATE.format(
            samples="\n".join(formatted_samples)
        )

        self.logger.debug("Generated prompt for LLM analysis", prompt=prompt)

        # Initialize a separate LLM client for the summary analysis
        summary_client = BedrockClient(
            model_id=model_id,
            temperature=temperature,
            max_tokens=max_tokens
        )

        try:
            # Send the prompt to the LLM
            response, raw_response = summary_client.complete_chat_once(
                prompt, SUMMARY_SYSTEM_PROMPT)

            # Log the analysis
            self.logger.info(
                f"Generated RAG system analysis",
                samples_analyzed=len(formatted_samples)
            )

            # Create the SystemAnalysis object
            token_usage = None
            if hasattr(raw_response, "token_usage"):
                token_usage = raw_response.token_usage

            cost_usd = None
            if hasattr(raw_response, "cost_usd"):
                cost_usd = raw_response.cost_usd

            return SystemAnalysis(
                analysis=response,
                samples_analyzed=len(formatted_samples),
                token_usage=token_usage,
                cost_usd=cost_usd
            )
        except Exception as e:
            self.logger.error(f"Error during summary analysis: {e}")
            return SystemAnalysis(
                analysis=f"Failed to generate analysis due to an error: {str(e)}",
                samples_analyzed=0
            )


# Main entry point for testing the evaluator
if __name__ == "__main__":
    import os
    from dotenv import load_dotenv
    from systems.rag_result import RAGResult
    from services.ds_data_morgana import QAPair

    # Load environment variables
    load_dotenv()

    # Check if required environment variables are set
    if not os.environ.get("RACE_AWS_ACCESS_KEY_ID") or not os.environ.get("RACE_AWS_SECRET_ACCESS_KEY"):
        print("Error: RACE_AWS_ACCESS_KEY_ID or RACE_AWS_SECRET_ACCESS_KEY environment variables not set")
        exit(1)

    # Create sample RAG results
    rag_result1 = RAGResult(
        question="What is retrieval-augmented generation?",
        answer="Retrieval-augmented generation (RAG) is a technique that combines retrieval of relevant documents with text generation.",
        context=[
            "Retrieval-augmented generation (RAG) is an AI framework that combines information retrieval with text generation."],
        doc_ids=["doc1"],
        total_time_ms=100.0,
        qid="1",
        system_name="TestRAGSystem"
    )

    # Create sample reference QA pairs
    reference1 = QAPair(
        question="What is retrieval-augmented generation?",
        answer="Retrieval-augmented generation (RAG) is an AI framework that enhances large language models by retrieving external knowledge.",
        context=[
            "Retrieval-augmented generation (RAG) is an AI framework that enhances large language models by retrieving external knowledge."],
        question_categories=[],
        user_categories=[],
        document_ids=["doc1"],
        qid="1"
    )

    # Initialize the evaluator with Claude Sonnet 3.5 and gold references enabled (default)
    evaluator_with_refs = LLMEvaluator(use_gold_references=True)

    print("\nEvaluating RAG result with gold references...")

    # Evaluate the RAG results with gold references
    result_with_refs = evaluator_with_refs.evaluate(
        [rag_result1], [reference1])

    # Initialize the evaluator with gold references disabled
    evaluator_no_refs = LLMEvaluator(use_gold_references=False)

    print("\nEvaluating RAG result without gold references...")

    # Evaluate the RAG results without gold references
    result_no_refs = evaluator_no_refs.evaluate([rag_result1], [reference1])

    # Print the aggregated results
    print("\n" + "="*80)
    print("Aggregated Metrics with Gold References:")
    for key, value in result_with_refs.metrics.items():
        if isinstance(value, float):
            print(f"{key}: {value:.4f}")
        else:
            print(f"{key}: {value}")

    print("\nAggregated Metrics without Gold References:")
    for key, value in result_no_refs.metrics.items():
        if isinstance(value, float):
            print(f"{key}: {value:.4f}")
        else:
            print(f"{key}: {value}")

    # Print row-level results for evaluation with gold references
    if result_with_refs.rows:
        print("\nRow-level Results with Gold References:")
        for row in result_with_refs.rows:
            print(f"\nQID: {row.qid}")
            print(f"Relevance Score: {row.metrics['relevance_score']}")
            print(f"Faithfulness Score: {row.metrics['faithfulness_score']}")
            print(f"Evaluation Notes: {row.metrics['evaluation_notes']}")

    print("="*80 + "\n")

    # print summary of evaluation
    summary_result = evaluator_with_refs.summarize_evaluation(
        result_with_refs,
        [reference1],
        num_lowest_relevance=1,
        num_lowest_faithfulness=1
    )
    print("\nSummary of Evaluation:")
    print(summary_result)
