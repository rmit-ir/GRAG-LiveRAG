"""
LLM-based evaluator for RAG system assessment.

This module implements an evaluator that uses Claude Sonnet 3.5 through the Bedrock client
to assess the quality of responses generated by a RAG system based on relevance and faithfulness.
"""
from typing import List, Dict, Any, Optional
import json
import re
import time
from statistics import mean
import concurrent.futures
from threading import Lock

from evaluators.evaluator_interface import EvaluatorInterface
from evaluators.evaluation_result import EvaluationResult, EvaluationResultRow
from systems.rag_result import RAGResult
from services.ds_data_morgana import QAPair
from services.llms.bedrock_client import BedrockClient
from utils.logging_utils import get_logger
from evaluators.llm_evaluator.prompts import SYSTEM_PROMPT, EVALUATION_PROMPT_TEMPLATE


class LLMEvaluator(EvaluatorInterface):
    """
    LLM-based evaluator for RAG system assessment.
    
    This evaluator uses LLM through the Bedrock client to assess
    the quality of responses generated by a RAG system based on:
    
    1. Relevance - Measures the correctness and relevance of the answer to the question
       on a four-point scale:
       2: The response correctly answers the user question and contains no irrelevant content
       1: The response provides a useful answer to the user question, but may contain irrelevant
          content that do not harm the usefulness of the answer
       0: No answer is provided in the response (e.g., "I don't know")
       -1: The response does not answer the question whatsoever
    
    2. Faithfulness - Assess whether the response is grounded in the retrieved passages
       on a three-point scale:
       1: Full support, all answer parts are grounded
       0: Partial support, not all answer parts are grounded
       -1: No support, all answer parts are not grounded
    """
    
    def __init__(
        self,
        model_id: str = "anthropic.claude-3-5-sonnet-20241022-v2:0",
        temperature: float = 0.0,
        max_tokens: int = 2048,
        use_gold_references: bool = True,
        silent_errors: bool = True,
        num_threads: int = 1
    ):
        """
        Initialize the LLM evaluator.
        
        Args:
            model_id: The model ID to use for evaluation
            temperature: The temperature parameter for generation (lower for more deterministic results)
            max_tokens: Maximum number of tokens to generate
            use_gold_references: Whether to include gold reference answers in the evaluation prompt
            silent_errors: Whether to silently handle errors by returning default scores (True) or raise exceptions (False)
            num_threads: Number of threads to use for parallel evaluation (>1 for parallel, 1 for sequential), note this only speed up API calls
        """
        self.logger = get_logger("llm_evaluator")
        self.logger.info(f"Initializing LLM evaluator with model: {model_id}")
        
        # Initialize the Bedrock client with Claude Sonnet 3.5
        self.client = BedrockClient(
            model_id=model_id,
            system_message=self._get_system_prompt(),
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        self.evaluator_name = "llm_evaluator"
        self.use_gold_references = use_gold_references
        self.silent_errors = silent_errors
        self.num_threads = max(1, num_threads)
    
    def _get_system_prompt(self) -> str:
        """
        Get the system prompt for the LLM evaluator.
        
        Returns:
            The system prompt as a string
        """
        return SYSTEM_PROMPT
    
    def _create_evaluation_prompt(self, rag_result: RAGResult, reference: Optional[QAPair] = None) -> str:
        """
        Create the evaluation prompt for a single RAG result.
        
        Args:
            rag_result: The RAG result to evaluate
            reference: Optional reference QA pair with gold standard answer
            
        Returns:
            The evaluation prompt as a string
        """
        # Format the documents
        documents = ""
        for i, doc in enumerate(rag_result.context, 1):
            documents += f"\nDocument {i}:\n{doc}\n"
        
        # Format the gold reference section
        gold_reference = ""
        if self.use_gold_references and reference is not None:
            gold_reference = f"""GOLD REFERENCE ANSWER:
{reference.answer}

"""
        
        # Fill in the template
        prompt = EVALUATION_PROMPT_TEMPLATE.format(
            question=rag_result.question,
            answer=rag_result.answer,
            gold_reference=gold_reference,
            documents=documents
        )
        
        return prompt
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """
        Parse the LLM response to extract the evaluation scores.
        
        Args:
            response: The LLM response as a string
            
        Returns:
            A dictionary containing the evaluation scores and notes
        """
        # Try to extract JSON using regex
        json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # If no code block, try to find JSON directly
            json_match = re.search(r'(\{.*\})', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                self.logger.error(f"Failed to extract JSON from LLM response: {response}")
                return {
                    "relevance_score": 0,
                    "faithfulness_score": 0,
                    "evaluation_notes": "Failed to parse LLM response"
                }
        
        try:
            result = json.loads(json_str)
            # Ensure the required fields are present
            if "relevance_score" not in result or "faithfulness_score" not in result:
                self.logger.error(f"Missing required fields in parsed JSON: {result}")
                return {
                    "relevance_score": 0,
                    "faithfulness_score": 0,
                    "evaluation_notes": "Missing required fields in LLM response"
                }
            return result
        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to parse JSON: {e}")
            return {
                "relevance_score": 0,
                "faithfulness_score": 0,
                "evaluation_notes": f"JSON parsing error: {str(e)}"
            }
    
    def _evaluate_single(self, rag_result: RAGResult, reference: Optional[QAPair] = None) -> Dict[str, Any]:
        """
        Evaluate a single RAG result.
        
        Args:
            rag_result: The RAG result to evaluate
            reference: Optional reference QA pair with gold standard answer
            
        Returns:
            A dictionary containing the evaluation metrics
        """
        try:
            # Create the evaluation prompt
            prompt = self._create_evaluation_prompt(rag_result, reference)
            
            # Send the prompt to the LLM
            _, response = self.client.query(prompt)
            
            # Parse the LLM response
            evaluation = self._parse_llm_response(response)
            
            # Log the evaluation
            self.logger.info(
                f"Evaluated RAG result",
                qid=rag_result.qid,
                relevance_score=evaluation["relevance_score"],
                faithfulness_score=evaluation["faithfulness_score"]
            )
            
            return evaluation
        except Exception as e:
            if self.silent_errors:
                self.logger.error(f"Error during evaluation: {e}")
                return {
                    "relevance_score": 0,
                    "faithfulness_score": 0,
                    "evaluation_notes": f"Evaluation error: {str(e)}"
                }
            else:
                self.logger.exception(e)
                raise e
    
    def _evaluate_with_threads(self, rag_results: List[RAGResult], references_by_qid: Dict[str, QAPair]) -> List[EvaluationResultRow]:
        """
        Evaluate RAG results using multiple threads.
        
        Args:
            rag_results: List of RAG results to evaluate
            references_by_qid: Dictionary mapping qid to reference QA pair
            
        Returns:
            List of EvaluationResultRow objects
        """
        # Filter out results with no qid
        valid_results = [r for r in rag_results if r.qid is not None]
        
        # Create a lock for thread-safe operations
        lock = Lock()
        rows = []
        
        def evaluate_and_collect(rag_result):
            # Get the reference QA pair if available
            reference = references_by_qid.get(rag_result.qid) if self.use_gold_references else None
            
            # Evaluate the RAG result
            evaluation = self._evaluate_single(rag_result, reference)
            
            # Create row-level result
            row = EvaluationResultRow(
                qid=rag_result.qid,
                metrics={
                    "relevance_score": evaluation.get("relevance_score", 0),
                    "faithfulness_score": evaluation.get("faithfulness_score", 0),
                    "evaluation_notes": evaluation.get("evaluation_notes", "")
                },
                evaluator_name=self.evaluator_name
            )
            
            # Thread-safe append to rows list
            with lock:
                rows.append(row)
            
            return row
        
        # Use ThreadPoolExecutor to parallelize evaluation
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_threads) as executor:
            # Submit all tasks and wait for them to complete
            list(executor.map(evaluate_and_collect, valid_results))
        
        return rows
    
    def evaluate(self, rag_results: List[RAGResult], references: List[QAPair]) -> EvaluationResult:
        """
        Evaluate a list of RAG results against a list of reference QA pairs.
        
        Args:
            rag_results: List of RAG results to evaluate
            references: List of reference QA pairs to compare against
            
        Returns:
            An EvaluationResult containing evaluation metrics and row-level results
        """
        start_time = time.time()
        self.logger.info(f"Evaluating {len(rag_results)} RAG results with {self.num_threads} threads")
        
        # Create a mapping of QA pairs by qid for easier lookup
        references_by_qid = {qa.qid: qa for qa in references if qa.qid is not None}
        
        # Skip results with no qid
        for rag_result in rag_results:
            if rag_result.qid is None:
                self.logger.warning("Skipping RAG result with no qid")
        
        # Evaluate using threads if num_threads > 1, otherwise use sequential evaluation
        if self.num_threads > 1:
            rows = self._evaluate_with_threads(rag_results, references_by_qid)
        else:
            # Sequential evaluation
            rows = []
            for rag_result in rag_results:
                # Skip if qid is None
                if rag_result.qid is None:
                    continue
                
                # Get the reference QA pair if available
                reference = references_by_qid.get(rag_result.qid) if self.use_gold_references else None
                
                # Evaluate the RAG result
                evaluation = self._evaluate_single(rag_result, reference)
                
                # Create row-level result
                row = EvaluationResultRow(
                    qid=rag_result.qid,
                    metrics={
                        "relevance_score": evaluation.get("relevance_score", 0),
                        "faithfulness_score": evaluation.get("faithfulness_score", 0),
                        "evaluation_notes": evaluation.get("evaluation_notes", "")
                    },
                    evaluator_name=self.evaluator_name
                )
                rows.append(row)
        
        # Extract scores for aggregation
        relevance_scores = [row.metrics["relevance_score"] for row in rows]
        faithfulness_scores = [row.metrics["faithfulness_score"] for row in rows]
        
        # Calculate aggregate metrics
        metrics = {
            "avg_relevance_score": mean(relevance_scores) if relevance_scores else 0,
            "avg_faithfulness_score": mean(faithfulness_scores) if faithfulness_scores else 0,
            "count": len(rows)
        }
        
        # Calculate total processing time
        total_time_ms = (time.time() - start_time) * 1000
        
        # Create the evaluation result
        result = EvaluationResult(
            metrics=metrics,
            evaluator_name=self.evaluator_name,
            sample_count=len(rows),
            system_name=rag_results[0].system_name if rag_results else None,
            rows=rows,
            total_time_ms=total_time_ms
        )
        
        self.logger.info(
            f"Evaluation complete",
            avg_relevance_score=metrics["avg_relevance_score"],
            avg_faithfulness_score=metrics["avg_faithfulness_score"],
            count=metrics["count"],
            total_time_ms=total_time_ms
        )
        
        return result


# Main entry point for testing the evaluator
if __name__ == "__main__":
    import os
    from dotenv import load_dotenv
    from systems.rag_result import RAGResult
    from services.ds_data_morgana import QAPair
    
    # Load environment variables
    load_dotenv()
    
    # Check if required environment variables are set
    if not os.environ.get("RACE_AWS_ACCESS_KEY_ID") or not os.environ.get("RACE_AWS_SECRET_ACCESS_KEY"):
        print("Error: RACE_AWS_ACCESS_KEY_ID or RACE_AWS_SECRET_ACCESS_KEY environment variables not set")
        exit(1)
    
    # Create sample RAG results
    rag_result1 = RAGResult(
        question="What is retrieval-augmented generation?",
        answer="Retrieval-augmented generation (RAG) is a technique that combines retrieval of relevant documents with text generation.",
        context=["Retrieval-augmented generation (RAG) is an AI framework that combines information retrieval with text generation."],
        doc_ids=["doc1"],
        total_time_ms=100.0,
        qid="1",
        system_name="TestRAGSystem"
    )
    
    # Create sample reference QA pairs
    reference1 = QAPair(
        question="What is retrieval-augmented generation?",
        answer="Retrieval-augmented generation (RAG) is an AI framework that enhances large language models by retrieving external knowledge.",
        context=["Retrieval-augmented generation (RAG) is an AI framework that enhances large language models by retrieving external knowledge."],
        question_categories=[],
        user_categories=[],
        document_ids=["doc1"],
        qid="1"
    )
    
    # Initialize the evaluator with Claude Sonnet 3.5 and gold references enabled (default)
    evaluator_with_refs = LLMEvaluator(use_gold_references=True)
    
    print("\nEvaluating RAG result with gold references...")
    
    # Evaluate the RAG results with gold references
    result_with_refs = evaluator_with_refs.evaluate([rag_result1], [reference1])
    
    # Initialize the evaluator with gold references disabled
    evaluator_no_refs = LLMEvaluator(use_gold_references=False)
    
    print("\nEvaluating RAG result without gold references...")
    
    # Evaluate the RAG results without gold references
    result_no_refs = evaluator_no_refs.evaluate([rag_result1], [reference1])
    
    # Print the aggregated results
    print("\n" + "="*80)
    print("Aggregated Metrics with Gold References:")
    for key, value in result_with_refs.metrics.items():
        if isinstance(value, float):
            print(f"{key}: {value:.4f}")
        else:
            print(f"{key}: {value}")
    
    print("\nAggregated Metrics without Gold References:")
    for key, value in result_no_refs.metrics.items():
        if isinstance(value, float):
            print(f"{key}: {value:.4f}")
        else:
            print(f"{key}: {value}")
    
    # Print row-level results for evaluation with gold references
    if result_with_refs.rows:
        print("\nRow-level Results with Gold References:")
        for row in result_with_refs.rows:
            print(f"\nQID: {row.qid}")
            print(f"Relevance Score: {row.metrics['relevance_score']}")
            print(f"Faithfulness Score: {row.metrics['faithfulness_score']}")
            print(f"Evaluation Notes: {row.metrics['evaluation_notes']}")
    
    print("="*80 + "\n")
