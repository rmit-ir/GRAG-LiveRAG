# LLM Evaluator

An LLM-based evaluator for assessing RAG (Retrieval-Augmented Generation) system responses using LLM through the Bedrock client.

## Overview

The LLM Evaluator uses a large language model to assess the quality of responses generated by RAG systems based on two key criteria:

1. **Relevance** - Measures the correctness and relevance of the answer to the question on a four-point scale:
   - `2`: The response correctly answers the user question and contains no irrelevant content
   - `1`: The response provides a useful answer to the user question, but may contain irrelevant content that does not harm the usefulness of the answer
   - `0`: No answer is provided in the response (e.g., "I don't know")
   - `-1`: The response does not answer the question whatsoever

2. **Faithfulness** - Assesses whether the response is grounded in the retrieved passages on a three-point scale:
   - `1`: Full support, all answer parts are grounded
   - `0`: Partial support, not all answer parts are grounded
   - `-1`: No support, all answer parts are not grounded

## Usage

### Basic Usage

```python
from evaluators.llm_evaluator.llm_evaluator import LLMEvaluator
from systems.rag_result import RAGResult
from services.ds_data_morgana import QAPair

# Initialize the evaluator
evaluator = LLMEvaluator()

# Create RAG results to evaluate
rag_results = [
    RAGResult(
        question="What is retrieval-augmented generation?",
        answer="Retrieval-augmented generation (RAG) is a technique that combines retrieval of relevant documents with text generation.",
        context=["Retrieval-augmented generation (RAG) is an AI framework that combines information retrieval with text generation."],
        doc_ids=["doc1"],
        total_time_ms=100.0,
        qid="1",
        system_name="TestRAGSystem"
    )
]

# Create reference QA pairs
references = [
    QAPair(
        question="What is retrieval-augmented generation?",
        answer="Retrieval-augmented generation (RAG) is an AI framework that enhances large language models by retrieving external knowledge.",
        context=["Retrieval-augmented generation (RAG) is an AI framework that enhances large language models by retrieving external knowledge."],
        question_categories=[],
        user_categories=[],
        document_ids=["doc1"],
        qid="1"
    )
]

# Evaluate the RAG results
evaluation_result = evaluator.evaluate(rag_results, references)

# Access the evaluation results
print(f"Average Relevance Score: {evaluation_result.metrics['avg_relevance_score']}")
print(f"Average Faithfulness Score: {evaluation_result.metrics['avg_faithfulness_score']}")
print(f"Total Evaluated: {evaluation_result.metrics['count']}")

# Access row-level results
for row in evaluation_result.rows:
    print(f"QID: {row.qid}")
    print(f"Relevance Score: {row.metrics['relevance_score']}")
    print(f"Faithfulness Score: {row.metrics['faithfulness_score']}")
    print(f"Evaluation Notes: {row.metrics['evaluation_notes']}")
```

### Configuration Options

The LLM Evaluator can be configured with the following parameters:

- `model_id`: The model ID to use for evaluation (default: "anthropic.claude-3-5-sonnet-20241022-v2:0")
- `temperature`: The temperature parameter for generation (default: 0.0)
- `max_tokens`: Maximum number of tokens to generate (default: 2048)
- `num_threads`: Important for parallel evaluation, LLMs are slow! (default: 1)

## Example Output

```bash
================================================================================
Aggregated Metrics with Gold References:
avg_relevance_score: 1
avg_faithfulness_score: 1
count: 1

Aggregated Metrics without Gold References:
avg_relevance_score: 2
avg_faithfulness_score: 1
count: 1

Row-level Results with Gold References:

QID: 1
Relevance Score: 1
Faithfulness Score: 1
Evaluation Notes: The response provides a correct but somewhat incomplete definition of RAG compared to the gold reference. It captures the core concept of combining retrieval with generation but misses mentioning that it specifically enhances LLMs with external knowledge. The response is fully supported by Document 1, which contains almost identical wording.
================================================================================
```

Notice with gold references provided, the evaluator is able to spot the tiny incompleteness in the answer (only 1), while without gold references, it gives a perfect score (highest 2).

## Environment Variables

The evaluator requires the following environment variables to be set:

- `RACE_AWS_ACCESS_KEY_ID`: AWS access key ID for Bedrock
- `RACE_AWS_SECRET_ACCESS_KEY`: AWS secret access key for Bedrock
- `RACE_AWS_SESSION_TOKEN`: AWS session token for Bedrock

## Implementation Details

The evaluator works by:

1. Creating a prompt that includes the question, RAG system response, and retrieved documents
2. Sending this prompt to the LLM
3. Parsing the LLM's response to extract the evaluation scores and notes
4. Aggregating the results across multiple evaluations

The system prompt instructs the LLM to act as an expert evaluator for RAG systems and to provide structured evaluations based on the defined criteria.
