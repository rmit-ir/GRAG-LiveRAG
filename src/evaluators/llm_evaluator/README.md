# LLM Evaluator

An LLM-based evaluator for assessing RAG (Retrieval-Augmented Generation) system responses using LLM through the Bedrock client.

## Overview

The LLM Evaluator uses a large language model to assess the quality of responses generated by RAG systems based on two key criteria:

1. **Relevance** - Measures the correctness and relevance of the answer to the question on a four-point scale:
   - `2`: The response correctly answers the user question and contains no irrelevant content
   - `1`: The response provides a useful answer to the user question, but may contain irrelevant content that does not harm the usefulness of the answer
   - `0`: No answer is provided in the response (e.g., "I don't know")
   - `-1`: The response does not answer the question whatsoever

2. **Faithfulness** - Assesses whether the response is grounded in the retrieved passages on a three-point scale:
   - `1`: Full support, all answer parts are grounded
   - `0`: Partial support, not all answer parts are grounded
   - `-1`: No support, all answer parts are not grounded

## Usage

### Basic Usage

```bash
# Check the help message
uv run scripts/evaluate.py --evaluator evaluators.llm_evaluator.llm_evaluator.LLMEvaluator --help

# Evaluate dmds_JK09SKjyanxs1_BasicRAGSystem.tsv against the gold reference
uv run scripts/evaluate.py --evaluator evaluators.llm_evaluator.llm_evaluator.LLMEvaluator --results data/rag_results/dmds_JK09SKjyanxs1_BasicRAGSystem.tsv --reference data/generated_qa_pairs/dmds_JK09SKjyanxs1.n5.tsv

# Same, but with 5 threads parallel evaluation
uv run scripts/evaluate.py --evaluator evaluators.llm_evaluator.llm_evaluator.LLMEvaluator --results data/rag_results/dmds_JK09SKjyanxs1_BasicRAGSystem.tsv --reference data/generated_qa_pairs/dmds_JK09SKjyanxs1.n5.tsv --no-silent_errors --num_threads 5

# Same, but ignoring gold reference
uv run scripts/evaluate.py --evaluator evaluators.llm_evaluator.llm_evaluator.LLMEvaluator --results data/rag_results/dmds_JK09SKjyanxs1_BasicRAGSystem.tsv --reference data/generated_qa_pairs/dmds_JK09SKjyanxs1.n5.tsv --no-use_gold_references --no-silent_errors --num_threads 5
```

## Example Output

### Gold References

```bash
================================================================================
Aggregated Metrics with Gold References:
avg_relevance_score: 1
avg_faithfulness_score: 1
count: 1

Aggregated Metrics without Gold References:
avg_relevance_score: 2
avg_faithfulness_score: 1
count: 1

Row-level Results with Gold References:

QID: 1
Relevance Score: 1
Faithfulness Score: 1
Evaluation Notes: The response provides a correct but somewhat incomplete definition of RAG compared to the gold reference. It captures the core concept of combining retrieval with generation but misses mentioning that it specifically enhances LLMs with external knowledge. The response is fully supported by Document 1, which contains almost identical wording.
================================================================================
```

Notice with gold references provided, the evaluator is able to spot the tiny incompleteness in the answer (only 1), while without gold references, it gives a perfect score (highest 2).

It prints out the costs too:

```bash
$ uv run scripts/evaluate.py --evaluator evaluators.llm_evaluator.llm_evaluator.LLMEvaluator --results data/rag_results/dmds_JK09SKjyanxs1_BasicRAGSystem.tsv --reference data/generated_qa_pairs/dmds_JK09SKjyanxs1.n5.tsv --no-silent_errors --num_threads 5

Evaluation complete!
Total time: 11381.10 ms (11.38 s)
Results evaluated: 5
Evaluation time: 10991.26 ms (10.99 s)
Average query eval time: 2198.25 ms
Total cost: $0.094815 USD
Average cost per query: $0.018963 USD
Output saved to:
  - Aggregated results: /Users/kun/Projects/rmit/research/live-rag/LiveRAG/data/evaluation_results/dmds_JK09SKjyanxs1_BasicRAGSystem.LLMEvaluator.evaluation.aggregated.tsv
  - Row-level results: /Users/kun/Projects/rmit/research/live-rag/LiveRAG/data/evaluation_results/dmds_JK09SKjyanxs1_BasicRAGSystem.LLMEvaluator.evaluation.rows.tsv
```

### System Analysis

Command

```bash
uv run scripts/evaluate.py --evaluator evaluators.llm_evaluator.llm_evaluator.LLMEvaluator --results data/rag_results/dmds_fJ20pJnq9zcO1_BasicRAGSystem_ec2_llm_4gpu.tsv --reference data/generated_qa_pairs/dmds_fJ20pJnq9zcO1.n100.tsv --num_threads 20
```

Note by default it will take the first 40 lowest scores results and analyze them.

```markdown
Based on the 33 samples provided, here's a comprehensive analysis:

1. PATTERN ANALYSIS

Key Patterns in Low-Performing Responses:

A. Faithfulness Issues (Most Common):

- Hallucination of plausible but unsupported details (especially prevalent in expert-domain questions)
- Reliance on general knowledge rather than retrieved documents
- Tendency to provide comprehensive answers even when document support is lacking

B. Relevance Issues:

- "I don't know" responses when relevant information is actually present (Samples 3, 9, 10)
- Missing key information while providing tangentially related details
- Over-generalization of specific questions

C. Response Behaviors:

- Conservative responses (saying no information is available) when partial information exists
- Mixing supported and unsupported claims within single responses
- Tendency to provide technically accurate but ungrounded information

2. CATEGORY CORRELATION

A. Question Categories Impact:

- "distant-from-document" linguistic variation shows highest failure rate (22 samples)
- "open-ended" questions have higher hallucination rates than factoid questions
- "long-search-query" questions often receive partially relevant responses
- "with-premise" questions show higher rates of unfaithful responses

B. User Category Patterns:

Expert Users:

- Higher rate of hallucination in technical responses
- More detailed but often ungrounded answers
- Higher frequency of partial matches rather than complete misses

Novice Users:

- More generic, cautious responses
- Higher rate of "no information available" responses
- Better faithfulness scores but lower relevance

3. ROOT CAUSES

A. Retrieval Issues:

- Insufficient semantic matching between questions and documents
- Poor handling of questions requiring synthesis across multiple documents
- Weak performance on questions requiring specific technical details

B. Generation Issues:

- Over-reliance on model's general knowledge vs retrieved content
- Lack of proper grounding mechanisms
- Poor balance between completeness and faithfulness

C. Systemic Issues:

- Difficulty maintaining faithfulness while providing comprehensive answers
- Weak handling of technical/domain-specific content
- Inconsistent treatment of partial information vs no information
- Poor integration of multiple document sources

Priority Areas for Improvement:

1. Strengthen grounding mechanisms to prevent hallucination
2. Improve retrieval for technical and domain-specific content
3. Better handling of partial information in documents
4. Enhanced cross-document synthesis capabilities
5. Better calibration between expert/novice user expectations

The analysis reveals a system that prioritizes providing comprehensive answers over maintaining strict faithfulness to sources, particularly struggling with technical content and questions requiring synthesis across multiple documents. The most critical issue appears to be the balance between providing useful information while maintaining faithfulness to retrieved documents.
```

## Environment Variables

The evaluator requires the following environment variables to be set:

- `RACE_AWS_ACCESS_KEY_ID`: AWS access key ID for Bedrock
- `RACE_AWS_SECRET_ACCESS_KEY`: AWS secret access key for Bedrock
- `RACE_AWS_SESSION_TOKEN`: AWS session token for Bedrock

## Implementation Details

The evaluator works by:

1. Creating a prompt that includes the question, RAG system response, and retrieved documents
2. Sending this prompt to the LLM
3. Parsing the LLM's response to extract the evaluation scores and notes
4. Aggregating the results across multiple evaluations

The system prompt instructs the LLM to act as an expert evaluator for RAG systems and to provide structured evaluations based on the defined criteria.
