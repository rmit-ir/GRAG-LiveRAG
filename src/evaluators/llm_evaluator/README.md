# LLM Evaluator

An LLM-based evaluator for assessing RAG (Retrieval-Augmented Generation) system responses using LLM through the Bedrock client.

## Overview

The LLM Evaluator uses a large language model to assess the quality of responses generated by RAG systems based on two key criteria:

1. **Relevance** - Measures the correctness and relevance of the answer to the question on a four-point scale:
   - `2`: The response correctly answers the user question and contains no irrelevant content
   - `1`: The response provides a useful answer to the user question, but may contain irrelevant content that does not harm the usefulness of the answer
   - `0`: No answer is provided in the response (e.g., "I don't know")
   - `-1`: The response does not answer the question whatsoever

2. **Faithfulness** - Assesses whether the response is grounded in the retrieved passages on a three-point scale:
   - `1`: Full support, all answer parts are grounded
   - `0`: Partial support, not all answer parts are grounded
   - `-1`: No support, all answer parts are not grounded

## Usage

### Basic Usage

```bash
# Check the help message
uv run scripts/evaluate.py --evaluator evaluators.llm_evaluator.llm_evaluator.LLMEvaluator --help

# Evaluate dmds_4p3PUk5HORIw_BasicRAGSystem.tsv against the gold reference
uv run scripts/evaluate.py --evaluator evaluators.llm_evaluator.llm_evaluator.LLMEvaluator --results data/rag_results/dmds_4p3PUk5HORIw_BasicRAGSystem.tsv --reference data/generated_qa_pairs/dmds_4p3PUk5HORIw.n5.tsv

# Same, but with 5 threads parallel evaluation
uv run scripts/evaluate.py --evaluator evaluators.llm_evaluator.llm_evaluator.LLMEvaluator --results data/rag_results/dmds_4p3PUk5HORIw_BasicRAGSystem.tsv --reference data/generated_qa_pairs/dmds_4p3PUk5HORIw.n5.tsv --no-silent_errors --num_threads 5

# Same, but ignoring gold reference
uv run scripts/evaluate.py --evaluator evaluators.llm_evaluator.llm_evaluator.LLMEvaluator --results data/rag_results/dmds_4p3PUk5HORIw_BasicRAGSystem.tsv --reference data/generated_qa_pairs/dmds_4p3PUk5HORIw.n5.tsv --no-use_gold_references --no-silent_errors --num_threads 5
```

### Configuration Options

The LLM Evaluator can be configured with the following parameters:

- `model_id`: The model ID to use for evaluation (default: "anthropic.claude-3-5-sonnet-20241022-v2:0")
- `temperature`: The temperature parameter for generation (default: 0.0)
- `max_tokens`: Maximum number of tokens to generate (default: 2048)
- `num_threads`: Important for parallel evaluation, LLMs are slow! (default: 1)

## Example Output

```bash
================================================================================
Aggregated Metrics with Gold References:
avg_relevance_score: 1
avg_faithfulness_score: 1
count: 1

Aggregated Metrics without Gold References:
avg_relevance_score: 2
avg_faithfulness_score: 1
count: 1

Row-level Results with Gold References:

QID: 1
Relevance Score: 1
Faithfulness Score: 1
Evaluation Notes: The response provides a correct but somewhat incomplete definition of RAG compared to the gold reference. It captures the core concept of combining retrieval with generation but misses mentioning that it specifically enhances LLMs with external knowledge. The response is fully supported by Document 1, which contains almost identical wording.
================================================================================
```

Notice with gold references provided, the evaluator is able to spot the tiny incompleteness in the answer (only 1), while without gold references, it gives a perfect score (highest 2).

It prints out the costs too:

```bash
$ uv run scripts/evaluate.py --evaluator evaluators.llm_evaluator.llm_evaluator.LLMEvaluator --results data/rag_results/dmds_4p3PUk5HORIw_BasicRAGSystem.tsv --reference data/generated_qa_pairs/dmds_4p3PUk5HORIw.n5.tsv --no-silent_errors --num_threads 5

Evaluation complete!
Total time: 11381.10 ms (11.38 s)
Results evaluated: 5
Evaluation time: 10991.26 ms (10.99 s)
Average query eval time: 2198.25 ms
Total cost: $0.094815 USD
Average cost per query: $0.018963 USD
Output saved to:
  - Aggregated results: /Users/kun/Projects/rmit/research/live-rag/LiveRAG/data/evaluation_results/dmds_4p3PUk5HORIw_BasicRAGSystem.LLMEvaluator.evaluation.aggregated.tsv
  - Row-level results: /Users/kun/Projects/rmit/research/live-rag/LiveRAG/data/evaluation_results/dmds_4p3PUk5HORIw_BasicRAGSystem.LLMEvaluator.evaluation.rows.tsv
```

## Environment Variables

The evaluator requires the following environment variables to be set:

- `RACE_AWS_ACCESS_KEY_ID`: AWS access key ID for Bedrock
- `RACE_AWS_SECRET_ACCESS_KEY`: AWS secret access key for Bedrock
- `RACE_AWS_SESSION_TOKEN`: AWS session token for Bedrock

## Implementation Details

The evaluator works by:

1. Creating a prompt that includes the question, RAG system response, and retrieved documents
2. Sending this prompt to the LLM
3. Parsing the LLM's response to extract the evaluation scores and notes
4. Aggregating the results across multiple evaluations

The system prompt instructs the LLM to act as an expert evaluator for RAG systems and to provide structured evaluations based on the defined criteria.
