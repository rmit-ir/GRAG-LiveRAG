---
abstract: |
  We provide a systematic understanding of the impact of specific
  components and wordings used in prompts on the effectiveness of
  rankers based on zero-shot Large Language Models (LLMs). Several
  zero-shot ranking methods based on LLMs have recently been proposed.
  Among many aspects, methods differ across (1) the ranking algorithm
  they implement, e.g., pointwise vs. listwise, (2) the backbone LLMs
  used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording used in
  prompts, e.g., the use or not of role-definition (role-playing) and
  the actual words used to express this. It is currently unclear whether
  performance differences are due to the underlying ranking algorithm,
  or because of spurious factors such as better choice of words used in
  prompts. This confusion risks to undermine future research. Through
  our large-scale experimentation and analysis, we find that ranking
  algorithms do contribute to differences between methods for zero-shot
  LLM ranking. However, so do the LLM backbones -- but even more
  importantly, the choice of prompt components and wordings affect the
  ranking. In fact, in our experiments, we find that, at times, these
  latter elements have more impact on the ranker's effectiveness than
  the actual ranking algorithms, and that differences among ranking
  methods become more blurred when prompt variations are considered.
author:
- Shuoqi Sun[^1] , Shengyao Zhuang, Shuai Wang, Guido Zuccon
bibliography:
- ecir2025-prompt-variations-camera-ready.bib
title: |
  An Investigation of Prompt Variations\
  for Zero-shot LLM-based Rankers
---

# Introduction

Large Language Models (LLMs) are massively parametrised transformer
models that have undergone training on a large extent of
text [@zhao2023survey]. Generative LLMs are capable of generating text
in response to textual input (the prompt or
context) [@brown2020language]. This prompting facility has been used to
instruct LLMs to perform specific
tasks [@brown2020language; @wang2023can; @white2023prompt; @zhuang2024setwise; @fan2023recommender; @zhuang2024promptreps].

In this paper, we investigate the use of LLMs to create zero-shot
rankers
[@ma2023zero; @sun2023chatgpt; @tang2023found; @zhuang2024beyond; @qin2023large; @zhuang2024setwise].
With zero-shot, we mean ranking methods that are not specifically
trained for ranking tasks (i.e. not contrastive fine-tuning). These
rankers operate by following the instructions provided in the prompt,
which include the query and the $k$ documents that should be considered
for ranking. Using the LLM, rankers then generate an answer to comply
with the ranking instruction. Finally, either the generated answer
contains the ranking provided by the method, or the logits of the answer
are examined to infer a ranking.

Four families of zero-shot LLM rankers have been proposed:
pointwise [@zhuang2024beyond], pairwise [@qin2023large],
listwise [@ma2023zero; @sun2023chatgpt; @tang2023found], and
setwise [@zhuang2024setwise]. They differ because of the ranking
algorithm (or mechanism) implemented in the instructions described in
the prompt. For example, in pointwise the LLM is instructed to determine
the relevance of a document, while in pairwise the LLM is instructed to
determine which of two documents is more relevant. Within each family,
one or more methods have been proposed. They typically differ in the
backbone LLM, the wording of the prompts, and in how the ranking is
formed once the answer is generated by the LLM.

Recent work has shown that, once these zero-shot LLM rankers are
compared using the same backbone LLM and fixing how the ranking is
derived (i.e. generation vs. logits), setwise methods are the most
effective [@zhuang2024setwise]. Depending on the dataset, pairwise and
listwise methods are similarly effective, with pointwise methods
providing lower effectiveness overall. While they did recognise that the
use of different backbone LLMs in previous work biased the comparison
between methods, they did not identify that the actual prompts used by
the different rankers differed not just in terms of the words used to
describe the ranking algorithm, i.e. the instruction associated with how
scoring should be performed, but also in terms of "*accessory wording*"
used. For example, compare the prompt of the PRP pointwise
approach [@qin2023large] and the one of the RankGPT listwise
approach [@sun2023chatgpt] -- see
Table [\[prompt-wording-example\]](#prompt-wording-example){reference-type="ref"
reference="prompt-wording-example"}: RankGPT's prompt includes also a
"role-playing" component (in the system part of the prompt: "You are
RankGPT \[\...\]"), absent in PRP.

With this respect, then, we ask: *What is the effect of such differences
in wording used in the prompts?* And, more broadly: Are differences in
effectiveness due to the actual ranking algorithm, or they are due to
the choice of words used in the prompts? Are differences due to LLM
characteristics such as backbone and size? These are important questions
because answering them will give us an understanding of what impacts the
effectiveness of LLM rankers , and will influence how methods should be
compared in future. We explore these directions of enquiry through a
wide array of experiments, for a total of 1,248 prompts, in which we fix
the backbone LLMs of the rankers, and we vary prompts in a controlled
manner, categorising different prompt components and investigating their
effects.

# Related Work

### Sensibility of LLMs to Prompt.

Previous work has shown that LLMs are sensitive to the prompt
formulation [@kim2023better; @thomas2024large; @koopman2023dr; @kamruzzaman2024prompting; @anagnostidis2024how-susceptible; @mizrahi2024state].
For example, Kim et al. [@kim2023better] explored the effect of various
prompts and prompting technology on deploying LLMs across language
generation tasks, showing dependency between prompt and effectiveness.
Similar findings were reported by Thomas et al. [@thomas2024large] when
considering prompt variations in the context of relevance labelling.
Kamruzzaman&Kim [@kamruzzaman2024prompting] explored the relation
between prompting strategies and social biases in LLM outputs: reduced
biases were observed in more complex prompts. The most comprehensive
analysis of the effect of prompt variations has been recently provided
by Mizrahi et al. across a range of NLP tasks [@mizrahi2024state]. These
previous works, among others, have demonstrated that the structure and
subtle nuances of prompt phrasing can dramatically impact the
performance of the LLMs, across a wide range of tasks and contexts;
however such an analysis does not exist for LLM rankers -- our work is
novel as it contributes a better understanding of this phenomenon in the
specific context of LLM rankers.

::: table*
  Method                                                                                             Prompt
  -------------------------------------------------------------------------------------------------- --------

  [@qin2023large]
  Query: {query}
  Does the passage answer the query?

  [@sun2023chatgpt]
  relevancy to the query. I will provide you with num passages, each indicated by
  number identifier \[\]. Rank the passages based on their relevance to query: {query}.
  {PASSAGES}
  Search Query: {query}.
  Rank the num passages above based on their relevance to the search query. The
  passages should be listed in descending order using identifiers. The most relevant
  passages should be listed first. The output format should be \[\] \> \[\], e.g., \[1\] \> \[2\].
  Only response the ranking results, do not say any word or explain.

:::

### Prompt Optimisation and Self-Optimisers.

Strategic prompt design is not only beneficial but necessary to harness
the full capabilities of LLMs. Recent studies have taken this further by
investigating LLMs as
self-optimisers [@yang2023large; @guo2023connecting; @sabbatella2024prompt].
These models utilize their generative capabilities to iteratively refine
prompts, thereby enhancing their performance on downstream tasks. In our
work we do not explore the adaptation of self-optimisers to prompts for
LLM rankers: but our study motivates pursuing this as a future
direction.

### Zero-Shot LLM Rankers.

We next review the four families of zero-shot generative LLM-based
rankers: pointwise [@zhuang2024beyond], pairwise [@qin2023large],
listwise [@ma2023zero; @sun2023chatgpt; @tang2023found], and
setwise [@zhuang2024setwise].

In pointwise, two approaches can be followed: *generation*
[@liang2022holistic; @nogueira2020document] and
*likelihood* [@zhuang2021deep; @zhuang2021tilde]. In *generation*, LLMs
are prompted with a query-document pair and asked to generate a binary
answer ("yes"/"no") to the question of whether the document is relevant
to the query: the likelihood of generating "yes\" (extracted from the
associated token logits) decides the ranking. In *likelihood*, a query
likelihood model is involved in ranking. The LLM is prompted with the
document and asked to produce a relevant query. Then, the ranking of
documents is based on the likelihood of generating the provided query
[@sachan2022improving], which is obtained from the associated token
logits. In our experiments with pointwise we implemented the generation
approach, which is more commonly used in previous work.

Pairwise methods compare the relevance of two documents to a query, by
prompting the LLM to answer which document is more relevant to the
query. The ranking of documents is based on the relative preferences
[@qin2023large].

For listwise ranking a list of documents and a query is passed to the
LLM via the prompt. LLMs are driven to generate the document labels of
relevant documents in a certain order; the ranking then depends on this
[@ma2023zero; @sun2023chatgpt; @pradeep2023rankvicuna].

In setwise, a set of documents and the query are provided to the LLM,
which is prompted to select the most relevant document, in an iterative
way. Then sorting algorithms are used to obtain the ranking based on the
preferences to documents' stopping occurs after $k$ documents have been
selected (if more than $k$ were given as input) [@zhuang2024setwise].

In common across all these methods is their use of LLM backbones in a
zero-shot fashion, i.e. without the need for further training the LLM.

# Methodology

Different strategies for using LLMs as rankers have been proposed; they
not only differ in the ranking algorithm and backbone used, but also in
the specific wording provided to the LLMs to perform the task. Next, we
aim to collect the original prompts used in the literature, which will
serve as the foundational prompt setting for our experiments. We then
analyse the individual characteristics of each prompt, building a
taxonomy of the components used in the prompts along with a list of
instantiations used for each component across the different prompts
(Section [3.1](#taxonomy){reference-type="ref" reference="taxonomy"}).
With this information, we aim then to design experiments where we can
systematically analyse the impact of components and specific wordings
associated with components across different ranking algorithms and
backbones. We do this by assembling prompt variations
(Section [3.2](#variations){reference-type="ref"
reference="variations"}). Experiment configurations are then outlined in
Section [3.3](#experiment_settings){reference-type="ref"
reference="experiment_settings"}.

## Prompts Collection and Taxonomy of Ranking Prompt Components {#taxonomy}

We started by collecting the original prompts used in current zero-shot
LLM-based
rankers [@zhuang2024beyond; @qin2023large; @ma2023zero; @sun2023chatgpt; @tang2023found; @zhuang2024setwise].
After the original prompts were collected, we analysed the prompts to
identify high level components that are present in at least one original
prompt, along with the associated variants. We also augmented the list
of components and variants with other wordings we devised to explore
specific categories further, e.g., tone words (see below). The analysis
revealed five components:

::: table*
:::

-   Evidence (EV): these are the query and the associated passages to
    rank.

-   Task Instruction (TI): the instructions associated to the specific
    ranking strategy: these outline to the LLM the algorithmic steps to
    follow to produce a ranking. Example wordings include "which passage
    is more relevant" (pairwise) and "is this passage relevant to the
    query" (pointwise).

-   Output Type (OT): the instructions that specify the format of the
    output the LLM needs to generate. For example, for pointwise ranking
    the LLM could be instructed to generated a Yes/No or a True/False
    answer.

-   Tone Words (TW): words that express a positive, negative, or neutral
    connotation and that help express the attitude of the prompt author
    towards the ranking instruction, e.g., "please" or "you better get
    this right or you will be punished".

-   Role Playing (RP): a description of the tool implemented by the LLM,
    used to make the LLM "impersonate" that role.

Note that TI and OT largely determine the ranker family that is
implemented, while TW and RP can be generally applied to any ranker
family and EV are always present.
Table [\[table:classification\]](#table:classification){reference-type="ref"
reference="table:classification"} lists the options we consider in our
experiments for each of these components (but EV, since it is always the
same). Some options refer to variations found in an original ranking
prompt formulation; we augmented these with wordings we devised to
explore additional alternatives for some of the components.

In addition to these components, our analysis of existing ranking
prompts identified different approaches in the ordering of some of the
components within the prompts; in particular:

-   Evidence Ordering (EO): the relative ordering of the query and
    passage(s) provided to the LLM -- whether the query is given first,
    followed by the passage(s), which we label as QF, or vice versa,
    passage(s) followed by the query (labelled PF).

-   Position of Evidence (PE): instruction to specify the position of
    the evidence in the prompt -- at the beginning (B) or at the end of
    the prompt (E).

## Building Prompt Variations {#variations}

To explore the role of the identified components, their interactions,
and the effect of specific wordings used to instantiate each of the
components, we setup a large scale experiment where prompts with unique
combinations of these aspects are built.

To build prompts, we first consider the ordering options we identified,
and build a prompt template for each combination: this gives rise to
four prompt templates, shown in
Table [1](#table:matrix){reference-type="ref" reference="table:matrix"}.

::: {#table:matrix}
   EO/PE  B                         E
  ------- ------------------------- -------------------------
    QF    RP+ TI (Q) + P + TW+ OT   RP+ TW+ OT + TI (Q) + P
    PF    RP+ P + TI (Q) + TW+ OT   RP+ TW+ OT + P + TI (Q)

  : Prompt templates that combine the five components with the four
  ordering options available. Q and P denote query text passage(s) text,
  respectively.
:::

Then, for each template, we consider all possible instantiations. The
number of variations of wordings for the task instruction (TI) and
output type (OT) components differ across families of rankers -- thus
giving rise to different number of prompt instantiations across each
family. Overall, our experiments consider 1,248 prompt variations: 768
unique prompts for pointwise; 48 for pairwise; 288 for listwise; and 144
for setwise.

Finally note that we did minor modifications to the original prompts
from previous works to improve their consistency: for example, some
prompts enclosed the query in quotes, while others appended the query
after a colon; other differences included the presence of multiple line
breaks. We settled on adapting a unique format for these aspects. From
preliminary experiments, we observed that these differences in prompt
formatting resulted in non statistically significant differences in
effectiveness: e.g., [@qin2023large]'s original prompt on COVID and
Llama 3-8B backbone obtained an nDCG@10 of 0.8014, while our adjusted
prompt lead to an nDCG@10 of 0.7966.

## Experiments Settings {#experiment_settings}

To experiment with the LLM rankers and the prompt variations, we setup a
two stage ranking pipeline in line with previous work. For the first
stage, we use the BM25 implementation from Pyserini [@lin2021pyserini]
to retrieve the top 100 documents for a query (k1=0.9, b=0.4). For the
second stage, we use the LLM ranker to re-rank these 100 documents.

As LLMs backbones, we selected instruction-tuned checkpoints from the
Flan-T5 family [@chung2024scaling], Mistral 7B [@jiang2023mistral], and
Llama3 8B [@llama3modelcard]. These are popular and highly-performant
open LLMs; we excluded close-source LLMs like GPT4 in our analysis
because of the high costs associated with running our experiments on
commercial APIs.

For Flan-T5 we considered checkpoints of different sizes: Large (783M),
XL (2.85B) and XXL (11.3B). This allowed us to explore the role of
backbone size in instruction following ability and ranking. We did not
consider extending the experiments to larger sizes of the other
backbones, e.g., Llama3 70B, because of the high computational costs
associated with doing this systematically.

We used three datasets from the ir_datasets [@macavaney:sigir2021-irds]
python library: TREC Deep Learning (DL) 2019 and 2020 (43 and 48 queries
respectively), and BeIR TREC COVID (50
queries) [@voorhees2021trec; @thakur2021beir]. We performed our analysis
using nDCG@10, the primary metric across these datasets, and tested
statistical significance with a paired, two-tails t-test.

Flan-T5 models are constrained by a maximum input length of 512 tokens.
By analysing the query and passage lengths within the evaluation
datasets, we found that methodologies like setwise and listwise, which
incorporate multiple passages within a single prompt, would exceed this
limit, with listwise requiring the longest input length. To ensure
equitable comparisons across different experimental setups, we uniformly
truncated the inputs for all methods and LLM backbones to adhere to the
512-token constraint. Our determination of truncation lengths was based
on a comprehensive analysis of the datasets employed. We calculated the
length of the longest non-query-and-passage prompt instruction (pure
instruction length) alongside the distribution of query and document
lengths. Consequently, we standardized the query length to 20 words and
the document length to 80 words. This decision was driven by the need to
preserve the integrity of instructional content within the prompts,
ensuring that the loss of such information would not compromise
experimental consistency across different LLM backbones. In our
datasets, while queries do not exceed 20 words, documents can reach up
to approximately 120 words, with the majority being about 80 words.
Although this truncation might result in some loss of document
information, our focus was not on evaluating the absolute ranking
performance but on ensuring equitable conditions for each ranker across
different backbones. This strategy ensures that, even with the most
extensive prompt variations and the shortest model input capabilities,
no instructional wording is omitted.

# Results Analysis

We base our empirical analysis along six main lines of enquiry that help
us investigate the impact prompts have on LLM rankers, including what
makes ranking prompts effective, how rankers respond to different
prompts, how ranking methods truly compare at the net of variations in
prompt wording, and the impact of LLM backbones.

## Are there better prompts?

For each family of rankers, we compare the effectiveness of the original
prompts with all other prompt variations. Results are displayed in
Figure [\[fig:stability\]](#fig:stability){reference-type="ref"
reference="fig:stability"}: for each family of rankers, the star symbols
identify the original prompts, while the boxplot shows the distribution
of effectiveness across all prompt variations. We found prompts that can
achieve higher effectiveness than the original prompts across all cases
(with most differences being statistically significant), with the
exception of listwise and pairwise on COVID when the Llama 3 backbone is
used. For the pairwise approach, only prompts for the FlanT5-XXL model
on the COVID dataset show significant differences, likely due to the
lower effectiveness variation inherent in the pairwise method. We also
identify cases where the original prompt was the worst among those
considered for the specific ranking family: for example for the listwise
prompt evaluated on DL19 and DL 20 with the FlanT5-Large backbone. We
present the actual nDCG@10 scores of original prompts and the best
prompts in
Table [\[table:original-best\]](#table:original-best){reference-type="ref"
reference="table:original-best"}.

::: figure*
<figure id="figure:stability-sub-large-19">
<p><img src="figures/stability/Stability-FlanT5-large-dl19.png"
alt="image" /> <span id="figure:stability-sub-large-19"
data-label="figure:stability-sub-large-19"></span></p>
<figcaption>DL 19</figcaption>
</figure>

<figure id="figure:stability-sub-large-20">
<p><img src="figures/stability/Stability-FlanT5-large-dl20.png"
alt="image" /> <span id="figure:stability-sub-large-20"
data-label="figure:stability-sub-large-20"></span></p>
<figcaption>DL 20</figcaption>
</figure>

<figure id="figure:stability-sub-large-covid">
<p><img src="figures/stability/Stability-FlanT5-large-covid.png"
alt="image" /> <span id="figure:stability-sub-large-covid"
data-label="figure:stability-sub-large-covid"></span></p>
<figcaption>COVID</figcaption>
</figure>

<figure id="figure:stability-sub-xl-19">
<p><img src="figures/stability/Stability-FlanT5-xl-dl19.png"
alt="image" /> <span id="figure:stability-sub-xl-19"
data-label="figure:stability-sub-xl-19"></span></p>
</figure>

<figure id="figure:stability-sub-xl-20">
<p><img src="figures/stability/Stability-FlanT5-xl-dl20.png"
alt="image" /> <span id="figure:stability-sub-xl-20"
data-label="figure:stability-sub-xl-20"></span></p>
</figure>

<figure id="figure:stability-sub-xl-covid">
<p><img src="figures/stability/Stability-FlanT5-xl-covid.png"
alt="image" /> <span id="figure:stability-sub-xl-covid"
data-label="figure:stability-sub-xl-covid"></span></p>
</figure>

<figure id="figure:stability-sub-xxl-19">
<p><img src="figures/stability/Stability-FlanT5-xxl-dl19.png"
alt="image" /> <span id="figure:stability-sub-xxl-19"
data-label="figure:stability-sub-xxl-19"></span></p>
</figure>

<figure id="figure:stability-sub-xxl-20">
<p><img src="figures/stability/Stability-FlanT5-xxl-dl20.png"
alt="image" /> <span id="figure:stability-sub-xxl-20"
data-label="figure:stability-sub-xxl-20"></span></p>
</figure>

<figure id="figure:stability-sub-xxl-covid">
<p><img src="figures/stability/Stability-FlanT5-xxl-covid.png"
alt="image" /> <span id="figure:stability-sub-xxl-covid"
data-label="figure:stability-sub-xxl-covid"></span></p>
</figure>

<figure id="figure:stability-sub-Mistral-7B-dl19">
<p><img src="figures/stability/Stability-Mistral-7B-dl19.png"
alt="image" /> <span id="figure:stability-sub-Mistral-7B-dl19"
data-label="figure:stability-sub-Mistral-7B-dl19"></span></p>
</figure>

<figure id="figure:stability-sub-Mistral-7B-dl20">
<p><img src="figures/stability/Stability-Mistral-7B-dl20.png"
alt="image" /> <span id="figure:stability-sub-Mistral-7B-dl20"
data-label="figure:stability-sub-Mistral-7B-dl20"></span></p>
</figure>

<figure id="figure:stability-sub-Mistral-7B-covid">
<p><img src="figures/stability/Stability-Mistral-7B-covid.png"
alt="image" /> <span id="figure:stability-sub-Mistral-7B-covid"
data-label="figure:stability-sub-Mistral-7B-covid"></span></p>
</figure>

<figure id="figure:stability-sub-Llama3-8B-dl19">
<p><img src="figures/stability/Stability-Llama3-8B-dl19.png"
alt="image" /> <span id="figure:stability-sub-Llama3-8B-dl19"
data-label="figure:stability-sub-Llama3-8B-dl19"></span></p>
</figure>

<figure id="figure:stability-sub-Llama3-8B-dl20">
<p><img src="figures/stability/Stability-Llama3-8B-dl20.png"
alt="image" /> <span id="figure:stability-sub-Llama3-8B-dl20"
data-label="figure:stability-sub-Llama3-8B-dl20"></span></p>
</figure>

<figure id="figure:stability-sub-Llama3-8B-covid">
<p><img src="figures/stability/Stability-Llama3-8B-covid.png"
alt="image" /> <span id="figure:stability-sub-Llama3-8B-covid"
data-label="figure:stability-sub-Llama3-8B-covid"></span></p>
</figure>
:::

::::: table*
::: subtable
:::

::: subtable
:::
:::::

## What are the characteristic of the best prompts?

Table [\[tab:best-settings-of-adjusted-prompt\]](#tab:best-settings-of-adjusted-prompt){reference-type="ref"
reference="tab:best-settings-of-adjusted-prompt"} reports the optimal
prompt components for various ranking methods and datasets. We identify
notable patterns as below.

**Task Instruction and Output Type:** We analyse task instructions (TI)
and output types (OT) together due to their inherent interrelation; OT
largely depends on the ranking algorithm that the TI implements. We
analyse these separately for each ranking family because instructions
and output types vary across ranking families. We do not analyse
pairwise here as for this method there are no alternative choices for
these wordings.

*Pointwise ranking* does not show a consistent optimal choice; however,
Task Instruction #3 is most prevalent, while Output Type #2 (judging
relevance on a numerical scale) is seldom optimal.

*Listwise ranking* shows that the best task instruction varies by
dataset and LLM backbone. Output Type #2 appears in 80% of the most
effective configurations.

*Setwise ranking* studied with a single task instruction, finds Output
Type #3 (label and explain the most relevant passage) as the most common
among top-performing prompts, appearing in 53% of cases.

**Tone Words:** The inclusion of tone words does not present a clear
trend, with the percentage of prompts achieving a higher effectiveness
with no tone words or each of the five tone words distributed as
follows: 18% (no tone words), 17%, 18%, 20%, 15%, and 12%. This
indicates a relatively uniform influence of tone words choice on prompt
effectiveness. Furthermore, no consistent patterns were observed
regarding the influence of LLM backbones or datasets on the
effectiveness related to tone words. Nevertheless, including a tone word
in the prompt led to increased effectiveness in 82% of the cases,
underscoring the potential benefit of tone words in enhancing prompt
performance.

**Role Playing:** Role playing often leads to the best effectiveness for
pointwise and pairwise prompts (80% and 66% of the cases, respectively),
it has mixed effects for setwise (is present in the best prompt about
half of the times), while it is not associated with the best
effectiveness for listwise (13%). Overall, 55% of the prompts with
highest effectiveness include role playing wording. Role playing was
originally only used by one ranking method, RankGPT [@sun2023chatgpt],
highlighting the difficulty in comparing different ranking methods if
their prompt variations are not fairly explored.

**Evidence Ordering:** For pointwise ranking, presenting passage text
before query text is preferred in 86% of top-performing prompts. The
preference is less clear in other ranking types: 40% for pairwise; 40%
for listwise; 46% for setwise where the best prompts have the passage
text before the query text. Considering model backbones, Flan-T5 tends
to perform best when presented with passage text before query text (66%
of cases); while results are mixed for the other backbone models.

**Position of Evidence:** Among the best prompts, there tend to be an
overall preference for prompts that provide the evidence at the
beginning (before any other instruction): this is the case in 63% of the
best prompts, with pointwise and listwise prompts exhibiting more often
this pattern (73% and 67% respectively). Across all datasets, most best
prompts for the FlanT5-XXL backbone have evidence at the beginning. This
is also the case for Llama3-8B for DL19 and COVID and for Mistral-7B for
COVID.

**Summary.** The analysis of the prompt templates associated with the
prompts leading to the best effectiveness across all ranking families
did not highlight any specific prompt wording combination that is more
conducive of best effectiveness than others across all datasets and
backbones -- though we found that tone words and role playing are
frequent among most of the best prompts. This analysis is further
extended in Section [\[stability\]](#stability){reference-type="ref"
reference="stability"} where the stability of rankers across prompts
variations is considered.

## Which ranking method is most effective?

Before our analysis of prompt variations, the only comparison of ranking
methods across the four families within a consistent setting of datasets
and backbone was provided by Zhuang et al. [@zhuang2024setwise].
According to their results, the best performing rankers were setwise and
pairwise (depending on dataset and backbone), followed by listwise and
then pointwise, which were distinguishably worse.

Our analysis of prompt variations reveals a somewhat different picture.
Consider the top results for each ranking family across datasets and LLM
backbone
(Figure [\[fig:stability\]](#fig:stability){reference-type="ref"
reference="fig:stability"}). Similarly to previous findings, we also
observe that overall pairwise and setwise methods deliver the best
results. However, we find that pointwise can be as competitive as these
previous methods if instructed with specific prompts. This is the case
throughout all datasets and LLM backbones, with the exception of Llama
3. For Llama 3 on DL datasets, we observe that pointwise significantly
underperforms other methods. We also observe that there are instances in
which pointwise ranking can far exceed others: this is the case on DL19
when using FLanT5-Large and XL backbones (best pointwise nDCG@10
respectively 0.6918 and 0.7010, in most cases statistically
significantly outperforming the others), and on DL20 when using
Mistral-7B (best pointwise 0.6486).

:::::: table*
::: subtable
:::

::: subtable
:::

::: subtable
:::
::::::

## Are ranking methods stable?

[]{#stability label="stability"} We consider the four ranking families
independently, and study the variance of their effectiveness across all
prompt variations for that family. Results are displayed in
Figure [\[fig:stability\]](#fig:stability){reference-type="ref"
reference="fig:stability"}. We observe that pointwise methods display
the largest variability in effectiveness due to prompt variations, with
some prompt variations delivering poor effectiveness. Setwise and
pairwise instead do better, displaying lower variability: setwise
achieves this across all backbones and datasets investigated, while
pairwise displays larger variability in specific conditions, e.g., when
using Mistral, and for COVID when using Llama3.

Two key insights arise: 1) ranking algorithms are susceptible to prompt
variations and may exhibit wide-ranging performance variations, and 2)
different ranking algorithms have varying sensitivity to prompts,
indicating the potential for some to mitigate the impact of these
variations. This suggests that more comprehensive prompting
optimisation, e.g., via self-prompting, may further improve
effectiveness across all rankers.

## Does the LLM size matter?

We answer this question by focusing on the FlanT5 models only, which
come in three different sizes (first three rows of plots in
Figure [\[fig:stability\]](#fig:stability){reference-type="ref"
reference="fig:stability"}). We observe that in general larger models
deliver higher effectiveness and reduced variations across the board.
However, the pointwise family represents an exception. Improvements are
observed when passing from FlanT5-large to FlanT5-XL. However when using
FlanT5-XXL we observe both decreased effectiveness and large variance in
effectiveness across the prompt variations.

## Does the LLM backbone matter?

We compare three backbones across the results from
Figure [\[fig:stability\]](#fig:stability){reference-type="ref"
reference="fig:stability"}): FlanT5-XXL (11.3B parameters), Mistral-7B
and Llama3-8B: the last two are of comparable size, while the first has
approximately 40-60% more parameters. We observe that generally Llama3
and FlanT5 outperform Mistral-based rankers. Llama3 and FlanT5 have
overall similar effectiveness, though on COVID Llama3-based rankers
consistently outperform those with FlanT5. We also observe that
Mistral-based rankers exhibit larger variance in effectiveness due to
prompt variations than rankers based on the other backbones.

# Further Discussion and Limitations

Latency is an important factor affecting the deployment of rankers in
production. We did not consider query latency as one of the dimensions
of analysis and comparison because our focus was on varying the prompts
to measure ranking effectiveness. Zhuang et al. [@zhuang2024setwise]
provided a first insight into comparing latency among the zero-shot LLM
rankers we considered. Latency of the methods and prompts we considered
in our analysis would be similar to that performed there; however we
note that some of the prompts we consider are longer than others (e.g.,
if the role playing component is added to prompts) -- longer prompts
result in increased query latency.

Overall, our experiments considered 1,248 prompt variations. While this
is a large number, many more prompt variations could have been designed
and investigated (and the space of prompt variations is virtually
infinite). However, sensibly increasing the number of prompt variations
in our experiments would have been infeasible as it would have exceeded
our compute budget. We ran these prompts across three datasets, and
using five different LLMs with up to 11B parameters. The execution of
these experiments required a large amount of compute power; we ran
experiments across three clusters: one with Nvidia A100 GPUs, another
with Nvidia H100 GPUs, and a smaller one with Nvidia A6000 GPUs.
Overall, the experiments took in excess of 12,400 GPU-hours to execute.
Varying prompt for the pairwise ranking results in large compute
requirements compared to other methods because of the extensive number
of pairwise computations (and thus LLM inferences) required by this
method to answer a query. Although we only considered zero-shot LLM
ranking approaches and thus did not conduct any LLM training, we
recognize that our experiments may have still consumed substantial
energy, thereby contributing to CO2 emissions [@scells2022reduce] and
water consumption [@zuccon2023beyond].

In our analysis we considered instruction-tuned checkpoints from three
LLM backbones. A wider range of LLMs could have been considered,
importantly including OpenAI's GPT models such as GPT3.5 and GPT4.
GPT3.5 for example was found more effective than Llama 2 and Vicuna
based LLM rankers in a limited set of experiments in previous
work [@zhuang2024setwise]. We were however restricted to using
non-commercial models because of the high costs involved in performing
our large number of experiments with commercial APIs. For example, using
the cost estimates from [@zhuang2024setwise], executing our experiments
for pairwise, listwise and setwise across all considered prompts using
GPT3.5 alone would have costed approximately USD \$5,000.

# Conclusions

Recent works have shown the promise of using generative LLMs to
implement effective zero-shot re-rankers. Although distinctions among
various methods primarily arise from the underlying ranking algorithms,
further differences emerge based on the characteristics of the prompts
employed to implement these algorithms. These characteristics are not
associated with the actual ranking algorithm: e.g., they may be wordings
related to a role-playing strategy, or tone words. In this paper, we
analysed these prompts and mapped prompt wordings into a set of
components to form prompt templates that allowed us to better understand
the content of these prompts. We then performed a systematic analysis of
prompt variations across different types of zero-shot LLM rankers. Our
analysis revealed that ranking effectiveness varies considerably across
different implementations of prompt components. Optimal prompt wording
showed variability depending on the ranking method, dataset, and LLM
backbone employed, suggesting that automatic prompt optimization,
tailored to specific ranking methods and datasets, may be more effective
than manual prompt engineering for optimizing ranking performance. We
make code, runs and analysis available at
<https://github.com/ielab/zeroshot-rankers-prompt-variations>.

### Acknowledgment.

Shuoqi Sun is partially supported by the Australian Research Council
through the ARC Centre of Excellence for Automated Decision-Making and
Society (ADM+S, CE200100005).

[^1]: This work was conducted while Shuoqi Sun was a student at The
    University of Queensland.
