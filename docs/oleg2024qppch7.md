# Evaluating the Effectiveness of Entropy as a QPP Method

## 7.1 Introduction

This chapter presents an efficient post-retrieval QPP method specifically designed for NIR models. The primary goal is to demonstrate the benefits of the proposed sARE evaluation framework in practice by applying it to evaluate the performance of the new scores-based QPP method. Through this evaluation, we aim to highlight the unique features of the method and identify significant differences in effectiveness compared to other existing scores-based QPP approaches. The ultimate objective is to showcase the capability of the new entropy-based QPP method in predicting the performance of NIR models.

While existing QPP methods have shown promise with traditional IR models, their effectiveness often diminishes when applied to NIR models [86]. To address this limitation, current QPP research focuses on developing new methods that can accurately predict the performance of NIR models. However, many of these methods rely on pre-trained language models and demand significant computational resources for prediction generation.

In this chapter, we propose a new QPP method based on the concept of entropy, which measures uncertainty or disorder within a system according to information theory. By leveraging the entropy of retrieval scores, the proposed method predicts the effectiveness of retrieval results generated by NIR models. It is straightforward, efficient, unsupervised, and model-agnostic approach that can be applied to any IR ranking model. The simplicity and computational efficiency of the method make it suitable for practical implementation in real-world scenarios.

This chapter's aim: To evaluate the effectiveness of the entropy-based QPP method in predicting the performance of NIR models using a distribution-based framework and detailed statistical analysis to compare its advantages and limitations to similar existing methods. To guide our evaluation, we formulate the following research questions:

RQ5.1: How does the entropy-based QPP method compare to existing QPP approaches in terms of effectiveness in predicting the performance of NIR models?

RQ5.2: What are the advantages and limitations of the entropy-based QPP method compared to similar existing methods?
Addressing these research questions will allow us to thoroughly evaluate the performance, highlight the unique features, and assess the similarities and differences of the entropy-based QPP method compared to an existing well-known QPP approach. By conducting a comprehensive evaluation with the sARE framework, this chapter contributes to the advancement of QPP techniques, particularly in the context of NIR models. The results will provide valuable insights, demonstrate the potential of score-based QPP methods within NIR models, and encourage further research and adoption by practitioners and researchers.

## 7.2 Predicting Performance of Neural IR Models

In recent years, the prediction of performance for NIR models has been gaining increasing attention. Researchers have been exploring various methods to estimate the effectiveness of these models, particularly in the context of QPP. In this section, we review prior work in this field and provide the foundation for our proposed approach.

A study conducted by Faggioli et al. [86] investigated the performance of QPP methods on NIR systems, specifically those based on pre-trained LLMs. The findings revealed that existing QPP methods perform poorly when applied to NIR systems. In comparison to traditional retrieval systems, QPP methods exhibited a statistically significant decrease in performance on neural models. Notably, in scenarios where semantic signals play a prominent role, such as passage retrieval, the performance of QPP methods dropped by up to $10 \\%$ when compared to the bag-ofwords approach. Furthermore, existing QPP methods struggled to predict performance accurately for neural models, particularly on queries where they deviated the most from traditional retrieval approaches. The study also observed a correlation between QPP performance and the ability of neural architectures to handle lexical matches. Based on these findings, the authors concluded that current QPP methods are not well-suited for NIR systems, emphasizing the need for new models that account for semantic signals.

Zendel et al. [240] introduced a framework that incorporates additional reference queries to enhance the estimation of retrieval result effectiveness. The framework utilizes a linear combination of predictions obtained from reference queries. This approach was further extended by Datta et al. [71], who proposed using a ratio instead of a linear combination, resulting in improved performance, especially for neural re-ranking models. Both approaches aim to enhance QPP estimators by incorporating information from additional reference queries and rely on existing QPP predictors. In the following research, we propose a new method that can be used as a base predictor within these frameworks to further enhance the effectiveness prediction of neural reranking models.

Singh et al. [200] proposed a novel QPP method called QPP-PRP, which leverages pairwise rank preferences obtained from an auxiliary model called DuoT5 to estimate the retrieval quality of NIR models. QPP-PRP calculates the QPP score as the odds ratio of the log-probabilities of observing documents ranked better and worse than a pivot document, based on the pairwise

probabilities from DuoT5. Experimental evaluations conducted on the MS-MARCO passage ranking corpus demonstrated that QPP-PRP outperformed two baseline methods, NQC and $\\operatorname{UEF}(\\mathrm{NQC})$. While not explicitly stated in the paper, the NQC method is often adapted to different models by removing the query length and corpus normalization factors and using the SD of the RSVs as the QPP score instead, as it was suggested by the authors in the original work [198]. In the subsequent chapter, we use the SD method as a baseline and propose a novel method that does not rely on a reference model for effectiveness estimation.

Both NQC and SD are score-based QPP methods, as discussed in Chapter 2 (Subsection 2.4.2). Score-based predictors utilize the RSVs generated by the retrieval system to estimate the success of the search. NQC and SD are among the most widely used score-based predictors due to their simplicity and low computational cost. These methods are particularly appealing for real-world applications, as they do not require access to collection statistics, which can be computationally expensive. Moreover, they can be seamlessly integrated into existing retrieval systems, enabling real-time predictions. The NQC method measures the SD of the retrieval scores while incorporating additional corpus and query length normalization. It posits that a low dispersion of RSVs indicates query drift, wherein irrelevant documents dominate the retrieved results. While SD-based QPP methods have shown promise, their effectiveness relies on the number of documents used for estimation. Inspired by this hypothesis, our approach centers on using entropy as a measure of dispersion and centrality. Shtok et al. [198] briefly introduced an additional QPP method to measure the dispersion of the scores; the authors proposed measuring the KL divergence between the RSVs and the mean score, which under certain conditions is rank-equivalent to measuring entropy. Our empirical analysis demonstrates that entropy remains robust to the number of included documents, and its performance shows improvement up to a certain $k$, followed by consistent results as the number of top- $k$ documents increases.

## 7.3 Methodology and Experimental Setup

In this section, we present the methodology and experimental setup used to examine the empirical analysis of the proposed QPP method, aiming to validate its effectiveness. To accomplish this, we provide a comprehensive overview of the experimental setup in Subsection 7.3.1. This includes the datasets, the retrieval models, and the evaluation measures used. Next, we describe the scorebased QPP methods used as baselines and how our proposed entropy-based QPP method relates to them. Setting up the stage to present the results of our experiments in Section 7.4. Lastly, in Subsection 7.5, we engage in a discussion, delving into the implications and insights derived from the observed results.

Table 7.1: Average retrieval effectiveness scores with standard deviations in parenthesis measured using AP and nDCG@10.

|  |  | AP | nDCG@10 |
| :-- | :-- | :-- | :-- |
| Robust04 | GDMTL | $0.284(0.207)$ | $0.511(0.288)$ |
|  | QL | $0.248(0.208)$ | $0.437(0.297)$ |
| TREC-DL20 | GDMTL | $0.528(0.240)$ | $0.754(0.196)$ |
|  | QL | $0.371(0.247)$ | $0.491(0.261)$ |

### 7.3.1 Experimental Setup

To evaluate the quality of our proposed QPP method, we conducted experiments using two probabilistic retrieval models: Query Likelihood (QL) with Dirichlet smoothing [126] ${ }^{1}$ and GDMTL, a state-of-the-art Neural IR (NIR) model. Both these models estimate the probability $\\mathrm{P}(d \\mid q)$ of document $d$ being relevant to query $q$. The GDMTL model is a multi-task learning approach that combines discriminative and generative tasks, such as query generation, to enhance document ranking performance. It was trained by fine-tuning a Transformer-based LM on the MS-MARCO passage ranking task and has demonstrated its generalization capabilities across various tasks and datasets, including document reranking [66]. The arrangement of documents by their QL scores aligns with the Probability Ranking Principle (PRP), which dictates ranking documents based on their probability of relevance. Within this framework, models assign a probability of relevance to each document independently, optimizing ranking to maximize user utility. This principle also extends to the GDMTL scores.

Our experiments were conducted on two ad hoc retrieval TREC collections: the Robust04 [223] documents retrieval collection and the TREC-DL20 [61] passage retrieval collection. The RoBUSTO4 collection consists of 249 judged topics, while the TREC-DL20 collection has 54 judged topics. For both collections, we used the topic titles as the queries for retrieval. We calculated the AP and nDCG@10 measures to evaluate the retrieval effectiveness. Originally AP served as one of the four evaluation measures for the Robust04 collection, and it is a commonly used metric for evaluating retrieval effectiveness in prior work on QPP; and nDCG@10 was used in the TREC-DL20 passage retrieval task. The average retrieval effectiveness scores obtained from the GDMTL and QL models for both collections are provided in Table 7.1.

To comprehensively evaluate the performance of the various QPP methods, we utilize well-established correlation measures, including Pearson's $r$ and Kendall's $\\tau$. Additionally, we incorporate novel metrics, sARE and sMARE, detailed in Chapter 6, as well as pairwise accuracy introduced in Chapter 5. To optimize the parameter $k$, which signifies the number of top documents, we employ a robust approach. This involves repeated two-fold cross-validation with 30 repetitions. In each repetition, we alternate between using the folds as training and test sets, ensuring that each fold serves as the test set once. The training set is used to fine-tune the parameter

[^0]
[^0]:    ${ }^{1}$ Implemented using the Lemur-Indri toolkit (Version 5.14).

$k$ for a specific evaluation measure, while the test set is dedicated to evaluating the performance of the QPP methods. Subsequently, we calculate the evaluation measure as the average across the test folds, and the final reported value represents the average performance across all 30 repetitions. By employing this diverse array of measures and a rigorous evaluation methodology, we can comprehensively assess the performance and effectiveness of our proposed QPP method.

### 7.3.2 Score-based Query Performance Prediction

Score-based QPP methods are rooted in the idea that the scores assigned to the top $k$ documents can provide valuable insights into the success of a retrieval. This assumption rests on the belief that there is a noticeable difference in the distribution of RSVs between successful and unsuccessful retrievals. To illustrate this concept, Figure 7.1 showcases the observed distinctions in score distributions between the best and worst results. Each plot in the figure depicts the distribution of the top 500 RSVs for queries ranked at the top and bottom, as evaluated by AP and nDCG@10, using the GDMTL retrieval model. The visual representation underscores that the RSV distributions of superior queries differ from those of poorer queries, lending anecdotal support to the assumptions underpinning score-based QPP methods. Additionally, we present AP and nDCG@10 scores for both query categories, along with SD and entropy values for the RSV distributions. The detailed computation of SD and entropy is provided in Subsection 7.3.3. It is worth noting that while the direction of SD and entropy values is reversed, they consistently align with the order of AP and nDCG@10 scores in all subfigures, except for Subfigure 7.1d, where the SD value is higher for the unsuccessful query.

Given the extensive and well-established use of SD as a score-based QPP method, we have chosen it as the primary baseline for our comparative analysis. Another commonly employed score-based QPP technique is WIG, which we also incorporate into our evaluations by adapting it to calculate the mean score of the top $k$ documents. Our proposed method, which forms the foundation of our approach, can be viewed as a variation of the KL divergence method initially mentioned by Shtok et al. [198]. The KL divergence was computed between the score distribution of the top $k$ documents, utilizing sum normalization and a uniform distribution. The authors proposed the KL divergence method as a potential alternative to the NQC method. While they demonstrated that the KL divergence method was competitive with NQC, it has largely remained overlooked. Our proposed method can be characterized as a generalization of the KL method, both of which are based on the entropy of the scores, although our method uses softmax normalization. The primary difference between our proposed method and the KL divergence lies in the normalization approach. It is important to note that the original KL divergence method had to be further adapted to accommodate mixed scores (both positive and negative). Our adaptation of the KL divergence method involves subtracting the minimum score from all scores before dividing by the sum, ensuring that all scores are non-negative. This normalization approach applies shift and scale transformations to the scores, a technique previously proposed for document fusion [154].

Moreover, it is worth mentioning that the Indri scores used are logarithmic QL scores, ${ }^{2}$ which makes softmax normalization more suitable. The exponent used in softmax normalization reverses the logarithmic scores back into the original QL probabilities. Therefore, applying the softmax function to the logarithmic scores is equivalent to applying sum normalization to QL probabilities. While the PRP assumes that the probability of relevance for each document is independent of other documents, in practice, this is often not the case as highly similar documents tend to be redundant. Consequently, by normalizing the scores to sum to one, we effectively assume that the probability of relevance depends on other documents, a reasonable assumption often made when evaluating retrieval effectiveness using measures that aim to capture result diversity.

In addition to these baselines, we have included kurtosis and skewness as potential QPP methods. These statistical measures help us understand the shape of the score distribution. Kurtosis assesses the \"tailedness\" of the distribution, while skewness evaluates its \"asymmetry.\" Both measures are commonly used in statistics to characterize distribution shapes.

### 7.3.3 ENTROPY AS QPP

Entropy-based QPP is a score-based method that utilizes the entropy of the scores of the top $k$ documents, normalized using the softmax function. Entropy serves as a measure of uncertainty or information content within a probability distribution. In our specific context, it quantifies the uncertainty in the relevance probabilities of documents within the list $\\mathrm{D}_{q}$ for a given query $q$. The entropy $\\mathrm{H}(q)$ for a query $q$ is mathematically defined as follows:

$$
\\mathrm{H}(q)=-\\sum_{d \\in \\mathrm{D}_{q}} \\mathrm{P}(d \\mid q) \\log \\mathrm{P}(d \\mid q)
$$

where $\\mathrm{P}(d \\mid q)$ represents the probability of document $d$ being relevant to query $q$. To transform the raw retrieval scores $\\mathrm{S}(q, d)$ into probabilities, we apply the softmax function:

$$
\\mathrm{P}(d \\mid q)=\\frac{\\exp (\\mathrm{S}(q, d))}{\\sum_{d^{\\prime} \\in \\mathrm{D}_{q}} \\exp \\left(\\mathrm{~S}\\left(q, d^{\\prime}\\right)\\right)}
$$

The softmax normalization ensures that all values are non-negative and sum to one, adhering to the characteristics of a probability distribution. It is crucial to note that entropy is maximized when all the probabilities in the distribution are equal. This implies a uniform distribution of scores, signifying that the model is uncertain and incapable of distinguishing between different documents. In such cases, all documents are assigned the same probability of relevance, indicating a failure of the retrieval model to discriminate between relevant and non-relevant documents effectively. Conversely, lower entropy corresponds to a more focused and successful retrieval

[^0]
[^0]:    ${ }^{2}$ This detail pertains to the original Indri framework implementation and all following versions. The lemur project: https://sourceforge.net/p/lemur/code/HEAD/tree/indri/branches/indri-pre-concurrency/ include/indri/DirichletTermScoreFunction.hpp. Accessed: 2024-02-07.

![img-65.jpeg](img-65.jpeg)
(a) Robust04
Successful query (677):
$\\mathrm{AP}=0.91, \\mathrm{SD}=1.22$, Entropy $=3.06$;
Unsuccessful query (314):
$\\mathrm{AP}=0.0, \\mathrm{SD}=0.71$, Entropy $=5.87$.
(b) Robust04
Successful query (311):
nDCG@10=1.0, SD=1.46, Entropy $=4.93$;
Unsuccessful query (309):
nDCG@10=0.0, SD=0.79, Entropy $=5.81$.
![img-66.jpeg](img-66.jpeg)
(c) TREC-DL20
Successful query (324585):
$\\mathrm{AP}=0.94, \\mathrm{SD}=1.94$, Entropy $=2.03$;
Unsuccessful query (118440):
$\\mathrm{AP}=0.04, \\mathrm{SD}=1.93$, Entropy $=2.04$.
(d) TREC-DL20
Successful query (1113256):
nDCG@10=1.0, SD=1.39, Entropy $=2.38$;
Unsuccessful query (673670):
nDCG@10=0.0, SD=2.77, Entropy $=4.08$.

Figure 7.1: Histogram plots with KDE of the GDMTL model RSV distributions for the top 500 documents comparing successful and unsuccessful results.
outcome. For our experiments, we adopt negative entropy, denoted as $-\\mathrm{H}(q)$, as our QPP method for the sake of convenience. This metric's higher values (closer to zero) indicate better retrieval performance.

To provide a concrete understanding of the probability $\\mathrm{P}(d \\mid q)$, let us consider the QL model. Within this model, $\\mathrm{P}(d \\mid q)$ represents the likelihood that a document pertains to the same topic as the query. In the QL model, $\\mathrm{P}(d \\mid q)$ is estimated through the interplay of the language model of the query and that of the document. The relationship can be expressed as:

$$
\\mathrm{P}(d \\mid q) \\stackrel{\\text { rank }}{=} \\mathrm{P}(q \\mid d) \\mathrm{P}(d)
$$

Here, $\\mathrm{P}(q \\mid d)$ denotes the probability that the query could be generated based on the document's language model, while $\\mathrm{P}(d)$ represents the document's probability within the collection. In practice, $\\mathrm{P}(d)$ is often assumed to be uniform and thus disregarded. Consequently, the probability $\\mathrm{P}(d \\mid q)$ is proportional to $\\mathrm{P}(q \\mid d)$, which aligns with the document's ranking score.

For the sake of comparison with baseline methods, we also compute several other QPP methods. These include the KL divergence method proposed by Shtok et al. [198], which is rank equivalent to our entropy-based approach. The KL divergence is defined as:

$$
\\mathrm{KL}(\\mathrm{P} \\| \\mathrm{U})=\\sum_{d \\in \\mathrm{D}_{q}} \\mathrm{P}(d \\mid q) \\log \\frac{\\mathrm{P}(d \\mid q)}{\\mathrm{U}(d \\mid q)}
$$

where $\\mathrm{U}(d \\mid q)$ represents the uniform distribution and can be replaced with $1 / k$, where $k$ corresponds to the number of documents (scores) in $\\mathrm{D}_{q}$ :

$$
\\begin{aligned}
\\mathrm{KL}(\\mathrm{P} \\| \\mathrm{U}) & =\\sum_{d \\in \\mathrm{D}_{q}} \\mathrm{P}(d \\mid q) \\log \\frac{\\mathrm{P}(d \\mid q)}{1 / k} \\\\
& =\\sum_{d \\in \\mathrm{D}_{q}} \\mathrm{P}(d \\mid q)\\left(\\log \\mathrm{P}(d \\mid q)-\\log \\frac{1}{k}\\right) \\\\
& =\\sum_{d \\in \\mathrm{D}_{q}} \\mathrm{P}(d \\mid q)(\\log \\mathrm{P}(d \\mid q)+\\log k) \\\\
& =\\sum_{d \\in \\mathrm{D}_{q}} \\mathrm{P}(d \\mid q) \\log \\mathrm{P}(d \\mid q)+\\sum_{d \\in \\mathrm{D}_{q}} \\mathrm{P}(d \\mid q) \\log k
\\end{aligned}
$$

Given that $\\sum_{d \\in \\mathrm{D}_{q}} \\mathrm{P}(d \\mid q)=1$ by definition, we can simplify the equation to:

$$
\\mathrm{KL}(\\mathrm{P} \\| \\mathrm{U})=\\log k+\\sum_{d \\in \\mathrm{D}_{q}} \\mathrm{P}(d \\mid q) \\log \\mathrm{P}(d \\mid q)
$$

The first term, $\\log k$, is a constant and can be disregarded for ranking purposes. The second term corresponds to the negative entropy of the scores, which aligns with our proposed method in Equation 7.1. Therefore, when the value of $k$ is fixed, the KL divergence method and our entropy-based approach yield the same ranking results. However, in practice, some queries may have fewer than $k$ documents retrieved for them. In such cases, the two methods differ in their ranking. Furthermore, these methods vary in the way they normalize scores: the KL divergence method employs sum normalization, whereas our approach utilizes softmax normalization. This nuanced difference in score normalization impacts their ranking performance, making it crucial to consider these distinctions when choosing a QPP method for specific scenarios.

We also employ the standard deviation (SD) and the mean of the scores as QPP methods. The standard deviation $(\\sigma)$ is calculated as follows:

$$
\\sigma=\\sqrt{\\frac{\\sum_{d \\in D_{q}}(\\mathrm{~S}(q, d)-\\mu)^{2}}{\\left|\\mathrm{D}_{q}\\right|}}
$$

where $\\mu$ is computed as the average of the raw retrieval scores over the list $\\mathrm{D}_{q}$. While all four QPP methods focus on the distribution of retrieval scores, each captures a different aspect of that distribution. Our proposed entropy-based QPP method measures the uncertainty in relevance probabilities, providing an estimate of the retrieval model's performance. By comparing it with standard deviation and mean-based approaches, we can assess its effectiveness and relative advantages and disadvantages. Finally, we include kurtosis and skewness as potential QPP methods, both calculated using the implementation provided by SciPy (Version 1.10.1) [221].

## 7.4 Experimental Results

In this section, we present the results of the empirical evaluation of our proposed QPP method Entropy and compare it to the baseline methods $K L, S D$, Mean, Kurtosis, and Skewness. The evaluation focuses on prediction quality, robustness, and similarity.

### 7.4.1 Prediction Quality

Tables 7.2 and 7.3 provide a comparison of prediction quality for the various QPP methods, with AP and nDCG@10 serving as indicators of retrieval effectiveness. Lower values of the sMARE error measure indicate superior performance, while higher values of correlation coefficients (Pearson's $r$ and Kendall's $\\tau$ ) imply better performance. These tables display mean prediction quality across all queries and incorporate a two-fold cross-validation procedure with 30 repetitions, ensuring a fair comparison with hyperparameter tuning for each method to achieve optimal performance.

For the document retrieval task with the ROBUSTO4 collection, the Entropy method demonstrated prediction quality comparable to SD, surpassing the other QPP methods. When applied to the GDMTL retrieval model, Entropy slightly outperformed SD, while both methods were similarly effective for the QL model. This trend was more pronounced with AP as the evaluation metric and less pronounced with nDCG@10. For the passage retrieval task with the TRECDL20 collection, the Mean method exhibited better performance than SD and Entropy for the GDMTL retrieval model, while SD and Entropy performed better for the QL model. In general, the Entropy method performed comparably to SD for both retrieval models, with only minor differences. Interestingly, the overall trends observed for AP and nDCG@10 were consistent, with the QPP methods showing better performance with AP.

Table 7.2: Comparison of prediction quality for Mean, Skewness, Kurtosis, KL, SD, and Entropy as QPP methods. Retrieval effectiveness is evaluated using AP. Darker cell color represents better prediction quality. Lower values indicate better performance for the sMARE error measure.

|  |  | GDMTL |  |  | QL |  |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  |  | sMARE | P-r | K- $\\tau$ | sMARE | P-r | K- $\\tau$ |
| Robust04 | Mean | 0.272 | 0.282 | 0.212 | 0.310 | 0.023 | 0.076 |
|  | Skew | 0.264 | 0.352 | 0.239 | 0.285 | 0.211 | 0.150 |
|  | Kurtosis | 0.262 | 0.326 | 0.215 | 0.286 | 0.153 | 0.150 |
|  | KL | 0.286 | 0.193 | 0.160 | 0.281 | 0.204 | 0.151 |
|  | SD | 0.219 | 0.520 | 0.356 | 0.223 | 0.475 | 0.344 |
|  | Entropy | 0.215 | 0.527 | 0.383 | 0.223 | 0.387 | 0.345 |
| TREC-DL20 | Mean | 0.301 | 0.235 | 0.184 | 0.308 | 0.088 | 0.056 |
|  | Skew | 0.302 | 0.203 | 0.123 | 0.299 | 0.266 | 0.101 |
|  | Kurtosis | 0.317 | 0.061 | 0.063 | 0.300 | 0.182 | 0.052 |
|  | KL | 0.313 | 0.117 | 0.098 | 0.231 | 0.338 | 0.086 |
|  | SD | 0.294 | 0.233 | 0.158 | 0.203 | 0.617 | 0.422 |
|  | Entropy | 0.301 | 0.216 | 0.142 | 0.218 | 0.390 | 0.374 |

It is worth noting that the QPP methods struggled to provide accurate predictions for the GDMTL model in the passage retrieval task, particularly when measured with nDCG@10. This observation aligns with recent findings by Faggioli et al. [86] and emphasizes the need for further improvement of QPP methods for NIR models on datasets they were originally trained on.

In addition to assessing mean prediction quality across automatically selected $k$ values, we conducted a comprehensive evaluation of the QPP methods using their respective optimal hyperparameter values for $k$. This rigorous analysis covered both collections, Robust04 and TREC-DL20, and relied on the sMARE error measure for AP to determine the optimal $k$ values. Notably, we utilized only the sMARE-AP metric for setting the optimal $k$ values due to its computational efficiency compared to the more resource-intensive pairwise accuracy. ${ }^{3}$ For the assessment of prediction quality, we employed two evaluation metrics: pairwise accuracy, as introduced in Chapter 5, and the sMARE-AP metric. Our evaluation comprised two main parts. Firstly, we aimed to discern statistically significant differences among the QPP methods. Secondly, we investigated the sensitivity of these methods to variations in performance across queries. To evaluate statistical significance, we employed Tukey's HSD test with a significance level (FWER) set at 0.05 .

In Figure 7.2, we present the initial analysis, which aims to assess the statistical significance of differences between the QPP methods. The results for both the Robust04 and TREC-DL20 collections are displayed in the left and right columns, respectively. Within each collection, the

[^0]
[^0]:    ${ }^{3}$ Calculating pairwise accuracy is significantly more computationally expensive than sMARE-AP for determining optimal $k$ values and unlikely to yield different results.

Table 7.3: Comparison of prediction quality for Mean, Skewness, Kurtosis, KL, SD, and Entropy as QPP methods. Retrieval effectiveness is evaluated using nDCG@10. Darker cell color represents better prediction quality. Lower values indicate better performance for the sMARE error measure.

|  |  | GDMTL |  |  | QL |  |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  |  | sMARE | P-r | K- $\\tau$ | sMARE | P-r | K- $\\tau$ |
| Robust04 | Mean | 0.290 | 0.213 | 0.149 | 0.311 | 0.064 | 0.076 |
|  | Skew | 0.279 | 0.238 | 0.179 | 0.281 | 0.176 | 0.168 |
|  | Kurtosis | 0.271 | 0.310 | 0.205 | 0.278 | 0.113 | 0.171 |
|  | KL | 0.296 | 0.171 | 0.120 | 0.281 | 0.194 | 0.176 |
|  | SD | 0.242 | 0.431 | 0.281 | 0.236 | 0.379 | 0.304 |
|  | Entropy | 0.238 | 0.393 | 0.302 | 0.236 | 0.251 | 0.303 |
| TREC-DL20 | Mean | 0.240 | 0.226 | 0.211 | 0.289 | 0.145 | 0.139 |
|  | Skew | 0.338 | 0.119 | 0.035 | 0.314 | 0.261 | 0.144 |
|  | Kurtosis | 0.338 | 0.135 | 0.025 | 0.316 | 0.201 | 0.133 |
|  | KL | 0.334 | 0.104 | 0.062 | 0.306 | 0.224 | 0.162 |
|  | SD | 0.345 | 0.076 | $-0.004$ | 0.238 | 0.412 | 0.294 |
|  | Entropy | 0.338 | 0.065 | 0.004 | 0.264 | 0.266 | 0.255 |

top row showcases results for the sMARE-AP metric, while the bottom row presents results for pairwise accuracy. The QPP methods are arranged in order of their mean prediction quality across all queries, with the top-performing method on the left and the least effective on the right. The figures visually depict the Tukey's HSD CI for each method, where non-overlapping CIs signify statistically significant differences. Notably, our findings consistently reveal that the Mean method performed significantly worse than other methods, whereas the Entropy and SD methods displayed superiority, with statistically significant differences emerging only when measured using pairwise accuracy for the TREC-DL20 collection (Figure 7.2d).

Furthermore, the analysis employing Tukey's HSD test yielded valuable insights into the characteristics of the evaluation metrics and their sensitivity. Across all four plots, the CIs constructed for sMARE-AP were consistently wider when compared to those constructed for pairwise accuracy. This observation suggests that the pairwise accuracy metric exhibits greater sensitivity to performance differences. The variance in CIs width can be attributed to the number of samples generated by each metric. While sMARE-AP is calculated on a per-query basis, pairwise accuracy is computed for every pair of queries, resulting in a substantially larger sample size. For example, in the case of the Robust04 and TREC-DL20 collections, sMARE-AP is based on 249 and 54 samples, whereas pairwise accuracy relies on a substantial 30,876 and 1,431 samples. This noteworthy difference in sample size accounts for the broader CIs observed for sMARE-AP in comparison to pairwise accuracy.

![img-67.jpeg](img-67.jpeg)
(a) Tukey's HSD CIs for the ROBUSTO4 Collection with prediction quality measured by sMARE-AP (lower is better).
![img-68.jpeg](img-68.jpeg)
(c) Tukey's HSD CIs for the ROBUSTO4 Collection with prediction quality measured by pairwise accuracy (higher is better).
(b) Tukey's HSD CIs for the TREC-DL20 Collection with prediction quality measured by sMAREAP (lower is better).
![img-69.jpeg](img-69.jpeg)
(d) Tukey's HSD CIs for the TREC-DL20 Collection with prediction quality measured by pairwise accuracy (higher is better).

Figure 7.2: The figures present a comparative analysis of the QPP methods using Tukey's HSD CIs. Evaluation encompasses both GDMTL and QL models, which exhibited consistent trends. Retrieval effectiveness is assessed by AP, with results presented for the ROBUSTO4 and TRECDL20 collections. Nonoverlapping CIs indicate statistically significant differences with a FWER of 0.05 .

In the second phase of our analysis, we examine the sensitivity of the QPP methods regarding the variation in AP across queries. To investigate this, we categorized pairs of queries into buckets based on their AP difference and computed the pairwise accuracy for each bucket. The results are presented in Figure 7.3. On the x-axis, we represent the AP difference between query pairs, while the $y$-axis displays the average pairwise accuracy for all pairs with the same AP difference. The error bars depict the $95 \\%$ confidence intervals for the average pairwise accuracy computed

using 5,000 bootstrap samples. For the Robust04 collection (Figure 7.3a), we divided the query pairs into ten equally sized buckets. However, for the TREC-DL20 collection (Figure 7.3b), we employed five equally sized buckets, adjusting for the smaller number of queries in this collection. Despite these differences, the trends observed in both collections remain consistent.

Notably, the Entropy and SD methods exhibit the most significant trends, with their performance increasing as the AP difference between query pairs grows. Conversely, the Mean method once again demonstrates only a subtle performance increase as the AP difference expands, with the other three QPP methods falling in between. This analysis underscores that the Entropy and SD methods are more responsive to variations in AP between queries, whereas the Mean method exhibits lower sensitivity. These findings align with those presented in Figure 7.2.

### 7.4.2 Robustness - Hyperparameter Analysis

All the discussed QPP methods rely on a single hyperparameter, denoted as $k$, which determines the number of top-ranked documents used in the computation. Figure 7.4 provides a comprehensive comparison of the QPP methods across a range of $k$ values [5, 10, 20, ..., 1000]. We measure the prediction quality using sMARE for both retrieval models on both the Robust04 and TREC-DL20 collections, utilizing AP to evaluate effectiveness. The results reveal that the Entropy method exhibits more robust performance concerning variations in the hyperparameter $k$ compared to SD. Specifically, for the GDMTL model on the Robust04 collection, as the value of $k$ increases, the prediction quality of the Entropy method remains relatively stable, reaching a plateau around $k \\approx 400$. In contrast, for the QL model, both the Entropy and SD methods display similar trends, with the Entropy method achieving its best performance across a wider range of $k$ values than SD.

The SD method achieves its minimal error using a narrower range of $k$, a range that varies depending on the specific collection and retrieval model under evaluation, as previously demonstrated by Shtok et al. [198]. In contrast, the Mean method exhibits the poorest performance, with error rates increasing as $k$ grows, reaching its lowest error for $k=5$. This analysis demonstrates an advantage of using the Entropy method, which provides consistent and robust predictions across a broader range of $k$ values, making it less sensitive to hyperparameter variations and, consequently, easier to fine-tune. Such robustness holds practical significance, reducing the need for extensive hyperparameter tuning and enhancing the overall utility of the Entropy method as a QPP approach.

### 7.4.3 Similarity Between Entropy and SD

We conducted an in-depth investigation into the similarity between the two best-performing QPP methods, namely, the Entropy and the $S D$ method. Our analysis focused on their predictions across various queries, using the sARE-AP error measure, as discussed in Chapter 6. This measure quantifies the absolute difference between a QPP method's prediction and the ground truth

![img-70.jpeg](img-70.jpeg)
(a) Pairwise accuracy across buckets of AP difference for the Robust04 collection.
![img-71.jpeg](img-71.jpeg)
(b) Pairwise accuracy across buckets of AP difference for the TREC-DL20 collection.

Figure 7.3: A comparative analysis of the QPP methods across buckets of AP difference. Evaluation encompasses both GDMTL and QL models. Retrieval effectiveness is assessed by AP, with results presented for the Robust04 and TREC-DL20 collections. The error bars represent the $95 \\%$ CI for the average pairwise accuracy, using 5,000 bootstrap samples.
relevance score for each query, with the ground truth represented by the query's AP score. The sARE-AP values provide insights into the errors of the QPP methods on a per-query basis, allowing us to assess the similarity of their predictions by comparing these values.

Table 7.4 presents the Pearson's $r$ correlation coefficients for the sARE-AP values between the Entropy and SD methods across different scenarios, including the GDMTL and QL models on both the Robust04 and TREC-DL20 collections. These coefficients provide insights into the degree of similarity between the two methods' predictions.

![img-72.jpeg](img-72.jpeg)

Figure 7.4: Comparison of the QPP approaches, over a range of $k$ values $[5,10,20, \\ldots, 1000]$. The $k$ parameter represents the number of top-ranked document scores used by each method. Prediction quality is measured using the sMARE-AP metric (lower values indicate better performance).

Our findings reveal a significant correlation between the methods, with correlation coefficients ranging from $r=0.53$ to $r=0.92$. Specifically, for the Robust04 collection, strong correlations were observed with $r=0.83$ and $r=0.90$, while for the TREC-DL20 collection, the correlations were also substantial, ranging from $r=0.53$ to $r=0.92$. The only exception was the case of the GDMTL model on TREC-DL20, where the correlation was lower. This deviation can be attributed to the lower prediction quality in that case, as indicated in Table 7.2. In situations where methods exhibit lower prediction quality, they may manifest a broader range of per-query sARE values, in line with the \"Anna Karenina principle [57].\" This principle posits that deviating further from the optimal ranking allows for more diverse errors. Consequently, this phenomenon can lead to a lower correlation between the two methods. Interestingly, in Table 7.2, the sMARE-AP values for Entropy and SD were almost identical for the Robust04- QL setting, while the highest correlation in Table 7.4 was observed for the TREC-DL20 - GDMTL setting, which had a more

|  | GDMTL | QL |
| :-- | :--: | :--: |
| Robust04 | 0.83 | 0.90 |
| TREC-DL20 | 0.53 | 0.92 |

Table 7.4: Pearson's $r$ correlation coefficients for the sARE-AP values between Entropy and SD for the GDMTL and QL models on the Robust04 and TREC-DL20 collections.
considerable difference in sMARE values, and an even bigger difference in correlation. This suggests that the correlation between the two methods is not directly related to the difference in their performance but instead to the characteristics of the collection and retrieval model.

To delve deeper into this analysis, Figure 7.5 visually compares the Entropy and SD methods through scatter plots and fitted linear regression lines for each collection and retrieval model. Each point on these plots represents the sARE-AP values per query, providing a visual representation of the methods' performance. On the Robust04 collection, the majority of queries demonstrated similar sARE values for both methods, as evidenced by the fitted lines being close to the diagonal. This consistency was observed across both retrieval models, suggesting that the Entropy and SD methods yield similar predictions for the Robust04 collection. However, on TREC-DL20, we observed greater variation in query error values, indicating more diverse performance outcomes for the predictors based on the query. Furthermore, Figure 7.5 illustrates that the prediction quality varied more for the GDMTL model compared to the QL model, further highlighting the challenges of prediction on TREC-DL20, particularly for the GDMTL model.

In summary, our observation of a high, though not perfect, similarity between the Entropy and SD methods suggests that they can effectively complement each other. Combining these methods in an ensemble has the potential to enhance prediction quality. Moreover, understanding the situations where these methods yield similar or distinct predictions provides valuable insights for researchers, aiding in the selection of the most suitable QPP methods for specific tasks. The creation of diverse QPP method ensembles can be advantageous, allowing for error correction and overall performance improvement. Although using a single retrieval model for each type in this study may limit the generalizability of the results, the observed trends are consistent with those found by Faggioli et al. [86], who used a wider range of models and datasets and noted consistency within each group of models.

Overall, our experimental results highlight the promising predictive performance of the Entropy method in information retrieval, especially within the Robust04 document collection. This method capitalizes on the inherent uncertainty in relevance probabilities, enabling it to effectively discriminate between successful and unsuccessful results. These findings underscore the significance of the Entropy method as a valuable addition to the field of QPP. Its straightforward implementation, requiring only a single hyperparameter, and computational efficiency make it an accessible and practical baseline for future research.

![img-73.jpeg](img-73.jpeg)

Figure 7.5: sARE-AP values per query for Entropy and SD. The plot illustrates the per-query sARE-AP values for the GDMTL and QL models on the Robust04 (left) and TREC-DL20 (right) collections. Each fitted line represents the linear regression between the two methods, and the shaded area around the line represents the $95 \\%$ confidence interval.

## 7.5 Discussion

### 7.5.1 Entropy in Neural Networks

Cross-entropy loss is one of the most commonly used optimization objectives in neural networks. During pre-training, neural language models (e.g., BERT) are trained to minimize cross-entropy loss. This loss function also serves as the foundation for many learning-to-rank losses, such as the pointwise binary cross-entropy loss, pairwise RankNet loss [38], and listwise ListNet loss [41]. In essence, neural networks are naturally \"entropy-aware\" as they are optimized to minimize entropy. During training, the smaller the entropy is, the better the model is believed to be trained. This focus on entropy during training raises an interesting question: does the entropy of predictions during inference correlate with the model's prediction performance? We conjecture that models with smaller inference entropy are more robust and, therefore, likely to be more effective. This idea stems from the notion that a well-trained model should exhibit more confident predictions, leading to lower entropy in its output distribution.

The cross-entropy loss for a single training data point has the general form of:

$$
-\\sum_{i} y_{i} \\log \\frac{\\exp (\\hat{y} i)}{\\sum_{j}^{n} \\hat{y}^{i} \\exp \\left(\\hat{y}^{i}\\right)}
$$

![img-74.jpeg](img-74.jpeg)

Figure 7.6: Comparison of entropy and SD for $k=1000$ across 249 queries. The plot illustrates the distribution of entropy and SD values for the GDMTL RSVs on the Robust04 collection. The spread along the SD axis is much smaller than that along the entropy axis, with multiple points overlapping each other.
where $y_{i} \\in 0,1$ represents the ground truth label and $\\hat{y} \\in \\mathbb{R}$ is the model's prediction score. This formulation ensures that the predicted probabilities are aligned with the true labels, driving the model towards minimizing its entropy during training.

# 7.5.2 EntropY VS SD

Figure 7.4 illustrates the relationship between the optimal hyperparameter values, $k$, for SD and entropy. Interestingly, the optimal $k$ for SD is relatively small, while the optimal $k$ for entropy is considerably larger. This discrepancy could be attributed to their mathematical properties. For entropy, the minimum value (zero) is attained when one document has a probability of 1.0, and all others have probabilities of 0.0 . The maximum value $(\\log (k))$ occurs when all documents have the same probability. On the other hand, SD reaches its minimum value of zero when all documents have the same score and its maximal value when the scores are evenly distributed around the mean (symmetrical bi-modal distribution) ${ }^{4}$ and has no theoretical upper limit. However, we can establish an empirical upper bound for SD by using Popoviciu's inequality on variances, given the upper bound $b$ and lower bound $a$ of the scores as $(b-a) / 2$. Consequently, the empirical maximal value of SD becomes fixed and independent of $k$. As a result, SD loses information as $k$ increases, as depicted in Figure 7.6. In contrast, Figure 7.7 shows that the maximal entropy increases logarithmically with $k$. This suggests that entropy retains information more effectively

[^0]
[^0]:    ${ }^{4}$ In practice, would be reached for an equal number of relevant and non-relevant documents, perfectly scored.

![img-75.jpeg](img-75.jpeg)

Figure 7.7: Empirical value ranges of entropy and SD with respect to $k$ across 249 queries. The plot was generated for the GDMTL RSVs on the Robust04 collection. The minimal values for each are zero; therefore, the maximal value represents the range. The empirical maximal value of SD depends only on the range of the scores and does not directly depend on $k$, while the maximal value of entropy increases logarithmically with $k$.
than SD for large $k$. Overall, this comparison highlights the different behaviors of entropy and SD concerning their optimal $k$ values and information retention. Interestingly, this notion of entropy capturing more information than SD has also been observed in other fields [31, 161].

## 7.6 SUMMARY

This chapter presents a comprehensive evaluation employing the previously introduced pairwise accuracy and sARE QPP evaluation metrics. The evaluation assesses the effectiveness of a novel QPP method, Entropy, and conducts a comparative analysis with five baseline methods, including SD based on NQC and Mean based on WIG. This assessment spans two document retrieval collections: ROBUSTO4 and TREC-DL20.

To address the first research question, \"How does the entropy-based QPP method compare to existing QPP approaches in terms of effectiveness in predicting the performance of NIR models?\", the findings indicate that Entropy performs comparably to SD with the QL retrieval approach. However, it outperforms SD with the GDMTL retrieval approach on ROBUSTO4, while both methods fall short on TREC-DL20 for the GDMTL model. To respond to the second research question, \"What are the advantages and limitations of the entropy-based QPP method compared to similar existing methods?\", the study analyzes the sensitivity of Entropy to changes in the hyperparameter $k$, demonstrating its higher robustness compared to SD. Furthermore, the experiments reveal that SD and Entropy excel or struggle with different queries, suggesting that

they capture distinct properties of the retrieval score distribution. These findings present the potential of utilizing Entropy as a feature or integrating it into ensemble models for further improvements, offering a promising direction for future research on score-based QPP methods.

In conclusion, the results demonstrate that Entropy is a simple yet promising method for QPP, comparable to current approaches while displaying higher robustness. These findings strongly support the adoption of Entropy as a valuable addition to the repertoire of QPP methods. This chapter exemplifies the previously proposed evaluation methods in practice, providing a comprehensive assessment of a new QPP method, offering detailed analysis into the effectiveness of Entropy and its comparative performance with existing methods in predicting the performance of NIR models.
